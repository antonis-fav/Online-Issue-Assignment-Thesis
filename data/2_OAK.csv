id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13389392,Address vulnerabilities found by dependency checker plugin,"{noformat}
One or more dependencies were identified with known vulnerabilities in Jackrabbit Oak:aggs-matrix-stats-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/aggs-matrix-stats-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
bcprov-jdk15on-1.65.jar (pkg:maven/org.bouncycastle/bcprov-jdk15on@1.65, cpe:2.3:a:bouncycastle:legion-of-the-bouncy-castle-java-crytography-api:1.65:*:*:*:*:*:*:*) : CVE-2020-28052
commons-io-2.6.jar (pkg:maven/commons-io/commons-io@2.6, cpe:2.3:a:apache:commons_io:2.6:*:*:*:*:*:*:*) : CVE-2021-29425
cxf-core-3.3.6.jar (pkg:maven/org.apache.cxf/cxf-core@3.3.6, cpe:2.3:a:apache:cxf:3.3.6:*:*:*:*:*:*:*) : CVE-2020-13954, CVE-2021-22696, CVE-2021-30468
elasticsearch-core-7.1.1.jar (pkg:maven/org.elasticsearch/elasticsearch-core@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
fluent-hc-4.5.12.jar (pkg:maven/org.apache.httpcomponents/fluent-hc@4.5.12, cpe:2.3:a:apache:httpclient:4.5.12:*:*:*:*:*:*:*) : CVE-2020-13956
groovy-2.5.2.jar (pkg:maven/org.codehaus.groovy/groovy@2.5.2, cpe:2.3:a:apache:groovy:2.5.2:*:*:*:*:*:*:*) : CVE-2020-17521
groovy-all-2.4.17.jar (pkg:maven/org.codehaus.groovy/groovy-all@2.4.17, cpe:2.3:a:apache:groovy:2.4.17:*:*:*:*:*:*:*) : CVE-2020-17521
guava-15.0.jar (pkg:maven/com.google.guava/guava@15.0, cpe:2.3:a:google:guava:15.0:*:*:*:*:*:*:*) : CVE-2018-10237, CVE-2020-8908
guava-18.0.jar (pkg:maven/com.google.guava/guava@18.0, cpe:2.3:a:google:guava:18.0:*:*:*:*:*:*:*) : CVE-2018-10237, CVE-2020-8908
hibernate-validator-5.3.6.Final.jar (pkg:maven/org.hibernate/hibernate-validator@5.3.6.Final, cpe:2.3:a:hibernate:hibernate-validator:5.3.6:*:*:*:*:*:*:*, cpe:2.3:a:redhat:hibernate_validator:5.3.6:*:*:*:*:*:*:*) : CVE-2020-10693
http2-client-9.4.27.v20200227.jar (pkg:maven/org.eclipse.jetty.http2/http2-client@9.4.27.v20200227, cpe:2.3:a:eclipse:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.27:20200227:*:*:*:*:*:*) : CVE-2019-17638, CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
httpclient-4.5.12.jar (pkg:maven/org.apache.httpcomponents/httpclient@4.5.12, cpe:2.3:a:apache:httpclient:4.5.12:*:*:*:*:*:*:*) : CVE-2020-13956
httpclient-osgi-4.5.12.jar/META-INF/maven/org.apache.httpcomponents/httpclient-cache/pom.xml (pkg:maven/org.apache.httpcomponents/httpclient-cache@4.5.12, cpe:2.3:a:apache:httpclient:4.5.12:*:*:*:*:*:*:*) : CVE-2020-13956
jackson-databind-2.10.3.jar (pkg:maven/com.fasterxml.jackson.core/jackson-databind@2.10.3, cpe:2.3:a:fasterxml:jackson-databind:2.10.3:*:*:*:*:*:*:*) : CVE-2020-25649
java-xmlbuilder-1.1.jar (pkg:maven/com.jamesmurty.utils/java-xmlbuilder@1.1) : CWE-611: Improper Restriction of XML External Entity Reference ('XXE')
javax-websocket-server-impl-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty.websocket/javax-websocket-server-impl@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:java-websocket_project:java-websocket:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
javax.servlet-3.0.0.v201112011016.jar (pkg:maven/org.eclipse.jetty.orbit/javax.servlet@3.0.0.v201112011016, cpe:2.3:a:eclipse:jetty:3.0.0:201112011016:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:3.0.0:201112011016:*:*:*:*:*:*) : CVE-2009-5045, CVE-2009-5046, CVE-2017-7656, CVE-2017-7657, CVE-2017-7658, CVE-2020-27216, CVE-2021-28169, CVE-2021-34428
javax.websocket-api-1.0.jar (pkg:maven/javax.websocket/javax.websocket-api@1.0, cpe:2.3:a:java-websocket_project:java-websocket:1.0:*:*:*:*:*:*:*) : CVE-2020-11050
jdom2-2.0.6.jar (pkg:maven/org.jdom/jdom2@2.0.6, cpe:2.3:a:jdom:jdom:2.0.6:*:*:*:*:*:*:*) : CVE-2021-33813
jetty-http-9.4.27.v20200227.jar (pkg:maven/org.eclipse.jetty/jetty-http@9.4.27.v20200227, cpe:2.3:a:eclipse:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.27:20200227:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.27:20200227:*:*:*:*:*:*) : CVE-2019-17638, CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
jetty-io-8.2.0.v20160908.jar (pkg:maven/org.eclipse.jetty/jetty-io@8.2.0.v20160908, cpe:2.3:a:mortbay_jetty:jetty:8.2.0:20160908:*:*:*:*:*:*) : CVE-2021-28165
jetty-io-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty/jetty-io@9.4.18.v20190429, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2021-28165
jetty-io-9.4.27.v20200227.jar (pkg:maven/org.eclipse.jetty/jetty-io@9.4.27.v20200227, cpe:2.3:a:mortbay_jetty:jetty:9.4.27:20200227:*:*:*:*:*:*) : CVE-2021-28165
jetty-server-8.2.0.v20160908.jar (pkg:maven/org.eclipse.jetty/jetty-server@8.2.0.v20160908, cpe:2.3:a:eclipse:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:8.2.0:20160908:*:*:*:*:*:*) : CVE-2017-7656, CVE-2017-7657, CVE-2017-7658, CVE-2017-9735, CVE-2019-10241, CVE-2019-10247, CVE-2020-27216, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
jetty-server-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty/jetty-server@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
jetty-util-8.2.0.v20160908.jar (pkg:maven/org.eclipse.jetty/jetty-util@8.2.0.v20160908, cpe:2.3:a:eclipse:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:8.2.0:20160908:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:8.2.0:20160908:*:*:*:*:*:*) : CVE-2017-7656, CVE-2017-7657, CVE-2017-7658, CVE-2019-10247, CVE-2020-27216, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
junit-4.12.jar (pkg:maven/junit/junit@4.12) : CVE-2020-15250
lang-mustache-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/lang-mustache-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
log4j-1.2.16.jar (pkg:maven/log4j/log4j@1.2.16, cpe:2.3:a:apache:log4j:1.2.16:*:*:*:*:*:*:*) : CVE-2019-17571, CVE-2020-9488
log4j-1.2.17.jar (pkg:maven/log4j/log4j@1.2.17, cpe:2.3:a:apache:log4j:1.2.17:*:*:*:*:*:*:*) : CVE-2019-17571, CVE-2020-9488
log4j-api-2.11.1.jar (pkg:maven/org.apache.logging.log4j/log4j-api@2.11.1, cpe:2.3:a:apache:log4j:2.11.1:*:*:*:*:*:*:*) : CVE-2020-9488
log4j-over-slf4j-1.7.30.jar (pkg:maven/org.slf4j/log4j-over-slf4j@1.7.30, cpe:2.3:a:apache:log4j:1.7.30:*:*:*:*:*:*:*) : CVE-2020-9488
mongo-java-driver-3.12.7.jar (pkg:maven/org.mongodb/mongo-java-driver@3.12.7, cpe:2.3:a:mongodb:java_driver:3.12.7:*:*:*:*:*:*:*) : CVE-2021-20328
netty-3.7.0.Final.jar (pkg:maven/io.netty/netty@3.7.0.Final, cpe:2.3:a:netty:netty:3.7.0:*:*:*:*:*:*:*) : CVE-2014-0193, CVE-2014-3488, CVE-2015-2156, CVE-2019-16869, CVE-2019-20444, CVE-2019-20445, CVE-2021-21290, CVE-2021-21295, CVE-2021-21409, POODLE vulnerability in SSLv3.0 support
netty-transport-4.1.47.Final.jar (pkg:maven/io.netty/netty-transport@4.1.47.Final, cpe:2.3:a:netty:netty:4.1.47:*:*:*:*:*:*:*) : CVE-2021-21290, CVE-2021-21295, CVE-2021-21409
netty-transport-4.1.52.Final.jar (pkg:maven/io.netty/netty-transport@4.1.52.Final, cpe:2.3:a:netty:netty:4.1.52:*:*:*:*:*:*:*) : CVE-2021-21290, CVE-2021-21295, CVE-2021-21409
oak-jackrabbit-api-1.34.0.jar (pkg:maven/org.apache.jackrabbit/oak-jackrabbit-api@1.34.0, cpe:2.3:a:apache:jackrabbit:1.34.0:*:*:*:*:*:*:*, cpe:2.3:a:apache:jackrabbit_oak:1.34.0:*:*:*:*:*:*:*) : CVE-2015-1833
oak-segment-1.6.0.jar (pkg:maven/org.apache.jackrabbit/oak-segment@1.6.0, cpe:2.3:a:apache:jackrabbit:1.6.0:*:*:*:*:*:*:*, cpe:2.3:a:apache:jackrabbit_oak:1.6.0:*:*:*:*:*:*:*) : CVE-2015-1833, CVE-2020-1940
org.apache.felix.webconsole-4.2.10-all.jar: jquery-1.8.3.js (pkg:javascript/jquery@1.8.3) : CVE-2012-6708, CVE-2015-9251, CVE-2019-11358, CVE-2020-11022, CVE-2020-11023
org.apache.felix.webconsole-4.2.10-all.jar: jquery-ui-1.9.2.js (pkg:javascript/jquery-ui-dialog@1.9.2, pkg:javascript/jquery-ui-tooltip@1.9.2) : CVE-2010-5312, CVE-2012-6662, CVE-2016-7103
pom.xml (pkg:maven/org.apache.jackrabbit/oak-jackrabbit-api@1.22.8-SNAPSHOT, cpe:2.3:a:apache:jackrabbit:1.22.8:snapshot:*:*:*:*:*:*, cpe:2.3:a:apache:jackrabbit_oak:1.22.8:snapshot:*:*:*:*:*:*) : CVE-2015-1833
pom.xml (pkg:maven/org.apache.jackrabbit/oak-solr-core@1.22.8-SNAPSHOT, cpe:2.3:a:apache:jackrabbit_oak:1.22.8:snapshot:*:*:*:*:*:*, cpe:2.3:a:apache:solr:1.22.8:snapshot:*:*:*:*:*:*) : CVE-2012-6612, CVE-2013-6397, CVE-2013-6407, CVE-2013-6408, CVE-2015-8795, CVE-2015-8796, CVE-2015-8797, CVE-2017-3163, CVE-2017-3164, CVE-2018-11802, CVE-2018-1308, CVE-2019-0193, CVE-2020-13941, CVE-2021-27905, CVE-2021-29262, CVE-2021-29943
org.apache.servicemix.bundles.dom4j-2.1.1_1.jar (pkg:maven/org.apache.servicemix.bundles/org.apache.servicemix.bundles.dom4j@2.1.1_1, cpe:2.3:a:dom4j_project:dom4j:2.1.1.1:*:*:*:*:*:*:*) : CVE-2020-10683
org.apache.sling.commons.logservice-1.0.4.jar (pkg:maven/org.apache.sling/org.apache.sling.commons.logservice@1.0.4, cpe:2.3:a:apache:sling:1.0.4:*:*:*:*:*:*:*) : CVE-2016-5394, CVE-2016-6798
parent-join-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/parent-join-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
pdfbox-2.0.19.jar (pkg:maven/org.apache.pdfbox/pdfbox@2.0.19, cpe:2.3:a:apache:pdfbox:2.0.19:*:*:*:*:*:*:*) : CVE-2021-27807, CVE-2021-27906, CVE-2021-31811, CVE-2021-31812
preflight-2.0.19.jar (pkg:maven/org.apache.pdfbox/preflight@2.0.19, cpe:2.3:a:apache:pdfbox:2.0.19:*:*:*:*:*:*:*) : CVE-2021-27807, CVE-2021-27906, CVE-2021-31811, CVE-2021-31812
rank-eval-client-7.1.1.jar (pkg:maven/org.elasticsearch.plugin/rank-eval-client@7.1.1, cpe:2.3:a:elastic:elasticsearch:7.1.1:*:*:*:*:*:*:*, cpe:2.3:a:elasticsearch:elasticsearch:7.1.1:*:*:*:*:*:*:*) : CVE-2019-7614, CVE-2019-7619, CVE-2020-7009, CVE-2020-7014, CVE-2020-7019, CVE-2020-7020, CVE-2020-7021
sentiment-analysis-parser-0.1.jar (pkg:maven/edu.usc.ir/sentiment-analysis-parser@0.1, cpe:2.3:a:data_tools_project:data_tools:0.1:*:*:*:*:*:*:*) : CVE-2018-18749
sis-netcdf-1.0.jar (pkg:maven/org.apache.sis.storage/sis-netcdf@1.0, cpe:2.3:a:storage_project:storage:1.0:*:*:*:*:*:*:*) : CVE-2021-20291
snakeyaml-1.17.jar (pkg:maven/org.yaml/snakeyaml@1.17, cpe:2.3:a:snakeyaml_project:snakeyaml:1.17:*:*:*:*:*:*:*) : CVE-2017-18640
solr-solrj-8.6.3.jar (pkg:maven/org.apache.solr/solr-solrj@8.6.3, cpe:2.3:a:apache:solr:8.6.3:*:*:*:*:*:*:*) : CVE-2021-27905, CVE-2021-29262, CVE-2021-29943
spring-core-4.3.24.RELEASE.jar (pkg:maven/org.springframework/spring-core@4.3.24.RELEASE, cpe:2.3:a:pivotal_software:spring_framework:4.3.24:release:*:*:*:*:*:*, cpe:2.3:a:springsource:spring_framework:4.3.24:release:*:*:*:*:*:*, cpe:2.3:a:vmware:spring_framework:4.3.24:release:*:*:*:*:*:*, cpe:2.3:a:vmware:springsource_spring_framework:4.3.24:release:*:*:*:*:*:*) : CVE-2020-5421
tagsoup-1.2.1.jar (pkg:maven/org.ccil.cowan.tagsoup/tagsoup@1.2.1, cpe:2.3:a:tag_project:tag:1.2.1:*:*:*:*:*:*:*) : CVE-2020-29242, CVE-2020-29243, CVE-2020-29244, CVE-2020-29245
tika-core-1.24.1.jar (pkg:maven/org.apache.tika/tika-core@1.24.1, cpe:2.3:a:apache:tika:1.24.1:*:*:*:*:*:*:*) : CVE-2021-28657
vorbis-java-tika-0.8.jar (pkg:maven/org.gagravarr/vorbis-java-tika@0.8, cpe:2.3:a:flac_project:flac:0.8:*:*:*:*:*:*:*) : CVE-2017-6888
websocket-common-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty.websocket/websocket-common@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:java-websocket_project:java-websocket:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:websocket-extensions_project:websocket-extensions:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
websocket-server-9.4.18.v20190429.jar (pkg:maven/org.eclipse.jetty.websocket/websocket-server@9.4.18.v20190429, cpe:2.3:a:eclipse:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:java-websocket_project:java-websocket:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:jetty:jetty:9.4.18:20190429:*:*:*:*:*:*, cpe:2.3:a:mortbay_jetty:jetty:9.4.18:20190429:*:*:*:*:*:*) : CVE-2020-27216, CVE-2020-27218, CVE-2020-27223, CVE-2021-28165, CVE-2021-28169, CVE-2021-34428
xmpbox-2.0.19.jar (pkg:maven/org.apache.pdfbox/xmpbox@2.0.19, cpe:2.3:a:apache:pdfbox:2.0.19:*:*:*:*:*:*:*) : CVE-2021-27807, CVE-2021-27906, CVE-2021-31811, CVE-2021-31812
zookeeper-3.4.6.jar (pkg:maven/org.apache.zookeeper/zookeeper@3.4.6, cpe:2.3:a:apache:zookeeper:3.4.6:*:*:*:*:*:*:*) : CVE-2016-5017, CVE-2017-5637, CVE-2018-8012, CVE-2019-0201, CVE-2021-21409
zookeeper-3.5.7.jar (pkg:maven/org.apache.zookeeper/zookeeper@3.5.7, cpe:2.3:a:apache:zookeeper:3.5.7:*:*:*:*:*:*:*) : CVE-2021-21409
-1,548 {noformat}",candidate_oak_1_22,[],OAK,Task,Major,2021-07-13 13:15:55,0
13381686,Cold Standby SSL certificates should be configurable,"The cold standby is able to do SSL connections to the primary, but currently only using on-the-fly generated certificates. This means that data is transferred over an encrypted connection but there is no protection against a man in the middle yet.

With this issue we want to:
* make server and client certificates configurable
* optionally validate the client certificate
* optionally only allow matching subjects in client and server certificates ",cold-standby,['segment-tar'],OAK,Improvement,Major,2021-06-02 12:03:13,0
13380389,Test failure: LeaseUpdateSocketTimeoutIT.leaseUpdateFailureOnSocketTimeout,"The test fails consistently on Jenkins but succeeds on Travis and when running locally.
{noformat}org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: Configured cluster node id 1 already in use: leaseEnd 1621945215196 > 1621945095197 - 119999ms in the future
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.createInstance(ClusterNodeInfo.java:628)
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.getInstance(ClusterNodeInfo.java:471)
	at org.apache.jackrabbit.oak.plugins.document.ClusterNodeInfo.getInstance(ClusterNodeInfo.java:440)
	at org.apache.jackrabbit.oak.plugins.document.mongo.LeaseUpdateSocketTimeoutIT.newClusterNodeInfo(LeaseUpdateSocketTimeoutIT.java:153)
	at org.apache.jackrabbit.oak.plugins.document.mongo.LeaseUpdateSocketTimeoutIT.leaseUpdateFailureOnSocketTimeout(LeaseUpdateSocketTimeoutIT.java:107)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
{noformat}",continuous_integration,['mongomk'],OAK,Bug,Minor,2021-05-25 16:50:29,1
13375040,Improve oak-run compact to better support Azure compaction,"Currently {{oak-run compact}} for Azure Segment Store always compacts the source container in place. It should be better to allow compacting the source container to a different container, allowing thus to skip source container cleanup. Moreover, in order to speed up compaction, Azure compaction should always employ a persistent disk cache, whose path for storing the segments and size should be configurable.",tooling,"['segment-azure', 'segment-tar']",OAK,Improvement,Major,2021-04-26 10:45:42,0
13372233,Breaking recovery lock issue,"There won't be a leaseEndTime when the recovering clusterId, the one referred to in {{recoveryBy}}, is not active anymore. The implementation of {{RecoveryLock.tryBreakRecoveryLock()}} should only call {{getLeaseEndTime()}} when {{recovering.isActive()}} is true.",candidate_oak_1_22 candidate_oak_1_6,['documentmk'],OAK,Bug,Major,2021-04-14 08:06:02,1
13329558,oak-run explore should support Azure Segment Store,"{{oak-run explore}} should accept Azure URIs for the segment store in order to be able to browse azure segments. 

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2020-09-26 21:14:22,0
13328580,Fix OSGi wiring after netty update to 4.1.52.Final,"After netty update in OAK-9210,  {{OSGiIT}} fails with the following exception:

{code}
ERROR: Bundle org.apache.jackrabbit.oak-segment-tar [41] Error starting file:/var/folders/jh/rvxkcm515dl3bksp1zlzdws80000gn/T/1600699057212-0/bundles/org.apache.jackrabbit.oak-segment-tar_1.35.0.SNAPSHOT.jar (org.osgi.framework.BundleException: Unable to resolve org.apache.jackrabbit.oak-segment-tar [41](R 41.0): missing requirement [org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate) Unresolved requirements: [[org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate)])
org.osgi.framework.BundleException: Unable to resolve org.apache.jackrabbit.oak-segment-tar [41](R 41.0): missing requirement [org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate) Unresolved requirements: [[org.apache.jackrabbit.oak-segment-tar [41](R 41.0)] osgi.wiring.package; (osgi.wiring.package=com.oracle.svm.core.annotate)]
	at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:4368)
	at org.apache.felix.framework.Felix.startBundle(Felix.java:2281)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1539)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)
	at java.base/java.lang.Thread.run(Thread.java:835)
{code}",cold-standby,['segment-tar'],OAK,Bug,Major,2020-09-21 14:41:38,0
13327862,Bump netty dependency from 4.1.17.Final to 4.1.52.Final,"The current version presents several vulnerability issues: 

BDSA-2018-4022,BDSA-2018-4482,BDSA-2019-2642,BDSA-2019-2643.",cold-standby,['segment-tar'],OAK,Task,Major,2020-09-16 12:56:31,0
13326424,PersistentRedisCache: failure to write segment is not an error,"Failure to write a segment to the redis cache results in an error level log message with a stack trace. However, this is expected behaviour: socket timeouts prevent the cache from effectively slowing down a request. OTOH too many socket timeouts make the cache ineffective, so it's good to have a way to log such errors when debugging. My suggestion is therefore to change the log level to ""debug"" and avoid the stack trace.",patch,['segment-azure'],OAK,Bug,Minor,2020-09-08 09:23:23,0
13301237,Improve azure archive recovery during startup,"During repository startup if archive directory is not closed properly, recovery will be performed. During that procedure, segents are copied to the backup directory and deleted from the source direcory, one by one.

It can create problems and negativelly impact other ongoing actiivties, which are accessing the same archive. This activity, for example, can be repository cloning in order to create new environment. 

Proposed patch, after creating backup is not deleting all segments from archive, but only segments which could not be recovered. 

[^proposal.patch]

 

+API change+

Proposed patch is changing major version of exported SPI package, org.apache.jackrabbit.oak.segment.spi.persistence.split.

 ",Patch,"['segment-azure', 'segment-tar']",OAK,Improvement,Major,2020-04-27 11:21:56,0
13291031,ObservationManager.addEventListener() throws NPE with invalid paths in filter,"While registering a resource change listener, we encountered the following exception : 

 
{code:java}
05.03.2020 23:39:00.728 *ERROR* [FelixDispatchQueue] org.apache.sling.resourceresolver FrameworkEvent ERROR (java.lang.NullPointerException)
java.lang.NullPointerException: null
at org.apache.jackrabbit.oak.commons.PathUtils.unifyInExcludes(PathUtils.java:501) [org.apache.jackrabbit.oak-commons:1.8.17]
at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.addEventListener(ObservationManagerImpl.java:240) [org.apache.jackrabbit.oak-jcr:1.8.17]
at org.apache.sling.jcr.resource.internal.JcrListenerBaseConfig.register(JcrListenerBaseConfig.java:136) [org.apache.sling.jcr.resource:3.0.16.1]
{code}
 

On further debugging, we found that issues lies in this snippet : 
{code:java}
if (exclude.equals(include) || isAncestor(exclude, include)) {
 includesRemoved.add(include);{code}
'exclude' can be null if the getOakPath() method returns a null. This NPE causes listeners(ResourceChangeListener in our case) to fail at registration.",Observation,['jcr'],OAK,Bug,Minor,2020-03-11 09:31:47,1
13289743,Oak run recovery fails when running on mongo replicaSet with auth enabled,"When running oak run jar in recovery mode on a mongo replica set with auth enabled. it fails to pass the auth data for a findOne command called in *GetRootRevisionsCallable*

 
{code:java}
DBObject root = collection.findOne(new BasicDBObject(Document.ID, ""0:/""));{code}
Stack Trace as below
 07:07:27.790 [MongoDocumentStore replica set info provider] ERROR o.a.j.o.p.d.m.replica.ReplicaSetInfo - Can't connect to the Mongo instance07:07:27.790 [MongoDocumentStore replica set info provider] ERROR o.a.j.o.p.d.m.replica.ReplicaSetInfo - Can't connect to the Mongo instancejava.util.concurrent.ExecutionException: com.mongodb.MongoQueryException: Query failed with error code 13 and error message 'not authorized on dampro64tmp to execute command { find: ""nodes"", filter:

{ _id: ""0:/"" }

, limit: 1, singleBatch: true }' on server at java.util.concurrent.FutureTask.report(FutureTask.java:122) at java.util.concurrent.FutureTask.get(FutureTask.java:192) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.getRootRevisions(ReplicaSetInfo.java:346) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.updateRevisions(ReplicaSetInfo.java:269) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.updateReplicaStatus(ReplicaSetInfo.java:181) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.updateLoop(ReplicaSetInfo.java:144) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.run(ReplicaSetInfo.java:133) at java.lang.Thread.run(Thread.java:745)Caused by: com.mongodb.MongoQueryException: Query failed with error code 13 and error message 'not authorized on DB to execute command { find: ""nodes"", filter:

{ _id: ""0:/"" }

, limit: 1, singleBatch: true }' on server at com.mongodb.operation.FindOperation$1.call(FindOperation.java:722) at com.mongodb.operation.FindOperation$1.call(FindOperation.java:711) at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:471) at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:415) at com.mongodb.operation.FindOperation.execute(FindOperation.java:711) at com.mongodb.operation.FindOperation.execute(FindOperation.java:83) at com.mongodb.Mongo$3.execute(Mongo.java:826) at com.mongodb.Mongo$3.execute(Mongo.java:813) at com.mongodb.DBCursor.initializeCursor(DBCursor.java:877) at com.mongodb.DBCursor.hasNext(DBCursor.java:144) at com.mongodb.DBCursor.one(DBCursor.java:683) at com.mongodb.DBCollection.findOne(DBCollection.java:829) at com.mongodb.DBCollection.findOne(DBCollection.java:792) at com.mongodb.DBCollection.findOne(DBCollection.java:739) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.GetRootRevisionsCallable.call(GetRootRevisionsCallable.java:58) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.GetRootRevisionsCallable.call(GetRootRevisionsCallable.java:34) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) at org.apache.jackrabbit.oak.plugins.document.mongo.replica.ReplicaSetInfo.getRootRevisions(ReplicaSetInfo.java:340) ... 5 common frames omitted",candidate_oak_1_6,['run'],OAK,Task,Minor,2020-03-05 09:03:15,1
13281162,Access azure segments metadata in a case-insensitive way,"We use azcopy to copy segments from one azure blob container to another for testing. There is a bug in the current version of azcopy (10.3.3), which makes all metadata keys start with a capital letter - ""type"" becomes ""Type"". As a consequence, the current implementation can not find the segments in the azure blob storage. 
 
The azcopy issue was already reported [1] in 2018. I have little hope that azcopy will be fixed soon.
 
Therefore I suggest a patch to oak-segment-azure, that would be backward compatible and ignore the case of the keys when reading metadata. We should be strict in what we write and tolerant in what we read.  ",azureblob,['segment-azure'],OAK,Improvement,Major,2020-01-23 10:52:52,0
13264800,Oak run check command must return the status of repository consistency check,"Currently the consistency check reports only if the command runs successfully (return code 0) or fails (return code 1).
Into this logic will also add the status of repository consistency:
- checking only the last revision: will return 0 if the revision is consistent and the command runs successfully OR will return 1 if the revision is inconsistent or job did not run successfully (some errors/exception were encountered during the run)
- checking multiple revisions: will return 0 if at least one revision is consistent and the job runs successfully OR will return 1 if none of the revisions are consistent or the command did not run successfully (some errors/exception were encounter during the run) ",oak-run segment-tar,[],OAK,Improvement,Major,2019-10-28 11:32:37,0
13260246,Merge may fail when commit root is a bundled node,"Changing and merging descendant nodes of a bundled node fails when the commit root of the changes is located on a bundled node. The merge tries to apply the final commit changes on a document that does not exist (because the bundled node is located on an ancestor document).

The exception is misleading but looks like this:
{noformat}
Caused by: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: Conflicting concurrent change. Update operation failed: key: 3:/foo/bar/baz update {_revisions.r16d8bf18282-0-1=SET_MAP_ENTRY c, _modified=MAX 1570010920}
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:398) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStoreWithTiming(Commit.java:278) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:262) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:230) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:218) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:320) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:282) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$500(DocumentNodeStoreBranch.java:56) [org.apache.jackrabbit.oak-store-document:1.18.0]
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:548) [org.apache.jackrabbit.oak-store-document:1.18.0]
{noformat}",candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Major,2019-10-03 07:16:14,1
13257125,Node bundling exposes hidden properties,The DocumentNodeStore node bundling feature may expose a hidden internal property when a bundled node structure is deleted and re-created with a non-bundling nodetype.,candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Minor,2019-09-17 13:53:52,1
13251784,Backport OAK-8066 to 1.10 and 1.8,Backport OAK-8066 to 1.10 and 1.8.,TarMK,['segment-tar'],OAK,Improvement,Blocker,2019-08-20 08:50:59,2
13240824,oak-run check should expose repository statistics for the last good revision,"{{oak-run check}} should expose the head node and property counts for the last good revision. Currently these are only logged at the end of the check operation as
{noformat}
Checked X nodes and Y properties.{noformat}",tooling,"['oak-run', 'segment-tar']",OAK,Improvement,Minor,2019-06-21 09:53:54,0
13236196,Add remote store monitoring  for Azure,"Add remote store monitoring
Implement the remote store monitoring for Azure Store. This should include:
- request_count : number of request to azure store
- error_count : number of failed requests to azure store
- duration : duration of a request to azure store in nanoseconds ",TarMK,['segment-azure'],OAK,New Feature,Major,2019-05-29 09:11:13,2
13235965,oak-run check should have an option for specifying memory mapping,"{{oak-run check}} currently uses memory mapping by default when building the {{FileStore}}. This setting should be configurable, to allow switching memory mapping off.",tooling,"['run', 'segment-tar']",OAK,New Feature,Minor,2019-05-28 10:16:13,0
13229853,Orphaned branch commit entries after restart,"The DocumentNodeStore does not clean up orphaned branch commit entries ({{_bc}}) after a restart. Cleanup for those entries happens while the DocumentNodeStore is running as well, but only when the branch is not referenceable anymore. In some cases it may happen that a branch is referenced right until the DocumentNodeStore is disposed and the cleanup must happen on startup.",candidate_oak_1_8,['documentmk'],OAK,Bug,Minor,2019-04-24 12:06:17,1
13218575,The cold standby server cannot handle blob requests for long blob IDs,"If the standby client issues a request for a binary ID larger than 8192 bytes, it will fail on the server side due to the current frame limitation, set to 8192 bytes:
{noformat}
28.02.2019 00:01:36.034 *WARN* [primary-32] org.apache.jackrabbit.oak.segment.standby.server.ExceptionHandler Exception caught on the server
io.netty.handler.codec.TooLongFrameException: frame length (35029) exceeds the allowed maximum (8192)
        at io.netty.handler.codec.LineBasedFrameDecoder.fail(LineBasedFrameDecoder.java:146) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.fail(LineBasedFrameDecoder.java:142) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.decode(LineBasedFrameDecoder.java:131) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.LineBasedFrameDecoder.decode(LineBasedFrameDecoder.java:75) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1342) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:934) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [org.apache.jackrabbit.oak-segment-tar:1.10.1]
        at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2019-02-28 09:13:13,0
13218371,Add refresh head revision time to background update stats,The background update stats does not have timing information for refreshing the head revision.,candidate_oak_1_6,['documentmk'],OAK,Improvement,Minor,2019-02-27 15:12:39,1
13217162,Log warning for too many transient modifications of direct child nodes,"In a first step towards resolving OAK-8066, I want to add some logging regarding the number of transiently modified direct child nodes in {{DefaultSegmentWriter}} ",TarMK,['segment-tar'],OAK,Technical task,Blocker,2019-02-21 10:56:42,3
13217128,Nodes with many direct children can lead to OOME when saving,{{DefaultSegmentWriter}} keeps a map of [child nodes|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/DefaultSegmentWriter.java#L805] of a node being written. This can lead to high memory consumption in the case where many child nodes are added at the same time. The latter could happen in the case where a node needs to be rewritten because of an increase in the GC generation from a concurrently completed revision garbage collection.,TarMK,['segment-tar'],OAK,Improvement,Major,2019-02-21 08:35:49,3
13216958,The cold standby client doesn't correctly handle backward references,"The logic from {{StandbyClientSyncExecution#copySegmentHierarchyFromPrimary}} has a flaw when it comes to ""backward references"". Suppose we have the following data segment graph to be transferred from primary: S1, which references \{S2, S3} and S3 which references S2. Then, the correct transfer order should be S2, S3 and S1.

Going through the current logic employed by the method, here's what happens:
{noformat}
Step 0: batch={S1}

Step 1: visited={S1}, data={S1}, batch={S2, S3}, queued={S2, S3}

Step 2: visited={S1, S2}, data={S2, S1}, batch={S3}, queued={S2, S3}

Step 3: visited={S1, S2, S3}, data={S3, S2, S1}, batch={}, queued={S2, S3}.{noformat}
Therefore, at the end of the loop, the order of the segments to be transferred will be S3, S2, S1, which might trigger a {{SegmentNotFoundException}} when S3 is further processed, because S2 is missing on standby (see OAK-8006).

/cc [~frm]",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2019-02-20 15:20:23,0
13214937,Intermittent test failure of CompactionAndCleanupIT.testMixedSegments,"{{CompactionAndCleanupIT.testMixedSegments}} fails every 50-th build or so:
{code:java}
[ERROR] testMixedSegments(org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT)  Time elapsed: 1.064 s  <<< FAILURE!
java.lang.AssertionError: Mixed segments found: 7395f14c-15dd-4224-ad53-ec981f46f5cd
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.testMixedSegments(CompactionAndCleanupIT.java:701)
{code}
 

This might have the same root cause as OAK-8033. But no causality found yet.",TarMK,[],OAK,Bug,Critical,2019-02-11 08:11:35,3
13214414,Node states sometimes refer to more than a single generation of segments after a full compaction,"Due to a regression introduced with OAK-7867 a full compaction can sometimes cause nodes that are written concurrently to reference segments from more than a single gc generation.

This happens when the {{borrowWriter}} method needs to [create a new writer|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentBufferWriterPool.java#L197-L201]. In this case the new writer will be of the generation of the current head state instead of the generation associated with the current write operation in progress.

 

cc [~frm], [~ahanikel]

 ",TarMK,['segment-tar'],OAK,Bug,Major,2019-02-07 16:09:53,3
13212224,SegmentBlob#readLongBlobId might cause SegmentNotFoundException on standby,"When persisting a segment transferred from master, among others, the cold standby needs to read the binary references from the segment. While this usually doesn't involve any additional reads from any other segments, there is a special case concerning binary IDs larger than 4092 bytes. These can live in other segments (which got transferred prior to the current segment and are already on the standby), but it might also be the case that the binary ID is stored in the same segment. If this happens, the call to {{blobId.getSegment()}}[0], triggers a new read of the current, un-persisted segment . Thus, a {{SegmentNotFoundException}} is thrown:
{noformat}
22.01.2019 09:35:59.345 *ERROR* [standby-run-1] org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSync Failed synchronizing state.
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment d40a9da6-06a2-4dc0-ab91-5554a33c02b0 not found
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readSegmentUncached(AbstractFileStore.java:284) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.lambda$readSegment$10(FileStore.java:498) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentCache$NonEmptyCache.lambda$getSegment$0(SegmentCache.java:163) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3932) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) [com.adobe.granite.osgi.wrapper.guava:15.0.0.0002]
        at org.apache.jackrabbit.oak.segment.SegmentCache$NonEmptyCache.getSegment(SegmentCache.java:160) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:498) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:153) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:98) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(SegmentBlob.java:206) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.SegmentBlob.readBlobId(SegmentBlob.java:163) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore$3.consume(AbstractFileStore.java:262) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.Segment.forEachRecord(Segment.java:601) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readBinaryReferences(AbstractFileStore.java:257) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:533) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.copySegmentFromPrimary(StandbyClientSyncExecution.java:225) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.copySegmentHierarchyFromPrimary(StandbyClientSyncExecution.java:194) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.compareAgainstBaseState(StandbyClientSyncExecution.java:101) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSyncExecution.execute(StandbyClientSyncExecution.java:76) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.jackrabbit.oak.segment.standby.client.StandbyClientSync.run(StandbyClientSync.java:165) [org.apache.jackrabbit.oak-segment-tar:1.10.0]
        at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:347) [org.apache.sling.commons.scheduler:2.7.2]
        at org.quartz.core.JobRunShell.run(JobRunShell.java:202) [org.apache.sling.commons.scheduler:2.7.2]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:834){noformat}
 

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentBlob.java#L205",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2019-01-28 12:24:15,0
13208546,Add multi-threaded segment transfer to oak-run segment-copy,"The transfer of segments between different persistence types when using {{oak-run segment-copy}} can be sped up by employing multiple threads in the transfer. The idea is to try to load {{n}} segments from the source, which are then consumed by the writer on the target, keeping the ordering of the segments in the process. ",tooling,"['oak-run', 'segment-azure']",OAK,Improvement,Minor,2019-01-09 13:36:03,0
13185131,"deadlock TarMK flush, lucene","We are getting the following deadlock. Please help! (Production environment)

I have already annotated possible locks and synchronized blocks in between:

{noformat}
""TarMK flush [/opt/condat/epet9/sling/repository/segmentstore]"":
  waiting to lock Monitor@0x00007fedfc00cc28 (Object@0x00000004795519a8, a org/apache/jackrabbit/oak/segment/SegmentId),
  which is held by ""oak-lucene-14""
""oak-lucene-14"":
 waiting for ownable synchronizer 0x00000003c13818c0, (a java/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync),
 which is held by ""TarMK flush [/opt/condat/epet9/sling/repository/segmentstore]""

Thread 28883: (state = BLOCKED)
 - org.apache.jackrabbit.oak.segment.SegmentId.getSegment() @bci=12, line=121 (Compiled frame)
        synchronized (this) 
 - org.apache.jackrabbit.oak.segment.Record.getSegment() @bci=4, line=70 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.BlockRecord.read(int, byte[], int, int) @bci=49, line=57 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.read(byte[], int, int) @bci=314, line=189 (Compiled frame)
 - com.google.common.io.ByteStreams.read(java.io.InputStream, byte[], int, int) @bci=43, line=828 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[], int, int) @bci=4, line=695 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[]) @bci=5, line=676 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.getString() @bci=93, line=103 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.readString(int) @bci=189, line=524 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=15, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=37, line=167 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.AbstractFileStore$4.consume(int, org.apache.jackrabbit.oak.segment.RecordType, int) @bci=24, line=354 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.forEachRecord(org.apache.jackrabbit.oak.segment.Segment$RecordConsumer) @bci=48, line=716 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.AbstractFileStore.populateTarBinaryReferences(org.apache.jackrabbit.oak.segment.Segment, org.apache.jackrabbit.oak.segment.file.TarWriter) @bci=25, line=349 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(org.apache.jackrabbit.oak.segment.SegmentId, byte[], int, int) @bci=136, line=657 (Compiled frame)
        fileStoreLock.writeLock().lock(); Zeile 639
        bis: populateTarBinaryReferences
 - org.apache.jackrabbit.oak.segment.SegmentBufferWriter.flush() @bci=383, line=383 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.flush() @bci=165, line=148 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentWriter.flush() @bci=4, line=143 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$7.call() @bci=7, line=373 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$7.call() @bci=1, line=370 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.TarRevisions.doFlush(java.util.concurrent.Callable) @bci=25, line=224 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.TarRevisions.flush(java.util.concurrent.Callable) @bci=42, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.flush() @bci=20, line=370 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$2.run() @bci=15, line=233 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.SafeRunnable.run() @bci=21, line=67 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)

Locked ownable synchronizers:
    - <0x00000003c1361228>, (a java/util/concurrent/locks/ReentrantLock$NonfairSync)
    - <0x00000003c13818c0>, (a java/util/concurrent/locks/ReentrantReadWriteLock$NonfairSync)
    - <0x00000003c1c3a0d8>, (a java/util/concurrent/ThreadPoolExecutor$Worker)

Thread 31035: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(int) @bci=83, line=967 (Interpreted frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(int) @bci=10, line=1283 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock() @bci=5, line=727 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore$8.call() @bci=158, line=567 (Compiled frame)
    fileStoreLock.readLock().lock();
 - org.apache.jackrabbit.oak.segment.file.FileStore$8.call() @bci=1, line=542 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(org.apache.jackrabbit.oak.segment.SegmentId, java.util.concurrent.Callable) @bci=1, line=95 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(org.apache.jackrabbit.oak.segment.SegmentId) @bci=14, line=542 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentId.getSegment() @bci=38, line=125 (Compiled frame)
    synchronized (this) 
 - org.apache.jackrabbit.oak.segment.Record.getSegment() @bci=4, line=70 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.BlockRecord.read(int, byte[], int, int) @bci=49, line=57 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.read(byte[], int, int) @bci=314, line=189 (Compiled frame)
 - com.google.common.io.ByteStreams.read(java.io.InputStream, byte[], int, int) @bci=64, line=833 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[], int, int) @bci=4, line=695 (Compiled frame)
 - com.google.common.io.ByteStreams.readFully(java.io.InputStream, byte[]) @bci=5, line=676 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentStream.getString() @bci=93, line=103 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.Segment.readString(int) @bci=189, line=524 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.readLongBlobId(org.apache.jackrabbit.oak.segment.Segment, int) @bci=15, line=212 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentBlob.length() @bci=124, line=115 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.<init>(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeBuilder, java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$BlobFactory) @bci=204, line=409 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.<init>(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeBuilder, java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$BlobFactory) @bci=25, line=589 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.fileLength(java.lang.String) @bci=64, line=176 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.copyFilesToLocal(org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory$CORFileReference, boolean, boolean) @bci=195, line=214 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.prefetchIndexFiles() @bci=96, line=170 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.directory.CopyOnReadDirectory.<init>(org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier, org.apache.lucene.store.Directory, org.apache.lucene.store.Directory, boolean, java.lang.String, java.util.concurrent.Executor) @bci=85, line=81 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.wrapForRead(java.lang.String, org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.lucene.store.Directory, java.lang.String) @bci=35, line=122 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.reader.DefaultIndexReaderFactory.createReader(org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.jackrabbit.oak.spi.state.NodeState, java.lang.String, java.lang.String, java.lang.String) @bci=61, line=102 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.reader.DefaultIndexReaderFactory.createReaders(org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition, org.apache.jackrabbit.oak.spi.state.NodeState, java.lang.String) @bci=20, line=61 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.plugins.index.lucene.reader.LuceneIndexReaderFactory, org.apache.jackrabbit.oak.plugins.index.lucene.hybrid.NRTIndexFactory) @bci=17, line=68 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker$1.leave(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=30, line=132 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=66, line=153 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.MapRecord.compare(org.apache.jackrabbit.oak.segment.MapRecord, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=197, line=415 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=909, line=608 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.childNodeChanged(java.lang.String, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=43, line=148 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.MapRecord.compare(org.apache.jackrabbit.oak.segment.MapRecord, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=400, line=457 (Compiled frame)
 - org.apache.jackrabbit.oak.segment.SegmentNodeState.compareAgainstBaseState(org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeStateDiff) @bci=909, line=608 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(org.apache.jackrabbit.oak.spi.commit.Editor, org.apache.jackrabbit.oak.spi.state.NodeState, org.apache.jackrabbit.oak.spi.state.NodeState) @bci=34, line=52 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.diffAndUpdate(org.apache.jackrabbit.oak.spi.state.NodeState) @bci=140, line=142 (Compiled frame)
 - org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.update(org.apache.jackrabbit.oak.spi.state.NodeState) @bci=36, line=113 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call() @bci=79, line=135 (Compiled frame)
 - org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call() @bci=1, line=128 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Compiled frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)

Locked ownable synchronizers:
    - <0x0000000476194f30>, (a java/util/concurrent/ThreadPoolExecutor$Worker)   

{noformat}

 ",deadlock,['segment-tar'],OAK,Bug,Blocker,2018-09-14 12:25:29,2
13184064,Clarify update semantics on deleted nodes,"It's not entirely clear to me what behavior we expect if:
 
 * node 1 creates a document
 * node 2 deletes it
 * node 1 tries to update it

(and, related to that, whether the behavior really matters in practice)",candidate_oak_1_8,['documentmk'],OAK,Task,Major,2018-09-10 14:12:44,1
13181865,CheckCommand should consistently use an alternative journal if specified,"Callers of the {{check}} command can specify an alternative journal with the {{\-\-journal}} option. This option instructs the {{ConsistencyChecker}} to check the revisions stored in that file instead of the ones stored in the default {{journal.log}}.

I spotted at least two problems while using {{\-\-journal}} on a repository with a corrupted {{journal.log}} that didn't contain any valid revision.

First, the path to the {{FileStore}} is validated by {{FileStoreHelper#isValidFileStoreOrFail}}, which checks for the existence of a {{journal.log}} in the specified folder. But if a {{journal.log}} doesn't exist and the user specified a different journal on the command line this check should be ignored.

Second, when opening the {{FileStore}} the default {{journal.log}} is scanned to determine the initial revision of the head state. If a user specifies an alternative journal on the command line, that journal should be used instead of the default {{journal.log}}. It might be that the default journal contains no valid revision, which would force the system to crash when opening a new instance of {{FileStore}}.",technical_debt,"['run', 'segment-tar']",OAK,Bug,Major,2018-08-29 10:12:04,2
13181405,Documentation for direct binary access is unclear,"In the opening paragraph of the documentation on Direct Binary Access, reference is made to S3DataStore and AzureDataStore as blob stores that support this feature, but is is not clear whether they are mentioned as examples or as an exhaustive list of supporting blob stores.  It is only on further examination of the documentation that you are able to determine that the list was specific and not as examples.
We should change the documentation so it is more clear up front which blob stores support the feature.",doc,['doc'],OAK,Bug,Minor,2018-08-27 16:38:04,1
13180468,Test failure: SecurityProviderRegistrationTest.testRequiredUserAuthenticationFactoryNotAvailable(),Some SecurityProviderRegistrationTest.testRequiredUserAuthenticationFactoryNotAvailable() fails every now and then. The last occurrence on travis-ci was here: https://travis-ci.org/apache/jackrabbit-oak/jobs/414016611 but I've also seen the same test fail on other infrastructure.,continuous_integration,['pojosr'],OAK,Bug,Minor,2018-08-22 13:09:41,1
13179095,Backport OAK-6648 to Oak 1.6,"We have seen instances were offline compaction did not clean up all reclaimable tar files on the first run but would reclaim them on subsequent runs. Apparently this is caused by OAK-6648, which fixes an issue where the file reaper is invoked without a prior call to {{System.gc}} reclaiming potential references.",TarMK,['segment-tar'],OAK,Task,Major,2018-08-15 14:05:52,3
13177340,Replace usage of static ValueFactoryImpl methods,The ValueFactoryImpl has a few static methods that are used to create a {{Value}} from a {{PropertyState}} or {{PropertyValue}}. Those methods should be refactored to make it easier to  add a {{BlobAccessProvider}} for OAK-7569.,technical_debt,"['core', 'jcr', 'security-spi', 'store-spi']",OAK,Improvement,Minor,2018-08-07 09:08:37,1
13177087,oak-commons: upgrade to project default mockito version,"Unfortunately, mocking varargs seems to work different in current mockito versions. With the 2:* default from the parent pom, I'm getting the test failure below:
{noformat}
[ERROR] Tests run: 12, Failures: 8, Errors: 0, Skipped: 0, Time elapsed: 0.828 s <<< FAILURE! - in org.apache.jackrabbit.oak.commons.PerfLoggerTest
[ERROR] logAtDebugMessageStartWithInfoLog(org.apache.jackrabbit.oak.commons.PerfLoggerTest)  Time elapsed: 0.045 s  <<< FAILURE!
org.mockito.exceptions.verification.junit.ArgumentsAreDifferent:

Argument(s) are different! Wanted:
logger.debug(
    <any string>,
    <any java.lang.Object[]>
);
-> at org.apache.jackrabbit.oak.commons.PerfLoggerTest.verifyDebugInteractions(PerfLoggerTest.java:227)
Actual invocation has different arguments:
logger.debug(
    ""message [took 0ms]"",
    ""argument""
);
-> at org.apache.jackrabbit.oak.commons.PerfLogger.end(PerfLogger.java:223)

        at org.apache.jackrabbit.oak.commons.PerfLoggerTest.verifyDebugInteractions(PerfLoggerTest.java:227)
        at org.apache.jackrabbit.oak.commons.PerfLoggerTest.logAtDebugMessageStartWithInfoLog(PerfLoggerTest.java:144)


{noformat}",candidate_oak_1_6,['commons'],OAK,Task,Minor,2018-08-06 12:01:27,1
13176090,Introduce oak-run segment-copy for moving around segments in different storages,"Often there's the need to transform a type of {{SegmentStore}} (e.g. local TarMK) into *the exact same* counter-part, using another persistence type (e.g. Azure Segment Store). While {{oak-upgrade}} partially solves this through sidegrades (see OAK-7623), there's a gap in the final content because of the level at which {{oak-upgrade}} operates (node store level). Therefore, the resulting sidegraded repository doesn't contain all the (possibly stale, unreferenced) data from the original repository, but only the latest head state. A side effect of this is that the resulting repository is always compacted.

Introducing a new command in {{oak-run}}, namely {{segment-copy}}, would allow us to operate at a lower level (i.e. segment persistence), dealing only with constructs from {{org.apache.jackrabbit.oak.segment.spi.persistence}}: journal file, gc journal file, archives and archive entries. This way the only focus of this process would be to ""translate"" a segment between two persistence formats, without caring about the node logic stored inside (referenced/unreferenced node/property).",tooling,"['oak-run', 'segment-tar']",OAK,Improvement,Major,2018-08-01 11:34:04,0
13174984,Refactor AzureCompact and Compact,"{{AzureCompact}} in {{oak-segment-azure}} follows closely the structure and logic of {{Compact}} in {{oak-segment-tar}}. Since the only thing which differs is the underlying persistence used (remote in Azure vs. local in TAR files), the common logic should be extracted in a super-class, extended by both. ",tech-debt technical_debt tooling,['segment-tar'],OAK,Improvement,Major,2018-07-26 20:31:35,0
13173120,Remove strategy to optimize secondary reads,OAK-3865 introduced a strategy to optimize reads from secondaries. This has been superseded by OAK-6087. This task is about removing the old strategy.,technical_debt,['mongomk'],OAK,Task,Major,2018-07-19 07:26:21,1
13172835,Revert changes done by OAK-6770,"With the changes from OAK-6770 applied, it seems that {{repository.home}} attribute is not correctly set, causing the repository to be written one level up in the directory hierarchy from where it was supposed to. ",osgi,['segment-tar'],OAK,Technical task,Major,2018-07-18 06:05:59,2
13172627,Prevent commits in the past,"This is similar to OAK-3883, but must prevent commits with revisions that are older than already present in the repository. At runtime, this is already taken care of with static fields in the Revision class, but on startup the clock may have jumped into the past since Oak was stopped.",resilience,['documentmk'],OAK,Improvement,Minor,2018-07-17 11:49:32,1
13172315,oak-run check should support Azure Segment Store,"{{oak-run check}} should accept Azure URIs for the segment store in order to be able to check for data integrity. This will come handy in the light of remote compacted segment stores and/or sidegraded remote segment stores (see OAK-7623, OAK-7459).

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2018-07-16 06:43:34,0
13171335,SegmentNodeStore - sidegrade support between TarPersistence and AzurePersistence,"Azure support for segment-tar (OAK-6922) allowed us to plug another storage option for the segment store. Since sometimes there's the need to compare how local vs remote storage behaves, a sidegrade from local tar storage to remote azure storage must be implemented.

This would allow us to replicate the exact repository content, changing only the underlying storage mechanism. Analogous to OAK-7459, the Azure Segment Store connection details will be supplied in the following format:
 * an URI with the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where _az_ identifies the cloud provider
 * a secret key supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",azure migration,"['segment-tar', 'upgrade']",OAK,New Feature,Major,2018-07-11 08:23:12,0
13167181,Commit fails when forced journal push throws exception,"OAK-3976 introduced a force push of of a journal entry during the commit when the accumulated changes reach 100'000 elements.

Creating the journal entry however may fail with a DocumentStoreException and fail the commit even though all changes, including the one on the commit root, made it to the DocumentStore.",candidate_oak_1_6,['documentmk'],OAK,Bug,Minor,2018-06-20 14:43:15,1
13167093,CacheActionDispatcher not memory bound,The persistent cache has an async write mode enabled by default and the write queue is maintained by the CacheActionDispatcher. The queue has a fixed size of 16'384 and is not memory bound. It may happen that the queue retains a lot of memory (multiple GB of heap) when the pending write actions reference big cache values and cause OOME.,candidate_oak_1_4 candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Major,2018-06-20 07:36:41,1
13166027,MissingLastRevSeekerTest fails on MongoDB with secondary preferred,"MissingLastRevSeekerTest actually does not run on MongoDB right now, but changing the test accordingly revealed an issue with the MongoDB specific implementation of MissingLastRevSeeker when running on a replica-set and secondary preferred read preference.

The class MongoMissingLastRevSeeker uses the configured read preference when checking for cluster node info entries that require recovery. This is inconsistent because the candidate nodes are read with primary read preference.",candidate_oak_1_6,"['documentmk', 'mongomk']",OAK,Bug,Minor,2018-06-14 07:31:01,1
13155694,oak-run compact should support Azure Segment Store,"{{oak-run compact}} should accept Azure URIs for the segment store in order to enable OffRC for Azure Segment Store.

-Proposed options to add:-
 * -{{azure-connection}}: connection URL to to connect to the Azure Storage-
 * -{{azure-container}}: name of the container to use-
 * -{{azure-root-path}}: segment store directory-

The Azure URI will be taken as argument and will have the following format: {{az:[https://myaccount.blob.core.windows.net/container/repo]}}, where *az* identifies the cloud provider. The last missing piece is the secret key which will be supplied as an environment variable, i.e. _AZURE_SECRET_KEY._",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2018-04-27 12:22:01,0
13154803,Allow collection of IOTraces during normal operation,"The IO tracing facility introduced with OAK-5655 should also be available during normal operation. The idea is to log IO reads intercepted via an {{IOMonitor}} instance to a logger. If {{DEBUG}} logging is not enabled for that logger at the time when the {{FileStore}} is instantiated, then no tracing would take place.",monitoring tooling,['segment-tar'],OAK,New Feature,Major,2018-04-24 13:29:05,3
13154787,Remove dependency to commons-codec,The module oak-store-document currently only uses a single utility method from commons-codec. It shouldn't be too difficult to remove this dependency.,technical_debt,['documentmk'],OAK,Improvement,Minor,2018-04-24 12:26:14,1
13153315,Introduce SegmentNodeStoreMonitorService for exposing writerGroups as an OSGi config property,"It would be useful to expose {{writerGroups}} in {{SegmentNodeStoreStats}} through an OSGi config property. Since this is a low level configuration related to monitoring, it shouldn't be added to {{SegmentNodeStoreService}}, but to a newly created service, {{SegmentNodeStoreMonitorService}} that would handle exposing monitoring related stuff. ",tooling,['segment-tar'],OAK,Improvement,Minor,2018-04-18 10:46:59,0
13152910,Contribute a 'proc' subtree for the Segment Node Store,"With guidance from [~mduerig], I recently developed a way to expose Segment Node Store's internal information through the NodeState API. 

The concept is similar in spirit to the proc file system in Linux: the proc subtree exposes internal information in a straightforward manner, enabling consumers to rely on a well-understood API to access the data. This proc subtree shelters tooling from variations of the internal APIs of the Segment Store. As long as the data exported through the proc subtree is stable, the same tools are going to work across different versions of the Segment Store with minimal to no modifications.

The proc subtree has been developed in [this branch on GitHub|https://github.com/francescomari/jackrabbit-oak/tree/proc]. I created this issue in order to review the work done so far, and to track the contribution of the proc subtree in Oak.",tooling,['segment-tar'],OAK,Improvement,Major,2018-04-17 06:53:58,2
13151579,Expose UI for collecting IO traces,"[http://svn.apache.org/viewvc?view=revision&revision=1827841] introduced utility classes to collect IO traces. See e.g. https://issues.apache.org/jira/browse/OAK-5655?focusedCommentId=16415730&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16415730. Currently the only way to run such traces is via code or JUnit (see {{IOTracerRunner}}).

Going forward we should wire this functionality to {{oak-run}} to make it more generally useful. 

 

[~frm], FYI.",tooling,"['oak-run', 'segment-tar']",OAK,Improvement,Major,2018-04-11 08:05:29,3
13151561,Changes kept in memory when update limit is hit in commit hook,"In some cases no persisted branch is created by the DocumentNodeStore when the number of changes hit the update limit. This happens when the current branch state is in-memory and the commit hook contributes changes that reach the update limit. The implementation keeps those changes in memory, which may lead to a commit way bigger than specified by the update limit.",candidate_oak_1_6 candidate_oak_1_8,['documentmk'],OAK,Bug,Major,2018-04-11 06:30:29,1
13149438,SegmentNodeStoreStats should expose stats for previous minute per thread group,"The current ""CommitsCountPerWriter"" stats exposed by {{SegmentNodeStoreStats}} are hard to follow since there can be too many writers at a time. To improve this, a more coarse-grained version of this metric should be added, in which commits are recorded for groups of threads. The groups should be configurable and represent regexes to be matched by individual thread names. An additional group (i.e. ""other"") will group all threads not matching any of the defined group regexes. 

The current behaviour will be split in two:
* ""CommitsCountOtherThreads"" will expose a snapshot of threads currently in ""other"" group
* ""CommitsCountPerGroup"" will expose an aggregate of commits count per thread group for the previous minute.

Both metrics will be reset each minute.",tooling,['segment-tar'],OAK,Improvement,Minor,2018-04-02 08:55:03,0
13146525,Migrate to the MongoDB Java driver API 3.0,The {{MongoDocumentStore}} currently uses the old API in {{com.mongodb}}. Starting with MongoDB Java driver 3.0 a new client API was introduced in {{com.mongodb.client}}. New features like client sessions are only available in the new API and MongoDB may remove some deprecated methods/classes/interfaces in the future. The implementation should be migrated to the new client API.,technical_debt,['mongomk'],OAK,Technical task,Major,2018-03-20 12:09:46,1
13144981,CommitsTracker data is always empty when exposed via JMX,"Due to duplicate registration of {{SegmentNodeStoreStats}} in both {{SegmentNodeStore}}  and {{LockBasedScheduler}}, we end up with two instances of this MBean. The former gets exposed via JMX and always returns empty tables for CommitsCountPerWriter and QueuedWriters, while the latter correctly tracks these data, but is not exposed. To address this, we should stick to only one instance of {{SegmentNodeStoreStats}}, used in both {{SegmentNodeStore}} and {{LockBasedScheduler}}.

While at this, two additional points to be addressed:
# {{CommitsTracker}} needs to be unit tested
# commits count map size needs to be configurable via {{SegmentNodeStoreStats}}",tooling,['segment-tar'],OAK,Bug,Major,2018-03-14 10:02:29,0
13144371,Transform CacheWeightEstimator into a unit test,We should transform {{CacheWeightEstimator}} from a stand-alone utility into a unit test such that we can regularly run in on a CI.,technical_debt,['segment-tar'],OAK,Improvement,Major,2018-03-12 16:51:18,3
13143234,SegmentParser#parseBlob does not long ids of external blobs,Support for long ids of external blobs where introduces with OAK-3107. {{SegmentParser.parseBlob()}} is still oblivious about them.,technical_debt tooling,['segment-tar'],OAK,Bug,Major,2018-03-07 14:40:04,3
13141749,Remove debug logging to the console during tests,OAK-4707 introduced logging at debug logging to the system console for sorting out test failures on Jenkins. Since we haveen't seen these failures for a while and that issue is fixed I would like to remove the extra logging again to avoid cluttering the console unnecessarily.,technical_debt,['segment-tar'],OAK,Improvement,Major,2018-03-01 10:00:43,3
13137875,Improve SegmentNodeStoreStats to include number of commits per thread and threads currently waiting on the semaphore,"When investigating the performance of  {{segment-tar}}, the source of the writes (commits) is a very useful indicator of the cause.

To better understand which threads are currently writing in the repository and which are blocked on the semaphore, we need to improve {{SegmentNodeStoreStats}} to:
 * expose the number of commits executed per thread
 * expose threads currently waiting on the semaphore",tooling,['segment-tar'],OAK,Improvement,Major,2018-02-12 14:22:03,0
13136571,Remove deprecated deep option from check command,"With OAK-5595 we have enabled deep traversals by default when using the check command. At the same time we have deprecated the --{{deep}} option.

Since all these happened for {{1.8}}, the next logical step to do for {{1.10}} is to remove this option altogether.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2018-02-06 14:14:57,0
13135379,Reduce calls to DocumentStore,Analyze and further reduce calls to the DocumentStore when content is written to the repository. ,performance,['documentmk'],OAK,Improvement,Major,2018-02-01 10:52:00,1
13135038,oak-run check should have an option to check the segments checksums,"{{oak-run check}} does currently *not* check the checksums of the segments. As a consequence, there is no quick way of determining the state of the repository (corrupt/valid), after corrupting some random node record, as we currently do in {{CheckRepositoryTestBase#corruptRecord}}. To determine that, there needs to be an attempt to read the corrupt record as part of a traversal.

An easier way would be to have a new dedicated option for this (i.e., {{--segments}}) which checks by default the content of segments against the checksums from all the tar files in the specified location. Additionally, it could accept as an argument a list of tar files, the segments of which to be checked.",tooling,"['run', 'segment-tar']",OAK,New Feature,Minor,2018-01-31 10:29:01,0
13134479,Remove support for binaries and documents in persistent cache,"The persistent cache currently stores binaries up to one MB by default. However most of the BlobStore implementations already provide some form of caching. E.g. for S3 a cache on the local filesystem is maintained and when using a Jackrabbit FileDataStore the persistent cache is actually unnecessary.

Support for documents in the persistent cache should also be removed. In contrast to other cache entries, documents are mutable and may cause consistency issues when enabled with the persistent cache. ",technical_debt,['documentmk'],OAK,Task,Minor,2018-01-29 15:46:48,1
13134438,Add configurable repository size cap to SegmentOverflowExceptionIT,"{{SegmentOverflowExceptionIT}} potentially consumes a lot of disk space. Running it for 10 minutes on a AWS m4.4xlarge instance with 900 / 3000 IOPS resulted in 80GB being taken up. 
Currently the test can be time boxed but not size boxed. I suggest to add another option to cap the repository size in addition to the existing {{-Dtimeout}}.",test,['segment-tar'],OAK,Improvement,Major,2018-01-29 12:39:03,3
13134388,Avoid call for child node when bundle contains all children,"When nodes are bundled in a document, the DocumentNodeStore keeps track of whether all children are included in a document. The presence of the hidden {{:doc-has-child-non-bundled}} property indicates there are non bundled child nodes. For the case when a document contains all children in the bundle, the DocumentNodeStore still does a find call on the DocumentStore when asked for an unknown child node.",bundling candidate_oak_1_6,['documentmk'],OAK,Improvement,Minor,2018-01-29 08:48:43,1
13133362,Node.getMixinNodeTypes() may check for child node named jcr:mixinTypes,In some cases a call to {{Node.getMixinNodeTypes()}} may result in a check whether there is a child node named {{jcr:mixinTypes}}. ,performance,['jcr'],OAK,Improvement,Minor,2018-01-24 13:37:15,1
13132712,guava: ListenableFuture.transform() changes to transformAsync in version 20,"See https://google.github.io/guava/releases/19.0/api/docs/com/google/common/util/concurrent/Futures.html#transform(com.google.common.util.concurrent.ListenableFuture,%20com.google.common.util.concurrent.AsyncFunction)",technical_debt,['segment-tar'],OAK,Technical task,Major,2018-01-22 13:40:37,3
13131603,Update documentation for oak-run check,We should review and update the documentation of [{{oak-run check}}|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#check]. E.g. to include the new options from OAK-6373.,documentation,"['doc', 'oak-run', 'segment-tar']",OAK,Task,Major,2018-01-17 15:05:40,0
13131600,Document TarMK specific MBeans,Currently the [TarMK documentation|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#monitoring-via-jmx] only mentions {{SegmentRevisionGarbageCollection}}. We should review that paragraph and also include documentation for all other relevant JMX endpoints.,documentation,"['doc', 'segment-tar']",OAK,Task,Major,2018-01-17 15:01:58,3
13131220,Race condition on revisions head between compaction and scheduler could result in skipped commit,"There is a race condition on {{TarRevisions#head}} between a running compaction trying to set the new head [0] and the scheduler doing the same after executing a specific commit [1]. If the compaction thread is first, then the head assignment in the scheduler will fail and not be re-attempted. 

IMO, the simple if statement should be changed to a while loop in which the head is refreshed and the commit is re-applied against the new head, before attempting again to set a new head in {{TarRevisions}}. This is somehow similar to what we previously had [2], but without the unneeded optimistic/pessimistic strategies involving tokens.

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/FileStore.java#L764
[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/scheduler/LockBasedScheduler.java#L253
[2] https://github.com/apache/jackrabbit-oak/blob/1.6/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentNodeStore.java#L686",scalability,['segment-tar'],OAK,Bug,Blocker,2018-01-16 12:17:50,0
13129793,SNFE after full compaction,"In some cases we observed a {{SNFE}} right after a the cleanup following a full compaction:

{noformat}
31.12.2017 04:25:19.816 *ERROR* [pool-17-thread-22] org.apache.jackrabbit.oak.segment.SegmentNotFoundExceptionListener Segment not found: a82a99a3-f1e9-49b7-a1e0-55e7fec80c41. SegmentId age=609487478ms,segment-generation=GCGeneration{generation=4,fullGeneration=2,isCompacted=true}
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment a82a99a3-f1e9-49b7-a1e0-55e7fec80c41 not found
        at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.readSegmentUncached(AbstractFileStore.java:276)
        at org.apache.jackrabbit.oak.segment.file.FileStore.lambda$readSegment$5(FileStore.java:478)
        at org.apache.jackrabbit.oak.segment.SegmentCache.lambda$getSegment$0(SegmentCache.java:116)
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)
        at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:113)
        at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:478)
        at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:154)
        at org.apache.jackrabbit.oak.segment.CachingSegmentReader$1.apply(CachingSegmentReader.java:94)
        at org.apache.jackrabbit.oak.segment.CachingSegmentReader$1.apply(CachingSegmentReader.java:90)
        at org.apache.jackrabbit.oak.segment.ReaderCache.get(ReaderCache.java:118)
        at org.apache.jackrabbit.oak.segment.CachingSegmentReader.readString(CachingSegmentReader.java:90)
        at org.apache.jackrabbit.oak.segment.MapRecord.getEntry(MapRecord.java:220)
        at org.apache.jackrabbit.oak.segment.MapRecord.getEntry(MapRecord.java:173)
        at org.apache.jackrabbit.oak.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:423)
        at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.<init>(MemoryNodeBuilder.java:143)
        at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.<init>(SegmentNodeBuilder.java:93)
        at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.createChildBuilder(SegmentNodeBuilder.java:148)
        at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNode(MemoryNodeBuilder.java:331)
        at org.apache.jackrabbit.oak.core.SecureNodeBuilder.<init>(SecureNodeBuilder.java:112)
        at org.apache.jackrabbit.oak.core.SecureNodeBuilder.getChildNode(SecureNodeBuilder.java:329)
        at org.apache.jackrabbit.oak.core.MutableTree.getTree(MutableTree.java:290)
        at org.apache.jackrabbit.oak.core.MutableRoot.getTree(MutableRoot.java:220)
        at org.apache.jackrabbit.oak.core.MutableRoot.getTree(MutableRoot.java:69)
        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.getItem(SessionDelegate.java:442)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.getItemInternal(SessionImpl.java:167)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.access$400(SessionImpl.java:82)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl$3.performNullable(SessionImpl.java:229)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl$3.performNullable(SessionImpl.java:226)
        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.performNullable(SessionDelegate.java:243)
        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.getItemOrNull(SessionImpl.java:226)
{noformat}

",compaction,['segment-tar'],OAK,Bug,Blocker,2018-01-10 09:44:03,3
13126821,Update documentation for cold standby,"Improve monitoring section of cold standby in {{oak-doc}} to include missing MBean screenshots.

-[~mduerig], [~frm]: How about adding a *Benchmarking* section to the cold standby page covering a bit ways to use the new {{Oak-Segment-Tar-Cold}} fixture and also running {{ScalabilityStandbySuite}} on top of it?-",documentation,"['doc', 'segment-tar', 'tarmk-standby']",OAK,Documentation,Major,2017-12-22 14:17:46,0
13126227,Compaction should log generation info,When compaction starts it should also log the current gc generation and the new gc generation it is going to create. ,compaction gc,['segment-tar'],OAK,Improvement,Minor,2017-12-20 09:24:53,3
13125964,ArrayIndexOutOfBoundsException when upgrading from Oak 1.6,"When starting on an Oak 1.6 repository and the {{gc.log}} file is present the TarMK fails with:
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 5
        at org.apache.jackrabbit.oak.segment.file.GCJournal$GCJournalEntry.parseString(GCJournal.java:217)
        at org.apache.jackrabbit.oak.segment.file.GCJournal$GCJournalEntry.fromString(GCJournal.java:204)
        at org.apache.jackrabbit.oak.segment.file.GCJournal.read(GCJournal.java:115)
        at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.compact(FileStore.java:750)
        at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.compactFull(FileStore.java:731)
        at org.apache.jackrabbit.oak.segment.file.FileStore.compactFull(FileStore.java:385)
        at org.apache.jackrabbit.oak.segment.tool.Compact.run(Compact.java:273)
        at org.apache.jackrabbit.oak.run.CompactCommand.execute(CompactCommand.java:72)
        at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
{noformat}",migration,['segment-tar'],OAK,Bug,Critical,2017-12-19 16:41:02,3
13125830,Segment-Tar-Cold fixture should have options for secure communication and one shot runs,"The newly introduced {{Segment-Tar-Cold}} fixture should support secure communication between primary and standby via a {{--secure}} option. Moreover, the current implementation allows only for continuous sync between primary and standby. It should be possible to allow a ""one-shot run"" of the sync to easily measure and compare specific metrics ({{--oneShotRun}} option).",cold-standby,"['benchmarks', 'segment-tar', 'tarmk-standby']",OAK,Improvement,Minor,2017-12-19 09:07:54,0
13125593,Document oak-run compact arguments and system properties,"Ensure {{oak-doc}} is up to date with the current version of {{oak-run compact}}, its current command line arguments and system properties. ",documentation,"['doc', 'segment-tar']",OAK,Task,Major,2017-12-18 10:47:03,3
13124683,oak-run compact reports success even when it was cancelled,When {{oak-run compact}} gets cancelled because running out of disk space it will send a corresponding warning to the logs and bail out. However on the console it will still report success. ,production tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-12-13 14:09:31,3
13124679,Segment.toString: Record table should include an index into the hexdump,Currently the Segment dump created in {{Segment.toString}} includes a list of records with their offsets. However these offsets do no match the ones in the subsequent raw byte dump of the segment. We should add a raw offsets to the list of records so finding the actual data that belongs to a record doesn't involve manually fiddling with logical / physical offset translation. ,tooling,['segment-tar'],OAK,Improvement,Minor,2017-12-13 14:05:42,3
13124359,Remove dangling reference to compress-interval system property from oak-run documentation,"{{oak-doc/src/site/markdown/command_line.md}} refers to the {{compress-interval}} system property, which does not exist any more. ",documentation,['doc'],OAK,Improvement,Major,2017-12-12 13:02:53,3
13124338,Offline compaction corrupts repository,Offline compaction can corrupt the repository in some cases: when offline compaction is cancelled by the {{CancelCompactionSupplier}} the corresponding return value is not correctly passed up the call chain resulting in a incomplete compacted head state being set as the compacted head state (instead of being discarded). ,corruption data-corruption,['segment-tar'],OAK,Bug,Blocker,2017-12-12 11:00:34,3
13122808,Test failure: ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout,"Seen on an internal Windows Jenkins node:

h3. Regression

org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout

h3. Error Message

{noformat}
Values should be different. Actual: { root = { ... } }
{noformat}

h3. Stacktrace

{noformat}
java.lang.AssertionError: Values should be different. Actual: { root = { ... } }
	at org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT.testSyncFailingDueToTooShortTimeout(ExternalPrivateStoreIT.java:87)
{noformat}

h3. Standard Output

{noformat}
22:41:13.646 INFO  [main] FileStoreBuilder.java:340         Creating file store FileStoreBuilder{version=1.8-SNAPSHOT, directory=target\junit2834122541179880349\junit3041268421527563090, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore], maxFileSize=1, segmentCacheSize=0, stringCacheSize=0, templateCacheSize=0, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1, memoryMapping=false, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}}
22:41:13.646 INFO  [main] FileStore.java:241                TarMK opened at target\junit2834122541179880349\junit3041268421527563090, mmap=false, size=0 B (0 bytes)
22:41:13.646 DEBUG [main] FileStore.java:247                TAR files: TarFiles{readers=[],writer=target\junit2834122541179880349\junit3041268421527563090\data00000a.tar}
22:41:13.646 DEBUG [main] TarWriter.java:185                Writing segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to target\junit2834122541179880349\junit3041268421527563090\data00000a.tar
22:41:13.646 INFO  [main] FileStoreBuilder.java:340         Creating file store FileStoreBuilder{version=1.8-SNAPSHOT, directory=target\junit2834122541179880349\junit4470899745425503556, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore], maxFileSize=1, segmentCacheSize=0, stringCacheSize=0, templateCacheSize=0, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1, memoryMapping=false, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}}
22:41:13.646 INFO  [main] FileStore.java:241                TarMK opened at target\junit2834122541179880349\junit4470899745425503556, mmap=false, size=0 B (0 bytes)
22:41:13.646 DEBUG [main] FileStore.java:247                TAR files: TarFiles{readers=[],writer=target\junit2834122541179880349\junit4470899745425503556\data00000a.tar}
22:41:13.646 DEBUG [main] TarWriter.java:185                Writing segment 8d19c7dc-8b48-4e10-a58d-31c15c93f2fe to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.646 INFO  [main] DataStoreTestBase.java:127        Test begin: testSyncFailingDueToTooShortTimeout
22:41:13.646 INFO  [main] SegmentNodeStore.java:120         Creating segment node store SegmentNodeStoreBuilder{blobStore=DataStore backed BlobStore [org.apache.jackrabbit.core.data.FileDataStore]}
22:41:13.646 INFO  [main] LockBasedScheduler.java:155       Initializing SegmentNodeStore with the commitFairLock option enabled.
22:41:13.708 DEBUG [main] StandbyServer.java:248            Binding was successful
22:41:13.708 DEBUG [main] TarWriter.java:185                Writing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to target\junit2834122541179880349\junit3041268421527563090\data00000a.tar
22:41:13.739 DEBUG [main] TarRevisions.java:240             TarMK journal update null -> 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c
22:41:13.755 DEBUG [standby-1] GetHeadRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for current head
22:41:13.755 DEBUG [primary-1] ClientFilterHandler.java:53  Client /127.0.0.1:65480 is allowed
22:41:13.755 DEBUG [primary-1] RequestDecoder.java:42       Parsed 'get head' message
22:41:13.755 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get head' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [primary-1] GetHeadRequestHandler.java:43 Reading head for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [primary-1] GetHeadResponseEncoder.java:36 Sending head 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.755 DEBUG [standby-1] ResponseDecoder.java:82      Decoding 'get head' response
22:41:13.755 DEBUG [standby-run-23] StandbyClientSyncExecution.java:103 Found missing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [standby-run-23] StandbyClientSyncExecution.java:124 Inspecting segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [standby-1] GetReferencesRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.755 DEBUG [primary-1] RequestDecoder.java:48       Parsed 'get references' message
22:41:13.771 DEBUG [primary-1] GetReferencesRequestHandler.java:39 Reading references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetReferencesResponseEncoder.java:34 Sending references of segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:94      Decoding 'get references' response
22:41:13.771 DEBUG [standby-run-23] StandbyClientSyncExecution.java:184 Found reference from 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [standby-run-23] StandbyClientSyncExecution.java:124 Inspecting segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [standby-1] GetReferencesRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:48       Parsed 'get references' message
22:41:13.771 DEBUG [primary-1] GetReferencesRequestHandler.java:39 Reading references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetReferencesResponseEncoder.java:34 Sending references of segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:94      Decoding 'get references' response
22:41:13.771 INFO  [standby-run-23] StandbyClientSyncExecution.java:196 Copying data segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a from primary
22:41:13.771 DEBUG [standby-1] GetSegmentRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:45       Parsed 'get segment' message
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get segment' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentRequestHandler.java:39 Reading segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:125 Segment with size 192 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentResponseEncoder.java:43 Sending segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:86      Decoding 'get segment' response
22:41:13.771 DEBUG [standby-run-23] TarWriter.java:185      Writing segment 4cea1684-ef05-44f5-a869-3ef2df6e0c9a to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.771 INFO  [standby-run-23] StandbyClientSyncExecution.java:196 Copying data segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 from primary
22:41:13.771 DEBUG [standby-1] GetSegmentRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:45       Parsed 'get segment' message
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get segment' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentRequestHandler.java:39 Reading segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:125 Segment with size 448 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetSegmentResponseEncoder.java:43 Sending segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [standby-1] ResponseDecoder.java:86      Decoding 'get segment' response
22:41:13.771 DEBUG [standby-run-23] TarWriter.java:185      Writing segment 4a5183bd-bcdf-41ab-a557-6f19143bbc91 to target\junit2834122541179880349\junit4470899745425503556\data00000a.tar
22:41:13.771 DEBUG [standby-1] GetBlobRequestEncoder.java:33 Sending request from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90 for blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.771 DEBUG [primary-1] RequestDecoder.java:39       Parsed 'get blob' request
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:120 Message 'get blob id' received from client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetBlobRequestHandler.java:41 Reading blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 for client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] CommunicationObserver.java:130 Binary with size 5242880 sent to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.771 DEBUG [primary-1] GetBlobResponseEncoder.java:41 Sending blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.786 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 1/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 2/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 1/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.802 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 3/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.802 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 2/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.818 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 4/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 3/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.818 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.818 DEBUG [primary-1] ChunkedBlobStream.java:128   Sending chunk 5/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880 to client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 4/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:90      Decoding 'get blob' response
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:150     Received chunk 5/5 of size 1048576 from blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:159     All checks OK. Appending chunk to disk to C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp 
22:41:13.833 DEBUG [standby-1] ResponseDecoder.java:167     Received entire blob c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880
22:41:13.880 DEBUG [standby-run-23] ResponseDecoder.java:66 Processing input stream finished! Deleting file C:\Windows\TEMP\c3ac16ad0c8bc3a3ff30cef8e296af92d53058c13a8930406d3f08e271b4b57b#5242880.tmp
22:41:13.896 DEBUG [standby-run-23] TarRevisions.java:240   TarMK journal update null -> 4a5183bd-bcdf-41ab-a557-6f19143bbc91.0000000c
22:41:13.911 WARN  [standby-1] ExceptionHandler.java:37     Exception caught on client 9aa63ed8-347b-4f00-ae7c-f984e0623e90
io.netty.handler.timeout.ReadTimeoutException: null
22:41:13.911 INFO  [standby-run-23] StandbyClientSyncExecution.java:82 updated head state successfully: true in 156ms.
22:41:13.911 DEBUG [standby-run-23] StandbyClient.java:157  Channel closed
22:41:16.137 DEBUG [main] StandbyClientSync.java:277        Group shut down
22:41:16.137 DEBUG [main] StandbyServer.java:219            Channel disconnected
22:41:16.137 DEBUG [main] StandbyServer.java:219            Channel disconnected
22:41:16.137 DEBUG [main] StandbyServer.java:230            Boss group shut down
22:41:16.137 DEBUG [main] StandbyServer.java:236            Worker group shut down
22:41:16.137 INFO  [main] DataStoreTestBase.java:132        Test end: testSyncFailingDueToTooShortTimeout
22:41:16.137 DEBUG [main] Scheduler.java:134                The scheduler FileStore background tasks was successfully shut down
22:41:16.137 DEBUG [main] TarRevisions.java:236             Head state did not change, skipping flush
22:41:16.184 INFO  [main] FileStore.java:480                TarMK closed: target\junit2834122541179880349\junit4470899745425503556
22:41:16.184 DEBUG [main] Scheduler.java:134                The scheduler FileStore background tasks was successfully shut down
22:41:16.184 DEBUG [main] TarRevisions.java:236             Head state did not change, skipping flush
22:41:16.199 INFO  [main] FileStore.java:480                TarMK closed: target\junit2834122541179880349\junit3041268421527563090
{noformat}
",test-failure,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-12-05 08:52:10,2
13121939,Estimation for FULL can be off sometimes,"Since OAK-6883, FULL estimation compares segmentstore size with the previous FULL. There can be cases where the current segmentstore is smaller than the previous FULL (i.e. due to TAIL cleaning up more). This leads to FULL being skipped for much more than anticipated.

A case to illustrate this scenario:

    Start Oak with a 10 GB repo
    GC #1: run FULL results in segmenstore of 20GB
    GC #2: run TAIL results in segmentstore of 11GB
    GC #3: run FULL (saturday) - skipped because the reference is 20GB from the previous FULL

FULL be executed again only when the segmentstore grows back above 20GB, which might be too late.

Estimation should take this situation into account this and take a better decision.",compaction gc,['segment-tar'],OAK,Bug,Major,2017-11-30 15:23:28,3
13120892,High read IO in compaction retry cycles,"We have seen unusual high read IO in compaction retry cycles in our longevity tests at Adobe.

!cpu.png|width=1000!

Above picture shows the CPU utilisation during an online compaction run, which starts at 03:00. At about 03:22 CPU user time drops and CPU waiting for IO time increases. This point in time coincides with the start of the first retry cycle of compaction. 
",performance,['segment-tar'],OAK,Bug,Major,2017-11-27 11:00:15,3
13120406,test failure seen in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT,"{noformat}
INFO] Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file C:\projects\apache\oak\trunk\oak-segment-tar\target\failsafe-reports\2017-11-23T08-48-55_999-jvmRun1.dumpstream
[ERROR] Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 17.984 s <<< FAILURE! - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
[ERROR] offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 5.024 s  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:143)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:108)

{noformat}
",test-failure windows,['segment-tar'],OAK,Bug,Critical,2017-11-23 08:10:31,2
13119685,Document tail compaction,"We need to add documentation of tail compaction:
* What is it, how does it work?
* How is it configured and scheduled?
* How can it be monitored, what are the related log entries?
* What are its limitations?
* What if it fails?",documentation,"['doc', 'segment-tar']",OAK,Documentation,Major,2017-11-20 15:17:59,3
13117925,Enable the -Dcache of offline compaction,The {{-Dcache}} option currently has no effect when used in conjunction with the {{compact}} run mode of {{oak-run}}. However we should enable users to configure the segment cache size through this option if necessary. ,compaction gc tooling,['segment-tar'],OAK,Improvement,Major,2017-11-13 12:49:01,3
13116641,Cold standby performance regression due to segment caching,"The changes to the segment cache introduced in r1793527 [0] introduced a performance regression on the primary for the case in which a standby is attached to it. Below a benchmark duration comparison between primary w/o and w/ standby for r1793527 (after the segment cache changes) and r1793526 (before the changes) :

|Oak 1.6 r1793527 (20170502)|{noformat}
# BasicWriteTest                   C     min     10%     50%     90%     max       N
Oak-Segment-Tar                    1      19      21      22      26     160    2491
Oak-Segment-Tar-DS                 1      56      59      63      70     181     919
Oak-Segment-Tar-Cold(Shared DS)    1      58      66     159     177     372     302
{noformat}|
|Oak 1.6 r1793526 (20170502)|{noformat}
# BasicWriteTest                   C     min     10%     50%     90%     max       N
Oak-Segment-Tar                    1      19      21      22      25      52    2584
Oak-Segment-Tar-DS                 1      56      60      63      69     158     925
Oak-Segment-Tar-Cold(Shared DS)    1      57      60      64      70     122     915
{noformat}|

[0] https://github.com/apache/jackrabbit-oak/commit/efafa4e1710621b7f3b8e92d0b2681669185fcd4",cold-standby performance scalability,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-11-07 11:47:37,0
13116579,Provide a way to tune inline size while storing binaries,"SegmentNodeStore currently inlines binaries of size less that 16KB (Segment.MEDIUM_LIMIT) even if external BlobStore is configured. 

Due to this behaviour quite a bit of segment tar storage consist of blob data. In one setup out of 370 GB segmentstore size 290GB is due to inlined binary. If most of this binary content is moved to BlobStore then it would allow same repository to work better in lesser RAM

So it would be useful if some way is provided to disable this default behaviour and let BlobStore take control of inline size i.e. in presence of BlobStore no inlining is attempted by SegmentWriter.",performance scalability,['segment-tar'],OAK,Improvement,Major,2017-11-07 04:55:40,0
13116445,Offline compaction should not use mmap on Windows,The offline compaction tool should do an effort to detect whether it is being run on windows and disable memory mapping if so. Rational: with memory mapping enabled it might fail to remove the old tar files (see OAK-4274 and [JDK-4724038|http://bugs.java.com/view_bug.do?bug_id=4724038]).,compaction gc tooling,['segment-tar'],OAK,Improvement,Major,2017-11-06 16:38:38,2
13116443,FileStore.compact does not persist compacted head to journal,"When {{FileStore.compact()}} returns the {{journal.log}} does not necessarily contain the head created by the compactor. This can lead to problems downstream like e.g. in OAK-6894 where the compactor tool wrote the wrong (i.e. uncompacted) head to the {{journal.log}}. 

Proposed fix is to call on of the {{FileStore.flush()}} methods after compaction and add a test case that verifies the {{journal.log}} contains the correct head state. ",compaction gc,['segment-tar'],OAK,Bug,Major,2017-11-06 16:32:51,3
13115677,Unknown channel option 'TCP_NODELAY' for channel warning in cold standby,"After the netty upgrade in OAK-6564, there's a recurring warning appearing in the server thread:
{noformat}
18:54:44.691 [main] WARN  io.netty.bootstrap.ServerBootstrap - Unknown channel option 'TCP_NODELAY' for channel '[id: 0xa64bc5c4]'
{noformat}

We need to see what's causing it (i.e. was that option removed in the latest version? if yes, is there a substitute/change needed?).

/cc [~frm]",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Minor,2017-11-02 16:58:12,2
13115540,Log SegmentStore size at startup,"It would be useful if we can log the segmentstore size at time of startup. FileStore already computes the size to initialize the FileStoreStats so we just need to log it

This size often help when customer report issues and provide log files",production,['segment-tar'],OAK,Improvement,Minor,2017-11-02 04:32:10,2
13113327,Executions of background threads might pile up,"The background threads used in {{FileStore}} are implemented by wrapping {{Runnable}} instances in {{SafeRunnable}}, and by handing the {{SafeRunnable}} instances over to a {{ScheduledExecutorService}}. 

The documentation of {{ScheduledExecutorService#scheduleAtFixedRate}} states that ""if any execution of a task takes longer than its period, then subsequent executions may start late, but will not concurrently execute"". This means that if an execution is delayed, the piled up executions might fire in rapid succession.

This way of running the periodic background threads might not be ideal. For example, it doesn't make much sense to flush the File Store five times in a row. On the other hand, if the background tasks are coded with this caveat in mind, this issue might not be a problem at all. For example, flushing the File Store five times in a row might not be a problem if many of those executions don't do much and return quickly.

Tasks piling up might be a problem when it comes to release the resource associated with the {{FileStore}} in a responsive way. Since the {{ScheduledExecutorService}} is gracefully shut down, it might take some time before all the scheduled background tasks are processed and the {{ScheduledExecutorService}} is ready to be terminated.",production,['segment-tar'],OAK,Bug,Major,2017-10-31 15:28:15,3
13113323,Background threads might not be automatically restarted,"The background threads used in {{FileStore}} are implemented by wrapping {{Runnable}} instances in {{SafeRunnable}}, and by handing the {{SafeRunnable}} instances over to a {{ScheduledExecutorService}}. 

The documentation of {{ScheduledExecutorService#scheduleAtFixedRate}} states that ""if any execution of the task encounters an exception, subsequent executions are suppressed"". But a {{SafeRunnable}} always re-throws any {{Throwable}} that it catches, effectively preventing itself from executing again in the future.

There is more than one solution to this problem. One of these is to never re-throw any exception. Even if it doesn't always make sense, e.g. in case of an {{OutOfMemoryError}}, never re-throwing an exception would better fulfil the assumption that background threads should always be up and running even in case of error.",resilience,['segment-tar'],OAK,Bug,Critical,2017-10-31 15:13:59,2
13113277,OffRC always logs 0 for the number of compacted nodes in gc.log,"After an offline compaction the {{gc.log}} always contains 0 for the number of compacted nodes. This is caused by {{org.apache.jackrabbit.oak.segment.tool.Compact.compact()}} instantiating a new {{FileStore}} to run cleanup. That file store has new {{GCMonitor}} instance, which did no see any of the nodes written by the compaction that was run on the previous {{FileStore}} instance. 

",compaction gc tooling,['segment-tar'],OAK,Bug,Major,2017-10-31 10:57:49,2
13113047,TarMK disk space check is not synchronized with FileStore opened state,"It seems that the disk space check is not properly synchronized with {{FileStore}} as I revealed a race condition while using oak-upgrade during migration to {{segment-tar}}.

The {{FileStore}} instance is closed while TarMK disk check tries to execute and it seems it is dependent on the state of segment ({{org.apache.jackrabbit.oak.segment.file.FileStore.checkDiskSpace(FileStore.java:541)}} that needs to be opened. 

{noformat}
30.10.2017 11:26:05.834 WARN   o.a.j.o.s.f.Scheduler: The scheduler FileStore background tasks takes too long to shut down
30.10.2017 11:26:11.674 INFO   o.a.j.o.s.f.FileStore: TarMK closed: /data/cq/crx-quickstart/repository-segment-tar-20171030-112401/segmentstore
30.10.2017 11:26:11.676 ERROR  o.a.j.o.s.f.SafeRunnable: Uncaught exception in TarMK disk space check [/data/cq/crx-quickstart/repository-segment-tar-20171030-112401/segmentstore]
java.lang.IllegalStateException: already shut down
    at org.apache.jackrabbit.oak.segment.file.ShutDown.keepAlive(ShutDown.java:42)
    at org.apache.jackrabbit.oak.segment.file.FileStore.size(FileStore.java:302)
    at org.apache.jackrabbit.oak.segment.file.FileStore.checkDiskSpace(FileStore.java:541)
    at org.apache.jackrabbit.oak.segment.file.FileStore.access$300(FileStore.java:102)
    at org.apache.jackrabbit.oak.segment.file.FileStore$3.run(FileStore.java:237)
    at org.apache.jackrabbit.oak.segment.file.SafeRunnable.run(SafeRunnable.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
{noformat}",concurrency production,"['segment-tar', 'upgrade']",OAK,Bug,Minor,2017-10-30 14:50:45,3
13113043,The compaction estimator should take the compaction type (tail vs. full) into consideration,"Currently the compaction estimator unconditionally looks at the growth of the repository since the last compaction run. This turn out to be not optimal when interleaving tail and full compaction. It would be better to have the estimator look at the growth of the repository since last full compaction when running full compaction. 

cc [~frm]",compaction gc,['segment-tar'],OAK,Improvement,Critical,2017-10-30 14:46:56,2
13112267,Segment-Tar-Cold fixture doesn't correctly set up standby blob store,"When {{--shareDataStore}} option is used for {{Segment-Tar-Cold}}, the standby instance ends up without a blob store configured.",cold-standby,"['benchmarks', 'segment-tar', 'tarmk-standby']",OAK,Bug,Minor,2017-10-26 12:27:31,0
13110217,Refactor FileStore.close,{{FileStore.close}} should take better advantage of the {{Closer}} instance to close its resources (including the file store lock). Also the order of the close calls should be aligned with their dependencies. ,refactoring technical_debt,['segment-tar'],OAK,Improvement,Major,2017-10-18 08:10:43,3
13108896,Implement support for disabling indexes which are replaced with newer index,"For upgrade case in many applications older index type is set to {{disabled}} when new index is provisioned. If the new index is async then it would take some time for reindex and till then any query which used to make use of old index would end up traversing the repository

To avoid such a scenario we should only mark older index as ""disabled"" only if the newer index is reindex. ",docs-impacting,['indexing'],OAK,New Feature,Major,2017-10-12 11:59:08,4
13107951,Provide a way to for persistent cache to determine which all nodes can be cached,"Currently persistent cache if enabled for nodes caches all nodes accessed on the system. It would be better if it can be configured to only cache those nodes which are not volatile so that caching can be effective

Purpose of this issue is to
* Provide an extension point in PersistentCache logic to check if a node is to be cached
* Provide an impl which relies on some static OSGi config to determine that

Later we can make this impl dynamic i.e. rely on access pattern to cache imp stuff",doc-impacting,['documentmk'],OAK,Improvement,Minor,2017-10-09 11:18:10,4
13107238,Exceptions are inhibited in oak-run compact,"Exceptions thrown by {{oak-run compact}} are inhibited so the exit code of the command is not correct in case of error. 

Example: 
{code}
$ java -jar oak-run-1.7.8-R1809845.jar compact test-oak-run/
Apache Jackrabbit Oak 1.7.8-R1809845
Compacting test-oak-run-6.3.0
With default access mode
    before
        Thu Oct 05 15:14:22 CEST 2017, journal.log
        Thu Oct 05 15:14:23 CEST 2017, data00000a.tar
        Thu Oct 05 15:14:23 CEST 2017, manifest
        Thu Oct 05 15:14:23 CEST 2017, repo.lock
    size 119.1 MB (119133142 bytes)
    -> compacting
org.apache.jackrabbit.oak.segment.file.InvalidFileStoreVersionException: Using a too recent version of oak-segment-tar
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkStoreVersion(ManifestChecker.java:81)
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkManifest(ManifestChecker.java:70)
	at org.apache.jackrabbit.oak.segment.file.ManifestChecker.checkAndUpdateManifest(ManifestChecker.java:51)
	at org.apache.jackrabbit.oak.segment.file.FileStore.<init>(FileStore.java:191)
	at org.apache.jackrabbit.oak.segment.file.FileStoreBuilder.build(FileStoreBuilder.java:343)
	at org.apache.jackrabbit.oak.segment.tool.Compact.newFileStore(Compact.java:165)
	at org.apache.jackrabbit.oak.segment.tool.Compact.compact(Compact.java:135)
	at org.apache.jackrabbit.oak.segment.tool.Compact.run(Compact.java:128)
	at org.apache.jackrabbit.oak.run.SegmentTarUtils.compact(SegmentTarUtils.java:183)
	at org.apache.jackrabbit.oak.run.CompactCommand.execute(CompactCommand.java:93)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
    after
        Thu Oct 05 15:14:22 CEST 2017, journal.log
        Thu Oct 05 15:14:23 CEST 2017, data00000a.tar
        Thu Oct 05 15:14:23 CEST 2017, manifest
        Thu Oct 05 15:14:23 CEST 2017, repo.lock
    size 119.1 MB (119133142 bytes)
    removed files []
    added files []
Compaction succeeded in 211.9 ms (0s).
{code}

A quick fix would be to wrap the exception into a {{RuntimeException}}:
{code}
--- a/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/tool/Compact.java
+++ b/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/tool/Compact.java
@@ -127,7 +127,7 @@ public class Compact implements Runnable {
         try {
             compact();
         } catch (Exception e) {
-            e.printStackTrace();
+            throw new RuntimeException(""Failed to run compact"", e);
         }
     }
{code}",compaction gc tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-10-05 14:16:27,2
13105674,Standby server should send timely responses to all client requests,"Currently all the {{GetXXXRequestHandler}} (where XXX stands for Blob, Head, References and Segment), on the server discard client requests which cannot be satisfied (i.e. the requested object does not exist (yet) on the server). A more transparent approach would be to timely respond to all client requests, clearly stating that the object was not found. This would improve a lot debugging for example, because all requests and their responses could be easily followed from the client log, without needing to know what actually happened on the server.

Below, a possible implementation for {{GetHeadRequestHandler}}, suggested by [~frm] in a comment on OAK-6678:

{noformat}
String id = reader.readHeadRecordId();

if (id == null) {
    ctx.writeAndFlush(new NotFoundGetHeadResponse(msg.getClientId(), id));
    return;
}

ctx.writeAndFlush(new GetHeadResponse(msg.getClientId(), id));
{noformat}
",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Minor,2017-09-28 06:55:40,0
13104303,TarWriter.close() must not throw an exception on subsequent invocations,"Invoking TarWriter.close() on an already closed writer throws an {{ISE}}. According to the general contract this is not allowed:

{code}
* Closes this stream and releases any system resources associated
* with it. If the stream is already closed then invoking this
* method has no effect.
{code}

We should adjust the behvaviour of that method accordingly. 

Failing to comply with that general contract causes {{TarWriter}} instances to fail in try-resource statements when multiple wrapped streams are involved.

Consider 

{code}
try (
    StringWriter string = new StringWriter();
    PrintWriter writer = new PrintWriter(string);
    WriterOutputStream out = new WriterOutputStream(writer, Charsets.UTF_8))
{
    dumpHeader(out);
    writer.println(""----------------------------------------"");
    dumpHex(out);
    writer.println(""----------------------------------------"");
    return string.toString();
}
{code}

This code would cause exceptions to be thrown if e.g. the {{PrintWriter.close}} method would not be idempotent. ",technical_debt,['segment-tar'],OAK,Bug,Minor,2017-09-22 14:34:25,3
13104227,Improve cold standby resiliency to incoherent configs,"In order to correctly configure cold standby there are two OSGi configurations that need to be provided. Among other settings, {{org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.config}} needs {{standby=B""true""}} and {{org.apache.jackrabbit.oak.segment.standby.store.StandbyStoreService.config}} needs {{mode=""standby""}}. The problem is that sometimes we have {{mode=""standby""}} in {{StandbyStoreService}} and {{standby=B""false""}} in {{SegmentNodeStoreService}} which leads to starting a problematic standby instance (with primary behaviour enabled, e.g. indexing, etc.). This problem stems from the fact that there are two components whose configuration should be coordinated. Proposals to mitigate this:

# Keep the {{mode=""standby""}}, but merge the configuration of {{StandbyStoreService}} in the one for {{SegmentNodeStoreService}} and eliminate {{StandbyStoreService}} altogether
# {{StandbyStoreService}} should derive {{mode=""standby""}} from {{""standby=B""true""}} in {{SegmentNodeStoreService}}
# {{SegmentNodeStoreService}} should derive {{""standby=B""true""}} from {{mode=""standby""}} in {{StandbyStoreService}} even if this is backwards when compared to how the synchronization currently happens, with {{StandbyStoreService}} waiting for for a proper initialisation of {{SegmentNodeStoreService}}
# Make {{StandbyStoreService}} configuration mandatory, but require a {{mode=""off""}} setting. This way the removal of {{standby=B""true""}} from {{SegmentNodeStoreService}} would be guaranteed and any synchronization between the two components would be avoided.

/cc  [~frm], [~volteanu], [~mduerig]
",cold-standby resilience,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Major,2017-09-22 09:11:35,0
13103614,ReadOnly connection to fresh SegmentNodeStore setup failing,"Started a server with Oak version 1.7.7 and tried to connect oak-run-1.7.7 to same setup. This resulted in following exception

{noformat}
2017-09-20 19:47:22,213 INFO  [main] o.a.j.o.segment.file.FileStore - Creating file store FileStoreBuilder{version=1.7.7, directory=/path/to/repository/segmentstore, blobStore=DataStore backed BlobStore [org.apache.jackrabbit.oak.plugins.blob.datastore.OakFileDataStore], maxFileSize=256, segmentCacheSize=256, stringCacheSize=256, templateCacheSize=64, stringDeduplicationCacheSize=15000, templateDeduplicationCacheSize=3000, nodeDeduplicationCacheSize=1048576, memoryMapping=true, gcOptions=SegmentGCOptions{paused=false, estimationDisabled=false, gcSizeDeltaEstimation=1073741824, retryCount=5, forceTimeout=60, retainedGenerations=2, gcType=FULL}} 
2017-09-20 19:47:22,243 WARN  [main] o.a.j.o.s.file.tar.TarReader - Unable to load index of file data00000a.tar: Unrecognized magic number 
2017-09-20 19:47:22,243 INFO  [main] o.a.j.o.s.file.tar.TarReader - No index found in tar file data00000a.tar, skipping... 
2017-09-20 19:47:22,243 WARN  [main] o.a.j.o.s.file.tar.TarReader - Could not find a valid tar index in /path/to/repository/segmentstore/data00000a.tar, recovering read-only 
2017-09-20 19:47:22,243 INFO  [main] o.a.j.o.s.file.tar.TarReader - Recovering segments from tar file /path/to/repository/segmentstore/data00000a.tar 
2017-09-20 19:47:22,315 INFO  [main] o.a.j.o.s.file.tar.TarReader - Regenerating tar file/path/to/repository/segmentstore/data00000a.tar.ro.bak 
2017-09-20 19:47:22,460 ERROR [main] o.a.j.oak.index.IndexCommand - Error occurred while performing index tasks 
java.lang.IllegalArgumentException: invalid segment buffer
	at org.apache.jackrabbit.oak.segment.data.SegmentDataLoader.newSegmentData(SegmentDataLoader.java:37) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.data.SegmentData.newSegmentData(SegmentData.java:66) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.writeSegment(AbstractFileStore.java:212) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore.access$000(AbstractFileStore.java:66) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.AbstractFileStore$1.recoverEntry(AbstractFileStore.java:125) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarReader.generateTarFile(TarReader.java:213) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarReader.openRO(TarReader.java:162) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles.<init>(TarFiles.java:298) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles.<init>(TarFiles.java:58) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.tar.TarFiles$Builder.build(TarFiles.java:167) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.ReadOnlyFileStore.<init>(ReadOnlyFileStore.java:74) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.segment.file.FileStoreBuilder.buildReadOnly(FileStoreBuilder.java:383) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.SegmentTarFixtureProvider.configureSegment(SegmentTarFixtureProvider.java:63) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.NodeStoreFixtureProvider.create(NodeStoreFixtureProvider.java:71) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.cli.NodeStoreFixtureProvider.create(NodeStoreFixtureProvider.java:47) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.index.IndexCommand.execute(IndexCommand.java:98) ~[oak-run-1.7.7.jar:1.7.7]
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49) [oak-run-1.7.7.jar:1.7.7]

{noformat}

Post restart of server oak-run was able to connect fine. So looks like issue with very fresh setup only

Command used for oak-run
{noformat}
java -jar oak-run-1.7.7.jar index --fds-path=/path/to/repository/datastore --checkpoint head --reindex --index-paths=/oak:index/lucene /path/to/repository/segmentstore --metrics
{noformat}",tooling,['segment-tar'],OAK,Bug,Minor,2017-09-20 14:20:00,3
13102961,Test failure: DocumentNodeStoreTest.disabledBranchesWithBackgroundWrite,"{{DocumentNodeStoreTest.disabledBranchesWithBackgroundWrite}} fails when I try a clean build on Windows, as per r1808698.

{noformat}
[ERROR] Failures:
[ERROR]   DocumentNodeStoreTest.disabledBranchesWithBackgroundWrite:3199 expected:<1> but was:<0>
{noformat}",test-failure,['documentmk'],OAK,Bug,Major,2017-09-18 14:00:09,1
13102805,Syncing big blobs fails since StandbyServer sends persisted head,"With changes for OAK-6653 in place, {{ExternalPrivateStoreIT#testSyncBigBlog}} and sometimes {{ExternalSharedStoreIT#testSyncBigBlob}} are failing on CI:

{noformat}
org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT)  Time elapsed: 96.82 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
...
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT)  Time elapsed: 95.254 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

Partial stacktrace:
{noformat}
14:09:08.355 DEBUG [main] StandbyServer.java:242            Binding was successful
14:09:08.358 DEBUG [standby-1] GetHeadRequestEncoder.java:33 Sending request from client Bar for current head
14:09:08.359 DEBUG [primary-1] ClientFilterHandler.java:53  Client /127.0.0.1:52988 is allowed
14:09:08.360 DEBUG [primary-1] RequestDecoder.java:42       Parsed 'get head' message
14:09:08.360 DEBUG [primary-1] CommunicationObserver.java:79 Message 'get head' received from client Bar
14:09:08.362 DEBUG [primary-1] GetHeadRequestHandler.java:43 Reading head for client Bar
14:09:08.363 WARN  [primary-1] ExceptionHandler.java:31     Exception caught on the server
java.lang.NullPointerException: null
	at org.apache.jackrabbit.oak.segment.standby.server.DefaultStandbyHeadReader.readHeadRecordId(DefaultStandbyHeadReader.java:32) ~[oak-segment-tar-1.8-SNAPSHOT.jar:1.8-SNAPSHOT]
	at org.apache.jackrabbit.oak.segment.standby.server.GetHeadRequestHandler.channelRead0(GetHeadRequestHandler.java:45) ~[oak-segment-tar-1.8-SNAPSHOT.jar:1.8-SNAPSHOT]
{noformat}",cold-standby resilience,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-09-17 05:53:00,0
13102497,Create a more complex IT for cold standby,"At the moment all integration tests for cold standby are using the same scenario in their tests: some content is created on the server (including binaries), a standby sync cycle is started and then the content is checked on the client. The only twist here is using/not using a data store for storing binaries.

Although good, this model could be extended to cover many more cases. For example, {{StandbyDiff}} covers the following 6 cases node/property added/changed/deleted. From these, with the scenario described, the removal part is never tested (and the change part is covered in only one test). 

It would be nice to have an IT which would add content on the server, do a sync, remove some of the content, do a sync and then call OnRC. This way all cases will be covered, including if cleanup works as expected on the client.

/cc [~frm]",cold-standby technical_debt test,"['segment-tar', 'tarmk-standby']",OAK,Task,Major,2017-09-15 08:10:43,0
13102494,Improve cold standby logging,"With the new feature which allows chunking in blob transfer between server and client, there are various places in which more meaningful log messages could be used. For example, on the server, there's a tally of the no. of chunks sent/total no. of chunks, but this part is missing on the client, i.e. no. of chunks received/total no. of chunks. 

Another case which would benefit from improved logging is when a big blob can't be sent fully from the server to the client in {{readTimeoutMs}}. The current exception message is a bit scarce in details (e.g. {{""Unable to load remote blob "" + blobId + "" at "" + path + ""#"" + pName}}). This could also mean that the remote blob doesn't exist on the server in the first place. A better option would be to advise about increasing {{readTimeoutMs}}.

Finally, the same log level should be used everywhere, since currently {{DEBUG}} and {{INFO}} are  interchangeably mixed.

/cc [~frm]",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Task,Minor,2017-09-15 08:00:45,0
13102284,Refactor StandbyDiff for better clarity and understandability,"{{StandbyDiff}} still makes use of the {{logOnly}} property for deciding when to act upon node/property changes. The official documentation of {{logOnly}} states that it helps for

{quote}
/**
     * read-only traversal of the diff that has 2 properties: one is to log all
     * the content changes, second is to drill down to properly level, so that
     * missing binaries can be sync'ed if needed
     */
{quote}

but it's use is a bit misleading. The first call to {{StandbyDiff}} is always with {{logOnly==false}}, while subsequent calls are done with {{logOnly==true}}. Implementing {{StandbyDiff}} without this mechanism would result in better clarity and maintainability.

Another minor improvement is to rename {{#binaryCheck}} methods and {{#readBinary}} to {{#fetchBinary}} and {{#fetchAndStoreBlob}} which is more appropriate to their purpose.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Minor,2017-09-14 15:14:12,0
13102249,Move DocumentNodeStore into its own bundle,"Move the DocumentNodeStore implementation and the two backends (MongoDB, RDB) into its own bundle. This will make it possible to release oak-core and the NodeStore implementation independently.",modularization technical_debt,"['core', 'documentmk']",OAK,Task,Major,2017-09-14 13:06:16,1
13101964,ResponseDecoder should check that the length of the received blob matches the length of the sent blob,"As already explained in OAK-6659, there can be cases in which deleting the previous spool file fails (Windows) and new (duplicate) content is added under the hood to the old file. This way the persisted blob doesn't match in content and id with the original sent by the server.

A first improvement here is to not allow the decoding to continue if the old spool file cannot be deleted. For this, the call to {{File#delete}} needs to be replaced with {{java.nio.file.Files#delete}} which would throw an exception if something wrong happens.

By ensuring that the spool file has the same size as the original blob we solve this problem. This check is sufficient, since all the chunks received are individually checked by hash, before appending them to the spool file. Moreover, the single threaded nature of the client ensures that races in which a new thread starts appending new content, after the length check has just passed can never happen.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Major,2017-09-13 13:40:25,0
13101940,Cold standby should fail loudly when a big blob can't be timely transferred,"Due to changes done in OAK-4969, currently there are two 'sync blob' cycles triggered by {{StandbyDiff#childNodeChanged}}. The test scenario is the same as the one in {{DataStoreTestBase#testSyncBigBlob}}: on the primary file store, a new big blob (1GB) is added and then a standby sync is triggered to sync this content to the secondary file store. 

The first 'sync blob' cycle happens as a result of {{#process}} being called in {{StandbyDiff#childNodeChanged}}. Therefore, a new 'get blob' request is created on the client and the server starts sending chunks from the big blob. Now, if the time needed for transferring the entire blob from server to client exceeds {{readTimeoutMs}} an {{IllegalStateException}} will be correctly thrown by {{StandbyDiff#readBlob}}, but will be swallowed by the {{StandbyDiff#childNodeChanged}} in its catch clause. A second 'sync blob' cycle will be triggered and, -this might succeed with the same {{readTimeoutMs}} for which it was failing before-, if {{readTimeoutMs * 2}} is enough, the blob will be synced on the standby. This happens because the server will continue sending the remaining chunks after {{IllegalStateException}} was thrown (first 'sync blob' cycle).

The consequence of these two 'sync blob' cycles is that sometimes, deleting the temporary file to which chunks are spooled to on the client fails (see Windows for example and OAK-6641 specifically). This way, instead of deleting the previous incomplete transfer, new chunks from the second 'sync blob' cycle are added. The blob persisted in the blob store on the client won't have the same size and id as the initial blob sent by the server.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Critical,2017-09-13 12:12:40,0
13101632,Standby server must always send the persisted head to clients,"Currently the standby server sends an un-persisted head record to clients. Under normal circumstances, the TarMK flush thread is able to persist it and its corresponding segment at a 5 seconds interval.
However, there are cases (uploading a very large blob > 10 GB) in which the flush thread writes the segment too late, and the 20s allowed by {{FileStoreUtil#readSegmentWithRetry}} are not enough. Therefore the server can't read the segment containing the head record and a timeout occurs on the client.",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Major,2017-09-12 12:46:53,0
13101295,test failure seen in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT,"{noformat}
Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@6bb75258
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@5b04476e
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 18.543 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
offRCUpgradesSegments(org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT)  Time elapsed: 5.578 sec  <<< FAILURE!
java.lang.AssertionError: Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.checkSegmentVersion(UpgradeIT.java:143)
        at org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT.offRCUpgradesSegments(UpgradeIT.java:109)


Results :

Failed tests:
  UpgradeIT.offRCUpgradesSegments:109->checkSegmentVersion:143 Segment version mismatch. Expected V_13, found V_12 expected:<V_13> but was:<V_12>
{noformat}
",test-failure windows,['segment-tar'],OAK,Bug,Blocker,2017-09-11 15:03:38,2
13101189,Backport OAK-6110 to 1.6 (Offline compaction uses too much memory),Backport the fix from OAK-6110 to the 1.6 branch.,compaction gc memory perfomance,['segment-tar'],OAK,Task,Major,2017-09-11 08:00:52,3
13100831,test failure in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT,"{noformat}
Tests run: 10, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 99.858 sec <<< FAILURE! - in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
testSyncBigBlob(org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT)  Time elapsed: 71.122 sec  <<< ERROR!
java.lang.RuntimeException: Error occurred while obtaining InputStream for blobId [8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4#1073741824]
Caused by: java.io.IOException: org.apache.jackrabbit.core.data.DataStoreException: Record 8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4 does not exist
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Record 8098b6ac1491be80b7e58a85767ede178c432866d90caf6726f556406ecc84a4 does not exist

{noformat}

(might be specific to Windows)",cold-standby,"['segment-tar', 'tarmk-standby']",OAK,Bug,Blocker,2017-09-08 14:38:13,0
13100451,Confusing log entries when memory requirements are not met at start of OnRC,"When the memory requirements for running OnRC are not met before the estimation phase the estimator will run nevertheless. The process will only be cancelled at the beginning of the compaction phase. The entries in the log file reflect this:

{code}
TarMK GC #1: canceling compaction because available memory level 306.4 MB (306395472 bytes) is too low...
TarMK GC #1: estimation started
TarMK GC #1: estimation completed in 343.5 ms (343 ms). Estimation skipped because of missing gc journal data (expected on first run)
TarMK GC #1: running full compaction
TarMK GC #1: compaction started ...
TarMK GC #1: unable to estimate number of nodes for compaction, missing gc history.
TarMK GC #1: compaction cancelled: Not enough memory.
TarMK GC #1: cleaning up after failed compaction
{code}

However they can easily be (mis-)read as compaction being re-triggered after having been cancelled and then being cancelled again. ",compaction gc,['segment-tar'],OAK,Improvement,Major,2017-09-07 12:57:33,3
13100179,The backup command should not silently upgrade the FileStore,"The backup command in oak-run should not accidentally perform a transparent upgrade of the FileStore. Instead, it should use a strict version check to fail fast if the code is run on an outdated version of the FileStore.",production resilience tooling,['run'],OAK,Technical task,Major,2017-09-06 15:20:58,2
13100178,Replace standby blob chunk size configuration with feature flag,We should remove the {{StandbyStoreService#BLOB_CHUNK_SIZE}} OSGi configuration and replace it with a feature flag. Rational: we expect customer to rarely change this thus not justifying the additional configuration complexity and testing overhead. ,cold-standby configuration,"['segment-tar', 'tarmk-standby']",OAK,Improvement,Major,2017-09-06 15:16:36,0
13100175,Avoid oak-run compact inadvertently upgrading the segment format ,"The transparent upgrade feature for segments from version 12 to 13 should not cause ""accidental upgrades"". That is, running Oak 1.8 oak-run compact on a store with segment version 12 should bail out unless a special option was specified on the command line. 

",gc production resilience,"['run', 'segment-tar']",OAK,Improvement,Major,2017-09-06 15:09:40,3
13099813,Add new segment-tar fixture for attaching a cold-standby to benchmarked primary,"If this fixture is chosen, a cold standby instance will be started, syncing with the primary every {{n}} seconds. All the benchmarks specified via {{[testcases]}} argument will be run on primary instance, and all statistics and reports will be linked to primary.

This could work similarly to {{Oak-Segment-Tar-DS}} and have dedicated options like {{--no-data-store}}, {{--private-data-store}} or {{--shared-data-store}}. ",cold-standby,"['benchmarks', 'segment-tar']",OAK,Improvement,Minor,2017-09-05 11:47:22,0
13099592,Provide list of all bundled nodes within a given DocumentNodeState,"For OAK-6353 we need to know all bundled nodestate in a given parent. For this purpose we should provide following method in DocumentNodeState

{code}
public Iterable<DocumentNodeState> getBundledNodesStates() {
{code}",bundling,['documentmk'],OAK,Improvement,Minor,2017-09-04 11:50:59,4
13098847,Move BulkTransferBenchmark to oak-benchmarks module,{{BulkTransferBenchmark}} should be moved from {{oak-segment-tar}} to {{oak-benchmarks}} to allow standard run of this cold standby related benchmark.,cold-standby,['segment-tar'],OAK,Task,Minor,2017-08-31 11:45:38,0
13098793,Improve resource management in BulkTransferBenchmark,"BulkTransferBenchmark might improperly dispose of test resources if error conditions occur. This is mostly due to improper resource tracking and finalization in BenchmarkBase, but similar mistakes have been made in BulkTransferBenchmark too.",cold-standby,['segment-tar'],OAK,Bug,Major,2017-08-31 07:52:15,2
13098752,SegmentWriteOperation.isOldGeneration() too eager,"The {{SegmentWriteOperation.isOldGeneration()}} predicate includes some segments that are not ""old"". This leads to more deferred compaction operations than strictly necessary. The affected segments are those generated by tail compaction. Tail compaction created segments should only be included in the predicate once they are from another full compaction operation. Otherwise referencing such segments is fine as they will not be reclaimed in a cleanup following a tail compaction.",compaction gc,['segment-tar'],OAK,Bug,Major,2017-08-31 07:00:26,3
13098388,rep:excerpt not working for content indexed by aggregation in lucene,"I mentioned that properties that got indexed due to an aggregation are not considered for excerpts (highlighting) as they are not indexed as stored fields.

See the attached patch that implements a test for excerpts in {{LuceneIndexAggregationTest2}}.

It creates the following structure:

{code}
/content/foo [test:Page]
 + bar (String)
 - jcr:content [test:PageContent]
  + bar (String)
{code}

where both strings (the _bar_ property at _foo_ and the _bar_ property at _jcr:content_) contain different text. 

Afterwards it queries for 2 terms (""tinc*"" and ""aliq*"") that either exist in _/content/foo/bar_ or _/content/foo/jcr:content/bar_ but not in both. For the former one the excerpt is properly provided for the later one it isn't.",excerpt,['lucene'],OAK,Bug,Major,2017-08-29 20:58:32,4
13097912,UpgradeIT produces unwanted output,"When {{UpgradeIT}} is executed, the following output is produced.

{noformat}
Running org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@75e01201
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@75e01201
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Apache Jackrabbit Oak 1.6.1
===> true
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook
===> org.apache.jackrabbit.oak.plugins.document.*, org.apache.jackrabbit.oak.plugins.segment.*, org.apache.jackrabbit.oak.segment.SegmentNodeBuilder, org.apache.jackrabbit.oak.spi.commit.EmptyHook, org.apache.jackrabbit.oak.spi.commit.CommitInfo
===> true
===> org.apache.jackrabbit.oak.segment.SegmentNodeStore@75e01201
===> SegmentNodeBuilder{path=/}
===> null
===> { property-name-5-0 = property-value-5-0, property-name-5-1 = property-value-5-1, property-name-5-2 = property-value-5-2, property-name-5-3 = property-value-5-3, property-name-5-4 = property-value-5-4, property-name-5-5 = property-value-5-5, property-name-5-6 = property-value-5-6, property-name-5-7 = property-value-5-7, property-name-5-8 = property-value-5-8, property-name-5-9 = property-value-5-9, node-5-3 = { ... }, node-5-4 = { ... }, node-5-9 = { ... }, node-5-1 = { ... }, node-5-2 = { ... }, node-5-7 = { ... }, node-5-8 = { ... }, node-5-5 = { ... }, node-5-0 = { ... }, node-5-6 = { ... } }
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 14.17 sec - in org.apache.jackrabbit.oak.segment.upgrade.UpgradeIT
{noformat}

The test should not produce any output, especially such a useless one.",test,['segment-tar'],OAK,Improvement,Minor,2017-08-28 08:48:54,3
13097268,Add tooling API,"h3. Current situation
Current segment store related tools are implemented ad-hoc by potentially relying on internal implementation details of Oak Segment Tar. This makes those tools less useful, portable, stable and potentially applicable than they should be.

h3. Goal
Provide a common and sufficiently stable Oak Tooling API for implementing segment store related tools. The API should be independent of Oak and not available for normal production use of Oak. Specifically it should not be possible to it to implement production features and production features must not rely on it. It must be possible to implement the Oak Tooling API in Oak 1.8 and it should be possible for Oak 1.6.

h3. Typical use cases
* Query the number of nodes / properties / values in a given path satisfying some criteria
* Aggregate a certain value on queries like the above
* Calculate size of the content / size on disk
* Analyse changes. E.g. how many binaries bigger than a certain threshold were added / removed between two given revisions. What is the sum of their sizes?
* Analyse locality: measure of locality of node states. Incident plots (See https://issues.apache.org/jira/browse/OAK-5655?focusedCommentId=15865973&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15865973).
* Analyse level of deduplication (e.g. of checkpoint) 

h3. Validation
Reimplement [Script Oak|https://github.com/mduerig/script-oak] on top of the tooling API. 

h3. API draft
* Whiteboard shot of the [API entities|https://wiki.apache.org/jackrabbit/Oakathon%20August%202017?action=AttachFile&do=view&target=IMG_20170822_163256.jpg] identified initially.
* Further [drafting of the API|https://github.com/mduerig/oak-tooling-api] takes place on Github for now. We'll move to the Apache SVN as soon as considered mature enough and have a consensus of where to best move it. ",tooling,['segment-tar'],OAK,New Feature,Major,2017-08-24 08:49:36,3
13096992,Lucene index: include/exclude key pattern list,"Similar to OAK-4637 but for lucene indexes

In some cases, property indexes contain many nodes, and updating them can be slow. Right now we have filters for node and mixin types, path (include and exclude). 

An include and exclude list of values (patterns) would be useful. For example the property ""status"", if we only ever run queries with the condition ""status = 'ACTIVE'"", then nodes with status INACTIVE and DONE don't need to be indexed.",docs-impacting,['lucene'],OAK,Improvement,Major,2017-08-23 11:04:18,4
13096362,Fix OSGi wiring after netty update to 4.1.x,"After netty update in OAK-6564, {{OSGiIT}} fails with the following exception:

{code}
Running org.apache.jackrabbit.oak.osgi.OSGiIT
ERROR: Bundle org.apache.jackrabbit.oak-segment-tar [36] Error starting file:/oak-it-osgi/target/test-bundles/oak-segment-tar.jar (org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.jackrabbit.oak-segment-tar [36]: Unable to resolve 36.0: missing requirement [36.0] osgi.wiring.package; (osgi.wiring.package=com.ning.compress))
org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.jackrabbit.oak-segment-tar [36]: Unable to resolve 36.0: missing requirement [36.0] osgi.wiring.package; (osgi.wiring.package=com.ning.compress)
	at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:3974)
	at org.apache.felix.framework.Felix.startBundle(Felix.java:2037)
	at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1291)
	at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:304)
	at java.lang.Thread.run(Thread.java:745)
{code}",cold-standby,['segment-tar'],OAK,Bug,Major,2017-08-21 08:07:14,2
13095635,GetBlobResponseEncoder should not write all chunks at once,"{{GetBlobResponseEncoder}} writes too fast all the chunks, leaving the channel in a not-writable state, after the first write. The problem is not visible at a first glance, especially when using small blobs for testing. Increasing the blobs size, as done for OAK-6538, revealed the problem. Not only this triggers hidden {{OutOfMemory}} errors on either server or client, but sometimes incomplete blobs are sent along, which are interpreted by the client as valid.

A more elegant solution, which also solves the memory consumption problem, would be to use {{ChunkedWriteHandler}} which employs complex logic on how and when to write the chunks. {{ChunkedWriteHandler}} must be used in conjunction with a custom {{ChunkedInput<ByteBuf>}} implementation to generate {{header}} + {{payload}} chunks from an {{InputStream}}, as done currently. This way the server will send more chunks only when the previous one was consumed by the client.

/cc [~frm]",cold-standby,['segment-tar'],OAK,Improvement,Major,2017-08-18 13:31:33,0
13095632,Update netty dependency to 4.1.x,"Taking into account the improvements listed at [0], and also the individual issues solved since our current netty version (4.0.41.Final) was released, I propose to bump up netty version to latest 4.1.14.Final.

/cc [~frm]

[0] http://netty.io/wiki/new-and-noteworthy-in-4.1.html ",cold-standby,['segment-tar'],OAK,Improvement,Minor,2017-08-18 13:16:06,0
13094761,gc.log should contain recordId of compacted root after offline compaction,Running offline gc currently adds an entry to the {{gc.log}} with a {{NULL}} record id. We should improve this and record the record id of the compacted root node state. ,compaction gc,['segment-tar'],OAK,Improvement,Major,2017-08-15 13:41:37,3
13094728,Implement ITs for rolling upgrade,We need basic IT coverage for the rolling upgrade scenario from Oak 1.6. to Oak 1.8. See OAK-6531.,IT migration test upgrade,['segment-tar'],OAK,Task,Major,2017-08-15 10:23:24,3
13094458,Progress indicator for compaction ,"We should implement a basic progress indicator for compaction displaying percent completed. This could be done by estimating the number of nodes from the entries in the {{gc.log}} and the timestamp of the last entry in the {{journal.log}}. Such a feature would explicitly *not* give an ETA as too many factors have an impact here (concurrent activity, IO bandwidth, quota, etc.).",compaction gc operation,['segment-tar'],OAK,New Feature,Major,2017-08-14 12:53:07,3
13094046,Move gcType to SegmentGCOptions,The {{gcType}} property should move from the {{FileStore}} class to the {{SegmentGCOptions}} along with all other GC related properties. ,gc,['segment-tar'],OAK,Improvement,Major,2017-08-11 08:28:48,3
13093446,Investigate cold standby memory consumption ,"In an investigation from some time ago, 4GB of heap were needed for transferring 1GB blob and 6GB for 2GB blob. This was in part due to using {{addTestContent}} [0] in the investigation, which allocates a huge {{byte[]}} on the heap. 

OAK-5902 introduced chunking for transferring blobs between primary and standby. This way, the memory needed for syncing a big blob should be around the chunk size used. Solving the way test data is created, it should be possible to transfer a big blob (e.g. 2.5 GB) with less memory.

[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/test/java/org/apache/jackrabbit/oak/segment/standby/DataStoreTestBase.java#L96",cold-standby,['segment-tar'],OAK,Task,Minor,2017-08-09 12:03:56,0
13093176,Implement rolling upgrade from Oak 1.6,"The segment format changes introduced for tail compaction (OAK-3349) must not require an explicit migration step. Instead there should be a rolling migration during normal operation. 

Things to consider:
* Segments from Oak 1.6
* Changes in tar index formats induced by the segment format changes
* Changes in gc.log induced by the segment format changes
* Required changes in the repository manifest and its interpretation
",migration upgrade,['segment-tar'],OAK,New Feature,Major,2017-08-08 14:18:38,2
13092410,Implement unit tests for OnlineCompactor,{{OnlineCompactor}} needs more unit test coverage going forward. ,compaction gc,['segment-tar'],OAK,Task,Major,2017-08-04 08:35:16,3
13092407,Improve tail compactions resilience when base state cannot be determined,"This is a follow up to OAK-3349: in tail compaction the {{FileStore.GarbageCollector.getBase()}} might fail to determine the base state to rebase onto. In this case we should fall back to full compaction and report (log, JMX) the problem and its exact cause. 

Failing to determine the base state might be caused by a missing or invalid {{gc.log}} file or a invalid or missing record id for the base state being recorded in {{gc.log}}. None of these cases should impact system stability. 

",compaction gc,['segment-tar'],OAK,Improvement,Major,2017-08-04 08:31:38,3
13092402,Properly handle tail compactions in deduplication caches,"This is a follow up to OAK-3349, which introduced tail compactions:

The deduplication caches currently only take the full generations into account and ignore the tail generations. Cache generations need to be a monotonically increasing, ordered sequence consisting of the full and tail part of the gc generation. See {{FileStoreBuilder.EvictingWriteCacheManager.evictOldGeneration}}. Optimally we find a way to decouple the segment generation from the cache generations as these are really separate concerns. ",compaction gc,['segment-tar'],OAK,Task,Major,2017-08-04 08:20:22,3
13085141,Add flag for controlling percentile of commit time used in scheduler,"In OAK-4732 the 50th percentile of the last 1000 commits is used as wait time before returning the current root. In order to parametrise the value of the percentile, it would be nice to have a new feature flag, e.g. {{oak.scheduler.head.lockWaitPercentile}}. Setting it to {{0}} would basically disable this. Setting it to a different value might be interesting in future experiments.",Performance scalability,['segment-tar'],OAK,Improvement,Minor,2017-07-06 09:17:41,0
13083418,Cleanup constants in Segment class,"Some of the constants in the {{Segment}} class still refer to the old 255 segment references limit. We should fix the comments, the constants and their usage to reflect the current situation where that limit has been lifted. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2017-06-29 10:24:16,3
13082516,Refactor monitoring of deduplication caches,Currently monitoring is of the deduplication caches is hard wired into the cache manager. It would be cleaner (and is in fact a pre-requisite for OAK-5790) to decouple the monitoring from the caches. ,technical_debt,['segment-tar'],OAK,Improvement,Major,2017-06-26 14:20:44,3
13081072,oak-run check should also check checkpoints ,"{{oak-run check}} does currently *not* traverse and check the items in the checkpoint. I think we should change this and add an option to traverse all, some or none of the checkpoints. When doing this we need to keep in mind the interaction of this new feature with the {{filter}} option: the paths passed through this option need then be prefixed with {{/root}}. ",tooling,"['run', 'segment-tar']",OAK,Improvement,Blocker,2017-06-20 10:18:03,0
13079958,Improve log reporting if multiple indexes compete for same query,"Currently if multiple indexes compete for same query i.e. they report cost > 0 and less than infinity then the log output is confusing. For e.g. for a query like

{noformat}
QueryEngineImpl Parsing xpath statement: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')]
QueryEngineImpl XPath > SQL2: select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */
QueryImpl cost using filter Filter(query=select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */ fullText=""foo.png"", path=/content/dam//*)
QueryImpl cost for aggregate lucene is 2.2424751E7
QueryImpl cost for lucene-property[/oak:index/index-a][/oak:index/index-b] is 146914.0
QueryImpl cost for reference is Infinity
QueryImpl cost for ordered is Infinity
QueryImpl cost for nodeType is Infinity
QueryImpl cost for property is Infinity
QueryImpl cost for traverse is Infinity
QueryImpl query execute select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */
QueryImpl query plan [dam:Asset] as [a] /* lucene:index-b(/oak:index/index-b) +(((...) +:ancestors:/content/dam ft:(""foo.png"") where (contains([a].[*], 'foo.png')) and (isdescendantnode([a], [/content/dam])) */
QueryImpl query execute select [jcr:path], [jcr:score], * from [dam:Asset] as a where contains(*, 'foo.png') and isdescendantnode(a, '/content/dam') /* xpath: /jcr:root/content/dam//element(*, dam:Asset)[jcr:contains(., 'foo.png')] */
QueryImpl query plan [dam:Asset] as [a] /* lucene:index-b(/oak:index/index-b) +(((...)) +:ancestors:/content/dam ft:(""foo.png"") where (contains([a].[*], 'foo.png')) and (isdescendantnode([a], [/content/dam])) */
{noformat}

Here both index-a and index-b satisfy the query. However from logs it appears that both have same cost. While actually the reported cost is the lowest one which in this case would be for index-b

{noformat}
QueryImpl cost for lucene-property[/oak:index/index-a][/oak:index/index-b] is 146914.0
{noformat}

So as a fix
* Report cost for multiple plans returned by same index type separately
* If cost is less than infinity and that plan is eventually not selected then also log its plan. This would help to see why specific plan lost
",candidate_oak_1_4 candidate_oak_1_6,['query'],OAK,Improvement,Minor,2017-06-15 04:47:45,4
13079441,MapRecord#getKeys should should initialize child iterables lazily,"Recently we saw OutOfMemory using [oakRepoStats|https://github.com/chetanmeh/oak-console-scripts/tree/master/src/main/groovy/repostats] script with a SegmentNodeStore setup where uuid index has 16M+ entries and thus creating a very flat hierarchy. This happened while computing Tree#getChildren iterator which internally invokes MapRecord#getKeys to obtain an iterable for child node names.

This happened because code in getKeys computes the key list eagerly by calling bucket.getKeys() which recursivly calls same for each child bucket and thus resulting in eager evaluation.
{code}
        if (isBranch(size, level)) {
            List<MapRecord> buckets = getBucketList(segment);
            List<Iterable<String>> keys =
                    newArrayListWithCapacity(buckets.size());
            for (MapRecord bucket : buckets) {
                keys.add(bucket.getKeys());
            }
            return concat(keys);
        }
{code}

Instead here we should use same approach as used in MapRecord#getEntries i.e. evalate the iterable for child buckets lazily
{code}
        if (isBranch(size, level)) {
            List<MapRecord> buckets = getBucketList(segment);
            List<Iterable<MapEntry>> entries =
                    newArrayListWithCapacity(buckets.size());
            for (final MapRecord bucket : buckets) {
                entries.add(new Iterable<MapEntry>() {
                    @Override
                    public Iterator<MapEntry> iterator() {
                        return bucket.getEntries(diffKey, diffValue).iterator();
                    }
                });
            }
            return concat(entries);
        }
{code}",candidate_oak_1_6,['segment-tar'],OAK,Improvement,Minor,2017-06-13 10:19:56,3
13078419,Add missing @Nonnull annotations,There is a few places where this annotation is missing and should be added.,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:07:22,3
13078418,Closeable.close in StandbyStoreService declares exception that is never thrown,"Remove the throws clause for the {{IOException}}, which never thrown.
",cold-standby technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:05:32,3
13078417,LockBasedScheduler.execute declares exception that is never thrown,"Remove the throws clause for the {{InterruptedException}}, which never thrown.",technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:03:08,3
13078414,Declare immutable field of FileStore.CompactionResult final,Immutable fields should be final,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 20:00:12,3
13078413,Declare StandbyStoreService.closer final,The field is immutable and should thus be declared final.,cold-standby technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 19:57:31,3
13078412,Convert FileStore.maxFileSize fiels into local variable,That field is only used in the constructor an can thus be converted to a local variable. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 19:55:13,3
13078411,Remove unused field SegmentNodeStore.reader,That field is not used any more since the explicit commit queue was introduced. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-06-08 19:52:30,3
13076144,Unreferenced argument reference in method SegmentBufferWriter.writeRecordId,This is a leftover from when we switched from record ids to record numbers as at that point it became unnecessary to track such references. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-05-31 09:34:10,3
13074194,Test failure: VersionGCTest.gcMonitorInfoMessages,"Regression

org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorInfoMessages
Failing for the past 1 build (Since Failed#330 )
Took 28 ms.
Error Message

expected:<3> but was:<7>

Stacktrace

java.lang.AssertionError: expected:<3> but was:<7>
	at org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorInfoMessages(VersionGCTest.java:224)

Failed run: [Jackrabbit Oak #330|https://builds.apache.org/job/Jackrabbit%20Oak/330/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/330/console]

",CI Jenkins test-failure,"['continuous integration', 'documentmk']",OAK,Bug,Major,2017-05-23 13:10:44,1
13074117,Minor performance impact by collecting data for SegmentCache statistics ,"OAK-5956 improved the statistics collected for the segment cache. As I expected this had an impact on performance as measured by our micro benchmarks. An impact is visible for {{ConcurrentReadTest}}, {{ConcurrentReadWriteTest}} and {{ConcurrentWriteTest}}. Impact on full stack operation is yet to be determined (I assume it is neglectable though). 

{noformat}
# ConcurrentReadTest               C     min     10%     50%     90%     max       N 
Oak-Segment-Tar (base)             1      43     101     112     129     219     525
Oak-Segment-Tar (OAK-5956)         1      45     104     118     138     264     496
{noformat}

The impact seems to be mostly caused by the {{SegmentId.onAccess}} callback. 

Possible solutions:
* Replace the {{SegmentId.onAccess}} callback with a direct reference to the underlying counter. 
* Allow disabling of the cache statistics. 
* Do nothing and accept the performance impact. 

The first approach is least attractive as it breaks encapsulation. Depending on the impact of this on full stack operations I'd either go with the 2nd or 3rd option.  ",perfomance,['segment-tar'],OAK,Improvement,Minor,2017-05-23 08:50:07,3
13070838,oak-run compact should have an option to disable/enable memory mapping,"Before 1.6 {{oak-run compact}} had a way to explicitly enable/disable memory mapping of the tar files. Somehow this got lost in {{oak-segment-tar}}. 

We need to add this back as e.g. on Windows memory mapping does not work well and we need to be able to explicitly disable it. ",gc,['segment-tar'],OAK,Bug,Major,2017-05-10 14:52:02,3
13070389,IllegalStateException when closing the FileStore during garbage collection,"When the file store is shut down during gc compaction is properly aborted. Afterwards it will trigger a cleanup cycle though, which runs concurrently to the proceeding shutdown potentially causing an {{ISE}}:

{noformat}
at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
at org.apache.jackrabbit.oak.segment.file.TarWriter.close(TarWriter.java:333)
at org.apache.jackrabbit.oak.segment.file.TarWriter.createNextGeneration(TarWriter.java:376)
at org.apache.jackrabbit.oak.segment.file.FileStore.newWriter(FileStore.java:682)
at org.apache.jackrabbit.oak.segment.file.FileStore.access$1700(FileStore.java:100)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.cleanup(FileStore.java:1069)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.cleanupGeneration(FileStore.java:1195)
at org.apache.jackrabbit.oak.segment.file.FileStore$GarbageCollector.run(FileStore.java:803)
at org.apache.jackrabbit.oak.segment.file.FileStore.gc(FileStore.java:387)
{noformat}
",gc,['segment-tar'],OAK,Bug,Major,2017-05-09 11:52:33,2
13069938,MongoMissingLastRevSeeker may return incomplete candidate set,The set returned by {{MongoMissingLastRevSeeker.getCandidates()}} may be incomplete. See also discussion in OAK-4535 and on [oak-dev|https://lists.apache.org/thread.html/36ade745b7f6a0417aab578c21ca9fb072a7d6e5c43c724b85a153bf@%3Coak-dev.jackrabbit.apache.org%3E].,candidate_oak_1_6,['mongomk'],OAK,Bug,Minor,2017-05-08 10:17:29,1
13069223,Add unit test coverage for IOUtils.humanReadableByteCount,"There is no unit test coverage for {{IOUtils.humanReadableByteCount}} in {{oak-commons}}.

I will add a patch shortly.",unit-test-missing,['commons'],OAK,Test,Minor,2017-05-04 17:44:42,3
13069198,Test failure: VersionGCTest.gcMonitorStatusUpdates,"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #253 has failed.
First failed run: [Jackrabbit Oak #253|https://builds.apache.org/job/Jackrabbit%20Oak/253/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/253/console]

{code}
java.lang.AssertionError: expected:<[INITIALIZING, COLLECTING, UPDATING, SPLITS_CLEANUP, IDLE]> but was:<[INITIALIZING, COLLECTING, CHECKING, COLLECTING, DELETING, SORTING, DELETING, UPDATING, SPLITS_CLEANUP, IDLE]>
	at org.apache.jackrabbit.oak.plugins.document.VersionGCTest.gcMonitorStatusUpdates(VersionGCTest.java:207)
{code}
",CI jenkins test-failure,"['continuous integration', 'documentmk']",OAK,Bug,Major,2017-05-04 16:30:46,1
13069197,Add unit test coverage for IOUtils.copy,"There is no unit test coverage for {{IOUtils.copy}} in {{oak-commons}}.

I will add a patch shortly.",unit-test-missing,['commons'],OAK,Test,Minor,2017-05-04 16:30:26,3
13068885,Add unit test coverage for IOUtils.writeInt/writeLong and IOUtils.readInt/readLong,"There is no unit test coverage for IOUtils.writeInt(), IOUtils.writeLong(), IOUtils.readInt(), and IOUtils.readLong() in oak-commons.

I am working on a patch and will have one to submit shortly.",easyfix patch test,['commons'],OAK,Test,Minor,2017-05-03 19:04:05,3
13065492,Offline compaction uses too much memory ,"Using offline compaction on a repository with nodes having many direct child node I observed a steady increase of heap usage. This is cause by using a {{MemoryNodeBuilder}} in {{CompactDiff.childNodeAdded()}}, which causes all those child nodes to be cached in memory. 

Changing the line

{code}
child = EMPTY_NODE.builder();
{code}

to 

{code}
child = writer.writeNode(EMPTY_NODE).builder();
{code}

fixes the problem as the latter returns a {{SegmentNodeBuilder}} where the former returns a {{MemoryNodeBuilder}}.",memory performance,['segment-tar'],OAK,Bug,Major,2017-04-20 11:36:17,3
13065446,Excessive memory usage by the cached segment references,"The references of a segment to other segments are cached within {{Segment.readReferencedSegments()}}. However caching the {{SegmentId}} instances themselves leads to excessive heap usage as each id also keeps a reference to its underlying segment. 

I suggest to cache those references as msb, lsb pairs instead and create the {{SegmentId}} instance on the fly when required. ",perfomance,['segment-tar'],OAK,Bug,Major,2017-04-20 08:59:43,3
13064720,org.apache.jackrabbit.oak.management.ManagementOperation should use TimeDurationFormatter,See OAK-6020.,technical_debt,['core'],OAK,Task,Minor,2017-04-18 14:17:43,3
13064699,Avoid reads from MongoDB primary,"With OAK-2106 Oak now attempts to read from a MongoDB secondary when it detects the requested data is available on the secondary.

When multiple Oak cluster nodes are deployed on a MongoDB replica set, many reads are still directed to the primary. One of the reasons why this is seen in practice, are observers and JCR event listeners that are triggered rather soon after a change happens and therefore read recently modified documents. This makes it difficult for Oak to direct calls to a nearby secondary, because changes may not yet be available there.

A rather simple solution for the observers may be to delay processing of changes until they are available on the near secondary.

A more sophisticated solution discussed offline could hide the replica set entirely and always read from the nearest secondary. Writes would obviously still go to the primary, but only return when the write is available also on the nearest secondary. This guarantees that any subsequent read is able to see the preceding write.",scalability,['mongomk'],OAK,Improvement,Major,2017-04-18 13:32:49,1
13063204,Assign meaningful names to cold standby threads,"For easier understanding of the cold standby behaviour, threads used by the cold standby implementations should be assigned more human readable names.",cold-standby,['segment-tar'],OAK,Improvement,Major,2017-04-11 14:06:13,2
13062916,Test failure: StandbyTestIT.testSyncLoop,"Jenkins CI failure: https://builds.apache.org/view/J/job/Jackrabbit%20Oak/

The build Jackrabbit Oak #144 has failed.
First failed run: [Jackrabbit Oak #144|https://builds.apache.org/job/Jackrabbit%20Oak/144/] [console log|https://builds.apache.org/job/Jackrabbit%20Oak/144/console]

Error Message

{code}
expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
{code}

Stacktrace

{code}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
    at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:126)
{code}

Standard Output

[^stdout.log]

Also failed at https://builds.apache.org/job/Jackrabbit%20Oak/394/",CI flaky-test jenkins,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-04-10 14:41:34,2
13062144,Cleanup blocks writers,"The refactoring from OAK-6002 moved the cleanup of the tar readers into the read lock, which blocks concurrent writers from progressing. This was a problem with {{oak-segment}} before and fixed with OAK-3329.
As cleanup can take up to a couple of minutes on busy system we should re-establish the former behaviour. ",cleanup gc,['segment-tar'],OAK,Improvement,Major,2017-04-06 15:38:54,2
13061466,Test failure: CompactionAndCleanupIT.concurrentCleanup,"That test fails for me on every 2nd run or so. This seems to be a regression introduced with OAK-6002. 

{code}
org.junit.ComparisonFailure: Expected nothing to be cleaned but generation 'b' for file data00002b.tar indicates otherwise. 
Expected :a
Actual   :b


at org.junit.Assert.assertEquals(Assert.java:115)
org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.concurrentCleanup(CompactionAndCleanupIT.java:1252)
{code}

[~frm], could you have a look?

",gc test-failure,['segment-tar'],OAK,Bug,Major,2017-04-04 16:05:05,2
13061396,Add TarFiles to the architecture diagram,The newly created {{TarFiles}} should be added to the architecture diagram for oak-segment-tar.,documentation,"['doc', 'segment-tar']",OAK,Improvement,Major,2017-04-04 09:56:35,2
13061085,Remove segment graph functionality from oak-run,"We could probably remove the segment graph functionality from oak-run. This has been implemented mainly (and solely?) for the purpose of analysing the problems around OAK-3348 and I assume it would quickly start falling behind as we move forward. Also for this kind of analysis I have switched to [oak-script|https://github.com/mduerig/script-oak], which is far more flexible. 

Let's decide closer to cutting 1.8 how to go forward here.",technical_debt tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2017-04-03 10:31:54,2
13061054,DocumentNodeStore.compare() fails with IllegalStateException in read-only mode,"Comparing node states in read-only mode may fail with an IllegalStateException when the journal is used to perform a diff.

{noformat}
java.lang.IllegalStateException: Root document does not have a lastRev entry for local clusterId 0
    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199)
    at com.google.common.cache.LocalCache.get(LocalCache.java:3932)
    at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)
    at org.apache.jackrabbit.oak.plugins.document.MemoryDiffCache.getChanges(MemoryDiffCache.java:83)
    at org.apache.jackrabbit.oak.plugins.document.TieredDiffCache.getChanges(TieredDiffCache.java:50)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.compare(DocumentNodeStore.java:1632)
[...]
Caused by: java.lang.IllegalStateException: Root document does not have a lastRev entry for local clusterId 0
    at org.apache.jackrabbit.oak.plugins.document.JournalDiffLoader.call(JournalDiffLoader.java:82)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffImpl(DocumentNodeStore.java:2428)
{noformat}

See also OAK-6011.",candidate_oak_1_6,['documentmk'],OAK,Bug,Minor,2017-04-03 08:02:55,1
13060292,Simplify cancellation of compaction by timeout ,Cancellation of (force) compaction by timeout is currently implemented on top of the {{CancelCompactionSupplier}}. It should better be implemented within though to simplify the implementation of OAK-3349. ,compaction gc refactoring technical_debt,['segment-tar'],OAK,Improvement,Major,2017-03-30 08:08:45,3
13060282,Introduce a FailingDocumentStore,Introduce a DocumentStore wrapper that can be instructed to fail after some number of operations or with some probability.,candidate_oak_1_4,['documentmk'],OAK,Test,Minor,2017-03-30 07:50:05,1
13060276,Add record id of the compacted root to the GC journal,We should also add the record id of the root node resulting from compaction to the gc log. This would have been helpful a couple of times already in the past for testing and post mortems. It will likely also be a requirement to implement the tail compaction approach form OAK-3349. ,compaction gc,['segment-tar'],OAK,Improvement,Major,2017-03-30 07:22:31,3
13059956,Revisions.setHead(Function) should return the new head or null instead of boolean,"Currently {{Revisions.setHead(Function, Option)}} returns a {{boolean}} to indicate success or failure. The caller has no access to the head resulting from this call. I would thus like to change this into the record id of the new head in case of success and {{null}} otherwise. ",refactoring,['segment-tar'],OAK,Improvement,Major,2017-03-29 07:53:41,3
13058163,Support path exclusion in secondary nodestore,"Secondary NodeStore feature (OAK-4180) for now currently supports path inclusion. It would be useful to have support for path exclusion also.

Using this a user can can include all content  under / but exclude /oak:index/uuid/:index entries.",secondary-nodestore,['documentmk'],OAK,Improvement,Major,2017-03-22 09:34:03,4
13057878,Document Metrics related classes and interfaces,"The Metrics related classes and interfaces in {{org.apache.jackrabbit.oak.stats}} and {{org.apache.jackrabbit.oak.plugins.metric}} are largely undocumented. Specifically it is not immediately how they should be used, how a new {{Stats}} instance should be added, what the effect this would have and how it would (or would) not be exposed (e.g. via JMX). 

",documentation technical_debt,['core'],OAK,Improvement,Major,2017-03-21 12:40:44,4
13057868,Improve cache statistics of the segment cache,"The statistics provided by the segment cache are off due to the fact it serves as 2nd level cache: as it doesn't see all the hits in the 1st level cache ({{SegmentId.getSegment()}}), it reports a hit/miss rate that is to low. 

We should look into how we could expose better statistics wrt. caching of segments. Possible consolidated over 1st and 2nd level caches. ",monitoring,['segment-tar'],OAK,Improvement,Major,2017-03-21 12:19:04,3
13057842,Unify and simplify the deduplication caches ,"As a preparation to enable better monitoring and add more precise monitoring probes to the deduplication caches I would like to unify their interfaces and simplify their setup. 
* Don't expose the cache statistics via the {{FileStore}} and leverage the the {{FileStoreBuilder}} instead for this.
* All deduplication caches should implement a unified {{Cache}} interface to simplify wrapping them (e.g. for additional access statistics collection). 
* Replace the ad-hoc collection of cache statistics in the {{NodeWriteStats}} inner class of the {{SegmentWriter}} and replace it with a more structured approach. 
* Expose additional cache access statistics via Metrics. 
* The additional statistics should discriminate caches access occurring as regular writes from such occurring during compaction. ",monitoring refactoring technical_debt,['segment-tar'],OAK,Improvement,Major,2017-03-21 10:12:38,3
13057836,PriorityCache statistics should support load exception count,Currently {{CacheStats.loadExceptionCount()}} always returns 0 on a cache statistics retrieved from the {{PriorityCache}}. I would like to implement this statistics by returning the number of times a {{put()}} failed on that cache because it did not find an empty slot. ,monitoring,['segment-tar'],OAK,Improvement,Major,2017-03-21 09:46:03,3
13056362,Remove CachedNodeDocument,The CachedNodeDocument interface was introduced with OAK-891 but then the feature was later removed with OAK-2937. The interface is not used anywhere and should be removed.,candidate_oak_1_4 technical_debt,"['core', 'documentmk']",OAK,Improvement,Minor,2017-03-15 17:22:25,1
13056257,Remove unused depth parameter SegmentWriteOperation#writeNode and related methods,That depth parameter is a leftover from when the node de-duplication cache used the depth of a node in the tree for its eviction strategy. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2017-03-15 11:06:36,3
13049537,OOM in SegmentReferenceLimitTestIT,Running that IT (with 4g heap) currently results in an {{OOME}}. We need to check whether the expectations are still valid for Segment Tar and either adapt the test or look into the memory consumption. ,test-failure,['segment-tar'],OAK,Bug,Major,2017-03-09 09:07:43,3
13048893,Cold standby should allow syncing of blobs bigger than 2.2 GB,"Currently there is a limitation for the maximum binary size (in bytes) to be synced between primary and standby instances. This matches {{Integer.MAX_VALUE}} (2,147,483,647) bytes and no binaries bigger than this limit can be synced between the instances.

Per comment at [1], the current protocol needs to be changed to allow sending of binaries in chunks, to surpass this limitation.

[1] https://github.com/apache/jackrabbit-oak/blob/1.6/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/standby/client/StandbyClient.java#L125",cold-standby,['segment-tar'],OAK,Improvement,Minor,2017-03-07 13:54:09,0
13047970,segment-tar should have a tarmkrecovery command,"{{oak-segment}} had a {{tarmkrecovery}} command responsible with listing candidates for head journal entries. We should re-enable this also for {{oak-segment-tar}}.

/cc [~mduerig] [~frm]",technical_debt tooling,"['run', 'segment-tar']",OAK,New Feature,Minor,2017-03-03 10:07:49,3
13047961,Evaluate utility of RepositoryGrowthTest benchmark,"{{RepositoryGrowthTest}} is a benchmark which makes use of the deprecated {{SegmentFixture}}. Since OAK-5834 removes the old {{oak-segment}} module and the code associated with it, {{RepositoryGrowthTest}} was also removed. If there's value in it, we can adapt it to work with the new {{SegmentTarFixture}}.

/cc [~chetanm]",benchmark,"['run', 'segment-tar']",OAK,Task,Minor,2017-03-03 09:51:37,0
13047677,Oak upgrade usage note refers to oak-run,"Running {{java -jar oak-upgrade*.jar}} prints 

{noformat}
Usage: java -jar oak-run-*-jr2.jar upgrade [options] jcr2_source [destination]
       (to upgrade a JCR 2 repository)

       java -jar oak-run-*-jr2.jar upgrade [options] source destination
       (to migrate an Oak repository)
{noformat}

Which incorrectly refers to {{oak-run upgrade}}. The latter will send me back to {{oak-run}}: ""This command was moved to the oak-upgrade module"". ",production tooling usability,['upgrade'],OAK,Bug,Minor,2017-03-02 13:34:03,1
13046916,Compressed segments,"It would be interesting to see the effect of compressing the segments within the tar files with a sufficiently effective and performant compression algorithm:

* Can we increase overall throughput by trading CPU for IO?
* Can we scale to bigger repositories (in number of nodes) by squeezing in more segments per MB and thus pushing out onset of thrashing?
* What would be a good compression algorithm/library?
* Can/should we make this optional? 
* Migration and compatibility issues?
",scalability,['segment-tar'],OAK,New Feature,Major,2017-02-28 09:47:04,0
13046660,Potential expensive call to NodeState.getChildNodeCount() in constructor of Template,"On of the {{Template}} constructors (the one used when writing templates) performs a call to {{NodeState.getChildNodeCount()}} to determine the value of {{Template.childName}}. I have seen this call comping up in performance traces on various occasions, which leads me to believe there is room for improvement here. ",performance,['segment-tar'],OAK,Improvement,Minor,2017-02-27 15:04:13,3
13046007,Remove the deprecated oak-segment module,"The {{oak-segment}} module has been deprecated for 1.6 with OAK-4247. We should remove it entirely now:

* Remove the module
* Remove fixtures and ITs pertaining to it
* Remove references from documentation where not needed any more

An open question is how we should deal with the tooling for {{oak-segment}}. Should we still maintain this in trunk and keep the required classes (which very much might be all) or should we maintain the tooling on the branches? What about new features in tooling? 
",deprecation technical_debt,['segmentmk'],OAK,Task,Minor,2017-02-24 12:46:27,0
13045682,TarMK: Implement tooling to repair broken nodes,"With {{oak-run check}} we can determine the last good revision of a repository and use it to manually roll back a corrupted segment store. 

Complementary to this we should implement a tool to roll forward a broken revision to a fixed new revision. Such a tool needs to detect which items are affected by a corruption and replace these items with markers. With this the repository could brought back online and the markers could be used to identify the locations in the tree where further manual action might be needed. ",production technical_debt tooling,"['run', 'segment-tar']",OAK,New Feature,Major,2017-02-23 17:07:20,0
13045662,Chronologically rebase checkpoints on top of each other during compaction,"Currently the compactor does just a rewrite of the super root node without any special handling of the checkpoints. It just relies on the node de-duplication cache to avoid fully exploding the checkpoints. 
I think this can be improved by subsequently rebasing checkpoints on top of each other during compaction. (Very much like checkpoints are handled in migration). ",compaction gc performance,['segment-tar'],OAK,Improvement,Major,2017-02-23 16:00:16,3
13045626,Perform update of single node in one remote call if possible,"If a single node is modified in a commit then currently it performs 2 remote calls

# The actual update
# Update of commit root

as for single node update commitRoot == node being updated we can optimize this case to see if both operations can be done in same call",performance,['documentmk'],OAK,Improvement,Major,2017-02-23 13:56:27,1
13045336,Test failure: segment.standby.MBeanIT.testClientAndServerEmptyConfig,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #472 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #472|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/472/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/472/console]",test-failure windows,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-02-22 18:54:25,0
13045164,Consistency check incorrectly fails for broken partial paths ,"To better explain the bug I'll describe the content of the revisions:
# Valid Revision
Adds child nodes {{a}}, {{b}}, {{c}}, {{d}}, {{e}}, {{f}} with various properties (blobs included)
# Invalid Revision
Adds child node {{z}} with some blob properties and then corrupts the {{NODE}} record holding {{z}}.
Now when the consistency check is run, it correctly detects that the second revision is broken, *marks the path {{/z}} as corrupt* and then continues checking the first valid revision. Because of a check introduced for OAK-5556 [1], which tries to validate the user provided absolute paths before checking them, the checker tries to check {{/z}} in the first revision, where of course it can't find it. Therefore the check incorrectly fails for this revision, although it shouldn't have to.

/cc [~mduerig], [~frm]",tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-02-22 09:40:34,0
13045162,Remove duplicate code for background operation timing log,There are multiple places in DocumentNodeStore where background operations log timing. This should be consolidated.,technical_debt,"['core', 'documentmk']",OAK,Improvement,Minor,2017-02-22 09:36:32,1
13044658,Test failure: org.apache.jackrabbit.oak.run.osgi.SegmentNodeStoreConfigTest.testDeadlock,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=unittesting #465 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=unittesting #465|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=unittesting/465/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=unittesting/465/console]",test-failure windows,"['continuous integration', 'pojosr']",OAK,Bug,Major,2017-02-20 21:59:47,3
13042975,InitialContent depends on document.bundlor.BundlingConfigInitializer,"[~chetanm], in the light of OAK-4975 a dependency to the document nodestore code got introduced in {{org.apache.jackrabbit.oak.plugins.nodetype.write.InitialContent}} by adding the following line:
{code}
        BundlingConfigInitializer.INSTANCE.initialize(builder);
{code}

the {{BundlingConfigInitializer}} is defined in the {{org.apache.jackrabbit.oak.plugins.document.bundlor}}.

To me that looks quite troublesome and I don't think the generic JCR-InitialContent should have any dependency on the document nodestore code base.

Why not defining a dedicated {{RepositoryInitializer}} for that kind of init an making sure it is listed in the (default) setup scenarios (or at least in those that actually have a document store and thus require this)?
",modularization tech-debt,['core'],OAK,Bug,Minor,2017-02-14 15:29:45,1
13042569,Revisit FileStoreStats mbean stats format,"This is a bigger refactoring item to revisit the format of the exposed data, moving towards having it in a more machine consumable friendly format.",monitoring,['segment-tar'],OAK,Improvement,Major,2017-02-13 13:06:28,3
13042568,Expose IOMonitor stats via JMX,"Followup of OAK-5632 and OAK-5631, to expose the collected data via JMX for external use.",monitoring,['segment-tar'],OAK,New Feature,Minor,2017-02-13 13:04:48,3
13041683,Simplify consistency check,"The current implementation of the consistency check ({{ConsistencyChecker}},{{CheckCommand}})
is cluttered with unnecessary checks regarding deprecated arguments of the {{check}} command. 
With OAK-5595, deep traversals are enabled by default, therefore the code needs to be revised to take this into account. The same applies to the argument taken by {{--bin}} option, which was removed in OAK-5604.

Moreover, {{ConsistencyChecker}} could be refactored in order to better distinguish when:
* a full path at the given revision is checked
* a node and its properties are checked
* a node and its descendants are checked",tooling,"['run', 'segment-tar']",OAK,Improvement,Major,2017-02-09 12:26:23,0
13041595,Test failure: segment.standby.ExternalSharedStoreIT.testProxyFlippedIntermediateByteChange2,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=integrationTesting #443 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.7 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_MK,profile=integrationTesting #443|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=integrationTesting/443/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.7%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_MK,profile=integrationTesting/443/console]",test-failure windows,"['continuous integration', 'segmentmk']",OAK,Bug,Major,2017-02-09 05:54:16,2
13041582,Test failure: org.apache.jackrabbit.mk.util.CommitGateIT.test,"Jenkins Windows CI failure: https://builds.apache.org/job/Oak-Win/

The build Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #443 has failed.
First failed run: [Oak-Win/Windows slaves=Windows,jdk=JDK 1.8 (unlimited security) 64-bit Windows only,nsfixtures=SEGMENT_TAR,profile=integrationTesting #443|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/443/] [console log|https://builds.apache.org/job/Oak-Win/Windows%20slaves=Windows,jdk=JDK%201.8%20(unlimited%20security)%2064-bit%20Windows%20only,nsfixtures=SEGMENT_TAR,profile=integrationTesting/443/console]",test-failure windows,['continuous integration'],OAK,Bug,Major,2017-02-09 04:04:42,1
13041411,"The check command should accept a non-argument ""bin"" option for checking binaries","Currently the {{--bin}} option expects a {{Long}} argument, as the {{LENGTH}} up to which to scan the content of binary properties. The {{--bin}} option should be simplified so that it doesn't take any arguments. Running {{check}} without the {{--bin}} flag won't scan any binary properties, while including {{--bin}} option will scan all binaries, no matter their size.

If an argument is given with {{--bin}}, there will be a failure and a warning will be displayed.

The message displayed at the end of the consistency check will be changed to take into account whether binary properties were traversed or not.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2017-02-08 15:25:25,0
13041330,Test coverage for CheckCommand,"We should add tests for {{o.a.j.o.r.CheckCommand}} in order to validate recent changes introduced by adding/removing options and their arguments (see OAK-5275, OAK-5276, OAK-5277, OAK-5595). There is also a new feature introduced by OAK-5556 (filter paths) and a refactoring in OAK-5620 which must be thoroughly tested in order to avoid regressions.",tooling,"['run', 'segment-tar']",OAK,Task,Minor,2017-02-08 09:39:12,0
13040675,The check command should do deep traversals by default,"Only checking accessibility of the root nodes doesn't make much sense. Even more so because the file store automatically rolls back on startup if a root revision is not accessible. In terms of not doing full traversals, it is more interesting to restrict by path (aka OAK-5556).

The {{--deep}} option will still be accepted, but there will be a failure when it is specified. An explanation that full traversal is now done regardless of that option will be printed.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2017-02-06 16:08:57,0
13040625,"The check command doesn't do any check when ""deep"" option is not provided","When the {{check}} command is used without {{--deep}} option, there is no check/traversal being done against the repository.

First relevant line in code is [1], where a check is supposed to happen, but due to a mismatch between argument expected/argument provided, {{null}} is always returned without checking anything. The method which should do the actual check [2] expects a set of paths to be traversed, but this set is always empty. Therefore, relevant code for running the check is never executed [3].

[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L120
[2] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L183
[3] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/file/tooling/ConsistencyChecker.java#L194",tooling,"['run', 'segment-tar']",OAK,Bug,Major,2017-02-06 12:51:21,0
13040030,org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1399 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1399|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1399/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1399/console]",cold-standby test-failure ubuntu windows,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-02-03 04:10:23,2
13039435,Reduce reads with overlapping previous documents,"Reading a node state in a past revision can become expensive when the change history in the previous documents have overlapping changes. In this case, the changes in the previous documents must be merge sorted to find the correct value for the properties on the node. The more overlapping ranges there are, the more sorting is needed.

There is a prominent node in the repository that seems to create quite many of those previous documents. The {{/:async}} nodes gets frequent updates and is therefore split on a regular basis. Because the properties on this node are not all updated at the same time, it is quite likely that previous document ranges overlap.",candidate_oak_1_4 observation performance,"['core', 'documentmk']",OAK,Improvement,Major,2017-02-01 11:19:24,1
13039208,Allow filter paths for Check command,It would be good if the {{check}} command would allow for filtering on content path. This would help in quickly identifying what is the good revision of a specific broken node in cases of very large repos.,tooling,"['segment-tar', 'segmentmk']",OAK,New Feature,Major,2017-01-31 15:01:09,0
13038925,Document TarMK design,"We should improve our documentation of the internal design of the the TarMK. There is currently a [single section|http://jackrabbit.apache.org/oak/docs/nodestore/segment/overview.html#design]. 

* Add a high level class diagram and description of the overall structure of the TarMK. 
* Decide what to do with {{segmentmk.md}}. My preference would be to incorporate everything from it we didn't cover so far into {{segment/overview.md}}, {{segment/records.md}} and {{segment/tar.md}}. 
* Rewrite, clarify the design section in {{segment/overview.md}}. 
",documentation,['segment-tar'],OAK,Technical task,Major,2017-01-30 16:13:41,2
13038747,Improve indexing resilience,grouping the improvements for indexer resilience in this issue for easier tracking,resilience,['lucene'],OAK,Epic,Critical,2017-01-29 12:22:52,4
13038623,Test failure: security.authentication.ldap.LdapProviderTest (Address already in use),"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting #1390 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting #1390|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting/1390/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_TAR,profile=integrationTesting/1390/console]",test-failure ubuntu windows,"['auth-ldap', 'continuous integration']",OAK,Bug,Major,2017-01-28 06:53:48,3
13038418,Test failure: standalone.RepositoryBootIT.repositoryLogin,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1386 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1386|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1386/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1386/console]",test-failure,['continuous integration'],OAK,Bug,Major,2017-01-27 11:19:23,4
13038374,Test failure: segment.standby.ExternalSharedStoreIT/BrokenNetworkTest.test...,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1384 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1384|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1384/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1384/console]",test-failure ubuntu,"['continuous integration', 'segmentmk']",OAK,Bug,Major,2017-01-27 06:57:47,2
13037770,SNFE when running compaction after a cancelled gc,When a gc run is cancelled the subsequent run (compaction phase to be precise) can result in a {{SNFE}}. The reason for this is the node deduplication cache not being purged in the cancellation case. This causes the subsequent compaction to reference node states from that cache that have been cleaned. ,compaction gc,['segment-tar'],OAK,Bug,Blocker,2017-01-25 17:16:16,3
13037624,Standby Automatic Cleanup should be on by default,"The OSGi setting controlling standby automatic cleanup, {{standby.autoclean}}, should be set to {{true}} by default. When the automatic cleanup is on, the {{cleanup()}} method will be called on standby, provided the size of the store increases over 25% on a sync cycle.",cold-standby,['segment-tar'],OAK,Improvement,Minor,2017-01-25 09:06:49,2
13037570,Reduce lookup for oak:index node under each changed node,"Currently {{IndexUpdate}} does a lookup for {{oak:index}} node under each changed node. This is done to pickup index definitions and create IndexEditor based on those so as to index content under that subtree. 

This lookup results in extra remote calls on DocumentNodeStore based setup as for non leaf nodes NodeStore has to check from remote storage to determine if {{oak:index}} node is present or not.

This lookup can be avoided by
# Having an {{Editor}} which adds a hidden property {{:oak-index-present}} in parent node upon addition of {{oak:index}} as a child node
# IndexUpdate would then does a lookup for {{oak:index}} node only if such a hidden property is found

For upgrade we would have some logic which would make use of Nodetype index to identify all such nodes and mark them

Discussion [thread|https://lists.apache.org/thread.html/70d5ffff0f950d7fc25bc1bbb41527f5672825f8cf2b238f54df2966@%3Coak-dev.jackrabbit.apache.org%3E] on oak-dev",performance,['indexing'],OAK,Improvement,Major,2017-01-25 04:10:07,4
13036145,Test failure: LdapDefaultLoginModuleTest address already in use,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1375 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_TAR,profile=unittesting #1375|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1375/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_TAR,profile=unittesting/1375/console]",test-failure ubuntu windows,"['auth-ldap', 'continuous integration']",OAK,Bug,Major,2017-01-19 09:12:56,3
13035890,Overdue document split with many cluster nodes,"There are cases where a document split is overdue and it continues to grow until it hits the MongoDB limit of 16MB.

It gets more likely, the more cluster nodes are running and when they all update the same property with a somewhat larger value. E.g. the :childOrder property of a node with many children.

The current split logic has multiple triggers that will result in creating a previous document.

- There are 100 old changes for a cluster node that can be moved
- A node was recreated with a binary bigger than 4k
- The main document is bigger than 256k and 30% of its size can be moved to a previous document

The last condition may cause the uncontrolled growth of the document when there are many cluster nodes. If all cluster nodes continuously change a property on a node, then none of the cluster nodes will be able to move 30% of the document.",candidate_oak_1_4,"['core', 'documentmk']",OAK,Bug,Minor,2017-01-18 14:36:33,1
13035466,Improve the transaction rate of the TarMK,"The TarMK's write throughput is limited by the way concurrent commits are processed: rebasing and running the commit hooks happen within a lock without any explicit scheduling. This epic covers improving the overall transaction rate. The proposed approach would roughly be to first make scheduling of transactions explicit, then add monitoring on transaction to gather a better understanding and then experiment and implement explicit scheduling strategies to optimise particular aspects. 

h2. Summary of ideas mentioned in an offline sessions

h3. Advantages of explicit scheduling:
* Control over (order) of commits
* Sophisticated monitoring (commit statistics, e.g. commit rate, time in queue, etc.) 
* Favour certain commits (e.g. checkpoints)
* Reorder commits to simplify rebasing
* Suspend the compactor on concurrent commits and have it resume where it left off afterwards
* Parallelise certain commits (e.g. by piggy backing)
* Implement a concurrent commit editor. we'd need to take care of proper access to the shared state; [~frm] maybe introduce the idea of a common context to enforce concurrent access semantics.

h3. Scheduler Implementation
* Expedite
* Prioritise
* Defer
* Collapse
* Coalesce
* Parallelise
* Piggy back: can we piggy back commits on top of each other? The idea would be while processing the changes of one commit to also check them for conflicts with the changes of other commits waiting to commit. If a conflict is detected there, that other commit can immediately be failed (given the current commit doesn't fail).
* Merging non conflicting commits. Given multiple transactions ready to commit at the same time. Can we process them as one (given they don't conflict) instead of one after each other, which requires rebasing the later transaction to be rebase on the former.
* Shield the file store from {{InterruptedException}} because of thread boundaries introduced
* Implement tests, benchmarks and fixtures for verification
",scalability,['segment-tar'],OAK,Epic,Major,2017-01-17 10:45:57,3
13035392,Test failure: RepositoryBootIT.repositoryLogin,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1369 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=SEGMENT_MK,profile=integrationTesting #1369|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1369/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=SEGMENT_MK,profile=integrationTesting/1369/console]",test-failure ubuntu,"['continuous integration', 'examples']",OAK,Bug,Major,2017-01-17 04:01:49,4
13033612,Test failure: BasicServerTest.testServerOk() Address already in use,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1363 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_RDB,profile=unittesting #1363|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1363/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_RDB,profile=unittesting/1363/console]",test-failure,"['continuous integration', 'run']",OAK,Bug,Major,2017-01-11 06:43:51,4
13033423,Add a persistence-dependent namespace when running CLI commands,"Commands in oak-run currently live in a flat namespace. If a command is specific to only one implementation, it will leave along other implementation-specific commands without any means of distinguishing what belongs where.

I would like to add a layer of indirection to the oak-run command line interface, so to parse commands in the following fashion:

{noformat}
oak-run segment debug /path/to/folder
oak-run mongo debug mongodb://host:12345
oak-run rdb debug jdbc:oracle:oci8:scott/tiger@myhost
{noformat}

In this scenario, oak-run would become a simple entry point that would delegate to implementation-specific command line utilities based on the first argument. In the previous example, {{segment}}, {{mongo}} and {{rdb}} would delegate to three different implementation specific CLI utilities. Each of these CLI utilities will understand the {{debug}} command and will collect command-line parameters as it sees fit.

If the code for a command is so generic that can be reused from different commands, it can be parameterised and reused from different implementation-specific commands.

The benefit of this approach is that we can start moving commands closer to the implementations. This approach would benefit oak-run as well, which is overloaded with many commands from many different implementations.",tooling,['run'],OAK,Improvement,Major,2017-01-10 15:45:25,2
13033355,Skip processing of queued changes if async index update is detected in ExternalIndexObserver,"ExternalIndexObserver is currently backed by a queue (its wrapped in BackgroundObserver). Currently it processed the changes one by one as received from the queue. If this processing takes long time then its possible that it would lag behind the async indexing cycle.

So ExternalIndexObserver may be busy indexing changes from [r1-r2] but async indexing is already done indexing changes upto r3 (r3 > r2) and IndexTracker would move to newer index version. In such case work done by ExternalIndexObserver is wasted. 

This can be optimized by ensuring that ExternalIndexObserver can see the lastIndexTo of :async as per latest entry in queue. If that is newer than one its processing then it can skip processing the queue entry and thus free up space in queue",performance,['lucene'],OAK,Improvement,Major,2017-01-10 12:00:48,4
13033276,Test Failure: org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.compactionNoBinaryClone,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1360 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.8 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1360|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1360/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.8%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1360/console]",test-failure,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2017-01-10 05:38:07,3
13032769,Test failure: TomcatIT.testTomcat(),"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1357 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1357|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1357/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1357/console]",test-failure ubuntu,"['continuous integration', 'examples']",OAK,Test,Major,2017-01-07 04:04:59,4
13032183,Test failure: segment.standby.BrokenNetworkTest,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1355 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=SEGMENT_MK,profile=unittesting #1355|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1355/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=SEGMENT_MK,profile=unittesting/1355/console]

Initially reported test failure: testProxyFlippedStartByte()
Additional test failure reported via OAK-5476: testProxySSLSkippedBytes()
Additional test failure reported via OAK-5477: testProxyFlippedStartByteSSL()
Additional test failures reported via OAK-5478: all tests failed",test-failure,"['continuous integration', 'tarmk-standby']",OAK,Bug,Major,2017-01-05 07:42:26,2
13030476,Test failure: ...stats.ClockTest.testClockDrift,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1351 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=unittesting #1351|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1351/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=unittesting/1351/console]",test-failure,['continuous integration'],OAK,Bug,Major,2016-12-24 07:30:44,3
13029783,Cancelled garbage collection not reported to GCMonitor,When revision garbage collection is cancelled because one of the conditions in {{CancelCompactionSupplier}} then this should be reported to {{GCMonitor.skipped}}. Currently it is not. ,gc monitoring production technical_debt,['segment-tar'],OAK,Bug,Major,2016-12-21 15:19:49,3
13029507,Test failure: CompactionAndCleanupIT,"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1338 has failed.
First failed run: [Apache Jackrabbit Oak matrix/Ubuntu Slaves=ubuntu,jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting #1338|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1338/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/Ubuntu%20Slaves=ubuntu,jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_NS,profile=integrationTesting/1338/console]",test-failure,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2016-12-20 16:49:36,3
13029400,Improve code coverage of oak-segment-tar,Improve code coverage of oak-segment-tar.,technical_debt,['segment-tar'],OAK,Improvement,Major,2016-12-20 09:57:22,2
13029134,Clarify the various directories and their usages in SegmentNodeStoreService,"In {{SegmentNodeStoreService}} there is {{repository.home}}, {{DIRECTORY}}, {{getRootDirectory()}}, {{getDirectory()}} and {{getBaseDirectory()}} mostly without documentation about their intention. I think we should clarify, document and consolidate them. ",technical_debt,['segment-tar'],OAK,Task,Major,2016-12-19 13:27:31,0
13028399,Remove ReversedLinesFileReaderTestParamBlockSize,"{{org.apache.jackrabbit.oak.segment.file.ReversedLinesFileReaderTestParamBlockSize}} should actually have been removed with OAK-4467, where we replaced our {{ReversedLinesFileReader}} implementation with the one from {{commons-io}}. ",technical_debt,['segment-tar'],OAK,Task,Minor,2016-12-15 14:46:13,3
13028126,Remove legacy upgrade code from AbstractFileStore.collectFiles,{{AbstractFileStore.collectFiles()}} contains legacy upgrade code dating back to special handling of binaries in older version of {{oak-segment}} (bulkFiles). We should remove this code. ,technical_debt,['segment-tar'],OAK,Improvement,Minor,2016-12-14 15:34:03,3
13028123,Possible null dereference in MapRecord,"{{MapEntry.compareTo()}} passes possibly {{null}} {{MapEntry.value}} to {{ComparisonChain.compare(Comparable, Comparable)}}, which does not accept {{null}} values. ",technical_debt,['segment-tar'],OAK,Bug,Major,2016-12-14 15:22:44,3
13027751,Static code analysis and code cleanup,"We should run some static analysis (i.e. sonar, find bugs, etc.) on our code base and fix the most sever issues. ",technical_debt,['segment-tar'],OAK,Task,Major,2016-12-13 10:28:28,3
13027713,The tarmkdiff command does too many things,"The {{tarmkdiff}} command is actually the combination of two commands. 

The first command, activated when the {{\-\-list}} flag is specified, list available revisions in the Segment Store. For this command, only the {{\-\-output}} option is relevant. If other options are specified, they are ignored.

The second command is the proper logic of {{tarmkdiff}}. This logic is activated only if the {{\-\-list}} flag is not specified. For this command, every option on the command line is relevant.

The logic listing available revisions in the Segment Store should be encapsulated in its own command, without cluttering the CLI of {{tarmkdiff}}.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-13 08:01:58,2
13027472,"The check command defines a useless default value for the ""bin"" option","The {{check}} command enables the traversal of binary properties via the {{--bin}} option. The user could provide a value for this option to specify the amount of bytes that should be traversed for every binary value. The default value for the {{--bin}} option is zero, effectively disabling the traversal of binary properties. Instead, if a value for this property is not specified, the tools should traverse the binary properties in their entirety. A value should be specified only to restrict the amount of bytes to traverse.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-12 14:59:27,0
13027471,"The check command overloads the meaning of the ""deep"" option","The {{--deep}} option accepted by the {{check}} command is semantically overloaded. It is used both as a flag to enable deep content traversal and as a way to specify the frequency of debug messages printed by the tool. 

This option should be split in two. In particular, {{--deep}} should retain its behaviour of on/off flag for deep traversal, and a new command line option should be introduced to specify the interval of debug messages.",tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-12 14:52:58,0
13027470,The check command should accept the path to the store as a positional argument,The {{check}} tool requires the path to the store to be specified. The path is passed to the tool via a required option {{--path}}. This way of specifying the path to the store is verbose for no good reason. It would be nicer if the path to the Segment Store would be specified via a positional argument instead.,tooling,"['run', 'segment-tar']",OAK,Improvement,Minor,2016-12-12 14:48:33,0
13027002,Better default for size delta estimation ,The default value for the size delta estimation used during garbage collection should be changed to 1GB.,gc,['segment-tar'],OAK,Bug,Major,2016-12-09 13:47:59,2
13026547,Test failure: ExternalPrivateStoreIT. testSyncUpdatedBinaryProperty(),"Jenkins CI failure: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/

The build Apache Jackrabbit Oak matrix/jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1320 has failed.
First failed run: [Apache Jackrabbit Oak matrix/jdk=JDK 1.7 (latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting #1320|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1320/] [console log|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/jdk=JDK%201.7%20(latest),nsfixtures=DOCUMENT_RDB,profile=integrationTesting/1320/console]",test-failure,"['continuous integration', 'segment-tar']",OAK,Bug,Major,2016-12-08 03:50:37,2
13025018,Avoid updating the index nodestate if no change is done in index,"As noted in OAK-5211 directory listing was getting modified (due to reorder) even if no change happens in index. 

Another place where we update state post index close is at "":status"" node where we store {{lastUpdated}} and {{indexedNodes}} post index close. In normal cases LuceneIndexEditor avoids initializing the IndexWriter if there is no change. However it can happen that when any node gets deleted the editor performs a delete operation. It can happen that tree being deleted is not indexed but still editor would do this as it cannot determine that easily. And in doing that IndexWriter would be initialized.

Currently IndexWriter being initialized is considered same as index updated. Due to this index status nodes gets unnecessarily updated even if there is no change in index which causes the IndexTracker to reopen the index even when it has not changed. 

We should make this more explicit and find a way to determine if index has been updated or not",performance,['lucene'],OAK,Improvement,Minor,2016-12-02 08:59:46,4
13025009,OakDirectory should not save dir listing if no change is done,"[~alex.parvulescu] noted that OakDirectory saves the directory listing even if no actual change happened in the directory. Only change that happens is the order of entries in set. 

In normal cases LuceneIndexEditor avoids initializing the IndexWriter if there is no change. However it can happen that when any node gets deleted the editor performs a delete operation. It can happen that tree being deleted is not indexed but still editor would do this as it cannot determine that easily. This would lead to OakDirectory being closed without any change and thus can lead save of dir listing with just change in order of entries",performance,['lucene'],OAK,Improvement,Minor,2016-12-02 08:09:35,4
13024389,Deprecate stubs and fixtures related to oak-segment,I would like to deprecate the various fixtures and stubs for {{oak-segment}} and replace them where possible with its corresponding variants of {{oak-segment-tar}},deprecation technical_debt,"['commons', 'it', 'jcr', 'lucene', 'run', 'segmentmk', 'solr', 'tarmk-standby']",OAK,Improvement,Major,2016-11-30 10:46:41,3
13023967,Non default MissingIndexProviderStrategy is not being passed to child editor,"{{IndexUpdate}} allows passing in a custom {{MissingIndexProviderStrategy}}. However the custom provider is only stored as instance variable in {{IndexUpdate}} and does not get passed to child IndexUpadate instance.

As a fix it should be stored in IndexUpdateRootState and accessed from that",candidate_oak_1_4,['core'],OAK,Bug,Minor,2016-11-29 05:30:04,4
13023742,Get rid of test dependency to json.org JSON parser,"See <http://mail-archives.apache.org/mod_mbox/www-legal-discuss/201611.mbox/%3C0CE2E8C9-D9B7-404D-93EF-A1F8B07189BF%40apache.org%3E>

",legal,['remoting'],OAK,Task,Blocker,2016-11-28 14:16:12,2
13023222,Increase default size of the observation queue from 1000 to 10000,"To mitigate problems when hitting the observation queue limit (OAK-2683) we should bump the default size from 1000 to 10000.

cc [~stefanegli]",resilience,"['core', 'jcr']",OAK,Improvement,Blocker,2016-11-24 15:49:21,3
13022465,Collect stats around number of nodes traversed by AsyncIndexer,"At times we see very long time in async index update cycle

{noformat}
06.11.2016 18:16:18.703 *INFO* [aysnc-index-update-async] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate [async] AsyncIndex update run completed in 25.58 min. Indexed 7498 nodes
06.11.2016 18:41:43.088 *INFO* [aysnc-index-update-async] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate [async] AsyncIndex update run completed in 24.79 min. Indexed 28335 nodes
{noformat}

It would be good to also include the number of nodes traversed in that diff. For the record such high times were seen on a setup which did not had persistent cache enabled which probably caused slow diff",candidate_oak_1_4,['query'],OAK,Improvement,Minor,2016-11-22 10:32:23,4
13020824,Change default size of the node deduplication cache,The current default of the node deduplication cache is 8M. We should consider changing this to a smaller value still resulting in effective compactions. ,compaction gc perfomance,['segment-tar'],OAK,Improvement,Major,2016-11-15 15:16:07,3
13020468,Backup is not incremental i.e. previous tar files are duplicated,"Performing two backups via {{RepositoryManagementMBean.startBackup}}, directory size increases not only with the delta, but also again with the size of existing tar files. This lead me to the conclusion that backup is not incremental.",operations production tooling,['segment-tar'],OAK,Improvement,Minor,2016-11-14 10:26:15,0
13018193,Journal diff not working for changes in bundled node,For changes in bundled nodes diff is not reporting change in properties of bundled node,bundling,['documentmk'],OAK,Bug,Major,2016-11-04 18:23:37,4
13017636,Improve GC scalability on TarMK,"This issue is about making TarMK gc more scalable: 
* how to deal with huge repositories.
* how to deal with massive concurrent writes.
* how can we improve monitoring to determine gc health. 
** Monitor deduplication caches (e.g. deduplication of checkpoints)

Possible avenues to explore:
* Can we partition gc? (e.g. along sub-trees, along volatile vs. static content)
* Can we pause and resume gc? (e.g. to give precedence to concurrent writes) 
* Can we make gc a real background process not contending with foreground operations? 

This issue is a follow up to OAK-2849, which was about efficacy of gc.",gc scalability,['segment-tar'],OAK,Epic,Major,2016-11-03 13:40:55,3
13017275,Optimise ImmutableRecordNumbers,{{ImmutableRecordNumbers}} is based on a map. This turns to be expensive as the items in the map are accessed very frequently. I would like to look into way of optimising this using a linear storage model instead. ,perfomance,['segment-tar'],OAK,Improvement,Major,2016-11-02 15:14:31,3
13017273,Add @Description annotations to methods in RepositoryManagementMBean,The methods in {{RepositoryManagementMBean}} do not provide any description to the end user and might seem unclear for someone trying to trigger them via JMX.,osgi-config,['segment-tar'],OAK,Task,Trivial,2016-11-02 15:13:14,0
13017185,Upgrade to Tika 1.15 version,"Oak Lucene index is currently using Tika 1.5 version while current latest release of Apache Tika is 1.14, I think there are lots of ""interesting"" bugs fixed, and possibly improvements (performance, more accurate text extraction, etc.) we could get at almost 0 cost by just bumping the version number.

Release notes https://tika.apache.org/1.15/index.html",candidate_oak_1_4,['lucene'],OAK,Improvement,Major,2016-11-02 11:50:17,4
13017154,Remove the old estimation OSGi setting (compaction.gainThreshold),"Currently, there are two implementations for finding out the gain in repository size after running compaction: the old one, {{CompactionGainEstimate}} and the new one, {{SizeDeltaGcEstimation}}. Similarly, there are also two configurations for customising them, in {{SegmentNodeStoreService}}, {{compaction.gainThreshold}} and {{compaction.sizeDeltaEstimation}}.

At the moment both of them are exposed as OSGi configurations, but only the new one should be exposed (e.g. {{compaction.sizeDeltaEstimation}}). 

It must be evaluated whether it makes sense to keep the logic associated with the old implementation.",osgi-config,['segment-tar'],OAK,Improvement,Minor,2016-11-02 10:19:33,0
13017105,Support bundling of nodes present in version store,"Bundling logic would not work for node structures which are present in versionstore i.e. nodes stored under /jcr:system/jcr:versionStorage as the nodes there always have type {{nt:frozenNode}}. So any node structure which gets version would not get benefit of bundling

Currently bundling logic looks for {{jcr:primaryType}} and {{jcr:mixinTypes}} for determining type information. To support bundling for nodes stored in version store we should also look for 

* jcr:frozenPrimaryType
* jcr:frozenMixinTypes

These properties contains the type information of original node stored which got versioned",bundling,['documentmk'],OAK,Improvement,Major,2016-11-02 06:47:31,4
13016843,Improve caching of segments,"Various aspects of how Segment Tar caches segments could possibly improved. The current cache is a result of replacing the former ad-hoc cache with a proper one in OAK-3055. While the former was prone to contention under concurrent load the current cache is too oblivious about reads: read accesses are always served through {{SegmentId.segment}} and never actually hit the cache. This results in frequently accessed segments not to be seen as such by the cache and potentially being prematurely evicted. 

Possibly approaches to address this problem include: 
* Reinstantiating the cache we had pre OAK-3055 but making in fully concurrent. 
* Convey the information about read accesses to the current cache. 
* In either of the above cases avoid bulk segments from being placed into the cache. ",performance scalability,['segment-tar'],OAK,Improvement,Major,2016-11-01 09:35:34,3
13015847,Remove DocumentStore.update(),OAK-3018 removed the single production usage of DocumentStore.update(). I propose we remove the method to reduce maintenance.,technical_debt,"['documentmk', 'mongomk', 'rdbmk']",OAK,Task,Major,2016-10-27 14:41:54,1
13015762,Improve the usage of the SegmentWriter for compaction,"I would like to improve how the {{SegmentWriter}} is used for compaction. In particular I dislike how the {{SegmentBufferWriter}} needs to be looped into {{SegmentWriter.writeNode()}}.
Furthermore creating a {{SegmentWriter}} for offline compaction with its own cache (instead of using the caches we have) is a bit wasteful wrt. to memory. ",refactoring,['segment-tar'],OAK,Improvement,Major,2016-10-27 10:13:52,3
13015707,Standby test failures,"I've recently seen a couple of the standby tests fail. E.g. on Jenkins: https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/1245/

{noformat}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
	at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:122)
{noformat}

{noformat}
java.lang.AssertionError: expected: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }> but was: org.apache.jackrabbit.oak.segment.SegmentNodeState<{ checkpoints = { ... }, root = { ... } }>
	at org.apache.jackrabbit.oak.segment.standby.StandbyTestIT.testSyncLoop(StandbyTestIT.java:122)
{noformat}

{{org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT.testProxySkippedBytes}}:
{noformat}
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-27 07:27:57,2
13015240,Offline compaction explodes checkpoints ,"Running offline compaction on a repository with checkpoints will explode those into full copies. Observed e.g. with OAK-5001. 

I think we should consider improving this by compacting checkpoints on top of each other in the proper order ({{oak-upgrade}} does this successfully). 

[~alex.parvulescu], WDYT? What was our take on this in the previous Oak versions? ",compaction gc tooling,['segment-tar'],OAK,Improvement,Major,2016-10-25 21:30:07,3
13014980,Persistent cache should not cache those paths which are covered by DocumentNodeStateCache,With OAK-4180 its possible to use a SegmentNodeStore as secondary store and thus like a cache for certain set of path. In such kind of setup persistent cache should not cache those NodeStates which are covered by DocumentNodeStateCache,secondary-nodestore,['documentmk'],OAK,Improvement,Major,2016-10-25 04:50:53,4
13014773,SegmentBufferWriter should not depend on SegmentTracker,"The former depends on the latter only for generation sequence numbers of segments, which are subsequently used to generate the segment meta information. I suggest to replace that dependency with a generalised one. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-10-24 16:00:36,3
13014772,Simplify GCListener,We should simplify {{GCListener}} to minimise the boilerplate necessary in {{FileStoreBuilder}}. ,technical_debt,['segment-tar'],OAK,Improvement,Major,2016-10-24 15:56:02,3
13014670,Server time unavailable with authenticated connection to MongoDB,"The MongoDocumentStore gets the current server time with the {{serverStatus}} command. When MongoDB is configured with authentication, the command may fail because it requires the [clusterMonitor|https://docs.mongodb.com/manual/reference/built-in-roles/#clusterMonitor] role.

The method will then simply log a WARN message and assume no time difference. Maybe there is a different command we can use to get the time on the server?",resilience,"['core', 'mongomk']",OAK,Bug,Minor,2016-10-24 09:59:25,1
13014649,Config option to disable specific bundling config,"With OAK-4975 Oak would be shipping some default bundling config. An application might want to disable such bundling and for those cases we need to support some config option to disable bundling for specific nodetypes.

*Proposal*

Have a boolean property {{disabled}} on bundling config for specific nodetype to indication that this bundling config is not to be used
",bundling docs-impacting,['documentmk'],OAK,Improvement,Minor,2016-10-24 07:53:54,4
13014175,Enable configuring QueryEngineSettings via OSGi config,"Oak QueryEngine exposes few settings options via {{QueryEngineSettings}}. Currently they can be configured via

# System properties
# JMX - The settings are not persistent 

We should have a way to configure them via OSGi also. A simple option can be to have a OSGi component which obtains a reference to {{QueryEngineSettingsMBean}} and then modifies the config upon activation",docs-impacting,['query'],OAK,Improvement,Minor,2016-10-21 09:26:39,4
13013781,Test failure: SegmentDataStoreBlobGCIT,"SegmentDataStoreBlobGCIT seems to crash the JVM on Java 7. Following is the relevant part of the build output.

{noformat}
[INFO] --- maven-failsafe-plugin:2.19.1:integration-test (default) @ oak-segment-tar ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.oak.segment.file.FileStoreIT
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.301 sec - in org.apache.jackrabbit.oak.segment.file.FileStoreIT
Running org.apache.jackrabbit.oak.segment.file.SegmentReferenceLimitTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec - in org.apache.jackrabbit.oak.segment.file.SegmentReferenceLimitTestIT
Running org.apache.jackrabbit.oak.segment.file.LargeNumberOfPropertiesTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.001 sec - in org.apache.jackrabbit.oak.segment.file.LargeNumberOfPropertiesTestIT
Running org.apache.jackrabbit.oak.segment.SegmentOverflowExceptionIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0 sec - in org.apache.jackrabbit.oak.segment.SegmentOverflowExceptionIT
Running org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 45.78 sec - in org.apache.jackrabbit.oak.segment.standby.ExternalPrivateStoreIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverSslTestIT
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.202 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverSslTestIT
Running org.apache.jackrabbit.oak.segment.standby.BrokenNetworkIT
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 63.024 sec - in org.apache.jackrabbit.oak.segment.standby.BrokenNetworkIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverMultipleClientsTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.052 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverMultipleClientsTestIT
Running org.apache.jackrabbit.oak.segment.standby.MBeanIT
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.287 sec - in org.apache.jackrabbit.oak.segment.standby.MBeanIT
Running org.apache.jackrabbit.oak.segment.standby.FailoverIPRangeIT
Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.691 sec - in org.apache.jackrabbit.oak.segment.standby.FailoverIPRangeIT
Running org.apache.jackrabbit.oak.segment.standby.StandbyTestIT
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.303 sec - in org.apache.jackrabbit.oak.segment.standby.StandbyTestIT
Running org.apache.jackrabbit.oak.segment.standby.RecoverTestIT
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.415 sec - in org.apache.jackrabbit.oak.segment.standby.RecoverTestIT
Running org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.002 sec - in org.apache.jackrabbit.oak.segment.standby.ExternalSharedStoreIT
Running org.apache.jackrabbit.oak.segment.SegmentDataStoreBlobGCIT

Results :

Tests run: 65, Failures: 0, Errors: 0, Skipped: 3

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10:17 min
[INFO] Finished at: 2016-10-19T20:45:40+00:00
[INFO] Final Memory: 63M/553M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:2.19.1:integration-test (default) on project oak-segment-tar: Execution default of goal org.apache.maven.plugins:maven-failsafe-plugin:2.19.1:integration-test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /apps/jenkins/workspace/oak-segment-tar && /opt/jdk-7/jre/bin/java -Xmx512m -XX:MaxPermSize=64m -XX:+HeapDumpOnOutOfMemoryError -Dupdate.limit=100 -Djava.awt.headless=true -jar /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefirebooter4283069132546797078.jar /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefire8963659563100379656tmp /apps/jenkins/workspace/oak-segment-tar/target/surefire/surefire_03767892930481742588tmp
{noformat}",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-20 08:02:49,2
13013555,Standalone tooling for segment tar,"Currently there is no easy way to run the tools provided by {{oak-run}} against a snapshot version of Segment Tar. In order to have better CI coverage (e.g. benchmarks) of Segment Tar, we need to introduce a way for running such tools independently of {{oak-run}}. Eventually {{oak-run}} should even be using that tooling front-end instead of directly depending on {{oak-segment-tar}}. ",tooling,['segment-tar'],OAK,New Feature,Major,2016-10-19 14:52:47,3
13013458,Review the support for wildcards in bundling pattern,"Bundling pattern currently supports wild card pattern. This makes it powerful but at same time can cause issue if it misconfigured. 

We should review this aspect before 1.6 release to determine if this feature needs to be exposed or not. ",bundling,['documentmk'],OAK,Task,Major,2016-10-19 10:01:25,4
13013457,Review the security aspect of bundling configuration,"The config for node bundling feature in DocumentNodeStore is currently stored under {{jcr:system/rep:documentStore/bundlor}}. This task is meant to 

* Review the access control aspect - This config should be only updatetable by system admin
* Config under here should be writeable via JCR api",bundling,['documentmk'],OAK,Task,Major,2016-10-19 09:59:05,4
13013452,Test failure: BrokenNetworkTest,"On some machines the {{BrokenNetworkTest}} fails:

{noformat}
Failed tests:
  BrokenNetworkTest.testProxyFlippedEndByteSSL:103->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedIntermediateByte:88->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedIntermediateByteSSL:93->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxyFlippedStartByte:78->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySSLSkippedBytes:63->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySSLSkippedBytesIntermediateChange:73->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
  BrokenNetworkTest.testProxySkippedBytesIntermediateChange:68->useProxy:113->useProxy:146 expected:<{ root = { ... } }> but was:<{ root : { } }>
{noformat}

Stack traces are all similar to 
{noformat}
testProxySkippedBytesIntermediateChange(org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest)  Time elapsed: 5.577 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.useProxy(BrokenNetworkTest.java:146)
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.useProxy(BrokenNetworkTest.java:113)
	at org.apache.jackrabbit.oak.segment.standby.BrokenNetworkTest.testProxySkippedBytesIntermediateChange(BrokenNetworkTest.java:68)
{noformat}",test-failure,['segment-tar'],OAK,Bug,Major,2016-10-19 09:40:37,2
13013446,SegmentRevisionGC MBean should report more detailed gc status information  ,"Regarding this, the current ""Status"" is showing the last log info. This is useful, but it would also be interesting to expose the real-time status. For monitoring it would be useful to know exactly in which phase we are, e.g. a field showing on of the following:
- idle
- estimation
- compaction
- compaction-retry-1
- compaction-retry-2
- compaction-forcecompact
- cleanup


",gc monitoring,['segment-tar'],OAK,Improvement,Major,2016-10-19 09:09:55,2
13013225,SetPropertyTest benchmark fails on Segment Tar,"The {{SetPropertyTest}} fails on Oak Segment Tar:

{noformat}
javax.jcr.InvalidItemStateException: This item [/testfb3e8f1a/ca1ef350-f650-4466-b9e3-7f77d83e6303] does not exist anymore
	at org.apache.jackrabbit.oak.jcr.delegate.ItemDelegate.checkAlive(ItemDelegate.java:86)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl$ItemWriteOperation.checkPreconditions(ItemImpl.java:96)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl$35.checkPreconditions(NodeImpl.java:1366)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.prePerform(SessionDelegate.java:615)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:205)
	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:112)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.internalSetProperty(NodeImpl.java:1363)
	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.setProperty(NodeImpl.java:506)
	at org.apache.jackrabbit.oak.benchmark.SetPropertyTest.runTest(SetPropertyTest.java:65)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.execute(AbstractTest.java:372)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.runTest(AbstractTest.java:221)
	at org.apache.jackrabbit.oak.benchmark.AbstractTest.run(AbstractTest.java:197)
	at org.apache.jackrabbit.oak.benchmark.BenchmarkRunner.main(BenchmarkRunner.java:456)
	at org.apache.jackrabbit.oak.run.BenchmarkCommand.execute(BenchmarkCommand.java:26)
	at org.apache.jackrabbit.oak.run.Mode.execute(Mode.java:63)
	at org.apache.jackrabbit.oak.run.Main.main(Main.java:49)
{noformat}
",test-failure,['segment-tar'],OAK,Bug,Minor,2016-10-18 15:29:59,0
13013213,Optimise MutableRecordNumbers,{{MutableRecordNumbers}} is based on a map. This turns to be expensive as the items in the map are accessed very frequently. I would like to look into way of optimising this using a linear storage model instead. ,performance,['segment-tar'],OAK,Improvement,Major,2016-10-18 14:37:26,3
13013126,SegmentWriter buffers child node list changes,"The {{SegmentWriter}} currently buffers the list of child nodes changed on a nodestate update [0] (new node or updated node). This can be problematic in a scenario where there are a large number of children added to a node (ie. unique index size seen to spike above {{10MM}} in one case).

To have a reference for the impact of this, at the {{SegmentWriter}} level, for a list of map entries of almost {{3MM}} items, I saw it take up around {{245MB}} heap.

This issue serves to track a possible improvement here in how we handle this update scenario.




[0] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-segment-tar/src/main/java/org/apache/jackrabbit/oak/segment/SegmentWriter.java#L516",performance,['segment-tar'],OAK,Improvement,Major,2016-10-18 10:27:28,3
13012799,Provide status for gc process ,This is the {{oak-segment-tar}} side of OAK-4919,gc monitoring,['segment-tar'],OAK,Technical task,Major,2016-10-17 09:29:18,3
13012756,Isolate corrupted index and make async indexer more resilient,"Currently if any one of the async index gets corrupted it brings down the whole async indexer and no other index gets updated untill system administrator reindexes the problamatic async index. 

Instead of fail all we should isolate such corrupted index and mark them as corrupted. And still let async indexer progress for other working indexes. 

This would ensure that one corrupted index does not affect the whole system and allow the application to work partially. 

Feature branch - https://github.com/chetanmeh/jackrabbit-oak/compare/trunk...chetanmeh:OAK-4939?expand=1",docs-impacting,"['lucene', 'query']",OAK,Improvement,Major,2016-10-17 04:31:46,4
13012046,Too many segment cache misses,"Running the {{ConcurrentWriteTest}} benchmark and monitoring the hits and misses of the segment cache (LIRS), I noticed that some segments are loaded over and over again (up to 3000 times). ",performance,['segment-tar'],OAK,Bug,Major,2016-10-13 16:18:45,3
13011274,SegmentS3DataStoreStatsTest failing,"The tests are currently failing with 

{noformat}
ava.lang.RuntimeException: Unable to invoke method 'activate' for class org.apache.jackrabbit.oak.segment.SegmentNodeStoreService

	at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.invokeMethod(OsgiServiceUtil.java:262)
	at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.activateDeactivate(OsgiServiceUtil.java:86)
	at org.apache.sling.testing.mock.osgi.MockOsgi.activate(MockOsgi.java:162)
	at org.apache.sling.testing.mock.osgi.MockOsgi.activate(MockOsgi.java:173)
	at org.apache.sling.testing.mock.osgi.context.OsgiContextImpl.registerInjectActivateService(OsgiContextImpl.java:142)
	at org.apache.jackrabbit.oak.segment.SegmentS3DataStoreStatsTest.registerSegmentNodeStoreService(SegmentS3DataStoreStatsTest.java:113)
	at org.apache.jackrabbit.oak.segment.SegmentS3DataStoreStatsTest.testUseS3BlobStore(SegmentS3DataStoreStatsTest.java:74)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:239)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:117)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:43)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:239)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
Caused by: java.lang.NoSuchMethodError: org.apache.jackrabbit.oak.spi.state.RevisionGC.<init>(Ljava/lang/Runnable;Ljava/util/concurrent/Executor;)V
	at org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.registerSegmentStore(SegmentNodeStoreService.java:471)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.registerNodeStore(SegmentNodeStoreService.java:339)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStoreService.activate(SegmentNodeStoreService.java:304)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.invokeMethod(OsgiServiceUtil.java:253)
	... 38 more
{noformat}

Most likely our changes in OAK-4835 caused this regression. ",regression test-failure,['it'],OAK,Bug,Major,2016-10-11 07:22:28,3
13011087,Better feedback from method invocations on RevisionGCMBean,"The methods to invoke and cancel revision gc return void. This is by design as those calls are asynchronous. The idea is that {{RevisionGC.getRevisionGCStatus()}} would return the current status of an ongoing gc operation. However, currently that method only returns the status of the asynchronous task that was fired off. It should instead be able to convey back the real status of the underlying operation. ",monitoring,['core'],OAK,Improvement,Major,2016-10-10 15:59:23,3
13011031,Interrupt online revision cleanup on documentmk,Sub task of OAK-4835 for the {{document}} specific changes,management,"['core', 'documentmk']",OAK,Technical task,Major,2016-10-10 13:20:11,1
13011027,Interrupt online revision cleanup on segment-tar,Sub task of OAK-4835 for the {{segment-tar}} specific changes,management,['segment-tar'],OAK,Technical task,Major,2016-10-10 13:13:38,3
13010103,Enable persistent caches by default,The diff persistent cache is important for efficient processing of external changes and should be enabled by default.,observation,"['core', 'documentmk']",OAK,Improvement,Minor,2016-10-06 08:48:58,1
13010089,Include option to stop GC in RevisionGCMBean and RepositoryManagementMBean,"With OAK-4765 Oak Segment Tar acquired the capability for stopping a running revision gc task. This is currently exposed via {{SegmentRevisionGCMBean.stopCompaction}}. I think it would make to expose this functionality through {{RevisionGCMBean}} and {{RepositoryManagementMBean}} also/instead. 

[~mreutegg], [~alex.parvulescu] WDYT? Could the document node store also implement this or would we just not support it there? ",management production resilience,['core'],OAK,Improvement,Major,2016-10-06 07:57:14,3
13009902,FileStore cleanup should not leak out file handles,"{{FileStore.cleanup()}} currently returns a list of {{File}} instances relying on the caller to remove those files. This breaks encapsulation as the file store is the sole owner of these files and only the file store should be removing them.

I suggest to replace the current cleanup method with one that returns {{void}}. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-10-05 15:42:41,3
13009826,Document conflict handling,We should add documentation how Oak deals with conflicts. This was once documented in the Javadocs of {{MicroKernel.rebase()}} but got lost along with that class. Note that OAK-1553 refines conflict handling but this refinement has not been implemented in all backends yet. ,documentation,['doc'],OAK,Task,Major,2016-10-05 09:56:31,3
13009491,Missing BlobGarbageCollection MBean on standby instance,"The {{BlobGarbageCollection}} MBean is no longer available on a standby instance, this affects non-shared datastore setups (on a shared datastore you'd only need to run blob gc on the primary).
This change was introduced by OAK-4089 (and backported to 1.4 branch with OAK-4093).",candidate_oak_1_4 gc,"['segment-tar', 'segmentmk']",OAK,Bug,Major,2016-10-04 10:24:00,2
13009466,Make merge semaphore in SegmentNodeStore fair by default,"Currently the merge semaphore in SegmentNodeStore is by default non fair. OAK-3588 provided a config option to make it fair.

We should change the default to fair so as to ensure writer threads never get starved. 

Eventually this change would need to be backported to branches. Further going forward OAK-4122 would replace the lock",candidate_oak_1_4,"['segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-10-04 08:02:33,4
13009230,Remove logging of cleaned segment id on cleanup,"With OAK-2404 we started logging all segment ids that a cleanup cycle removes. While this is useful for post mortems in the case of a {{SNFE}}, it also increases log files by many megabytes. 

Since OAK-2405 added some additional gc information, which is logged along with the missing segment in the case of a {{SNFE}}, I think the logging of the cleaned segment ids is not superfluous and we should remove it. ",gc logging monitoring,['segment-tar'],OAK,Improvement,Major,2016-10-03 13:24:28,3
13009176,Avoid running GC too frequently,"As {{RevisionGCMBean.startRevisionGC()}} can be used to manually invoke a gc cycle, there is the danger of running into a {{SNFE}} when gc is run multiple times in quick succession (due to the retention time being based on number of generations). We should come up with a mechanism to prevent this scenario. ",compaction gc,['segment-tar'],OAK,Improvement,Blocker,2016-10-03 07:26:29,3
13008676,Avoid queries for first level previous documents during GC,Revision GC in a first phase finds all documents for deleted nodes and their previous documents. Reading the previous documents can be avoided in some cases. The ids of first level previous documents can be derived directly from the previous map in the main document.,candidate_oak_1_4 performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-29 19:06:27,1
13008560,Test failure: HeavyWriteIT,"Said test sometimes fails with the following stack trace:

{noformat}
09:46:40.900 ERROR [main] SegmentId.java:127                Segment not found: 8399230c-9338-47e3-acf5-b92d326cf171. SegmentId age=7473ms,gc-count=32,gc-status=success,store-generation=29,reclaim-predicate=(generation<=27),segment-generation27
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 8399230c-9338-47e3-acf5-b92d326cf171 not found
at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1345) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1285) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1013) ~[oak-core-1.5.8.jar:1.5.8]
at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:974) ~[oak-core-1.5.8.jar:1.5.8]
at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:285) ~[oak-core-1.5.8.jar:1.5.8]
at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:92) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1285) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:123) ~[oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.Record.getSegment(Record.java:70) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeState.getStableIdBytes(SegmentNodeState.java:139) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeState.getStableId(SegmentNodeState.java:122) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeState.fastEquals(SegmentNodeState.java:633) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:604) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:265) [oak-segment-tar-0.0.13-SNAPSHOT.jar:na]
at org.apache.jackrabbit.oak.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:85) [test-classes/:na]
{noformat}

This is a problem with the test, not a regression:

{noformat}
Segment not found: 8399230c-9338-47e3-acf5-b92d326cf171. SegmentId age=7473ms,gc-count=32,gc-status=success,store-generation=29,reclaim-predicate=(generation<=27),segment-generation27
{noformat}

This means the missing segment was successfully gc'ed at GC #32. Its generation was 27 while the store just got bumped to generation 29. This causes a cleanup of all generations <= 27. 

The test itself calls {{FileStore.gc()}} in quick succession while at the same time writing to the store. This is likely to at some point cause a write to be based on an already collected segment. I suggest to fix this by increasing the number of retained generations to a sufficiently high value (for this test). 

On a side node, this issue (and being able to to a root cause analysis) validates the additional logging that we added with OAK-2405! 
",test,['segment-tar'],OAK,Bug,Minor,2016-09-29 14:57:21,3
13008550,Reduce query batch size for deleted documents,"MongoVersionGCSupport uses the default batchSize when it queries for possibly deleted documents. The default will initially read 100 documents and then as many as fit into a 4MB response. Depending on the document size a couple of thousand will fit in there and take time to process. It may happen that the MongoDB cursor then times out and the VersionGC fails.

An easy and safe solution is to reduce the batch size to a given number of documents.",resilience,['mongomk'],OAK,Improvement,Minor,2016-09-29 14:30:59,1
13006824,Analyse effects of simplified record ids,"OAK-4631 introduced a simplified serialisation for record ids. This causes their footprint on disk to increase from 3 bytes to 18 bytes. OAK-4631 has some initial analysis on the effect this is having on repositories as a whole. 

I'm opening this issue as a dedicated task to further look into mitigation strategies (if necessary). ",performance,['segment-tar'],OAK,Improvement,Major,2016-09-22 11:51:36,3
13006821,Missing export for org.apache.jackrabbit.oak.backup package,"Oak segment tar does not export {{org.apache.jackrabbit.oak.backup}}, which makes backup restore functionality unavailable from OSGi containers. 

Instead of just exporting this package I think we should:
* Separate API from implementation
* Add semantic versioning to the exported API

/cc [~frm]",API OSGi,['segment-tar'],OAK,Bug,Major,2016-09-22 11:44:29,2
13006493,Provide generic option to interrupt online revision cleanup,JMX binding for stopping a running compaction process,compaction gc management monitoring,['core'],OAK,Improvement,Major,2016-09-21 13:18:24,3
13006454,Document storage format changes,"This issue serves as collection of all changes to the storage format introduced with  Oak Segment Tar and their impact. Once sufficiently stabilised this information should serve as basis for the documentation in {{oak-doc}}. 

|| Change || Rational || Impact || Migration || Since || Issues ||
|Generation in segment header |Required to unequivocally determine the generation of a segment during cleanup. Segment retention time is given in number of generations (2 by default). |No performance, space impact expected |offline |0.0.2 |OAK-3348 | 
|Stable id for node states |Required to efficiently determine equality of node states. This can be seen as an intermediate step to decoupling the address of records from their identity. The next step is to introduce logical record ids (OAK-4659). |Node states increase by the size of one record id (3 bytes / 20 bytes after OAK-4631). On top of that there is an additional block record à 18 bytes per node state. |offline |0.0.2 |OAK-3348
|Binary index in tar files |Avoid traversing the repository to collect the gc roots for DSGC. Fetch them from an index instead. |Additional index entry per tar file. Adds a couple of bytes per external binary to each tar file. Exact size to be determined. [~frm] could you help with this? OAK-4740 is a regression wrt. to resiliency caused by this change (and the fact that the blob store might return blob ids longer than 2k chars).  |offline |0.0.4 |OAK-4101
|Simplified record ids |Preparation and precondition for logical record ids (OAK-4659). At the same time the simplest possible fix for OAK-2896. The latter leads to degeneration of segment sizes, which in turn has adverse effects on overall performance, resource utilisation and memory requirements. Without this fix OAK-2498 would need to be fixed in a different way that would require other changes in the storage format. I started to regard this issue as removing a premature optimisation (which caused OAK-2498). OTOH with OAK-4844 we should also start looking into mitigations and what those would mean to size vs. simplicity vs. performance.  |Record ids grow from 3 bytes to 18 bytes when serialised into records. Impact on repositories to be assessed but can be anywhere between almost none to x6. OAK-4812 is a performance regression caused by this chance. Its overall impact is yet to be assessed. |offline |0.0.10 |OAK-4631, OAK-4844
|Storage format versioning |In order to be able to further evolve the storage format with minimal impact on existing deployments we need to carefully versions the various storage entities (segments, tar files, etc.) |No performance, space impact expected |offline |0.0.2/ 0.0.10 |OAK-4232, OAK-4683, OAK-4295
|Logical record ids |We need to separate addresses of records from their identity to be able to further scale the TarMK. OAK-3348 (the online compaction misery) can be seen as a symptom of failing to understand this earlier. The stable ids introduced with OAK-3348 are a first step into this direction. However this is not sufficient to implement features like e.g. background compaction (OAK-4756), partial compaction (OAK-3349) or incremental compaction (OAK-3350).  |A small size overhead per segment for the logical id table. Further impact to be evaluated ([~frm], please add your assessment here). |offline |0.0.14 (planned) |OAK-4659
|External index for segments |Avoid recreating tar files if indexes are corrupt/missing. Just recreate the indexes. |Faster startup after a crash. Overall less disk space usage as no unnecessary backup files are created. |online |not yet planned |OAK-4649
|In-place journal |Reduce complexity by in-lining the journal log. Less files, less chances to break something. Also the granularity of the log would increase as flushing of the persisted head would not be required any more. Resilience would improve as the roll-back functionality could operate at a finer granularity. |No more journal.log. Better resiliency. Significant risk for regression of OAK-4291 if not implemented properly. Most likely a significant refactoring of some parts of the code is required before we can proceed with this issue.  |online |not yet planned |OAK-4103
|Root record types |With the information currently available from the segment headers we cannot collect statistics about segment usage on repositories of non trivial sizes. This fix would allow us to build more scalable tools to that respect.  |None expected wrt. to performance and size under normal operation. |offline |0.0.14 (planned) (waiting for OAK-4659 as implementation depends on how we progress there) |OAK-2498

Misc ideas currently on the back burner:
* SegmentMK: Arch segments (OAK-1905)
* Extension headers for segments (no issue yet)
* More memory efficient serialisation of values (e.g. boolean) (no issue yet)
* Protocol Buffer for serialising records (no issue yet)

",documentation,"['doc', 'segment-tar']",OAK,Technical task,Major,2016-09-21 10:26:36,2
13006181,Use optional resolution for optional dependencies,OAK-4775 upgraded Netty to 4.0.41.Final. This version seem to bring a couple of optional dependencies that we should also specify as optional in the Import-Package clause. ,OSGi,['segment-tar'],OAK,Improvement,Major,2016-09-20 13:18:23,3
13005873,Improve revision GC resilience,Revision GC may currently fail when a document with a malformed id is read from the DocumentStore. E.g. a document stored accidentally in the nodes collection or malformed for some other reason.,resilience,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-19 10:57:25,1
13005064,Remove usage of Tree in LuceneIndexEditor,"{{LuceneIndexEditor}} currently creates 2 tree instances for determining IndexRule. [~ianeboston] highlighted this on list [1] and this is something which we should avoid and remove usage of Tree api

This was earlier done so as to simplify future support for conditional rules (OAK-2281) which might need access to ancestor which is not possible with NodeState api.  As that is not going to be done so we can get rid of Tree construction in the editor.

[1] https://lists.apache.org/thread.html/7d51b45296f5801c3b510a30a4847ce297707fb4e0d4c2cefe19be62@%3Coak-dev.jackrabbit.apache.org%3E
",performance,['lucene'],OAK,Improvement,Minor,2016-09-15 04:19:03,4
13004849,Basic cache consistency test on exception,"OAK-4774 and OAK-4793 aim to check if the cache behaviour of a DocumentStore implementation when the underlying backend throws an exception even though the operation succeeded. E.g. the response cannot be sent back because of a network issue.

This issue will provide the DocumentStore independent part of those tests.",resilience,"['core', 'documentmk']",OAK,Test,Minor,2016-09-14 11:47:13,1
13004315,Optimise stable ids ,"Currently {{SegmentNodeState#getStableId()}} returns a string with all its associated overhead:
* high memory requirements (42 characters plus the overhead of a {{String}} instance. The raw requirements are a mere 20 bytes (long msb, long lsb, int offset). The memory overhead is problematic as the stable id is used as key in the node deduplication cache (See OAK-4635).
* high serialisation cost. I have seen {{getStableId()}} occurring in stack traces. This is to be expected as that method is called quite often when comparing node states. 

This issue is to explore options for reducing both CPU and memory overhead of stable ids. ",memory performance,['segment-tar'],OAK,Improvement,Major,2016-09-12 14:51:00,3
13004306,Node writer statistics is skewed,The statistics about writing nodes collected by the {{SegmentWriter}} instances is a bit off. This was caused by the changes introduced with OAK-4570. Starting with these changes also base node states of a node being written are de-duplicated. Collecting the node writer stats does not differentiate however e.g. the cache hits/misses between deduplication for the base state or the actual state being written. ,monitoring,['segment-tar'],OAK,Bug,Major,2016-09-12 14:00:13,3
13003584,ClusterNodeInfo may renew lease while recovery is running,ClusterNodeInfo.renewLease() does not detect when it is being recovered by another cluster node.,resilience,"['core', 'documentmk']",OAK,Bug,Major,2016-09-08 13:51:31,1
13003477,Check usage of DocumentStoreException in MongoDocumentStore,"With OAK-4771 the usage of DocumentStoreException was clarified in the DocumentStore interface. The purpose of this task is to check usage of the DocumentStoreException in MongoDocumentStore and make sure MongoDB Java driver specific exceptions are handled consistently and wrapped in a DocumentStoreException. At the same time, cache consistency needs to be checked as well in case of a driver exception. E.g. invalidate if necessary.",resilience,"['core', 'mongomk']",OAK,Task,Minor,2016-09-08 07:30:14,1
13003229,Clarify exceptions in DocumentStore,"The current DocumentStore contract is rather vague about exceptions. The class JavaDoc mentions implementation specific runtime exceptions, but does not talk about the DocumentStoreException used by all the DocumentStore implementations. We should make this explicit in all relevant methods.",resilience,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-07 13:12:10,1
13003226,Missing exception handling in ClusterNodeInfo.renewLease(),ClusterNodeInfo.renewLease() does not handle a potential DocumentStoreException on {{findAndModify()}}. This may leave {{previousLeaseEndTime}} in an inconsistent state and a subsequent {{renewLease()}} call then considers the lease timed out. ,resilience,"['core', 'documentmk']",OAK,Bug,Major,2016-09-07 13:04:05,1
13002851,Adjust default timeout values for MongoDocumentStore,"Some default values timeouts of the MongoDB Java driver do not work well with the lease time we use in Oak.

Per default there is no socket timeout set and the driver waits for a new connection up to 120 seconds, which is too log for lease update operations. 

See also OAK-4739.",resilience,"['core', 'mongomk']",OAK,Improvement,Minor,2016-09-06 08:09:13,1
13002746,A parallel approach to garbage collection,"Assuming that:

# Logic record IDs are implemented.
# TAR files are ordered in reverse chronological order.
# When reading segments, TAR files are consulted in order.
# Segments in recent TAR files shadow segments in older TAR files with the same segment ID.

A new algorithm for garbage collection can be implemented:

# Define the input for the garbage collection process. The input consists of the current set of TAR files and a set of record IDs representing the GC roots.
# Traverse the GC roots and mark the records that are still in use. The mark phase traverses the record graph and produces a list of record IDs. These record IDs are referenced directly or indirectly by the given set of GC roots and need to be kept. The list of record IDs is ordered by segment ID first and record number next. This way, it is possible to process this list in one pass and figure out which segment and which record should be saved at the end of the garbage collection.
# Remove unused records from segments and rewrite them in a new set of TAR files. The list is produced in the previous step is traversed. For each segment encountered, a new segment is created containing only the records that were marked in the previous phase. This segment is then saved in a new set of TAR files. The set of new TAR files is the result of the garbage collection process. 
# Add the new TAR files to the system. The system will append the new TAR files to the segment store. The segments in these TAR files will shadow the ones in older TAR files.
# Remove TAR files from the old generation. It is safe to do so because the new set of TAR files are currently shadowing the initial set of TAR files.

While the garbage collection process is running, the system can work as usual by starting a fresh TAR file. The result of the garbage collection is made visible atomically only at the end, when the new TAR files are integrated into the running system.",gc scalability,['segment-tar'],OAK,New Feature,Major,2016-09-05 15:08:09,2
13002722,Leaderboard in ConsolidatedListenerMBean,"ConsolidatedListenerMBean contains various stats about JCR event listeners. However, it is rather difficult to get an overview of how expensive listeners are.

The MBean should expose a simple leaderboard that orders the listener according to the processing time (producer & consumer time).",observation,['jcr'],OAK,Improvement,Minor,2016-09-05 12:30:56,1
13002681,Include initial cost in stats for observation processing,"The jackrabbit-jcr-commons {{ListenerTracker}} collects timing for JCR event listeners. It tracks producer (oak internal) and consumer (JCR EventListener) time. The initial producer cost is currently not reflected in these stats, because {{ChangeProcessor}} in oak-jcr does an initial {{hasNext()}} on the {{EventIterator}} outside of the {{ListenerTracker}}. For some listeners this initial producer time may even account for the entire cost when the event filter rejects all changes.",observation,['jcr'],OAK,Improvement,Minor,2016-09-05 07:58:40,1
13002069,Improve FileStoreStatsMBean,"We should add further data to that MBean (if feasible):

* Number of commits
* Number of commits queuing (blocked on the commit semaphore)
* Percentiles of commit times (exclude queueing time)
* Percentiles of commit queueing times 
* Last gc run / size before gc and after gc / time gc took broken down into the various phases

",monitoring,['segment-tar'],OAK,Improvement,Major,2016-09-01 14:27:20,2
13001917,Reduce DocumentStore reads for local changes (2),"There is another case where the local cache is incorrectly updated, which leads to unnecessary reads from the DocumentStore. See also OAK-4715.",performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-09-01 07:06:43,1
13001743,(Slightly) prioritise reads over writes ,"When fetching the current root from the {{SegmentNodeStore}} an older revision will be returned when a commit is being processed concurrently. I think it would make sense to wait for a short time in this case increasing the chance of returning an up to date state. The idea is that this would lower the rebasing work that need to be done later on should the returned root be used for further modifications. 

An interesting value for the wait time is to use  the median (or more general a percentile) of the commit time of the last say 1000 commits. This would mean that (for the median) we have a 50% chance of getting up to date date. For a 90% percentile we would have longer wait times but then a 90% chance of getting up to date date. ",Performance scalability,['segment-tar'],OAK,Improvement,Minor,2016-08-31 16:23:34,0
13001737,Refine forced compaction,"Forced compaction currently acquires an exclusive write lock on the repository blocking all concurrent commits during the complete time it needs to finish compaction. I think we should refine this:

* Add a time out so we could limit the time during which the repository does not accept writes while still giving compaction another chance to finish.

* Boost the compaction threads priority. This could actually already be done during the regular compaction cycles to increase the changes to finish in time. 

",compaction gc,['segment-tar'],OAK,Improvement,Major,2016-08-31 16:16:04,3
13001638,Oak Segment Tar tests should not check node store fixture,Checking the node store fixture {{commons.FixturesHelper#getFixtures()}} is a left over from when the segment node store was part of {{oak-core}} and we wanted to avoid running the tests multiple times. As we are now in a separate module this check is not necessary any more. It is currently even harmful as certain tests are skipped. The default value for the fixtures is still {{SEGMENT_MK}} because {{oak-segment-tar}} cannot yet depend on Oak 1.5.9 (not released yet) where the default was switched to {{SEGMENT_TAR}} (see OAK-4706).,tests,['segment-tar'],OAK,Improvement,Major,2016-08-31 12:39:27,3
13001354,Prefetch external changes,"In a cluster with listeners that are registered to receive external changes, pulling in external changes can become a bottleneck. While processing those external changes, further local changes are put into the observation queue leading to a system where the queue eventually fills up.

Instead of processing external changes one after another, the implementation could prefetch them as they come in and if needed pull them in parallel.",observation,"['core', 'documentmk']",OAK,Improvement,Major,2016-08-30 15:12:41,1
13001317,Optimize PathRev as/from String,PathRev instances are used as keys for various cache entries and asString() / fromString() methods are called frequently when the persistent cache is enabled.,performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-08-30 13:59:18,1
13000913, Reduce DocumentStore reads for local changes,"The observation test added for OAK-4528 shows significant time spent in reading documents from the store when local changes are processed. Since those changes were done on the local cluster node, they should be served from cache and not reach out to the underlying store.",observation performance,"['core', 'documentmk']",OAK,Improvement,Minor,2016-08-29 10:06:52,1
13000012,Run tests against SEGMENT_TAR fixture,Oak's integration tests still run against the {{SEGMENT_MK}} fixture. I suggest we switch to the {{SEGMENT_TAR}} fixture. ,testing,"['parent', 'segment-tar']",OAK,Task,Major,2016-08-25 13:29:13,3
12999637,Optimize read of old node state,"Reading a node state with an old revision from a document can be expensive when many changes happened on a property in the meantime.

A typical stack trace looks like this:

{noformat}
	at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getPreviousDocument(NodeDocument.java:1337)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$1.apply(PropertyHistory.java:70)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$1.apply(PropertyHistory.java:63)
	at com.google.common.collect.Iterators$8.transform(Iterators.java:794)
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:646)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at com.google.common.collect.Iterators$PeekingImpl.hasNext(Iterators.java:1139)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$2.refillQueue(PropertyHistory.java:121)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$2.computeNext(PropertyHistory.java:96)
	at org.apache.jackrabbit.oak.plugins.document.PropertyHistory$2.computeNext(PropertyHistory.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.jackrabbit.oak.plugins.document.ValueMap$1$3.nextIterator(ValueMap.java:105)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:98)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.next(MergeSortedIterators.java:85)
	at com.google.common.collect.Iterators$PeekingImpl.peek(Iterators.java:1162)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.adjustFirst(MergeSortedIterators.java:117)
	at org.apache.jackrabbit.oak.plugins.document.util.MergeSortedIterators.next(MergeSortedIterators.java:78)
	at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getLatestValue(NodeDocument.java:1972)
	at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:990)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:1079)
{noformat}

The read operation goes through the property history until it finds the most recent change. The old the read revision, the more changes are scanned.",observation performance,"['core', 'documentmk']",OAK,Improvement,Major,2016-08-24 11:43:48,1
12998462,Avoid concurrent calls to FileStore.cleanup() and FileStore.compact(),At the moment it is possible to have concurrent calls to {{FileStore.cleanup}} and to {{FileStore.compact()}}. The former is called from the latter and also from {{FileStore.flush()}} (this is tracked in OAK-4138). We should change this status quo and also make the calls to {{compact()}} and {{cleanup()}} mutually exclusive.,cleanup gc,"['segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-08-19 11:21:10,3
12997573,SNFE thrown while testing FileStore.cleanup() running concurrently with writes,"{{SegmentNotFoundException}} is thrown from time to time in the following scenario: plenty of concurrent writes (each creating a {{625 bytes}} blob) interrupted by a cleanup. 

Stack trace (including some debugging statements added by me):
{code:java}
Pre cleanup readers: []
Before cleanup readers: [/Users/dulceanu/work/test-repo/data00000a.tar]
Initial size: 357.4 kB
After cleanup readers: [/Users/dulceanu/work/test-repo/data00000a.tar]
After cleanup size: 357.4 kB
Final size: 361.0 kB
Exception in thread ""pool-5-thread-74"" org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Cannot copy record from a generation that has been gc'ed already
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1207)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:1096)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:1013)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:1013)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:987)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.access$700(SegmentWriter.java:379)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$8.execute(SegmentWriter.java:337)
	at org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.execute(SegmentBufferWriterPool.java:105)
	at org.apache.jackrabbit.oak.segment.SegmentWriter.writeNode(SegmentWriter.java:334)
	at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:111)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:550)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:571)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:627)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:287)
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT$1.run(CompactionAndCleanupIT.java:961)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 4fb637cc-5013-4925-ab13-0629c4406481 not found
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1341)
	at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:123)
	at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:94)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1199)
	... 18 more
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Invalid segment format. Dumping segment 4fb637cc-5013-4925-ab13-0629c4406481
00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000040 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000090 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000C0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000D0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000120 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000130 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000140 39 37 39 31 31 36 30 38 2D 63 31 63 65 2D 34 62 97911608-c1ce-4b
00000150 35 63 2D 61 36 33 37 2D 39 36 61 65 39 34 38 38 5c-a637-96ae9488
00000160 61 37 65 38 2E 30 61 62 34 30 36 38 36 00 00 00 a7e8.0ab40686...
00000170 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000180 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000190 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001A0 00 00 00 00 30 30 30 30 34 30 30 00 30 30 30 30 ....0000400.0000
000001B0 30 30 30 00 30 30 30 30 30 30 30 00 30 30 30 30 000.0000000.0000
000001C0 30 30 30 31 33 30 30 00 31 32 37 35 34 36 30 33 0001300.12754603
000001D0 37 32 32 00 30 31 32 33 30 37 00 20 30 00 00 00 722.012307. 0...
000001E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000210 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000230 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000240 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000250 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000260 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000270 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000280 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000290 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................

	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1015)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:972)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:283)
	at org.apache.jackrabbit.oak.segment.SegmentCache.getSegment(SegmentCache.java:92)
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1275)
	... 21 more
Caused by: java.lang.IllegalStateException: Invalid segment format. Dumping segment 4fb637cc-5013-4925-ab13-0629c4406481
00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000040 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000080 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000090 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000C0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000D0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000000F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000110 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000120 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000130 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000140 39 37 39 31 31 36 30 38 2D 63 31 63 65 2D 34 62 97911608-c1ce-4b
00000150 35 63 2D 61 36 33 37 2D 39 36 61 65 39 34 38 38 5c-a637-96ae9488
00000160 61 37 65 38 2E 30 61 62 34 30 36 38 36 00 00 00 a7e8.0ab40686...
00000170 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000180 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000190 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001A0 00 00 00 00 30 30 30 30 34 30 30 00 30 30 30 30 ....0000400.0000
000001B0 30 30 30 00 30 30 30 30 30 30 30 00 30 30 30 30 000.0000000.0000
000001C0 30 30 30 31 33 30 30 00 31 32 37 35 34 36 30 33 0001300.12754603
000001D0 37 32 32 00 30 31 32 33 30 37 00 20 30 00 00 00 722.012307. 0...
000001E0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000001F0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000200 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000210 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000220 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000230 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000240 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000250 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000260 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000270 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000280 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000290 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002A0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
000002B0 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................

	at com.google.common.base.Preconditions.checkState(Preconditions.java:150)
	at org.apache.jackrabbit.oak.segment.Segment.<init>(Segment.java:185)
	at org.apache.jackrabbit.oak.segment.file.FileStore$15.call(FileStore.java:1292)
	at org.apache.jackrabbit.oak.segment.file.FileStore$15.call(FileStore.java:1)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1011)
	... 25 more
0
{code}
",cleanup gc,"['segment-tar', 'segmentmk']",OAK,Bug,Major,2016-08-16 12:55:29,0
12996938,Cleanup creates new generation of tar file without removing any segments ,"On some deployments I have seen tar files with a quite hight generation post-fix (e.g. 'v'). From the log files I could deduce that this particular tar file was rewritten multiple times without actually any segment being removed.
I assume this is caused by the 25% gain threshold not taking the sizes contributed by the index and the graph entries into account.

The attached test case can be used to verify the above hypothesis.",cleanup gc,['segment-tar'],OAK,Bug,Minor,2016-08-12 13:05:48,0
12994907,Improve documentation about structure of TAR files,"Improve the page at [1] to include a picture of the contents of a TAR file, as done for segments in [2], to cover missing parts (e.g. binary references files) and to better align it with latest oak-segment-tar improvements.

[1] http://jackrabbit.apache.org/oak/docs/nodestore/segment/tar.html
[2] http://jackrabbit.apache.org/oak/docs/nodestore/segment/records.html",documentation,"['doc', 'segment-tar']",OAK,Sub-task,Minor,2016-08-04 13:58:46,0
12994527,Improve cache eviction policy of the node deduplication cache,"{{NodeCache}} uses one stripe per depth (of the nodes in the tree). Once its overall capacity (default 1000000 nodes) is exceeded, it clears all nodes from the stripe with the greatest depth. This can be problematic when the stripe with the greatest depth contains most of the nodes as clearing it would result in an almost empty cache. 

",perfomance,['segment-tar'],OAK,Improvement,Major,2016-08-03 09:10:58,3
12993523,External invocation of background operations,"The background operations (flush, compact, cleanup, etc.) are historically part of the implementation of the {{FileStore}}. They should better be scheduled and invoked by an external agent. The code deploying the {{FileStore}} might have better insights on when and how these background operations should be invoked. See also OAK-3468.
",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-07-29 12:08:41,3
12993518,Unify RecordCacheStats and CacheStats,There is {{org.apache.jackrabbit.oak.cache.CacheStats}} in {{oak-core}} and {{org.apache.jackrabbit.oak.segment.RecordCacheStats}} in {{oak-segment-tar}}. Both exposing quite similar functionality. We should try to unify them as much as possible. ,refactoring technical_debt,"['core', 'segment-tar']",OAK,Improvement,Minor,2016-07-29 11:58:50,3
12993517,Align GCMonitorMBean MBean with new generation based GC,"The {{GCMonitorMBean}} MBean still dates back to the old {{oak-segment}}. We need to review its endpoints and only keep those that make sense for {{oak-segment-tar}}, adapt the others as necessary any add further functionality as required. 

Specifically I think we should get rid of the time series for {{getRepositorySize()}} and {{getReclaimedSize()}}.

Also the name {{getRepositorySize()}} is confusing and we should change it. It leads callers to think it would return current size of the repository opposed to the size it had after the last cleanup. (There is {{FileStoreStatsMBean.getRepositorySize()}} for the latter.)",production,['segment-tar'],OAK,Task,Major,2016-07-29 11:55:50,0
12993514,Align SegmentRevisionGC MBean with new generation based GC,"The {{SegmentRevisionGC}} MBean still dates back to the old {{oak-segment}}. We need to review its endpoints and only keep those that make sense for {{oak-segment-tar}}, adapt the others as necessary any add further functionality as required. ",production,['segment-tar'],OAK,Task,Major,2016-07-29 11:51:45,3
12992910,Report age of oldest queue entry in EventListenerMBean and ConsolidatedListenerMBean,This is related to JCR-4000 and the remaining work in Oak that hooks into the ListenerTracker and exposes the info also in the consolidated listener MBean.,observation,"['core', 'jcr']",OAK,Improvement,Minor,2016-07-27 12:28:35,1
12992628,Separate persistent cache for diff and local_diff,"The DocumentNodeStore currently uses a single persistent cache for all types (node, nodeChildren, diff, etc.). With this setup it is not possible to assign a specific amount of disk space for some cache type(s). If there are many inserts for one cache type, entries of another type may become unavailable. In practice this can be a problem for the local_diff cache entries that are important for efficient node state comparison.

Separating the diff and local_diff cache entries would also allow for different configuration options like compression and different behaviour when the async write back queue is full.",observation,"['core', 'documentmk']",OAK,Improvement,Major,2016-07-26 16:00:27,1
12992483,Collection of references retrieves less when large number of blobs added,"When large number of external blobs are added to the DataStore (50000) and a cycle of compaction executed then the reference collection logic only returns lesser number of blob references. It reports correct number of blob references when number of blobs added are less indicatingsome sort of overflow.
Another related issue observed when testing with lesser number of blobs is that the references returned are double the amount expected, so maybe there should be some sort of de-duplication which should be added.

Without compaction the blob references are returned correctly atleast till 100000 (ExternalBlobId#testNullBlobId)",datastore gc,['segment-tar'],OAK,Bug,Major,2016-07-26 05:08:58,2
12991809,Clarify implementation and documentation of TarReader#mark,"There is a few peculiarities with that method:

* The Javadoc ""A bulk segment is reclaimable if it is in bulkRefs"" is wrong. It should be ""A bulk segment is reclaimable if it is *not* in bulkRefs"".
* (Why) is it necessary to iterate in reverse over the entries in tar file?
* Why the extra check for bulk references in the else branch?
* The condition {{!reclaim.remove(id)}} is always true as {{id}} can only be in {{reclaim}} it it had been added in the same iteration (as ids are unique). But this would have been in the if branch, contradicting us being in the else branch. 
",cleanup gc,['segment-tar'],OAK,Improvement,Major,2016-07-22 13:09:18,3
12991357,Move DocumentMK specific methods from DocumentNodeStore,"There are some DocumentMK specific methods in DocumentNodeStore, which should be moved to the DocumentMK.",technical_debt,"['core', 'documentmk']",OAK,Technical task,Minor,2016-07-21 07:11:08,1
12991135,Improve FileStore.size calculation,"A new approach for calculating {{FileStore::size}} is needed because this method is prone to lock contention and should not be called too often.

The steps to implement the approach are:
# reduce the lock surface of the size() method. This should be simple enough by creating a copy of the readers / writer inside the lock and do the actual size calculation on that snapshot but outside of the lock.
# lower size() visibility to package to avoid misuse (from monitoring tools)
# remove {{approximateSize}} and associated logic and replace it with {{size()}}.
",resilience,['segment-tar'],OAK,Task,Minor,2016-07-20 14:24:43,0
12990383,Overflow to disk threshold too high,"The overflow to disk threshold for {{StringSort}} used by JournalEntry is too high. JournalEntry assumes the threshold is in bytes, whereas the threshold is actually the number of Strings.",observation,"['core', 'documentmk']",OAK,Bug,Minor,2016-07-18 13:07:35,1
12990286,JournalEntry.applyTo() creates complete change tree in memory,"While applying the changes from {{StringSort}} to the diff cache, the method recreates the entire change tree in memory. Depending on the revision range, the number of changes can be very high and cause an OOME.",observation,"['core', 'documentmk']",OAK,Bug,Major,2016-07-18 07:56:39,1
12990274,Define oak:Resource nodetype as non referenceable alternative to nt:resource,"In most cases where code uses JcrUtils.putFile [1] it leads to
creation of below content structure

{noformat}
+ foo.jpg (nt:file)
   + jcr:content (nt:resource)
       - jcr:data
{noformat}

Due to usage of nt:resource each nt:file node creates a entry in uuid
index as nt:resource is referenceable. So if a system has 1M
nt:file nodes then we would have 1M entries in /oak:index/uuid as in
most cases the files are created via [1] and hence all such files are
referenceable

The nodetype defn for nt:file does not mandate that the
requirement for jcr:content being nt:resource. To support such non referenceable files we would define a new nodeType similar to nt:resource but which is non referenceable.

See [2] for related discussion

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-jcr-commons/src/main/java/org/apache/jackrabbit/commons/JcrUtils.java#L1062
[2] http://jackrabbit-oak.markmail.org/thread/qicpzm5ltnzfsd42",docs-impacting,['core'],OAK,Improvement,Major,2016-07-18 07:04:38,4
12989714,Specify thread pool name which should be used by Async Indexing task,"While running Oak in Sling we rely on Sling Scheduler to ensure that async indexing task are run on leader (OAK-1246) with specified frequency. 

Be default Sling Scheduler uses a default pool for managing all tasks. It can happen that number of task can be quite hight which can then lead to default thread pool getting exhausted and that causes async indexing to get delayed.

To ensure that async indexing is not affected by such scenarios we should make use of a dedicated thread pool. This is now supported by SLING-5831",docs-impacting,['core'],OAK,Improvement,Minor,2016-07-15 08:36:47,4
12989393,SegmentNodeState.fastEquals() can trigger two I/O operations,"The implementation of {{SegmentNodeState.fastEquals()}} compares the stable IDs of two instances of {{SegmentNodeState}}. In some cases, reading the stable ID would trigger a read of an additional record, the block record containing the serialized version of the segment ID.

This issue is about evaluating the performance implications of this strategy and, in particular, if it would be better to store the serialized stable ID in the node record itself.",performance,['segment-tar'],OAK,Improvement,Minor,2016-07-14 09:08:48,2
12987139,Add info about event generation and consumption by observer,"I'm not sure if it's possible in the current scheme of things (implementation), but it'd useful to be able to easily differentiate between slow diff calculation or slow observer as a reason to see why observation queue might fill up.",candidate_oak_1_4 monitoring observation performance,"['core', 'jcr']",OAK,Improvement,Minor,2016-07-06 13:16:54,1
12987119,Cache update blocks new commits,"When caches are updated after a commit (within CommitQueue.Callback.headOfQueue()), other threads are blocked when they try to acquire new revisions from the queue.",concurrency,"['core', 'documentmk']",OAK,Improvement,Minor,2016-07-06 11:01:10,1
12986196,diff calculation in DocumentNodeStore should try to re-use journal info on diff cache miss,"Currently, diff information is filled into caches actively (local commits pushed in local_diff, externally read changes pushed into memory_diff). At the time of event processing though, the entries could have already been evicted.
In that case, we fall back to computing diff by comparing 2 node-states which becomes more and more expensive (and eventually fairly non-recoverable leading to OAK-2683).

To improve the situation somewhat, we can probably try to consult journal entries to read a smaller-superset of changed paths before falling down to comparison.

/cc [~mreutegg], [~chetanm], [~egli]",observation resilience,"['core', 'documentmk']",OAK,Improvement,Minor,2016-07-01 14:12:30,1
12979401,CI failing on branches due to unknown fixture SEGMENT_TAR,"These failures are caused by adding the SEGMENT_TAR fixture to the matrix. That one doesn't exit in the branches thus the {{IllegalArgumentException}} ""No enum constant"".

See discussion http://markmail.org/message/oaptnvco5y2a4rjk",CI build jenkins,[],OAK,Bug,Major,2016-06-15 15:53:49,4
12979389,Finalise SegmentCache,"{{SegmentCache}} needs documentation, management instrumentation and monitoring tests and logging. ",cache monitoring production,['segment-tar'],OAK,Task,Major,2016-06-15 15:07:22,3
12979326,Decouple SegmentReader from Revisions,"The {{SegmentReader.readHeadState()}} introduces a de-facto dependency to {{Revisions}} as access to the latter is required for obtaining the record id of the head. 

To decouple SegmentReader from Revisions I propose to replace {{SegmentReader.readHeadState()}} with {{SegmentReader.readHeadState(Revisions revisions)}}. As this results in a lot of boilerplate for callers (i.e. {{fileStore.getReader().getHeadState(fileStore.getRevisions())}}), we should also introduce a convenience method {{FileStore.getHead()}} clients could use to that matter.
",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-06-15 10:31:10,3
12979249,More compact storage format for Documents,"Aim of this task is to evaluate storage cost of current approach for various Documents in DocumentNodeStore. And then evaluate possible alternative to see if we can get a significant reduction in storage size.

Possible areas of improvement
# NodeDocument
## Use binary encoding for property values - Currently property values are stored in JSON encoding i.e. arrays and single values are encoded in json along with there type
## Use binary encoding for Revision values - In a given document Revision instances are a major part of storage size. A binary encoding might provide more compact storage
# Journal - The journal entries can be stored in compressed form

Any new approach should support working with existing setups i.e. provide gradual change in storage format. 

*Possible Benefits*
More compact storage would help in following ways
# Low memory footprint of Document in Mongo and RDB
# Low memory footprint for in memory NodeDocument instances - For e.g. property values when stored in binary format would consume less memory
# Reduction in IO over wire - That should reduce the latency in say distributed deployments where Oak has to talk to remote primary

Note that before doing any such change we must analyze the gains. Any change in encoding would make interpreting stored data harder and also represents significant change in stored data where we need to be careful to not introduce any bug!",performance,['documentmk'],OAK,Improvement,Major,2016-06-15 05:23:12,4
12978771,Upgrade commons-io to 2.5 and remove ReversedLinesFileReader,"For OAK-2605 we copied the source of {{ReversedLinesFileReader}} to Oak to get the fix for IO-471 in. As this is now fixed in {{commons-io}} 2.5, I suggest we upgrade our dependency and remove that duplicated class.",technical_debt,"['auth-external', 'blob', 'commons', 'core', 'examples', 'parent', 'pojosr', 'run', 'segment-tar', 'segmentmk', 'webapp']",OAK,Improvement,Major,2016-06-14 12:34:33,3
12977615,Remove SegmentNodeStore.getSuperRoot(),I like to remove {{SegmentNodeStore.getSuperRoot}} That method leaks implementations details (e.g. checkpoints). Access to the super root is still possible through lower level APIs (e.g. {{SegmentReader#readHeadState}}. ,technical_debt,['segment-tar'],OAK,Technical task,Major,2016-06-10 13:56:07,3
12977548,Improve logging during compaction cycles,When compaction needs to go into cycles because of concurrent commits the number of total cycles should be logged alongside with the number of attempted cycles. ,compaction gc,['segment-tar'],OAK,Improvement,Major,2016-06-10 09:39:20,3
12977379,Setup Windows builds ,As [discussed | http://markmail.org/message/2dk6i3yxjfkknrzp] we should also have CI coverage on Windows.,CI build infrastructure jenkins,[],OAK,Technical task,Blocker,2016-06-09 20:50:55,3
12976918,Consistently use the term segment-tar,"We should make an effort to consistently use the term ""segment-tar"" instead of ""SegmentMK"", ""TarMK"", etc. in logging, exceptions, labels, descriptions, documentation etc.",documentation production,"['doc', 'segment-tar']",OAK,Task,Minor,2016-06-08 16:13:59,3
12976915,Implement a proper template cache,"The template cache is currently just a map per segment. This is problematic in various ways: 
* A segment needs to be in memory and probably loaded first only to read something from the cache. 
* No monitoring, instrumentation of the cache
* No control over memory consumption 

We should there for come up with a proper template cache implementation in the same way we have done for strings ({{StringCache}}) in OAK-3007. Analogously that cache should be owned by the {{CachingSegmentReader}}. ",cache monitoring production,['segment-tar'],OAK,Improvement,Major,2016-06-08 16:08:39,3
12976909,SegmentNodeStore and SegmentStore builders should log their parameters on build(),{{SegmentNodeStoreBuilder}} and {{FileStoreBuilder}} should log the arguments used to build new instances of the respective classes when one of its {{build()}} methods is called. This facilitates post mortem analysis of log files.,logging production,['segment-tar'],OAK,Improvement,Minor,2016-06-08 15:51:47,3
12976870,Release oak-segment-tar,"Tweak our setup in order to be able to cut an initial release of {{oak-segment-tar}} and perform the release. 

",release,['segment-tar'],OAK,Task,Major,2016-06-08 13:38:34,2
12976859,Collect write statistics ,"We should come up with a good set of write statistics to collect like number of records/nodes/properties/bytes. Additionally those statistics should be collected for normal operation vs. compaction related operation. This would allow us to more precisely analyse the effect of compaction on the overall system. 
",compaction gc monitoring,['segment-tar'],OAK,Improvement,Major,2016-06-08 13:12:24,3
12976857,Reduce number of calls to NodeBuilder.getNodeState from MergingNodeStateDiff,The result of the static {{MergingNodeStateDiff.merge}} method is only used in the base case of a recursive diff. In all other cases while traversing the child diffs that result is simply discarded. As calculating the result involves an extra call to {{NodeBuilder.getNodeState}} it inflicts a performance penalty in the case of segment-tar: in that case this call causes the changes in the builder to be written ahead into the store. I figure it is simple enough to specialise the merge method in a way so that call is only done when its result is actually used. ,perfomance,['core'],OAK,Improvement,Major,2016-06-08 13:06:54,3
12976475,Fix the errors reported by the Javadoc tool in JDK8,Some Javadoc is not strict enough according to the Javadoc tool shipped in JDK8.,candidate_oak_1_4,[],OAK,Bug,Major,2016-06-07 14:44:49,2
12976451,Segments created by an unsuccessful compaction run should get cleaned,Cleaning of segment created by an unsuccessful compaction run currently only works if forced compaction is enabled. Otherwise those segments will only get cleaned in a much later cleanup cycle. ,compaction gc,['segment-tar'],OAK,Bug,Major,2016-06-07 13:58:44,3
12976346,HeavyWriteIT sporadically fails,"I've seen {{HeavyWriteIT}} fail sporadically on my local checkout.

{noformat}
3d13e2927fc0d75454a692ef5c8703880dc2ea0d
org.apache.jackrabbit.oak.segment.SegmentNotFoundException: Segment 31b75992-aaf7-4f2b-a5de-b5a268c1fdb3 not found

	at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1377)
	at org.apache.jackrabbit.oak.segment.file.FileStore$14.call(FileStore.java:1317)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.load(CacheLIRS.java:1011)
	at org.apache.jackrabbit.oak.cache.CacheLIRS$Segment.get(CacheLIRS.java:972)
	at org.apache.jackrabbit.oak.cache.CacheLIRS.get(CacheLIRS.java:283)
	at org.apache.jackrabbit.oak.segment.SegmentCache.geSegment(SegmentCache.java:80)
	at org.apache.jackrabbit.oak.segment.file.FileStore.readSegment(FileStore.java:1317)
	at org.apache.jackrabbit.oak.segment.SegmentId.getSegment(SegmentId.java:111)
	at org.apache.jackrabbit.oak.segment.RecordId.getSegment(RecordId.java:94)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.isOldGeneration(SegmentWriter.java:1010)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNodeUncached(SegmentWriter.java:906)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.writeNode(SegmentWriter.java:885)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.access$700(SegmentWriter.java:319)
	at org.apache.jackrabbit.oak.segment.SegmentWriter$8.execute(SegmentWriter.java:277)
	at org.apache.jackrabbit.oak.segment.SegmentBufferWriterPool.execute(SegmentBufferWriterPool.java:110)
	at org.apache.jackrabbit.oak.segment.SegmentWriter.writeNode(SegmentWriter.java:274)
	at org.apache.jackrabbit.oak.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:111)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:516)
	at org.apache.jackrabbit.oak.segment.SegmentNodeStore.merge(SegmentNodeStore.java:284)
	at org.apache.jackrabbit.oak.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:91)
{noformat}

I suspect this is a problem with {{isOldGeneration}} itself not being prepared for the old segment actually being gone. ",gc,['segment-tar'],OAK,Bug,Critical,2016-06-07 08:19:55,3
12976344,checkpointDeduplicationTest sometimes fails on Jenkins,"{{CompactionAndCleanupIT.checkpointDeduplication}} irregularly [fails|https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/938/jdk=latest1.7,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/console] on Jenkins. 

This might point to an issue with the de-duplication caches, which are crucial in getting the checkpoints de-duplicated. 

{code}
checkpointDeduplicationTest(org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT)  Time elapsed: 0.15 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<[7211975a-04ce-45ff-aff5-16795ec2cc72]:261932> but was:<[11083c4b-9b2e-4d17-a8c0-8f6b1f2a3173]:261932>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.jackrabbit.oak.segment.CompactionAndCleanupIT.checkpointDeduplicationTest(CompactionAndCleanupIT.java:899)
{code}
",compaction gc test,['segment-tar'],OAK,Bug,Critical,2016-06-07 08:12:27,3
12976334,Remove segment version argument from segment writer and and related classes,"The {{SegmentWriter}} and its related classes accept a {{SegmentVersion}} argument. This is confusing since that version is only stored in the segment's segment version field. The writer cannot and does not actually write segments at older version than the latest (12). 

I suggest we remove the explicit segment version from all classes where it can be specified and hard code the segment version to 12 for now. This is the only segment version {{segment-tar}} currently supports anyway. Should  the need to support other segment version arise in the future, we need to decide at that point how to parametrise {{segment-tar}} on the segment version. ",refactoring,['segment-tar'],OAK,Improvement,Major,2016-06-07 07:40:43,3
12975788,Optimize RevisionVector methods,"{{RevisionVector}} is used in very critical paths and we should look into optimzing some of its critical method

",performance,['documentmk'],OAK,Improvement,Minor,2016-06-05 06:30:35,4
12975375,Optimize PathUtils.concat by using a properly sized StringBuilder,{{PathUtils.concat}} does not specify the default size of StringBuilder. Default string constructor uses a string.length() + 16 as the buffer size. Given size of appended path is known we can properly size the string builder buffer to avoid any expansion during actual append,performance,['commons'],OAK,Improvement,Minor,2016-06-03 04:25:04,4
12975125,Optimize Revison fromString and toString implementation,"Current implementation of Revision {{fromString}} and {{toString}} make use of std JDK API to perform string manipulation. While running some performance test it was seen that these 2 methods are called quite frequently and that adds up to some decent times. Further they also generate quite a bit of short lived objects.

!hot-methods.png!

It would be worthwhile to perform a micro benchmark of these method and optimize them further such that they perform better and also generate less garbage. The micro optimized code would be bit more complex but if performance numbers are better we can look into changing the current implementation",performance,['documentmk'],OAK,Improvement,Major,2016-06-02 11:04:58,4
12974757,Move temp files to target directory,"(Some of?) the changes done in the other subtasks cause the temporary files to be created in the systems temporary folder instead of the target folder as before. This causes issues on system where the temporary folder resides on a small partition. 

See my [comment |https://issues.apache.org/jira/browse/OAK-4208?focusedCommentId=15296019&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15296019] on OAK-4208 where we have been seeing this on the Apache Jenkins instance. See also  INFRA-11837.",test,[],OAK,Technical task,Minor,2016-06-01 10:21:52,2
12972439,Correlate index with the index definition used to build it,"Currently, if the definition of an index is changed without reindexing, it will get in an ""inconsistent"" state. 

Of course, the reindexing is usually necessary, but it would be useful to know with which definition the index was built. This could increase the visibility of the indexing state and help debugging issues related to it.

Some questions this improvement should respond to:
# What is the definition of the index when the (re)indexing was triggered?
# Are there any changes in the definition since the trigger? Which?

I can imagine a solution built by ""versioning"" the definition nodes (oak:QueryIndexDefinition). When the reindex is triggered, a new version of the node is created and the indexer stores a reference to it.
This would also allow the indexer to keep using the same definition until a new reindex, even if changes are made meanwhile (i.e. use a fixed version instead of the latest definition).",docs-impacting,"['lucene', 'query']",OAK,Improvement,Major,2016-05-24 16:36:42,4
12972025,Run SegmentParserTest off memory store instead of file store,Running of the memory store would improve test speed without impacting test coverage.,test,['segment-tar'],OAK,Improvement,Major,2016-05-23 10:30:23,3
12972022,Remove deprecated string cache,OAK-3007 replaced the now deprecated strings cache with a proper cache. We should now remove the former. ,technical_debt,['segment-tar'],OAK,Technical task,Major,2016-05-23 10:21:29,3
12972018,Decouple FileStoreStatsTest,"That test is currently unnecessarily strongly tied to the file store, which makes it prone to failing if implementation details in the store change. ",technical_debt,['segment-tar'],OAK,Improvement,Major,2016-05-23 10:09:02,3
12970406,Refactor SegmentTracker,"The {{SegmentTracker}} class has become the dumping ground for everything that wouldn't fit else where. In a personal discussion with [~frm], we figured that this class might be a good starting point refactoring {{segment-tar}} towards better encapsulation. 
The aim would be to return {{SegmentTracker}} to its initial purpose (i.e. tracking segments) and move all unrelated concerns elsewhere.",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-05-17 15:36:45,3
12970308,Overly zealous warning about checkpoints on compaction ,"{{FileStore.compact}} logs a warning {{TarMK GC #{}: compaction found {} checkpoints, you might need to run checkpoint cleanup}} if there is more than a single checkpoints. 

AFIK this is now the norm as async indexing has uses 2 checkpoints ([~chetanm], [~edivad] please clarify). 

In any case should we improve this and not hard code any number of expected checkpoints. Maybe make the threshold configurable?",compaction gc logging,"['segment-tar', 'segmentmk']",OAK,Improvement,Major,2016-05-17 09:23:47,3
12962944,BlobReferenceRetriever#collectReferences should allow exceptions,"{{BlobReferenceRetriever#collectReferences}} currently does not allow implementations to throw an exception. In case anything goes wrong during reference collection, implementations should be able to indicate this through an exception so the DSGC can safely abort. ",datastore gc resilience,"['core', 'segment-tar']",OAK,Improvement,Major,2016-04-27 09:34:09,3
12962623,Fix test failures in SegmentDataStoreBlobGCIT,"{{SegmentDataStoreBlobGCIT#gcWithInlined}}, {{gc}}, {{gcLongRunningBlobCollection}} and {{consistencyCheckWithGc}} fail since the removal of the old cleanup strategy in OAK-4276. 

The test setup needs to be adapted to the brutal strategy: i.e. {{setup()}} needs to simulate so many compaction cycles until a subsequent cleanup actually remove the segments in question. 

This is not sufficient though as then {{SegmentTracker#collectBlobReferences}} causes a SNFE for those segment ids actually removed but still in the segment id tables. ",cleanup gc,['segment-tar'],OAK,Task,Major,2016-04-26 15:08:29,3
12962573,Align property labels and descriptions in SegmentNodeStoreService,"We need to align / improve the labels and descriptions in {{SegmentNodeStoreService}} to match their actual purpose. At the same time I would opt for changing ""compaction"" to ""revision gc"" in all places where it is used synonymously for the latter. ",production,['segment-tar'],OAK,Task,Major,2016-04-26 12:06:47,3
12962140,Cost per entry for Lucene index of type v1 should be higher than that of v2,"Currently default cost per entry for Lucene index of type
# v1 - which uses query time aggregation
# v2 - which uses index time aggregation

Are same. However given that query time aggregation would require more effort it should result in a higher cost per entry.

This fact impacts the result in cases like OAK-2081 (see last few comments) where with usage of limits both index are currently considered equals",candidate_oak_1_4,['lucene'],OAK,Bug,Minor,2016-04-25 04:19:11,4
12961897,Proper versioning of storage format,"OAK-3348 introduced changes to the segment format (which has been bumped to 12 with OAK-4232). However it also changes the format of the tar files (the gc generation of the segments is written to the index file) which would also require proper versioning.

In a offline discussion [~frm] brought up the idea of adding a manifest file to the store that would specify the format versions of the individual components. ",resilience technical_debt,['segment-tar'],OAK,Task,Major,2016-04-22 20:05:14,2
12961896,Consider making FileStore.writer volatile,That filed is not volatile although access by different threads. We should consider changing it to volatile.,technical_debt,['segment-tar'],OAK,Improvement,Minor,2016-04-22 20:01:12,3
12961891,Document oak-segment-tar,"Document Oak Segment Tar. Specifically:
* New and changed configuration and monitoring options
* Changes in gc (OAK-3348 et. all)
* Changes in segment / tar format (OAK-3348)
",documentation gc,"['doc', 'segment-tar']",OAK,Task,Major,2016-04-22 19:43:02,2
12961884,FileStore.flush prone to races leading to corruption,"There is a small window in {{FileStore.flush}} that could lead to data corruption: if we crash right after setting the persisted head but before any delay-flushed {{SegmentBufferWriter}} instance flushes (see {{SegmentBufferWriterPool.returnWriter()}}) then that data is lost although it might already be referenced from the persisted head.

We need to come up with a test case for this. 

A possible fix would be to return a future from {{SegmentWriter.flush}} and rely on a completion callback. Such a change would most likely also be useful for OAK-3690. 
",resilience,['segment-tar'],OAK,Bug,Critical,2016-04-22 19:36:57,3
12961872,Update segment parser to work with the new segment format,{{SegmentParser}} does not correctly handle the record id added to the segment node states: for those segment node state containing an actual record id the segment parser should process it and call the respective call backs. ,gc tooling,"['run', 'segment-tar']",OAK,Task,Minor,2016-04-22 19:23:42,3
12961864,Remove the gc generation from the segment meta data,The segment meta info (OAK-3550) still contains the segment's gc generation. As with OAK-3348 the gc generation gets written to the segment header directly we should remove it from the segment meta info and update {{oak-run graph}} accordingly. ,compaction gc tooling,"['run', 'segment-tar']",OAK,Task,Minor,2016-04-22 19:14:59,3
12961852,TarReader.calculateForwardReferences only used by oak-run graph tool,{{TarReader.calculateForwardReferences}} is not used for production but only for tooling so it would be good if we could remove that method from the production code an put it into a tooling specific module. ,cleanup gc tooling,['segment-tar'],OAK,Task,Minor,2016-04-22 19:09:49,3
12961845,Disable / remove SegmentBufferWriter#checkGCGen,"{{SegmentBufferWriter#checkGCGen}} is an after the fact check for back references (see OAK-3348), logging a warning if detects any. As this check loads the segment it checks the reference for, it is somewhat expensive. We should either come up with a cheaper way for this check or remove it (at least disable it by default). ",assertion compaction gc,['segment-tar'],OAK,Task,Major,2016-04-22 19:03:44,3
12961706,Rework failing tests in CompactionAndCleanupIT,The fix for OAK-3348 caused some of the tests in {{CompactionAndCleanupIT}} to fail and I put the to ignored for the time being. We need to check whether the test expectations still hold and rework them as required. ,cleanup compaction gc tests,['segment-tar'],OAK,Task,Major,2016-04-22 15:58:09,3
12961701,Garbage left behind when compaction does not succeed,"As a result of the new cleanup approach introduced with OAK-3348 (brutal) a compaction cycle that is not successful (either because of cancellation of because of giving up waiting for the lock) leaves garbage behind, which is only cleaned up 2 generations later. 

We should look into ways to remove such garbage more pro-actively. 
",cleanup compaction gc,['segment-tar'],OAK,Improvement,Major,2016-04-22 15:48:55,3
12961684,Make the number of retained gc generation configurable,"The number of retained gc generations (""brutal"" cleanup strategy) is currently hard coded to 2. I think we need to make this configurable. ",cleanup gc,['segment-tar'],OAK,Task,Major,2016-04-22 15:34:56,3
12961679,Rework memory estimation for compaction,As a result of OAK-3348 we need to partially rework the memory estimation step done for deciding whether compaction can run or not. In {{oak-segment}} there was a {{delta}} value derived from the compaction map. As the latter is gone in {{oak-segment-next}} we need to decide whether there is another way to derive this delta or whether we want to drop it entirely. ,compaction gc,['segment-tar'],OAK,Task,Major,2016-04-22 15:29:31,3
12961673,Compaction cannot be cancelled ,"As a result of the de-duplication cache based online compaction approach from OAK-3348 compaction cannot be cancelled any more (in the sense of OAK-3290). 

As I assume we still need this feature we should look into ways to re-implement it on top of the current approach. 

Also I figure implementing a [partial compaction | https://issues.apache.org/jira/browse/OAK-4122?focusedCommentId=15223924&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15223924] approach on top of a commit scheduler (OAK-4122) would need a feature of this sort. ",compaction gc,['segment-tar'],OAK,Improvement,Major,2016-04-22 15:20:26,3
12961656,Finalise de-duplication caches,"OAK-3348 ""promoted"" the record cache to a de-duplication cache, which is heavily relied upon during compaction. Now also node states go through this cache, which can seen as one concern of the former compaction map (the other being equality). 
The current implementation of these caches is quite simple and served its purpose for a POC for getting rid of the ""back references"" (OAK-3348). Before we are ready for a release we need to finalise a couple of things though:

* Implement cache monitoring and management
* Make cache parameters now hard coded configurable
* Implement proper UTs 
* Add proper Javadoc
* Fine tune eviction logic and move it into the caches themselves (instead of relying on the client to evict items pro-actively)
* Fine tune caching strategies: For the node state cache the cost of the item is determined just by its position in the tree. We might want to take further things into account (e.g. number of child nodes). Also we might want to implement pinning so e.g. checkpoints would never be evicted. 
* Finally we need to decide who should own this cache. It currently lives with the {{SegmentWriter}}. However this is IMO not the correct location as during compaction there is dedicated segment writer whose cache need to be shared with the primary's segment writer upon successful completion. ",caching compaction gc monitoring,['segment-tar'],OAK,Task,Major,2016-04-22 14:29:25,3
12961650,Refactor / rework compaction strategies ,"After the changes from OAK-3348 many if not all of the options in {{CompactionStrategy}} do not apply any more. Specifically the new ""brutal"" strategy is hard coded to always be in effect. We need to:

* Decide which cleanup methods we want to keep supporting,
* decide which options to expose through CompactionStrategy. E.g. {{cloneBinaries}} was so far always set to {{false}} and I would opt to remove the option as implementing might be tricky with the de-duplication cache based compaction we now have,
* optimally refactor {{CompactionStrategy}} into a proper abstract data type. ",compaction gc technical_debt,['segment-tar'],OAK,Task,Major,2016-04-22 14:05:01,3
12960635,Provide a way to abort an async indexing run,"In some cases where a user is tweaking the indexing config it can happen that he saves the config mid way which triggers a long indexing run. Currently there is no easy way to abort such a run and only way to avoid wasting time in the long indexing cycle is to shut down the system.

For such cases it would be good to provide an ""abort"" operation as part of {{IndexStatsMBean}} which user can invoke to abort any run safely and cleanly",docs-impacting,['query'],OAK,Improvement,Major,2016-04-21 07:39:17,4
12960367,Implement fixtures for running again oak-segment and/or oak-segment-next,"We need fixtures to run UTs / ITs against either or both segment implementations {{oak-segment}} and {{oak-segment-next}}. 

Ideally we can enable them individually through e.g. environment variables. A standard build would run against {{oak-segment}} so not to affect others. {{oak-segment-next}} could be enabled on request locally or for the CI. 
Once we deprecate {{oak-segment}} we would switch the default fixture to {{oak-segment-next}}. ",testing,['segment-tar'],OAK,Task,Blocker,2016-04-20 14:35:39,2
12960329,CLONE - Cross gc sessions might introduce references to pre-compacted segments,"I suspect that certain write operations during compaction can cause references from compacted segments to pre-compacted ones. This would effectively prevent the pre-compacted segments from getting evicted in subsequent cleanup phases. 

The scenario is as follows:
* A session is opened and a lot of content is written to it such that the update limit is exceeded. This causes the changes to be written to disk. 
* Revision gc runs causing a new, compacted root node state to be written to disk.
* The session saves its changes. This causes rebasing of its changes onto the current root (the compacted one). At this point any node that has been added will be added again in the sub-tree rooted at the current root. Such nodes however might have been written to disk *before* revision gc ran and might thus be contained in pre-compacted segments. As I suspect the node-add operation in the rebasing process *not* to create a deep copy of such nodes but to rather create a *reference* to them, a reference to a pre-compacted segment is introduced here. 

Going forward we need to validate above hypothesis, assess its impact if necessary come up with a solution.
",cleanup compaction gc,['segmentmk'],OAK,Improvement,Critical,2016-04-20 12:47:20,3
12960318,CLONE - FileStore.containsSegment returns alway true (almost),"{{FileStore.containsSegment()}} looks [funky|https://github.com/mduerig/jackrabbit-oak/blob/36cb3bf6e5078e3afa75581fb789eeca7b5df2e2/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/FileStore.java#L1197-L1197]. This ""optimisation"" causes it to always return {{true}}. 

{{containsSegment}} is used for deduplication and revision gc. The current implementation causes {{SNFE}} exceptions once gc is effective (as I experienced while working on OAK-3348). ",compaction gc stability,['segment-tar'],OAK,Bug,Critical,2016-04-20 12:29:20,3
12960315,CLONE - BackgroundThread should log and re-throw instances of Error,If the run method of a {{BackgroundThread}} instance hits an {{Error}} it dies silently. Instead it should log an re-throw the error. ,resilience,['segment-tar'],OAK,Improvement,Major,2016-04-20 12:24:48,3
12960313,CLONE - TarReader#loadGraph wrongly detects segment graph as corrupt ,{{org.apache.jackrabbit.oak.plugins.segment.file.TarReader#loadGraph}} sometimes detects a segment graph as corrupt although it isn't. This results in cleanup rewriting the tar file (all over again). ,cleanup gc,['segment-tar'],OAK,Bug,Major,2016-04-20 12:19:56,3
12960275,Deprecate oak-segment,"Before the next major release we need to deprecate {{oak-segment}} and make {{oak-segment-tar}} the new default implementation:

* Deprecate all classes in {{oak-segment}}
* Update documentation to reflect this change
* Update tooling to target {{oak-segment-tar}} (See OAK-4246). 
* Update dependencies of upstream modules / projects from {{oak-segment}} to {{oak-segment-tar}}. 
* Ensure {{oak-segment-tar}} gets properly released (See OAK-4258). 
* Tests run against the {{SEGMENT_TAR}} fixture.",technical_debt,"['segment-tar', 'segmentmk']",OAK,Task,Critical,2016-04-20 09:20:15,2
12960273,Update segment tooling to choose target store,"We need to add command line options segment specific tooling so users could chose between {{oak-segment}} and {{oak-segment-next}}. {{oak-segment}} should be the default until deprecated, where {{oak-segment-next}} should be made the default. ",tooling,['segment-tar'],OAK,Task,Blocker,2016-04-20 09:16:20,2
12959937,Bump segment version to 12,"We need to bump {{SegmentVersion}} to 12 to properly reflect the change in persistence format. At the same time we need to remove our dependencies to older segment versions as this is a non backward compatible change. 

All segment stores written by code prior to this change will not work any more with code once this change has been applied and vice versa. 

",compatibility version,['segment-tar'],OAK,Technical task,Major,2016-04-19 08:47:03,3
12958434,Remove deprecated constructors from SegmentNodeStore,Switch API clients to {{SegmentNodeStoreBuilder}} instead. ,technical_debt,['segmentmk'],OAK,Technical task,Major,2016-04-13 10:27:52,2
12958002,Use another NodeStore as a local cache for a remote Document store,"DocumentNodeStore makes use of persistent cache to speed up its processing and save on making remote calls for data already present in cache

In addition to that we can look into make use of Segment NodeStore as kind of ""local copy"" for certain paths in repository and route calls to it if possible. As part of this task I would like to prototype such an approach. At high level it would work as below

# At start bootstrap the setup and shutdown it down
# Use a modified ""sidegrade"" and copy over the NodeStats from Document store to Segment store. In such a copy we also store some Document specific properties like {{readRevision}} and {{lastRevision}} as hidden property in Segment NodeStates
# In DocumentNodeStore we refactor the current code to extract a 
## {{AbstractDocumentNodeState}} - Abase class which has some logic move out from {{DocumentNodeState}}
## {{SegmentDocumentNodeState}} extends above and delegate calls to a wrapped {{SegmentNodeState}}
## {{DocumentNodeState}} would also extend {{AbstractDocumentNodeState}} and hence delegate to some calls to parent. In this when a call comes for {{getChildNode}} it can check if that can be served by a local copy of {{SegmentNodeStore}} for given {{rootRevision}} then it delegates to that
# For update plan is to make use of {{Observer}} which listens to changes and updates the local copy for certain configured paths. 
## Key aspect to address here is handle the restart case where in a cluster a specific node restarts after some time then how it refreshes itself there

*Usage*
Following 2 OSGi configs would need to be seed

* org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.config
{noformat}
secondary=B""true""
{noformat}
* org.apache.jackrabbit.oak.plugins.document.secondary.SecondaryStoreCacheService.config
{noformat}
includedPaths=[ \
  ""/"",
  ]
{noformat}

With these settings if DocumentNodeStoreService gets started it would pickup the cache and use it. Change {{includedPaths}} depending on paths in repository which you want to include in secondary store.

*Feature Docs*
http://jackrabbit.apache.org/oak/docs/nodestore/document/secondary-store.html",secondary-nodestore,['documentmk'],OAK,New Feature,Major,2016-04-12 07:08:01,4
12955262,Too verbose logging during revision gc,"{{FileStore.cleanup}} logs the segment id of any forward reference found when including those in the reference graph. The logged information can amount to several MBs impacting normal operation. Furthermore the actually reclaimed segments are logged, which also makes the log files explode. Finally the processing of the references and individual tar files might be too wordy. 

",cleanup gc logging,['segment-tar'],OAK,Improvement,Blocker,2016-04-01 10:34:58,2
12952080,Decouple revision cleanup from the flush thread,"I suggest we decouple revision cleanup from the flush thread. With large repositories where cleanup can take several minutes to complete it blocks the flush thread from updating the journal and the persisted head thus resulting in larger then necessary data loss in case of a crash. 

/cc [~alex.parvulescu]",resilience,['segment-tar'],OAK,Improvement,Major,2016-03-21 16:15:40,3
12951446,JaasConfigSpiTest fails intermittently with missing LoginModule exception,"Following failure is seen on some CI

{noformat}
Running org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.98 sec <<< FAILURE!
defaultConfigSpiAuth(org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest)  Time elapsed: 0.97 sec  <<< ERROR!
java.lang.reflect.UndeclaredThrowableException
	at com.sun.proxy.$Proxy16.login(Unknown Source)
	at javax.jcr.Repository$login.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest.defaultConfigSpiAuth(JaasConfigSpiTest.groovy:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.jackrabbit.oak.run.osgi.OakOSGiRepositoryFactory$RepositoryProxy.invoke(OakOSGiRepositoryFactory.java:485)
	... 39 more
Caused by: javax.jcr.LoginException: No LoginModules configured for jackrabbit.oak
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:288)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:244)
	... 44 more
Caused by: javax.security.auth.login.LoginException: No LoginModules configured for jackrabbit.oak
	at javax.security.auth.login.LoginContext.init(LoginContext.java:272)
	at javax.security.auth.login.LoginContext.<init>(LoginContext.java:520)
	at org.apache.jackrabbit.oak.spi.security.authentication.JaasLoginContext.<init>(JaasLoginContext.java:49)
	at org.apache.jackrabbit.oak.security.authentication.LoginContextProviderImpl.getLoginContext(LoginContextProviderImpl.java:85)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:164)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:280)
	... 45 more
{noformat}",CI test,['pojosr'],OAK,Task,Minor,2016-03-18 06:14:36,4
12949107,Replace the commit semaphore in the segment node store with a scheduler,"{{SegmentNodeStore}} currently uses a semaphore to coordinate concurrent commits thus relying on the scheduling algorithm of that implementation and ultimately of the JVM for in what order commits are processed. 

I think it would be beneficial to replace that semaphore with an explicit queue of pending commit. This would allow us to implement a proper scheduler optimising for e.g. minimal system load, maximal throughput or minimal latency etc. A scheduler could e.g. give precedence to big commits and order commits along the order of its base revisions, which would decrease the amount of work to be done in rebasing. 


",operations performance scalability throughput,['segment-tar'],OAK,New Feature,Major,2016-03-11 14:45:34,0
12949098,CompactionMap#get not transitive across compaction map generations,"{{CompactionMap#get(RecordId before)}} searches through the compaction maps until it finds one containing {{before}} returning its value. However that one might already have been compacted again an be present as key in a later compaction map generation. 

A correct implementation of {{CompactionMap#get(RecordId before)}} should consider the transitive closure over all maps starting at {{before}}. Note however that in this case we would also need to stop removing keys from the compaction map after cleanup as this would break transitivity again. (See http://svn.apache.org/viewvc?view=revision&revision=1673791)",compaction gc,['segmentmk'],OAK,Bug,Major,2016-03-11 14:21:13,3
12948663,Cached lucene index gets corrupted in case of unclean shutdown and journal rollback in SegmentNodeStore,"Currently Oak Lucene support would copy index files to local file system as part of CopyOnRead feature. In one of the setup it has been observed that index logic was failing with following error

{noformat}
04.02.2016 17:47:52.391 *WARN* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier [/oak:index/lucene] Found local copy for _2ala.cfs in MMapDirectory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 9320 differs from remote 3714150. Content would be read from remote file only
04.02.2016 17:47:52.399 *WARN* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier [/oak:index/lucene] Found local copy for segments_28je in MMapDirectory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/crx/author/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 1214 differs from remote 1175. Content would be read from remote file only
04.02.2016 17:47:52.491 *ERROR* [oak-lucene-3] org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker Failed to open Lucene index at /oak:index/lucene
org.apache.lucene.index.CorruptIndexException: codec header mismatch: actual header=1953790076 vs expected header=1071082519 (resource: SlicedIndexInput(SlicedIndexInput(_2ala.fnm in _2ala.cfs) in _2ala.cfs slice=8810:9320))
	at org.apache.lucene.codecs.CodecUtil.checkHeader(CodecUtil.java:128)
	at org.apache.lucene.codecs.lucene46.Lucene46FieldInfosReader.read(Lucene46FieldInfosReader.java:56)
	at org.apache.lucene.index.SegmentReader.readFieldInfos(SegmentReader.java:215)
{noformat}

Here size of __2ala.cfs_ differed from remote copy and possible other index file may have same size but different content. Comparing the modified time of the files with those in Oak it can be seen that one of file system was older than one in Oak

{noformat}

_2alr.cfs={name=_2alr.cfs, size=1152402, sizeStr=1.2 MB, modified=Thu Feb 04 17:52:31 GMT 2016, osModified=Feb 4 17:52, osSize=1152402, mismatch=false}
_2ala.cfe={name=_2ala.cfe, size=224, sizeStr=224 B, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=224, mismatch=false}
_2ala.si={name=_2ala.si, size=252, sizeStr=252 B, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=252, mismatch=false}
_2ala.cfs={name=_2ala.cfs, size=3714150, sizeStr=3.7 MB, modified=Thu Feb 04 17:47:28 GMT 2016, osModified=Feb 4 17:17, osSize=9320, mismatch=true}
_14u3_29.del={name=_14u3_29.del, size=1244036, sizeStr=1.2 MB, modified=Thu Feb 04 16:37:35 GMT 2016, osModified=Feb 4 16:37, osSize=1244036, mismatch=false}
_2akw.si={name=_2akw.si, size=252, sizeStr=252 B, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=252, mismatch=false}
_2akw.cfe={name=_2akw.cfe, size=224, sizeStr=224 B, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=224, mismatch=false}
_2akw.cfs={name=_2akw.cfs, size=4952761, sizeStr=5.0 MB, modified=Thu Feb 04 16:37:07 GMT 2016, osModified=Feb 4 16:37, osSize=4952761, mismatch=false}
{noformat}

And on same setup the system did saw a rollback in segment node store 
{noformat}

-rw-rw-r--. 1 crx crx  25961984 Feb  4 16:47 data01357a.tar
-rw-rw-r--. 1 crx crx  24385536 Feb  4 16:41 data01357a.tar.bak
-rw-rw-r--. 1 crx crx    359936 Feb  4 17:18 data01358a.tar
-rw-rw-r--. 1 crx crx    345088 Feb  4 17:17 data01358a.tar.bak
-rw-rw-r--. 1 crx crx  70582272 Feb  4 18:35 data01359a.tar
-rw-rw-r--. 1 crx crx  66359296 Feb  4 18:33 data01359a.tar.bak
-rw-rw-r--. 1 crx crx    282112 Feb  4 18:46 data01360a.tar
-rw-rw-r--. 1 crx crx    236544 Feb  4 18:45 data01360a.tar.bak
-rw-rw-r--. 1 crx crx    138240 Feb  4 18:56 data01361a.tar
{noformat}

So one possible cause is that 
# At some time earlier to 17:17 lucene index got updated and __2ala.cfs_ got created. 
# Post update the head revision in Segment store was updated but the revision yet to made it to journal log
# Lucene CopyOnRead logic got event for the change and copied the file
# System crashed and hence journal did not got updated
# System restarted and per last entry in journal system suffered with some ""data loss"" and hence index checkpoint also moved back
# As checkpoint got reverted index started at earlier state and hence created a file with same name __2ala.cfs_ 
# CopyOnRead detected file length change and logged a warning routing call to remote
# However other files like _2ala.si, _2ala.cfe which were created in same commit had same size but likely different content which later cause lucene query to start failing

In such a case a restart after cleaning the existing index content would have brought back the system to normal state.

So as a fix we would need to come up with some sanity check at time of system startup",resilience,['lucene'],OAK,Bug,Critical,2016-03-10 05:31:47,4
12948139,Reclaimed size reported by FileStore.cleanup is off,"The current implementation simply reports the difference between the repository size before cleanup to the size after cleanup. As cleanup runs concurrently to other commits, the size increase contributed by those is not accounted for. In the extreme case where cleanup cannot reclaim anything this can even result in negative values being reported. 

We should either change the wording of the respective log message and speak of before and after sizes or adjust our calculation of reclaimed size (preferred). ",cleanup gc,['segment-tar'],OAK,Bug,Minor,2016-03-08 19:49:35,0
12948135,Implement FileStore.size through FileStore.approximateSize,"{{FileStore.size()}} is prone to lock contention and should not be called too often. As OAK-2879 already introduced an approach for tracking the current size of the file store without having to lock, we might as well promote his to be ""the official"" implementation. 

[~frm] WDYT?",resilience,['segment-tar'],OAK,Technical task,Minor,2016-03-08 19:43:06,0
12948120,Replace journal.log with an in place journal,Instead of writing the current head revision to the {{journal.log}} file we could make it an integral part of the node states: as OAK-3804 demonstrates we already have very good heuristics to reconstruct a lost journal. If we add the right annotations to the root node states this could replace the current approach. The latter is problematic as it relies on the flush thread properly and timely updating {{journal.log}}. See e.g. OAK-3303. ,resilience,['segment-tar'],OAK,New Feature,Minor,2016-03-08 19:14:51,0
12948112,Break cyclic dependency of FileStore and SegmentTracker,"{{SegmentTracker}} and {{FileStore}} are mutually dependent on each other. This is problematic and makes initialising instances of these classes difficult: the {{FileStore}} constructor e.g. passes a not fully initialised instance to the {{SegmentTracker}}, which in turn writes an initial node state to the store. Notably using the not fully initialised {{FileStore}} instance!",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-03-08 19:03:23,2
12947985,Lucene index appear to be corrupted with compaction enabled,"While running on SegmentNodStore and online compaction enabled it can happen that access to Lucene index start failing with SegmentNotFoundException

{noformat}
Caused by: org.apache.jackrabbit.oak.plugins.segment.SegmentNotFoundException: Segment a949519a-8903-44f9-a17e-b6d83fb32186 not found
       at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:870)
       at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:136)
       at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:108)
       at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
       at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.getNewStream(SegmentBlob.java:64)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.loadBlob(OakDirectory.java:259)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexFile.readBytes(OakDirectory.java:307)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.readBytes(OakDirectory.java:404)
       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory$OakIndexInput.readByte(OakDirectory.java:411)
       at org.apache.lucene.store.DataInput.readVInt(DataInput.java:108)
       at org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2397)
       at org.apache.lucene.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekCeil(BlockTreeTermsReader.java:1973)
       at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:225)
       at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:78)
       at org.apache.lucene.search.ConstantScoreAutoRewrite.rewrite(ConstantScoreAutoRewrite.java:95)
       at org.apache.lucene.search.MultiTermQuery$ConstantScoreAutoRewrite.rewrite(MultiTermQuery.java:220)
       at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
       at org.apache.lucene.search.BooleanQuery.rewrite(BooleanQuery.java:418)
       at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:636)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:683)
       at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:378)
{noformat}

The above segmentId was mentioned in the compaction log

{noformat}
06.03.2016 02:03:30.706 *INFO* [TarMK flush thread [/app/repository/segmentstore], active since Sun Mar 06 02:03:29 GMT 2016, previous max duration 8218ms] org.apache.jackrabbit.oak.plugins.segment.file.TarReader-GC Cleaned segments from data00233a.tar:
       37ec786e-a9f7-46eb-a3b5-ce5d4777ea01, f36051fe-d8c4-46d1-ac1d-081946389eb6, fae91ff2-8ca6-4ac1-a8d8-d4bd09b7f6a6, 16d87f09-721b-4155-a9c8-b8ecf471bfc3,
       e641f1a3-b323-44e6-aad0-7b894a1efb69, edc9d141-6c05-42c9-a2a2-d7130fd9c826, b602372c-b17a-448a-a8e9-8bdccc64fb82, acc2f032-07ba-46ed-a9c7-d3a05ab53d7a,
       a7323ed2-b2de-4006-ae51-e4f84165a0e4, cb320c70-5ca9-4ed1-a972-e87a6bba9f9b, f45afd7e-5417-42dd-a2f7-4624f74b6c6e, c66f66ef-cdd0-4327-abc6-bf910cb5768d,
       7f925a07-ff56-4613-ac8f-272a0e481926, 4ad044ec-3b2d-4c3e-aeb0-d5f5a04bc23e, 82f1c3aa-2e0c-421c-a033-e4ffcb6002c7, 1387655b-f633-4011-a55c-d9580e40929b,
       c50c94fc-2e8b-4904-a37f-0a33cc001312, 7915e9ce-bb9d-4628-ad6f-e7f2844b2399, e7cd013b-a147-426a-af29-fa025058a08a, f16d43b0-2113-4808-aea6-5910102e5c7d,
...
*31edad2e-e14b-463d-a6af-540bac6009f1*,
...,
*a949519a-8903-44f9-a17e-b6d83fb32186*,
...
{noformat}

*Note that system recovered after a restart so the corruption was transient*
",resilience,['lucene'],OAK,Bug,Blocker,2016-03-08 12:09:13,4
12947896,Include timestamp in journal log entries,"Currently the journal log has entries like below. At times while debugging crash or some issue we need to determine the probable root state at some point in the past. 

{noformat}
3dea11bb-bd43-4319-a37d-59df778a7271:260988 root
a7a509ac-a9d4-4e2c-a0d8-df71ebe123a0:259736 root
1d889da9-b41c-4889-a0cd-a9aa9dcc1737:259992 root
b78e4aa6-ec68-4e70-a364-f04ccbf4c3b3:259964 root
{noformat}

Currently there is no way to determine from above log what is the root state wrt time. So we need to workaround that by reading each root state and look for some path which has some time related property. To simplify such case it would be helpful to also include timestamp while adding a journal entry

{noformat}
1d889da9-b41c-4889-a0cd-a9aa9dcc1737:259992 root 1457408708772
b78e4aa6-ec68-4e70-a364-f04ccbf4c3b3:259964 root 1457408708899
{noformat}

*Key points*
# Timestamp comes at end
# Such a feature can be enabled without affecting backward compatibility - Just that new entries would have timestamp included
# {{JournalReader}} - Just reads the first column so would work as is",candidate_oak_1_4,['segmentmk'],OAK,Improvement,Minor,2016-03-08 03:45:39,4
12944433,Allow use of pre extrcated text cache for incremental indexing,"Pre Extraction support was implemented with an assumption that such big indexing would happen as part of reindex so it was used in reindex phase only. Reason to avoid using it in incremental indexing (non reindex case) were
# Incremental index would does not have text for newly added files. So checking with pre extracted cache would not be useful
# PreExtraction logic keeps in memory state (blobs_empty.txt,blobs_error.txt) which would then unnecessary hog memory.

However in some cases people make use of new incremental migration feature in upgrade. Which would lead to one big incremental indexing step once next migration is done and that would then not able to make use of pre extraction support.

So as a fix we should provide a policy option to ignore the reindex clause per admin setting",docs-impacting,['lucene'],OAK,Improvement,Minor,2016-02-25 09:26:21,4
12943081,FileStore.containsSegment returns alway true (almost),"{{FileStore.containsSegment()}} looks [funky|https://github.com/mduerig/jackrabbit-oak/blob/36cb3bf6e5078e3afa75581fb789eeca7b5df2e2/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/FileStore.java#L1197-L1197]. This ""optimisation"" causes it to always return {{true}}. 

{{containsSegment}} is used for deduplication and revision gc. The current implementation causes {{SNFE}} exceptions once gc is effective (as I experienced while working on OAK-3348). ",compaction gc stability,['segmentmk'],OAK,Bug,Critical,2016-02-24 20:52:14,3
12941554,NPE in oak-run graph when repository contains bulk segments,"{code}
Exception in thread ""main"" java.lang.NumberFormatException: null
	at java.lang.Long.parseLong(Long.java:404)
	at java.lang.Long.valueOf(Long.java:540)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentGraph.asLong(SegmentGraph.java:494)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentGraph.writeNode(SegmentGraph.java:464)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentGraph.writeSegmentGraph(SegmentGraph.java:173)
	at org.apache.jackrabbit.oak.run.GraphCommand.execute(GraphCommand.java:92)
	at org.apache.jackrabbit.oak.run.Mode.execute(Mode.java:63)
{code}

The cause for this is not properly checking the info map before deciding whether to print a bulk or a data node. ",tooling,['run'],OAK,Bug,Major,2016-02-23 21:53:59,3
12941553,NPE when running oak-run from within the IDE,"Running oak-run from within the IDE causes a {{NPE}}:

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:434)
	at java.util.Properties.load0(Properties.java:353)
	at java.util.Properties.load(Properties.java:341)
	at org.apache.jackrabbit.oak.run.Main.getProductVersion(Main.java:76)
	at org.apache.jackrabbit.oak.run.Main.getProductVersion(Main.java:66)
	at org.apache.jackrabbit.oak.run.Main.getProductInfo(Main.java:53)
	at org.apache.jackrabbit.oak.run.Main.printProductInfo(Main.java:86)
{code}

This is caused by not checking the return value of {{getResourceAsStream}} for {{null}} when trying to load {{/META-INF/maven/org.apache.jackrabbit/oak-run/pom.properties}}. That file is not on the class path when running from within the IDE. ",tooling,['run'],OAK,Bug,Major,2016-02-23 21:48:57,3
12941339,Some classes from o.a.j.o.plugins.segment.compaction should be exported,"Classes {{org.apache.jackrabbit.oak.plugins.segment.compaction.CompactionStrategy}} and {{org.apache.jackrabbit.oak.plugins.segment.compaction.CompactionStrategyMBean}} should be exported. The former is used in the public API of multiple classes from {{org.apache.jackrabbit.oak.plugins.segment.file}} and {{org.apache.jackrabbit.oak.plugins.segment}}, while the latter is used as interface type for a service registered in the whiteboard.",technical_debt,['segment-tar'],OAK,Bug,Major,2016-02-23 09:31:16,2
12940564,DocumentNodeStore: required server time accuracy,"The DocumentNodeStore currently requires that the local time and the persistence time differ at most 2 seconds.

I recently tried to run a cluster with two Windows machines, and despite them being configured to use the same NTP service, they were still 4..5 s off.

https://blogs.technet.microsoft.com/askds/2007/10/23/high-accuracy-w32time-requirements/ seems to confirm that by default, Windows can't provide the required accuracy.

One workaround seems to be to install custom ntp clients; but do we really want to require this?",documentation,"['doc', 'documentmk']",OAK,Documentation,Minor,2016-02-19 17:13:43,1
12938830,Expedite commits from the compactor,"Concurrent commits during compaction cause those to be re-compacted. Currently it seems that the compaction thread can end up waiting for some time to acquire the commit lock [1], which in turn causes more commits to pile up to be re-compacted. I think this could be improved by tweaking the lock such that the compactor could jump ahead of the queue. I.e. use a lock which can be acquired in expedited mode. 

[1] SegmentNodeStore#commitSemaphore",compaction gc perfomance,['segment-tar'],OAK,Improvement,Major,2016-02-12 15:42:17,3
12938753,Set online compaction default to paused,As online compaction is still not at the point where we would like it to have we will need to disable it by default for the upcoming major release. ,compaction gc,['segmentmk'],OAK,Bug,Blocker,2016-02-12 09:28:21,3
12936960,Add S3 datastore support for Text Pre Extraction,Text pre extraction feature introduced in OAK-2892 only supports FileDataStore. For files present in S3 we should add support for S3DataStore,docs-impacting,['run'],OAK,Improvement,Minor,2016-02-05 06:36:28,4
12936387,Add segment size to segment graph,The segment graph produced by {{oak-run graph}} should also contain the sizes of the segments. ,gc tooling,['run'],OAK,Improvement,Major,2016-02-03 16:45:09,3
12936278,Forward edges missing in SegmentGraph ,"The graph produced by {{oak-run graph}} does not include forward edges (i.e. references from older segments to newer segments). Such references where introduced with  OAK-1828. See also OAK-3864, where this has been fixed for the file store cleanup.

",cleanup gc technical_debt tooling,['run'],OAK,Bug,Major,2016-02-03 08:28:56,3
12934509,Log ids of segments being released for gc because of their age. ,When {{CompactionStrategy.CleanupType#CLEAN_OLD}} releases a segment for gc because of its age it should log a message. This helps to determine the root cause of a {{SNFE}}. ,cleanup gc,['segmentmk'],OAK,Technical task,Major,2016-01-27 16:28:26,3
12934109,oak-run primary/standby should check segment version,The primary and standby run modes should exit with an error if run on a store with non matching segment version.,resilience tooling,['run'],OAK,Technical task,Critical,2016-01-26 13:36:03,2
12934107,oak-run checkpoint should check segment version,The checkpoint runmode should exit with an error if run on a store with non matching segment version.,resilience tooling,['run'],OAK,Technical task,Major,2016-01-26 13:33:00,3
12934106,oak-run backup/recover should check segment version,Backup/restore should exit with an error if run on a store with non matching segment version. ,resilience tooling,['run'],OAK,Technical task,Major,2016-01-26 13:30:58,3
12933019,Commit fails even though change made it to the DocumentStore,"In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore. The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails. On the Oak side the call will be considered as failed, but the change was actually applied.

The issue can be reproduced with the test attached to OAK-1641 and a replica-set with 3 nodes. Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself.",resilience,"['core', 'documentmk']",OAK,Bug,Major,2016-01-21 08:54:42,1
12932451,Add filter capabilities to the segment graph run mode,I like to add a filter capability to {{oak-run graph}} to specify the inclusion criteria of segments via a regular expression.,tooling,['run'],OAK,Improvement,Major,2016-01-19 15:35:37,3
12932118,Robuster test expectations for FileStoreIT,{{FileStoreIT.testRecovery}} currently hard codes expected segment offsets. I would like to refactor this to make it more robust against changes in how exactly records are stored / de-duplicated. ,technical_debt,['segmentmk'],OAK,Improvement,Major,2016-01-18 10:45:13,3
12930181,Avoid commit from too far in the future (due to clock skews) to go through,"Following up [discussion|http://markmail.org/message/m5jk5nbby77nlqs5] \[0] to avoid bad commits due to misbehaving clocks. Points from the discussion:
* We can start self-destruct mode while updating lease
* Revision creation should check that newly created revision isn't beyond leaseEnd time
* Implementation done for OAK-2682 might be useful

[0]: http://markmail.org/message/m5jk5nbby77nlqs5",resilience,"['core', 'documentmk']",OAK,Improvement,Major,2016-01-14 16:16:46,1
12930165,Collision may mark the wrong commit,"In some rare cases it may happen that a collision marks the wrong commit. OAK-3344 introduced a conditional update of the commit root with a collision marker. However, this may fail when the commit revision of the condition is moved to a split document at the same time.",resilience,"['core', 'documentmk']",OAK,Bug,Minor,2016-01-14 15:14:50,1
12930078,Lucene index / compatVersion 2: search for 'abc!' does not work,"When using a Lucene fulltext index with compatVersion 2, then the following query does not return any results. When using compatVersion 1, the correct result is returned.

{noformat}
SELECT * FROM [nt:unstructured] AS c 
WHERE CONTAINS(c.[jcr:description], 'abc!') 
AND ISDESCENDANTNODE(c, '/content')
{noformat}

With compatVersion 1 and 2, searching for just 'abc' works. Also, searching with '=' instead of 'contains' works.",docs-impacting,['lucene'],OAK,Bug,Major,2016-01-14 08:43:32,4
12929747,Don't pass the compaction map to FileStore.cleanup,That argument is unused and I'll remove it thus. ,technical_debt,['segmentmk'],OAK,Technical task,Minor,2016-01-13 09:07:53,3
12929505,Refactor RecordWriter.write to always return a RecordId,"I think it would be cleaner if {{RecordId.write}} would always return a {{RecordId}} instead of depending on its type parametrisation and would like to refactor it to that respect.. 

This is also a pre-requisite for my work on OAK-3348 and might also be for OAK-3864. ",technical_debt,['segment-tar'],OAK,Technical task,Major,2016-01-12 14:54:19,3
12929502,Move createSegmentWriter() from FileStore to SegmentTracker,"I think it makes sense to move said method. This simplifies the code in various places as it somewhat decouples the concern ""writing segments"" from an implementation ({{FileStore}}). 

Also this is somewhat a prerequisite for my current work on OAK-3348.",technical_debt,['segmentmk'],OAK,Technical task,Major,2016-01-12 14:41:06,3
12929459,Filestore cleanup removes referenced segments,"In some situations {{FileStore.cleanup()}} may remove segments that are still referenced, subsequently causing a {{SNFE}}. 

This is a regression introduced with OAK-1828. 

{{FileStore.cleanup()}} relies on the ordering of the segments in the tar files: later segments only reference earlier segments. As we have seen in other places this assumption does not hold any more (e.g. OAK-3794, OAK-3793) since OAK-1828.
 {{cleanup}} traverses the segments backwards maintaining a list of referenced ids. When a segment is not in that list, it is removed. However, this approach does not work with forward references as those are only seen later when the segment has been removed already. 

cc [~alex.parvulescu], [~frm]",regression,['segmentmk'],OAK,Bug,Blocker,2016-01-12 11:05:42,3
12929232,Move integration tests in a different Maven module,"While moving the Segment Store and related packages into its own bundle, I figured out that integration tests contained in {{oak-core}} contribute to a cyclic dependency between the (new) {{oak-segment}} bundle and {{oak-core}}.

The dependency is due to the usage of {{NodeStoreFixture}} to instantiate different implementations of {{NodeStore}} in a semi-transparent way.

Tests depending on {{NodeStoreFixture}} are most likely integration tests. A clean solution to this problem would be to move those integration tests into a new Maven module, referencing the API and implementation modules as needed.",modularization technical_debt,[],OAK,Improvement,Major,2016-01-11 17:08:56,2
12929149,Review slow running tests,"Some of the tests executed during a normal {{mvn clean test}} execution seem to be very slow if compared with the rest of the suite. On my machine, some problematic tests are:

{noformat}
Running org.apache.jackrabbit.oak.spi.blob.FileBlobStoreTest
Tests run: 18, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 10.982 sec
Running org.apache.jackrabbit.oak.plugins.document.BasicDocumentStoreTest
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.961 sec
Running org.apache.jackrabbit.oak.plugins.document.BulkCreateOrUpdateTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.076 sec
Running org.apache.jackrabbit.oak.plugins.document.ConcurrentDocumentStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.054 sec
Running org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteServiceTest
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 982.526 sec
Running org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreTest
Tests run: 53, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.132 sec
Running org.apache.jackrabbit.oak.plugins.document.LastRevRecoveryAgentTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.068 sec
Running org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStorePerformanceTest
Tests run: 10, Failures: 0, Errors: 0, Skipped: 10, Time elapsed: 10.006 sec
Running org.apache.jackrabbit.oak.plugins.document.rdb.RDBDocumentStoreTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.017 sec
Running org.apache.jackrabbit.oak.plugins.document.VersionGCWithSplitTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.128 sec
Running org.apache.jackrabbit.oak.security.authentication.ldap.LdapLoginStandaloneTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.96 sec
{noformat}

These tests should be analyzed for potential errors or moved to the integration test phase.",test,[],OAK,Improvement,Major,2016-01-11 10:50:47,1
12928734,Simplify SegmentGraphTest,{{SegmentGraphTest}} has a somewhat complicated setup phase to build a segment store of a certain structure. This is will probably prove unreliable when underlying implementation details of how segments are written change (e.g. with OAK-3348). I would like to refactor the test such that it becomes independent of such implementation details. ,gc,['segmentmk'],OAK,Improvement,Minor,2016-01-08 21:02:51,3
12928689,oak-run compact should check segment version,Off line compaction should exit with a warning if run on a store with non matching segment version. It should provide a {{--force}} option to override this behaviour such that it can still be used for explicit upgrading. ,gc resilience tooling,['run'],OAK,Technical task,Critical,2016-01-08 17:06:48,3
12928688,TarMK tools should check whether they run against a matching version of the repository,"Running tools that write into a segment store might result in unwanted upgrading if the version of the tool uses a more recent segment version than the store. E.g. off line compaction currently upgrades segment format 10 to 11. 

To protected against inadvertent upgrades, a tool should check whether the segment version of the store matches its expectation (currently 11). If not, the tool should exit with a respective warning / error. For some tools it can make sense to provide a flag (e.g. {{--force}}) to override this. With this e.g. offline compaction can still be used for upgrading a segment store if explicitly told to do so. ",resilience tooling,['run'],OAK,Improvement,Critical,2016-01-08 17:02:40,3
12928653,Improve SegmentGraph resilience ,Currently {{SegmentGraph}} just bails out upon hitting a {{SNFE}}. I would like to improve this and include the error in the generated graph. ,gc resilience tooling,['run'],OAK,Improvement,Major,2016-01-08 14:38:28,3
12928039,Adjust package export declarations ,"We need to adjust the package export declarations such that they become manageable with our branch / release model. 

See http://markmail.org/thread/5g3viq5pwtdryapr for discussion.

I propose to remove package export declarations from all packages that we don't consider public API / SPI beyond Oak itself. This would allow us to evolve Oak internal stuff (e.g. things used across Oak modules) freely without having to worry about merges to branches messing up semantic versioning. OTOH it would force us to keep externally facing public API / SPI reasonably stable also across the branches. Furthermore such an approach would send the right signal to Oak API / SPI consumers regarding the stability assumptions they can make. 

An external API / SPI having a (transitive) dependency on internals might be troublesome. In doubt I would remove the export version here until we can make reasonable guarantees (either through decoupling the code or stabilising the dependencies). 

I would start digging through the export version and prepare an initial proposal for further discussion. 

/cc [~frm], [~chetanm], [~mmarth]",api modularization technical_debt,[],OAK,Task,Blocker,2016-01-06 15:31:32,3
12927995,Clean up the FileStore constructor,"The {{FileStore}} constructor consists of more than 150 LoC and is a mess as it depends on the order of initialisation, calls overrideable methods handles different concerns (read only vs. read / write) etc. 

We should up with a cleaner way of instantiating a file store.",technical_debt,['segmentmk'],OAK,Technical task,Major,2016-01-06 11:08:47,2
12923060,Disable compaction gain estimation if compaction is paused,"I think we should disable compaction estimation when compaction is paused. Estimation interferes with the caches, wastes CPU and IO cycles and is not essential for Oak's operation when compaction is disabled. The only reason it was unconditionally enabled initially is to gather the respective information in production. I think this has turned out to be not too useful so it is safe to disable. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-12-18 16:28:37,3
12923058,Provide option to pass external data store to oak-run check,The {{check}} run mode currently has no option to be run with an external data store. We should probably add such an option. Or/and ensure the check works probably for a segment store with external binaries even if no data store is present.,tooling,"['run', 'segmentmk']",OAK,Improvement,Major,2015-12-18 16:23:57,3
12922638,Clean up the fixtures code in core and jcr modules,"oak-core and oak-jcr modules uses the fixture mechanism to provide NodeStore implementations to the unit/integration tests. There is a few problems with the fixture implementation:

* the {{NodeStoreFixture}} class is duplicated between two modules and supports different set of options (eg. the oak-core version doesn't support the RDB node store at all, while the oak-jcr doesn't support MemoryNodeStore)
* it isn't possible to set the MongoDB URL manually from the Maven command line (it can be done for the RDB, though), which makes running the tests on a Mongo replica hard,
* the Mongo fixture doesn't remove the test database after the test is done.

There should be just one NodeStoreFixture implementation (the oak-jcr can reuse the oak-core version), supporting all values of the {{Fixture}} enum. The Mongo fixture should be more customisable and also should clean-up the database.",tech-debt,"['core', 'jcr']",OAK,Task,Major,2015-12-17 09:12:51,3
12922590,SessionMBean not getting registered due to MalformedObjectNameException,"Due to changes done in OAK-3477 SessionMBean is not getting registered as it contains ',' in the ObjectName. Unfortunately the exception thrown gets lost and this did not got detected so far

{noformat}
javax.management.MalformedObjectNameException: Invalid character in value: `,'
	at javax.management.ObjectName.checkValue(ObjectName.java:1009)
	at javax.management.ObjectName.construct(ObjectName.java:725)
	at javax.management.ObjectName.<init>(ObjectName.java:1425)
	at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardUtils.registerMBean(WhiteboardUtils.java:79)
	at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardUtils.registerMBean(WhiteboardUtils.java:68)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$RegistrationTask.run(RepositoryImpl.java:523)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
{noformat}

The name passed for ObjectName is 
{code}
{name=admin@session-11@Dec 17, 2015 9:57:11 AM, type=SessionStatistics}
{code}

",regresion,['core'],OAK,Bug,Minor,2015-12-17 04:29:15,4
12922345,Drop module oak-js,"The Oak checkout contains a module {{oak-js}}, which is mostly empty apart from a TODO statement. As we didn't work on this and AFAIK do not intend to work on this in the near future, I propose to drop the module for now. ",technical_debt,[],OAK,Task,Minor,2015-12-16 11:19:24,3
12921095,Tool for detecting references to pre compacted segments,"While OAK-3560 allows us to detect reference to pre compacted segments through manual inspection, we also need tooling to help detect such cases on site, during longevity tests and for UT/IT.  ",compaction gc tooling,['segmentmk'],OAK,Technical task,Major,2015-12-11 16:29:57,3
12920482,Compaction progress logger: reported number of nodes and binaries is too high,"Once a compaction cycle is through the compaction progress logger prints a message like:

{noforma}
Finished compaction: 26 nodes, 7 properties, 0 binaries.
{noformat}

However the number for nodes and properties includes those items deduplicated through the compaction map, effectively counting some items multiple times even though those where compacted only once and reused later. ",compaction gc,['segmentmk'],OAK,Bug,Major,2015-12-09 16:42:02,3
12920359,Test failure: HeavyWriteIT,"{{org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT}} failed on Jenkins:

{noformat}
heavyWrite[usePersistedMap: false](org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT)  Time elapsed: 106.519 sec  <<< ERROR!
java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.<init>(Segment.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.<init>(Segment.java:198)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:1177)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:224)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	at org.apache.jackrabbit.oak.plugins.segment.RecordId.getSegment(RecordId.java:88)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:506)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:79)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:381)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$UnconnectedHead.update(MemoryNodeBuilder.java:651)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$ConnectedHead.update(MemoryNodeBuilder.java:729)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.head(MemoryNodeBuilder.java:171)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.access$300(MemoryNodeBuilder.java:88)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$UnconnectedHead.update(MemoryNodeBuilder.java:650)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder$ConnectedHead.update(MemoryNodeBuilder.java:729)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.head(MemoryNodeBuilder.java:171)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.exists(MemoryNodeBuilder.java:273)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:506)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:515)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createProperties(HeavyWriteIT.java:156)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:148)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:149)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:129)
{noformat}

Seen at build 597",ci jenkins,['segmentmk'],OAK,Bug,Critical,2015-12-09 08:34:48,3
12920357,Test failure: ExternalSharedStoreIT,"{{org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT}} fails on Jenkins: 

{noformat}
testSync(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT)  Time elapsed: 54.498 sec  <<< FAILURE!
java.lang.AssertionError: expected:<{ root = { ... } }> but was:<{ root : { } }>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.testSync(DataStoreTestBase.java:104)
{noformat}

Seen at builds 163, 164, 598, 601
",ci jenkins,['tarmk-standby'],OAK,Bug,Major,2015-12-09 08:29:48,2
12920094,Implement tooling for tracing a node through the revision history,To diagnose certain issues with gc / checkpoints / indexing we need a tool to trace the evolution of a given node through the revision history. ,tooling,"['run', 'segmentmk']",OAK,Task,Major,2015-12-08 14:33:39,3
12919998,Move the Segment Store into its own bundle,"This epic tracks the work done to move the Segment Store into an independent bundle, detached from oak-core.",modularization technical_debt,['segmentmk'],OAK,Epic,Major,2015-12-08 08:23:33,2
12919659,Compactor should log revisions acting upon,"For post mortem analysis it would be helpful to have the revisions that where involved in a compaction run. I.e. the revision that was compacted, the revisions of the cycles (if any) and the revision that is ultimately applied?",compaction gc,['segmentmk'],OAK,Technical task,Blocker,2015-12-07 11:30:25,3
12919023,Offline compaction doesn't clean up unreferenced tar files,This is a regression introduced with OAK-3329 where cleaning up unreferenced tar files was taken out of {{FileStore#cleanup}}. ,cleanup gc,['segmentmk'],OAK,Bug,Major,2015-12-04 17:00:15,3
12917203,Change default of compaction.forceAfterFail to false,"Having {{compaction.forceAfterFail}} set to {{true}} will block repository writes for an extended period of time (minutes, probably hours) if all previous compaction cycles couldn't catch up with the latest changes. I think this is not acceptable and we should change the default to {{false}}: if compaction is not able to catch up the recommendation should be to move it to a quieter time. ",compaction gc,['segmentmk'],OAK,Task,Major,2015-12-01 09:58:17,3
12917183,Improve handling of IOException,"{{SegmentStore.writeSegment}} doesn't specify its behaviour in the face of IO errors. Moreover {{FileStore.writeSegment}} just catches any {{IOException}} and throws a {{RuntimeException}} with the former as its cause. 

I think we need to clarify this as an immediate cause of the current state is that some of the {{SegmentWriter}} write methods *do* throw an {{IOException}} and some *don't*. ",resilience technical_debt,['segmentmk'],OAK,Improvement,Major,2015-12-01 08:41:23,3
12917177,More resilient BackgroundThread implementation,"Currently {{BackgroundThread}} dies silently when hit by an uncaught exception. We should log a warning. 

Also calling {{Thread#start}} from within the constructor is an anti-pattern as it exposes {{this}} before fully initialised. This is potentially causing OAK-3303. ",resilience,['segmentmk'],OAK,Improvement,Major,2015-12-01 08:28:50,3
12916934,Improve SegmentMK resilience,Epic for collection SegmentMK resilience improvements,resilience,['segment-tar'],OAK,Epic,Major,2015-11-30 10:59:05,3
12916638,Decouple SegmentBufferWriter from SegmentStore,"Currently {{SegmentBufferWriter.flush()}} directly calls {{SegmentStore.writeSegment()}} once the current segment does not have enough space for the next record. We should try to cut this dependency as {{SegmentBufferWriter}} should only be concerned with providing buffers for segments. Actually writing these to the store should be handled by a higher level component. 

A number of deadlock (e.g. (OAK-2560, OAK-3179, OAK-3264) we have seen is one manifestation of this troublesome dependency. ",technical_debt,['segment-tar'],OAK,Technical task,Major,2015-11-27 13:34:17,3
12916181,Partial re-index from last known good state,"ATM indexes break (by whatever circumstances) users need to perform a full re-index. Depending on the size off the repository this can take a long time.
If the user knows that the indexes were in a good state at a certain revision in the past then it would be very useful, if the user could trigger a ""partial"" re-index where only the content added after a certain revision was updated in the index.",resilience,"['indexing', 'lucene']",OAK,New Feature,Major,2015-11-25 15:32:13,4
12915355,Potential test failure: CompactionAndCleanupIT#testMixedSegments,{{CompactionAndCleanupIT#testMixedSegments}} might fail under some circumstances. It can be certainly be made to fail by increasing concurrency. I suspect this to be caused by OAK-3348. ,compaction gc,['segmentmk'],OAK,Bug,Major,2015-11-23 14:59:44,3
12914080,Remove HierarchicalCacheInvalidator,As discussed in OAK-2187 and due to changes done in OAK-3002 HierrachialCacheInvalidator is now redundant and should be removed. ,technical_debt,['mongomk'],OAK,Task,Minor,2015-11-18 11:03:37,4
12913682,Inconsistent read of hierarchy ,"This is similar to OAK-3388, but about hierarchy information like which child nodes exist at a given revision of the parent node. This issue only occurs in a cluster.",resilience,"['core', 'documentmk']",OAK,Bug,Major,2015-11-17 12:35:04,1
12911832,Backport TarMK revision gc related issues,"Some of the issues related to TarMK revision gc should be back ported to the branches. This issue is for keeping track of which issues and which svn revisions we consider for back porting. The task consists of the following steps:

# Identify issue to back port
# Merge the respective commits into a private forks of the 1.0 and 1.2 branches
# Run tests on builds from the private forks
# On success merge the private forks to the 1.0 and 1.2 branches and update the fix versions of the respective issues. 
    * Update the svn merge info with the respective merged svn revisions. 
    * Update the fix versions of the affected issues.

[~dhasler]: FYI
[~alex.parvulescu], [~frm]: please refrain from merging potential conflicting changes into the branches in the meanwhile. 

",compaction gc,['segmentmk'],OAK,Task,Major,2015-11-10 11:12:36,3
12908277,Tooling for writing segment graphs to a file,[Gephi|https://gephi.org/] turned out to be very valuable for examining segment graphs. I would like to add some tooling so we could dump the segment graph of a {{FileStore}} to a file. ,tooling,['run'],OAK,New Feature,Major,2015-10-27 16:36:57,3
12908135,Use write concern of w:majority when connected to a replica set,"Currently while connecting to Mongo MongoDocumentStore relies on default write concern provided as part of mongouri. 

Recently some issues were seen where Mongo based Oak was connecting to 3 member replica set and there were frequent replica state changes due to use of VM for Mongo. This caused data loss and corruption of data in Oak.

To avoid such situation Oak should default to write concern of majority by default. If some write concern is specified as part of mongouri then that should take precedence. This would allow system admin to take the call of tweaking write concern if required and at same time allows Oak to use the safe write concern.",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-10-27 06:41:11,1
12907331,o.a.j.o.api should not depend on Guava,The o.a.j.o.api has multiple dependencies on classes exported by the Google Guava bundle. The the o.a.j.o.api package should be made independent from Google Guava.,technical_debt,[],OAK,Improvement,Major,2015-10-23 08:52:41,2
12905902,Unchecked assignements in calls to performVoid(),"Most if not all calls to {{SessionDelegate#performVoid}} pass a raw type to that method instead of parametrizing it with the {{Void}} type, which leads to an ""unchecked assignment"" warning. ",technical_debt,['jcr'],OAK,Improvement,Major,2015-10-19 10:33:05,3
12904464,Test failure: CompactionMapTest.removeSome,"Said test fails sporadically:

{noformat}
at org.junit.Assert.assertNull(Assert.java:562)
at org.apache.jackrabbit.oak.plugins.segment.CompactionMapTest.removeSome(CompactionMapTest.java:156)
{noformat}

This is a regression introduced with OAK-3501: the {{recent}} map gets not cleared when {{segmentIdMap}} is empty. This can happen when a recent key is removed again while there are no other changes. ",compaction gc,['segmentmk'],OAK,Bug,Major,2015-10-13 07:26:55,3
12904208,Uniformization of compaction log messages,"The logs generated during different phases of tar garbage collection (compaction) are currently quite heterogenous and difficult to grep/parse.

I propose with the attached patch to uniformize these logs, changing the following:
# all logs start with the prefix {{TarMK GargabeCollection \{\}#:}}
# different phases of garbage collection are easier to identify by the first word after prefix, e.g. estimation, compaction, cleanup
# all values are also printed in a standard unit, with the following format: {{<human_readable_value> (<standard_unit_value>)}}. This makes extraction of information much easier.
# messages corresponding to the same cycle (run) can be grouped by including the runId in the prefix.

Note1: I don't have enough visibility, but the changes might impact any system relying on the old format. Yet, I've seen they have changed before so this might not be a real concern.

Note2: the runId is implemented as a static variable, which is reset every time the class is reloaded (e.g. at restart), so it is unique only during one run.

Below you can find an excerpt of old logs and new logs to compare:

NEW:
{code}
12.10.2015 16:11:56.705 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: started
12.10.2015 16:11:56.707 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: estimation started
12.10.2015 16:11:59.275 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: estimation completed in 2.569 s (2567 ms). Gain is 16% or 1.1 GB/1.3 GB (1062364160/1269737472 bytes), so running compaction
12.10.2015 16:11:59.275 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: compaction started, strategy=CompactionStrategy{paused=false, cloneBinaries=false, cleanupType=CLEAN_OLD, olderThan=36000000, memoryThreshold=5, persistedCompactionMap=true, retryCount=5, forceAfterFail=true, compactionStart=1444659116706, offlineCompaction=false}
12.10.2015 16:12:05.839 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.Compactor Finished compaction: 420022 nodes, 772259 properties, 20544 binaries.
12.10.2015 16:12:07.459 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:11:56 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: compaction completed in 8.184 s (8183 ms), after 0 cycles
12.10.2015 16:12:11.912 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:12:11 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: cleanup started. Current repository size is 1.4 GB (1368899584 bytes)
12.10.2015 16:12:12.368 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:12:11 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: cleanup marking file for deletion: data00008a.tar
12.10.2015 16:12:12.434 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 16:12:11 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK GarbageCollection #1: cleanup completed in 522.8 ms (522 ms). Post cleanup size is 1.2 GB (1217132544 bytes)and space reclaimed 151.8 MB (151767040 bytes). Compaction map weight/depth is 0 B/1 (0 bytes/1).
{code} 

OLD:
{code}
12.10.2015 15:54:55.115 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK compaction started
12.10.2015 15:54:56.082 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore Estimated compaction in 967.6 ms, gain is 7% (1083809280/1170960384) or (1.1 GB/1.2 GB), so running compaction
12.10.2015 15:54:56.083 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK compaction running, strategy=CompactionStrategy{paused=false, cloneBinaries=false, cleanupType=CLEAN_OLD, olderThan=36000000, memoryThreshold=5, persistedCompactionMap=true, retryCount=5, forceAfterFail=true, compactionStart=1444658095115, offlineCompaction=false}
12.10.2015 15:55:01.986 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.Compactor Finished compaction: 419878 nodes, 771824 properties, 20542 binaries.
12.10.2015 15:55:03.273 *INFO* [TarMK compaction thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:54:55 CEST 2015, previous max duration 0ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK compaction completed after 0 cycles in 7190ms
12.10.2015 15:55:08.032 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:55:08 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK revision cleanup started. Current repository size 1.3 GB
12.10.2015 15:55:08.719 *INFO* [TarMK flush thread [/Users/volteanu/workspace/test/qp/quickstart-author-4502/crx-quickstart/repository/segmentstore], active since Mon Oct 12 15:55:08 CEST 2015, previous max duration 10ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK revision cleanup completed in 688.0 ms. Post cleanup size is 1.3 GB and space reclaimed 0. Compaction map weight/depth is 0 B/1.
{code}",compaction gc,['core'],OAK,Improvement,Major,2015-10-12 15:11:48,3
12903762,Improve logging during cleanup,We should have better logging during cleanup. E.g. why a file has been skipped / cleaned etc. ,cleanup gc,['segmentmk'],OAK,Improvement,Major,2015-10-09 15:01:22,3
12903311,Remove DocumentNodeStore.diff(),"The method is only used by the DocumentMK class, which is now considered a test helper (OAK-2907) and part of the API anymore.

The method should be removed from the DocumentNodeStore and functionality moved to the DocumentMK.find() method.",technical_debt,"['core', 'mongomk']",OAK,Improvement,Minor,2015-10-08 09:32:54,1
12902999,MemoryDiffCache should also check parent paths before falling to Loader (or returning null),"Each entry in {{MemoryDiffCache}} is keyed with {{(path, fromRev, toRev)}} for the list of modified children at {{path}}. A diff calcualted by {{DocumentNodeStore.diffImpl}} at '/' (passively via loader) or {{JournalEntry.applyTo}} (actively) fill each path for which there are modified children (including the hierarchy)

But, if an observer calls {{compareWithBaseState}} on a unmodified sub-tree, the observer will still go down to {{diffImpl}} although cached parent entry can be used to answer the query.",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-10-07 13:55:53,1
12902989,Deadlock when closing a concurrently used FileStore 2.0,"A deadlock was detected while stopping the {{SegmentCompactionIT}} using the exposed MBean.

{noformat}
""main@1"" prio=5 tid=0x1 nid=NA waiting for monitor entry
 waiting for pool-1-thread-10@2111 to release lock on <0xae8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.dropCache(SegmentWriter.java:871)
  at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.close(FileStore.java:1031)
  - locked <0xae7> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT.tearDown(SegmentCompactionIT.java:282)

""pool-1-thread-10@2111"" prio=5 tid=0x1d nid=NA waiting for monitor entry
  java.lang.Thread.State: BLOCKED
 blocks main@1
 waiting for main@1 to release lock on <0xae7> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
  at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.writeSegment(FileStore.java:1155)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:253)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.prepare(SegmentWriter.java:350)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:468)
  - locked <0xae8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:719)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1211)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1156)
  at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1147)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1175)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:451)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:474)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:530)
  at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:208)
{noformat}",resilience,['segmentmk'],OAK,Bug,Major,2015-10-07 13:27:14,3
12902929,LastRevRecovery for self async?,"Currently, when a cluster node starts and discovers that it wasn't properly shutdown, it first runs the complete LastRevRecovery and only continues startup when done.

However, when it fails to acquire the recovery lock, which implies that a different cluster node is already running the recovery on its behalf, it simply skips recovery and continues startup?

So what is it? Is running the recovery before proceeding critical or not? If it is, this code in {{LastRevRecoveryAgent}} needs to change:

{code}
        //TODO What if recovery is being performed for current clusterNode by some other node
        //should we halt the startup
        if(!lockAcquired){
            log.info(""Last revision recovery already being performed by some other node. "" +
                    ""Would not attempt recovery"");
            return 0;
        }
{code}

If it's not critical, we may want to run the recovery always asynchronously. 
cc [~mreutegg]  and [~chetanm]",resilience,['documentmk'],OAK,Bug,Major,2015-10-07 08:58:41,1
12902653,CompactionMapTest does not close file store,Each test in the class retains about 16MB of heap and subsequent tests may fail with an OOME. E.g. see recent build failure on travis: https://travis-ci.org/apache/jackrabbit-oak/builds/83848567,cleanup compaction gc,"['core', 'segmentmk']",OAK,Bug,Minor,2015-10-06 10:04:52,1
12902425,Confusing SNFE whith oak-run debug,"Running 

{{oak-run debug  /path/to/segmentStore}}

can result in {{SNFE}} s being logged if the file store has been compacted before. This can happen even though the repository is actually consistent according to {{oak-run check}}. 

The reason is {{debug}} traversing all node states of all record ids in all tar files. When a previous cleanup of a tar file did not reach 25% gain it will not clean up that file leaving behind segments possibly containing node states pointing to limbo. Those nodes would result in said {{SNFE}} of {{oak-run}} debug. But as those node states are not reachable from the head node state the repository is still consistent itself. ",cleanup gc tooling,['run'],OAK,Improvement,Major,2015-10-05 14:16:10,3
12902419,NodeDocument.getNodeAtRevision can go into property history traversal when latest rev on current doc isn't committed,"{{NodeDocument.getNodeAtRevision}} tried to look at latest revisions entries for each property in current document. But it just looks at the *last* entry for a given property. In case this last entry isn't committed, the code would go into previous documents to look for a committed value.

(cc [~mreutegg])",performance,"['core', 'mongomk']",OAK,Bug,Major,2015-10-05 13:12:45,1
12902115,Replace BackgroundThread with Scheduler,I think we should replace the background thread with some kind of a scheduler. The goal would be to decouple threading from scheduling. IMO threads should not be managed by the application but by the container. ,technical_debt,['segment-tar'],OAK,Technical task,Major,2015-10-02 13:07:22,2
12900998,Improve the logging capabilities of offline compaction,"The offline compaction instantiates {{FileStore}} using a deprecated constructor. This constructor forces a no-op {{GCMonitor}} that swallows log messages and caught exception.

It would be more appropriate to create the {{FileStore}} using the corresponding {{Builder}}. This has the side effect of configuring a {{LoggingGCMonitor}}, which provides way more information than the current default.",compaction gc,"['core', 'run']",OAK,Improvement,Major,2015-09-28 10:52:10,2
12895116,Background update may create journal entry with incorrect id,The conflict check does not consider changes that are made visible between the rebase and the background read.,resilience,"['core', 'mongomk']",OAK,Bug,Critical,2015-09-21 13:12:19,1
12864161,Avoid instanceof check in LastRevRecoveryAgent,"Similar to OAK-3390, the instanceof check in LastRevRecoveryAgent does not work when the MongoDocumentStore is wrapped.",technical_debt,"['core', 'mongomk']",OAK,Bug,Major,2015-09-15 07:27:07,1
12863902,Multiplexing NodeStore support in Oak layer,"Supporting multiplexing repository would have impact on various places in Oak design. There are various sub components in Oak which maintain there own storage built on top of NodeStore. For e.g. indexes are stored within NodeStore, permissions are also stored within NodeStore. Adding multiplexing support would impact such stores in following ways

The most basic application of multiplexing support is to support private and shared storage. Under this an Oak application would have a private store and a shared store. Content under certain paths would be stored under private repo while all other content is stored under shared repo

# *Writing* - Any content written via JCR API passes through some {{CommitHooks}}. These hooks are responsible for updating the indexes, permission store etc. Now if any path say /foo/bar gets modified the commits hooks would need to determine under which path in NodeStore should the derived data (index entries, permission etc) should be stored. For simple case of private and shared store where we have 2 sets of paths private and shared these hooks would need to be aware of that and use different path in NodeStore to store the derived content. Key point to note here that any such storage has to differentiate wether the path from which the content is being derived is a private path or shared path

# *Reading* - Reading requirement compliments the writing problem. While performing any JCR operation Oak might need to invoke QueryIndex, PermissionStore etc. These stores in turn would need to perform a read from there storage area within NodeStore. For multiplexing support these components would then need to be aware that there storage can exist in both shared and private stores

h4. Terms Used

# _private repo_ (PR) - Set of paths which are considered private to the application. Tentative example /lib,/apps
# _shared repo_ (SR) - Set of paths which are considered shared and different versions of the application can perform read and write operations on them. Tentative example /content, /etc/workflow/instances
# {{PathToStoreMapper}} - Responsible for mapping a path to store type. For now it can just answer either PR or SR. But the concept can be generalized 

Aim of this story is to prototype changes in Oak layer in a fork to asses the impact on current implementation",multiplexing,['core'],OAK,Epic,Major,2015-09-14 15:17:51,4
12863035,Inconsistent read in cluster with clock differences,This issue is similar to OAK-2929 but related to how the DocumentNodeStore reads a node state when there is a clock difference between multiple cluster nodes. The node state read from a NodeDocument may not be correct when there is a clock difference.,resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-09-10 12:39:42,1
12862389,Collapsing external events in BackgroundObserver even before queue is full leads to JournalEntry not getting used,"BackgroundObserver currently merges external events if the last one in queue is also an external event. This leads to diff being done for a revision pair which is different from the ones pushed actively into cache during backgroud read (using JournalEntry) i.e. diff queries for {{diff(""/a/b"", rA, rC)}} while background read had pushed results of {{diff(""/a/b"", rA, rB)}} and {{diff(""/a/b"", rB, rC)}}.

(cc [~mreutegg], [~egli], [~chetanm], [~mduerig])",performance resilience,['core'],OAK,Improvement,Major,2015-09-08 11:49:32,3
12862325,Boosting fields not working as expected,"When the boost support was added the intention was to support a usecase like 

{quote}
For the fulltext search on a node where the fulltext content is derived from multiple field it should be possible to boost specific text contributed by individual field. Meaning that if a title field is boosted more than description, the title (part) in the fulltext field will mean more than the description (part) in the fulltext field.
{quote}

This would enable a user to perform a search like _/jcr:root/content/geometrixx-outdoors/en//element(*, cq:Page)\[jcr:contains(., 'Keyword')\]_ and get a result where pages having 'Keyword' in title come above in search result compared to those where Keyword is found in description.

Current implementation just sets the boost while add the field value to fulltext field with the intention that Lucene would use the boost as explained above. However it does not work like that and boost value gets multiplies with other field and hence boosting does not work as expected",doc-impacting,['lucene'],OAK,Bug,Major,2015-09-08 06:27:07,4
12862191,Reduce PerfLogger isDebugEnabled overhead,"If a node is cached, 1/4 of the time which is used to call DocumentNodeStore.getNode is spent in PerfLogger.start and PerfLogger.end just for checking whether or not debug logging is enabled (this is likely much less if no TurboFilters are used).

To reduce the overhead of the PerfLogger, it should not check if debug is enabled in end() if start is below 0 anyway. Moreover, it would help to check only every second if debug is really enabled.",performance,['core'],OAK,Improvement,Minor,2015-09-07 10:06:36,1
12861787,Incremental compaction,"This is OAK-3349 taken to the extreme: given a segment that is almost not referenced any more we could just rewrite the still referenced content. That is, say a segment contains two properties reachable from the current root node state and all its remaining content is not reachable from the root node state. In that case we could rewrite these two properties and create a new root node state referencing the rewritten properties. This would effectively make the segment eligible for being gc-ed. 
Such an approach would start from segments that are sparse and compact these instead of compacting everything as we currently do, which might cause a lot of copying around stuff that already is compact. The challenging part here is probably finding the segments that are sparse as this involves inverting the reference graph. 

Todo: Asses feasibility and impact, implement prototype.",compaction gc scalability,['segment-tar'],OAK,New Feature,Minor,2015-09-04 09:00:14,3
12861783,Partial compaction,"On big repositories compaction can take quite a while to run as it needs to create a full deep copy of the current root node state. For such cases it could be beneficial if we could partially compact the repository thus splitting full compaction over multiple cycles. 
Partial compaction would run compaction on a sub-tree just like we now run it on the full tree. Afterwards it would create a new root node state by referencing the previous root node state replacing said sub-tree with the compacted one. 

Todo: Asses feasibility and impact, implement prototype.",compaction gc scalability,['segment-tar'],OAK,New Feature,Major,2015-09-04 08:45:58,2
12861779,Cross gc sessions might introduce references to pre-compacted segments,"I suspect that certain write operations during compaction can cause references from compacted segments to pre-compacted ones. This would effectively prevent the pre-compacted segments from getting evicted in subsequent cleanup phases. 

The scenario is as follows:
* A session is opened and a lot of content is written to it such that the update limit is exceeded. This causes the changes to be written to disk. 
* Revision gc runs causing a new, compacted root node state to be written to disk.
* The session saves its changes. This causes rebasing of its changes onto the current root (the compacted one). At this point any node that has been added will be added again in the sub-tree rooted at the current root. Such nodes however might have been written to disk *before* revision gc ran and might thus be contained in pre-compacted segments. As I suspect the node-add operation in the rebasing process *not* to create a deep copy of such nodes but to rather create a *reference* to them, a reference to a pre-compacted segment is introduced here. 

Going forward we need to validate above hypothesis, assess its impact if necessary come up with a solution.
",cleanup compaction gc,['segment-tar'],OAK,Improvement,Critical,2015-09-04 08:32:39,3
12861773,Ineffective cleanup after compaction due to references to root,"The cleanup phase after a compaction run is currently not able to remove the pre compacted segments as the previous (pre-compacted) root is still being referenced. Those references are coming from:

* The {{before}} [local variable|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/file/FileStore.java#L653] in {{FileStore.flush}}.
* The [current head|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentNodeStore.java#L85-L85] of the {{SegmentNodeStore}}.",cleanup compac gc,['segmentmk'],OAK,Improvement,Major,2015-09-04 08:17:52,3
12861552,Commit may add collision marker for committed revision,"It may happen that a commit adds a collision marker for a revision which is already committed. {{Collision.markCommitRoot()}} does not perform a conditional update when it adds the collision marker. Though, it checks the document after the update if the marked revision is committed.",resilience,"['core', 'mongomk']",OAK,Bug,Minor,2015-09-03 12:20:39,1
12861256,SplitOperations purges _commitRoot entries too eagerly,"OAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.

The purge operation is too eager and removes all such entries, which may result in an undetected hierarchy conflict.",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-09-02 08:55:22,1
12861236,FileStore lock contention with concurrent writers,"Concurrently writing to the file store can lead to a sever lock contention in {{FileStore#readSegment}}. That method searches the current {{TarWriter}} instance for the segment once it could not be found in any of the {{TarReader}} instances. This is the point where synchronizes on the {{FileStore}} instance, which leads to  the contention. 
The effect is only observable once the segment cache becomes full and reads actually need to go to the file store. Thus a possible improvement could be to pin segments from the current tar writer to the cache. Alternatively we could try to ease locking by employing read/write locks where possible. ",compaction,['segmentmk'],OAK,Improvement,Major,2015-09-02 07:26:38,3
12861233,TarMK cleanup blocks writers,"TarMK cleanup exclusively locks the {{FileStore}}, which causes concurrent writers to block until cleanup finished. Initially cleanup was expected to be reasonably fast, however I have seen it taking dozens of minutes under certain circumstances (most likely many tar files with many small segments, aka OAK-2896).",cleanup gc,['segmentmk'],OAK,Improvement,Major,2015-09-02 07:06:17,3
12860662,ConcurrentModificationException when running SegmentOverflowExceptionIT,"{noformat}
Running org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2,426.873 sec <<< FAILURE!
run(org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT)  Time elapsed: 2,426.373 sec  <<< ERROR!
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)
	at java.util.ArrayList$Itr.next(ArrayList.java:831)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.wasCompactedTo(CompactionMap.java:60)
	at org.apache.jackrabbit.oak.plugins.segment.Record.wasCompactedTo(Record.java:64)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.equals(SegmentBlob.java:213)
	at com.google.common.base.Objects.equal(Objects.java:55)
	at org.apache.jackrabbit.oak.plugins.memory.AbstractPropertyState.equal(AbstractPropertyState.java:53)
	at org.apache.jackrabbit.oak.plugins.memory.AbstractPropertyState.equals(AbstractPropertyState.java:90)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1176)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT.run(SegmentOverflowExceptionIT.java:130)
{noformat}

This is caused by concurrently accessing the underlying list of maps in {{CompactionMap#remove}}. ",compaction gc,['segmentmk'],OAK,Bug,Major,2015-08-31 12:15:08,3
12859572,Self recovering instance may not see all changes,"When a DocumentNodeStore instance is killed and restarted, the _lastRev recovery mechanism is triggered on startup. It may happen that the restarted instance does not see all changes that were recovered.",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-08-27 11:08:40,1
12858956,SNFE in SegmentOverflowExceptionIT ,"{{org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowExceptionIT}} can fail with an {{SNFE}}. This is somewhat expected due to the low segment retention time used for this test. That time is apparently needed for this test to reproduce the original issue. So I'd rather not touch it. 

I propose to ignore that exception and retry a couple of times until failing the test. ",test,['segmentmk'],OAK,Improvement,Major,2015-08-26 11:45:50,3
12858640,SegmentOverflowExceptionIT runs forever unless it fails,Currently {{SegmentOverflowExceptionIT}} runs forever or until it fails. We should add a time out after which the test is considered passed.,test,['segmentmk'],OAK,Improvement,Major,2015-08-25 14:36:20,3
12858615,Revision gc blocks repository shutdown,Shutting down the repository while revision gc is running might block for a long time until either compaction estimation/compaction or clean up has finished. We should provide a way to interrupt those operations for a timely shut down. ,cleanup compaction gc,['segmentmk'],OAK,Improvement,Major,2015-08-25 13:14:25,3
12858277,Improve DocumentMK resilience,Collection of DocMK resilience improvements,resilience,"['mongomk', 'rdbmk']",OAK,Epic,Critical,2015-08-24 11:57:55,1
12858273,Improve indexing resilience,As discussed bilaterally grouping the improvements for indexer resilience in this issue for easier tracking,resilience,['lucene'],OAK,Epic,Critical,2015-08-24 11:52:32,4
12857823,Deadlock between persisted compaction map and cleanup 2,"Just seen this deadlock while running {{SegmentCompactionIT}}:

{noformat}
""pool-1-thread-47"":
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:910)
	- waiting to lock <0x0000000700110bd0> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:211)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	- locked <0x0000000700328b88> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:121)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:103)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1098)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:85)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:81)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.remove(MemoryNodeBuilder.java:355)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.modify(SegmentCompactionIT.java:448)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:430)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:406)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
""TarMK flush thread [target/SegmentCompactionIT9065337410200765612dir], active since Fri Aug 21 06:53:18 GMT+00:00 2015, previous max duration 40846ms"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:145)
	- waiting to lock <0x0000000700328b88> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.compress(PersistedCompactionMap.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.remove(PersistedCompactionMap.java:155)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.remove(CompactionMap.java:108)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.cleanup(FileStore.java:699)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:628)
	- locked <0x0000000700110bd0> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	- locked <0x000000070017f1c0> (a java.util.concurrent.atomic.AtomicReference)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$1.run(FileStore.java:413)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.jackrabbit.oak.plugins.segment.file.BackgroundThread.run(BackgroundThread.java:70)
{noformat}",cleanup compaction gc,['segmentmk'],OAK,Bug,Major,2015-08-21 11:30:38,3
12857550,Optimize NodeDocument.getNewestRevision(),"Most of the time NodeDocument.getNewestRevision() is able to quickly identify the newest revision, but sometimes the code falls to a more expensive calculation, which attempts to read through available {{_revisions}} and {{_commitRoot}} entries. If either of those maps are empty, the method will go through the entire revision history.",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-08-20 14:45:01,1
12856839,DocumentNodeStore.retrieve() should not throw IllegalArgumentException,"{{DocumentNodeSTore#retrieve(checkpoint)}} may throw an {{IllegalArgumentException}} via {{Revision.fromString(checkpoint)}}.

The javadocs say that it returns a {{NodeState}} or {{null}}. The exception prevents recovery of {{AsyncIndexUpdate}} from a bad recorded checkpoint.",resilience,"['core', 'mongomk', 'rdbmk']",OAK,Improvement,Minor,2015-08-18 07:32:20,1
12856168,Deadlock when closing a concurrently used FileStore,"A deadlock was detected while stopping the {{SegmentCompactionIT}} using the exposed MBean.

{noformat}
Found one Java-level deadlock:
=============================
""pool-1-thread-23"":
  waiting to lock monitor 0x00007fa8cf1f0488 (object 0x00000007a0081e48, a org.apache.jackrabbit.oak.plugins.segment.file.FileStore),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x00007fa8cc015ff8 (object 0x00000007a011f750, a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter),
  which is held by ""pool-1-thread-23""

Java stack information for the threads listed above:
===================================================
""pool-1-thread-23"":
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.writeSegment(FileStore.java:948)
	- waiting to lock <0x00000007a0081e48> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.flush(SegmentWriter.java:228)
	- locked <0x00000007a011f750> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.prepare(SegmentWriter.java:329)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:447)
	- locked <0x00000007a011f750> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:698)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1190)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:85)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:81)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setChildNode(MemoryNodeBuilder.java:346)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeAdded(AbstractRebaseDiff.java:211)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:527)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:531)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord$2.childNodeChanged(MapRecord.java:404)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:488)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:394)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.spi.state.AbstractRebaseDiff.childNodeChanged(AbstractRebaseDiff.java:219)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:488)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compareBranch(MapRecord.java:565)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.compare(MapRecord.java:470)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:583)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.prepare(SegmentNodeStore.java:446)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.optimisticMerge(SegmentNodeStore.java:471)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.execute(SegmentNodeStore.java:527)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:205)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:426)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:399)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
""main"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.dropCache(SegmentWriter.java:850)
	- waiting to lock <0x00000007a011f750> (a org.apache.jackrabbit.oak.plugins.segment.SegmentWriter)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.close(FileStore.java:830)
	- locked <0x00000007a0081e48> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT.tearDown(SegmentCompactionIT.java:266)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:36)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)

Found 1 deadlock.
{noformat}",resilience,['segmentmk'],OAK,Bug,Critical,2015-08-14 14:06:12,3
12851653,Deadlock between persisted compaction map and cleanup,"Just seen this deadlock while running {{SegmentCompactionIT}}:

{noformat}
""TarMK flush thread [target/SegmentCompactionIT3250704011919039778dir], active since Wed Aug 05 09:25:57 GMT+00:00 2015, previous max duration 2325ms"" daemon prio=10 tid=0x00007f5674872800 nid=0x5dc8 waiting for monitor entry [0x00007f5666a00000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:145)
	- waiting to lock <0x0000000707fc7fe8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.compress(PersistedCompactionMap.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.remove(PersistedCompactionMap.java:155)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.remove(CompactionMap.java:108)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.cleanup(FileStore.java:694)
	- locked <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:628)
	- locked <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	- locked <0x00000007000aed60> (a java.util.concurrent.atomic.AtomicReference)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$1.run(FileStore.java:413)
	at java.lang.Thread.run(Thread.java:745)
	at org.apache.jackrabbit.oak.plugins.segment.file.BackgroundThread.run(BackgroundThread.java:70)

""pool-1-thread-34"" prio=10 tid=0x00007f55ec002800 nid=0x5dea waiting for monitor entry [0x00007f56648de000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:904)
	- waiting to lock <0x000000070017b330> (a org.apache.jackrabbit.oak.plugins.segment.file.FileStore)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.readSegment(SegmentTracker.java:210)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:149)
	- locked <0x0000000707fc7fe8> (a org.apache.jackrabbit.oak.plugins.segment.SegmentId)
	at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:400)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:215)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:121)
	at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:103)
	at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1074)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1098)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1154)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1135)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:399)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1126)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:433)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomWriter.call(SegmentCompactionIT.java:406)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",cleanup compaction gc,['segmentmk'],OAK,Bug,Critical,2015-08-05 10:39:15,3
12851413,Compaction slow on repository with continuous writes,"OAK-2734 introduced retry cycles and the option to force compaction when all cycles fail. However OAK-2192 introduced a performance regression: each compaction cycle takes in the order of the size of the repository to complete instead of in the order of the number of remaining changes to compact. This is caused by comparing compacted with pre-compacted node states, which is necessary to avoid mixed segments (aka OAK-2192). To fix the performance regression I propose to pass the compactor an additional node state (the 'onto' state). The diff would then be calculated across the pre compacted states, which performs in the order of number of changes. The changes would then be applied to the 'onto' state, which is a compacted state to avoid mixed segments. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-08-04 14:58:15,3
12849923,Improve binary comparison during repeated upgrades,"OAK-2619 introduced the possibility to run the same upgrade repeatedly and achieve incremental upgrades. This reduces the time taken for {{CommitHook}} processing, because the diff is reduced.

However, for incremental upgrades to be really useful, the content-copy phase needs to be fast. Currently an optimization that was proposed in OAK-2626 was lost due to the implementation of a better solution to some part of the problem. ",performance,['upgrade'],OAK,Improvement,Major,2015-07-29 12:38:43,1
12849891,Extend documentation for SegmentNodeStoreService in http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore,"Currently the documentation at http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore only documents the properties
# repository.home and
# tarmk.size
All the other properties like customBlobStore, tarmk.mode, .... are not documented. Please extend that. Also it would be good, if the table could be extended with what type is supported for the individual properties.",documentation,"['doc', 'segment-tar']",OAK,Technical task,Major,2015-07-29 10:10:48,2
12849611,Lucene Version should be based on IndexFormatVersion,"Currently in oak-lucene where ever call is made to Lucene it passes Version.LUCENE_47 as hardcoded version. To enable easier upgrade of Lucene and hence change of defaults for fresh setup this version should be instead based on {{IndexFormatVersion}}.

Say
* For IndexFormatVersion set to V2 (current default) - Lucene version used is LUCENE_47
* For IndexFormatVersion set to V3 (proposed) - Lucene version used would be per Lucene library version

If the index is reindexed then it would automatically be updated to the latest revision",technical_debt,['lucene'],OAK,Sub-task,Major,2015-07-28 13:20:30,4
12848545,SNFE in persisted comapation map when using CLEAN_OLD,"When using {{CLEAN_OLD}} it might happen that segments of the persisted compaction map get collected. --The reason for this is that only the segment containing the root of the map is pinned ({{SegmentId#pin}}), leaving other segments of the compaction map eligible for collection once old enough.--

{noformat}
org.apache.jackrabbit.oak.plugins.segment.SegmentNotFoundException: Segment 95cbb3e2-3a8c-4976-ae5b-6322ff102731 not found
        at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:919)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:134)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:108)
        at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:82)
        at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:154)
        at org.apache.jackrabbit.oak.plugins.segment.MapRecord.getEntry(MapRecord.java:186)
        at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:118)
        at org.apache.jackrabbit.oak.plugins.segment.PersistedCompactionMap.get(PersistedCompactionMap.java:100)
        at org.apache.jackrabbit.oak.plugins.segment.CompactionMap.get(CompactionMap.java:93)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.uncompact(SegmentWriter.java:1023)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1033)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:100)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore$Commit.<init>(SegmentNodeStore.java:418)
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:204)
{noformat}",compaction gc,['segmentmk'],OAK,Bug,Critical,2015-07-23 12:50:01,3
12845515,Performance degradation of UnsavedModifications on MapDB,"UnsavedModifications performance degrades when used in combination with the MapDB backed MapFactory. Calls become more and more expensive the longer the instance is in use. The is caused by a limitation of MapDB, which does not remove empty BTree nodes.

A test performed with random paths added to the map and later removed again in a loop shows a increase to roughly 1 second to read keys present in the map when the underlying data file is about 50MB in size.",performance,"['core', 'mongomk']",OAK,Bug,Major,2015-07-16 08:30:51,1
12845482,AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic,"At times the CopyOnWrite reports following exception

{noformat}
15.07.2015 14:20:35.930 *WARN* [pool-58-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed
org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204)
	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219)
	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63)
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366)
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311)
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105)
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: _2s7.fdt
	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory$COWLocalFileReference.fileLength(IndexCopier.java:837)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory.fileLength(IndexCopier.java:607)
	at org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141)
	at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529)
	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502)
	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508)
	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202)
	... 10 common frames omitted
{noformat}",resilience,['lucene'],OAK,Bug,Major,2015-07-16 04:38:56,4
12844356,Cache recently extracted text to avoid duplicate extraction,"It can happen that text can be extracted from same binary multiple times in a given indexing cycle. This can happen due to 2 reasons

# Multiple Lucene indexes indexing same node - A system might have multiple Lucene indexes e.g. a global Lucene index and an index for specific nodeType. In a given indexing cycle same file would be picked up by both index definition and both would extract same text
# Aggregation - With Index time aggregation same file get picked up multiple times due to aggregation rules

To avoid the wasted effort for duplicate text extraction from same file in a given indexing cycle it would be better to have an expiring cache which can hold on to extracted text content for some time. The cache should have following features
# Limit on total size
# Way to expire the content using [Timed Evicition|https://code.google.com/p/guava-libraries/wiki/CachesExplained#Timed_Eviction] - As chances of same file getting picked up are high only for a given indexing cycle it would be better to expire the cache entries after some time to avoid hogging memory unnecessarily 

Such a cache would provide following benefit
# Avoid duplicate text extraction - Text extraction is costly and has to be minimized on critical path of {{indexEditor}}
# Avoid expensive IO specially if binary content are to be fetched from a remote {{BlobStore}}",performance,['lucene'],OAK,Improvement,Major,2015-07-11 07:54:53,4
12843210,LastRevRecoveryAgent can update _lastRev of children but not the root,"As mentioned in [OAK-2131|https://issues.apache.org/jira/browse/OAK-2131?focusedCommentId=14616391&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14616391] there can be a situation wherein the LastRevRecoveryAgent updates some nodes in the tree but not the root. This seems to happen due to OAK-2131's change in the Commit.applyToCache (where paths to update are collected via tracker.track): in that code, paths which are non-root and for which no content has changed (and mind you, a content change includes adding _deleted, which happens by default for nodes with children) are not 'tracked', ie for those the _lastRev is not update by subsequent backgroundUpdate operations - leaving them 'old/out-of-date'. This seems correct as per description/intention of OAK-2131 where the last revision can be determined via the commitRoot of the parent. But it has the effect that the LastRevRecoveryAgent then finds those intermittent nodes to be updated while as the root has already been updated (which is at first glance non-intuitive).

I'll attach a test case to reproduce this.

Perhaps this is a bug, perhaps it's ok. [~mreutegg] wdyt?",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-07-07 15:01:49,1
12842853,FileStore.size doesn't throw IOException but declares it as thrown,I suggest to remove the throws clause and fix the affected clients. ,technical_debt,['segmentmk'],OAK,Improvement,Minor,2015-07-06 10:34:13,3
12842397,Add a compound index for _modified + _id,"As explained in OAK-1966 diff logic makes a call like

bq. db.nodes.find({ _id: { $gt: ""3:/content/foo/01/"", $lt: ""3:/content/foo010"" }, _modified: { $gte: 1405085300 } }).sort({_id:1})

For better and deterministic query performance we would need to create a compound index like \{_modified:1, _id:1\}. This index would ensure that Mongo does not have to perform object scan while evaluating such a query.

Care must be taken that index is only created by default for fresh setup. For existing setup we should expose a JMX operation which can be invoked by system admin to create the required index as per maintenance window",performance resilience,['mongomk'],OAK,Improvement,Blocker,2015-07-02 18:08:38,1
12842389,Use a lower bound in VersionGC query to avoid checking unmodified once deleted docs,"As part of OAK-3062 [~mreutegg] suggested

{quote}
As a further optimization we could also limit the lower bound of the _modified
range. The revision GC does not need to check documents with a _deletedOnce
again if they were not modified after the last successful GC run. If they
didn't change and were considered existing during the last run, then they
must still exist in the current GC run. To make this work, we'd need to
track the last successful revision GC run. 
{quote}

Lowest last validated _modified can be possibly saved in settings collection and reused for next run",performance,"['mongomk', 'rdbmk']",OAK,Improvement,Major,2015-07-02 17:58:34,1
12842186,RemoteServerIT failing due to address already in use,"Some of RemoteServerIT failing with Address already in use. Possibly the test setup needs to be changed to use random available port 

{noformat}
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.eclipse.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.eclipse.jetty.server.Server.doStart(Server.java:291)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServer.start(RemoteServer.java:54)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.setUp(RemoteServerIT.java:134)
{noformat}

[1] https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/236/",CI Jenkins,['remoting'],OAK,Bug,Minor,2015-07-02 04:29:16,2
12841717,Improve segment cache in SegmentTracker,"The hand crafted segment cache in {{SegmentTracker}} is prone to lock contentions in concurrent access scenarios. As {{SegmentNodeStore#merge}} might also end up acquiring this lock while holding the commit semaphore the situation can easily lead to many threads being blocked on the commit semaphore. The {{SegmentTracker}} cache doesn't differentiate between read and write access, which means that reader threads can block writer threads. ",doc-impacting resilience scalability,['segmentmk'],OAK,Improvement,Major,2015-06-30 15:16:32,3
12841626,Make compaction gain estimate threshold configurable,Currently compaction is skipped if the compaction gain estimator determines that less than 10% of space could be reclaimed. Instead of relying on a hard coded value of 10% we should make this configurable. ,compaction gc,['segmentmk'],OAK,New Feature,Minor,2015-06-30 09:17:12,3
12841623,Improve compaction gain estimation logging for the case where there are no tar readers,"When compaction is started on a new and sufficiently small repository such that there is yet no tar reader the compaction gain estimator logs the somewhat confusing message {{gain is 0% (0/0) or (0 B/0 B)}}. 

We should improve the logging for this case.",compaction gc,['segmentmk'],OAK,Improvement,Minor,2015-06-30 09:13:31,3
12841614,Move oak-it-osgi to top level ,"The {{oak-it}} module only contains the single {{oak-it-osgi}} module, which in turn consists of a single test class. 

I suggest to at least move {{oak-it-osgi}} to the top level reactor to be consistent with the flat module hierarchy we adopted. IMO an even better solution would be to move the single test class to {{oak-run}}. Not sure whether this is viable though due to the custom assembly necessary. ",modularization technical_debt,['it'],OAK,Improvement,Minor,2015-06-30 08:53:06,3
12841567,RemoteServerIT test are failing on the CI server,"Most of the test in {{RemoteServerIT}} at times fail on the CI server [1] with following exception

{noformat}
Error Message

/home/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/latest1.7/label/Ubuntu/nsfixtures/SEGMENT_MK/profile/unittesting/oak-remote/target/test-classes/org/apache/jackrabbit/oak/remote/http/handler/addNodeMultiPathProperty.json (No such file or directory)
Stacktrace

java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Apache%20Jackrabbit%20Oak%20matrix/jdk/latest1.7/label/Ubuntu/nsfixtures/SEGMENT_MK/profile/unittesting/oak-remote/target/test-classes/org/apache/jackrabbit/oak/remote/http/handler/addNodeMultiPathProperty.json (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:127)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:117)
	at com.google.common.io.ByteSource$AsCharSource.openStream(ByteSource.java:404)
	at com.google.common.io.CharSource.read(CharSource.java:155)
	at com.google.common.io.Files.toString(Files.java:391)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.load(RemoteServerIT.java:119)
	at org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT.testPatchLastRevisionAddMultiPathProperty(RemoteServerIT.java:1199)
{noformat}

[1] https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/232/testReport/",CI Jenkins,['remoting'],OAK,Bug,Minor,2015-06-30 05:03:47,4
12841279,Suspend commit on conflict,"A DocumentNodeStore cluster currently shows a conflict behavior, which
is not intuitive. A modification may fail with a conflict even though
before and after the conflict, the external change is not visible to
the current session. There are two aspects to this issue.

1) a modification may conflict with a change done on another cluster
node, which is committed but not yet visible on the current cluster node.

2) even after the InvalidItemStateException caused by the conflict, a
refreshed session may still not see the external change.

The first aspect is a fundamental design decision and cannot be changed
easily.

The second part can be addressed by suspending the commit until the external
conflict becomes visible on the current cluster node. This would at least
avoid the awkward situation where the external change is not visible after
the InvalidItemStateException.

The system would also become more deterministic. A commit currently goes
into a number of retries with exponential back off, but there's no guarantee
the external modification becomes visible within those retries. 
",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-06-29 09:18:49,1
12840750,DocumentRootBuilder: revisit update.limit default,"update.limit decides whether a commit is persisted using a branch or not. The default is 10000 (and can be overridden using the system property).

A typical call pattern in JCR is to persist batches of ~1024 nodes. These translate to more than 10000 changes (see PackageImportIT), due to JCR properties, and also indexing commit hooks.

",candidate_oak_1_4 performance,"['mongomk', 'rdbmk']",OAK,Improvement,Blocker,2015-06-26 06:43:59,4
12839799,Long running MongoDB query may block other threads,Most queries on MongoDB are usually rather fast and the TreeLock acquired in MongoDocumentStore (to ensure cache consistency) is released rather quickly. However there may be cases when a query is more expensive and a TreeLock is held for a long time. This may block other threads from querying MongoDB and limit concurrency.,concurrency,[],OAK,Bug,Major,2015-06-23 09:38:48,1
12839549,Use batch-update in backgroundWrite,"(From an earlier [post on the list|http://markmail.org/thread/mkrvhkfabit4osli]) The DocumentNodeStore.backgroundWrite goes through the heavy work of updating the lastRev for all pending changes and does so in a hierarchical-depth-first manner. Unfortunately, if the pending changes all come from separate commits (as does not sound so unlikely), the updates are sent in individual update calls to mongo (whenever the lastRev differs). Which, if there are many changes, results in many calls to mongo.

OAK-2066 is about extending the DocumentStore API with a batch-update method. That one, once available, should thus be used in the {{backgroundWrite}} as well.",performance,"['core', 'documentmk']",OAK,Improvement,Major,2015-06-22 13:23:29,1
12838792,"SegmentStore cache does not take ""string"" map into account","The SegmentStore cache size calculation ignores the size of the field Segment.string (a concurrent hash map). It looks like a regular segment in a memory mapped file has the size 1024, no matter how many strings are loaded in memory. This can lead to out of memory. There seems to be no way to limit (configure) the amount of memory used by strings. In one example, 100'000 segments are loaded in memory, and 5 GB are used for Strings in that map.

We need a way to configure the amount of memory used for that. This seems to be basically a cache. OAK-2688 does this, but it would be better to have one cache with a configurable size limit.",doc-impacting resilience scalability,['segmentmk'],OAK,Bug,Major,2015-06-18 13:40:24,3
12838394,Optimize docCache and docChildrenCache invalidation by filtering using journal,"This subtask is about spawning out a [comment|https://issues.apache.org/jira/browse/OAK-2829?focusedCommentId=14588114&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14588114] on OAK-2829 re optimizing docCache invalidation using the newly introduced external diff journal:

{quote}
Attached OAK-2829-improved-doc-cache-invaliation.patch which is a suggestion on how to avoid invalidating the entire document cache when doing a {{backgroundRead}} but instead making use of the new journal: ie only invalidate from the document cache what has actually changed.

I'd like to get an opinion ([~mreutegg], [~chetanm]?) on this first, I have a load test pending locally which found invalidation of the document cache to be the slowest part thus wanted to optimize this first.

Open still/next:
 * also invalidate only necessary parts from the docChildrenCache
 * junits for all of these
{quote}",scalability,"['core', 'mongomk']",OAK,Sub-task,Major,2015-06-17 07:00:23,1
12836776,Broken link on documentation site,"http://jackrabbit.apache.org/oak/docs/command_line.html points to http://jackrabbit.apache.org/oak/docs/oak-mongo-js/oak.html, which doesn't exit. ",documentation,['doc'],OAK,Bug,Minor,2015-06-10 09:18:44,1
12834989,Add MBean to enforce session refresh on all open sessions,"Long running sessions limit the efficient of revision gc as they keep reference to old node states. 

We should consider to add an MBean through which all sessions can be enforced to refresh. This would provide clients with means to fine tune revision gc by first calling that MBean and then triggering a gc cycle. IMO this is preferable to directly enforcing a refresh from the garbage collector. The latter is too invasive and also not required when there are no long running sessions. Offering this functionality to clients as an additional knob to turn is safer. ",compaction doc-impacting gc,"['core', 'jcr']",OAK,New Feature,Major,2015-06-03 10:06:37,3
12834514,Sampling rate feature CompactionGainEstimate is not efficient,"The sampling rate feature introduced with OAK-2595 is not efficient. It only prevents uuids from being stored in the bloom filter while the visited set is not affected and thus keeps growing. 

I will remove the feature again for now. We should look for a better solution once this becomes a problem. Will follow up on OAK-2939 re. this. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-06-02 08:07:48,3
12834513,Sampling rate feature CompactionGainEstimate is not efficient,"The sampling rate feature introduced with OAK-2595 is not efficient. It only prevents uuids from being stored in the bloom filter while the visited set is not affected and thus keeps growing. 

I will remove the feature again for now. We should look for a better solution once this becomes a problem. Will follow up on OAK-2939 re. this. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-06-02 08:06:25,3
12834195,Estimation of required memory for compaction is off,"Currently compaction will be skipped if some rough estimation determines that there is not  enough memory to run. That estimation however assumes that each compaction cycle requires as much space as the compaction map already takes up. This is too conservative. Instead the amount of memory taken up by the last compaction cycle should be a better estimate. 

",compaction gc,['segmentmk'],OAK,Bug,Major,2015-06-01 09:27:08,3
12834183,Remove code related to directmemory for off heap caching,"DocumentNodeStore has some code related to off heap which makes use of Apache Directmemory (OAK-891). This feature was not much used and PersistentCache made this feature obsolete.

Recently it was mentioned on Directmemory that there is not much activity going on [1] in that project and it might be referred to attic. In light of that we should remove this feature from Oak

[1] http://markmail.org/thread/atia2ecaa2mugmjx",technical_debt,['mongomk'],OAK,Task,Major,2015-06-01 08:37:19,4
12833739,Limit the scope of exported packages,"Oak currently exports *a lot* of packages even though those are only used by Oak itself. We should probably leverage OSGi subsystems here and only export the bare minimum to the outside world. This will simplify evolution of Oak internal APIs as with the current approach changes to such APIs always leak to the outside world. 

That is, we should have an Oak OSGi sub-system as an deployment option. Clients would then only need to deploy that into their OSGi container and would only see APIs actually meant to be exported for everyone (like e.g. the JCR API). At the same time Oak could go on leveraging OSGi inside this subsystem.

cc [~bosschaert] as you introduced us to this idea. ",modularization osgi technical_debt,[],OAK,Sub-task,Major,2015-05-29 10:20:53,2
12833670,Parent of unseen children must not be removable,"With OAK-2673, it's now possible to have hidden intermediate nodes created concurrently.
So, a scenario like:
{noformat}
start -> /:hidden
N1 creates /:hiddent/parent/node1
N2 creates /:hidden/parent/node2
{noformat}
is allowed.

But, if N2's creation of {{parent}} got persisted later than that on N1, then N2 is currently able to delete {{parent}} even though there's {{node1}}.",concurrency technical_debt,"['core', 'mongomk']",OAK,Bug,Minor,2015-05-29 04:59:11,1
12833421,ReferenceEditor newIds consuming lots of memory during migration,"In a large migration its seen that {{ReferenceEditor}} {{newIds}} can consume lots of memory as it records all the uuid property. This system has 33 million uuid index and the set was consuming ~1.5G of memory

We should look into ways such that it does not have to maintain such a big in memory state",resilience,['core'],OAK,Improvement,Minor,2015-05-28 13:35:03,4
12833338,DocumentNodeStore background update thread handling of persistence exceptions,"Seen in a log file:

{noformat}
27.05.2015 11:34:48.130 *WARN* [DocumentNodeStore background update thread] org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore Background operation failed: org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: com.ibm.db2.jcc.am.SqlTransactionRollbackException: DB2 SQL Error: SQLCODE=-911, SQLSTATE=40001, SQLERRMC=68, DRIVER=3.65.77
org.apache.jackrabbit.oak.plugins.document.DocumentStoreException: com.ibm.db2.jcc.am.SqlTransactionRollbackException: DB2 SQL Error: SQLCODE=-911, SQLSTATE=40001, SQLERRMC=68, DRIVER=3.65.77
{noformat}

We need to decide whether these are harmless in that the operation will be repeated anyway. If the answer is yes, we may want to tune the log message. If the answer is no, we need to dig deeper.

[~mreutegg] wdyt?",resilience,"['mongomk', 'rdbmk']",OAK,Improvement,Minor,2015-05-28 07:40:06,1
12832953,oak-jcr bundle should be usable as a standalone bundle,Currently oak-jcr bundle needs to be embedded within some other bundle if the Oak needs to be properly configured in OSGi env. Need to revisit this aspect and see what needs to be done to enable Oak to be properly configured without requiring the oak-jcr bundle to be embedded in the repo,modularization osgi technical_debt,['jcr'],OAK,Improvement,Major,2015-05-27 04:44:07,4
12832746,Review and improve Oak and Jcr repository setup,"There is the {{Oak}} and {{Jcr}} builder classes for setting up Oak and Jcr repositories. Both builders don't have clear semantics regarding the life cycle of the individual components they register. On top of that the requirements regarding those life cycles differ depending on whether the individual components run within an OSGi container or not. In the former case the container would already manage the life cycle so the builder should not. 

IMO we should specify the builders to only be used for non OSGi deployments and have the manage the life cycles of the components they instantiate. OTOH for OSGi deployments we should leverage OSGi subsystems to properly set things up.",modularization technical_debt,"['core', 'jcr']",OAK,Improvement,Major,2015-05-26 15:26:53,2
12832736,Move DocumentMK to test,"The DocumentMK class is not directly used (when using the JCR API), but it is only really used by tests. So it should be moved to tests.

The DocumentMK.Builder class needs to be moved first (to a top level class for example).",technical_debt,['documentmk'],OAK,Improvement,Major,2015-05-26 15:00:08,1
12831690,Update to Jackrabbit 2.10.1,"OAK-2748 introduced a snapshot dependency to Jackrabbit 2.10.1-SNAPSHOT. Now that 2.10.1 is released, the snapshot dependency can be removed again.",technical_debt,['parent'],OAK,Improvement,Minor,2015-05-21 10:35:42,1
12831661,DataStoreBlobStore should expose a buffer input stream for getInputStream call,"DataStoreBlobStore directly exposes the InputStream from the wrapped DataStore. In most cases underlying DataStore exposes a LazyFileInputStream [0] which is not buffered.

For performance reason the stream finally exposed at the BlobStore layer should be buffered one. See [1] for the discussion

[1] http://markmail.org/thread/xi4isnzw57vphcsq
[0]
https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-data/src/main/java/org/apache/jackrabbit/core/data/LazyFileInputStream.java#L102 
",performance,['blob'],OAK,Improvement,Minor,2015-05-21 09:02:53,4
12831424,Putting many elements into a map results in many small segments. ,"There is an issue with how the HAMT implementation ({{SegmentWriter.writeMap()}} interacts with the 256 segment references limit when putting many entries into the map: This limit gets regularly reached once the maps contains about 200k entries. At that points segments get prematurely flushed resulting in more segments, thus more references and thus even smaller segments. It is common for segments to be as small as 7k with a tar file containing up to 35k segments. This is problematic as at this point handling of the segment graph becomes expensive, both memory and CPU wise. I have seen persisted segment graphs as big as 35M where the usual size is a couple of ks. 

As the HAMT map is used for storing children of a node this might have an advert effect on nodes with many child nodes. 

The following code can be used to reproduce the issue: 

{code}
SegmentWriter writer = new SegmentWriter(segmentStore, getTracker(), V_11);
MapRecord baseMap = null;

for (;;) {
    Map<String, RecordId> map = newHashMap();
    for (int k = 0; k < 1000; k++) {
        RecordId stringId = writer.writeString(String.valueOf(rnd.nextLong()));
        map.put(String.valueOf(rnd.nextLong()), stringId);
    }

    Stopwatch w = Stopwatch.createStarted();
    baseMap = writer.writeMap(baseMap, map);
    System.out.println(baseMap.size() + "" "" + w.elapsed());
}
{code}

",performance,['segment-tar'],OAK,Bug,Critical,2015-05-20 16:32:35,3
12831411,Avoid accessing binary content if the mimeType is excluded from indexing,"Currently the recommended way to exclude certain types of files from getting indexed is to add them to {{EmptyParser}} in Tika Config. However looking at how Tika works even if mimetype is provided as part metadata. 

Tika Detector try to determine the mimetype by actually reading some bytes from InputStream [1] before looking up from passed MetaData. This would cause unnecessary IO in case large number of binaries are excluded.

We would need to look for way where any access to binary content which is not being indexed can be avoided. One option can to expose a multi value config property which takes a list of mimetypes to be excluded from indexing. If the mimeType provided as part of JCR data is part of that excluded list then call to Tika should be avoided

[1] https://github.com/apache/tika/blob/trunk/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java#L446",performance,['lucene'],OAK,Improvement,Minor,2015-05-20 15:44:16,4
12831410,RepositoryImpl should not manage the lifecycle of ContentRepository,"{{RepositoryImpl}} uses an instance of {{ContentRepository}} that is passed as an external dependency in its constructor.

{{RepositoryImpl}} is not responsible for the creation of the {{ContentRepository}} instance and, as such, should not manage its lifecycle. In particular, the {{ContentRepository#close}} method should not be called when the {{RepositoryImpl#shutdown}} method is executed.",modularization resilience technical_debt,['jcr'],OAK,Sub-task,Major,2015-05-20 15:39:49,2
12831360,Speed up lucene indexing post migration by pre extracting the text content from binaries,"While migrating large repositories say having 3 M docs (250k PDF) Lucene indexing takes long time to complete (at time 4 days!). Currently the text extraction logic is coupled with Lucene indexing and hence is performed in a single threaded mode which slows down the indexing process. Further if the reindexing has to be triggered it has to be done all over again.

To speed up the Lucene indexing we can decouple the text extraction
from actual indexing. It is partly based on discussion on OAK-2787

# Introduce a new ExtractedTextProvider which can provide extracted text for a given Blob instance
# In oak-run introduce a new indexer mode - This would take a path in repository and would then traverse the repository and look for existing binaries and extract text from that

So before or after migration is done one can run this oak-run tool to create this store which has the text already extracted. Then post startup we need to wire up the ExtractedTextProvider instance (which is backed by the BlobStore populated before) and indexing logic can just get content from that. This would avoid performing expensive text extraction in the indexing thread.

See discussion thread http://markmail.org/thread/ndlfpkwfgpey6o66",performance,"['lucene', 'run']",OAK,New Feature,Major,2015-05-20 12:43:05,4
12830964,SegmentBlob does not return blobId for contentIdentity,{{SegmentBlob}} currently returns recordId for {{contentIdentity}} even when an external DataStore is configured. Given that recordId is not stable it would be better to return the blobId as part of  {{contentIdentity}} if external DataStore is configured,resilience,['segmentmk'],OAK,Bug,Minor,2015-05-19 09:45:28,4
12830954,ArrayIndexOutOfBoundsException in UnsavedModifications.put(),"In rare cases a commit may fail to update the pending changes on {{_lastRev}}    of documents. The stack trace is:

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.mapdb.BTreeMap.replace(BTreeMap.java:1174)
        at org.apache.jackrabbit.oak.plugins.document.UnsavedModifications.put(UnsavedModifications.java:90)
        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$10.track(DocumentNodeStore.java:1990)
        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.applyChanges(DocumentNodeStore.java:1056)
        at org.apache.jackrabbit.oak.plugins.document.Commit.applyToCache(Commit.java:598)
        at org.apache.jackrabbit.oak.plugins.document.CommitQueue.afterTrunkCommit(CommitQueue.java:127)
        at org.apache.jackrabbit.oak.plugins.document.CommitQueue.done(CommitQueue.java:83)
{noformat}",resilience,"['core', 'mongomk']",OAK,Bug,Critical,2015-05-19 09:14:14,1
12830925,Add support for generating mongo export command to oak-mongo,"At time to analyse a issue with {{DocumentNodeStore}} running on Mongo we need a dump of various documents so as to recreate the scenario locally. In most case if issue is being observed for a specific path like /a/b then its sufficient to get Mongo documents for /, /a, /a/b and all the split documents for those paths.

It would be useful to have a function in oak-mongo which generates the required export command. For e.g. for path like /a/b following export command would dump all required info

{noformat}
mongoexport -h <mongo server> --port 27017 --db <db name> --collection nodes --out all-required-nodes.json --query '{$or:[{_id : /^4:p\/a\/b\//},{_id : /^3:p\/a\//},{_id : /^2:p\//},{_id:{$in:[""2:/a/b"",""1:/a"",""0:/""]}}]}'
{noformat}",tooling,['run'],OAK,Improvement,Minor,2015-05-19 06:40:46,4
12830668,Tests for SegmentNodeStoreService,{{SegmentNodeStoreService}} currently has no test coverage whatsoever. We should change that.,technical_debt,['segmentmk'],OAK,Improvement,Major,2015-05-18 12:43:30,2
12830649,Support migration without access to DataStore,"Migration currently involves access to DataStore as its configured as part of repository.xml. However in complete migration actual binary content in DataStore is not accessed and migration logic only makes use of

* Dataidentifier = id of the files
* Length = As it gets encoded as part of blobId (OAK-1667)

It would be faster and beneficial to allow migration without actual access to the DataStore. It would serve two benefits

# Allows one to test out migration on local setup by just copying the TarPM files. For e.g. one can only zip following files to get going with repository startup if we can somehow avoid having direct access to DataStore
{noformat}
>crx-quickstart# tar -zcvf repo-2.tar.gz repository --exclude=repository/repository/datastore --exclude=repository/repository/index --exclude=repository/workspaces/crx.default/index --exclude=repository/tarJournal
{noformat}
# Provides faster (repeatable) migration as access to DataStore can be avoided which in cases like S3 might be slow.  Given we solve how to get length

*Proposal*
Have a DataStore implementation which can be provided a mapping file having entries for blobId and length. This file would be used to answer queries regarding length and existing of blob and thus would avoid actual access to DataStore.

Going further this DataStore can be configured with a delegate which can be used as a fallback in case the required details is not present in pre computed data set (may be due to change in content after that data was computed)

",docs-impacting performance,['upgrade'],OAK,New Feature,Major,2015-05-18 10:46:35,4
12830643,ConsistencyChecker#checkConsistency can't cope with inconsistent journal,"When running the consistency checker against a repository with a corrupt journal, it fails with an {{ISA}} instead of trying to skip over invalid revision identifiers:

{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Bad record identifier: foobar
at org.apache.jackrabbit.oak.plugins.segment.RecordId.fromString(RecordId.java:57)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:227)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:178)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:156)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.<init>(FileStore.java:166)
at org.apache.jackrabbit.oak.plugins.segment.file.FileStore$ReadOnlyStore.<init>(FileStore.java:805)
at org.apache.jackrabbit.oak.plugins.segment.file.tooling.ConsistencyChecker.<init>(ConsistencyChecker.java:108)
at org.apache.jackrabbit.oak.plugins.segment.file.tooling.ConsistencyChecker.checkConsistency(ConsistencyChecker.java:70)
at org.apache.jackrabbit.oak.run.Main.check(Main.java:701)
at org.apache.jackrabbit.oak.run.Main.main(Main.java:158)
{noformat}

",resilience tooling,['run'],OAK,Bug,Minor,2015-05-18 10:01:39,3
12830634,NPE in SegmentWriter.writeMap,"Under some rare conditions which are not entirely clear yet {{SegmentWriter.writeMap}} results in a {{NPE}}:

{noformat}
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:366)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapLeaf(SegmentWriter.java:417)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:475)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:511)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:711)
{noformat}

This happens when the {{base}} passed to {{writeMap(MapRecord base, Map<String, RecordId> changes)}} is not null but doesn't contain some of the keys *removed* through the updates provided in the passed {{changes}}. ",resilience,['segmentmk'],OAK,Bug,Major,2015-05-18 09:51:34,3
12830623,Compaction should check for required disk space before running,In the worst case compaction doubles the repository size while running. As this is somewhat unexpected we should check whether there is enough free disk space before running compaction and log a warning otherwise. This is to avoid a common source of running out of disk space and ending up with a corrupted repository. ,compaction doc-impacting gc resilience,['segmentmk'],OAK,Improvement,Major,2015-05-18 09:19:48,2
12829503,Bypass CommitQueue for branch commits,"Currently all commits go through the CommitQueue. This applies to commits that fit into memory, branch commits, merge commits and even reset commits.

The guarantee provided by the CommitQueue is only necessary for commits that affect the head revision of the store: commits that fit into memory and merge commits.

Branch and reset commits should bypass the CommitQueue to avoid unnecessary delays of commits. ",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-05-13 09:55:35,1
12829452,Log stats around time spent in extracting text from binaries,"For issues like OAK-2787 it would helpful if we collect some stats around how much time is spent in extracting text from binaries.

For that purpose I would like add some logging around text extraction",tooling,['lucene'],OAK,Improvement,Minor,2015-05-13 04:49:38,4
12828838,CompactionMap#compress() inefficient for large compaction maps,"I've seen {{CompactionMap#compress()}} take up most of the time spent in compaction. With 40M record ids in the compaction map compressing runs for hours. 

I will back this with numbers as soon as I have a better grip on the issue.  ",compaction doc-impacting gc,['segmentmk'],OAK,Improvement,Major,2015-05-11 14:59:47,3
12828761,Run background read and write operation concurrently,OAK-2624 decoupled the background read from the background write but the methods implementing the operations are synchronized. This means they cannot run at the same time and e.g. an expensive background write may unnecessarily block a background read.,technical_debt,"['core', 'mongomk']",OAK,Improvement,Minor,2015-05-11 09:53:12,1
12828314,CopyOnReadDirectory mode might delete a valid local file upon close,"{{CopyOnReadDirectory}} currently deletes local files which are not found in remote upon close. The list of remote file is fixed for a given revision however list of local files may vary. 

{{IndexTracker}} opens a new {{IndexNode}} upon update before closing the older one. When CopyOnRead is enabled it can happen that same local directory might be in use by two wrapper directories at the same time. 

This introduces a race condition in {{removeDeletedFiles}} method as by the time it is invoked a newer wrapped directory might have started adding new files so those files would get included in the listing done for local directory and hence cause them to be deleted as they would not be found in remote directory which is pinned to older revision. Leading to following exception

{noformat}
Caused by: java.io.FileNotFoundException: /path/to/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/4/_1r.cfe (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:241)
	at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:193)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnReadDirectory$FileReference.openLocalInput(IndexCopier.java:393)
	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnReadDirectory.openInput(IndexCopier.java:221)
	at org.apache.lucene.store.Directory.copy(Directory.java:185)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexMBeanImpl.dumpIndexContent(LuceneIndexMBeanImpl.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{noformat}

As a fix the list of local file should be maintained as progress is made once the CopyOnRead instance gets created to ensure it does not pick up files which are added once the directory is closed",resilience,['lucene'],OAK,Bug,Major,2015-05-08 10:30:33,4
12827982,Improve revision gc on SegmentMK,"This is a container issue for the ongoing effort to improve revision gc of the SegmentMK. 

I'm exploring 
* ways to make the reference graph as exact as possible and necessary: it should not contain segments that are not referenceable any more and but must contain all segments that are referenceable. 
* ways to segregate the reference graph reducing dependencies between certain set of segments as much as possible. 
* Reducing the number of in memory references and their impact on gc as much as possible.

",compaction gc,['segment-tar'],OAK,Epic,Major,2015-05-07 10:40:07,3
12827624,Log NodePropBundle id for which no bundle is found,"At times in migration following exception is seen

{noformat}
Caused by: java.lang.NullPointerException
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.createProperties(JackrabbitNodeState.java:311)
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.<init>(JackrabbitNodeState.java:149)
at org.apache.jackrabbit.oak.upgrade.JackrabbitNodeState.getChildNodeEntries(JackrabbitNodeState.java:255)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1014)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1015)
at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1015)
{noformat}

This would happen if the NodePropBundle is null for a given id. It would be good to add a NPE check in loader itself",resilience,['upgrade'],OAK,Improvement,Minor,2015-05-06 11:26:46,4
12827602,Test failure: OSGiIT,"The OSGiIT tests are failing silently, so not failing the build when the tests don't pass.

{code}
Running org.apache.jackrabbit.oak.osgi.OSGiIT
[main] INFO org.ops4j.pax.exam.spi.DefaultExamSystem - Pax Exam System (Version: 3.4.0) created.
[main] INFO org.ops4j.pax.exam.junit.impl.ProbeRunner - creating PaxExam runner for class org.apache.jackrabbit.oak.osgi.OSGiIT
[main] INFO org.ops4j.pax.exam.junit.impl.ProbeRunner - running test class org.apache.jackrabbit.oak.osgi.OSGiIT
ERROR: org.apache.jackrabbit.oak-lucene (23): [org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService(1)] The activate method has thrown an exception
java.lang.NullPointerException: Index directory cannot be determined as neither index directory path [localIndexDir] nor repository home [repository.home] defined
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:236)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService.createTracker(LuceneIndexProviderService.java:197)
	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService.activate(LuceneIndexProviderService.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code}",CI Jenkins,['it'],OAK,Bug,Critical,2015-05-06 09:42:44,4
12827338,Refactor TarMK,"Container issue for refactoring the TarMK to make it more testable, maintainable, extensible and less entangled. 

For example the segment format should be readable, writeable through standalone means so tests, tools and production code can share this code. Currently there is a lot of code duplication involved here. ",technical_debt,['segment-tar'],OAK,Task,Major,2015-05-05 15:59:52,3
12827300,Test failure: DefaultAnalyzersConfigurationTest,"{{org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest.org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest}} fails on Jenkins.

See e.g. https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/123/jdk=latest1.7,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=unittesting/console

Seen on {{DOCUMENT_RDB}} and {{SEGMENT_MK}} with Java 1.7. and 1.8. 

{noformat}
Tests run: 13, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 23.572 sec <<< FAILURE!
org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest  Time elapsed: 23.255 sec  <<< ERROR!
com.carrotsearch.randomizedtesting.ThreadLeakError: 21 threads leaked from SUITE scope at org.apache.jackrabbit.oak.plugins.index.solr.configuration.DefaultAnalyzersConfigurationTest: 
   1) Thread[id=32, name=oak-scheduled-executor-13, state=TIMED_WAITING, group=main]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:1125)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:807)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
   2) Thread[id=25, name=oak-scheduled-executor-6, state=TIMED_WAITING, group=main]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:1125)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.poll(ScheduledThreadPoolExecutor.java:807)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
...
{noformat}",CI jenkins,['solr'],OAK,Bug,Major,2015-05-05 14:49:27,1
12827231,Comparing node states for external changes is too slow,"Comparing node states for local changes has been improved already with OAK-2669. But in a clustered setup generating events for external changes cannot make use of the introduced cache and is therefore slower. This can result in a growing observation queue, eventually reaching the configured limit. See also OAK-2683.",docs-impacting scalability,"['core', 'mongomk']",OAK,Improvement,Blocker,2015-05-05 09:33:07,1
12825863,Change default for oak.maxLockTryTimeMultiplier,"The default multipler is currently 3, which translates into a lock try timeout of 6 seconds. This is rather low and may result in merge failures even when a commit acquired the merge lock exclusively. I would like to increase it to 30.",doc-impacting resilience,"['core', 'mongomk']",OAK,Improvement,Minor,2015-04-29 08:56:22,1
12825846,Release merge lock in retry loop,"The DocumentNodeStoreBranch retries merges in two phases. First it retries merges while holding the merge lock non-exclusive and performing sleeps between attempts. If those retries fail the next phase will acquire the merge lock exclusively and perform retries. In the first phase the merge lock is released when the commit goes to sleep, while in the second it is not and may block other commits while sleeping.

DocumentNodeStoreBranch should be changed to release the exclusive lock when the commit goes to sleep.",concurrency,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-29 07:54:45,1
12824211,Refactor the optimize logic regarding path include and exclude to avoid duplication,"{{ObservationManagerImpl}} has a optimize method which process the list of includes and excludes and removes redundant clauses. That logic is now also being used in index filtering (OAK-2599) and is getting duplicated.

Going forward we need to refactor this logic so that both places can use it without copying. Possibly making it part of PathUtils

[~mduerig] Also suggested to further optimize
bq. Also PathFilter#optimise could be further optimised by removing entries that subsume each other (e.g. including /a/b, /a is the same as including (/a. ",technical_debt,['core'],OAK,Improvement,Minor,2015-04-27 09:46:38,4
12823779,Save Lucene directory listing as array property,"OakDirectory has to at times perform directory listing specially at the time of opening of index. With DocumentNodeStore such listing of child nodes ""might"" be slow if there are lots more deleted nodes and GC has not cleared them so far (due to OAK-1557). 

As seen in OAK-2808 Lucene might be creating and deleting lot more files. To speed up such lookup one OakDirectory can save the listing of child nodes as an array property once the writer is closed. ",performance,['lucene'],OAK,Improvement,Major,2015-04-24 15:30:59,4
12823366,oak-run: register JMX beans ,"When starting up oak with oak-run the JMX beans are not registered, but it would be convenient for the registration to happen.",tooling,['run'],OAK,Improvement,Minor,2015-04-23 10:09:41,3
12823041,Conditional remove on DocumentStore,"The DocumentStore API allows for conditional inserts (only add document if not present yet) and updates (using findAndModify() with a condition), but it doesn't allow you to remove a document given some conditions are met.

This feature is required to make sure the VersionGarbageCollector does not remove document that are modified concurrently. See OAK-2778.",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-22 14:25:41,1
12822994,Clear excess references before cleanup,"{{FileStore#cleanup}} would be more efficient when getting rid of as much references as possibly beforehand. Excess references are contributed by the current {{TarWriter}} instance and segment cache in {{SegmentTracker}}. 

Those excess references turn out to be especially harmful with many concurrent writers continuously writing to the repository. Starting with a certain write load clean up will become completely blocked. ",compaction gc resilience,['segmentmk'],OAK,Improvement,Major,2015-04-22 11:24:40,3
12822976,Make contributions to reference graph from TarWriter less conservative ,"{{TarWriter#cleanup}} currently adds all its references to the initial elements of the reference graph. It would be sufficient though to just add the references in the tar writer's own graph. 

At the same time I'd like to rename that method from {{cleanup}} to {{collectReferences}}, which better reflects its semantics. ",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-04-22 10:26:56,3
12822650,Time limit for HierarchicalInvalidator,"This issue is related to OAK-2646. Every now and then I see reports of background reads with a cache invalidation that takes a rather long time. Sometimes minutes. It would be good to give the HierarchicalInvalidator an upper limit for the time it may take to perform the invalidation. When the time is up, the implementation should simply invalidate the remaining documents.",resilience,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-21 11:42:18,1
12822289,Improve system resilience in case of index corruption,"I have had issues in cases when the async Lucene index had gotten corrupted. With this index unusable the only option was to re-index.  The problem however was that there were ongoing queries relying on this index even during re-indexing. Because the original index was corrupted these queries led to further load on the system (traversals afair).

I wonder if we could improve the system resilience in such situations.
One thing I could think of: could we maybe fallback to the last known non-corrupted index state while the re-index is running? This would at least take off the load due to new incoming queries.",resilience,"['indexing', 'lucene', 'query']",OAK,Improvement,Major,2015-04-20 11:35:22,4
12821795,Remove Nullable annotation in Predicates of BackgroundObserver,"{code}
@Override
            public int getLocalEventCount() {
                return size(filter(queue, new Predicate<ContentChange>() {
                    @Override
                    public boolean apply(@Nullable ContentChange input) {
                        return input.info != null;
                    }
                }));
            }

            @Override
            public int getExternalEventCount() {
                return size(filter(queue, new Predicate<ContentChange>() {
                    @Override
                    public boolean apply(@Nullable ContentChange input) {
                        return input.info == null;
                    }
                }));
            }
{code}

both methods should probably check for {{input}} being null before accessing {{input.info}}",technical_debt,['core'],OAK,Bug,Major,2015-04-17 07:50:32,4
12821153,Oak builder changes its state during repository creation,"The {{Oak#createContentRepository()}} method changes the state of the builder at every invocation. In particular, it always adds a new {{CommitHook}}.

The observable behavior is that all the {{IndexEditor}} instances are executed twice when the {{Oak}} and {{Jcr}} builders are used together - i.e. when both an instance of {{Repository}} and {{ContentRepository}} are needed.",technical_debt,['core'],OAK,Sub-task,Major,2015-04-15 13:26:25,2
12820818,Fair mode for backgroundOperationLock,The backgroundOperationLock in DocumentNodeStore uses the default non-fair acquisition order. According to JavaDoc of ReentrantReadWriteLock it is possible that a background operation task gets delayed for a long time when the system is under load. We should probably consider using the fair mode for the backgroundOperationLock to make sure background operation tasks do not get delayed excessively.,concurrency,"['core', 'mongomk']",OAK,Improvement,Major,2015-04-14 13:42:39,1
12820786,Configurable maxLockTryTimeMS,OAK-2127 introduced a maxLockTryTimeMS to allow a writer to proceed with a merge even if the merge lock is currently acquired by another thread. The value is currently hardcoded.,doc-impacting,"['core', 'mongomk']",OAK,Improvement,Minor,2015-04-14 11:04:36,1
12820540,Failed to read from tar file ,"Under some rare circumstances there is a warning in the logs:

{noformat}
11:57:47.375 WARN  [pool-1-thread-24] FileStore.java:865    Failed to read from tar file target/SegmentCompactionIT1331315031754226278dir/data01460a.tar
java.io.IOException: Stream Closed
        at java.io.RandomAccessFile.seek(Native Method) ~[na:1.7.0_75]
        at org.apache.jackrabbit.oak.plugins.segment.file.FileAccess$Random.read(FileAccess.java:105) ~[classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.file.TarReader.readEntry(TarReader.java:502) ~[classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.readSegment(FileStore.java:860) ~[classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.getSegment(SegmentTracker.java:128) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentId.getSegment(SegmentId.java:108) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readString(Segment.java:348) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readPropsV11(Segment.java:476) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.loadTemplate(Segment.java:449) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:402) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.Segment.readTemplate(Segment.java:396) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:79) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNodeCount(SegmentNodeState.java:357) [classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomReader.readRandomTree(SegmentCompactionIT.java:410) [test-classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomPropertyReader.call(SegmentCompactionIT.java:446) [test-classes/:na]
        at org.apache.jackrabbit.oak.plugins.segment.SegmentCompactionIT$RandomPropertyReader.call(SegmentCompactionIT.java:439) [test-classes/:na]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_75]
{noformat}

This happens due to a race between {{FileStore#readSegment}} reading from tar files and already removed by {{FileStore#flush}}. This isn't a problem as the tar file in question is still present at a newer generation and the {{FileStore}} will eventually read from that one. However the warning looks rather scaring and somewhat implies a defect. 

We should either lower the log level or remove the race. ",gc resilience,['segmentmk'],OAK,Improvement,Minor,2015-04-13 14:02:11,3
12820397,Consolidated JMX view of all EventListener related statistics,"Oak Observation support exposes a {{EventListenerMBean}} [1] which provide quite a bit of details around registered observation listeners. However in a typical application there would be multiple listeners registered. To simplify monitoring it would be helpful to have a _consolidated_ view of all listeners related statistics.

Further the stats can also include some more details which are Oak specific
* Subtree paths to which the listener listens to - By default JCR Api allows single path however Oak allows a listener to register to multiple paths
* If listener is enabled to listen to cluster local and cluster external changes
* Size of queue in BackgroundObserver
* Distribution of change types present in the queue - Local, External etc

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-api/src/main/java/org/apache/jackrabbit/api/jmx/EventListenerMBean.java",monitoring observation,['core'],OAK,Improvement,Major,2015-04-12 14:45:45,1
12820275,Use non unique PathCursor in LucenePropertyIndex,"{{LucenePropertyIndex}} currently uses unique PathCursor [1] due to which the cursor would maintain an in memory set of visited path. This might grow big if result size is big and cursor is traversed completely.

As with current impl the path would not be duplicated we can avoid using unique cursor

[1] https://github.com/apache/jackrabbit-oak/blob/trunk/oak-lucene/src/main/java/org/apache/jackrabbit/oak/plugins/index/lucene/LucenePropertyIndex.java#L1153-1154",resilience,['lucene'],OAK,Improvement,Minor,2015-04-11 06:33:43,4
12819969,Ignore lesser used old cache entries while invalidating cache entries in background read,"Currently the Cache invalidation logic used in MongoDocumentStore check for cache consistency for all the entries present in the cache. With use of persistent cache its possible that pressure on backend cache would be reduced and some of the cache entries are not being accessed for long time.

Cache invalidation logic should take into account such access statistics and not perform consistency check for cached instance which are not accessed for some long time (10 mins?). Such cache entries should be directly discarded.

PS: Looking at [1] it appears that Guava cache does not enforces a global LRU eviction policy. The policy is maintained per segment table

[1] http://stackoverflow.com/questions/10236057/guava-cache-eviction-policy",performance,"['cache', 'mongomk']",OAK,Improvement,Major,2015-04-10 07:43:21,4
12819945,Change default cache distribution ratio if persistent cache is enabled,"By default the cache memory in DocumentNodeStore is distributed in following ratio

* nodeCache - 25%
* childrenCache - 10%
* docChildrenCache - 3%
* diffCache - 5%
* documentCache - Is given the rest i.e. 57%

However off late we have found that with persistent cache enabled we can lower the cache allocated to Document cache. That would reduce the time spent in invalidating cache entries in periodic reads. So far we are using following ration in few setup and that is turning out well

* nodeCachePercentage=35
* childrenCachePercentage=20
* diffCachePercentage=30
* docChildrenCachePercentage=10
* documentCache - Is given the rest i.e. 5%

We should use the above distribution by default if the persistent cache is found to be enabled
",performance,"['mongomk', 'rdbmk']",OAK,Improvement,Major,2015-04-10 05:17:25,4
12819638,Oak instance does not close the executors created upon ContentRepository creation,"Oak.createContentRepository does not closes the executors it creates upon close. It should close the executor if that is created by itself and not passed by outside

Also see recent [thread|http://markmail.org/thread/rryydj7vpua5qbub].",CI Jenkins,['core'],OAK,Bug,Minor,2015-04-09 11:23:41,4
12819623,MongoDiffCacheTest.sizeLimit() uses too much memory,The diff created by the test uses a lot of memory. Either test test should be changed or the implementation should ignore further changes once a threshold is reached.,CI,['core'],OAK,Test,Minor,2015-04-09 10:06:23,1
12819614,Compaction does not finish on repository with continuous writes ,"A repository with continuous writes can keep the compactor from completing causing the repository size to grow indefinitely. 

This effect is caused by the compactor trying to catch up with changes that occurred while compacting. I.e. compacting them on top of the already compacted head. When there is a steady stream of incoming changes it can happen that the compactor never actually catches up. ",compaction doc-impacting gc,['segmentmk'],OAK,Bug,Major,2015-04-09 09:21:38,3
12819364,NPE when calling Event.getInfo(),"On a very busy site, we're observing an NPE in the code that should gather information about a JCR event for our custom event handler. ",observation,['core'],OAK,Bug,Major,2015-04-08 15:56:16,2
12818886,IndexCopier fails to delete older index directory upon reindex,"{{IndexCopier}} tries to remove the older index directory incase of reindex. This might fails on platform like Windows if the files are still memory mapped or are locked.

For deleting directories we would need to take similar approach like being done with deleting old index files i.e. do retries later.

Due to this following test fails on Windows (Per [~julian.reschke@gmx.de] )

{noformat}
Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec <<< FAILURE!
deleteOldPostReindex(org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest)  Time elapsed: 0.02 sec  <<< FAILURE!
java.lang.AssertionError: Old index directory should have been removed
        at org.junit.Assert.fail(Assert.java:93)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertFalse(Assert.java:68)
        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest.deleteOldPostReindex(IndexCopierTest.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
{noformat}",resilience,['lucene'],OAK,Bug,Minor,2015-04-07 10:32:11,4
12818550,Misleading warn message about local copy size different than remote copy in oak-lucene with copyOnRead enabled,"At times following warning is seen in logs

{noformat}
31.03.2015 14:04:57.610 *WARN* [pool-6-thread-7] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _0.cfs in NIOFSDirectory@/path/to/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/path/to/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 1040384 differs from remote 1958385. Content would be read from remote file only
{noformat}

The file length check provides a weak check around index file consistency. In some cases this warning is misleading. For e.g. 

# Index version Rev1 - Task submitted to copy index file F1 
# Index updated to Rev2 - Directory bound to Rev1 is closed
# Read is performed with Rev2 for F1 - Here as the file would be locally created the size would be different as the copying is in progress

In such a case the logic should ensure that once copy is done the local file gets used",resilience,['lucene'],OAK,Improvement,Minor,2015-04-06 07:05:18,4
12787285,Test failures on Jenkins,"This issue is for tracking test failures seen at our Jenkins instance that might yet be transient. Once a failure happens too often we should remove it here and create a dedicated issue for it. 

h2. Current issues

|| Test                                                                                       || Builds || Fixture      || JVM || Branch ||
| org.apache.jackrabbit.oak.jcr.ConcurrentAddIT.addNodesSameParent | 427, 428, 758 | DOCUMENT_NS, SEGMENT_MK, DOCUMENT_RDB | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.jcr.ConcurrentAddIT.addNodes\[DocumentNodeStore\[RDB\] on jdbc:h2:file:./target/oaktest\] | 829, 846 | | | 1.4 |
| org.apache.jackrabbit.oak.jcr.ConcurrentAddIT| 720, 818 | DOCUMENT_RDB | 1.7 | 1.5 |
| org.apache.jackrabbit.oak.jcr.ConcurrentAddReferenceTest.addReferences[DocumentNodeStore\[RDB] on jdbc:h2:file:./target/oaktest] |  778, 850 | RDB, NS | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.testReorder\[RDBDocumentStore on jdbc:h2:file:./target/oaktest] |  851 | | | 1.2 |
| org.apache.jackrabbit.oak.osgi.OSGiIT.listServices | 851 | | | 1.2 |
| org.apache.jackrabbit.oak.plugins.document.BulkCreateOrUpdateClusterTest.testConcurrentWithConflict\[RDBFixture:RDB-Derby(embedded)] | 797, 823, 841, 842, 843, 847, 848, 850, 853 | | | 1.4, 1.5 |
| org.apache.jackrabbit.oak.plugins.document.BulkCreateOrUpdateTest | 731, 732, 767 | DOCUMENT_RDB, DOCUMENT_NS | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteServiceIT.testLargeStartStopFiesta | 803, 823, 849, 853 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.document.DocumentDiscoveryLiteServiceTest | 361, 608 | DOCUMENT_NS, SEGMENT_MK | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreTest.recoverBranchCommit | 805 | | | 1.0 |
| org.apache.jackrabbit.oak.plugins.document.blob.RDBBlobStoreTest | 673, 674, 786, 787 | SEGMENT_MK, DOCUMENT_NS, DOCUMENT_RDB | 1.7, 1.8 |  |
| org.apache.jackrabbit.oak.plugins.document.blob.RDBBlobStoreTest.testUpdateAndDelete[MyFixture: RDB-Derby(embedded)] | 780, 785, 786, 787 | RDB, NS, | 1.7, 1.8 | 1.5, 1.4 |
| org.apache.jackrabbit.oak.plugins.document.persistentCache.BroadcastTest | 648, 679 | SEGMENT_MK, DOCUMENT_NS | 1.8 |  |
| org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopierTest.reuseLocalDir                 | 81      | DOCUMENT_RDB | 1.7   | |
| org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinitionTest | 770 | DOCUMENT_RDB, DOCUMENT_NS, SEGMENT_MK | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexEditorTest | 490, 623, 624, 656, 679 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexHookIT.testPropertyAddition | 775, 782, 783, 789, 821, 832 | Segment, rdb | 1.7, 1.8 | 1.5, 1.2, 1.0 |
| org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.testNativeMLTQuery | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTestIT.testNativeMLTQueryWithStream | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.plugins.index.solr.query.SolrQueryIndexTest | 148, 151, 490, 656, 679 | SEGMENT_MK, DOCUMENT_NS, DOCUMENT_RDB | 1.5, 1.7, 1.8 | |
| org.apache.jackrabbit.oak.plugins.index.solr.server.EmbeddedSolrServerProviderTest.testEmbeddedSolrServerInitialization | 490, 656, 679 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.plugins.index.solr.util.NodeTypeIndexingUtilsTest | 663 | SEGMENT_MK | 1.7 | |
| org.apache.jackrabbit.oak.plugins.index.solr.util.NodeTypeIndexingUtilsTest.testSynonymsFileCreation | 627 | DOCUMENT_RDB |1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.ExternalBlobIT.testDataStoreBlob | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.ExternalBlobIT.testNullBlobId | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.ExternalBlobIT.testSize | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite\[usePersistedMap: false] | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite\[usePersistedMap: true] | 841 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest | 731, 766, 767, 773, 777, 815 | SEGMENT_MK | 1.7, 1.8 | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxyFlippedEndByte | 804 | | | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxyFlippedIntermediateByteSSL | 777 | Segment | 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxyFlippedStartByteSSL | 773, 843 | Segment | 1.8 | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.BrokenNetworkTest.testProxySSLSkippedBytes | 788,806 | Segment | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT | 722, 731, 733, 755, 759, 776, 812, 815, 816, 817 | SEGMENT_MK | 1.7, 1.8 | 1.4, 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedIntermediateByte | 837, 848 | | | |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedIntermediateByte2 | 837 | | | | 
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedIntermediateByteChange2 | 797, 837 | | | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxyFlippedStartByte | 794, 837, 848 | | | 1.5 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxySkippedBytes | 788, 837, 848 | Segment | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testProxySkippedBytesIntermediateChange | 779, 776, 773 | Segment | 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalPrivateStoreIT.testSync | 837 | | | |
| org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT.testProxySkippedBytes | 788 | Segment | 1.7, 1.8 | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.FailoverSslTestIT | 759 | SEGMENT_MK | 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.FailoverSslTestIT.testFailoverSecure | 794, 846 | | | 1.4 |
| org.apache.jackrabbit.oak.plugins.segment.standby.StandbyTest.testSync | 839 | | | |
| org.apache.jackrabbit.oak.remote.http.handler.RemoteServerIT | 643 | DOCUMNET_NS | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.run.osgi.DocumentNodeStoreConfigTest.testRDBDocumentStoreRestart | 621 | DOCUMENT_NS | 1.8 | |
| org.apache.jackrabbit.oak.run.osgi.DocumentNodeStoreConfigTest.testRDBDocumentStore_CustomBlobStore | 52, 181, 399 |  SEGMENT_MK, DOCUMENT_NS | 1.7 | |
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapDefaultLoginModuleTest | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest | 689 | SEGMENT_MK | 1.8 | |
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticate | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticateCaseInsensitive | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticateFail | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testAuthenticateValidateTrueTrue | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetGroups | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetMembers | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetUserByForeignRef | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetUserByRef | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testGetUserByUserId | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testListUsersWithMissingUid | 833 | | | | 
| org.apache.jackrabbit.oak.security.authentication.ldap.LdapProviderTest.testSplitDNIntermediatePath | 833 | | | | 
| org.apache.jackrabbit.oak.spi.security.authorization.cug.impl.* | 648 | SEGMENT_MK, DOCUMENT_NS | 1.8 |  |
| org.apache.jackrabbit.oak.standalone.RepositoryBootIT | 755 | DOCUMENT_NS | 1.7 | |
| org.apache.jackrabbit.oak.standalone.RepositoryBootIT.repositoryLogin | 778, 781, 793 | RDB, NS | 1.7, 1.8 | 1.5, 1.4 |

h2. fixed or not happening for a while

|| Test                                                                                       || Builds || Fixture      || JVM || Branch ||
| Build crashes: malloc(): memory corruption | 477 | DOCUMENT_NS | 1.5 | |
| org.apache.jackrabbit.j2ee.TomcatIT | 589 | SEGMENT_MK | 1.8 | |
| org.apache.jackrabbit.j2ee.TomcatIT.testTomcat | 489, 493, 597, 648, 801 | DOCUMENT_NS, SEGMENT_MK | 1.7 |  |
| org.apache.jackrabbit.oak.jcr.ConcurrentFileOperationsTest.concurrent | 110, 382 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.MoveRemoveTest.removeExistingNode | 115 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.jcr.OrderedIndexIT.oak2035                                         | 76, 128 | SEGMENT_MK , DOCUMENT_RDB  | 1.5   | |
| org.apache.jackrabbit.oak.jcr.RepositoryTest.addEmptyMultiValue | 115 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.jcr.cluster.NonLocalObservationIT | 731 | DOCUMENT_RDB | 1.8 | |
| org.apache.jackrabbit.oak.jcr.nodetype.NodeTypeTest.updateNodeType | 243, 400 | DOCUMENT_RDB | 1.5, 1.8 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.disjunctPaths | 121, 157, 396 | DOCUMENT_RDB | 1.5, 1.7, 1.8 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.removeSubtreeFilter | 94 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTest.testReorder | 155 | DOCUMENT_RDB | 1.8 | |
| org.apache.jackrabbit.oak.jcr.observation.ObservationTesttestMove | 308 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.query.QueryPlanTest.nodeType | 272 | DOCUMENT_RDB | 1.8 | |
| org.apache.jackrabbit.oak.jcr.query.QueryPlanTest.propertyIndexVersusNodeTypeIndex | 90 | DOCUMENT_RDB | 1.5 | |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest | 171 | SEGMENT_MK | 1.8 | |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest.testNoSuggestions | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest.testSuggestSql | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.jcr.query.SuggestTest.testSuggestXPath | 783 | Segment, rdb | 1.7, 1.8 | 1.0 |
| org.apache.jackrabbit.oak.jcr.version.VersionablePathNodeStoreTest.testVersionablePaths | 361 | DOCUMENT_RDB | 1.7 | |
| org.apache.jackrabbit.oak.osgi.OSGiIT | 767, 770 | SEGMENT_MK, DOCUMENT_RDB, DOCUMENT_NS | 1.7, 1.8 | |
| org.apache.jackrabbit.oak.osgi.OSGiIT.bundleStates | 163, 656 | SEGMENT_MK, DOCUMENT_RDB, DOCUMENT_NS | 1.5, 1.7 | |
| org.apache.jackrabbit.oak.plugins.segment.standby.StandbyTestIT.testSyncLoop                 | 64      | ?            | ?     | |
| org.apache.jackrabbit.oak.run.osgi.JsonConfigRepFactoryTest.testRepositoryTar                | 41      | ?            | ?     | |
| org.apache.jackrabbit.oak.run.osgi.PropertyIndexReindexingTest.propertyIndexState | 492 | DOCUMENT_NS | 1.5 | |
| org.apache.jackrabbit.oak.stats.ClockTest.testClockDriftFast | 115, 142 | SEGMENT_MK, DOCUMENT_NS | 1.6, 1.8 | |
| org.apache.jackrabbit.oak.upgrade.cli.SegmentToJdbcTest.validateMigration | 486 | DOCUMENT_NS | 1.7|  |
| org.apache.jackrabbit.test.api.observation.PropertyAddedTest.testMultiPropertyAdded          | 29      | ?            | ?     | |
",CI Jenkins,['it'],OAK,Epic,Major,2015-04-01 07:40:32,3
12787040,High memory usage of CompactionMap,"In environments with a lot of volatile content the {{CompactionMap}} can end up eating a lot of memory. From {{CompactionStrategyMBean#getCompactionMapStats}}:

{noformat}
[Estimated Weight: 317,5 MB, Records: 39500094, Segments: 36698], 
[Estimated Weight: 316,4 MB, Records: 39374593, Segments: 36660], 
[Estimated Weight: 315,4 MB, Records: 39253205, Segments: 36620], 
[Estimated Weight: 315,1 MB, Records: 39221882, Segments: 36614], 
[Estimated Weight: 314,9 MB, Records: 39195490, Segments: 36604], 
[Estimated Weight: 315,0 MB, Records: 39182753, Segments: 36602], 
[Estimated Weight: 360 B, Records: 0, Segments: 0],
{noformat}


This causes compaction to be skipped:

{noformat}
2015-03-30:30.03.2015 02:00:00.038 *INFO* [] [TarMK compaction thread [/foo/bar/crx-quickstart/repository/segmentstore], active since Mon Mar 30 02:00:00 CEST 2015, previous max duration 3854982ms] org.apache.jackrabbit.oak.plugins.segment.file.FileStore Not enough available memory 5,5 GB, needed 6,3 GB, last merge delta 1,3 GB, so skipping compaction for now
{noformat}",compaction gc resilience,['segmentmk'],OAK,Improvement,Major,2015-03-31 14:26:33,3
12787023,Troublesome AbstractTree.toString,"the default {{toString}} for all tree implementations calculates a string containing the path, the toString of all properties as well as the names of all child tree... this is prone to cause troubles in case for trees that have plenty of properties and children.

i would strongly recommend to review this and make the toString of trees both meaningful and cheap.",technical_debt,['core'],OAK,Improvement,Major,2015-03-31 13:29:15,4
12785837,Track root state revision when reading the tree,"Currently the DocumentNodeState has two revisions:

- {{getRevision()}} returns the read revision of this node state. This revision was used to read the node state from the underlying {{NodeDocument}}.
- {{getLastRevision()}} returns the revision when this node state was last modified. This revision also reflects changes done further below the tree when the node state was not directly affected by a change.

The lastRevision of a state is then used as the read revision of the child node states. This avoids reading the entire tree again with a different revision after the head revision changed because of a commit.

This approach has at least two problems related to comparing node states:

- It does not work well with the current DiffCache implementation and affects the hit rate of this cache. The DiffCache is pro-actively populated after a commit. The key for a diff is a combination of previous and current commit revision and the path. The value then tells what child nodes were added/removed/changed. As the comparison of node states proceeds and traverses the tree, the revision of a state may go back in time because the lastRevision is used as the read revision of the child nodes. This will cause misses in the diff cache, because the revisions do not match the previous and current commit revisions as used to create the cache entries. OAK-2562 tried to address this by keeping the read revision for child nodes at the read revision of the parent in calls of compareAgainstBaseState() when there is a diff cache hit. However, it turns out node state comparison does not always start at the root state. The {{EventQueue}} implementation in oak-jcr will start at the paths as indicated by the filter of the listener. This means, OAK-2562 is not effective in this case and the diff needs to be calculated again based on a set of revisions, which is different from the original commit.

- When a diff is calculated for a parent with many child nodes, the {{DocumentNodeStore}} will perform a query on the underlying {{DocumentStore}} to get child nodes modified after a given timestamp. This timestamp is derived from the lower revision of the two lastRevisions of the parent node states to compare. The query gets problematic for the {{DocumentStore}} if the timestamp is too far in the past. This will happen when the parent node (and sub-tree) was not modified for some time. E.g. the {{MongoDocumentStore}} has an index on the _id and the _modified field. But if there are many child nodes the _id index will not be that helpful and if the timestamp is too far in the past, the _modified index is not selective either. This problem was already reported in OAK-1970 and linked issues.

Both of the above problems could be addressed by keeping track of the read revision of the root node state in each of the node states as the tree is traversed. The revision of the root state would then be used e.g. to derive the timestamp for the _modified constraint in the query. Because the revision of the root state is rather recent, the _modified constraint is very selective and the index on it would be the preferred choice.",performance,"['core', 'mongomk']",OAK,Improvement,Major,2015-03-26 10:28:08,1
12785538,Update lease without holding lock,"A lease update of the DocumentNodeStore on MongoDB will acquire a lock in MongoDocumentStore to perform the changes. The locking is only necessary for changes in the 'nodes' collection, because only those documents are cached and the locking makes sure the cache is consistent. The MongoDocumentStore must be changed to only acquire a lock when changes are done in the 'nodes' collection.",concurrency technical_debt,"['core', 'mongomk']",OAK,Improvement,Major,2015-03-25 14:50:54,1
12783856,SegmentOverflowException in HeavyWriteIT on Jenkins,"{noformat}
heavyWrite(org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT)  Time elapsed: 96.384 sec  <<< ERROR!
org.apache.jackrabbit.oak.plugins.segment.SegmentOverflowException: Segment cannot have more than 255 references 47a9dc3c-c6f9-4b5f-a61a-6711da8b68c2
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.getSegmentRef(SegmentWriter.java:353)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:382)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapLeaf(SegmentWriter.java:426)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:484)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMapBucket(SegmentWriter.java:511)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeMap(SegmentWriter.java:720)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1108)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$2.childNodeChanged(SegmentWriter.java:1091)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:396)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1082)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:1110)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.getNodeState(SegmentNodeBuilder.java:97)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:83)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.updated(MemoryNodeBuilder.java:214)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeBuilder.updated(SegmentNodeBuilder.java:79)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:501)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.setProperty(MemoryNodeBuilder.java:507)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createProperties(HeavyWriteIT.java:137)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:129)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.createNodes(HeavyWriteIT.java:130)
	at org.apache.jackrabbit.oak.plugins.segment.HeavyWriteIT.heavyWrite(HeavyWriteIT.java:110)
{noformat}

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/35/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/

cc [~alex.parvulescu]",CI jenkins,['segmentmk'],OAK,Bug,Critical,2015-03-21 12:03:42,3
12783618,Failed expectations in TarMK standby tests,"I see many similar test failures for {{FailoverMultipleClientsTestIT}} and {{RecoverTestIT}} on Jenkins. For example:

{noformat}
testSyncLoop(org.apache.jackrabbit.oak.plugins.segment.standby.StandbyTestIT): expected:<{ checkpoints = { ... }, root = { ... } }> but was:<{ root : { } }>
  testLocalChanges(org.apache.jackrabbit.oak.plugins.segment.standby.RecoverTestIT): expected: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState<{ root = { ... } }> but was: org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState<{ root = { ... } }>
{noformat}

See
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=jdk1.8.0_11,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=integrationTesting/console
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=SEGMENT_MK,profile=integrationTesting/console
https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=integrationTesting/console",CI Jenkins,['tarmk-standby'],OAK,Bug,Major,2015-03-20 14:20:42,3
12783612,Test failures in TarMK standby: Address already in use,"The following tests fail probably all for the same reason:

{noformat}
testProxySkippedBytes(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxySkippedBytesIntermediateChange(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedStartByte(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByte(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByte2(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByteChange(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
testProxyFlippedIntermediateByteChange2(org.apache.jackrabbit.oak.plugins.segment.standby.ExternalSharedStoreIT): proxy not started
{noformat}

Stacktraces always look something like:
{noformat}
java.lang.Exception: proxy not started
	at org.apache.jackrabbit.oak.plugins.segment.NetworkErrorProxy.reset(NetworkErrorProxy.java:87)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.useProxy(DataStoreTestBase.java:176)
	at org.apache.jackrabbit.oak.plugins.segment.standby.DataStoreTestBase.testProxySkippedBytes(DataStoreTestBase.java:118)
{noformat}

See https://builds.apache.org/job/Apache%20Jackrabbit%20Oak%20matrix/31/jdk=latest1.7,label=Ubuntu,nsfixtures=DOCUMENT_NS,profile=integrationTesting/console",CI jenkins,['tarmk-standby'],OAK,Bug,Major,2015-03-20 14:07:16,3
12783611,Repository Upgrade could shut down the source repository early,"I noticed that during the upgrade we can distinguish 2 phases: first copying the data from the source, then applying all the Editors (indexes and co.).
After phase 1 is done the repository upgrader could shut down the old repo to allow clearing some memory resources which might be used for the second phase.",resilience,['upgrade'],OAK,Improvement,Major,2015-03-20 14:07:07,4
12782216,TimeSeriesMax's frequent 'drops to 0',"The current implementation of TimeSeriesMax - which is what is backing eg the very important 'ObservationQueueMaxLength' statistics - has a very infamous behavior: it does very frequent, intermittent 'jumps back to 0'. This even though the queue-lengths are still at the previous highs, as can often be seen with subsequent measurements (which eg are still showing there are 1000 events in the observation queue).

The reason seems to be that
* the value is increased via {{TimeSeriesMax.recordValue()}} during a 1 second interval
* reset to 0 via {{TimeSeriesMax<init>.run()}} every second

So basically, every second the counter is reset, then during 1 second if any call to {{recordValue()}} happens, it is increased.

This in my view is rather unfortunate - as it can result in mentioned 'jumpy-0' behavior, but it can also jump to values in between if the largest queue does not reports its length during 1 second.

It sounds a bit like this was done this way intentionally? (perhaps to make it as inexpensive as possible) or could this be fixed?",observation tooling,['core'],OAK,Bug,Major,2015-03-16 14:53:21,3
12782058,Use buffered variants for IndexInput and IndexOutput,"Lucene provides a buffered variants for {{IndexInput}} and {{IndexOutput}}. Currently Oak extends these classes directly. For better performance itshould extend the buffered variants.

As discussed [here|https://issues.apache.org/jira/browse/OAK-2222?focusedCommentId=14178265#comment-14178265]",performance,['lucene'],OAK,Improvement,Major,2015-03-15 09:45:56,4
12781804,Cleanup Oak jobs on buildbot,"Since we're moving towards Jenkins, let's remove the buildbot jobs for Oak.
The buildbot configuration is here: https://svn.apache.org/repos/infra/infrastructure/buildbot/aegis/buildmaster/master1/projects",CI,[],OAK,Sub-task,Major,2015-03-13 13:28:04,3
12781803,Cleanup Oak Travis jobs,"Since we're moving toward Jenkins, let's remove the Travis jobs for Oak. ",CI,['it'],OAK,Sub-task,Major,2015-03-13 13:27:19,1
12781604,Too many reads for child nodes,"The DocumentNodeStore issues a lot of reads when sibling nodes are deleted, which are also index with a property index.

The following calls will become a hotspot:

{noformat}
	at org.apache.jackrabbit.oak.plugins.document.mongo.MongoDocumentStore.query(MongoDocumentStore.java:406)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildDocs(DocumentNodeStore.java:846)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildren(DocumentNodeStore.java:788)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getChildren(DocumentNodeStore.java:753)
	at org.apache.jackrabbit.oak.plugins.document.DocumentNodeState.getChildNodeCount(DocumentNodeState.java:194)
	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.getChildNodeCount(ModifiedNodeState.java:198)
	at org.apache.jackrabbit.oak.plugins.memory.MutableNodeState.getChildNodeCount(MutableNodeState.java:265)
	at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNodeCount(MemoryNodeBuilder.java:293)
	at org.apache.jackrabbit.oak.plugins.index.property.strategy.ContentMirrorStoreStrategy.prune(ContentMirrorStoreStrategy.java:456)
{noformat}

I think the code triggering this issue is in {{ModifiedNodeState.getChildNodeCount()}}. It keeps track of already deleted children and requests {{max += deleted}}. The actual {{max}} is always 1 as requested from {{ContentMirrorStoreStrategy.prune()}}, but as more nodes get deleted, the higher {{max}} gets passed to {{DocumentNodeState.getChildNodeCount()}}. The DocumentNodeStore then checks if it has the children in the cache, only to find out the cache entry has too few entries and it needs to fetch one more.

It would be best to have a minimum number of child nodes to fetch from MongoDB in this case. E.g. when NodeState.getChildNodeEntries() is called, the DocumentNodeState fetches 100 children.",performance,['mongomk'],OAK,Improvement,Major,2015-03-12 20:17:42,1
12781593,Release merge lock before branch is reset,MongoMK still holds the merge lock when it resets a persisted branch. Concurrency can be improved if the merge lock is released before the branch is reset.,concurrency technical_debt,"['core', 'mongomk']",OAK,Improvement,Major,2015-03-12 19:40:10,1
12781432,Annotate intermediate docs with property names,"Reading through a ValueMap can be very inefficient if the changes of a given
property are distributed sparsely across the previous documents. The current
implementation has to scan through the entire set of previous documents to
collect the changes.

Intermediate documents should have additional information about what properties
are present on referenced previous documents. 
",performance,['mongomk'],OAK,Improvement,Major,2015-03-12 08:17:33,1
12781228,Thread.interrupt seems to stop repository,"We have a sporadic problem with Sling's JCR installer 3.3.8 and Oak (tar mk). It seems to timing related: the JCR installer does a Thread#interrupt at one point and sometimes this brings the hole instance to stop. Nothing else is going on any more. 
While of course, a workaround is to remove the Thread.interrupt call in the JCR installer (which we did, see SLING-4477), I have the fear that this can happen with any code that is using the repository and gets interrupted.
This error is hard to reproduce, however with three people testing we could see this several times happening",doc-impacting resilience,['doc'],OAK,Improvement,Critical,2015-03-11 15:59:29,3
12780726,Allow excluding certain paths from getting indexed for particular index,"Currently an {{IndexEditor}} gets to index all nodes under the tree where it is defined (post OAK-1980).  Due to this IndexEditor would traverse the whole repo (or subtree if configured in non root path) to perform reindex. Depending on the repo size this process can take quite a bit of time. It would be faster if an IndexEditor can exclude certain paths from traversal

Consider an application like Adobe AEM and an index which only index dam:Asset or the default full text index. For a fulltext index it might make sense to avoid indexing the versionStore. So if the index editor skips such path then lots of redundant traversal can be avoided. 

Also see http://markmail.org/thread/4cuuicakagi6av4v",performance,"['core', 'query']",OAK,New Feature,Major,2015-03-10 06:17:29,4
12780520,more (jmx) instrumentation for observation queue,"While debugging issues with the observation queue it would be handy to have more detailed information available. At the moment you can only see one value wrt length of the queue: that is the maximum of all queues. It is unclear if the queue is that long for only one or many listeners. And it is unclear from that if the listener is slow or the engine that produces the events for the listener.

So I'd suggest to add the following details - possible exposed via JMX? :
# add queue length details to each of the observation listeners
# have a history of the last, eg 1000 events per listener showing a) how long the event took to be created/generated and b) how long the listener took to process. Sometimes averages are not detailed enough so such a in-depth information might become useful. (Not sure about the feasibility of '1000' here - maybe that could be configurable though - just putting the idea out here).
# have some information about whether a listener is currently 'reading events from the cache' or whether it has to go to eg mongo 
# maybe have a 'top 10' listeners that have the largest queue at the moment to easily allow navigation instead of having to go through all (eg 200) listeners manually each time.
",monitoring observation,['core'],OAK,Improvement,Blocker,2015-03-09 15:51:07,3
12780517,High memory consumption of CompactionGainEstimate,"{{CompactionGainEstimate}} keeps a set for the visited record ids. Each entry in that set is represented by an instance of {{ThinRecordId}}. For big repositories the instance overhead lead to {{OOME}} while running the compaction estimator. 
",compaction gc,['segmentmk'],OAK,Improvement,Major,2015-03-09 15:43:30,3
12780441,Commit does not ensure w:majority,"The MongoDocumentStore uses {{findAndModify()}} to commit a transaction. This operation does not allow an application specified write concern and always uses the MongoDB default write concern {{Acknowledged}}. This means a commit may not make it to a majority of a replica set when the primary fails. From a MongoDocumentStore perspective it may appear as if a write was successful and later reverted. See also the test in OAK-1641.

To fix this, we'd probably have to change the MongoDocumentStore to avoid {{findAndModify()}} and use {{update()}} instead.",resilience,"['core', 'mongomk']",OAK,Bug,Major,2015-03-09 09:38:12,1
12780152,observation processing too eager/unfair under load,"The current implementation of oak's observation event processing is too eager and thus unfair under load scenarios. 

Consider having many (eg 200) Eventlisteners but only a relatively small threadpool (eg 5 as is the default in sling) backing them. When processing changes for a particular BackgroundObserver, that one (in BackgroundObserver.completionHandler.call) currently processes *all changes irrespective of how many there are* - ie it is *eager*. Only once that BackgroundObserver processed all changes will it let go and 'pass the thread' to the next BackgroundObserver. Now if for some reason changes (ie commits) are coming in while a BackgroundObserver is busy processing an earlier change, this will lengthen that while loop. As a result the remaining (eg 195) *EventListeners will have to wait for a potentially long time* until it's their turn - thus *unfair*.

Now combine the above pattern with a scenario where mongo is used as the underlying store. In that case in order to remain highly performant it is important that the diffs (for compareAgainstBaseState) are served from the MongoDiffCache for as many cases as possible to avoid doing a round-trip to mongoD. The unfairness in the BackgroundObservers can now result in a large delay between the 'first' observers getting the event and the 'last' one (of those 200). When this delay increases due to a burst in the load, there is a risk of the diffs to no longer be in the cache - those last observers are basically kicked out of the (diff) cache. Once this happens, *the situation gets even worse*, since now you have yet new commits coming in and old changes still having to be processed - all of which are being processed through in 'stripes of 5 listeners' before the next one gets a chance. This at some point results in a totally inefficient cache behavior, or in other words, at some point all diffs have to be read from mongoD.

To avoid this there are probably a number of options - a few one that come to mind:
* increase thread-pool to match or be closer to the number of listeners (but this has other disadvantages, eg cost of thread-switching)
* make BackgroundObservers fairer by limiting the number of changes they process before they give others a chance to be served by the pool.",observation,['core'],OAK,Improvement,Critical,2015-03-06 19:34:26,3
12780035,Set pauseCompaction default to false,As we start seeing good results with the current approach to compaction I'd like to have it running per default. This allows us to gather more information while we are running up towards the 1.2 release. ,compaction gc,['segmentmk'],OAK,Improvement,Major,2015-03-06 10:33:30,3
12779689,RepositoryManager must not register WhiteboardExecutor with Oak,"{{org.apache.jackrabbit.oak.jcr.osgi.RepositoryManager}} currently registers the {{WhiteboardExecutor}} with Oak which internally again register with OSGi ServiceRegistry. This causes recursion as leading to stackoverflow.

As Oak creates an {{Executor}} in absence on explicitly provided one RepositoryManager should not set the {{Executor}}",osgi,['jcr'],OAK,Improvement,Minor,2015-03-05 05:08:53,4
12779007,SessionMBean should provide information about pending refresh,"The session auto refresh feature is implemented by marking sessions pending for refresh. The refresh operation itself however only happens on the next access to the session. 

It would be helpful if {{SessionMBean}} could expose the information whether a session has a pending refresh. Additionally we could expose the current {{RefreshStrategy}} to make the auto refresh behaviour more transparent. ",gc monitoring,['core'],OAK,Sub-task,Major,2015-03-03 10:25:15,3
12777494,Implement MBean monitoring garbage collection,"Provide monitoring for the garbage collection process:
* time series of repository size
* time series of space reclaimed
* time stamp of last clean up
* time stamp of last compaction
* last error
* time stamp when next gc run is scheduled
* ...",compaction gc monitoring,['segmentmk'],OAK,Sub-task,Major,2015-02-25 11:23:29,3
12774577,Truncate journal.log after off line compaction,After off line compaction the repository contains a single revision. However the journal.log file will still contain the trail of all revisions that have been removed during the compaction process. I suggest we truncate the journal.log to only contain the latest revision created during compaction.,compaction gc,['segmentmk'],OAK,New Feature,Minor,2015-02-12 14:20:06,3
12774256,Provide initial implementation of the Remote Operations specification,"To provide something that can be played with, and to verify the feasibility of the specification draft, an initial implementation of the HTTP API should be provided.

The API should follow the general behavior described [here|https://wiki.apache.org/jackrabbit/frm/RemoteOperations] and the HTTP semantics defined [here|https://wiki.apache.org/jackrabbit/frm/HttpOperations]. ",api remoting,['remoting'],OAK,Sub-task,Minor,2015-02-11 16:34:37,2
12773875,Root record references provide too little context for parsing a segment,"According to the [documentation | http://jackrabbit.apache.org/oak/docs/nodestore/segmentmk.html] the root record references in a segment header provide enough context for parsing all records within this segment without any external information. 

Turns out this is not true: if a root record reference turns e.g. to a list record. The items in that list are record ids of unknown type. So even though those records might live in the same segment, we can't parse them as we don't know their type. ",tools,['segment-tar'],OAK,Bug,Major,2015-02-10 15:50:46,2
12773538,Flag Document having many children,"Current DocumentMK logic while performing a diff for child nodes works as below

# Get children for _before_ revision upto MANY_CHILDREN_THRESHOLD (which defaults to 50). Further note that current logic of fetching children nodes also add children {{NodeDocument}} to {{Document}} cache and also reads the complete Document for those children
# Get children for _after_ revision with limits as above
# If the child list is complete then it does a direct diff on the fetched children
# if the list is not complete i.e. number of children are more than the threshold then it for a query based diff (also see OAK-1970)

So in those cases where number of children are large then all work done in #1 above is wasted and should be avoided. To do that we can mark those parent nodes which have many children via special flag like {{_manyChildren}}. One such nodes are marked the diff logic can check for the flag and skip the work done in #1

This is kind of similar to way we mark nodes which have at least one child (OAK-1117)

",performance,['mongomk'],OAK,Improvement,Major,2015-02-09 18:58:38,4
12772397,Move our CI to Jenkins,"We should strive for stabilization of our CI setup, as of now we had Buildbot and Travis.
It seems ASF Jenkins can perform jobs on different environments (*nix, Windows and others) so we can evaluate that and check if it better address our needs.",CI build infrastructure,[],OAK,Task,Critical,2015-02-04 15:35:23,3
12770899,Resolve the base directory path of persistent cache against repository home,"Currently PersistentCache uses the directory path directly. Various other parts in Oak which need access to the filesystem currently make use of {{repository.home}} framework property in OSGi env [1]

Same should also be used in PersistentCache

[1] http://jackrabbit.apache.org/oak/docs/osgi_config.html#SegmentNodeStore ",technical_debt,"['core', 'documentmk']",OAK,Improvement,Minor,2015-01-29 03:30:18,1
12768421,Support continuable sessions ,"Implement support for continuable sessions to keeps state across multiple client/server interactions. Continuable sessions do not require any additional state on the server (i.e. Oak) apart form the apparent repository state. 

To continue a session a client would obtain a continuation token from the current session. This token can be used on the next call to {{Repository.login}} to obtain a new {{Session}} instance that is based on the same repository revision that the session the token was obtained from. Additionally the token could contain information re. authentication so subsequent request can go through a simplified authentication procedure. ([~asanso]'s work on OAuth might be of help here.)

Transient changes are not supported in continuable sessions. Obtaining a continuation token from a session with transient changes results in an error. 

Continuable sessions are typically short lived (i.e. the time of a single HTTP request). Specifically continuable session do not retain the underlying repository revision from being garbage collected. Clients need to be able to cope with respective exceptions. 



",api,['core'],OAK,Sub-task,Major,2015-01-19 11:23:51,2
12768418,Oak API remoting,"Container issues for collecting tasks related to remoting the Oak API. Such a remoting should be:

* stateless on the Oak side apart from the apparent persisted state in the content repository, 

* independent from {{oak-jcr}}, but reusing JCR related plugins from {{oak-core}} as required (e.g. for name space and node type handling),

* agnostic of any protocol bindings,

* ...",api remoting,['remoting'],OAK,Task,Major,2015-01-19 11:10:06,2
12767764,Investigate ways to make revision gc more precise ,"Current approaches to revision garbage collection tend to be too conservative (too little space reclaimed, e.g. OAK-2045) or too aggressive (removing segments still being used, e.g. OAK-2384). 

This issue is to explore ways to make revision gc on TarMk more precise. ",gc,['segmentmk'],OAK,Task,Major,2015-01-15 15:58:05,3
12767762,Auto-refresh sessions on revision gc,"The approach to revision garbage collection taken in OAK-2192 assumes that long running background sessions call refresh once they become active again. Incidentally this is true as such background sessions usually are admin sessions and those are always auto-refreshed on access (see OAK-88, OAK-803, and OAK-960). However as soon as we move away from admin sessions this might not be true any more and we might start seeing {{SegmentNotFoundException}} s unless the user explicitly refreshes the session. 

To prevent this we should make all sessions auto refresh once revision gc runs. ",gc,['segmentmk'],OAK,Improvement,Major,2015-01-15 15:51:07,3
12767745,Monitoring to track old NodeStates,"We should add some monitoring that allows us to track ""old"" node states, which potentially block revision gc. 

Possible approaches:

* Add monitoring too old revisions (root node states) along with the stack traces from where they have been acquired.

* Include RecordId of root node state in the {{SessionMBean}}.

* Add additional tooling on top of the {{SessionMBean}} to make it easier to make sense of the wealth of information provided. 
",gc monitoring tooling,['segment-tar'],OAK,Sub-task,Major,2015-01-15 15:08:42,3
12767742,Provide more information in SegmentNotFoundException,"There is currently no way to distinguish between a {{SegmentNotFoundException}} occurring because of a removed segment by gc or because of another corruption. Optimally we would tell in the exception why the segment is gone, how old it was when gc removed it and who/what was still referring to it at that time. In order to do that, we probably need some kind of log for the following data: When a segment was removed (because a new generation of the .tar file was made, or because the .tar file was removed), we should log the segment, the file name, and the date+time of the removal. If the segment was then not found because it was too old, then another type of exception should be thrown instead, for example ""ReadTimeoutException"", with a message that contains as much data as possible: the data+time of the segment, date+time of the removal of the segment, about when compaction was run, date+time of the session login and last refresh, the stack trace of where the session was acquired.",gc monitoring,['segmentmk'],OAK,Sub-task,Major,2015-01-15 15:05:49,3
12767740,Improve monitoring capabilities for TarMk revision gc,"Container devoted to improving monitoring of the TarMk revision garbage collection process. The overall goal is to make it more transparent what revision gc does, how it performs, why it failed etc. ",gc monitoring tooling,['segment-tar'],OAK,Task,Major,2015-01-15 14:59:39,3
12767690,SegmentNodeStoreService prone to deadlocks,"The SegmentNodeStoreService is prone to deadlocks because of the way in which is synchronizes access to the _SegmentNodeStore_ delegate.

The issue can now be seen on #deactivate, when the deregistration is being synchronously broadcast and if a referring service calls #getNodeStore the deadlock happens.

{code}
Found one Java-level deadlock:
=============================
""qtp844483043-936"":
  waiting to lock monitor 0x000001d1aacc7208 (object 0x000001d231f52698, a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService),
  which is held by ""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)""
""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)"":
  waiting to lock monitor 0x000001d4d0907c88 (object 0x000001d2334be930, a java.lang.Object),
  which is held by ""pool-5-thread-4""
""pool-5-thread-4"":
  waiting to lock monitor 0x000001d1aacc7208 (object 0x000001d231f52698, a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService),
  which is held by ""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)""

Java stack information for the threads listed above:
===================================================
""qtp844483043-936"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:144)
	- waiting to lock <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:73)
	at org.apache.jackrabbit.oak.spi.state.ProxyNodeStore.getRoot(ProxyNodeStore.java:35)
	at org.apache.jackrabbit.oak.core.MutableRoot.<init>(MutableRoot.java:160)
	at org.apache.jackrabbit.oak.core.ContentSessionImpl.getLatestRoot(ContentSessionImpl.java:110)
	at org.apache.jackrabbit.oak.spi.security.authentication.AbstractLoginModule.getRoot(AbstractLoginModule.java:403)
	at org.apache.jackrabbit.oak.security.authentication.token.TokenLoginModule.getTokenProvider(TokenLoginModule.java:215)
	at org.apache.jackrabbit.oak.security.authentication.token.TokenLoginModule.login(TokenLoginModule.java:128)
	at org.apache.felix.jaas.boot.ProxyLoginModule.login(ProxyLoginModule.java:52)
	at sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:161)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:256)
	at com.adobe.granite.repository.impl.CRX3RepositoryImpl.login(CRX3RepositoryImpl.java:92)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:197)
	at org.apache.sling.jcr.base.AbstractSlingRepository2.login(AbstractSlingRepository2.java:297)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProviderInternal(JcrResourceProviderFactory.java:289)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProvider(JcrResourceProviderFactory.java:201)
	at org.apache.sling.resourceresolver.impl.tree.ResourceProviderFactoryHandler.login(ResourceProviderFactoryHandler.java:164)
	at org.apache.sling.resourceresolver.impl.tree.RootResourceProviderEntry.loginToRequiredFactories(RootResourceProviderEntry.java:95)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolverInternal(CommonResourceResolverFactoryImpl.java:109)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolver(CommonResourceResolverFactoryImpl.java:90)
	at org.apache.sling.resourceresolver.impl.ResourceResolverFactoryImpl.getResourceResolver(ResourceResolverFactoryImpl.java:93)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.getAnonymousResolver(SlingAuthenticator.java:839)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.doHandleSecurity(SlingAuthenticator.java:478)
	at org.apache.sling.auth.core.impl.SlingAuthenticator.handleSecurity(SlingAuthenticator.java:438)
	at org.apache.sling.engine.impl.SlingHttpContext.handleSecurity(SlingHttpContext.java:121)
	at org.apache.felix.http.base.internal.context.ServletContextImpl.handleSecurity(ServletContextImpl.java:335)
	at org.apache.felix.http.base.internal.handler.ServletHandler.doHandle(ServletHandler.java:337)
	at org.apache.felix.http.base.internal.handler.ServletHandler.handle(ServletHandler.java:300)
	at org.apache.felix.http.base.internal.dispatch.ServletPipeline.handle(ServletPipeline.java:93)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:50)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.i18n.impl.I18NFilter.doFilter(I18NFilter.java:128)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.sslfilter.internal.SslFilter.doFilter(SslFilter.java:55)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.sslfilter.internal.SslFilter.doFilter(SslFilter.java:89)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at com.adobe.granite.license.impl.LicenseCheckFilter.doFilter(LicenseCheckFilter.java:298)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.security.impl.ReferrerFilter.doFilter(ReferrerFilter.java:290)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.featureflags.impl.FeatureManager.doFilter(FeatureManager.java:115)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.sling.engine.impl.log.RequestLoggerFilter.doFilter(RequestLoggerFilter.java:75)
	at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108)
	at org.apache.felix.http.base.internal.handler.FilterHandler.handle(FilterHandler.java:80)
	at org.apache.felix.http.base.internal.dispatch.InvocationFilterChain.doFilter(InvocationFilterChain.java:46)
	at org.apache.felix.http.base.internal.dispatch.HttpFilterChain.doFilter(HttpFilterChain.java:31)
	at org.apache.felix.http.base.internal.dispatch.FilterPipeline.dispatch(FilterPipeline.java:76)
	at org.apache.felix.http.base.internal.dispatch.Dispatcher.dispatch(Dispatcher.java:49)
	at org.apache.felix.http.base.internal.DispatcherServlet.service(DispatcherServlet.java:67)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:722)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:229)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
""CM Event Dispatcher (Fire ConfigurationEvent: pid=org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)"":
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.unbindTopologyEventListener(DiscoveryServiceImpl.java:242)
	- waiting to lock <0x000001d2334be930> (a java.lang.Object)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.BindMethod.invoke(BindMethod.java:37)
	at org.apache.felix.scr.impl.manager.DependencyManager.invokeUnbindMethod(DependencyManager.java:1717)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.invokeUnbindMethod(SingleComponentManager.java:404)
	at org.apache.felix.scr.impl.manager.DependencyManager$MultipleDynamicCustomizer.removedService(DependencyManager.java:376)
	at org.apache.felix.scr.impl.manager.DependencyManager$MultipleDynamicCustomizer.removedService(DependencyManager.java:304)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.unregister(AbstractComponentManager.java:1011)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager$3.unregister(AbstractComponentManager.java:992)
	at org.apache.felix.scr.impl.manager.RegistrationManager.changeRegistration(RegistrationManager.java:141)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.unregisterService(AbstractComponentManager.java:1054)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:900)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:974)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:895)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.unregisterService(AbstractSlingRepositoryManager.java:258)
	at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.stop(AbstractSlingRepositoryManager.java:345)
	at com.adobe.granite.repository.impl.SlingRepositoryManager.deactivate(SlingRepositoryManager.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:974)
	at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.removedService(DependencyManager.java:895)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1506)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerRemoved(ServiceTracker.java:1401)
	at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.untrack(ServiceTracker.java:1261)
	at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1440)
	at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:940)
	at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:794)
	at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:544)
	at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4425)
	at org.apache.felix.framework.Felix.access$000(Felix.java:75)
	at org.apache.felix.framework.Felix$1.serviceChanged(Felix.java:402)
	at org.apache.felix.framework.ServiceRegistry.unregisterService(ServiceRegistry.java:153)
	at org.apache.felix.framework.ServiceRegistrationImpl.unregister(ServiceRegistrationImpl.java:128)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.unregisterNodeStore(SegmentNodeStoreService.java:320)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.deactivate(SegmentNodeStoreService.java:295)
	- locked <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:231)
	at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:39)
	at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:624)
	at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:508)
	at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:149)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.disposeImplementationObject(SingleComponentManager.java:355)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.deleteComponent(SingleComponentManager.java:170)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.doDeactivate(AbstractComponentManager.java:908)
	at org.apache.felix.scr.impl.manager.AbstractComponentManager.deactivateInternal(AbstractComponentManager.java:883)
	at org.apache.felix.scr.impl.manager.SingleComponentManager.reconfigure(SingleComponentManager.java:638)
	at org.apache.felix.scr.impl.config.ConfigurableComponentHolder.configurationUpdated(ConfigurableComponentHolder.java:328)
	at org.apache.felix.scr.impl.config.ConfigurationSupport.configurationEvent(ConfigurationSupport.java:290)
	at org.apache.felix.cm.impl.ConfigurationManager$FireConfigurationEvent.sendEvent(ConfigurationManager.java:2032)
	at org.apache.felix.cm.impl.ConfigurationManager$FireConfigurationEvent.run(ConfigurationManager.java:2002)
	at org.apache.felix.cm.impl.UpdateThread.run(UpdateThread.java:103)
	at java.lang.Thread.run(Thread.java:745)
""pool-5-thread-4"":
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:144)
	- waiting to lock <0x000001d231f52698> (a org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService)
	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.getNodeStore(SegmentNodeStoreService.java:73)
	at org.apache.jackrabbit.oak.spi.state.ProxyNodeStore.getRoot(ProxyNodeStore.java:35)
	at org.apache.jackrabbit.oak.core.MutableRoot.<init>(MutableRoot.java:160)
	at org.apache.jackrabbit.oak.core.ContentSessionImpl.getLatestRoot(ContentSessionImpl.java:110)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.<init>(SessionDelegate.java:160)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl$1.<init>(RepositoryImpl.java:273)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.createSessionDelegate(RepositoryImpl.java:271)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:257)
	at com.adobe.granite.repository.impl.CRX3RepositoryImpl.login(CRX3RepositoryImpl.java:92)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl$2.run(SlingRepositoryImpl.java:108)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl$2.run(SlingRepositoryImpl.java:100)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAsPrivileged(Subject.java:536)
	at com.adobe.granite.repository.impl.SlingRepositoryImpl.createAdministrativeSession(SlingRepositoryImpl.java:100)
	at org.apache.sling.jcr.base.AbstractSlingRepository2.loginAdministrative(AbstractSlingRepository2.java:362)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getResourceProviderInternal(JcrResourceProviderFactory.java:246)
	at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory.getAdministrativeResourceProvider(JcrResourceProviderFactory.java:209)
	at org.apache.sling.resourceresolver.impl.tree.ResourceProviderFactoryHandler.login(ResourceProviderFactoryHandler.java:162)
	at org.apache.sling.resourceresolver.impl.tree.RootResourceProviderEntry.loginToRequiredFactories(RootResourceProviderEntry.java:95)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getResourceResolverInternal(CommonResourceResolverFactoryImpl.java:109)
	at org.apache.sling.resourceresolver.impl.CommonResourceResolverFactoryImpl.getAdministrativeResourceResolver(CommonResourceResolverFactoryImpl.java:76)
	at org.apache.sling.resourceresolver.impl.ResourceResolverFactoryImpl.getAdministrativeResourceResolver(ResourceResolverFactoryImpl.java:98)
	at org.apache.sling.discovery.impl.cluster.ClusterViewServiceImpl.getClusterView(ClusterViewServiceImpl.java:132)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.getTopology(DiscoveryServiceImpl.java:418)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.handlePotentialTopologyChange(DiscoveryServiceImpl.java:466)
	at org.apache.sling.discovery.impl.DiscoveryServiceImpl.handleTopologyChanged(DiscoveryServiceImpl.java:650)
	- locked <0x000001d2334be930> (a java.lang.Object)
	at org.apache.sling.discovery.impl.topology.TopologyChangeHandler.handleTopologyChanged(TopologyChangeHandler.java:134)
	at org.apache.sling.discovery.impl.topology.TopologyChangeHandler.handleEvent(TopologyChangeHandler.java:124)
	at org.apache.felix.eventadmin.impl.handler.EventHandlerProxy.sendEvent(EventHandlerProxy.java:412)
	at org.apache.felix.eventadmin.impl.tasks.SyncDeliverTasks.execute(SyncDeliverTasks.java:118)
	at org.apache.felix.eventadmin.impl.handler.EventAdminImpl.sendEvent(EventAdminImpl.java:114)
	at org.apache.felix.eventadmin.impl.security.EventAdminSecurityDecorator.sendEvent(EventAdminSecurityDecorator.java:96)
	at org.apache.sling.jcr.resource.internal.OakResourceListener.sendOsgiEvent(OakResourceListener.java:243)
	at org.apache.sling.jcr.resource.internal.OakResourceListener.changed(OakResourceListener.java:133)
	at org.apache.jackrabbit.oak.plugins.observation.NodeObserver$NodeEventHandler.leave(NodeObserver.java:208)
	at org.apache.jackrabbit.oak.plugins.observation.FilteredHandler.leave(FilteredHandler.java:51)
	at org.apache.jackrabbit.oak.plugins.observation.EventGenerator$Continuation.run(EventGenerator.java:175)
	at org.apache.jackrabbit.oak.plugins.observation.EventGenerator.generate(EventGenerator.java:118)
	at org.apache.jackrabbit.oak.plugins.observation.NodeObserver.contentChanged(NodeObserver.java:156)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:117)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:111)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Found 1 deadlock.
{code}",resilience,['segmentmk'],OAK,Bug,Blocker,2015-01-15 09:29:50,4
12765997,SegmentNotFoundException when keeping JCR Value references,"With OAK-2192 revision gc started to remove segments older than a certain threshold. The underlying assumption was that old sessions would call refresh (i.e. auto refresh) anyway once they become active again. However, it turns out that refreshing a sessions does not affect JCR values as those are directly tied to the underlying record. Accessing those values after its segment has been gc'ed results in a {{SegmentNotFoundException}}. 

Keeping reference to JCR values is an important use case for Sling's {{JcrPropertyMap}}, which is widely used.",gc,['core'],OAK,Bug,Critical,2015-01-09 11:21:56,3
12762836,Sporadic test failure of OSGiIT.listBundles on Buildbot,"See http://markmail.org/message/idx2y2dwpkaxchsp for previous mention.

I suggest to use the mechanism from OAK-2371 to exclude the tests on that CI environment for now. ",CI buildbot test,['it'],OAK,Bug,Minor,2014-12-19 12:37:16,4
12760809,Wrong handling of InterruptedException in BackgroundThread,{{BackgroundThread}} catches {{InterruptedException}} but doesn't set the thread's interrupted status. ,resilience,['core'],OAK,Bug,Major,2014-12-10 17:31:47,3
12760088,SegmentMK consistency check,We need a tool to check a SegmentMK repository for consistency. Such a tool should start at the most recent version in the journal and traverse back until it finds the latest good revision. ,offline production resilience tools,['run'],OAK,New Feature,Major,2014-12-08 08:54:05,3
12757647,Switch default IndexFormatVersion to V2 ,"OAK-2276 added support for {{IndexFormatVersion}} where {{V1}} is compatible with existing {{LuceneIndex}} while {{V2}} is compatible with newer index implemention being worked on OAK-2278.

Once implementation in OAK-2278 is stable enough we should switch the default version to be used for fresh index (unless overrided with {{compatMode}} ) from V1 to V2",performance,['lucene'],OAK,Task,Major,2014-11-25 08:55:34,4
12754445,Add metadata about the changed value to a PROPERTY_CHANGED event on a multivalued property,"When getting _PROPERTY_CHANGED_ events on non-multivalued properties only one value can have actually changed so that handlers of such events do not need any further information to process it and eventually work on the changed value; on the other hand _PROPERTY_CHANGED_ events on multivalued properties (e.g. String[]) may relate to any of the values and that brings a source of uncertainty on event handlers processing such changes because there's no mean to understand which property value had been changed and therefore to them to react accordingly.
A workaround for that is to create Oak specific _Observers_ which can deal with the diff between before and after state and create a specific event containing the ""diff"", however this would add a non trivial load to the repository because of the _Observer_ itself and because of the additional events being generated while it'd be great if the 'default' events would have metadata e.g. of the changed value index or similar information that can help understanding which value has been changed (added, deleted, updated). ",observation,"['core', 'jcr']",OAK,Improvement,Major,2014-11-11 11:27:33,3
12753132,Observation events accessibility check should respect session refresh settings,This is related to OAK-2000. I think the accessibility check needs to respect the session refresh settings when acquiring the root object.,observation,['jcr'],OAK,Bug,Major,2014-11-05 19:04:16,3
12751610,CopyOnWriteDirectory implementation for Lucene for use in indexing,"Currently a Lucene index when is written directly to OakDirectory. For reindex case it might happen that Lucene merge policy read the written index files again and then perform a sgement merge. This might have lower performance when OakDirectroy is writing to remote storage.

Instead of that we can implement a CopyOnWriteDirectory on similar lines to  OAK-1724 where CopyOnReadDirectory support copies the  index locally for faster access. 

At high level flow would be

# While writing index the index file is first written to local directory
# Any write is done locally and once a file is written its written asynchronously to OakDirectory
# When IndexWriter is closed it would wait untill all the write is completed

This needs to be benchmarked with existing reindex timings to see it its actually beneficial",docs-impacting performance,['lucene'],OAK,New Feature,Major,2014-10-30 05:05:49,4
12748070,Print tar file graph in segment explorer,I think it would be useful if the segment explorer could print the graph of a tar file along with its references. ,tools,['core'],OAK,Improvement,Major,2014-10-14 16:18:35,3
12748057,No garbage collection after reaching generation z,"Tar garbage collection generates new generation of tar files once it determines a given file contains garbage. New generations will have the next lower case letter from the alphabet appended to its file name. When the letter 'z' is reached, no further garbage collection is done. 

I think we need to fix this as otherwise garbage collection just stops working arbitrarily. ",gc,['core'],OAK,Improvement,Minor,2014-10-14 15:26:19,3
12748055,Concurrent commit during compaction results in mixed segments,"Changes that are committed during a segment store compaction run will be compacted on top of the already compacted changes. However the compactor uses the wrong before state in this case. Instead of compacting against the compacted before state it uses the un-compacted before state. The resulting state will thus contain references to un-compacted state, making those not eligible for later clean up. ",compaction gc,"['core', 'segmentmk']",OAK,Bug,Major,2014-10-14 15:10:26,3
12747934,Fix intermittent failure in JaasConfigSpiTest,"Intermittent failures on windows are observed in JaasConfigSpiTest with following exception

{noformat}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.841 sec <<< FAILURE!
defaultConfigSpiAuth(org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest)  Time elapsed: 3.835 sec  <<< ERROR!
java.lang.reflect.UndeclaredThrowableException
	at $Proxy7.login(Unknown Source)
	at javax.jcr.Repository$login.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.apache.jackrabbit.oak.run.osgi.JaasConfigSpiTest.defaultConfigSpiAuth(JaasConfigSpiTest.groovy:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.jackrabbit.oak.run.osgi.OakOSGiRepositoryFactory$RepositoryProxy.invoke(OakOSGiRepositoryFactory.java:325)
	... 37 more
Caused by: javax.jcr.LoginException: No LoginModules configured for jackrabbit.oak
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:264)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:222)
	... 42 more
Caused by: javax.security.auth.login.LoginException: No LoginModules configured for jackrabbit.oak
	at javax.security.auth.login.LoginContext.init(LoginContext.java:256)
	at javax.security.auth.login.LoginContext.<init>(LoginContext.java:499)
	at org.apache.jackrabbit.oak.spi.security.authentication.JaasLoginContext.<init>(JaasLoginContext.java:49)
	at org.apache.jackrabbit.oak.security.authentication.LoginContextProviderImpl.getLoginContext(LoginContextProviderImpl.java:85)
	at org.apache.jackrabbit.oak.core.ContentRepositoryImpl.login(ContentRepositoryImpl.java:161)
	at org.apache.jackrabbit.oak.jcr.repository.RepositoryImpl.login(RepositoryImpl.java:256)
	... 43 more

Running org.apache.jackrabbit.oak.run.osgi.JsonConfigRepFactoryTest
{noformat}",CI buildbot test,['pojosr'],OAK,Task,Minor,2014-10-14 06:08:06,4
12746165,Observation tests sporadically failing,"{{JackrabbitNodeTest#testRenameEventHandling}} fails sporadically on the Apache buildbot with missing events (e.g. http://ci.apache.org/builders/oak-trunk-win7/builds/642). 

Same holds for other tests in the {{ObservationIT}} suite. 
",CI buildbot observation test,['jcr'],OAK,Bug,Major,2014-10-06 15:17:11,3
12744000,Release Oak 1.0.7,Issues for 1.0.7: https://issues.apache.org/jira/issues/?jql=project%20%3D%20OAK%20AND%20fixVersion%20%3D%201.0.7,Release,[],OAK,Task,Major,2014-09-25 11:26:20,1
12742399,Killing a node may stop async index update to to 30 minutes (Tar storage),"When killing a node that is running the sync index update, then this async index update will not run for up to 15 minutes, because the lease time is set to 15 minutes.

I think the lease time should be much smaller, for example 1 minute, or maybe even 10 seconds.

Also, we might need to better document this issue (in addition to the warning in the log file). For non cluster case we can do away with lease time out and this for such cases indexing would not get paused upon restart post abrupt shutdown",resilience,['query'],OAK,Improvement,Major,2014-09-18 09:27:09,4
12738281,"DocumentStore API: batch create, but no batch update","The DocumentStore API currently has a call for creating many nodes at once.

However, this will sometimes fail for large save operations in JCR, because in the DS persistence, JCR-deleted nodes are still present (with a deleted flag). This causes two subsequent sequences of

1) create test container
2) create many child nodes
3) remove test container

to behave very differently, depending on whether the test container is created for the first time or not.

(see CreateManyChildNodesTest)

",performance,"['core', 'documentmk', 'mongomk', 'rdbmk']",OAK,Improvement,Blocker,2014-09-01 15:04:34,1
12738253,JMX stats for operations being performed in DocumentNodeStore,"Currently DocumentStore performs various background operations like

# Cache consistency check
# Pushing the lastRev updates
# Synchrnizing the root node version

We should capture some stats like time taken in various task and expose them over JMX to determine if those background operations are performing well or not. For example its important that all tasks performed in background task should be completed under 1 sec (default polling interval). If the time taken increases then it would be cause of concern

See http://markmail.org/thread/57fax4nyabbubbef",tooling,['documentmk'],OAK,Improvement,Major,2014-09-01 12:05:40,4
12737509,MBean to provide consolidated cache stats ,Currently DocumentNodeStore has 5 different types of caches and each register there own MBean. To get a better understanding of the overall cache usage it would be good to have a {{ConsolidatedCacheStatsMBean}} which depicts all the stats in tabular form,monitoring,['core'],OAK,Improvement,Minor,2014-08-28 17:32:24,4
12735772,Long running JCR session prevent live cleanup in Segment FileStore,"Cleanup operation in SegmentNodeStore detects the un referenced garbage and clean it up. To determine the reference validity it starts with an initial set of SegmentId which have a live java reference. 

This works fine for simple setup but when Oak repository is used in an application (like Sling) where application code can create long running session (for observation) then such session are bound to old NodeState at time of startup. Such references prevent the cleanup logic to remove older revisions while system is running. Such revisions can only be removed via an offline compaction-> cleanup.

Need to find out a way where we can _migrate_ such old NodeState references to newer revisions",gc,['segmentmk'],OAK,Bug,Major,2014-08-21 12:32:27,3
12730767,Verify the maven baseline output and fix the warnings,"Currently the maven baseline plugin only logs the package version mismatches, it doesn't fail the build. It would be beneficial to start looking at the output and possibly fix some of the warnings (increase the OSGi package versions).",build modularization osgi technical_debt,['core'],OAK,Improvement,Major,2014-07-30 11:56:22,3
12730473,Observation events accessibility not checked correctly,Before delivering an observation event it is checked whether the respective item is actually accessible through the associated session. However the check is currently done against the state of the session from the time the event listener was registered instead of from the time the event is being sent. ,observation,"['core', 'jcr']",OAK,Bug,Major,2014-07-29 12:56:48,3
12729663,Limit no of children listed with ls command in Oak Console,Oak Console 'ls' command currently lists down all the child node which cause issue for node have large no of children. As a fix ls command should dump max say 50 child node and allow user to change the limit as part of arguments,console,['run'],OAK,Improvement,Minor,2014-07-25 06:13:55,4
12729658,Add command to dump Lucene index in Oak Console,"Add a command in Oak Run Console to dump lucene index and also provide stats related to Lucene index

",console,['run'],OAK,New Feature,Minor,2014-07-25 05:13:09,4
12729095,Implement full scale Revision GC for DocumentNodeStore,"So far we have implemented garbage collection in some form with OAK-1341. Those approaches help us remove quite a bit of garbage (mostly due to deleted nodes) but till some part is left

However full GC is still not performed due to which some of the old revision related data cannot be GCed like
* Revision info present in revision maps of various commit roots
* Revision related to unmerged branches (OAK-1926)
* Revision data created to property being modified by different cluster nodes

So having a tool which can perform above GC would be helpful. For start we can have an implementation which takes a brute force approach and scans whole repo (would take quite a bit of time) and later we can evolve it. Or allow system admins to determine to what level GC has to be done",resilience scalability,['mongomk'],OAK,New Feature,Major,2014-07-23 06:54:24,1
12728484,Add path exclusion to JackrabbitEventFilter,Implement the new Jackrabbit API introduced with JCR-3797,observation,"['core', 'jcr']",OAK,New Feature,Major,2014-07-21 07:20:33,3
12727885,Wrong values reported for OBSERVATION_EVENT_DURATION,The value reported for the {{RepositoryStatistics.Type#OBSERVATION_EVENT_DURATION}} statistic is wrong. Instead of the total time spent *processing* observation events it reports the total time *producing* observation events. ,monitoring observation,['jcr'],OAK,Bug,Major,2014-07-17 09:27:23,3
12727863,Fail fast on branch conflict,"The current MongoMK implementation performs retries when it runs into merge
conflicts caused by collisions. It may be possible to resolve a conflict by resetting
the branch back to the state as it was before the merge and re-run the commit hooks again.
This helps if the conflict was introduced by a commit hook. At the moment the retries
also happen when the conflict was introduced before the merge. In this case, a retry
is useless and the commit should fail fast.
",performance,"['core', 'mongomk']",OAK,Improvement,Minor,2014-07-17 07:09:16,1
12727333,Optimize the diff logic for large number of children case ,"DocumentNodeStore currently makes use of query to determine child nodes which have changed after certain time. Query used is something like

{noformat}
db.nodes.find({ _id: { $gt: ""3:/content/foo/01/"", $lt: ""3:/content/foo010"" }, _modified: { $gte: <start time> } }).sort({_id:1})
{noformat}

OAK-1966 tries to optimize the majority case where start times is recent and in that case it makes use of _modified index. However if the start time is quite old and a node has large number of children say 100k then it would involve scan of all those 100k nodes as _modified index would not be of much help. 

Instead of querying like this we can have a special handling for cases where large number of children are involved. It would involve following steps

After analyzing the runtime queries in most case it is seen that even with old modified time the number of change nodes is < 50

# Mark parent nodes which have large number of children say > 50
# On such nodes we would keep an array of \{modifiedtime, childName\} ## Array would be bounded say keep last 50 updates. This can be done via splice and push operators [1]
## Each entry in array would record modifiedtime and name of child node which was modified. 
## Array would be sorted on modifiedtime
# Each updated to any child belonging to such parent would also involve update to above array
# When we query for modified we check if the parent has such an array (if parent is in cache) and if that array has time entries from the required start time we directly make use of that and avoid the query

This should reduce needs for such queries in majority of cases

[1] http://docs.mongodb.org/manual/reference/operator/update-array/
 ",performance resilience,['mongomk'],OAK,Improvement,Major,2014-07-15 13:13:52,1
12726576,Performance degradation due to SessionDelegate.WarningLock,"In OAK-1703, we have added a new class WarningLock that internally uses an Exception to remember the stack trace. This seems to be used for every SessionDelegate object. With Java 6 and older, this is very problematic because it will cause ""java.lang.Throwable.fillInStackTrace(Native Method)"" to be called for almost every call to any of the Oak JCR methods, and ""fillInStackTrace(Native Method)"" is known to be be very slow. Java 7, I believe, will at some point give up and not fill in the stack trace any more. But with Java 6 and older, this is a big problem.",Performance,['jcr'],OAK,Bug,Major,2014-07-10 12:46:38,3
12726532,Expose URL for Blob source ,"In certain scenarios for performance reasons its desirable to have direct access to the Blob source. 

For e.g. if using a FileDataStore having a direct access to the native file system path of the blob (if not stored in chunks) is more useful than repository path e.g. native tools don't understand repository path, instead file system path can be passed directly to native tools for processing binary.

Another usecase being ability exposed signed S3 url which would allow access to binary content directly",datastore,['core'],OAK,Improvement,Major,2014-07-10 09:23:04,4
12726185,Session.logout performance poor,"Problem:
Session.logout was observed to take 14% of time in a performance test of a reasonably real-world load.

Method:
Use the attached sling junit test case to run 8 concurrent instances of the test. profile with YourKit or  similar and see >50% time taken by logout.

Expected:
Logout should be practically free.

Solution:
The attached patch avoids a bug in guava-15 (still present in guava-17 the latest) where the former use of addCallback triggered many CancellationExceptions when sessions were quickly created and logged out.
",Performance,['jcr'],OAK,Bug,Major,2014-07-08 20:50:32,3
12725972,Set correct OSGi package export version,"This issue serves as a reminder to set the correct OSGi package export versions before we release 1.2.

OAK-1536 added support for the BND baseline feature: the baseline.xml files in the target directories should help us figuring out the correct versions. ",modularization osgi technical_debt,[],OAK,Task,Critical,2014-07-08 08:52:12,3
12724756,MAX_QUEUED_CONTINUATIONS feature not working in EventGenerator class,"Since OAK-1422 the  {{Continuation}} created in {{fullQueue()}} is put to the front of the List. This causes it to be taken right off the list again on the next call to {{generate()}} instead of first continuing with the rest of the list allowing it to shrink. As a result the list may grow up to 2 x {{MAX_QUEUED_CONTINUATIONS}} instead of 1 + {{MAX_QUEUED_CONTINUATIONS}} as anticipated. 

",Observation,['core'],OAK,Bug,Major,2014-07-01 15:50:06,3
12717071,Generic operation tasks should be able to return specific results,"The generic interface for operation management tasks added with OAK-1160 does so far not provide a way for specific tasks to return a value apart from a genetic status. With the consistency checking we are starting to add (OAK-1448) such a needs start to arise. 

To address this I propose to change type of the tasks that can be passed to the constructor of {{ManagementOperation}}. The result type of the task (i.e. {{Callable}}) should change from {{Long}} to some generic container, which would carry the result of the task. ",monitoring,['core'],OAK,Improvement,Major,2014-05-28 16:01:06,3
12715768,Use SegmentMK for testing where possible,There are still a few places left where {{MicroKernelImpl}} is used for running tests. As {{SegementMK}} is the default for going forward I suggest we change those tests accordingly. ,test,"['it', 'solr', 'upgrade']",OAK,Improvement,Major,2014-05-21 14:43:48,3
12715514,"ISE: ""Unexpected value record type: f2"" is thrown when FileBlobStore is used","The stacktrace of the call shows something like
{code}
20.05.2014 11:13:07.428 *ERROR* [OsgiInstallerImpl] com.adobe.granite.installer.factory.packages.impl.PackageTransformer Error while processing install task.
java.lang.IllegalStateException: Unexpected value record type: f2
at org.apache.jackrabbit.oak.plugins.segment.SegmentBlob.length(SegmentBlob.java:101)
at org.apache.jackrabbit.oak.plugins.value.BinaryImpl.getSize(BinaryImpl.java:74)
at org.apache.jackrabbit.oak.jcr.session.PropertyImpl.getLength(PropertyImpl.java:435)
at org.apache.jackrabbit.oak.jcr.session.PropertyImpl.getLength(PropertyImpl.java:376)
at org.apache.jackrabbit.vault.packaging.impl.JcrPackageImpl.getPackage(JcrPackageImpl.java:324)
{code}

The blob store was configured correctly and according to the log also correctly initialized
{code}
20.05.2014 11:11:07.029 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService Initializing SegmentNodeStore with BlobStore [org.apache.jackrabbit.oak.spi.blob.FileBlobStore@7e3dec43]
20.05.2014 11:11:07.029 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService Component still not activated. Ignoring the initialization call
20.05.2014 11:11:07.077 *INFO* [FelixStartLevel] org.apache.jackrabbit.oak.plugins.segment.file.FileStore TarMK opened: crx-quickstart/repository/segmentstore (mmap=true)
{code}

Under which circumstances can the length within the SegmentBlob be invalid?
This only happens if a File Blob Store is configured (http://jackrabbit.apache.org/oak/docs/osgi_config.html). If a file datastore is used, there is no such exception.",resilience,['blob'],OAK,Bug,Major,2014-05-20 15:43:31,2
12715494,Document Oak Clustering,"the 'clustering' page in our oak documentation is currently an empty placeholder and it would be great if there would be some initial pointers.

[~chetanm], [~mreutegg], what do you think?",documentation,['doc'],OAK,Task,Major,2014-05-20 14:34:08,4
12713027,ConstraintViolationException seen with multiple Oak/Mongo with ConcurrentCreateNodesTest,"While running ConcurrentCreateNodesTest with 5 instances writing to same Mongo instance following exception is seen

{noformat}
Exception in thread ""Background job org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer@3f56e5ed"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:111)
    at org.apache.jackrabbit.oak.benchmark.AbstractTest$1.run(AbstractTest.java:481)
Caused by: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:225)
    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:679)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:553)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:417)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:414)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:308)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:127)
    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:414)
    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:100)
    ... 1 more
Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakConstraint0001: /: The primary type rep:root does not exist
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.constraintViolation(TypeEditor.java:150)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.getEffectiveType(TypeEditor.java:286)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.<init>(TypeEditor.java:101)
    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditorProvider.getRootEditor(TypeEditorProvider.java:85)
    at org.apache.jackrabbit.oak.spi.commit.CompositeEditorProvider.getRootEditor(CompositeEditorProvider.java:80)
    at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:53)
    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)
    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)
    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch$InMemory.merge(AbstractNodeStoreBranch.java:498)
    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch.merge(AbstractNodeStoreBranch.java:300)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:129)
    at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)
    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1275)
    at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:405)
    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:551)
    ... 7 more
{noformat}

This has been reported by [~rogoz]",concurrency,['mongomk'],OAK,Bug,Minor,2014-05-08 07:07:54,1
12712931,Debugging console,It would be nice to for {{oak-run}} to come with a debugging console like the {{cli}} mode in {{jackrabbit-standalone}}.,production tools,['run'],OAK,New Feature,Major,2014-05-07 20:21:02,4
12712602,Missing documentation around Ordered Index,"oak-doc is missing documentation about the usage of the OrderedIndex.
",doc,['doc'],OAK,Improvement,Major,2014-05-06 15:31:41,3
12711671,ConcurrentConflictTest fails occasionally,"Occurs every now and then on buildbot. E.g.:
http://ci.apache.org/builders/oak-trunk-win7/builds/16",concurrency,"['core', 'mongomk']",OAK,Bug,Minor,2014-05-01 07:28:50,1
12710305,Better cooperation for conflicting updates across cluster nodes,"Every now and then we see commit failures in a cluster when many sessions try to update the same property or perform some other conflicting update.

The current implementation will retry the merge after a delay, but chances are some session on another cluster node again changed the property in the meantime. This will lead to yet another retry until the limit is reached and the commit fails. The conflict logic is quite unfair, because it favors the winning session.

The implementation should be improved to show a more fair behavior across cluster nodes when there are conflicts caused by competing session.",concurrency scalability,"['core', 'documentmk']",OAK,Improvement,Major,2014-04-24 12:59:37,1
12710287,Inaccurate values reported by RepositoryStatsMBean,"This is a follow up from OAK-1757.

The accuracy of the values reported by {{RepositoryStatsMBean}} for {{SESSION_WRITE_DURATION}} and {{SESSION_READ_DURATION}} depend on the value of {{Clock#FAST_CLOCK_INTERVAL}}. 

The 1s reset interval of the duration counters might be to small for the inaccuracies of the clock resolution to average out. ",monitoring,['core'],OAK,Bug,Major,2014-04-24 10:51:34,3
12709986,Sporadic IllegalStateException in AbstractServiceTracker.getServices,"Seeing the below IlegalStateException about tracker being null several times on a 4-node oak-mongo cluster. There were no log.warn 'Timed out waiting for change processor to stop' near those errors (but there was once hour(s) before in one case).

{code}16.04.2014 05:34:50.908 *ERROR* [oak-executor-1619] org.apache.sling.extensions.threaddump.internal.Activator Uncaught exception in Thread Thread[oak-executor-1619,1,Configuration
 Admin Service]
java.lang.IllegalStateException: null
        at com.google.common.base.Preconditions.checkState(Preconditions.java:134)
        at org.apache.jackrabbit.oak.spi.whiteboard.AbstractServiceTracker.getServices(AbstractServiceTracker.java:60)
        at org.apache.jackrabbit.oak.spi.whiteboard.WhiteboardExecutor.execute(WhiteboardExecutor.java:40)
        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1.run(BackgroundObserver.java:130)
        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$ListenableFutureTask.run(BackgroundObserver.java:283)
        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$ListenableFutureTask.done(BackgroundObserver.java:278)
        at java.util.concurrent.FutureTask$Sync.innerSet(FutureTask.java:281)
        at java.util.concurrent.FutureTask.set(FutureTask.java:141)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:339)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{code}",concurrency,['core'],OAK,Bug,Major,2014-04-23 09:30:36,3
12709744,add docu how to connect to Mongo w/ credentials,"we should document how to use MongoMK when MongoDB requires credentials to connect to. According to [~chetanm] this would work as
{quote}
in OSGi config we can specify uri [1] to mongodb://admin:admin@localhost:27017

[1] http://api.mongodb.org/java/current/com/mongodb/MongoURI.html 
{quote}",documentation,"['doc', 'mongomk']",OAK,Improvement,Trivial,2014-04-22 11:21:11,4
12709720,Node name queries should use an index,"The node name queries don't use any index currently, making them really slow and triggering a lot of traversal warnings.

Simply adding node names to a property index would be too much content indexed, but as Lucene already indexes the node names, using this index would be one viable option.

{code}
/jcr:root//*[fn:name() = 'jcr:content']
/jcr:root//*[jcr:like(fn:name(), 'jcr:con%')] 
{code}",performance,['query'],OAK,Improvement,Major,2014-04-22 09:15:40,4
12709036,Cleanup documentation of _modCount,"The documentation of

  Document.MOD_COUNT

""The modification count on the document. This is an long value incremented on every modification.""

gives the impression that this is a mechanism that is part of the DocumentStore API contract (which IMHO it is not)",documentation,['mongomk'],OAK,Task,Minor,2014-04-17 12:26:45,1
12707639,Enable passing of a execution context to runTest in multi threaded runs,"Benchmark runner has support for specifying concurrency levels to execute the test with varying level of concurrency. In most cases the test case would be operating on a JCR session. With multi threaded runs we need a way to have jcr session bound to that thread of execution.

To support that {{AbstractTest}} should provide a way for client to provide a executionContext object which sub classes can provide. That context would be managed per thread and passed to the runTest method if not null",concurrency test,['benchmarks'],OAK,Improvement,Minor,2014-04-10 05:21:44,4
12707374,Improve warning logged on concurrent Session access,"OAK-1601 introduced warnings that are logged when a session is accessed concurrently from different threads. The modalities however differ from those of Jackrabbit 2. The message 

{code}
Attempt to perform ""sessionOperation"" while another thread is concurrently writing to ""session"". Blocking until the other thread is finished using this session. Please review your code to avoid concurrent use of a session.
{code}

is logged for the current thread

* if the current threads attempts a write operation while another thread already executes a write operation in Jackrabbit 2,
* if the current thread attempts a write operation while another thread already executes any operation. 

We should make these warnings identical to those of Jackrabbit 2.

",concurrency,['jcr'],OAK,Bug,Major,2014-04-09 07:26:44,3
12707136,Improve LargeOperationIT accuracy for document nodes store fixture,As [noted | https://issues.apache.org/jira/browse/OAK-1414?focusedCommentId=13942016&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13942016] on OAK-1414 {{LargeOperationIT}} is somewhat inaccurate for the document node store fixture where the collected data tends to be noisy. We should look into ways to make  the tests results more accurate for this case.,test,['jcr'],OAK,Improvement,Major,2014-04-08 08:48:51,3
12706946,"document atomicity of DS.update(collection, keys, update)","Please document (I'll assume it's similar to ""remove"", in that it is ""best effort"")?",concurrency,['mongomk'],OAK,Bug,Major,2014-04-07 13:21:51,1
12706594,"document atomicity of DS.remove(collection, keys)","[~mreutegg]
I believe it's best effort (looking at the MongoDB impl), but it would be good to clarify.

In particular, should the operation abort then one removal failed, or keep going? What's the expectation when a document doesn't exist?",documentation,"['core', 'mongomk']",OAK,Task,Major,2014-04-04 09:14:11,1
12706187,JCR Event Info should contain NodeType for all Events ,"OAK-1661 added node type information for {{NODE_ADDED}} and {{NODE_REMOVED}} events. We should consider adding this for all event types however.  Even property events would contain node type of the node the property is associated with (parent).

An implication of this is however that we also need to adapt the TCK as this will cause {{org.apache.jackrabbit.test.api.observation.GetInfoTest}} to fail, which expects the info map to be generally empty. ",observation,['jcr'],OAK,Improvement,Minor,2014-04-02 15:03:21,3
12706142,FileDataStore inUse map causes contention in concurrent env,"JR2 FileDataStore#inUseMap [1] is currently a synchronized map and that at times causes contention concurrent env. This map is used for supporting the Blob GC logic for JR2. 

With Oak this map content is not used. As a fix we can either

# Set inUseMap to a Guava Cache Map which has weak keys and value
# Set inUseMap to a no op map where all put calls are ignored
# Modify FDS to disable use of inUseMap or make {{usesIdentifier}} protected

#3 would be a proper fix and #2 can be used as temp workaround untill FDS gets fixed

[1] https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-data/src/main/java/org/apache/jackrabbit/core/data/FileDataStore.java#L118",concurrency,['core'],OAK,Improvement,Minor,2014-04-02 11:17:55,4
12705591,Creating multiple checkpoint on same head revision overwrites previous entries,"Currently when a checkpoint is created in DocumentNodeStore then it is saved in form of currentHeadRev=>expiryTime. Now if multiple checkpoints are created where head revision has not changed then only the last one would be saved and previous entries would be overridden as revision is used as key

One fix would be to change the expiry time only if the new expiry time is greater than previous entry. However doing that safely in a cluster (check then save) is currently not possible with DocumentStore API as the modCount check if only supported for Nodes.

",resilience,['mongomk'],OAK,Bug,Minor,2014-03-31 07:13:16,1
12703715,Implement noInternal from JackrabbitEventFilter,Implement the {{noInternal}} flag that will be added with JCR-3759. ,observation,['core'],OAK,Improvement,Minor,2014-03-26 13:15:19,3
12703438,Omit warnings about accessing commit related info when external events are excluded,"Currently we log a warning when calling {{Event.getUserID()}}, {{Event.getUserData()}} or {{Event.getDate()}} without first checking whether the event is not external. However we should inhibit such warnings for the case where the filter already excludes external events. ",observation,['core'],OAK,Improvement,Minor,2014-03-25 12:08:32,3
12702892,org.apache.jackrabbit.oak.plugins.document.mongo.CacheInvalidationIT fails,"Fails frequently on my W7 desktop:

testCacheInvalidationHierarchicalNotExist(org.apache.jackrabbit.oak.plugins.document.mongo.CacheInvalidationIT)  Time elapsed: 0.04 sec  <<< FAILURE!
java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:92)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertTrue(Assert.java:54)
        at org.apache.jackrabbit.oak.plugins.document.mongo.CacheInvalidationIT.testCacheInvalidationHierarchicalNotExist(CacheInvalidationIT.java:171)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
",CI,['mongomk'],OAK,Task,Minor,2014-03-21 16:03:50,4
12702434,DocumentNS: Implement refined conflict resolution for addExistingNode conflicts,Implement refined conflict resolution for addExistingNode conflicts as defined in the parent issue for the document NS.,resilience,['mongomk'],OAK,Sub-task,Major,2014-03-19 17:42:06,1
12702110,Expose RevisionGCMBean for supported NodeStores ,{{NodeStore}} implementations should expose the {{RevisionGCMBean}} in order to be interoperable with {{RepositoryManagementMBean}}. See OAK-1160.,monitoring,"['mongomk', 'segmentmk']",OAK,Improvement,Major,2014-03-18 10:19:10,3
12702108,Expose FileStoreBackupRestoreMBean for supported NodeStores,{{NodeStore}} implementations should expose the {{FileStoreBackupRestoreMBean}} in order to be interoperable with {{RepositoryManagementMBean}}. See OAK-1160.,monitoring,"['mongomk', 'segment-tar']",OAK,Improvement,Major,2014-03-18 10:16:05,0
12701867,Incorrect handling of addExistingNode conflict in NodeStore,"{{MicroKernel.rebase}} says: ""addExistingNode: node has been added that is different from a node of them same name that has been added to the trunk.""

However, the {{NodeStore}} implementation
# throws a {{CommitFailedException}} itself instead of annotating the conflict,
# also treats the equal childs with the same name as a conflict. ",concurrency observation technical_debt,['mongomk'],OAK,Bug,Major,2014-03-17 10:53:23,1
12700973,ShareableNodesTest.testAddShareableMixin failures,"I'm seeing this test fail consistently on our internal CI builds.

_org.apache.jackrabbit.core.observation.ShareableNodesTest.testAddShareableMixin_:
bq. Change processor already stopped

Stacktrace
{code}
java.lang.IllegalStateException: Change processor already stopped
	at com.google.common.base.Preconditions.checkState(Preconditions.java:150)
	at org.apache.jackrabbit.oak.jcr.observation.ChangeProcessor$RunningGuard.stop(ChangeProcessor.java:259)
	at org.apache.jackrabbit.oak.jcr.observation.ChangeProcessor.stop(ChangeProcessor.java:192)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.stop(ObservationManagerImpl.java:267)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationManagerImpl.dispose(ObservationManagerImpl.java:117)
	at org.apache.jackrabbit.oak.jcr.session.SessionContext.dispose(SessionContext.java:387)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl$10.perform(SessionImpl.java:465)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl$10.perform(SessionImpl.java:462)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:263)
	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.safePerform(SessionDelegate.java:306)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.safePerform(SessionImpl.java:129)
	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.logout(SessionImpl.java:462)
	at org.apache.jackrabbit.test.AbstractJCRTest.cleanUp(AbstractJCRTest.java:439)
	at org.apache.jackrabbit.test.AbstractJCRTest.tearDown(AbstractJCRTest.java:448)
	at org.apache.jackrabbit.test.api.observation.AbstractObservationTest.tearDown(AbstractObservationTest.java:67)
	at junit.framework.TestCase.runBare(TestCase.java:140)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:464)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at org.apache.jackrabbit.test.ConcurrentTestSuite.access$001(ConcurrentTestSuite.java:29)
	at org.apache.jackrabbit.test.ConcurrentTestSuite$2.run(ConcurrentTestSuite.java:67)
	at EDU.oswego.cs.dl.util.concurrent.PooledExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Thread.java:662)
{code}",observation,['core'],OAK,Bug,Major,2014-03-12 13:53:09,3
12699169,Implement low disk space and low memory monitoring,We should implement these monitoring for those MKs where it makes sense. ,monitoring,['core'],OAK,Sub-task,Minor,2014-03-06 15:03:55,3
12699101,Concurrent FlatTreeWithAceForSamePrincipalTest fails on Oak-Mongo,The benchmark test fails when run concurrently in a cluster. Setting up the test content fails with a conflict. I assume this happens because nodes in the permission store are populated concurrently and may conflict.,concurrency,"['mongomk', 'run']",OAK,Bug,Minor,2014-03-06 09:29:55,1
12698610,ObservationTest failure on Windows,"{{ObservationTest}} fails often on some Windows machines:

{noformat}
pathFilter[3](org.apache.jackrabbit.oak.jcr.observation.ObservationTest)  Time elapsed: 0.864 sec  <<< FAILURE!
java.lang.AssertionError: Missing events: [path = /events/only/here/below/this, type = 1]
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationTest.pathFilter(ObservationTest.java:315)
{noformat}

and

{noformat}
filterPropertyOfParent[2](org.apache.jackrabbit.oak.jcr.observation.ObservationTest)  Time elapsed: 0.88 sec  <<< FAILURE!
java.lang.AssertionError: Missing events: [path = /test_node/a/jcr:primaryType, type = 4, path = /test_node/a/foo, type = 4, path = /test_node/a/b, type = 1]
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationTest.filterPropertyOfParent(ObservationTest.java:614)
{noformat}

I have the suspicion this is an issue with the test similar to OAK-1486",observation,"['core', 'jcr']",OAK,Bug,Major,2014-03-04 11:13:04,3
12698426,BackgroundObserverTest occasionally failing,"{{BackgroundObserverTest.concurrentObservers}} occasionally fails for the Windows 7 CI build:

{noformat}
concurrentObservers(org.apache.jackrabbit.oak.spi.commit.BackgroundObs)  Time elapsed: 5.058 sec  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.jackrabbit.oak.spi.commit.BackgroundObserverTest.concurrentObservers(BackgroundObserverTest.java:62)
{noformat}",observation,['core'],OAK,Bug,Major,2014-03-03 13:57:15,3
12697726,Failing test for MergeSortedIterators,"While running Oak in a two node clutser following exception is seen. It basically comes because the AsynchUpdate tries to update async-status concurrently

{noformat}
27.11.2013 17:56:35.507 *ERROR* [pool-5-thread-1] org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@fcf98c2 : com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diff(MongoMK.java:165) ~[na:na]
	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:481) ~[na:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:103) ~[na:na]
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler:2.4.2]
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler:2.4.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.getNode(MongoNodeStore.java:507) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffFewChildren(MongoMK.java:313) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffImpl(MongoMK.java:229) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:168) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:165) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 11 common frames omitted
Caused by: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:103) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.next(MergeSortedIterators.java:85) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getLatestValue(NodeDocument.java:1041) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getNodeAtRevision(NodeDocument.java:456) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.readNode(MongoNodeStore.java:653) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.access$000(MongoNodeStore.java:80) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:510) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:507) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 23 common frames omitted
{noformat}",cluster,['mongomk'],OAK,Bug,Major,2014-02-27 15:25:45,1
12696542,Many extra events are dispatched from a move event,"When moving a node many extra events are dispatched in OAK in compared to other implementations

On Oak a node added and node remove events are dispatched for each node in the hierarchy being moved.  As well there is a property add and property remove event dispatched for each property in the node hierarchy.  

This compares to previous implementations where only a Node Moved, node added and node removed event is dispatched for the parentnode being moved.

See [0] for an example.

For me this is problematic for a couple of reasons:

1) We are dispatching more events than we did previously.  In cases where nodes are frequently moved this will add extra load on the system. 
2) It is becoming increasingly difficult to ignore events related to a move without spending extra cycles to make that determination. 
3) Many pre-existing event listeners will be executing on events that they previously would not have.

I know the JCR spec indicates that an implementation may choose to dispatch these events or not, but I suggest we change OAK to not throw these extra events.  If we do not many observation listeners will act on events they previously did not will likely cause problems.

Also, if we could add a simple marker in any event’s info map which is related to a node move (ie: the node removed, node added etc) it would be very helpful when trying to ignore events caused by a move.  (which I believe to be the case in many situations).

[0] 
Move “c” in the hierarchy below from /a/b to /a/z:

/a/b/c/d/e
to:
/a/z/c/d/e

Results in:

CRX2:
/a/b, type: {node removed}
/a/z/b, type: {node added}
/a/z/b, type: {node moved}

OAK:
/a/b/c, type: {node removed}
/a/z/c, type: {node moved}
/a/z/c, type: {node added}
/a/b/c/jcr:primaryType, type: {property removed}
/a/b/c/jcr:createdBy, type: {property removed}
/a/b/c/jcr:created, type: {property removed}
/a/b/c/d, type: {node removed}
/a/z/c/jcr:primaryType, type: {property added}
/a/z/c/jcr:createdBy, type: {property added}
/a/z/c/jcr:created, type: {property added}
/a/z/c/d, type: {node added}
/a/b/c/d/jcr:primaryType, type: {property removed}
/a/b/c/d/jcr:createdBy, type: {property removed}
/a/b/c/d/jcr:created, type: {property removed}
/a/b/c/d/e, type: {node removed}
/a/z/c/d/jcr:primaryType, type: {property added}
/a/z/c/d/jcr:createdBy, type: {property added}
/a/z/c/d/jcr:created, type: {property added}
/a/z/c/d/e, type: {node added}
/a/b/c/d/e/jcr:primaryType, type: {property removed}
/a/b/c/d/e/jcr:createdBy, type: {property removed}
/a/b/c/d/e/jcr:created, type: {property removed}
/a/z/c/d/e/jcr:primaryType, type: {property added}
/a/z/c/d/e/jcr:createdBy, type: {property added}
/a/z/c/d/e/jcr:created, type: {property added}
",observation,['jcr'],OAK,Improvement,Major,2014-02-21 14:10:42,3
12696536,JCR version purge,Expose capability to purge JCR versions so that higher level apps can start a clean up.,production resilience,['jcr'],OAK,Improvement,Major,2014-02-21 13:53:37,1
12696532,document oak:unstructured performance advantages,"For long child node lists it is much better (in terms of performance) to use a non-ordered node type. Unfortunately, nt:unstructured is ordered.
We should have a ""performance hint"" on this in the docs.",documentation,"['doc', 'jcr']",OAK,Task,Trivial,2014-02-21 13:42:25,3
12696529,Warn on huge multi-valued properties,It is an explicit design non-goal of Oak to support huge amounts of values in multi-valued properties. If a user still tries to create these we should at least throw a WARN in the logs to indicate that usage of MVPs is wrong.,production resilience,['jcr'],OAK,Improvement,Trivial,2014-02-21 13:40:13,3
12696503,Offline tool to repair MongoMK documents,"For cases where the document semantics in Mongo that are created by Oak get corrupted to a point that Oak does not come up anymore (but MongoDB is still available), we should have a mechanism to fix those inconsistencies.

Of course, one could use Mongo tools like cmdline or MongoHub to manually go in, but an automated approach would be preferable in the medium term.",production resilience tools,['mongomk'],OAK,Improvement,Minor,2014-02-21 12:03:16,4
12693122,Occasional ConcurrentFileOperationsTest failure,"Most recent test failure on buildbot http://ci.apache.org/builders/oak-trunk/builds/4290/steps/compile/logs/stdio says:

{noformat}
concurrent[2](org.apache.jackrabbit.oak.jcr.ConcurrentFileOperationsTest)  Time elapsed: 1.69 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: OakState0001: Unresolved conflicts in /test-node/session-6
{noformat}",concurrency,"['core', 'mongomk']",OAK,Bug,Minor,2014-02-04 15:24:49,1
12692131,Only one Observer per session,"As mentioned in OAK-1332, a case where a single session registers multiple observation listeners can be troublesome if events are delivered concurrently to all of those listeners, since in such a case the {{NamePathMapper}} and other session internals will likely suffer from lock contention.

A good way to avoid this would be to have all the listeners registered within a single session be tied to a single {{Observer}} and thus processed sequentially.

Doing so would also improve performance as the listeners could leverage the same content diff. As the listeners come from a single session and thus presumably from a single client, there's no need to worry about one client blocking the work of another.",observation,['jcr'],OAK,Improvement,Major,2014-01-29 20:32:24,3
12691621,DocumentNodeState#compareAgainstBaseState too slow,"{{DocumentNodeState#compareAgainstBaseState}} usually falls back to the default implementation in {{AbstractNodeState#compareAgainstBaseState(NodeState, NodeStateDiff)}}, which is slow. See also the TODO in the code. This negatively affects performance when generation observation events. ",observation,['mongomk'],OAK,Improvement,Major,2014-01-28 11:51:42,1
12691096,Expose the preferred transient space size as repository descriptor ,"The problem is that the different stores have different transient space characteristics. for example the MongoMK is very slow when handling large saves.

suggest to expose a repository descriptor that can be used to estimate the preferred transient space, for example when importing content.

so either a boolean like: 
  {{option.infinite.transientspace}}

or a number like:
  {{option.transientspace.preferred.size}}

the later would denote the average number of modified node states that should be put in the transient space before the persistence starts to degrade.",api,"['core', 'jcr']",OAK,Improvement,Major,2014-01-24 19:08:34,4
12689805,Implement RepositoryStatistics from Jackrabbit API,Implement repository statistics (TimeSeries) for those values it makes sense on Oak.,monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2014-01-20 12:35:07,3
12688733,Reduce calls to MongoDB,As discussed with Chetan offline we'd like to reduce the number of calls to MongoDB when content is added to the repository with a filevault package import.,performance,"['core', 'mongomk']",OAK,Improvement,Major,2014-01-14 13:21:44,1
12688013,Bundle nodes into a document,"For very fine grained content with many nodes and only few properties per node it would be more efficient to bundle multiple nodes into a single MongoDB document. Mostly reading would benefit because there are less roundtrips to the backend. At the same time storage footprint would be lower because metadata overhead is per document.

Feature branch - https://github.com/chetanmeh/jackrabbit-oak/compare/trunk...chetanmeh:OAK-1312

*Feature Docs* - http://jackrabbit.apache.org/oak/docs/nodestore/document/node-bundling.html",performance,"['core', 'documentmk']",OAK,New Feature,Major,2014-01-09 07:55:27,4
12684964,RandomizedReadTest fails with OutOfMemoryError: PermGen space,"This happened while running the maven build with {{-PintegrationTesting}}:

{code}
Running org.apache.jackrabbit.oak.jcr.random.RandomizedReadTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.418 sec
org.apache.maven.surefire.util.SurefireReflectionException: java.lang.reflect.InvocationTargetException; nested exception is java.lang.reflect.InvocationTargetException: null
java.lang.reflect.InvocationTargetException
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space

Results :

Tests run: 722, Failures: 0, Errors: 0, Skipped: 48
{code}

The crucial point being Surefire silently ignoring the following tests such that the build happily succeeds making following failures. Note, that test suite consists of  2003 tests in contrast to the 722 reported by Surefire. ",test,['jcr'],OAK,Bug,Major,2013-12-16 14:30:35,3
12684921,ObservationManager#removeEventListener prone to deadlocks ,"The contract for {{ObservationManager#removeEventListener}} mandates: ""A listener may be deregistered while it is being executed. The deregistration method will block until the listener has completed executing.""

However a strict implementation of this contract is prone to deadlocks: clients unregistering event listeners need to take care not to hold a lock that is also acquired from the event listener being unregistered as this will lead to a deadlock
",observation,['jcr'],OAK,Improvement,Major,2013-12-16 11:03:06,3
12684481,"Root.commit(String, CommitHook) : CommitHook is not part of oak-api","[~fmeschbe] had a look at the oak api and spotted the following problem:

Root#commit(String, CommitHook)

But the CommitHook interface is not part of the OAK API. we quickly searched for usages and found that this is only used for the Item#save case in oak-jcr to assert that the set of modifications is contained with the subtree defined by the specified target item.

IMO we should get rid of the flavour of Root#commit again and solve the Item-save issue differently. For example we could change it to Root#commit(String, String absPath) where the absPath would be the path of the target item...",api,['core'],OAK,Bug,Major,2013-12-13 16:10:59,3
12684282,Clean up RepositoryStub classes,"There are various overlapping RepositoryStub classes that need some clean up.

A while ago we decided to switch to Oak+TarMK as default TCK setup. The TCK configuration still points to OakRepositoryStub, which is derived from OakRepositoryStubBase. In OAK-1207 we changed OakRepositoryStubBase to use the TarMK. This duplicates code in OakTarMKRepositoryStub.",test,['jcr'],OAK,Task,Minor,2013-12-12 15:21:31,1
12682951,Failure in ObservationRefreshTest ,"Failed tests:   observation[2](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest): added nodes expected:<1000> but was:<442>

Tests run: 4, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 106.957 sec <<< FAILURE!
observation[3](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 53.047 sec  <<< FAILURE!
java.lang.AssertionError: added nodes expected:<1000> but was:<906>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
observation[2](org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest)  Time elapsed: 58.379 sec  <<< FAILURE!
java.lang.AssertionError: added nodes expected:<1000> but was:<396>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.jackrabbit.oak.jcr.observation.ObservationRefreshTest.observation(ObservationRefreshTest.java:119)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:695)
",observation,['jcr'],OAK,Bug,Major,2013-12-05 17:29:06,3
12682124,Make AsynchIndexUpdate task to run only on a single node in a cluster,"Currently the {{AsynchIndexUpdate }} job which performs the indexing in background is run on every node in a cluster. This at times causes commit failures when running Oak in a cluster using Mongo MK. As merging of indexed content say Lucene is tricky to implement it would be better to restrict this job to run as a singleton in a cluster

See http://markmail.org/thread/qff2fj7nqtbuhr4i for more discussion",cluster,['core'],OAK,Improvement,Major,2013-12-02 12:30:23,4
12681562,IllegalStateException in MergeSortedIterators,"While running Oak in a two node clutser following exception is seen. It basically comes because the AsynchUpdate tries to update async-status concurrently

{noformat}
27.11.2013 17:56:35.507 *ERROR* [pool-5-thread-1] org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@fcf98c2 : com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diff(MongoMK.java:165) ~[na:na]
	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:481) ~[na:na]
	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:52) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:103) ~[na:na]
	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler:2.4.2]
	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler:2.4.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199) ~[na:na]
	at com.google.common.cache.LocalCache.get(LocalCache.java:3932) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.getNode(MongoNodeStore.java:507) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffFewChildren(MongoMK.java:313) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK.diffImpl(MongoMK.java:229) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:168) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoMK$1.call(MongoMK.java:165) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 11 common frames omitted
Caused by: java.lang.IllegalStateException: Revisioned values for property 1:/oak:index/async-status: First element of next iterator must be greater than previous iterator
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.fetchNextIterator(MergeSortedIterators.java:103) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.util.MergeSortedIterators.next(MergeSortedIterators.java:85) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getLatestValue(NodeDocument.java:1041) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.NodeDocument.getNodeAtRevision(NodeDocument.java:456) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.readNode(MongoNodeStore.java:653) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore.access$000(MongoNodeStore.java:80) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:510) ~[na:na]
	at org.apache.jackrabbit.oak.plugins.mongomk.MongoNodeStore$2.call(MongoNodeStore.java:507) ~[na:na]
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724) ~[na:na]
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278) ~[na:na]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193) ~[na:na]
	... 23 common frames omitted
{noformat}",cluster,['mongomk'],OAK,Bug,Major,2013-11-27 12:45:23,1
12679183,DocumentNodeStore: annotate conflicts,Conflict handling is mostly implemented in MongoMK but it does not yet annotate conflicts on rebase.,technical_debt,['documentmk'],OAK,Bug,Minor,2013-11-14 09:15:19,1
12678216,Generic interfaces for operation tasks,"Could we add generic (i.e. MK independent) interfaces that can be used by higher levels to trigger certain ops tasks? The the application could decide when would be a good time to run them.
I am thinking especially about backup/restore (OAK-1158), MVCC revision cleanup (OAK-1158) and DSGC (OAK-377)",monitoring,"['core', 'mk']",OAK,New Feature,Major,2013-11-08 12:45:14,3
12676732,Observation listener PLUS,"Oak should provide an *extended and efficient JCR observation listener* mechanism to support common use cases not handled well by the restricted options of the JCR observation (only base path, node types and raw events). Those cases require listeners to register much more broadly and then filter out their specific cases themselves, thus putting too many events into the observation system and creating a huge overhead due to asynchronous access to the modified JCR data to do the filtering. This easily is a big performance bottleneck with many writes and thus many events.

Previous discussions [on the list|http://markmail.org/message/oyq7fnfrveceemoh] and in OAK-1120, and [latest discussion on the list|http://markmail.org/message/x2l6tv4m7bxjzqqq].

The goals should be:
* performance: handle filtering as early as possible, during the commit, where access to the modified data is already present
* provide robust implementation for typical filtering cases
* provide an asynchronous listener mechanism as in JCR
* minimize effect on the lower levels on Oak (a visible addition in oak-commons or oak-jcr should be enough)
* for delete events, allow filtering on the to-be-deleted data (currently not possible in jcr listeners that run after the fact)
* ignore external cluster events by default; have an extra option if you really want to register for external events
* if possible: design as an extension of the jcr observation to simplify migration for existing code
* if possible: provide an intelligent listener that can work with pure JCR (aka Jackrabbit 2) as well, by falling back to in-listener-filtering
* maybe: synchronous option using the same simple interface (instead of raw Oak plugins itself); however, not sure if there is a benefit if they can only read data and not change or block the session commit

Typical filtering cases:
- paths with globbing support (for example /content/foo/*/something)
- check for property values (equal, not equal, contains etc.), most importantly
sling:resourceType in Sling apps
- allow to check properties on child nodes as well, typically jcr:content
- check for any parent/ancestor as well (e.g. change deep inside a node type = foo structure should be triggered, even if the node with the type wasn't modified; very important to support efficiently)
- node types (already in jcr observation)
- created/modified/deleted events, separate from move/copy
- and more... a custom filter should be possible to pass through (with similar access as the {{Observer}})",observation performance,"['commons', 'jcr']",OAK,New Feature,Major,2013-10-30 22:30:24,3
12671609,Periodically poll for external events,"Currently external events are only reported along with local changes. That is, when local changes are persisted external changes are detected and reported along with the local changes. This might cause external events to be delayed indefinitely on cluster nodes without writes. 

We might want to implement a solution that regularly polls for external events. 
See OAK-1055 for why a previous implementation didn't work. 

",observation,['core'],OAK,Improvement,Minor,2013-10-01 16:19:55,3
12671374,Occasional test failure in ObservationTest.observation(),"The test occasionally fails with
{code}
Failed tests:
observation[1](org.apache.jackrabbit.oak.jcr.observation.ObservationTest):
Unexpected events: [EventImpl{type=8, jcrPath='/test_node/property',
userID='oak:unknown', identifier='/test_node', info={}, date=0,
userData=null, external=true}, EventImpl{type=16,
jcrPath='/test_node/n1/p1', userID='oak:unknown',
identifier='/test_node/n1', info={}, date=0, userData=null,
external=true}, EventImpl{type=4, jcrPath='/test_node/n1/p2',
userID='oak:unknown', identifier='/test_node/n1', info={}, date=0,
userData=null, external=true}, EventImpl{type=2, jcrPath='/test_node/n3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=8,
jcrPath='/test_node/n3/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n3', info={}, date=0, userData=null,
external=true}, EventImpl{type=8, jcrPath='/test_node/n3/p3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=2, jcrPath='/test_node/{4}',
userID='oak:unknown', identifier='/test_node/{4}', info={}, date=0,
userData=null, external=true}, EventImpl{type=8,
jcrPath='/test_node/{4}/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/{4}', info={}, date=0, userData=null,
external=true}, EventImpl{type=1, jcrPath='/test_node/n2',
userID='oak:unknown', identifier='/test_node/n2', info={}, date=0,
userData=null, external=true}, EventImpl{type=4,
jcrPath='/test_node/n2/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n2', info={}, date=0, userData=null,
external=true}, EventImpl{type=4, jcrPath='/test_node/property',
userID='oak:unknown', identifier='/test_node', info={}, date=0,
userData=null, external=true}, EventImpl{type=16,
jcrPath='/test_node/n1/p1', userID='oak:unknown',
identifier='/test_node/n1', info={}, date=0, userData=null,
external=true}, EventImpl{type=8, jcrPath='/test_node/n1/p2',
userID='oak:unknown', identifier='/test_node/n1', info={}, date=0,
userData=null, external=true}, EventImpl{type=2, jcrPath='/test_node/n2',
userID='oak:unknown', identifier='/test_node/n2', info={}, date=0,
userData=null, external=true}, EventImpl{type=8,
jcrPath='/test_node/n2/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n2', info={}, date=0, userData=null,
external=true}, EventImpl{type=1, jcrPath='/test_node/n3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=4,
jcrPath='/test_node/n3/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/n3', info={}, date=0, userData=null,
external=true}, EventImpl{type=4, jcrPath='/test_node/n3/p3',
userID='oak:unknown', identifier='/test_node/n3', info={}, date=0,
userData=null, external=true}, EventImpl{type=1, jcrPath='/test_node/{4}',
userID='oak:unknown', identifier='/test_node/{4}', info={}, date=0,
userData=null, external=true}, EventImpl{type=4,
jcrPath='/test_node/{4}/jcr:primaryType', userID='oak:unknown',
identifier='/test_node/{4}', info={}, date=0, userData=null,
external=true}]
{code}

As [noted before | http://markmail.org/message/lk3vrrcn5edib73d]  having {{external=true}} and also the event types indicate that the events are being seen ""in reverse"" (i.e. reverse diffing of the node states involved). ",observation,['core'],OAK,Bug,Major,2013-09-30 14:49:11,3
12668260,Optimise path parsing,"As Jukka [mentioned | https://issues.apache.org/jira/browse/OAK-978?focusedCommentId=13751242&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13751242] on OAK-978, is often on the critical path and the changes done there had a bad impact on performance:

{code}
Apache Jackrabbit Oak
# ReadPropertyTest               min     10%     50%     90%     max       N
Jackrabbit                         4       5       5       6      14   11287
Oak-Tar                           14      15      16      16      27    3855
{code}

Until we are able to come up with a better solution that separates parsing from name mapping, I suggest to use the following heuristic to shortcut path parsing: shortcut iff the JCR path does not start with a dot, does not contain any of {}[]/ and if it contains a colon the session does not have local re-mappings.
",performance,['core'],OAK,Sub-task,Major,2013-09-12 14:20:44,3
12662067,MBean to track sessions,"Create JMX MBean to track Session and session related information:

* stack trace from where the session has been acquired,
* age of the session,
* last (read/write) access to the session,
* last refresh of the session,
* conflict information (e.g. unresolved conflicts),
* session attributes,
* ...

",monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2013-08-06 08:31:18,3
12659287,Concurrent commits may cause duplicate observation events,"Chetan discovered that in some cases spurious observation events would be created when to sessions save concurrently. In a nutshell the problem occurs since the current implementation of observation expects a linear sequence of revisions (per cluster node). However on Root.commit there is a small race between rebasing and merging a branch: when another session saves inside this time frame, its branch will have the same base revision like that of the former session. In this case the sequence of revisions is effectively non linear.

Full discussion: http://markmail.org/message/cbzrztagurplxo4r",observation,['core'],OAK,Bug,Major,2013-07-23 08:55:00,3
12653704,Generating observation events takes too long when intermediate save calls are involved,Creating observation events is much more expensive when a transaction is broken down through intermediate save calls compared to only having a single save call. ,observation,['core'],OAK,Improvement,Major,2013-06-19 13:45:39,3
12652344,Mangement info and statistics for observation listeners,"Observation listeners (OAK-144) might create backward compatibility issues. To ease the transition we should provide useful information about registered listeners e.g.:
* Number of listeners
* Session (user) a listener belongs to
* Filter set for the listener
* Number of events fired
* Last couple of events fired
* Size of pending queue
* ...",monitoring,"['core', 'jcr']",OAK,Sub-task,Major,2013-06-12 08:57:40,3
12652343,Expose repository management data and statistics,We should come up with a way to expose repository management information and statistics. See JCR-2936 plus subtasks and JCR-3243 for how this is done in Jackrabbit 2. See [this discussion | http://apache-sling.73963.n3.nabble.com/Monitoring-and-Statistics-td4021905.html] for an alternative proposal.,monitoring,"['core', 'jcr']",OAK,New Feature,Major,2013-06-12 08:50:15,3
12652187,Enable stats for various caches used in Oak by default,"To get a better picture around the usage of cache it would be helpful to enable the [statistics|http://code.google.com/p/guava-libraries/wiki/CachesExplained#Statistics] for various caches used in Oak

{code:java}
nodeCache = CacheBuilder.newBuilder()
                        .weigher(...)
                        .maximumWeight(...)
                        .recordStats()
                        .build();
{code}

Once enabled it allows to get stats like below
{noformat}
CacheStats{hitCount=763322, missCount=51333, loadSuccessCount=0, loadExceptionCount=0, totalLoadTime=0, evictionCount=3496}
{noformat}

As stats collection adds a very minor overhead we can look into making this setting configurable. 

Untill we expose the stats via JMX one can extract the value in Sling env via approach mentioned in [this gist|https://gist.github.com/chetanmeh/5748650]",monitoring,[],OAK,Sub-task,Minor,2013-06-11 09:37:58,4
12642260,Implement backward compatible observation,"As [discussed | http://markmail.org/message/6bqycmx6vbq7m25c] we might want look into implementing an alternative approach to observation, which trades some scalability for improved backward compatibility. ",observation,"['core', 'jcr']",OAK,New Feature,Major,2013-04-12 15:44:22,3
12637725,Optimize Session.getNamespacePrefixes(),"Apache Sling uses Session.getNamespacePrefixes() quite often. E.g. when reading content through a JcrPropertyMap:

{code}
   java.lang.Thread.State: RUNNABLE
        at java.util.HashMap.put(HashMap.java:372)
        at org.apache.jackrabbit.oak.plugins.name.Namespaces.getNamespaceMap(Namespaces.java:64)
        at org.apache.jackrabbit.oak.plugins.name.ReadOnlyNamespaceRegistry.getPrefix(ReadOnlyNamespaceRegistry.java:116)
        at org.apache.jackrabbit.oak.jcr.SessionImpl.getNamespacePrefix(SessionImpl.java:529)
        - locked <0x00000007daf6c590> (a java.util.HashMap)
        at org.apache.jackrabbit.oak.jcr.SessionImpl.getNamespacePrefixes(SessionImpl.java:495)
        at org.apache.sling.jcr.resource.JcrPropertyMap.escapeKeyName(JcrPropertyMap.java:381)
        at org.apache.sling.jcr.resource.JcrPropertyMap.read(JcrPropertyMap.java:344)
        at org.apache.sling.jcr.resource.JcrPropertyMap.get(JcrPropertyMap.java:126)
        at org.apache.sling.jcr.resource.JcrPropertyMap.get(JcrPropertyMap.java:147)
{code}

I'd like to optimize the case when there are not session re-mapped namespaces. In this case the prefixes can be returned from the namespace registry as is. This avoids the loop and reduces calls on the Oak API. Initial testing shows a performance improvement by a factor of 3.",performance,['jcr'],OAK,Sub-task,Minor,2013-03-19 09:00:29,1
12635546,Observation generates NPE in an existing EventListener,"Because there is no user id passed on to the events generated by the _ChangeProcessor_, the sling EventListener throws a bunch of NPEs when it receives the events.

{code}
06.03.2013 11:33:13.866 *ERROR* [pool-4-thread-1] org.apache.jackrabbit.oak.plugins.observation.ChangeProcessor Unable to generate or send events java.lang.NullPointerException
at java.util.Hashtable.put(Hashtable.java:394)
at org.apache.sling.jcr.resource.internal.JcrResourceListener.sendOsgiEvent(JcrResourceListener.java:298)
at org.apache.sling.jcr.resource.internal.JcrResourceListener.onEvent(JcrResourceListener.java:218)
at org.apache.jackrabbit.oak.plugins.observation.ChangeProcessor$EventGeneratingNodeStateDiff.sendEvents(ChangeProcessor.java:154)
at org.apache.jackrabbit.oak.plugins.observation.ChangeProcessor.run(ChangeProcessor.java:117)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
{code}",observation,['core'],OAK,Improvement,Major,2013-03-06 10:46:34,3
12635349,Optimise TreeImpl.getBaseState() ,"Currently {{TreeImpl.getBaseState()}} calculates the base state of the tree on the fly on each call. As it turns out this method ends up being called by nearly every JCR method call. As recalculation is somewhat expensive since it recursively needs to calculate the base states of all parent trees, an optimisation would be to pre calculate the base state on instance creation.",performance,[],OAK,Sub-task,Minor,2013-03-05 15:00:56,3
12616917,Remove exported packages from Mongo MK Bundle,The oak-mongomk bundle currently exports couple of packages which are not required to be exported. These exports should be removed,osgi,['mongomk'],OAK,Task,Minor,2012-11-20 11:27:02,4
12613758,OSGi related dependencies should be set to provided scoped and not marked as optional,"Currently the OSGi related dependencies org.osgi.core and org.osgi.compendium are marked as optional. Due to this packages under org.osgi.* are marked as optional which is not correct. 

As such dependencies are already marked as provided scope they would not be included as part of transient dependencies. For more details refer to [1]

Fix: The optional flag should be removed

[1] http://markmail.org/thread/njukyten6fdipts3",osgi,['core'],OAK,Bug,Minor,2012-10-27 13:36:16,4
12610547,MongoDB microkernal integration with OSGi,Need to convert the oak-mongomk module to an OSGi bundle and expose the MongoMicroKernel as OSGi service,osgi,['mongomk'],OAK,Sub-task,Major,2012-10-05 11:55:43,4
12603584,MicroKernelService should set metatype to true to easier configuration,MicroKernelService currently uses @Component annotation without enabling metatype. If metatype is enabled it would simply the configuration of home directory. ,osgi,['mk'],OAK,Improvement,Minor,2012-08-15 13:42:24,4
12603581,Add import for org.h2 in oak-mk bundle,"The oak-mk bundle depends on H2 database. It internally uses Class.forName('org.h2.Driver"") to load the H2 driver. Due to usage of Class.forName Bnd is not able to add org.h2 package to Import-Package list. So it should have an explicit entry in the maven-bundle-plugin config as shown below

{code:xml}
<Import-Package>
  org.h2;resolution:=optional,
  *
</Import-Package>
{code}

Without this MicroKernalService loading would fail with a CNFE",osgi,['mk'],OAK,Bug,Major,2012-08-15 13:34:53,3
12549684,Create smoke-test build profile,"For quick turn around cycles during development we should have a way to run the most important tests only during a build and exclude longer running tests. 

I propose to create a Maven profile ""smoke-test"" which excludes long running tests. This ensures all tests are run by default but smoke testing can be used during development. 
",test,[],OAK,Improvement,Major,2012-04-05 12:01:30,3
12546795,MVCC causes write skew,Trans-session isolation differs from Jackrabbit 2. Snapshot isolation can result in write skew. See http://wiki.apache.org/jackrabbit/Transactional%20model%20of%20the%20Microkernel%20based%20Jackrabbit%20prototype,documentation test,[],OAK,Test,Major,2012-03-16 14:59:39,3
12545672,JCR bindings for Oak,"One of the proposed goals for the 0.1 release is at least a basic JCR binding for Oak. Most of that already exists in /jackrabbit/sandbox, we just need to decide where and how to place it in Oak. I think we should either put it all under o.a.j.oak.jcr in oak-core, or create a separate oak-jcr component for the JCR binding.

As for functionality, it would be nice if the JCR binding was able to do at least the following:

{code}
Repository repository = JcrUtils.getRepository(...);

Session session = repository.login(...);
try {
    // Create
    session.getRootNode().addNode(""hello"")
        .setProperty(""world"",  ""hello world"");
    session.save();

    // Read
    assertEquals(
        ""hello world"",
        session.getProperty(""/hello/world"").getString());

    // Update
    session.getNode(""/hello"").setProperty(""world"", ""Hello, World!"");
    session.save();
    assertEquals(
        ""Hello, World!"",
        session.getProperty(""/hello/world"").getString());

    // Delete
    session.getNode(""/hello"").delete();
    session.save();
    assertTrue(!session.propertyExists(""/hello/world""));
} finally {
    create.logout();
}
{code}
",jcr,['jcr'],OAK,New Feature,Major,2012-03-08 16:32:39,3
