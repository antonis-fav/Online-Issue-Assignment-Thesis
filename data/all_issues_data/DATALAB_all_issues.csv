id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13389635,Update user guide according to new changes,"1. Add description for GPU on GCP
2. Add description for Configuration page
3. Update images where it is necessary",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-07-14 13:52:46,0
13389586,[Azure] Edge status does not changed on DataLab WEB UI after scheduler triggering,"*Preconditions:*
 # Environment is created

*Steps to reproduce:*
 # Stop edge node

*Actual result:*
 # Docker runs with 0
 # Status is still in Stopping on DataLab WEB UI 

*Expected result:*
 # Docker runs with 0
 # Status is in Stopped on DataLab WEB UI",AZURE Back-end Debian Known_issues(release2.5) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-07-14 09:27:41,1
13389380,Actual jar name should be retrieved without pointing particular version,During service deploying use  jars without its version.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-07-13 11:47:07,2
13389181,[Library] Investigate how to retrieve lib list for EMR and for notebook,"EMR is based on RedHat, but notebook is on Debian.

And some lib is not available for Debian and like wise.

So, investigate how distinguish gaining available lib list between Debian and RedHat on one environment.",AWS AZURE Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-07-12 15:46:07,2
13388556,[EMR] It is impossible to install library,"If install any library from apt/yum, pip group:
 * docker runs with 1
 * Lib status is stuck in 'Installing' on DataLab WebUI",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-07-09 07:30:43,2
13388392,Extend t-shirt size for GPU type,"|*GPU type*|*Month cost*|*t-shirt size*|*zone*|
|nvidia-tesla-t4 |$255.50|S|us-west-1b/us-west1a|
|nvidia-tesla-k80|$328|M|us-west-1b|
|nvidia-tesla-p4 |$438|L|us-central-1c|
|nvidia-tesla-p100 |$1,065.80|XL|us-west-1b/us-west1a|
|nvidia-tesla-v100 |$1,810.40|XXL|west-1b/us-west1a|",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-07-08 12:49:48,0
13388389,[Front-end] Show library group depending on Notebook type,"Related Compute should have the same lib list as a notebook.  Please accustom it between DeepLearning and Dataproc.

Perhaps there are differences between other notebooks and related Computes.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-07-08 12:45:04,0
13388387,[Back-end] Show library group depending on Notebook type,"Related Compute should have the same lib list as a notebook.  Please accustom it between DeepLearning and Dataproc.

Perhaps there are differences between other notebooks and related Computes.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-07-08 12:43:14,1
13388371,[GCP][DeepLearning] Investigate appropriate way of spark creation,"Spark creation uses notebook image.

But for DeepLearning we use cloud image for creation. And during Apache Standalone cluster creation error shows up:
{code:java}
Requested disk size cannot be smaller than the image size
{code}",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-07-08 11:23:51,3
13388368,Do not locate cuda driver DEB package in /home/datalab-user,"If create any Notebook based on GPU (GCP) the cuda driver DEB package is located in / home/datalab-user and visualises in Notebook UI.

So, do not locate cuda driver DEB package in /home/datalab-user.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-07-08 11:16:16,3
13388366,[GCP] Investigate why DeepLearning creation fails with error connection reset by peer,"It was during auto-test running on GCP.

Investigate why DeepLearning creation fails with error connection reset by peer",DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-07-08 11:05:45,3
13388137,[Configuration] Sensitive information is editable in editor on DataLab WEB UI,"*AWS*

Do not rewrite the following values:
 +1. Billing:+
 1.1 Mongo username in two blocks

+2. Provisioning:+
 2.1. adminConnectors should be adminConnectors, but not ***********Connectors - *do not change it by default*
 2.2 ldap user
 2.3 keycloak user

+3. Self-service+
 3.1. guacamole username

*GCP*
 +1. Billing:+
 1.1 Mongo username

+2. Provisioning:+
 2.1. adminConnectors should be adminConnectors, but not ***********Connectors - *do not change it by default*
 2.2 ldap user
 2.3 keycloak user

+3. Self-service+
 3.1. guacamole username",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2021-07-07 09:15:35,1
13387932,Java dependency is absent for DeepLearning/Jupyter with TensorFlow,"1. GCP:

Java group is absent for:
 * Jupyter with TensorFlow
 * Deeplearning

2. GCP/AWS/AZURE
 * Get rid of pip2 from Network (F12)",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-07-06 12:54:59,1
13387930,[DeepLearning] Jupyter UI link leads to 404 error only if Notebook is created from custom image,"The bug was found on GCP.

*Preconditions:*
 # DeepLearning is created from custom image
 # User is located on List of resource page

*Steps to reproduce:*
 # Click on DeepLearning name popup
 # Click on Jupyter UI link

*Actual result:*
 # Jupyter UI link is not opened
 # 404 error shows up

*Expected result:*
 # Jupyter UI link is opened successfully
 # 404 error does not show up",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-07-06 12:31:55,3
13387926,Investigate how smart  to change edge node status after docker execution,"From Back-end side it was added addition instance status checker after edge docker execution because it is not changed by default after edge docker execution.

Ponder how to change edge status without additional instance status checker triggering.",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Minor,2021-07-06 12:23:13,1
13387889,[R package] Any library is not installed for Apache Standalone cluster,"*Preconditions:*
 # Apache Spark cluster is in running status

*Steps to reproduce:*
 # Install any library from R package o Apache standalone cluster

*Actual result:*
 # Docker runs with1
 # Library status is stuck in Installing on DataLab WEB UI

*Expected result:*
 # Docker runs with0
 # Library status is Installed on DataLab WEB UI",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-07-06 09:44:55,2
13387887,[R package] Get rid of extra characters in the end of message ,Remove 'v.' in the end of dependency message.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2021-07-06 09:36:03,2
13387849,Apache Zeppelin Creation fails on command  cp /opt/zeppelin-0.9.0-bin-netinst/interpreter/shell/zeppelin-shell-*.jar /opt/zeppelin/lib/interpreter/,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create a Zeppelin

*Actual result:*
 # Zeppelin creation fails

{code:java}
Command: ""sudo -S -p '[sudo] password: ' cp /opt/zeppelin-0.9.0-bin-netinst/interpreter/shell/zeppelin-shell-*.jar /opt/zeppelin/lib/interpreter/""
{code}
*Expected result:*
 # Zeppelin creation is successful",AWS AZURE Debian DevOps GCP Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-07-06 07:07:18,2
13387758,[GCP][DeepLEarning][Jupyter with TensorFlow] Apache Standalone Spark cluster creation fails ,"The bug was found on GCP during Apache Standalone Spark cluster creation only for DeepLearning and Jupyter with TensorFlow

*Preconditions:*
 # DeepLerning/Jupyter with TensorFlow is in running status

*Steps to reproduce:*
 # Create Apache Standalone Spark cluster for DeepLerning/Jupyter with TensorFlow

*Actual result:*

1. Apache Standalone Spark cluster creation fails
{code:java}
Failed to generate variables dictionary. Exception:'gpu_type'.
{code}
*Expected result:*

1. Apache Standalone Spark cluster creation is successfully

 
----
(i) Jupyter with Tensorflow creation on GCP from autotest",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Major,2021-07-05 15:42:17,3
13387755,Extend validation for infrastructure status scheduler checker,"+Case:+

*Preconditions:*

1.  Apache Standalone cluster is in running status

*Steps to reproduce:*
 # Terminate Notebook which has related  Apache Standalone cluster

*Actual result:*
 # Notebook/cluster statuses are terminating
 # After triggering of infrastructure status scheduler notebook status is terminated and spark cluster status is stopping
 # After the next triggering of infrastructure status scheduler notebook status is terminated and spark cluster status is terminated

*Expected result:*

1. Apache spark cluster should not change its status from terminating to stopping

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-07-05 15:28:18,1
13387753,[GCP] GPU instance creation fails on stage of fetching add-apt-repository,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create Notebook based on GPU or create Apache Standalone cluster based on GPU for Jupyter

*Actual result:*
 # Notebook/Apache Standalone cluster creation fails

{code:java}
Reading package lists...
E: Failed to fetch https://dl.bintray.com/sbt/debian/InRelease  403  Forbidden [IP: 172.31.0.3 3128]
E: The repository 'https://dl.bintray.com/sbt/debian  InRelease' is no longer signed.
{code}
*Expected result:*
 # Notebook/Apache Standalone cluster creation is successful

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2021-07-05 15:09:14,3
13387743,[GCP][RStudio][Apache Standalone cluster] Flight data visualization runs with error of collect method usage,"*Preconditions:*
 # Apache Standalone cluster is created on RStudio

*Steps to reproduce:*
 # Run Flight data visualization on Apache Standalone cluster

*Actual result:*

1. Flight data visualization playbook running fails
{code:java}
> delay <- collect(delay_sql)
Error in UseMethod(""collect"") : 
  no applicable method for 'collect' applied to an object of class ""SparkDataFrame""
> delay_melt <- melt(delay[c('Carrier', 'WorkDayDelay', 'WeekendDelay')])
Error in melt(delay[c(""Carrier"", ""WorkDayDelay"", ""WeekendDelay"")]) : 
  object 'delay' not found{code}
*Expected result:*

1. Flight data visualization playbook running is successful",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2021-07-05 13:57:09,3
13387699,[Configuration] Confirmation massage for yml editor is absent in some cases,"*Preconditions:*
 # Any instance is in processing status:

 * creating
 * stopping
 * starting
 * configuring
 * reconfiguring
 * creating image
 * terminating

*Steps to reproduce:*
 # Go to Configuration page
 # Edit self-service/provisioning/billing ymls
 # Click Save or Discard changes button

*Actual result:*
 # Confirmation buttons are absent
 # Name of confirmation dialog is cut

*Expected result:*
 # Confirmation buttons are present
 # Name of confirmation dialog is not cut",AWS AZURE Debian GCP RedHat front-end pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-07-05 10:42:06,0
13387678,[Compute] Add t-shirt size for GPU type,"|GPUtype|Month cost|t-shirt size|
|nvidia-tesla-t4 |$255.50|S|
|nvidia-tesla-p100 |$1,065.80|M|
|nvidia-tesla-v100 |$1,810.40|L|

For now t-shirt size is not added for clusters:
 * Dataproc
 * Apache Standalone cluster

Please assign t shirt size to computes.",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-07-05 09:39:40,0
13387356,[Jupyter] Flights_data_Preparation_Scala/Flights_data_Preparation_R running fail ,"The bug was found on AWS.

*Preconditions:*
 # Jupyter is in running status

*Steps to reproduce:*

1. Run Flights_data_Preparation_Scala/Flights_data_Preparation_R  on local kernel

*Actual result:*

1. Flights_data_Preparation_Scala/Flights_data_Preparation running fails

*Expected result:*

1. Flights_data_Preparation_Scala/Flights_data_Preparation running is successful

 ",AWS Debian DevOps Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-07-02 15:06:07,3
13387269,Project creation is allowed if total quota is depleted,"*Preconditions:*
 # Total quota is less than actual used total quota

*Steps to reproduce:*

1.  Go to a Project page

2. Create a new project

*Actual result:*
 # Project creation is allowed

*Expected result:*
 # Project creation is forbidden

 ",AWS AZURE Back-end Debian GCP Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-07-02 11:28:27,1
13387243,Get rid of EMR version  6.0.0,"# Leave two EMR versions (5.30.0 & 6.2.0)
 # Get rid of EMR version 6.0.0

!image-2021-07-02-12-23-32-252.png!",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-07-02 09:23:51,2
13387223,[Configuration] Mongo/keycloak user name should not be shown in editor on WEB DataLab UI,"# In provisioning. yml Keycloak user name convey in asterisk for editor on DataLab WEB UI
 # In billing.yml mongodb user name convey in asterisk for editor on DataLab WEB UI

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-07-02 07:41:51,1
13387209,Notebook creation fails on GCP on stage of pre-configured images searching,"*Preconditions:*
 # Project is created on GCP

*Steps to reproduce:*
 # Create any type of Notebook

*Actual result:*
 # Notebook creation fails

 
{code:java}
googleapiclient.errors.HttpError: <HttpError 400 when requesting https://compute.googleapis.com/compute/v1/projects/or2-msq-epmc-dlab-t1iylu/global/images/gcp-02-07-21-proj-local-tensor-primary-image-tensorflow-gpu-2.3.2?alt=json returned ""Invalid value for field 'image': 'gcp-02-07-21-proj-local-tensor-primary-image-tensorflow-gpu-2.3.2'. Must be a match of regex '[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?|[1-9][0-9]{0,19}'"". Details: ""[{'message': ""Invalid value for field 'image': 'gcp-02-07-21-proj-local-tensor-primary-image-tensorflow-gpu-2.3.2'. Must be a match of regex '[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?|[1-9][0-9]{0,19}'"", 'domain': 'global', 'reason': 'invalid'}]"">

{code}
 

*Expected result:*
 # Notebook is created successfully

 ",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Blocker,2021-07-02 06:40:24,1
13387072,[RStudio] In Lib and path is not indicated virtual env,"*Preconditions:*
 # User is located on RStudio UI

*Steps to reproduce:*
 # Go to Environment
 # Switch to Python

*Actual result:*
 # In lib is indicated python 3.8
 # In path is indicated python 3.8

!image-2021-07-01-16-55-12-937.png!

*Expected result:*
 # In lib is indicated python 3.7.9
 # In path is indicated python 3.7.9

----
Another way to check
 # Go to console
 # Run commands

{code:java}
import pip
print(pip.__version__)
{code}
It is shown pip version 20.0.2:

!image-2021-07-01-17-23-20-553.png!",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-07-01 14:03:20,2
13387014,Add None option for custom image,Add None option for custom image for all notebooks except for DeepLearning.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-07-01 09:54:18,0
13386996,[Apache Zeppelin] Spark drivers/executors memory are not allocated according to best practice,"Spark driver/executor memory is allocated according to the ticket https://issues.apache.org/jira/browse/DATALAB-2342/ But for Apache Zeppelin it does not work.

*Preconditions:*
 # Zeppelin is in running status

*Steps to reproduce:*
 # Run Playbook on Zeppelin
 # Run on notebook sudo ps aux | grep java
 # Calculate the memory by formula Spark executor memory = total executor memory * 0/90

*Actual result:*
 # Calculation is not equal to spark memory (sudo ps aux | grep java)

*Expected result:*
 # Calculation is equal to spark memory (sudo ps aux | grep java)",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-07-01 08:59:43,3
13386834,[GCP] Investigate Dataproc creation based on GPU,"----
Case:

Create Dataproc with GPU on the second Jupyter

*Result:*

In GCP console for master/workers GPU driver is absent:

!image-2021-06-30-18-15-40-894.png!

(?) Why GPU driver is absent in GCP console?

Running nvidia-smi on master/workers Dataproc it is:

!image-2021-06-30-18-21-31-022.png!

(?) Why command is not found?",GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-30 15:25:05,3
13386826,[Environment management] Get rid of extra scrollbar during endpoint disconnection,For small monitor extra vertical scrollbar shows up for endpoint disconnection on Environment management page.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2021-06-30 15:04:38,0
13386808,[Billing report] Adjust exporting billing period to selected date in DataLab WEB UI,"# If export billing data filtering by date in csv it is shown last day + 1 day. For example, data is filtered from Jun 25 to Jun 26 on DataLab  WEB UI. But in CSV reporting period is from Jun 25 to Jun 27.
 # Billing data is not filtered by date, but it is filtered by any drop down values. In csv it is shown the actual period of resources creation and last usage. But all period should be shown as it is in DataLab WEB UI.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 13:55:31,1
13386780,[Billing][Environment management] Drop down values are limited by grid border,"*Preconditions:*

1. User is located on Billing/Environment management page

*Steps to reproduce:*
 # Filter by any value in column
 # Click on drop down values

*Actual result:*

1.  Drop down values are limited by grid border

*Expected result:*

1. Drop down values are not limited by grid border

 ",AWS AZURE Debian Front-end GCP Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-06-30 13:26:03,0
13386777,[Google Chrome][Billing report] Sort function triggers for identical values,"*Preconditions:*
 # Billing is available
 # User is located on Billing report page

*Steps to reproduce:*
 # Filter User only by one value
 # Click on up/down sorting arrows

*Actual result:*
 # Grid values change their position

*Expected result:*
 # Grid values do not change their position

(i) This bug is reproduced for project column as well.

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2021-06-30 13:15:16,0
13386769,Investigate why sometimes Jupyter creation with GPU shows it is needed to install nvidia driver,"Jupyter creation was tested based on CPU/GPU options. The test result are:

!image-2021-06-30-15-33-28-794.png!
 But sometimes if create Jupyter based in GPU it is shown that:
 [^image-2021-06-30-15-34-22-941.png]  it is needed to install nvidia driver.


 Investigate why we have infor about nvidia-smi failing.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 12:36:13,3
13386751,[TensorFlow with Jupyter] Investigate why Create Models playbook runs with error,"Preparation playbook Create Models run with error. It fails on the first block:
{code:java}
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-2-ffde0fe9d8cf> in <module>
      4 get_ipython().run_line_magic('matplotlib', 'inline')
      5 import tensorflow as tf
----> 6 from tf.keras.models import Sequential, load_model
      7 from keras.layers import Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation
      8 from keras.optimizers import Adam

ModuleNotFoundError: No module named 'tf'
{code}",Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 11:49:25,3
13386747,Convey actual Notebook versions on DataLab WEB UI,Convey new versions for updated notebooks.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-30 11:30:38,3
13386746,[TensorFlow with Jupyter] Get rid of general python3 kernel,Kernel python3 is not needed.  We have local PySpark (Python-3.8.9/Spark-3.0.1) kernel,AWS Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 11:27:59,3
13386744,[AWS][DeepLearning] Get rid of (ubuntu 18.04) from DataLab WEB UI,"The all  DeepLearning version is not visible on DataLab WEB UI. So, do not convey ubuntu version to Front-end.",AWS Debian DevOp RedHat,['DataLab Main'],DATALAB,Task,Major,2021-06-30 11:17:40,3
13386743,[GCP][DeepLearning] Home directory /home/datalab-user should be by default,"DeepLearning has JupyterLab UI. And by default its home directory is /home/jupyter.

Change home directory from /home/jupyter to /home/datalab-user",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 11:13:09,3
13386742,[List of resources] Add hint for GPU tag,If hover the mouse GPU tag a hint should appear: GPU tag: GPU,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 11:03:42,0
13386716,[GCP][Deeplearning] Remote kernels are not available in kernel list,"*Preconditions:*
 # Any cluster is created on DeepLearning on GCP

*Steps to reproduce:*
 # Go to Jupyter UI
 # Open playbook
 # click on Kernel tab
 # Click on Change Kernel
 # Look at kernel list

*Actual result:*
 # Remote kernel is absent

*Expected result:*
 # Remote kernel is present",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2021-06-30 08:41:30,3
13386713,[GCP][DeepLearning] Find out if it is possible to use simple example for library verification,"Investigate for next DeeLearning types:

- pytorch-latest-gpu 

- rapids-latest-gpu-experimental
- chainer-latest-gpu-experimental
- xgboost-latest-gpu-experimental
- mxnet-latest-gpu-experimental
- cntk-latest-gpu-experimental
- caffe1-latest-gpu-experimental
(i)During CNTK testing the library CNTK is not found.
 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-30 08:29:03,3
13386554,[GCP] DeepLearning creation fails from custom image ,"*Preconditions:*
 # DeepLearning custom image is created

*Steps to reproduce:*
 # Create a DeepLearning from custom image

*Actual result:*
 # DeepLearning creation fails

*Expected result:*
 # DeepLearning creation is successful

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-29 15:40:37,3
13386551,[GCP][DeepLearning] Wrong status/information error for library installation,"# First of all try o install library from Apt/Yum package with unknown name and version:

 * name 'dddd'
 * name 'htop', version 'ddddd'

*Actual result:*
 # Docker runs with 1
 # Lib status sticks in Installing on DataLab WEB UI

*Expected result:*
 # Docker runs with 0
 # Lib status changes Invalid name or Invalid version 

----
2. Try to install library from others with unknown name and version from others group:
 * name 'dddddddddd'
 * name 'pivottablejs'  version 'ddddddddddd'
 * try to install library from Apt/Yum

Actual result:

1. For Apt/Yum library installation the error from other group is duplicated

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-06-29 15:00:21,2
13386524,[GCP][DeepLearning] Tensorboard link leads to 502 error,"*Preconditions:*
 # DeepLearning is running

*Steps to reproduce:*
 # Click on TensorBoard link

*Actual result:*
 # TensorBoard link is not opened
 # Error 502 appears

*Expected result:*
 # TensorBoard link is open successfully",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-29 12:48:18,3
13386519,[Environment management][List of resources] Add GPU type/count for filtering ,Add GPU type/count for filtering on Environment management/list of resources pages.,Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-06-29 12:29:08,0
13386483,[Zeppelin] Interpreter for Dataproc/Apache Standalone cluster is not found during playbook running,"*Preconditions:*
 # Dataproc is created on Zeppelin

*Steps to reproduce:*
 # Run playbook for Dataproc
{code:java}
Interpreter gcp not found{code}

*Actual result:*
 # Paybook running fails:

*Expected result:*
 # Playbook running is successful

----
 2. Get rid of PY2 Intrepeter for Dataproc/Apache Standalone cluster
----
3. Every first creation Dataproc on Zeppelin fails 
{code:java}
Traceback (most recent call last):
  File ""/usr/lib/python3.8/datalab/actions_lib.py"", line 1311, in install_python
    subprocess.run('{0} && sudo -i {1} install -U pip==9.0.3'.format(venv_command, pip_command), shell=True, check=True)
  File ""/usr/lib/python3.8/subprocess.py"", line 512, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'source /opt/python/python3.8.1/bin/activate && sudo -i /opt/python/python3.8.1/bin/pip3.8 install -U pip==9.0.3' returned non-zero exit status 127.
Traceback (most recent call last):
  File ""/root/scripts/zeppelin_install_dataengine-service_kernels.py"", line 79, in <module>
    conn.sudo('''bash -l -c 'unset http_proxy https_proxy; export gcp_project_id=""{0}""; export conf_resource=""{1}""; /usr/bin/python3 /usr/local/bin/create_configs.py --bucket {2} --cluster_name {3} --dataproc_version {4} --spark_version {5} --hadoop_version {6} --region {7} --user_name {8} --os_user {9} --pip_mirror {10} --application {11} --livy_version {12} --multiple_clusters {13} --r_enabled {14}' '''
  File ""<decorator-gen-4>"", line 2, in sudo
  File ""/usr/local/lib/python3.8/dist-packages/fabric/connection.py"", line 30, in opens
    return method(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/fabric/connection.py"", line 737, in sudo
    return self._sudo(self._remote_runner(), command, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/invoke/context.py"", line 213, in _sudo
    return runner.run(cmd_str, watchers=watchers, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/invoke/runners.py"", line 363, in run
    return self._run_body(command, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/invoke/runners.py"", line 422, in _run_body
    return self.make_promise() if self._asynchronous else self._finish()
  File ""/usr/local/lib/python3.8/dist-packages/invoke/runners.py"", line 489, in _finish
    raise UnexpectedExit(result)
invoke.exceptions.UnexpectedExit: Encountered a bad command exit co
{code}
 4. It is impossible to switch between interpreters via Zeppelin WEB UI",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-06-29 09:33:37,3
13386443,[Zeppelin] Rewrite Python 2 data preparation/visualization playbook to Python 3 data preparation/visualization,"We have got rid of from Py2. But we do not have appropriate playbook for Zeppelin for python3.

Please rewrite playbook from Python 2 data preparation/visualization playbook to Python 3 data preparation/visualization.",AWS Azure Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-29 07:09:57,3
13385897,[Notebook] Limit the length of custom tag,"Max length of custom tag should be not longer than 63 symbols. If it's longer than 63 characters the Notebook creation fails.
 # Limit length for custom tag up to 63 characters
 # Add information errors. 'Custom tag name cannot be longer than 63 characters.'",AWS Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2021-06-25 15:47:43,0
13385885,[RStudio] Dataproc connection is not esteblished by Livy,"Ponder about EMR?

*Preconditions:*
 # Dataproc is created on RStudio

*Steps to reproduce:*
 # Click on Connections tab
 # Click on New connection
 # Livy is selected
 # Indicate your master IP (see attachment)
 # Click on Test button
 # Look at /var/log/livy on mater Dataproc

*Actual result:*
 # Error appears about livy session
 # There is an error in livy log

*Expected result:*
 # Error does not appear about livy session
 # There is not error in livy log",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2021-06-25 14:48:43,3
13385868,Cut too long description for custom image,"- Cut too long description for custom image

- Add hint for description",AWS AZURE Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-06-25 14:01:49,0
13385809,[GCP] Select image drop down list should be mandatory,"If user does not chose image Create button should be disable. This case is related only for DeepLearning, but is not related for custom image.",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-25 09:30:12,0
13385808,[Front-end] Apache standalone cluster creation fails,"The bug was found on GCP. Perhaps it's related to other clouds.

*Preconditions:*
 # Notebook is running

*Steps to reproduce:*
 # Create Apache Standalone cluster

*Actual result:*

1. Apache Standalone cluster creation fails

*Expected result:*

1. Apache Standalone cluster creation is successfully

 ",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2021-06-25 09:26:32,0
13385803,[Notebook][Python3][Others] Wrong status for library installation,"1. [Python3][Others] If install unfound name the status is installation error, but should be Invalid name

2. [Python3][Others] If install unfound version the status is installation error, but should be Invalid version

3. [Python3][Others] Any library with valid and found name is installed in notebook (in console it is shown) but in DataLab Web UI it is shown as Installation error, but should be Installed

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-06-25 09:12:17,2
13385702,Superset creation fails on stage of configuration,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create superset

*Actual result:*
 # Superset creation fails

{code:java}
Failed configure superset: Encountered a bad command exit code!
Command: ""sudo -S -p '[sudo] password: ' sed -i 's/OS_USER/datalab-user/g' /opt/datalab/templates/.env""
Exit code: 2{code}
*Expected result:*
 # Superset creation is successful",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Major,2021-06-24 19:13:12,3
13385654,[RStudio][Dataproc]Investigate why R session terminates during Flight_data_Visualization_R playbook running,"If run Flight_data_Visualization_R on Rstudio or Dataproc the R session terminates on command:
{code:java}
top_flights_sql <- sql(""
SELECT t.cnt as FlightsAmt, carriers.description as Carrier 
FROM (
    SELECT count(*) as cnt, flights.UniqueCarrier as carrier_code 
    FROM flights 
    GROUP BY flights.UniqueCarrier LIMIT 6) t 
LEFT JOIN carriers 
  ON t.carrier_code = carriers.code
"")
{code}",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-24 14:55:02,3
13385646,Get rid of warning during Flight_data_Preparation_R playbook running,"This warning is reproduces for RStudio during Flight_data_Preparation_R playbook running, however the playbook runs successfully.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Trivial,2021-06-24 14:21:17,3
13385316,GPU type does not match the selected one,"The bug was found on GCP. Perhaps it's related to other clouds.

*Preconditions:*
 # Notebook is running

*Steps to reproduce:*
 # Create Apache Spark Standalone cluster or Jupyter Notebook with GPU

*Actual result:*

1.GPU type does not match the selected one

*Expected result:*

1. GPU typ is correct

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2021-06-23 06:23:59,3
13385200,[Dataproc] Issue with library installation from R package,"# If install library from R package on Dataproc it is shown error with library installation during the first attempt. If click on retry it will show success on DataLab Web UI 
 # After library installation from Web DataLab UI it is impossible to find them on master/slave via command R -e ""installed.packages()[,c(3:4)]"" | grep [lib_name]. In addition the library is not shown on kernel via Jupyter UI",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-22 15:38:52,2
13385197,[Notebook] Wrong behavior for library installation,"# If install unfound lib from apt/yum docker runs with 1 and status sticks in 'installing' on DataLab Web UI (/)
 # If install unfound lib from r package docker runs with 1 and status sticks in 'installing' on DataLab Web UI (/)
 #  If install lib with unfound version from r package the error is installation error, but should be invalid version (/)
 # If install lib with unfound version from apt-yum the error is installation error, but should be invalid version (/)",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-06-22 15:30:33,2
13385127,[AWS][GCP][Local kernel] Flights_data_Preparation_Python3 playbook runs with error,"*Preconditions:*

1. Flights_data_Preparation_Python3 is loaded to Jupyter

*Steps to reproduce:*

1. Run Flights_data_Preparation_Python3 for local kernel

*Actual result:*
 # Playbook running fails:

{code:java}
AttributeError: 'PipelinedRDD' object has no attribute 'toDF'{code}
*Expected result:*

1. Playbook running is successful

 ",AWS Debian DevOps GCP Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-06-22 09:46:55,3
13385091,[GCP]:Dataproc creation fails on RStudio/Apache Zeppelin,"*Preconditions:*
 # RStudio/Apache Zeppelin is created on GCP

*Steps to reproduce:*
 # Create Dataproc on RStudio

*Actual result:*
 # Dataproc creation fails

*Expected result:*
 # Dataproc creation is successful

----
 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-22 07:38:03,2
13384376,Add t-shirt size for GPU type,"|*GPU type*|*Month cost*|*t-shirt size*|*zone*|
|nvidia-tesla-t4 |$255.50|S|us-west-1b/us-west1a|
|nvidia-tesla-k80|$328|M|us-west-1b|
|nvidia-tesla-p4 |$438.|L|us-central-1c|
|nvidia-tesla-p100 |$1,065.80|XL|us-west-1b/us-west1a|
|nvidia-tesla-v100 |$1,810.40|XXL|west-1b/us-west1a|",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-06-17 14:25:20,0
13384368,[GCP] Get rid of GPU count with value '8' for Jupyter/Dataproc,"GPU count values for Jupyter and Dataproc master/slave should be for all available instances/GPU type in DataLab:
 * 1
 * 2
 * 4",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-17 13:55:02,0
13384145,[Front-end] Investigate how to show message about successfully self-service restarting,"If restart self-service the self-service is restarted successfully, but appears error message.

Front-end receives error message because self-service is in restarting status and is unavailable.

 

It was agreed for 502 error to show message about successfully restarting.

So, [~ytykhun] please implement it according our discussion.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-06-16 11:14:36,0
13383953,DataLab permissions are not triggered according to setting,"*Preconditions:*
 # User1 has not administrative permissions
 # Admin1 has administrative permissions

*Steps to reproduce:*
 # Login by User1

*Actual result:*
 # Administrative page is available for User1

*Expected result:*
 # Administrative page is not available for User1",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Critical,2021-06-15 14:36:59,1
13383441,[Jupyter] Connection to Local PySpark (Python-3.7.9 / Spark-3.0.1) fails,"This bug was found on GCP/ Perhaps it's related to other clouds.

*Preconditions:*
 # Jupyter is in running status

*Steps to reproduce:*
 # Go to Jupyter UI link
 # Switch to Local PySpark (Python-3.7.9 / Spark-3.0.1)

*Actual result:*
 # Error appears: 'A connection to the notebook server could not be established. The notebook will continue trying to reconnect. Check your network connection or notebook server configuration.'
 # Connection is not established to Local PySpark (Python-3.7.9 / Spark-3.0.1 )

*Expected result:*
 # Error does not show up
 # Connection is established to Local PySpark (Python-3.7.9 / Spark-3.0.1 )",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-06-11 15:12:17,2
13383170,[Dataproc]: Mismatch in GPU parameters,"*Preconditions:*
 # Dataproc with GPU is created
 # User is located on List of resource page

*Steps to reproduce:*
 # Click on Dataproc name

*Actual result:*
 # Popup shows up
 # GPU parameters for slave are empty
 # GPU parameters for master contain slave parameters

*Expected result:*
 # Popup shows up
 # GPU parameters for slave contain slave parameters
 # GPU parameters for master contain master parameters

 ",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Major,2021-06-10 11:55:31,1
13383166,[Dataproc] Investigate GPU usage for slave,"1. In GCP GPU kernel is pointed for slave, but not for second worker.

But nvidia-smi command is not run successfully for slave (not second worker). Why?
{code:java}
datalab-user@gcp-10-06-21-proj-local-des-dp-gpu-c-w-0:~$ nvidi-smiCommand 'nvidi-smi' not found, did you mean: command 'nvidia-smi' from deb nvidia-340 command 'nvidia-smi' from deb nvidia-utils-390Try: apt install <deb name>datalab-user@gcp-10-06-21-proj-local-des-dp-gpu-c-w-0:~$
{code}
2. Running code of GPU usage (https://weeraman.com/put-that-gpu-to-good-use-with-python-e5a437168c01) does not show GPU usage for Dataproc master via ssh. Why?",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-10 11:28:16,3
13383143,Investigate why scala version differs betwen Jupyter UI datalab.ini and in command output,"Scala version was checked only for GCP. Ponder if it is related to other clouds.
 # In datala.ini it is pointed: Version of Scala to be installed on notebook scala_version = *2.12.8*. For Scala local kernel command running util.Properties.versionString shows *2.12.10*, but and in Jupytetr UI for kernel it's *2.12.10*
 # For Apache Standalone Spark kernel command running util.Properties.versionString shows *2.12.10*, and in Jupyetr UI for kernel it's *2.12.10*
 # For Dataproc kernel command running util.Properties.versionString shows *2.12.12*, but in Jupyetr UI for kernel it's *2.12.8*

 ",Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-10 09:38:17,3
13382992,[GCP]: Investigate performance Dataproc and libaries usage,"*Preconditions:*
 # Dataproc based on CPU is created on Jupyter
 # Dataproc Master/slave are n1-highmem-4

*Case1:* Flights data Visualization R  playbook for Dataproc v.2.0.0-RC22-ubuntu18 requires libraries from R package ggplot2 and reshape2. ggplot 2 was installed on Dataproc master/slaves via ssh successfully. However running  Flights data Visualization R  playbook does not find ggplot2.

If  ggplot2 install via Jupyter UI running  Flights data Visualization R  playbook does find library ggplot2. 

*Questions:* 
 # Is the library installed in different places?
 # Why is not found the library if install via ssh terminal Dataproc?



*Case2:* Reshape2 installation was more than 40 minutes via Jupyter UI and after that i restart kernel. The attempt of reshape2 installation or running previously successful playbook causes a new error:
{code:java}
The code failed because of a fatal error:
	Invalid status code '500' from http://172.31.16.13:8998/sessions with error payload: ""java.lang.NullPointerException"".

Some things to try:
a) Make sure Spark has enough available resources for Jupyter to create a Spark context.
b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.
c) Restart the kernel.
{code}
 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2021-06-09 15:48:47,2
13382933,[Projects] Adjust space for project termination confirmation dialog after visiting Library management page,"Space between grid and information error is too big for big. The space becomes too big only after visiting Library management page.

Could you adjust distance of space?",AWS AZURE Debian GCP RedHat front-end pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-06-09 11:37:46,0
13382931,[Front-end][GCP]: Add new GPU parameters for Apache Standalone cluster,"'Add GPU' Checkbox should be in 'Add compute' dialog form

Add new parameters for Apache Standalone cluster:
 * Master GPU type - dropdown list
 * Slave GPU type - dropdown list
 * Master GPU сount - text box
 * Slave GPU сount - text box Should be special connection between input - ask [~mykolabodnar]

 ",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-09 11:26:24,0
13382930,[Back-end][GCP]: Add new parameters for Apache Standalone cluster,"'Add GPU' Checkbox should be in 'Add compute' dialog form

Add new parameters for Apache Standalone cluster:
 * Master GPU type - dropdown list
 * Slave GPU type - dropdown list
 * Master GPU сount - text box
 * Slave GPU сount - text box Should be special connection between input - ask [~mykolabodnar]

 ",Back-end Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-06-09 11:24:59,1
13382929,[Data Engine Service]: Library installation fails,"*Preconditions:*
 # Data Engine service is created

*Steps to reproduce:*
 # Install any lib on DES

*Actual result:*
 # Docker runs with 1
 # Library installation status is stuck in 'Installing' on WEB DataLab UI

*Expected result:*
 # Docker runs with 0
 # Library installation status is changes according to docker result",AWS Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-06-09 11:20:13,2
13382904,[GCP][Investigation][Dataproc][Apache Standalone cluster] Often Flights_data_Preparation/Visualisation_R playbooks fail,"*Preconditions:*
 # Dataproc is running on Jupyter

*Steps to reproduce:*
 # Run Flights_data_Preparation/Visualisation_R on Dataproc

*Actual result:*

1. Flights_data_Preparation/Visualisation_R running fails

Dataproc:
{code:java}
An error was encountered:
Spark package found in SPARK_HOME: /usr/lib/spark
[1] ""Error in writeJobj(con, object): invalid jobj 1""
{code}
Apache Standalone cluster:
{code:java}
An error was encountered:
Spark package found in SPARK_HOME: /opt/spark
[1] ""Error in writeJobj(con, object): invalid jobj 1""
{code}
*Expected result:*

1. Flights_data_Preparation/Visualisation_R running is running succesfully

 ",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2021-06-09 09:33:03,3
13382452,[Front-end]: Dataproc creation is forbidden,"*Preconditions:*
 # Jupyter is created on GCP

*Steps to reproduce:*
 # Create DataProc

*Actual result:*
 # Data proc creation is forbidden

*Expected result:*
 # Dataproc creation is allowed

{code:java}
message: ""Unrecognized field \""masterGPUType\"" (class com.epam.datalab.backendapi.resources.dto.gcp.GcpComputationalCreateForm), not marked as ignorable (18 known properties: \""config\"", \""gpuCount\"", \""dataproc_master_instance_type\"", \""gpu_tag\"", \""dataproc_slave_count\"", \""name\"", \""custom_tag\"", \""gpu_enabled\"", \""notebook_name\"", \""template_name\"", \""dataproc_master_count\"", \""dataproc_preemptible_count\"", \""check_inactivity_required\"", \""project\"", \""image\"", \""dataproc_slave_instance_type\"", \""gpuType\"", \""dataproc_version\""])\n at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 414] (through reference chain: com.epam.datalab.backendapi.resources.dto.gcp.GcpComputationalCreateForm[\""masterGPUType\""])""
{code}",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2021-06-07 11:34:16,0
13382440,[Back-end]: Dataproc creation is forbidden,"*Preconditions:*
 # Jupyter is created on GCP

*Steps to reproduce:*
 # Create DataProc

*Actual result:*
 # Data proc creation is forbidden

*Expected result:*
 # Dataproc creation is allowed

{code:java}
message: ""Unrecognized field \""masterGPUType\"" (class com.epam.datalab.backendapi.resources.dto.gcp.GcpComputationalCreateForm), not marked as ignorable (18 known properties: \""config\"", \""gpuCount\"", \""dataproc_master_instance_type\"", \""gpu_tag\"", \""dataproc_slave_count\"", \""name\"", \""custom_tag\"", \""gpu_enabled\"", \""notebook_name\"", \""template_name\"", \""dataproc_master_count\"", \""dataproc_preemptible_count\"", \""check_inactivity_required\"", \""project\"", \""image\"", \""dataproc_slave_instance_type\"", \""gpuType\"", \""dataproc_version\""])\n at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 414] (through reference chain: com.epam.datalab.backendapi.resources.dto.gcp.GcpComputationalCreateForm[\""masterGPUType\""])""
{code}",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Critical,2021-06-07 11:05:29,1
13382409,Sometimes Jupyter/Zeppelin notebook creation fail on command sudo -S -p '[sudo] password: ' mkfs.ext4 -F /dev/sdb1,"The bug was found on GCP. Perhaps it's related to all clouds.

*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create Jupyter/Zeppelin

*Actual result:*
 # Jupyter/Zeppelin creation fails

{code:java}
The file /dev/sdb1 does not exist and no size was specified.
sudo -S -p '[sudo] password: ' mkfs.ext4 -F /dev/sdb1
Traceback (most recent call last):
  File ""/root/scripts/zeppelin_configure.py"", line 184, in <module>
    subprocess.run(""~/scripts/{}.py {}"".format('configure_zeppelin_node', params), shell=True, check=True)
  File ""/usr/lib/python3.8/subprocess.py"", line 512, in run
    raise CalledProcessError(retcode, process.args,
{code}
*Expected result:*

1. Jupyter/zeppelin creation is successful",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-06-07 09:02:50,2
13382230,[Dataproc]: GPU template is not available,"*Preconditions:*

1.  Jupyter is created

*Steps to reproduce:*
 # Add Dataproc with GPU to Jupyter 

*Actual result:*

1. GPU template is not available

*Expected result:*

1. GPU template is available

 ",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-04 21:34:44,1
13382228,[Spark Standalone cluster]: Cluster configuration template JSON is not available,"Cluster configuration template JSON is not available is not available for all potential spark Standalone cluster.

*Preconditions:*
 # Notebooks are created
 # Spark Standalone cluster is created for one Notebook

*Steps to reproduce:*
 # Add Spark Standalone cluster for other Notebook
 # Check off cluster configuration check box

*Actual result:*

1. Cluster configuration template JSON is not available

*Expected result:*

1. Cluster configuration template JSON is available",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-04 21:14:31,0
13382224,[Py3][Others] Wrong status for library installation,"These bugs were found during libs installation from Py3 and others if install library with valid name/unfound name on Notebook/Apache spark standalone cluster. 

*Preconditions:*
 # Jupyter is in running status

*Steps to reproduce:*
 # Install valid/unfound lib name for python3/others

*Actual result:*

1.  Library is installed (valid name) or uninstalled (invalid name)

1. Library status is 'Installation error'

*Expected result:*

1.  Library is installed (valid name) or uninstalled (invalid name)

2. Status is installed (valid name) or invalid name (unfound name)",AWS AZURE Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-04 20:50:55,2
13382127,[GCP][Investigation][Spark Standalone cluster][Dataproc]: AIRBNB preparation fails with error unknown magic command 'matplotlib',"It was found on GCP. Perhaps it is related to other clouds.

*Preconditions:*
 # Spark Standalone cluster/Dataproc is running
 # AIRBNB preparation playbook is loaded to Jupyter

*Steps to reproduce:*
 # Run AIRBNB preparation playbook for Spark Standalone cluster/Dataproc

*Actual result:*
 # Playbook running fails

{code:java}
An error was encountered:
unknown magic command 'matplotlib'
UnknownMagic: unknown magic command 'matplotlib'
{code}
*Expected result:*
 # Playbook running is successfully",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Minor,2021-06-04 11:12:29,3
13382109,[GCP][Investigation][Spark Standalone cluster]: Three last diagrams are not displayed for Flights_data_Visualization_R,"It was found in GCP on Jupyter during Flights_data_Visualization_R running for Spark Standalone cluster. Perhaps it is related to other clouds.

Diagrams are not conveyed for Spark Standalone cluster for three blocks:
 - Let's find the top 10 of the most unpunctual airlines
 - Number of flight performed by top companies
 - The average Flight Distance per Company

However these diagrams are conveyed for lokal kernel.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-04 10:03:18,3
13382091,[GCP][Investigation][Spark Standalone cluster][Dataproc] Investigate why Flight data visualization Py3 fails,"The case was found on GCP during Flight data visualization Py3 running on Spark Standalone cluster. Perhaps it is related to other clouds.

But the same playbook is successfully running for local kernel.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-04 08:56:43,3
13382077,[Investigation] [Computational resources]: Investigate why playbook running are not shown on master UI,"The bug was found on GCP for all notebooks. Perhaps it is related to other clouds.

 ",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-04 07:38:23,3
13381889,[GCP][AWS][Jupyter][Local kernel]: Playbook Playbook Flights_data_Preparation_Python3/Airbnb preparation running fails due to cannot find python3.7,"The bug was found on GCP/AWS. Perhaps it is related to other clouds. 

*Preconditions:*

1.  Playbook Flights_data_Preparation_Python3/Airbnb preparation  are loaded to Jupyter

*Steps on reproduce:*
 # Run playbook via Jupyter UI

*Actual result:*
 # Playbook running fails due to cannot run program python3.7

*Expected result:*
 # Playbook running is successfully",AWS Debian DevOps GCP Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-06-03 11:39:27,2
13381853,[Rstudio][TensorFlow with RStudio][Spark]: Implement specific python versions usage via virtual envs,"To improve stability of python libraries installation during a notebook deployment, specific versions of python in insolated environment is required.

(i) Python version should be the same for local & remote kernels.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2021-06-03 07:34:31,2
13381649,Unavailable dependencies for jars building during SSN/external endpoint/ (TensorFlow) deployment,"SSN is not deployed if we do not use Nexus for jars usage (Java dependencies are not built from maven repository).

*Steps to reproduce:*
 # Deploy SSN via Jenkins job not indicating nexus credentials

*Actual result:*
 # SSN deployment fails

*Expected result:*
 # SSN deployment is successfully",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-02 09:11:33,1
13381511,[Azure][GCP]: Endpoint creation fails on command apt-get install -y docker-ce=5:20.10.6~3-0~ubuntu-bionic,"*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Deploy Endpoint on Azure/GCP via Jenkins Job

*Actual result:*
 # Endpoint creation fails

{code:java}
eading state information... E: Version '5:20.10.6~3-0~ubuntu-bionic' for 'docker-ce' was not found --- Logging error --- Traceback (most recent call last): File ""/var/lib/jenkins/workspace/DLAB-Azure-deploy-endpoint/infrastructure-provisioning/terraform/bin/deploy/endpoint_fab.py"", line 235, in ensure_docker_endpoint .format(args.docker_version)) File ""<decorator-gen-4>"", line 2, in sudo File ""/var/lib/jenkins/workspace/DLAB-Azure-deploy-endpoint/venv/lib/python3.7/site-packages/fabric/connection.py"", line 30, in opens return method(self, *args, **kwargs) File ""/var/lib/jenkins/workspace/DLAB-Azure-deploy-endpoint/venv/lib/python3.7/site-packages/fabric/connection.py"", line 716, in sudo return self._sudo(self._remote_runner(), command, **kwargs) File ""/var/lib/jenkins/workspace/DLAB-Azure-deploy-endpoint/venv/lib/python3.7/site-packages/invoke/context.py"", line 212, in _sudo return runner.run(cmd_str, watchers=watchers, **kwargs) File ""/var/lib/jenkins/workspace/DLAB-Azure-deploy-endpoint/venv/lib/python3.7/site-packages/invoke/runners.py"", line 271, in run return self._run_body(command, **kwargs) File ""/var/lib/jenkins/workspace/DLAB-Azure-deploy-endpoint/venv/lib/python3.7/site-packages/invoke/runners.py"", line 404, in _run_body raise UnexpectedExit(result) invoke.exceptions.UnexpectedExit: Encountered a bad command exit code! Command: ""sudo -S -p '[sudo] password: ' apt-get install -y docker-ce=5:20.10.6~3-0~ubuntu-bionic""{code}
*Expected result:*
 # Endpoint creation is successfully",AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-06-01 17:18:24,2
13381427,[Refactoring]: Main functions: review and refactoring,"Main functions for DataLab deployment and resources provisioning are located in separated script fails.

Base code refactoring (remove unused code, remove code duplication, ensure libs naming and versioning)",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-06-01 10:28:24,3
13381405,Investigate how to show message about successfully self-service restarting and provisioning/billing restarting (external endpoints),"If restart self-service the self-service is restarted successfully, but appears error message.

Front-end receives error message because self-service is in restarting status and is unavailable.

Also provisioning/billing restart for external endpoint.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-01 08:37:33,1
13381395,Values of dropdown list should be under dropdown,"This tasks was taken from the ticket https://issues.apache.org/jira/browse/DATALAB-2378.

See attachments in the ticket https://issues.apache.org/jira/browse/DATALAB-2378",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-06-01 07:43:13,0
13381378,[GCP]: Investigate why VPC network is still available after SSN termination,"After SSN termination VPC network is still available. Please investigate why it isn't terminated as well. And if it's possible to fix in the scope of this ticket, please fix. 

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2021-06-01 05:52:58,2
13381264,[List of resources]: Wrong values for information icon,"*Preconditions:*
 # User is located on List of resources page

*Steps to reproduce:*
 # Click on Information icon
 # Click on Release Notes link

*Actual result:*
 # 404 error appears for Release Note
 # Branch name is Unknown

*Expected result:*
 # The latest release note is open successfully
 # Branch name is the name from which DataLab is deployed

 
----
(i) Branch and version should be actual as well.

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-05-31 11:12:09,1
13378430,Notebook is not available for creation,"*Preconditions:*
 # Project is in running status

*Steps to reproduce:*
 # Create any template of Notebook

*Actual result:*
 # Template is not available

*Expected result:*
 # Template is available",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-05-14 06:53:36,3
13378276,[Jupyter][TensorFlow with Jupyter][Zeppelin]: Implement specific python versions usage via virtual envs,"The default list of python versions defined during the configuration which later can be selected by users. For now it is allowed to point the Python version 3.7.9.

To improve stability of python libraries installation during a notebook deployment, specific versions of python in insolated environment is required.

(i) Python version should be the same for local & remote Spark kernels.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2021-05-13 09:34:38,3
13378270,Project creation fails on Python3,"*Steps to reproduce:*
 # Run auto-test

*Actual result:*
 # Docker for project creation executes with 1
 # Project is still in Creating status in WEB DataLab UI

*Expected result:*
 # Docker for project creation executes with 0
 # Project is in Running status in WEB DataLab UI",AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Blocker,2021-05-13 08:42:57,2
13377833,[Azure]: DeepLearning creation fails on stage of spark-tensorflow-connector ,"*Preconditions:*
 # Edge is created on Azure

*Steps to reproduce:*
 # Create Deep Learning

*Actual result:*
 # Deep Learning creation fails

*Expected result:*
 # Deep Learning creation is successful

 ",AZURE Debian DevOps Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2021-05-11 11:05:05,2
13377218,[Environment management]: Reconfiguring overlaps stop icon,"*Preconditions:*
 # Apache Spark cluster is in reconfiguring status

*Steps to reproduce:*
 # Go to Environment Management page

*Actual result:*
 # Reconfiguring overlaps stop icon

*Expected result:*
 # Reconfiguring does not overlap stop icon

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2021-05-07 11:49:25,0
13377190,[Configuration page]: Information message for service restarting shows up with delay,"*Preconditions:*
 # User is located on Configuration page

Steps to reproduce:
 # Check off any service
 # Click on Restart button
 # Click on Yes button

*Actual result:*

1. Information message shows up with delay

*Expected result:*

1. Information message shows up without delay

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-05-07 09:09:14,0
13377061,[Promotion page]: Replace links for releases,"There is a source code in the promotion page [https://datalab.apache.org/#download]

Links for releases are obsolete, we should change them by the links from [https://archive.apache.org/dist/incubator/datalab/] ",Front-end,['DataLab Main'],DATALAB,Task,Minor,2021-05-06 15:49:12,0
13376816,[Py3]: Local Apache Toree - Scala kernel dies during connection,"The bug was found in branch DATALAB-2091 on GCP, perhaps it is related to others cloud.

*Preconditions:*
 # Jupyter is created
 # Flights_data_Preparation_Scala-Spark.ipynb is loaded in Jupyter

*Steps to reproduce:*

1. Open Flights_data_Preparation_Scala-Spark.ipynb

*Actual result:*
 # Connection is not established
 # Local Apache Toree - Scala kernel is dead

*Expected result:*

 1. Connection is established

2. Local Apache Toree - Scala kernel is alive",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-05-05 15:25:31,3
13376774,[Py3]: Library version is not rewritten,"The bug was found in branch DATALAB-2091.

*Preconditions:*
 # Library from Python3/Others group is installed successfully on Notebook
 # User is located on Manage library pop up

*Steps to reproduce:*
 # Type the same library from Python3/Others which has been already installed but indicate wrong version
 # Click Install button

*Actual result:*
 # The status changes to Invalid version
 # The version of library is the previous

*Expected result:*
 # The status changes to Invalid version
 # The version of library is new

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-05-05 10:26:40,2
13376766,[Scheduler]: It's impossible to switch between AM and PM,"The bug was found in branch *DATALAB-2091*.

*Preconditions:*
 # User is located on Scheduler pop up

*Steps to reproduce:*
 # Click on Chose start/finish time
 # If it is selected am click on pm and vice versa 
 # Click Assign button

*Actual result:*
 # New update is not applied

*Expected result:*
 # New update is applied

(i) The new update will be applied if first of all we change numbers and after that select PM or AM. But this case does not work for number (?)*12*.
----
(i) Please fix this in *dev* as well. ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-05-05 09:50:50,0
13376751,[Py3]: Library installation from group Python3/others fails if library name is not found,"The bug was found in branch *DATALAB-2091*.

*Preconditions:*
 # User is located on Manage libraries pop up

*Steps to reproduce:*
 # Install non-existent library in group Python3/Others

*Actual result:*
 # Docker runs with 1
 # Library status sticks Installing in WEB DATALAB UI

*Expected result:*
 # Docker runs with 0
 # Library status is 'Invalid name' in WEB DATALAB UI

 
 
 
 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-05-05 08:24:30,2
13376749,[Py3]: If Manage libraries is open too long it's impossible to install library,"The bug was found in branch DATALAB-2091.

*Preconditions:*
 # User is located on Manage libraries pop up

*Steps to reproduce:*
 # Install libraries during 15 minutes (do not close Manage libraries pop up)
 # After 15 minutes type any library name in Library name text field

*Actual result:*
 # Autocomplete does not work
 # It's impossible to install library
 # Error appears in Console

{code:java}
ERROR TypeError: Cannot read property 'match' of undefined{code}
*Expected result:*
 # Autocomplete works
 # It's possible to install library
 # Error does not appear in Console

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-05-05 08:14:04,0
13376722,SSN deployment fails on stage of self-service jar building,"*Steps to reproduce:*
 # Deploy SSN

*Actual result:*
 # SSN deployment fails

*Expected result:*
 # SSN deployment is successfully",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Blocker,2021-05-05 06:00:18,2
13375794,[Configuration page]: Checker of instance processing stage should verified other user resources,"It's forbidden to restart a provisioning if at least one of the resources is in processing stage.

Checker verifies only own resources. But the feature is for Admin. So, checker is supposed to verify through all resources (others users), not only own.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-04-29 09:23:58,0
13375571,[Configuration page]: Instance status is not distinguished by endpoint,"*Preconditions:*
 # Project1 has edge1 (internal endpoint) and edge2 (external endpoint)
 # In local endpoint one of the resource in in creating/terminating status
 # User is located in Configuration page

*Steps to reproduce:*
 # Choose external endpoint in dropdown list Selected endpoint
 # Check off Provisioning
 # Click on Restart button
 # Submit restart

*Actual result:*
 # Restart is not done
 # Information message appears:

{code:java}
Provisioning service: can not be restarted because one of resources is in processing stage. Please try again later.{code}
*Expected result:*
 # Restart is done

(i) Resources which are in processing stage should be distinguished by endpoints. Admin should be able to restart provisioning service for endpoint1(there aren't any resources which are in processing stage) even if for endpoint2 some resources are in processing stage.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-04-28 11:07:47,0
13375564,[Configuration page]: Investigate how to update new values in editor during endpoints switching,"If I switch between endpoints and go to editor in editor previous endpoint value is still available until I scroll down.

Investigate if it's possible to update values for current endpoint without scrolling.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-28 10:07:37,0
13375261,UI minor improvement for all pages,"# Values of dropdown list should be under dropdown (see attachments P1... + drop down in configuration page) -       (i) The implementation requires a lot of efforts, so this task will not be done in the scope of the ticket. The other ticket is created https://issues.apache.org/jira/browse/DATALAB-2407.
 # [Configuration page]: Move Restart button to the header page, near Refresh button (/) - Checked
 # [Library management]: Do UI more intuitive:
 Add pre-loader (load line) as it is implemented in all pages during refresh. Perhaps, under header Manage <notebook> Libraries
 What others action should we disable during lib list retrieving? What about if user wants to select a cluster(previously he was in notebook) or others groups? - WE decided not to disable others actions, because lib list is receiving by group for all notebooks.
 Change info message from 'Autocomplete is currently loading for <name> group' to 'Library list is being loaded at the moment. Please wait a minute.'. (/) - Checked.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2021-04-27 08:34:46,0
13375046,[List of Resource]: Cost value should be in one row.,Perhaps shrink compute width.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-26 11:15:51,0
13374066,[Configuration]: Alter messages showing up according to selected service during restarting,"[Configuration page]: Convey message for each particular services.
{code:java}
Oops!  Service_name restarting failed 
{code}
{code:java}
Success!  Service_name restarting started
{code}
You can ponder about others ways of implementation.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-21 08:44:40,0
13374042,[DevOps][DeepLearning]: Update DeepLearning notebook for using existing cloud images for deeplearning,"Now it is implemented for GCP/AWS.
It should be done for AZURE.",AWS Azure Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-04-21 07:34:53,3
13373834,[Config file] File.yml content which should be shown,"# Should we show apache licence in editor??? - {color:#00875a}No, don't show licence in editor.{color}
 # In billing.yml convey accessKeyId/secretAccessKey value in asterisk in section:

{code:java}
# Adapter for reading source data. Known types: file, s3file
{code}
Because now it's conveyed empty value and after others changes this empty value convey to configuration file on SSN.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-20 11:32:04,4
13373825,[Configuration][Local endpoint]: Service restarting leads to 404 error,"*Preconditions:*
 # Environment is created

*Steps to reproduce:*
 # Go to Configuration page
 # Restart Provisioning/Self-service/Provisioning services 

*Actual result:*

1.  Service does not restart

2. 404 error appears

*Expected result:*

1.  Service restarts

2. 404 error doe not show up

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-04-20 10:43:20,1
13373823,If self-service restarting fails do not show a message ' Success! Service restarting started! ',Show success message if self-service restarting is successful.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-04-20 10:29:36,0
13373812,[Configuration]: Discard changes does not work for external endpoint,"*Preconditions:*

1.  External endpoint is connected to DataLab

*Steps to reproduce:*
 # Go to Configuration page
 # Go to Billing or Provisioning tab
 # Change any value in editor
 # Hit Discard changes button

*Actual result:*
 # Changes are not discarded

*Expected result:*
 # Changes are discarded",AZURE Debian Front-end GCP RedHat aws pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-04-20 09:55:15,0
13373793,Change message for service restarting on confirmation dialog,"* From:

{code:java}
Can not restart provisioning because one of resources is in processing stage.
{code}
to: 
{code:java}
Provisioning: Can not be restarted  because one of resources is in processing stage. Please try again later.
{code}
 *  From:

{code:java}
Restarting billing service will make DataLab unavailable for some time.  
Can not restart provisioning because one of resources is in processing stage. 
{code}
to: 
{code:java}
Billing service: restarting will make DataLab unavailable for some time.
Can not be restarted because one of resources is in processing stage. Please try again later.
{code}
 The same changes for self-service.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-20 08:16:21,0
13372950,[DevOps][DeepLearning]: Obtain image list and description,Work in branch DATALAB-2091.,Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2021-04-16 09:12:23,3
13372938,[Back-end][DeepLearning]: Convey to front-end image list and description,Work in branch DATALAB-2091.,Back-end Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-04-16 08:47:32,1
13372935,[Front-end][DeepLearning]: Show image list and description on Web DataLab UI,Work in branch DATALAB-2091.,Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Major,2021-04-16 08:47:10,0
13371990,Sometimes project creation fails on stage of luarocks installation,"*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Create a project

*Actual result:*
 # Project creation fails

{code:java}
 Error: Failed installing dependency: https://luarocks.org/lua-resty-session-3.8-1.src.rock - Could not fetch rock file: Error fetching file: Failed downloading https://luarocks.org - Failed downloading https://luarocks.org/lua-resty-session-3.8-1.src.rock - /var/cache/luarocks/https___luarocks.org/lua-resty-session-3.8-1.src.rock
{code}
*Expected result:*
 # Project is created successful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-04-13 09:05:52,2
13370610,[Configuration]: Do not show self-service for external endpoint,"There is only a billing and a provisioning services for external endpoint. So, do not show self-service for external endpoint on Configuration page",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-04-09 09:42:53,0
13370303,[Front-end]: Do not allow to restart provserv if one of resource is in processing stage,"(i) {color:#de350b}Do not allow to restart provserv is any instance is in status:{color}
 - Creating
 - Configuring
 - Reconfiguring
 - Creating image
 - Stopping
 - Starting
 - Terminating
||Status||Notebook||Cluster||project (Edge node)||
||Creating|+| +| +|
||Configuring| +| +| -|
||Reconfiguring| +| +| -|
||Creating image| +| -| -|
||Stopping| +| +| +|
||Starting| +| +| +|
||Terminating| +| +| +|

 

Confirmation message shows up: 
{code:java}
'Oops!
 Can not restart provisioning because one of resource is in processing stage'
{code}
If restart provserv together with other service:
{code:java}
'Oops! Can not restart provisioning because one of resource is in processing stage'
 
Success! <Service_name> restarting started! 
{code}
Do it as it is implemented for project management.

Investigate what option should we choose: 
 [~vvitanska], I investigated these two options, and it will be better to implement the second one.
 # Show information message after hitting restart button
 # Show information message on confirmation dialog before hitting restart button",AWS AZURE Debian Front-end GCP Known_issues(release2.3) Known_issues(release2.4) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-04-08 07:13:27,0
13370294,Jupyter creation fails on stage of roxygen2 package installatoin,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create Jupyter

*Actual result:*
 # Jupyter creation fails

{code:java}
Requested: cd /usr/local/spark/R/lib/SparkR; R -e ""install.packages('roxygen2',repos='https://cloud.r-project.org')"" R -e ""devtools::check('.')""
Executed: sudo -S -p 'sudo password:'  /bin/bash -l -c ""cd /usr/local/spark/R/lib/SparkR; R -e \""install.packages('roxygen2',repos='https://cloud.r-project.org')\"" R -e \""devtools::check('.')\""""
{code}
*Expected result:*
 # Jupyter is created successfully

 

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Critical,2021-04-08 07:00:16,2
13370134,[Messages] & [Py3]: Do not show library installed with error if notebook is created from custom image,"Work in branch DATALAB-2091.

[Library management]: If Notebook2 is created from custom image do not show installation error for Notebook2",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Minor,2021-04-07 15:02:46,1
13370076,[Py3]: Ungit commit executes with error,"*Preconditions:*
 # Notebook is running
 # Git credentials are set up
 # Project is cloned to Notebook via ungit

*Steps to reproduce:*
 # Add a new file to the cloned project in Notebook web UI
 # Via ungit link got to the project
 # Hit 'Commit' button

*Actual result:*
 # Commit is not successful
 # The error appear:

{code:java}
TypeError: a bytes-like object is required, not 'str'{code}
*Expected result:*
 # Commit is successful

(i) Work in branch DATALAB-2091.",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-04-07 11:48:07,3
13370009,[Py3]: Jupyter creation from project/custom images fail,"This bug is related to project and custom images. The bug is not reproduced for Rstudio, TensorFlow with Jupyter, Zeppelin.

*Preconditions:*
 # Jupyter1 of one type is created in project1

*Steps to reproduce:*
 # Create Jupyter2 of the same type in project1

*Actual result:*
 # Notebook creation fails

*Expected result:*
 # Notebook creation is successful

(i) Work in branch DATALAB-2091.",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-04-07 08:58:08,2
13370005,[Front-end]: Retrieve the list of available GPU during endpoint deployment,"Work in branch DATALAB-2091.

This ticket is related only for GCP and for Jupyter, DeepLearning, TensorFlow with Jupyter.",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-04-07 08:49:36,0
13370003,[Back-end]: Retrieve the list of available GPU during endpoint deployment,"Work in branch DATALAB-2091.

This ticket is related only for GCP and for Jupyter, DeepLearning, TensorFlow with Jupyter.",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2021-04-07 08:46:42,1
13370002,[DevOps]: Retrieve the list of available GPU during endpoint deployment,"Work in branch DATALAB-2091.

This ticket is related only for GCP and for Jupyter, DeepLearning, TensorFlow with Jupyter.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2021-04-07 08:46:01,3
13369968,[Py3][Notebook]: Reconfiguration local spark fails,"*Preconditions:*
 # Notebook is in running status

*Steps to reproduce:*
 # Reconfigure local spark for Notebook

*Actual result:*
 # Reconfiguration fails

*Expected result:*
 # Reconfiguration is successful

(i) Reconfiguration also fails if reconfigure local spark on stage of Notebook creation.

(i) Work in Branch DATALAB-2091.

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-04-07 06:48:02,2
13369728,[List of resources]: Change the location of context menu,The context menu should show up above gear action if it is not enough  space between gear and bottom screen level.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-04-06 08:56:14,0
13369547,[AWS][Azure]: SSN creation fails on dowloading decorator-5.0.2.tar.gz,"*Steps to reproduce:*
 # Create SSN on Azure/GCP

*Actual result:*
 # SSN creation fails
{code:java}
Collecting decorator (from traitlets->qtconsole==4.7.7)
  Downloading https://files.pythonhosted.org/packages/71/bd/042e63027bea950da4e6f5545eb59b973868da854ab30a18128d3b884104/decorator-5.0.2.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-build-G4w6yi/decorator/setup.py"", line 4, in <module>
        exec(open('src/decorator.py').read(), dic)  # extract the __version__
      File ""<string>"", line 162
        print('Error in generated code:', file=sys.stderr)
                                              ^
    SyntaxError: invalid syntax
{code}
*Expected result:*
 1. SSN creation is successful",AWS Azure pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-04-05 07:14:24,2
13369329,[Library management][Py3]: Set of library tasks,"# Dependency during installation is not shown on DataLab Web UI. *fixed 12.04.21 (/)*
 # Adjust information message if lib version is unavailable. *fixed. that message was not supposed to be shown there (/)*
 # Obey consistency in error message. For example, add <ERROR>. [~vvitanska] This can not be implemented as some error messages have ""ERROR:"" at the beginning and some do not. If <ERROR> is added, some messages will begin with ""<ERROR> ERROR:"" and others just with ""<ERROR>"". {color:#00875a}OK{color}
 # Add comma between versions in information message.  *fixed 13.04.21 (/)*
 # Java dependency is not installed successfully. *fixed 14.04.21 (/)*",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-04-02 16:15:35,2
13369327,[Library management]: Extend validation for lib name,"If Library name contains space the docker runs with '1' and still remains 'installing' status for the library on the DataLab Web UI.

Now it's implemented that spaces are cut in the beginning/end of library name by the ticket https://issues.apache.org/jira/browse/DATALAB-2257.

But also check if space is inside of library name.

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-02 15:56:23,0
13369258,[Notebook]: Allocate spark driver/spark executor memory according to the Best practice,"Total executor memory = total RAM per instance / number of executors per instance


Spark.executors.memory = total executor memory * 0.90


Spark.driver.memory = spark.executors.memory


According to [https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/]

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-04-02 08:04:04,3
13369048,[Bucket browser]: Disabled buttons should not trigger,Upload files button should not trigger if it is disabled.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-04-01 08:14:34,0
13368322,[Azure]: Investigate why SSN termination fails,"[Azure]: Please investigate why SSN termination fails via jenkins job:
{code:java}
static IP address az-29-03-21-ssn-static-ip has been removed
Instance az-29-03-21-ssn has been terminated
[Error-2021-03-29 06:01:46]: Failed to terminate instances. Exception: 'SBN'
{u'error': u""[Error-2021-03-29 06:01:46]: Failed to terminate instances. Exception: 'SBN'""}
[Error-2021-03-29 06:01:46]: Failed to terminate ssn..
{u'error': u""[Error-2021-03-29 06:01:46]: Failed to terminate instances. Exception: 'SBN'[Error-2021-03-29 06:01:46]: Failed to terminate ssn..""}
{code}
If it's possible please fix in the scope of this ticket.",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-29 08:18:43,2
13367987,[Library management][Others group]: Shrink row space for information message,"Work  in branch 'DATALAB-2091'.

Shrink row space for information message in Others group.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2021-03-26 18:41:03,0
13367570,Instance status checker docker executes with '1' after provisioning restarting,"*Preconditions:*

1.  At least one notebook is in '-ing' status except 'running'

*Steps to reproduce:*

1.  Restart provisioning service

*Actual result:*
 # CheckInfrastructureStatusScheduler runs with '1'
 # Status is not changed from DataLab Web UI side

{code:java}
 raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: ParamValidationError(u""Parameter validation failed:\nInvalid type for parameter InstanceIds[0], value: None, type: <type 'NoneType'>, valid types: <type 'basestring'>"",) is not JSON serializable{code}
*Expected result:*
 # CheckInfrastructureStatusScheduler runs with '1'
 # Status is changed from DataLab Web UI side",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2021-03-25 12:56:15,1
13365799,Investigate why Some role is added during SSN deployment. It leads to collect resources (route table/vpc/subnet etc) after SSN termination,"On AWS SSN termination fails with error:
{code:java}
DeleteConflictException: An error occurred (DeleteConflict) when calling the DeleteRole operation: Cannot delete entity, must detach all policies first.
{code}
And sometime the role somehow is assigned to the instance.

Please investigate why this role is assigned and if you could fix it in the scope of this ticket.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-17 11:24:20,2
13365429,[Environment management]: Extra scrollbar shows up after clicking on gear icon,After clicking on gear icon on Environment management a vertical extra scrollbar appears.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-16 07:16:19,0
13364661,[Git credentials]: Previous data should be cleared up during adding a new account,"*Preconditions:*
 # SSN is created
 # User is located on 'Manage Git Credentials' popup
 # At least one git account is added

*Steps to reproduce:*

1.  Click on edit icon in List tab

2. Click on List tab

3. Click on Add account button

*Actual result:*

1.  The previous values of editing account is present

Expected result:

1. The previous values of editing account is absent",AWS Azure Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-03-12 14:01:58,0
13364651,[Manage git credential]: Get rid of extra vertical scrollbar,Get rid of  extra scrollbar during account adding,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-12 13:12:24,0
13364454,Font consistency for all pages,"For all value grid use Open-Sans 13px size.

Font should be changed in:
 * [List of resources]: Detailed billing popup (/)
 * [Configuration]: check box  (/)
 * [Project]: value in Group column (/)
 * [Project]: Confirmation dialog for stop/terminate edge node (it's still used Roboto font) (/)
 * [Project]: Edit project stage2/stage3 (it's still used Roboto font) (/)
 * [Bucket browser]: It's still used Roboto font (/)
 * [Git credentials]: Adjust size (/)
 * [List of resources]: Notebook, cluster popup (it's still used Roboto font) (/)
 * [Environment management]: confirmation management for endpoint deletion (it's still used Roboto font) (/)
 * [Configuration]: Edit file.yml (it's still used Roboto font) (/)
 * [Audit]: Numbers of items per page (it's still used Roboto font) (/)
 * [Create analytical tool][Add Compute]: Instance size/type (it's still used Roboto font) (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2021-03-12 11:58:28,0
13364110,Investigate possibility to show all drop down list values under drop down list,"Investigate it for:
 - [List of resources]: Project drop down list
 - [Project]: Group/Endpoint drop down lists for project editing

 - [Configuration]: Select endpoint drop down list

If it does not take much efforts please fix it in the scope of this ticket.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['Cloud endpoint'],DATALAB,Task,Minor,2021-03-12 10:47:14,0
13364058,Show that  external endpoint is inactive in drop down list,"1. If external endpoint is inactive show it for user.

*[Configuration]*: In Selected endpoint dropdown list show <endpoint_name (external endpoint), inactive>. If external endpoint is inactive do not allow a user to select it and show it as disabled.

*[Project]*: In edit project for endpoint show <endpoint_name (inactive)>. If external endpoint is inactive do not allow a user to select it and show it as disabled.

2. [Configuration]: Get rid of <:> from Selected endpoint drop down list.

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-12 08:44:03,0
13363484,[Library management]: Drop down values should not change from uppercase to lowercase after selecting,This case is related to resource type and status.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-10 09:32:25,0
13363233,Add external endpoint value,Add value (external endpoint) in Selected endpoint drop down list only to external endpoint.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-03-09 10:02:59,0
13362566,[AWS]: Investigate why instance status checker executes with '1',The instance status checker executes with '1' until EMR docker creation starting. Perhaps it's conected.,AWS Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-03-05 10:15:27,1
13362543,[Computational resources]: Investigate why EMR status was changed only from DataLab Web UI,"During autotest running EMR status was changed from running to terminated. However in AWS console the EMR was running.

Investigate this case, found out the reason and fix it.",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-03-05 08:53:42,1
13362520,[Billing]: Total sum should be sticky during scroll right/left,This case is related if it is only one row in billing grid.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-03-05 07:40:21,0
13362517,[Billing]: The right grid boarder is hidden,"This bug is reproduced when the billing is empty or it is available only one billing row in the grid.

*Preconditions:*
 # User is located on 'Billing' page

*Steps to reproduce:*
 # Resize window to see horizontal scroll bar
 # Look at the right side

*Actual result:*
 # The right grid boarder isn't visible

*Expected result:*
 # The right grid boarder is visible

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-03-05 07:17:59,0
13362313,[Configuration]: Set of task for external endpoint for provisioning/billing services,"# There is only a billing and a provisioning services for external endpoint. So, should we show self-service for external endpoint? (i) {color:#de350b}No, we should not show them (/) {color}
 # Provisioning/billing services restart does not work. Error 404. Now 500 error appears, 28.04.2021&01/06/2021 (/) 08.06.2021
 # Provisioning/billing.yml editing does not work. Error 400. Now 500 error appears. Now 400 error appears, 28.04.2021. Now 500 error (/) 08.06.2021.
 # In DataLab Web UI provisioning/billing.yml for external endpoint show not external endpoint values. They show values local endpoint (/) 28.04.2021
 # If choose active external endpoint in Select endpoint drop down list the 404 error appears. Now 500 error appears. (/) 28.04.2021
 # If self-service is not needed for external endpoint do not do point6. If restart a self-service message appears : 
{code:java}
Success!
Service restarting started!
{code}
However in Network 404 error appears (/) We do not need Self-service for external endpoint
 7.  If self-service is not needed for external endpoint do not do point7. If edit/save a self-service. yml a message appears : 
{code:java}
Success!
Service configuration saved!
{code}
However in Network 404 error appears. (/) We do not need Self-service for external endpoint",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-03-04 14:03:04,1
13362143,[Front-end][Configuration]: Show only active and connected endpoint,"External endpoint can be disconnected or in stopped status, so it will be impossible to edit yml file.

Please show only connected and active endpoint in Select endpoint drop down list on Configuration page.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-03-03 16:16:21,0
13362141,[Back-end][Configuration]: Show only active and connected endpoint,"External endpoint can be disconnected or in stopped status, so it will be impossible to edit yml file.

Please show only connected and active endpoint in Select endpoint drop down list on Configuration page.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-03-03 16:11:45,1
13362132,Spark reconfiguration fails on Spark standalone cluster ,"This bug was found on Jupyter. Perhaps, it's related to all type of notebooks and all clouds.

(?) In addition check if it impacts on reconfiguration Spark on creation stage.

*Preconditions:*
 # Spark cluster is created

*Steps to reproduce:*
 # Reconfigure Spark Cluster

*Actual result:*
 # Reconfiguration fails

*Expected result:*
 # Reconfiguration is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-03-03 15:38:34,2
13361895,[GCP]: Project creation fails on external endpoint,"*Preconditions:*
 # SSN is created on AWS
 # Endpoint is created on GCP
{code:java}
[CREATE FIREWALL FOR EDGE NODE]
Service account viaws-0203-multiple-gcp-03-ps-sa removed.
IAM role viaws-0203-multiple-gcp-03-87212-ps-role removed.
Service account viaws-0203-multiple-gcp-03-edge-sa removed.
IAM role viaws-0203-multiple-gcp-03-f2478-edge-role removed.
[Error-2021-03-02 15:23:56]: Failed to create firewall for Edge node.. Exception: string indices must be integers, not str
{u'error': u'[Error-2021-03-02 15:23:56]: Failed to create firewall for Edge node.. Exception: string indices must be integers, not str'}
{code}

*Steps to reproduce:*
 # Create an edge on GCP endpoint

*Actual result:*
 # Edge creation fails

 

*Expected result:*
 # Edge creation is successful",Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-03-02 15:44:17,2
13361800,[Notebook templates] & [AWS] Notebook links should be in lower case,"If Notebook name contains upper case these upper cases are in browser link, but should be in lower cases.
",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-03-02 09:29:06,3
13361154,[Part2]: Set of tasks for lib management,"# If notebook is not in running status expend library grid to the bottom.
 # Values in resource type/status should start from sentence case. the same should be in the Resource type/status dropdown list.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2021-02-26 10:20:29,0
13360646,[Environment management]: Get rid of select off check box if any items is not available,Get rid of select off check box if any items is not available on Environment management page.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-24 14:57:34,0
13360635,[Billing]: Convey total sum if ever it equals '0',Convey total sum if it equals '0'.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2021-02-24 14:36:28,0
13360621,[Audit]: Filter confirmation action is disabled,"*Preconditions:*
 # User is located on 'Audit' page

*Steps to reproduce:*
 # Click on any  dropdown list
 # Select any item

*Actual result:*
 # Confirmation filter action is disabled

*Expected result:*
 # Confirmation filter action is enabled

----
(i) If anything is not selected (by default) confirmation filter actions should be disabled.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-02-24 13:20:20,0
13360135,[Configuration]: UI improvements in editor,"# Ged rid of vertical line in editor
 # Adjust scrollbar to default one in editor",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2021-02-22 15:32:01,0
13360106,[Part1]: Set of tasks for lib management,"# Horizontal lines are absent for library grid (/)
 # Text box for preinstalled stage should not be limited by one row (/)
 # Add left padding for grid value (/)
 # Accustom scrollbar to the all default scrollbars (/)
 # Values in resource type/status should start from sentence case. the same should be in the Resource type/status dropdown list - move to the other ticket
 # Grid values between dropdown and header should be invisible (/)
 # If installed library sometimes appears image of library installation. And this image is not aligned by center (/)
 # In small screen Close/Install buttons overlap (/)
 # If notebook is not in running status expend library grid to the bottom - move to the other ticket
 # After library management visiting the additional infor popup (Audit) is extended (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Task,Major,2021-02-22 12:37:45,0
13360079,Replace logo for production Keycloak,DLab -> DataLab,AWS AZURE Debian DevOps GCP Internal RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-02-22 11:13:49,3
13359579,Change logo if page isn't found,DLab -> DataLab,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-19 12:38:20,0
13359293,[Environment management]: The right grid boarder is hidden,"*Preconditions:*
 # User is located on 'Environment management' page

*Steps to reproduce:*
 # Resize window to see horizontal scroll bar
 # Look at the right side

*Actual result:*
 # The right grid boarder isn't visible

*Expected result:*
 # The right grid boarder is visible

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-02-18 12:06:56,0
13359288,[Billing][Firefox browser]: Total sum cell does not match footer,"This bug is reproduced only for Firefox browser v85.0.2 (64-bit)

*Preconditions:*
 # Billing is available

*Steps to reproduce:*
 # Got to 'Billing report' page
 # Look at the total sum

*Actual result:*

1.  Total sum cell does not fit footer

*Expected result:*

1.  Total sum fits footer",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-02-18 11:52:29,0
13359271,[Auto-test]: Add year for SBN,"For SBN (uto-test running) and a year as well:
{code:java}
aws-18-02 -> aws-18-02-2021
Template <cloud>-<day>-<month>-<year>{code}",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-02-18 10:54:13,2
13359042,[List of resources]: 'Environment name' header is cut,"*Preconditions:*
 # Notebook is created

*Steps to reproduce:*
 # Go to 'List of resource'
 # Look at 'Environment name' header

*Actual result:*

1. 'Environment name' header is cut in the left side

*Expected result:*

2. 'Environment name' header is  not cut in the left side

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-02-17 11:06:12,5
13359037,[Browsers] Investigate performance during file uploading via Bucket browser in Safari,"It's only for Safari browser version 14.0.2 (15610.3.7.1.10, 15610).
----
I've uploaded 13 file, each file has 154.88 MB via Bucket Browser.

The upload process starts, but folder creation, cancel uploading was with big delay.

Could you investigate why it is reproduced only for Safari browser and fix it?",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2021-02-17 10:47:30,0
13358824,[GCP]: SSN deployment fails on steps of 'rsa' downloading ,"*Steps to reproduce:*
 # Deply SSN on GCP via Jenkins job

*Actual result:*
 # SSN deployment fails

{code:java}
Downloading rsa-4.7.1.tar.gz (38 kB) [91mERROR: Package 'rsa' requires a different Python: 2.7.17 not in '>=3.5, <4'{code}
*Expected result:*
 # SSN deployment is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-02-16 09:32:37,2
13358625,[Roles]: Distinguish each user in hint confirmation dialog,Each user should be start from a new row in hint.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Trivial,2021-02-15 10:31:47,0
13358168,Add a vertical scrollbar for project,If there are a lot of groups project grid extends. Please add a vertical scrollbar.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-11 14:22:45,5
13358162,Enhancement for dropdown list,"# [List of resources][Environment management][Billing][Audit]:Decrease left padding for values in dropdown list (/)
 # [List of resources]: Compute values in dropdown list are absent (/)
 # [Billing]: Product dropdown values should be hid under service charges if they cross each other (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-11 13:06:54,5
13358159,Extend validation for lib installation,"# Do not allow to add library with empty name
 # Do not allow to add library if this lib is selected, but is not installed
 # Do not allow to add library if this library is selected, but is not installed and you try to add it via 'enter' -> as a result library with empty name is added.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-02-11 12:33:14,5
13357945,Obey scrollbar consistency for the whole DataLab,For Chrome/Fire Fox browsers some scrollbars are different than another and for Safari browser they are absent.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-10 14:46:03,0
13357939,Jupyter/RStudio/Zeppelin/TensorFlow/JupyterLab/Data engine creation fails on stage of 'pbdZMQ' package installation,"This bug is related to Jupyter/RStudio/Zeppelin/TensorFlow/JupyterLab/Data engine.

*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create Jupyter

*Actual result:*
 # Jupyter creation fails

{code:java}
ERROR: this R is version 3.4.4, package 'pbdZMQ' requires R >= 3.5.0
[datalab-user@ip-172-31-48-184.us-west-2.compute.internal] out: Error: Failed to install 'unknown package' from URL:
[datalab-user@ip-172-31-48-184.us-west-2.compute.internal] out:   (converted from warning) installation of package ‘/tmp/Rtmpz82Ihn/remotes6ec65d453986/pbdZMQ’ had non-zero exit status
[datalab-user@ip-172-31-48-184.us-west-2.compute.internal] out: Execution halted
{code}
*Expected result:*
 # Jupyter creation is successful

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2021-02-10 13:45:16,2
13357926,Get rid of the error 'cannot read property 'length' of null',This error appears if in grid user contains null value.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-02-10 12:14:54,0
13357894,[GCP]: SSN deployment fails on command 'unattended-upgrades -v',"*Steps to reproduce:*
 # Deploy SSN via Jenkins job on GCP

*Actual result:*
 # SSN deployment fails

{code:java}
Requested: unattended-upgrades -v
Executed: sudo -S -p 'sudo password:'  /bin/bash -l -c ""unattended-upgrades -v""
Aborting.
{code}
*Expected result:*
 # SSN deployment is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Blocker,2021-02-10 08:53:25,2
13357883,SSN deployment fails in command of npm run build.prod,"*Steps to reproduce:*
 # Deploy SSN via Jenkins job

*Actual result:*
 # SSN deployment fails

{code:java}
[datalab-user@ec2-44-238-215-135.us-west-2.compute.amazonaws.com] out: [1m[31m[0m[39m[22m
[datalab-user@ec2-44-238-215-135.us-west-2.compute.amazonaws.com] out: [1m[31m[1m[31mERROR in [96msrc/app/administration/configuration/configuration.component.ts[0m:[93m192[0m:[93m53[0m - [91merror[0m[90m TS2345: [0mArgument of type 'boolean' is not assignable to parameter of type 'string'.[39m[22m
{code}
*Expected result:*
 # SSN deployment is successful",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-02-10 07:34:22,5
13357699,SSN deployment fails in stage of npm running,"*Steps to reproduce:*
 # Deploy SSN via Jenkins job

*Actual result:*
 # SSN deployment fails

{code:java}
Requested: npm run build.prod
Executed: sudo -S -p 'sudo password:'  /bin/bash -l -c ""cd /opt/datalab//sources/services/self-service/src/main/resources/webapp/ >/dev/null && npm run build.prod""
{code}
*Expected result:*
 # SSN deployment is successful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-02-09 07:42:59,5
13357507,[Bucket browser]: Any action is blocked during a file download only for Safari browser,"*Preconditions:*
 # Bucket browser popup is open via Safari browser

*Steps to reproduce:*
 # Upload any file more than 1 GB
 # Click close popup

*Actual result:*
 # Popup is closed

*Expected result:*
 # Popup is not closed
 # Any action is blocked

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2021-02-08 11:23:48,5
13357499,[Bucket browser]: Apply central horizontal alignment to confirmation message,[Bucket browser]: Apply central horizontal alignment to confirmation message about cancel uploading.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2021-02-08 10:58:31,5
13357495,"[Roles]: Add hints for group_name, user_name","# [Roles]: Add hint for group_name (/)
 # [Audit]: Add hint for group_name (/)
 # [Roles]: Cut users name and add hint for it for confirmation dialog of user removing (/)
 # [Role]: Apply fix size for 'Roles' dropdown list. (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-08 10:51:33,0
13357479,[Configuration]: Obey consistency for services,"# Rename services in Main settings and in every tab in Configuration page:
 - Self-service
 - Provisioning
 - Billing
 # Confirmation dialog: 
 - If it's self-service and the other (only one service) mentioned, then pointed 'Restarting self-service, <name> service will make DataLab unavailable for some time.'
 - If it's self-service and the other (more than one service) mentioned, then pointed 'Restarting self-service, <name>, <name> services will make DataLab unavailable for some time.'
 - If it's another (more than one service except self-service) mentioned, then pointed 'Restarting <name>, <name> services will make DataLab unavailable for some time.'
 - If it's another (only one service except self-service) mentioned, then pointed 'Restarting <name> service will make DataLab unavailable for some time.'
 # Apply central horizontal alignment for the confirmation message.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-08 09:25:48,5
13356492,Add new notebook templates for promotion page,"Add/change new Notebook templates for promotion page [https://datalab.incubator.apache.org|https://datalab.incubator.apache.org] / [https://datalab.apache.org|https://datalab.apache.org/] in section Features (Leverage the power of analytical tools):
 # Zeppelin -> Apache Zeppelin
 # Tensor Flow -> Jupyter with TensorFlow
 # RStudio with TensorFlow (implemented on AWS)
 # JupyterLab
 # Superset (implemented on GCP)
 # 'Add computational power to your jobs by deploying elastic Spark cluster on  memory, storage, compute, GPU optimized instances' -> 'Add computational power to your jobs by deploying (Standalone Apache Spark cluster/EMR (AWS)/Dataproc (GCP) on memory, storage, compute, GPU optimized instances'

 

 ",Front-end pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-03 12:32:25,0
13356216,[Project]: Apply vertical central alignment for grid values,Apply vertical central alignment in project grid.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2021-02-02 14:42:30,5
13356214,Get rid of bottom hint for for Safari browser,"Safari Version 14.0.2 (15610.3.7.1.10, 15610)

Ged rid of bottom hint for Safari browser:
 * Bucket Browser
 * Roles
 * List of Resources
 * Projects
 * Audit [after cutting and adding hint for to long name in description (bucket browsers/roles/group) ticket DATALAB-2274]

----
(i) Verified 04/02/2021

The bottom hint still shows up for tag. Thus it's fixed only for Bucket Browser.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-02 14:29:06,5
13356213,[Audit]: Cut value and add hint for too long name,"[Audit]: In description cut name and add hint for too long name:
 * Upload/delete/download file to Bucket browser
 * Update group - add/delete user",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-02 14:28:13,0
13356204,[Roles]: Add a hint for user_tag in 'Users' column,[Roles page]: If hover the mouse on user tag in 'Users' column a hint which contains a user name should show up.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-02 13:48:19,0
13356186,[Configuration]: Adjust confirmation message according to selected service(s),"# [Configuration page]: Check box should be cleared up after self service restarting
 # Adjust services/service in confirmation message depending if one or more than one service is selected",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-02-02 11:37:42,5
13355375,[Front-end][DATALAB-2091]: Get rid of pip2 package for Notebook,"Work  in branch 'DATALAB-2091'.

Change information for 'Others' group:

'Other group can include libs from Python 2 and Python 3 group' -> 'Other group can include libs from Python 2 and Python 3 groups. Some libs cannot be installed due to  Python 2 is no longer supported.'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-29 08:01:13,0
13355187,[DATALAB-2091]: Update Ungit,"Ungit versions 1.4.36 -> 1.5.15

[Ungit releases|https://github.com/FredrikNoren/ungit/releases]

(i) It should be updated in branch 'DATALAB-2091'.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-28 13:15:36,2
13354935,[DATALAB-2091][Back-end]: Update third-party software to new stable versions,"# Dropwizard versions 1.3.2 ->  1.3.21.
 [Github issues|https://github.com/apache/incubator-datalab/issues/1037]
 # Guava versions 24.1-jre -> 230.0-jre.
 [Github issues|https://github.com/apache/incubator-datalab/issues/1038]
 # Nodejs
 It's connected with ticket DATALAB-1365
 # Hibernate-validator versions 5.1.1.Final -> 5.1.3.FINAL
 [Github issues|https://github.com/apache/incubator-datalab/issues/1039]

(i) It should be updated in branch 'DATALAB-2091'.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:56:13,1
13354934,[DATALAB-2091]: Update EMR,"EMR versions 5.12.0(spark v.2.3.2)/6.0.0(spark v.2.4.4->v.6.2.0(spark v.3.0.1)/ 6.0.0?/5.12.0?

[EMR releases|https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html]

It should be updated in branch 'DATALAB-2091'.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:50:50,2
13354931,[DATALAB-2091][DevOps]: Update third-party software to new stable versions,"# MXNET versions .1.6.0.post0 -> 1.7.0. {color:#57d9a3}*done*{color}
 [MXNET releases|https://github.com/apache/incubator-mxnet/releases]
 # Keras versions 2.1.6 -> 2.4.0/2.3.1. {color:#57d9a3}*2.4.0*{color}
 [Keras releases|https://github.com/keras-team/keras/releases]
 # R versions 3.4.4??? -> R 4.0.3 ? {color:#57d9a3}*4.1.0*{color}
 [R releases|https://cran.r-project.org/bin/windows/base/old/]
 # Pip version 20.1 -> 21.0.1 *{color:#57d9a3}done{color}*

(i) It should be updated in branch 'DATALAB-2091'.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:38:50,2
13354927,[DATALAB-2091]: Update Spark Standalone cluster,"Spark Standalone cluster versions 1.4->2.2.0.

[Spark Standalone cluster releases|https://spark.apache.org/news/spark-2-3-1-released.html]

It should be updated in branch 'DATALAB-2091'.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:26:31,3
13354923,[DATALAB-2091]: Update Dataproc,"Dataproc versions 1.4->2.2.0(ubuntu). *{color:#57d9a3}2.0.0 already in 2091{color}*

[Dataproc releases|https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.0]

It should be updated in branch 'DATALAB-2091'.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:21:28,2
13354920,[DATALAB-2091]: Update Zeppelin,"Zeppelin versions 0.8.0->0.9.0.

[Apache Zeppelin releases|https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12341035&projectId=12316221]

It should be updated in branch 'DATALAB-2091'.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:12:22,2
13354919,[DATALAB-2091]: Update RStudio,"RStudio versions 1.2.5033 ->1.4.1103-4/1.3.1093-1.

[RStudio releases|https://support.rstudio.com/hc/en-us/articles/200716783-RStudio-Release-History]

It should be updated in branch 'DATALAB-2091'.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:10:13,2
13354917,[DATALAB-2091]: Update TensorFlow,"TensorFlow versions 2.1.0 -> 2.4.1/2.3.2?

[TensorFlow releases|https://github.com/tensorflow/tensorflow/releases]

It should be updated in branch DATALAB-2091",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-27 12:00:37,2
13354914,[DATALAB-2091]: Update Jupyter,"Jupyter versions 6.0.2 -> 6.1.6

[Jupyter releases|http://example.com/]

It should be updated in branch 'DATALAB-2091'.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-27 11:59:24,2
13354477,Cut too long user name and show hint for it,"Cut too long user name and show hint for it in:

1. For grid values in user column

2. For point of user dropdown list

3. [List of resource]: Align question icon by vertical.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2021-01-25 15:43:22,0
13354460,[Back-end]: Show GPU Dataproc parameters after Data Engine Service creation,Show user if Dataproc was created with GPU and what kind of parameters were used.,Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2021-01-25 14:57:10,1
13354456,[Front-end]: Show GPU Dataproc parameters after Data Engine Service creation,Show user if Dataproc was created with GPU and what kind of parameters were used.,Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-25 14:51:12,5
13354305,Cut space in the beginning/end of library name during installation,"If Library name contains space the docker runs with '1' and still remains 'installing' status for the library on the DataLab Web UI.

So, cut space  in the beginning/end of library name during installation.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-25 08:26:15,5
13354301,Status lib sticks in 'installing' if docker executes with '1',"If docker runs with 1 and depending on which stage it's failed the library status stick in 'Installing' for Web DataLab. Response file is not generated.

For example if install library with space.

We consider from UI side cut spaces in the beginning/end

But docker can fail for another symbol.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-25 08:09:10,2
13353984,Investigate if we need to update some java dependencies,"1. [https://github.com/apache/incubator-datalab/issues/1039]

2. [https://github.com/apache/incubator-datalab/issues/1038]

3. [https://github.com/apache/incubator-datalab/issues/1037]

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-01-22 15:19:47,1
13353742,[Azure]: Delete/upload a file should not relocate user ,"*Preconditions:*
 # User is located in bucket browser
 # There is a file 'Start' in bucket root

*Steps to reproduce:*
 # Create a folder 'New' in bucket root
 # Go to folder 'New' 
 # Upload the file 'Start' in the folder 'New'

*Actual result:*
 # User is relocated in bucket root

*Expected result:*
 # User is located in current folder 'New'",AZURE Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-21 12:34:24,5
13353675,Edge creation fails on stage of luarocks installation,"*Preconditions:*
1. SSN is created

*Steps to reproduce:*
 # Create an Edge node

*Actual result:*
 # Edge creation fails

*Expected result:*
 # Edge creation is successful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2021-01-21 08:35:18,2
13353504,[Front-end]: Support library installation if name contains square brackets,"Example of python library snowflake-connector-python[pandas].

Support possibility to add library even if this library is not found in auto-search.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-20 10:38:30,5
13353269,[GCP]: Fix issue with Superset authentication,"*Preconditions:*
 # Superset is created

*Steps to reproduce:*
 # Go to Superset UI

*Actual result:*
 # User is not successfully logged in Superset UI

*Expected result:*
 # User is successfully logged in Superset UI",Debian DevOps GCP Known_issues(release2.4),['DataLab Main'],DATALAB,Bug,Minor,2021-01-19 11:50:44,3
13353264,[DevOps][GCP]: Add possibility of Jupyter creation with GPU,"'Add GPU' Checkbox should be in 'Create analytical tool' popup only for Jupyter on GCP

Add new parameters for Jupyter  in jupyter_description.json: Master/Slave GPU type/count.

Show which GPU parameters have been used for Jupyter creation:
 - GPU tag in Tags  column [List of resources]
 - count/type in Size column [List of resources]",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2021-01-19 11:33:38,3
13353263,[Back-end][GCP]: Add possibility of Jupyter creation with GPU,"1. 'GPU' checkbox should be in 'Create analytical tool' popup only for Jupyter on GCP.

Add new parameters for Jupyter:
 * GPU type - dropdown list
 * GPU сount - text box Should be special connection between input - ask [~mykolabodnar]

Show which GPU parameters have been used for Jupyter creation:
 - GPU tag in Tags  column [List of resources]
 - count/type in Size column [List of resources]

----
2. Environment Management 

Show  GPU type for GPU instance on 'Environment Management' page.",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2021-01-19 11:32:12,1
13353262,[Front-end][GCP]: Add possibility of Jupyter creation with GPU,"'Add GPU' checkbox should be in 'Create analytical tool' popup only for Jupyter on GCP.

Add new parameters for Jupyter:
 * GPU type - dropdown list
 * GPU сount - text box Should be special connection between input - ask [~mykolabodnar]

Show which GPU parameters have been used for Jupyter creation:
 - GPU tag in Tags  column [List of resources]
 - count/type in Size column [List of resources]",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-19 11:31:24,5
13353240,[Back-end][Audit]: Support possibility of export log,Implement export log for Audit page as it's implemented  in Billing page.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2021-01-19 10:17:12,1
13353239,[Front-end][Audit]: Support possibility of export log,Implement export log for Audit page as it's implemented  in Billing page.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2021-01-19 10:16:38,0
13353227,[DevOps]: Support library installation if name contains square brackets,Example of python library snowflake-connector-python[pandas].,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-01-19 08:47:07,2
13353106,[Bucket browser]: Copy path to bucket browser/file with cloud protocol as well,"Protocol depends on cloud:
 # GCP - gs://BUCKET_NAME
 # AWS - s3a://BUCKET_NAME
 # Azure - wasbs://container_name@account_name.blob.core.windows.net",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2021-01-18 15:51:05,5
13353099,[DevOps]: Show Apache Spark standalone cluster version on DataLab Web UI,"# Show Data Engine version in Select cluster type dropdown value if user wants to create a Data Engine:

Apache Spark standalone cluster x.x.x.
 
2.  Show Data Engine version on Data Engine name popup
Cluster version: x.x.x",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2021-01-18 15:24:19,2
13353097,[Back-end]: Show Apache Spark standalone cluster version on DataLab Web UI,"# Show Data Engine version in Select cluster type dropdown value if user wants to create a Data Engine:

Apache Spark standalone cluster x.x.x.
 
2.  Show Data Engine version on Data Engine name popup
Cluster version: x.x.x",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2021-01-18 15:22:54,1
13353095,[Front-end]: Show Apache Spark standalone cluster version on DataLab Web UI,"# Show Data Engine version in Select cluster type dropdown value if user wants to create a Data Engine:

Apache Spark standalone cluster x.x.x.
 
2.  Show Data Engine version on Data Engine name popup
Cluster version: x.x.x",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2021-01-18 15:14:55,0
13353059,[List of resources]: Investigate how to show all content menu,"If window is shrank the context menu of notebook goes up and not all content is visible.

Could you add scrollbar or implement something else in order to see all content menu. ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-01-18 11:18:25,5
13353054,Get rid of extra confirmation dialog if edge node does not have any related resources,If edge node does not have any related resources in the project during termination do not show the second confirmation dialog.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-01-18 11:05:45,5
13352447,Ged rid of error message if self-service is in starting stage,If Self-service starts  a restart process do not convey error message that restart fails because it is in the process which are not finished yet.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-15 09:51:57,5
13352259,[Project page]: Header should be sticky during scroll down,"1. [Project page]: Header should be sticky during scroll down

2. [Environment Management]: In the last row add horizontal line.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-14 13:39:50,5
13352253,[Investigation] Check whether we need to rewrite Playbook AIRBNB for using GPU,"We have playbooks AIRBNB:
 * preparation
 * visualisation

We run this playbook for Jupyter Notebook or clusters.

Running the code uses CPU.

We want also to have a playbook which uses GPU.

Please, rewrite the playbook.",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2021-01-14 13:00:20,3
13352245,[Front-end][GCP]: Add new parameters for Dataproc,"'Add GPU' checkbox should be in 'Add compute' dialog form

Add new parameters for Dataproc: 
 * Master GPU type - dropdown list
 * Slave GPU type - dropdown list
 * Master GPU сount - text box
 * Slave GPU сount - text box Should be special connection between input - ask [~mykolabodnar]

 ",Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Major,2021-01-14 12:22:50,5
13352240,[Back-end][GCP]: Add new parameters for Dataproc,"'Add GPU' Checkbox should be in 'Add compute' dialog form

Add new parameters For Dataproc:
 * Master GPU type - dropdown list
 * Slave GPU type - dropdown list
 * Master GPU сount - text box
 * Slave GPU сount - text box Should be special connection between input - ask [~mykolabodnar]

 ",Back-end Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-14 12:03:36,1
13352169,[Confirmation dialog]: Ged rid of extra scrollbar for Edge node,"# If project contains more than one endpoint and only one endpoint is active the extra scrollbar appears during stopping/termination Edge node. So, get rid of this scrollbar.
 # If it's only one edge node in confirmation dialog ged rod of checkbox option.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-14 07:43:05,5
13351962,[Front-end]: Multiple endpoints support for Configuration page,"Configuration page covers only if endpoint is local.

However Configuration page should include:
 * external and local endpoints (more than one endpoints).
 * DataLab deployment on GKE.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-13 09:56:56,5
13351961,[Back-end]: Multiple endpoints support for Configuration page,"Configuration page covers only if endpoint is local.

However Configuration page should include:
 * external and local endpoints (more than one endpoints).
 * DataLab deployment on GKE.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2021-01-13 09:55:39,1
13351770,Library from R package is not installed on Dataproc,"*Preconditions:*
 # Dataproc is created on Zeppelin/Jupyter

*Steps to reproduce:*
 # Install valid library <csvread> from r package on Dataproc

*Actual result:*
 # Library is not installed:

Error in loadNamespace(name) : there is no package called ‘devtools’

*Expected result:*
 # Library is installed

----
(i) I do not know about RStudio. Because i am not able to create a Dataproc on RStudio: https://issues.apache.org/jira/browse/DATALAB-2113",Debian DevOps GCP pull-request-available,[],DATALAB,Bug,Major,2021-01-12 12:32:21,2
13351745,[GCP][Spark Standalone cluster]: Python 2.7.17 is used instead of Python 3.6,"*Preconditions:*
 # Spark Standalone cluster is created on Jupyter

*Steps to reproduce:*
 # Go to Jupyter UI
 # See kernel list
 # Run a command 

{code:java}
import platform
platform.python_version()
{code}
*Actual result:*
 # Python 3.6.x is in kernel list for Spark Standalone cluster
 # Python 2.7.17 is used for Spark Standalone cluster

*Expected result:*
 # Python 3.6.x is in kernel list for Spark Standalone cluster
 # Python 3.6.x is used for Spark Standalone cluster

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-01-12 11:28:21,3
13351744,Dataproc creation fails on RStudio,"*Preconditions:*
 # RStudio is created

*Steps to reproduce:*
 # Create a Dataproc

*Actual result:*
 # Dataproc creation fails

*Expected result:*
 # Dataproc is created successfully",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-01-12 11:25:57,2
13351698,It's impossible to install Java dependency,"*Preconditions:*
 # Manage library page is open

*Steps to reproduce:*
 # Go to pip2/pip3/apt-yum/others
 # Autocomplete is in loading process
 # Go to java group
 # Typa valid dependency <io.dropwizard.metrics:metrics-core:4.1.17>

*Actual result:*
 # It's impossible to add selected dependency

*Expected result:*
 # It's possible to add selected dependency

----
2. It's impossible to scroll down in drop down list",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-01-12 09:20:16,5
13351689,[GCP][Dataproc]Python 2.7.16 is used instead of Python 3.7,"*Preconditions:*
 # Dataproc is created on Jupyter

*Steps to reproduce:*
 # Go to Jupyter UI
 # See kernel list
 # Run a command 

{code:java}
import platform
platform.python_version()
{code}

*Actual result:*
 # Python 3.7.x is in kernel list for Dataproc
 # Python 2.7.16 is used for Dataproc

*Expected result:*
 # Python 3.7.x is in kernel list for Dataproc
 # Python 3.7 is used for Dataproc

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-01-12 09:06:29,3
13351686,DeepLearning creation fails on stage of CNTK instalation,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create a DeepLearning

*Actual result:*
 # DeepLearning creation fails

*Expected result:*
 # DeepLearning is created successfully

 ",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-01-12 08:45:57,2
13351492,[GKE]: DeepLearning/Jupyter with TensorFlow creation fails,"*Preconditions:*
 # DataLab is deployed on GKE
 # Endpoint is deployed onGCP

*Steps to reproduce:*
 # Create DeepLearning/Jupyter with TensorFlow

*Actual result:*
 # DeepLearning/Jupyter with TensorFlow creation fails

*Expected result:*
 # DeepLearning/Jupyter with TensorFlow creation is successful",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2021-01-11 14:41:25,3
13351171,Extra row should not be added every time after file.yml saving via DataLab Web UI,"In the top of billing.yml/self-service.yml/provisionong.yml extra row is added every time after saving changes, but it should not be added.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2021-01-08 22:16:20,1
13351068,Update DataLab deployment/development in Confluence,"Update documentation about DataLab Deployment and Development:

- [Deployment|https://cwiki.apache.org/confluence/display/DATALAB/Apache+DataLab+Deployment]

- [Development|https://cwiki.apache.org/confluence/display/DATALAB/Apache+DataLab+Development]",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-08 11:24:55,5
13351028,[Default lists] & [Roles&Permissions]: Behaviour for some roles should be changed after endpoint adding,"1. After adding endpoint all items should be checked off for:
 * Compute
 * Notebook
 * Compute Shapes
 * Notebook Shapes

2. Selecting for Compute/Notebook/Compute Shapes/Notebook Shapes does not work for user who is assigned to a group. For example for user1 it's allowed to create only one Notebook and one shape, but user1 is able to create all notebooks and shapes.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-08 07:35:11,1
13351025,[AWS][azure]Spring/# Specify are missed for billing.yml in Configuration page,"*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Go to Billing.yml on Configuration page
 # Look at the top of Billing.yml

*Actual result:*
 # 'spring:' is absent for Billing.yml for Azure
 # # Specify is absent for Billing.yml for AWS

*Expected result:*
 # 'spring:' is present for Billing.yml for Azure
 # # Specify is present for Billing.yml for AWS",AWS AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2021-01-08 07:20:15,1
13349210,[Roles]: It's impossible to scroll down for Chrome/Safari browsers,"*Preconditions:*
 # User1 is located on Roles page
 # Role1 contains a lot of users

*Steps to reproduce:*
 # Scroll down in user column for Role1

*Actual result:*
 # it's impossible to scroll down

*Expected result:*
 # It's possible to scroll down

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2021-01-05 14:13:48,5
13349164,[Front-end]: Support password change,"Implement password change in Configuration page.

In the first stage we consider mongo pasword.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-05 10:49:25,0
13349163,[Back-end]: Support password change,"Implement password change in Configuration page.

In the first stage we consider mongo pasword.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-05 10:47:54,1
13349160,Store password in native cloud storage,"All passwords which are in file.yml store in native cloud storage.

*1. Billing*
- mongo user
- mongo password

server
- server.ssl.key-store-password

keycloak
- resource
- credentials.secret

*2. Self service*
keycloak
- keycloak resource
- keycloak credentials.secret

*3. Provisioning*
ldap
- user
- password

stepCerts
- rootCA
- kid
- kidPassword

keycloak
- resource
- credentials.secret
- user
- password",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2021-01-05 10:39:24,3
13349116,Error message is not shown during project stopping/termination,"Convey error message received from Back-end during project stopping/termination if previous process is not finished.
----
*Preconditions:*
 # Notebook1 is in creating status
 # Notebook1 is in project1

*Steps to reproduce:*
 # Stop/terminate project1

*Actual result:*
 # Stopping/termination is not allowed
 # Appears error message: Oops! t is undefined

*Expected result:*
 # Stopping/termination is not allowed
 # Appears error message: Can not stop(terminate) environment because one of project resource is in processing stage",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2021-01-05 08:03:13,5
13349070,[Confirmation dialog]: Clear up all previous selected items after canceling,"Confirmation dialog we have for :
 * environment management page
 * configuration page
 * project page

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2021-01-05 07:36:28,5
13348965,[GKE]: Project creation fails,"*Preconditions:*
 # DataLab is deployed on GKE
 # Endpoint is created on GCP

*Steps to reproduce:*
 # Create a project

*Actual result:*
 # Project creation fails

*Expected result:*
 # Project is created successfully

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2021-01-04 15:55:15,3
13348914,Improvements for Configuration page,"# In report change the order for sub-menu. 'Adit' should be the first and then 'Billing'. (/)
 # Configuration: If any points are not selected the 'Restart' button should not be active. And vice versa if at least one point is selected the 'Restart' button should be active. (/)
 # The 'Save' and 'Discard' buttons are not used for 'Main' tab. So,  ged rid of the 'Save' and 'Discard' buttons  on 'Main' tab. (/)
 #  Only true admin can manage the configuration page. So do not show configuration page for project admin. (/)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2021-01-04 10:48:26,5
13348309,Helm charts deployment fails on stage to access to chart repository,"*Preconditions:*

1. All steps are done before running script to create GKE cluster and deploy Helm Charts are done (described in https://issues.apache.org/jira/secure/RapidBoard.jspa?rapidView=307&projectKey=DLAB&quickFilter=1520&quickFilter=1619)

*Steps to reproduce:*

1. Run python script to create GKE cluster and deploy Helm Charts:

*Actual result:*
 # Script fails:

{code:java}
Error: error initializing local helm home: Looks like ""https://kubernetes-charts.storage.googleapis.com"" is not a valid chart repository or cannot be reached: Failed to fetch https://kubernetes-charts.storage.googleapis.com/index.yaml : 403 Forbidden{code}
*Expected result:*
 # Script runs successfully",Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-12-29 13:48:13,3
13348147,It's impossible to save changes in provisioning.yml/self-service.yml/billing.yml,"*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Go to 'Configuration' page
 # Edit provisioning.yml
 # Click on 'Save' button
 # Click on 'Yes' in confirmation dialog

*Actual result:*
 # Changes are not saved
 # Error 500 appears

*Expected result:*
 # Changes are saved
 # Error 500 does not appear

----
(i) The same case is if edit self-service.yml or billing.yml.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-12-28 15:49:09,1
13348145,[AWS][Azure]: Get rid of extra asterisks,"[Configuration]: There are extra asterisks in billing.yml (see attachment).

Please get rid of them.
h4.  ",AWS AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-12-28 15:32:22,1
13348125,Despite cancel a restart a service restarting triggers,"*Preconditions:*
 # User is located on 'Main' tab ('Configuration' page)

*Steps to reproduce:*
 # Check off any service
 # Click 'Restart' button
 # Click 'No' button or close confirmation dialog

*Actual result:*
 # Selected service is restarted

*Expected result:*
 # Selected service isn't restarted

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-12-28 13:19:39,5
13348112,Set of UI changes for all pages,"# [List of resources]: Some values overlap is scroll down (/)
 # [Menu]: Change the gear for administration page (/)
 # [Roles][Projects]: align arrows with text (/)
 # Sort sub-menu items{color:#de350b} (/){color}",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-12-28 11:06:54,5
13348097,SSN deployment fails if FQDN and step-certificates are not indicated,"*Steps to reproduce:*
 # Deploy SSN and do not indicate FQDN/step-certificate

*Actual result:*
 # SSN deployment fails

*Expected result:*

1. SSN deployment is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-12-28 09:46:32,3
13348095,[GCP]: Project creation fails if DataLab is deployed in existing VPC/subnet,"*Preconditions:*
 # SSN is deployed in existing VPC/subnet

*Steps to reproduce:*
 # Create a project

*Actual result:*
 # Project creation fails

*Expected result:*
 # Project is created successfully

 ",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2020-12-28 09:26:33,3
13348090,[Azure]: Notebook creation fails on stage of devtools installation,"This bug was found for Juputer/DeepLearning perhaps it's related for another Notebooks.

*Preconditions:*
 # Project is created on AZURE

*Steps to reproduce:*
 # Create Jupyter/DeepLearning

*Actual result:*
 # Jupyter/DeepLearning creation fails:

 
{code:java}
Requested: R -e ""install.packages('devtools',repos='https://cloud.r-project.org')""
Executed: sudo -S -p 'sudo password:'  /bin/bash -l -c ""R -e \""install.packages('devtools',repos='https://cloud.r-project.org')\""""
{code}
 

*Expected result:*
 # Jupyter/DeepLearning creation is successful",AZURE Debian Known_issues(release2.5) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-12-28 09:01:58,2
13346789,[Back-end]: Augment a functionality for recreate edge node,"As user I want to use my instances in case if edge failing or again to create previously terminated edge in the same project using the same endpoint so that it allows me do not create project with new name and use the previous name.

If we terminate edge (or edge has been failed) we could not create the new edge in the same project and the same endpoint.

 
----
Edge not could be failed during stopping/starting/creating/terminating.

{color:#de350b}We should add retry during stopping/starting edge in case of failing (three attempts){color}

{color:#de350b}From which side it should be done (Back-end/DevOps)? - from Back-end should be done{color}

 One project can contain one endpoint/edge node or more.

*Statuses for recreate*:
 * edge node is terminated from Cloud Web Console - recreate should be

 * edge node is terminated from Web DataLab UI - recreate should be
 * edge node failed during stopping/starting - return cloud status - +recreate should NOT be+
 * edge node failed during creating - recreate should be 
 * edge node failed during terminating - recreate should be 

  If at least one instance exists - SMART recreate.

  If instances do not exist - create all resources.

Github issue: [https://github.com/apache/incubator-dlab/issues/731].

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-12-18 12:50:30,1
13346768,Obey consistency between 'List of resources' and 'Environment Management' pages,1. Implement header in 'List of resources' as it is implemented for 'Environment Management' page.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-12-18 11:06:16,5
13346341,[List of resources]: Header is broken if scroll down,"*Preconditions:*

1.  Notebooks are created

*Steps to reproduce:*
 # Go to 'List of resources' page
 # Scroll down

*Actual result:*
 # Header of Action is broken

*Expected result:*
 # Header of Action isn't broken

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-12-16 11:04:43,5
13345821,[Audit data]: Add instance_name and status in additional info,"In information show:
 * What resource has been changed
 * Previous status
 * Actual status

Update Notebook/Compute <insyance_name> status from <status> to <status>.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-12-14 10:09:32,1
13345808,Notebook creation fails on stage of 'IRkernel' installation,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create RStudio/Jupyter/Zeppelin/Rstudio with TensorFlow

*Actual result:*
 # RStudio/Jupyter/Zeppelin/Rstudio with TensorFlow creation fails with error:

Error: Failed to install 'IRkernel' from GitHub:
[datalab-user@172.31.16.2] out: (converted from warning) package ‘pbdZMQ’ is not available (for R version 3.4.4)

*Expected result:*
 # RStudio/Jupyter/Zeppelin/Rstudio with TensorFlow creation is successfully",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-12-14 09:32:26,2
13345508,Notebook creation fails on stage of 'devtools' installation,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create RStudio/Jupyter/Zeppelin/Rstudio with TensorFlow

*Actual result:*
 # RStudio/Jupyter/Zeppelin/Rstudio with TensorFlow creation fails

*Expected result:*
 # RStudio/Jupyter/Zeppelin/Rstudio with TensorFlow creation is successfully

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-12-11 12:09:17,2
13345487,[Infrastructure status checker]: Docker logs should be cleaned,"Delete docker logs which appear every 15 minutes.

For infrastructure status checker implement docker logs deletion as it is done for inactivity checker.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2020-12-11 10:46:54,2
13345483,[DevOps]: Handling failed status,"If status is failed and instance is absent in Cloud Web Console the status remains failed.

If status is failed and instance is present in Cloud Web Console the system has to try (3 attempts) to stop this instance and send notifications to user/admin.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-12-11 10:26:40,2
13345482,[Back-end]: Handling failed status,"If status is failed and instance is absent in Cloud Web Console the status remains failed.

If status is failed and instance is present in Cloud Web Console the system has to try (3 attempts) to stop this instance and send notifications to user/admin.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-12-11 10:24:36,1
13345300,[Front-end]: Notify a user about instance status in case of unsuccessful result,"_+All these notification have to be sent if specific action has failed and instance remains in Cloud Web Console.+_
----
+*Only for user*+

*Notify1 (instance action failed, but instance is stopped):*

DataLab failed to stop/create/start/reconfigure/create an image of/ a notebook/compute <instance_name> in project <project_name>. Now it is stopped. Please contact DataLab administrator to fix this issue.

*Notify2* *(instance action failed, while instance is running)**:*

DataLab failed to stop/create/start/reconfigure/create an image of/ a notebook/compute <instance_name> in project <project_name>, while its status in Cloud Web Console is running. Please contact DataLab administrator to fix this issue.
----
----
+*Only for admin*+

*Notify1* *(instance action failed, but instance is stopped)**:*

DataLab failed to stop/create/start/reconfigure/create an image of/ a notebook/compute <instance_name> in project <project_name>. Now it is stopped. For more information please view logs.

*Notify2* *(instance action failed, while instance is running)**:* 

DataLab failed to stop/create/start/reconfigure/create an image of/ a notebook/compute <instance_name> in project <project_name>, while its status in Cloud Web Console is running. For more information please view logs.

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-12-10 13:42:15,0
13345061,[Audit]: The column width should not be extended,If any grid value is too long the column width should have fixed width and not  be changed.,pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-12-09 12:04:19,5
13345014,Notebook creation fails due to update some pip library,"*Preconditions:*
 # Project is created

*Steps to reproduce:*
 # Create any Notebook

*Actual result:*
 # Notebook creation fails with error:

module 'lib' has no attribute 'Cryptography_HAS_TLSEXT_HOSTNAME'

*Expected result:*
 # Notebook creation is successful",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Blocker,2020-12-09 08:28:25,2
13344783,Scheduler of infrastructure status checking triggers with 500 error,"*Preconditions:*
 # Data Engine/Dara Engine Service are created

*Steps to reproduce:*
 # Go to self-service/provisioning logs
 # Wait for  'check_Infrastructure_Status_Scheduler' triggering

*Actual result:*
 # Appear an error 'org.quartz.core.JobRunShell: Job DEFAULT.6da64b5bd2ee-639a2a13-ec98-40dc-9353-9a2e7be10571 threw an unhandled Exception:
! javax.ws.rs.InternalServerErrorException: HTTP 500 Internal Server Error'

*Expected result:*
 # Any error is not appeared",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-12-08 10:47:33,1
13344769,[AWS]: EMR status is not synced up with cloud 'terminated' status,"*Preconditions:*
 # EMR is in running status in DataLab Web UI and in cloud console

*Steps to reproduce:*
 # Terminate only EMR master from cloud console
 # Wait according to scheduler status checking

*Actual result:*
 # EMR is still in running status in DataLab Web UI

*Expected result:*
 # EMR is in terminated status in DataLab Web UI",AWS Beck Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-12-08 09:49:19,1
13344756,[GCP][Autotest]: Some library groups are absent for Notebook ,"*Preconditions:*
 # Rstudio/Jupyetr/Zeppelin/DeepLEarning is created
 # User is located on 'List of resources' page

*Steps to reproduce:*
 # Click on action menu for Notebook 
 # Choose  'Manage libraries'
 # Click on 'Select group' drop down list

*Actual result:*
 # For RStudio it's Apt/Yum, Py2, Py3, Others
 # For Zeppelin it's Apt/Yum, Py2, Py3, Others
 # For Jupyter it's Apt/Yum, Py2, Py3, Others
 # For DeepLearning it's Apt/Yum, Py2, Py3, Others

*Expected result:*
 # For RStudio it's Apt/Yum, Py2, Py3, R packages, Others
 # For Zeppelin it's Apt/Yum, Py2, Py3, R packages, Java, Others
 # For Jupyter it's Apt/Yum, Py2, Py3, R packages, Java, Others
 # For DeepLearning it's Apt/Yum, Py2, Py3, Java, Others

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-12-08 09:07:19,3
13344233,['Role' page]Notebook/Compute templates and shape instances are absent for local endpoint,"*Preconditions:*
 # Project1 is created
 # User1 is assigned to the Project1

*Steps to reproduce:*
 # Go to list of resource
 # Create any notebook

*Actual result:*
 # Select template is disabled

*Expected result:*
 # Select template is enabled

(!) Notebook/cluster templates shapes are absent",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-12-04 12:36:26,1
13344197,[Back-end]: Augment administration page,"Implement 'Configuration' page in section 'Administration'.

Configuration' page consists of:
 * provisioning yml configuration.
 * self-service yml configuration.
 * billing yml configuration.
 * possibility to restart services.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-12-04 09:12:40,1
13343560,[Azure]: SSN deployment fails,"*Steps to reproduce:*

1. Deploy SSN on Azure

*Actual result:*

1. SSN deployment fails

*Expected result:*

1. SSN deployment is successful

 ",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-12-01 13:11:49,2
13342770,[AWS][GCP]: SSN creation fails on stage of touch /opt/datalab/tmp/docker_daemon_ensured ,"*Steps to reproduce:*

1. Create SSN via Jenkins on AWS/GCP

*Actual result:*

1. SSN creation fails

*Expected result:*

1. SSN creation is successful",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-11-26 07:53:37,3
13342623,Notebook/compute values are absent for 'Billing report' page,"*Preconditions:*
 # Billing is available on DataLab UI

*Steps to reproduce:*

1. Go to 'Billing report' page

*Actual result:*
 # Values (user/project/type/status/size) for Data Engine/Data Engine Service/Notebook are absent

*Expected result:*
 # Values (user/project/type/status/size) for Data Engine/Data Engine Service/Notebook are present

(i) In Attachment 'Example1' you can view what values should be.

 ",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-11-25 14:47:29,1
13342296,SSN deployment fails on step if  downloading qtconsole,"*Steps to reproduce:*

1. Deploy SSN via Jenkins job

*Actual result:*

1. SSN deployment fails

*Expected result:*

1. SSN deployment is successful
----
(!) the same error appears if terminate previous SSN.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-11-24 08:09:09,2
13342133,Disable gear icon if Notebook is in failed status,If Notebook is in Failed status the gear icon should be disabled.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-11-23 13:30:13,5
13341752,[Project page]: Adjust  group_tag according to the tags of the other pages,"# Add hint for group_tag if hover the mouse over.
 # Cut too long value in group_tag",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-11-20 09:43:44,5
13341601,Gear icon is disabled if Notebook contains Data Engine,"*Preconditions:*
 # Data Engine is created for Notebook1
 # Notebook1 is in running or stopped status

*Steps to reproduce:*
 # Go to 'Environment management page'

*Actual result:*

1. Gear icon is disabled for Notebook1

*Expected result:*

1. Gear icon is enabled for Notebook1",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-11-19 14:40:59,5
13341595,Do not show instance which is already terminated in confirmation dialog,"*Preconditions:*
 # Data Engine Service is terminated for Notebook1
 # Notebook1 is running
 # User is located on 'List of Resource' page

*Steps to reproduce:*

1. Stop notebook

*Actual result:*

1. It is pointed that 'Further Status' of DES will be  'Terminated' in confirmation dialog

*Expected result:*

1. DES is not mentioned in confirmation dialog

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-11-19 14:27:11,5
13341591,[Git Credentials]: Obey consistency for all grid values,"# Align host name value from the left side.
 # All grid values should have the same font, however host name values and icons are bold.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-11-19 14:15:23,5
13341534,Quota update fails,"*Preconditions:*

1. Quota is set up

*Steps to reproduce:*

1. Update any quota

*Actual result:*

1. 'Apply' button is disabled

*Expected result:*
 # 'Apply' button is enabled
 # Console error appears 'Cannot read property 'value' of undefined'
 # Console error appears 'Cannot read property 'validator' of null'",pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-11-19 09:03:20,5
13340470,[Front-end]: Augment administration page,"Implement 'Configuration' page in section 'Administration'.

Configuration' page consists of:
 * provisioning yml configuration.
 * self-service yml configuration.
 * billing yml configuration.
 * possibility to restart services.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-11-13 09:23:15,5
13340138,EMR creation fails,EMR creation fails on AWS,pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-11-12 08:00:16,2
13339494,DataLab instance status should be synced up with cloud status (Data Engine Service),"If change instance status from cloud console the DataLab instance status is not updated:
 # Edge node
 # Notebook
 # Cluster (master)

Please investigate the reason",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Critical,2020-11-09 10:45:22,1
13339222,Calendar is not visible only for Safari browser,"*Preconditions:*
 # SNN is created
 # User is logged in DataLab via Safari browser

*Steps to reproduce:*
 # Go to 'Billing report' or 'Audit' page
 # Click on Calendar

*Actual result:*
 # Calendar is not visible

*Expected result:*
 # Calendar is visible

----
2. Calendar does not filter during this year.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-11-06 18:14:53,5
13339218,SSN creation fails on step of Keycloack connection,"This bug was found on GCP. Perhaps it is related for all clouds.

*Steps to reproduce:*
 #  Create SSN

*Actual result:*
 # SSN creation fails

*Expected result:*
 # SSN creation is successful

----
On top of that I could not login Keycloak Web UI.",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-11-06 17:46:02,2
13339189,[Library management]: Get rid of console error during filter usage,"*Preconditions:*
1. Notebook is created
*Steps to reproduce:*
1. Click on filter grid
*Actual result:*
1. Any action is blocked on DataLab Web UI
2. The error appears in console (view attachment)
*Expected result:*
1. Any action is not blocked on DataLab Web UI
2. The error does not appear in console ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-11-06 14:29:54,5
13339165,[Audit][Billing]: Adjust calendar filter between selecting days and appropriate period,"If user selects 'this year'/'last year'/'last week' and so forth and then selects dates the previous label is still selected, but should not.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-11-06 12:07:01,5
13339154,Tag value is beyond tag boarder,"1. This bug is related to notebook_custom_tag/user_name_tag/group_name_tag
*Preconditions:*
1. Project is created
*Steps to reproduce:*
1. Create any notebook with too long custom_tag
Actual result:
1. Tag value is beyond tag boarder
Expected result:
1.  Tag value is with tag boarder

----
2. Limit group name. For example up to 40 or 50 symbols and add error message if user tries to enter more than 40 or 50 symbols
",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-11-06 10:23:11,5
13339148,[Git credentials]: DataLab UI sticks after typing more than 29 symbols in 'Host name' text box,"*Preconditions:*
1. SSN is created
*Steps to reproduce:*
1. Click on 'Git credentials' button
2. Enter more than 29 symbols in 'Host name' text box
*Actual result:*
1. DataLAb Web UI sticks
2. It's impossible to do any Web UI action
*Expected result:*
1. DataLAb Web UI does not stick
2. It's possible to do any Web UI action",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-11-06 09:47:50,5
13339138,[Azure][DeepLearning]: Sometimes DeepLearning creation fails on stage of nvidia installation,"*Preconditions:*
1. Project is created on Azure
*Steps to reproduce:*
1. Create DeepLearning
*Actual result:*
1. Deeplearning creation fails
*Expected result:*
1. Deeplearning creation is successful",AZURE Debian DevOps Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Major,2020-11-06 09:24:58,2
13338851,[GCP]: Superset creation fails due to 'argparse' is not defined,"*Preconditions:*
 # Project is created on GCP

*Steps to reproduce:*
 # Create Superset

*Actual result:*
 # Superset creation fails

*Expected result:*
 # Superset is created successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-11-04 15:09:21,2
13338621,Quota value change should be validated between total and project budgets,"# If project quota is more than total one validation triggers with delay (after typing the second bigger value)
 # If project quota is more than total one the project quota reducing does not validate with new project value",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-11-03 11:15:20,5
13338416,Role values change their position during update,"*Preconditions:*
 # Several groups are created
 # User is located on 'Roles' page

*Steps to reproduce:*
 # Add new user to existing group
 # Confirm change

*Actual result:*

1. Role values change their position

*Expected result:*

1. Role values does not change their position

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-11-02 12:32:14,5
13338396,[Resources list]: Adjust sensitive space for endpoint_tag during mouse clicking,"*Preconditions:*
 # Notebook is created

*Steps to reproduce:*
 # Go to 'List of resource' page
 # Click nearby user_tag

*Actual result:*
 # Endpoint_tag is selected

*Expected result:*
 # None of the tags is selected

----
 The same situation with tags we have for 'Project' and 'Roles' pages.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2020-11-02 11:22:23,5
13338383,Obey consistency in error messages,"# Every error message should contain dot in the end of sentence.
 # Use the same alignment for all error messages if it's possible.
 # Audit: add hint for too long value for action menu.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-11-02 09:42:00,5
13337694,Set of minor tasks for Bucket Browser and Audit,"# (/) Bucket browser: Align hint by left side for file name
 # (/) Bucket browser: three dots should not overlap symbol '>' in breadcrumb
 # (/) Audit: Obey consistency between grid value and dropdown value ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-10-28 15:45:21,5
13337689,[Audit]: Show in which projects actions are performed,"Project value is 'N/A' for some actions, but should be pointed in which project changes are done for:
 * Data management via bucket browser (only for project bucket)
 * following notebook links",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-10-28 15:21:24,1
13337590,[GCP]: Jupyterlab creation fails on stage of connection configuration,"*Preconditions:*
 # Project is created on GCP

*Steps to reproduce:*
 # Create Jupyterlab

*Actual result:*
 # Jupyterlab creation fails

*Expected result:*
 # Jupyterlab creation is successful

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-10-28 07:35:20,2
13337444,Do not terminate instance which have been terminated,"One endpoint can have several edge nodes in different projects.

So if you disconnect endpoint the docker triggers even for terminated edge node (status edge node changes from terminated to terminating and then  to terminated.) But status should not change for terminated edge node during endpoint disconnection.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-10-27 14:38:58,1
13337228,[Bucket browser]: Preloader should have a vertical alignment,"Bucket browser preloader does not have a vertical alignment for Firefox browser, but should.

Firefox v.82.0 (64-bit).",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-10-26 14:34:44,5
13337216,[Audit]: Value of dropdown list is overlapped by footer,"This bug reproduced for Safari/Firefox browsers.

*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Go to 'Audit' page
 # Filter by any value

*Actual result:*

1. Value of dropdown list is overlapped by footer

*Expected result:*

1. Value of dropdown list is not overlapped by footer",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-26 13:15:42,5
13337192,Confirmation buttons are absent for 'Role' updating,"1.

*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Go to 'Role' page
 # Add a new user
 # Save update

*Actual result:*
 # 'Yes'/'Now' buttons are absent

*Expected result:*
 # 'Yes'/'Now' buttons are present

----
2. Do not allow to add empty user. 
----
3. Accustom width line of adding user according to the other.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-10-26 11:55:26,5
13337185,[AWS][Zeppelin][RStudio & Jupyter with TensorFlow]: Notebook creation fails on stage of NumPy installation,"*Preconditions:*
 # Project is created on AWS

*Steps to reproduce:*
 # Create Zeppelin/RStudio with TensorFlow/Jupyter with TensorFlow

*Actual result:*
 # Notebook creation fails

*Expected result:*
 # Notebook creation is successful

 ",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-10-26 10:45:12,2
13337181,[GCP]: SSN termination should delete VPC as well,"*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Delete SSN via Jenkins job

*Actual result:*
 # SSN is terminated successful
 # SSN_VPC is still available

*Expected result:*
 # SSN is terminated successful
 # SSN_VPC is absent

 The same issue https://issues.apache.org/jira/browse/DATALAB-2406",Debian DevOps GCP Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2020-10-26 10:29:59,2
13337170,Update angular,"Current version of angular is 8.2.

Please update it up to 10.2.x",AWS AZURE Debian F# Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-10-26 09:26:50,5
13337167,Header is not uploaded for 'Billing report' page,"*Preconditions:*
 # SSN is created

*Steps to reproduce:*
 # Go to 'Billing report' page

*Actual result:*
 # Header is absent on 'Billing report' page

*Expected result:*
 # Header is present on 'Billing report' page

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-26 09:18:01,5
13337165,Add horizontal scrollbar on 'Environment management' page,# if monitor is smaller add horizontal scroll bar on 'Environment management' page (As it's implemented for 'Billing report' page). ,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Trivial,2020-10-26 09:09:12,5
13336934,Add extra validation for existing nat route during termination,"This case was on GCP, perhaps it's related to the other clouds.

*Case:*
 # SSN is created on GCP
 # External endpoint1 is created on GCP
 # Project1 is created on external endpoint1
 # Project2 is created on external endpoint1

*Steps:*
 # Delete Project 1
 # Disconnect external endpoint1

As a result edge node status for project1 changes from terminated to failed",Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-10-23 14:21:40,2
13336932,JupyterLab creation fails due to 'argparse' is not defined,"This bug was found on AWS, perhaps it's related to GCP. Please ponder about it.

*Preconditions:*
 1. Project is created on AWS
 *Steps to reproduce:*
 1. Create JypyterLab
 *Actual result:*
 1. JupyterLab creation fails
 *Expected result:*
 1. JupyterLab creation is successful",AWS Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-10-23 13:59:39,2
13336472,Replace logo and website link according to new name,"1.  Add DataLab logo in [https://whimsy.apache.org/pods/project/datalab]
 2. Replace from [http://datalab.apache.org|http://datalab.apache.org/] to [http://datalab.incubator.apache.org|http://datalab.incubator.apache.org/] in [https://whimsy.apache.org/pods/project/datalab], in our channels (github, youtube, twitter)

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-10-21 12:56:59,5
13336308,[GCP]: Value of master instance number is '0' for Dataproc,"*Preconditions:*

1. Any Notebook (Jupyter/Rstudio/Zeppelin) is created on GCP

*Steps to reproduce:*
 # Create Dataproc
 # Click Dataproc name popup

*Actual result:*

1. Value of master instance number is '0'

*Expected result:*

1. Value of master instance number is '1'",Debian Front-end GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-10-20 14:30:32,5
13336306,[GCP]:Dataproc creation fails on RStudio/Apache Zeppelin,"*Preconditions:*
 # RStudio/Apache Zeppelin is created on GCP

*Steps to reproduce:*
 # Create Dataproc on RStudio

*Actual result:*
 # Dataproc creation fails

*Expected result:*
 # Dataproc creation is successful

----
This error comes up for RStudio creation on AWS (10/02/2021).",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-10-20 14:15:50,2
13336080,[GCP]: Not all resources are terminated after endpoint disconnection,"*Preconditions:*
 # Any notebook/compute is created on remote endpoint for GCP

*Steps to reproduce:*
 # Disconnect remote endpoint on GCP
 # Wait docker finish of  execution
 # Delete endpoint via Jenkins job

*Actual result:*
 # Endpoint deletion fails

*Expected result:*
 # Endpoint deletion is successful

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-19 14:14:14,2
13336026,['Billing report' page]: Confirm filter action is disabled if paste value,"*Preconditions:*
 # Billing is available for DataLab Web UI
 # User is located on 'Billing resource' page

*Steps to reproduce:*
 # Any value is copied in buffer
 # Paste this value in 'Resource name' text box

*Actual result:*
 # Confirm filter action is disabled

*Expected result:*
 # Confirm filter action is enabled

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-19 09:09:06,5
13335608,[GCP][Azure]:Total instance number is absent or incorrect for Compute,"*Preconditions:*

1. Notebook is created on Azure/GCP

*Steps to reproduce:*
 # Create spark standalone cluster
 # Click on compute popup

*Actual result:*
 # Total instance number value is absent for Azure
 # Total instance number value is less than actual for GCP

*Expected result:*
 # Total instance number value is present for Azure
 # Total instance number value is correct for GCP",AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-15 12:53:10,5
13335144,Add scrollbar for features for Chrome browser,Google Chrome version 86.0.4240.75 (Official Build) (64-bit),AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-10-13 06:46:27,5
13335030,[GCP][Demo-env]:Billing is absent only for instance/edge/compute,"*Preconditions:*

Demo-env is created for release 2.4 on GCP

*Step to reproduce:*

1. Go to billing report page

*Actual result:*

1. Billing is absent for edge/notebook/compute

*Expected result:*
 # Billing is Present for edge/notebook/compute

----
However billing is present in GCP console for September",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Major,2020-10-12 14:43:09,1
13334965,[GCP]: Compute configuration fails,"This bug was found on Jupyter for Dataproc (Data Engine Service) creation and for Zepeplin for Spark Standalone cluster. Perhaps it is related to the other notebook templates.

*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*

1. Create Dataproc for Jupyter

*Actual result:*

1. Dataproc creation fails on stage of configuration

*Expected result:*

1. Dataproc creation is successful

 

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-10-12 09:18:11,2
13334403,['Billing report' page]: Filter does not work by 'Resource name',"*Preconditions:*
 # Billing is available on DataLab Web UI

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Filter by resource name

*Actual result:*

1. Grid value is not filtered by 'Resource name'

*Expected result:*

1. Grid value is filtered by 'Resource name'",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-10-08 10:46:22,6
13334131,[GCP]: SSN deployment fails on stage of mongo configuration,"*Steps to reproduce:*

1. Deploy SSN via Jenkins job

*Actual result:*
 # SSN deployment fails

{code:java}
Requested: python /opt/datalab/tmp/configure_mongo.py --datalab_path /opt/datalab/ 
Executed: sudo -S -p 'sudo password:'  /bin/bash -l -c ""python /opt/datalab/tmp/configure_mongo.py --datalab_path /opt/datalab/ ""{code}
*Expected result:*

1. SSN deployment is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-10-07 08:51:15,2
13334115,Rewrite infrastructure deployment on Python3,"There was a case when project creation was failed.

And SSN termination failed due to searching non-created bucket and did not continue  to search the other buckets.

As e result the other buckets are still available on cloud console. 

Could you take into consideration this case during rewriting for Py3?",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-10-07 07:13:45,2
13334112,[Billing page]: Footer is not sticky during scrolling down,"This bug is reproduced only for Safari browser
 *Preconditions:*
 1. Billing is available

*Steps to reproduce:*
 1. Go to 'Billing report'/'Audit' pages
 2. Scroll down

*Actual result:*
 1. Footer is not sticky during scrolling down on 'Billing report' page
 2. Footer is not sticky during scrolling down on 'Audit' page

*Expected result:*
 1. Footer is sticky during scrolling down on 'Billing report' page
 2. Footer is sticky during scrolling down on 'Audit' page",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-07 06:50:59,5
13333935,[Back-end]: Minor issues connected with localization,"*Preconditions:*
 1. Billing is available on DataLab Web UI

*Steps to reproduce:*
 1. Go to 'Billing report' page

2. Click 'Export' button

*Actual result:*
 1. Available reporting period is  not localized according to your browser setting

2. Currency symbol is is localized according to your browser setting

*Expected result:*

1. Available reporting period is localized according to your browser setting

2. Currency symbol is is not localized according to your browser setting. Instead of 'USD' should be '$' (before/after sum. it depends on your browser language)

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-10-06 13:47:45,6
13333934,UI issues connected with name replacement,"*1.*

*Preconditions:*

1. Billing is available for DataLab Web UI

*Steps to reproduce:*

1. Go to 'Billling report' page

*Actual result:*

1. Resource name value is absent

*Expected result:*
 # Resource name value is present

----
*2.*  There is an axtra value on 'Detailed billing' popup",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-10-06 13:45:38,5
13333933,[Front-end]: Minor issues connected with localization,"*1.*

*Preconditions:*
 1. Billing is available on DataLab Web UI

*Steps to reproduce:*
 1. Go to 'List of Resources' page

*Actual result:*
 1. Billing on the 'List of Resources' page is not localized

*Expected result:*
 1. Billing on the 'List of Resources' page is localized
----
*2.* Convert date period according to selected language for 'Billing report' page
----
*3. (-)* Convey language key to back-end for billing export (will do it in branch with back-end fix) - it will be done in task 2089.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-10-06 13:35:05,5
13333703,Update images on promotion page according to new features,"1. [https://datalab.apache.org|https://datalab.apache.org/]
2. Adjust promotion page to different devices",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-10-05 12:06:22,5
13333669,[Back-end]: Replace name in documentation,Replace name in documentation for DataLab.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-10-05 08:54:57,6
13333668,[DevOps]: Replace name in documentation,Replace name in documentation for DataLab.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-10-05 08:52:52,2
13333655,[Front-end]: Replace name in documentation,"Replace name in documentation for DataLab:

- LICENSE

- DISCLAIMER

- USER_GUIDE

- NOTICE

- README

- CONTRIBUTING",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-10-05 07:42:13,5
13330502,[Azure]: SSN creation fails,"*Steps to reproduce:*

1. Create SSN on AZURE

*Actual result:*

1. SSN creation fails

*Expected result:*

1. SSN is created successful",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-10-02 07:35:33,2
13330200,[Environment management page]: Drop down list values is broken,"*Preconditions:*

1. User is located on 'Environment management' page

*Steps to reproduce:*
 # Expand the filter header
 # Click 'Project'/'Endpoint' drop down list

*Actual result:*
 # Values of dropdown list is broken
 # A part of dropdown list name is visible in the right side

*Expected:*
 # Values of dropdown list is not broken
 # A part of dropdown list name is not visible in the right side
 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-09-30 15:56:57,5
13328522,[Front-end]: Replace old name by new one in all sources,"A new name has been approved 'Apache DataLab'.

So we should change from 'Apache DLab' to 'Apache DataLab'.

In sources instead of 'dlab' use 'datalab'.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-21 09:40:47,5
13328521,[DevOps]: Replace old name by new one in all sources,"A new name has been approved 'Apache DataLab'.

So we should change from 'Apache DLab' to 'Apache DataLab'.

In sources instead of 'dlab' use 'datalab'.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-21 09:39:03,3
13328516,[Back-end]: Replace old name by new one in all sources,"A new name has been approved 'Apache DataLab'.

So we should change from 'Apache DLab' to 'Apache DataLab'.

In sources instead of 'dlab' use 'datalab'.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-21 09:26:55,6
13328514,Replace old name by new one for promotion page,"A new name has been approved 'Apache DataLab'.

So we should change from 'incubator-dlab' to 'incubator-datalab'.

On top of that replace in twitter, youtube.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-21 09:21:22,5
13328509,Replace old name by new one in all documentations,"A new name has been approved 'Apache DataLab'.

So we should change from 'Apache DLab' to 'Apache DataLab'.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-21 09:13:28,6
13328503,Investigate if it's possible to change repo name ,"A new name has been approved 'Apache DataLab'.

So we should change from 'incubator-dlab' to 'incubator-datalab'.",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-21 09:05:51,6
13328241,[Next release][Plugin architecture][DevOps]: Implement possibility to turn on billing after DataLab deployment, Embed and run billing as a service.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-18 12:43:19,3
13328022,Updated documentation,"# Changes link promotion page in Contributing.md
 # Updated release note",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-17 09:20:32,7
13327991,[WPM]: Add additional instance shape for DeeLearning Notebook,"n1-highmem-16

n1-highmem-8",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2020-09-17 07:48:34,3
13327625,[Azure][GCP]: Sometimes notebook creation fails on command mkfs.ext4 -F /dev/sdc1,"*Preconditions:*

1. Project is created on Azyre

*Steps to reproduce:*

1. Create any Notebook

*Actual result:*

1. Notebook creation fails on stage of mkfs.ext4 -F /dev/sdc1

*Expected result:*
 # Notebook creation is successful


----
The big appeared on GCP 22.06.2021 for Zeppelin. ",AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-09-15 08:41:51,2
13327621,[Azure]: Sometimes notebook creation fails on command update-initramfs -u,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create any notebook

*Actual result:*

1. Notebook creation fails on stage of  update-initramfs -u

*Expected result:*

1. Notebook creation is successful",AZURE Debian DevOps Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-09-15 08:37:21,2
13327611,Downgrade RStudio to the previous version,Downgrade RStudio to version 1.2.5033.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-09-15 07:57:14,2
13327416,[Roles]: Notebook/shape sort,"# Sort notebook A-z
 # Sort shape from the smallest to the biggest size",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-09-14 11:07:48,5
13327384,[Environment management]: Space between compute status and icon should not change if expand head grid,Especially it is blatant in small windows.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-09-14 08:45:09,5
13327383,[Billing report][Chrome browser]: The following clicking the same sort icon should not change value position in grid,"Chrome v. 85.0.4183.102 (Official Build) (64-bit)
 If user again clicks on the same sort icon (it has been already sorted) value of grid should have the same position.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-09-14 08:29:18,5
13327377,[Detailed billing]: Enhancement in column width,"There are different space between columns. 

Allocate column width according to value of max symbols and make space between columns equal.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Trivial,2020-09-14 08:08:01,5
13327374,Library management popup should have fixed size, If info changes for 'No matches found' (via filter) the window size should be the same on 'Library management' popup,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-09-14 08:00:45,5
13327131,[Azure]: Notebook creation fails due to interrupt of SSH connection,"*Preconditions:*

1. SSN is created on Azure

*Steps to reproduce:*
 # Create any notebook

*Actual result:*
 # Notebook creation fails

*Expected result:*

1. Notebook creation is successful",AZURE Debian DevOps Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-09-11 17:15:25,2
13327123,[Billing report][List of Resource][Library management][Environment management]: It's impossible to filter by name,"*Preconditions:*

1. Billing is available

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Type any name in 'Resource name' column

*Actual resul:*

1. Confirmation action is disabled

*Expected result:*

1. Confirmation action is enabled
----
The same error is present for 'List of resource', 'Library management', 'Environment management' pages.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-09-11 16:41:24,5
13327119,Style consistency support for Compute popup,"Cluster alias has another style, but should have the style as it is implemented for the other text boxes on this popup.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-09-11 16:25:01,5
13327045,Change message for scheduler reminder,"# If there are several notebooks which will be stopped in the same project point all notebooks in this project. Get rid of spaces before comma and extra comma before 'will' So alter the message to 'Following notebook servers <notebook_name>, <notebook_name> in project <project_name> will be stopped and all computational resources will be stopped/terminated by a schedule in less than 15 minutes.'. (+)
 # Add space between resource and compute name and between project and project_name.(+)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-09-11 09:11:46,5
13326693,Job running on instance (Notebook/Spark Standalone) does not change last activity,"*Preconditions:*

1. Inactivity is true in self-service.yml

*Steps to reproduce:*

1. Run playbook on Notebook/Spark Standalone cluster

*Actual result:*

1. Last activity is not updated

*Expected result:*

1. Last activity is updated",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-09-09 14:48:53,2
13326649,Reminder is still receiving after scheduler (for stop) has triggered ,"*Preconditions:*
 # Notebook is in running status
 # Scheduler by time is set up for Notebook stopping

*Steps to reproduce:*
 # Wait till scheduler for stop  will triggers
 # Start the notebook
 # Wait till notebook will be in running status
 # Click 'Refresh' button or go to any page

*Actual result:*

1. Reminder show up after notebook stopping

*Expected result:*

1. Reminder does not  show up after notebook stopping",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-09-09 10:06:20,6
13326635,[Jupyter][Jupyter with TensorFlow][DeepLearning]: Notebook creation fails on stage of 'pyrsistent' download,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create Jupyter/DeepLEarning/Jupyter with TensorFlow

*Actual result:*

1. Jupyter/DeepLEarning/Jupyter with TensorFlow creations fail

ERROR: Package 'pyrsistent' requires a different Python: 2.7.17 not in '>=3.5'

*Expected result:*

1. Jupyter/DeepLEarning/Jupyter with TensorFlow creations are successful",AWS AZURE De Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-09-09 09:06:16,2
13326437,[Library management]: Get rid of errors in console,"1. (+)

*Preconditions:*
 # Notebook is created
 # Library management popup is open for notebook

*Steps to reproduce:*

1. Select any library from drop down list

*Actual result:*
 # Error appears 'main-es2015.d121af7be5b69299f89a.js:1 ERROR TypeError: Cannot read property '0' of null'

*Expected result:*
 # There isn't any error

----
2. Sometimes when available lib list is gained successful autocomplete does not trigger and if install library the name library is conveyed as a version and library name is empty. Error appears about lower case. (+)

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-09-08 10:36:10,5
13326322,Project creation fails for external endpoint on GCP,"*Preconditions:*
 # External endpoint on GCP is created
 # SSN is created on AWS

*Steps to reproduce:*

1. Create project on GCP endpoint

*Actual result:*

1. Project creation fails

*Expected result:*

1. Project creation is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-09-07 14:50:04,2
13326276,Investigate if it is possible to sort notebooks and shapes on 'Roles' page,"# Could you sort notebook A-z?
 # Could you sort shape from the smallest to the biggest size?",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-09-07 09:27:53,5
13326275,Adding hint/message for different cases,"1.  If user is not allowed to create any notebook add hint 'Notebook creations are not available.  (+)
 Please, check your permissions.' on create analytical tool popup (select template drop down list)

2. Shrink window size of 'Add compute' if user is  not allowed to create any compute (+)

3. Add message if Project Admin is not assigned to any project: 'You are not assigned to any project.' for Project page (+)

4. Add message for project_admin if  group with not admin permissions is not assigned to project : 'You have not permissions for groups which are not assigned to your projects.' for group page. (+)

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-09-07 09:14:33,5
13326266,[Plugin architecture][Front-end]: Implement possibility to turn on billing after DataLab deployment, Embed and run billing as a service.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-07 08:24:55,0
13326265,[Plugin architecture][Back-end]: Implement possibility to turn on billing after DataLab deployment, Embed and run billing as a service.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-09-07 08:23:26,6
13326058,Consistency should be in filter actions,"# (/) [List of resources][Environment management][Billing][Audit][Library management]: If action is not filtered the filter cancellation action should be disabled by default. If point is not selected in drop down list the filter confirmation icon should be disabled. (+)
 # (/) [List of resources][Environment management][Billing][Audit][Library management]: If any action is filtered the filter cancellation action should be enabled. If any  item is selected in drop down list the filter confirmation icon should be enabled. (+) 
 # (/) [Billing report page]: *Preconditions*: Arrows of horizontal general scrollbar are enabled both or only left one. *Steps to reproduce*: - Collapse main menu. - Extend main menu. *Actual result*: Arrow is disabled. *Expected result*: Arrow is enabled. (+)
 # (/) [Billing report page]: If data is not matched after filter convey 'No matches found' instead of 'No data available'.
 # (/) [Billing report page]: Filter header is not sticky (view attachment). (+)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-09-04 09:58:27,5
13325931,[AWS]: From time to time billing is not updated for current month,"*Preconditions:*
 # SSN is created in current month
 # Billing is available in S3 bucket

*Steps to reproduce:*

1. Go to 'Billing report' page

*Actual result:*

1. Billing is absent in Web DLab UI

*Expected result:*
 # Billing is present in Web DLab UI

----
Note: It is tryout only for September to load billing data",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-09-03 13:44:45,6
13325858,[Bucket browser]: User is allocated in another folder during refresh,"*Preconditions:*
 # Several folders are created
 #  Each folder has the same sub-folders
 # User is located in any sub-folder of bottom folder

*Steps to reproduce:*

1. Click 'Refresh' button

*Actual result:*

1. User remains in the same sub-folder of the folder

*Expected result:*

1. User is allocated to another folder

 
----
2. Fix date forma (it should be as it is implemented in audit)

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-09-03 07:23:53,5
13325734,For wrong library name convey 'invalid name' instead of 'invalid version' for Data Engine Service apt/yum group,"*Precondition:*

1. EMR 5.30.0 is created on RStudio

*Step to reproduce:*

1. Install library from apt/yum group <htoP> for EMR

*Actual result:*
 # Library is not installed
 # Installation status in 'invalid version'

*Expected result:*
 # Library is not installed
 # Installation status in 'invalid name'",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-09-02 13:39:51,2
13325709,Minor changes about library installation,"# Do not allow to fetch library from drop down list via 'Enter'. However 'Enter' can be used for adding library after the library has been selected from drop down list 
 # Figure out how we can enhance  autocomplete feature
 # Add hint for '+' icon if library is not selected from drop down list: 'Please select library from autocomplete'. This hint should appear if only available library group list is received.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-09-02 10:23:06,5
13325701,Allow to stop notebook if at least one instance is in running status during multiple choice,On 'Environment management' page allow to stop notebook if at least one instance is in running status during multiple choice.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-09-02 09:35:54,5
13325535,JupyterLab image is not created during the first notebook creation,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create JupyterLab

*Actual result:*
 # JupyterLab (instance) is created
 # JupyterLab (image) is not created

*Expected result:*
 # JupyterLab (instance) is created
 # JupyterLab (image) is created",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-09-01 14:28:02,2
13325519,[Environment Management]: Notebook link management,"# Support copy  notebook link for 'Environment Management' page
 # Get rid of context menu for notebook link on 'Environment Management' page",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-09-01 12:55:05,5
13325469,[GCP]: Investigate why VPC remains after SSN termination,"If VPCs remain and collect it is impossible to create SSN due to maximum VPC name reaching.

Investigate why VPC remains after SSN termination
----
+Case when maximum VPC name is reached+

*Step to reproduce:*

1. Create SSN via Jenkins job

*Actual result:*
 # SSN creatin fails: 'VPC vi-gcp-0109-vpc has been created
 Traceback (most recent call last):
 File ""/root/scripts/ssn_prepare.py"", line 115, in <module>
 ssn_conf['vpc_selflink'] = GCPMeta.get_vpc(ssn_conf['vpc_name'])['selfLink']'

*Expected result:*

1. SSN creation is successful

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2020-09-01 07:26:35,2
13324030,"[DeepLearning creation]: Docker runs with '0'/is still in executing, but from UI side it's failed","*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create DeepLearning

*Actual result:*
 # Docker runs with '0'/is still executed
 # Status is failed from UI side

*Expected result:*
 # Docker run with '0'/is still executing
 # Creation status is running from UI side or creating (if docker is executing)

 ",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-08-21 20:21:57,6
13323965,Edge creation fails,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create edge nde

*Actual result:*

1. SSN creation fails

*Expected result:*

1. SSN is created succwssful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-08-21 14:02:31,2
13323910,[TensorFlow with Jupyter]: Get rid of r package for Spark Standalone cluster,"'TensorFlow with Jupyter' do not support r package. And this r package is absent.

However on related Spark Standalone cluster r package is present, but should be absent.

So, Get rid of r package for Spark Standalone cluster on template 'TensorFlow with Jupyter'.",pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-08-21 08:41:05,5
13323764,Filter by calendar is not triggered,"*Preconditions:*
 # Project is created
 # User is located on 'Audit' page

*Steps to reproduce:*

1. Filter values via calendar

*Actual result:*

1. Values are not filtered according calendar

*Expected result:*

1. Values are filtered according calendar",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-08-20 14:34:15,5
13323702,Get rid of r package for DeepLearning/TensorFlow with Jupyter,"R package should not be available for:
 * Deeplearning 
 * Jupyter with TensorFlow",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-08-20 09:19:08,6
13323573,[Billing][Audit]:Set of grid issues,"*Preconditions:*
 # SSN is created
 # Billing is available

*Steps to reproduce:*
 # Extend grid header on 'Audit'/Billing page
 # Collapse/extend main menu
 # Scroll dawn grid on 'Billing report'/'Audit' page
 # Filter by any value in 'Billing report'/'Audit' page

*Actual result:*
 # Filter actions are enabled('Billing' page) and the first enabled, the second disabled ('Audit' page)
 # Arrows of scrollbar are not disabled/enabled appropriate if extend/collapse menu
 # There are visible some parts of values under the footer
 # The values of dropdown is cut

*Expected result:*

1. Filter actions are disabled if user has not fetched anything - it will be done in he ticket https://issues.apache.org/jira/browse/DLAB-2026

2. Arrows of scrollbar are disabled if all content of grid is visible; arrows of scrollbar are enabled if content of grid is not visible (+)

3. There aren't visible any parts of values under the footer -  (+)

4. The values of dropdown is not cut -  (+)

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-08-19 14:56:31,5
13323568,Grid values change their positions during refresh,"This bug is reproduced if refresh data on 'Billing report' page.

*Preconditions:*
 # Billing is available
 # User is located on 'Billing report' page

*Steps to reproduce:*

1. Click on 'Refresh' button several times

*Actual result:*

1. Grid values change their position

*Expected result:*

1. Grid values remain on their previous place",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-08-19 14:15:25,1
13323559,Consistency of grid values should be in 'Billing' report,"In the other pages we use 'Notebook' and 'Compute. 

That's why these values should be in 'Billing' report page.
 # Change vallues:

 * 'Computational' -> 'Compute'
 * 'Exploratory' -> 'Notebook'",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-08-19 13:15:49,1
13323552,[Let's Encrypt certificates]: Edge creation fails on stage of stopping nginx.service,"*Preconditions:*
 # Create ssn is created with extra values:

- --conf_letsencrypt_enable true

- --conf_letsencrypt_domain_name

2. New cloud DNS is created for SSN

*Steps to reproduce:*
 # Create project
 # Create new cloud DNS for edge node

*Actual result:*

1. Project creation fails

*Expected result:*

1. Project creation is successful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-08-19 12:04:24,2
13323536,Enhancement for library management,"1. Do not allow to add java library if library is not fetched by user form dropdown-list

2. Could we sort group in dropdown list in grid as it is sorted in select group drop down list?",AWS Azure Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-08-19 10:27:17,5
13323372,Do not show library with status 'invalid name' for notebook which is created from custom image,Do not show library with status 'invalid name' for notebook which has been created from custom image and this previous notebook contained library with status 'invalid name'.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-08-18 14:42:05,6
13323368,[Audit]: Instead of rows calculation should be pages  calculation,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Go to 'Audit' page

*Actual result:*

1. Real amount of page does not equal amount of pages calculation

*Expected result:*

1. Real amount of page equals amount of page calculation",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-08-18 14:20:22,6
13322701,Investigate how to prevent lib installation during the process of available lib group receiving,"  1. (/) If installed with wrong name htopp for jupyter (during the process of available lib list receiving) -> the docker runs with '1' and status remains 'installing'.

So, we should forbid to install a library if process of available lib list gaining has not finished.

2.  (-) During this process maybe add the image of flask (Can't do it,  because I receive a response with autocomplete status after user type library name ) 

3.   (-) Remove image of flask if we switch between cluster and notebook. (I tried to do it,  but we need some loader there because we can select group from the previous resource before groups for current resource uploads. I changed the loading message. It is in attachments )",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-08-14 08:24:47,5
13322557,Issues if installed library with unknown name,"This cases are when autocomplete is turned off:

+AWS+

1.  If installed with wrong name in some cases docker runs with '1' and status is stuck in 'installing'. It was:
 * for Spark cluster/EMR pip3 'acceSS1'/'pivottablejsw' - 'others' group (+)
 * for Jupyter pip3 'AccessS' (+)

2. If install htop (apt/yum) on EMR with correct name without version -> docker runs with '1', and status is in 'installing' (+)

*GCP:*

3. If installed with wrong name htopp for jupyter -> the docker runs with '1' and status remains 'installing' - we have a ticket for it https://issues.apache.org/jira/browse/DLAB-1998

4. If installed wrong name for notebook/Dataproc/Spark cluster: 'pRontosd' (pip2 group/pip3/others) the docker runs with '1' and status remains 'installing' (+)",AWS AZURE Debian DevOps GCP Known_issues(release2.4) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-08-13 14:47:01,2
13322347,Issues if install library with unfound version,"# If install library with wrong version:

 * for apt/yum for notebook/spark cluster. The library is installed and it is absent that it has been wrong version (+)
 * for pip3/pip2/others for notebook/compute/. Docker runs with '1' and status is stuck in 'installing' (+)",AWS AZURE Debian DevOps GCP RedHat pull-request-available,[],DATALAB,Bug,Major,2020-08-12 18:59:45,2
13322278,Lib groups are not adjusted for instance,"*(!)* 

1.1 Jupyter - (/) all groups are present (+)

1.2 Spark cluster on Jupyter - (x) R package is absent, get rid of pip2 (+)

1.3. EMR on jupyter - (x) R package is absent, get rid of pip2 (+)

2.1. Zeppelin - (/) all groups are present  (+)

2.1 Spark cluster on Zeppelin - (x) R package is absent, get rid of pip2 (+)

2.2 EMR on Zeppelin - (x) R package is absent, get rid of pip2 (+)

3.1. Rstudio - (/) all groups are present (+)

3.2. Spark cluster on RStudio  (+)

3.3. EMR on RStudio -  (x) R package is absent, get rid of pip2 (+)

4.1. TensorFlow with Jupyter - (x) pip2/pip3/r package/others are absent on GCP - (!)  r package is absent on AWS

4.2.Spark cluster on TensorFlow with Jupyter - (x) R package is absent, get rid of pip2

5.1. TensorFlow with Rstudio -  (!) all necessary packages are present on AWS, this template is present only for AWS (+)

5.2. Spark cluster on TensorFlow with Rstudio -  (x) R package is absent, get rid of pip2 (+)

 
----
*Jupyter/Zeppelin/DeepLearning/TensorFlow with Jupyter:*
 * apt/yum
 * pip2
 * pip3
 * r package
 * Java
 * Others

+Compute on Jupyter/Zeppelin/Deeplearning/TensorFlow with Jupyter:+
 * apt/yum
 * pip3
 * r package
 * Java
 * Others

*RStudio/Tensorflow with RStudio:*
 * apt/yum
 * pip2
 * pip3
 * r package
 * Others

+Compute on RStudio/TensorFlow with RStudio:+ 
 * apt/yum
 * pip3
 * r package
 * Others

(!) We don't have opportunity to install lib for JupyterLab and Superset 
----
----
*Preconditions:*

1. Any cluster is created

*Steps to reproduce:*

1. Open lib groups for compute

*Actual result:*
 # Pip2 group is present
 # R package is absent

*Expected result:*
 * Pip2 group is absent
 * R package is present

 
----
----
On top of that not all groups are present for TensorFlow with RStudio and TensorFlow with Jupyter. 

For TensorFlow with RStudio should be:
 * apt/yum
 * pip2
 * pip3
 * r package
 * others

For TensorFlow with Jupyter should be:
 * apt/yum
 * pip2
 * pip3
 * r package
 * Java
 * others",AWS AZURE Back-end Debian GCP Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-08-12 14:36:47,6
13322242,[TensorFlow][DeepLearning]: Convey new version to UI,"Convey new version to UI:

TensorFlow - 2.1.0

DeepLearning -2.4",AWS AZURE Debian DevOps GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2020-08-12 10:55:45,2
13322066,'$anyuser' group could not be updated,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Remove administration or the other role  for '$anyuser'

*Actual result:*
 # Administration role is removed
 # Appears error message 'Group $anyuser not found'

*Expected result:*
 # Administration role is removed

 ",AWS AZURE Back-end Debian GCP Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2020-08-11 15:38:44,1
13322063,Only assigned user to the project should gain notification about quota exceeding,"This ticket is related only to project quota.

User who is not assigned to a project should not gain notification about quota exceeding of this project.
----
*Preconditions:*
 # User1 is not assigned to project1
 # Project1 quota is exceeded

*Steps to reproduce:*

1. Login by user1

*Actual result:*

1. User1 receives notification about project1 quota exceeding

*Expected result:*

1. User1  does not receive notification about project1 quota exceeding",AWS AZURE Back-end Debian GCP Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-08-11 15:33:45,1
13322021,Set of UI changes,"# (/)Breadcrumb overlaps only in Firefox (+)
 # (/) Ged rid of '.' in grid for audit (+)
 # (/) Reconfigire. Change from 'Reconfigure compute *sp1*, requested for notebook <*jup*>.' to  'Reconfigure compute *sp1*, requested for notebook *jup*.' (+)
 # (/) In grid (in confirmation icon) action should be in present, not in past if add/remove groups in project (+)
 # (/) Get rid of dataproc creation with GPU shape for Jupyter (+)
 # (/) Get rid of extra horizontal line in dropdown list for group creation (+)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-08-11 13:26:23,5
13322016,Total quota shouldn't be reset if only project quota is updated to 'null',"*Preconditions:*
 # Project quota is set up (it isn't null)
 # Total quota is set up (it isn't null)

*Steps to reproduce:*

1. Set project quota for null

*Actual result:*
 # Project quota is null
 # Total quota is  null

*Expected result:*
 # Project quota is null
 # Total quota isn't null

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-08-11 13:11:40,5
13321973,Update images to user guide,"Updated images: 
 # Create a project (+)
 # List of Resources(empty) (+)
 # List of Resources(notebook creating) (+)
 # List of Resources(one notebook running) (+)
 # Notebook info (+)
 # library all images (+)
 # Connected Data Engine Service becomes Terminated while connected (if any) Data Engine (Standalone Apache Spark cluster) becomes Stopped. (+)
 # List of Resources(notebook terminating) (+)
 # List of Resources(notebook terminated) (+)
 # List of Resources(compute creating) (+)
 # Selecting roles (+)
 # Environment management (+)
 # Manage quotas (+)
 # Project quota exceed message (+)
 # Billing (+)
 # UI filters (+)

 

Added images: 

     1. Bucket browser (+)

     2. Audit (+)

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-08-11 09:42:45,5
13321781,TensorFlow with Jupyter creation fails on stage of opencv-python lib install,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create TensorFlow with Jupyter

*Actual result:*

1. TensorFlow with Jupyter creation fails

*Expected result:*

1. TensorFlow with Jupyter creation is successful",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-08-10 12:27:12,2
13321497,Add validation for library version,"Version field can contain only the following:

Apt/yum/pip/r package/others Groups: <0-9> <A-Z> <a-z> <-> <_> <:> </> <~> <.> <+>",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-08-07 10:57:01,5
13321269,Allow to create EMR/Dataproc with GPU for Jupyter,"(/) 1. Allow to create EMR/Dataproc with GPU for Jupyter (+)

(/) 2. Get rid of ungit link for Zeppelin (+)",AWS Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-08-06 09:07:27,5
13321075,[DevOps]: Convey 'invalid name' status if it is tryout to install lib with wrong name,If user installs library  with wrong name - convey invalid name in status column.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-08-05 10:41:48,2
13321074,[Front-end]: Convey 'invalid name' status if it is tryout to install lib with wrong name,If user installs library  with wrong name - convey invalid name in status column.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-08-05 10:39:57,5
13321073,[Back-end]: Convey 'invalid name' status if it is tryout to install lib with wrong name,"If user installs library  with wrong name - convey invalid name in status column.
----
(!) Invalid name is conveyed only for notebook/Spark cluster/, But should be for EMR as well",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-08-05 10:39:06,6
13321064,[Jupyter]: Custom Spark Standalone creation/reconfiguration fails,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*

1. Create custom DE/reconfigure existing one

*Actual result:*

1.  Data Engine creation/reconfiguration fails

*Expected result:*

1. Data Engine creation/reconfiguration is successful",AWS Debian GCP RedH pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-08-05 10:10:51,2
13321061,[AWS][GCP]: Deep Learning creation fails,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create Deep Learning

*Actual result:*

1. DeepLeearning creation fails

*Expected result:*

1. DeepLearning is created successfully",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-08-05 09:34:44,3
13320804,[DevOps]: Add information if autocomplete does not work," 
 # If autocomplete does not work show information:

'Autocomplete is currently unavailable for <name_group> group'
----
2. [Library management]: Do not allow to add library if it is not selected from 'Library name' from drop down list

 

 

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-08-04 08:08:37,2
13320410,[Front-end]: Add information if autocomplete does not work,"# If autocomplete does not work show information:

'Autocomplete is currently unavailable for <name_group> group'
----
2. [Library management]: Do not allow to add library if it is not selected from 'Library name' from drop down list

3. Sometimes notebook lib group is gained for compute or likewise (view attachment)

 

 

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-31 13:16:38,5
13320409,[Back-end]: Add information if autocomplete does not work,"# If autocomplete does not work show information:

'Autocomplete is currently unavailable for <name_group> group'.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-31 13:15:30,6
13320392,Show 'invalid version' if lib installation fails due to wrong version,"1. If lib installation fails due to wrong version convey invalid version

How it works now (for example if version contains space):
 - R package: Docker runs with '0', shows 'installation error' and it is retry
 - Pip/apt/yum packages: Docker runs with '1' and status is still in 'installing'

(i) removed versions from tmp log file name, for R it shows invalid version in version contains invalid characters. 

(+) Invalid characters are blocked from UI side
----
----
2. Augment error message for library installation (<bossa> from r package). Available version value is empty  

(i) ""available_versions"": [""""] was not empty, now it is []. error message is installation_error

(+) Now status is 'invalid name'
----
----
3. For EMR pandas/matplotlib are installed but it is shown 'error installation' due to some dependencies are not installed. This case is reproduced for pip3 group.  

(i) now ""installation_error"" status is shown only if there was any kind of error during lib installation and lib has not been installed. error message is returned to backend, but I am note sure whether it will be shown on UI or not. (should be shown)

(+)
----
----
 4. For EMR ggplot2 (r package) is not installed via DLab UI, but it can be installed via terminal

(i) fixed

(+)
----
----
 5. If user does not type library version and installation fails - convey 'installation error' instead of 'invalid version'. It was found for EMR (others group) 'pivottable' 

(i) previously returned invalid version with available versions = none. Now returns installation_error with error message 

 The bug is still reproduced for compute/Notebook. Docker runs with '1' and status is stuck in 'installing'.

(i) it is shown as 'invalid name' due to absence of any version for this lib
----
----
6. For EMR (it was found on RStudio, perhaps it is reproduced and for the other template): If install library from apt/yum package and indicate wrong version docker runs with '1' and library status remains 'installing' in DLab UI

(i) fixed

(+)
----
----
 7. Rstudio (others group): Pivottablejs installation with correct version <0.2.0> was stuck in 'installing' status and docker run with '1'   

(i) there was a whitespace in version < 0.2.0>

(+) Now whitespace is blocked from UI side
----
----
8. TensorFlow with RStudio (r package -> NAM ): docker runs with '1' and installation status is still 'installing' in Web UI (i) fixed

(+)",AWS AZURE Debian DevOps GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2020-07-31 11:54:05,2
13319977,[Back-end]: Convey dependency for custom image,"1.  Show dependencies if library is in 'installation error' status:

Convey which dependencies are installed for notebook (created from custom image) if such dependencies have been installed for notebook which is used for custom image creation

2. Show dependencies if library is in 'installed' status:

Convey which dependencies are installed for notebook (created from custom image) if such dependencies have been installed for notebook which is used for custom image creation

3. Don't show library if it is in 'invalid version' status:

Don't convey library for notebook (created from custom image) if such library has been tried to install for notebook which is used for custom image creation",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-29 13:28:35,6
13319976,[DevOps]: Convey dependency for custom image,"1.  Show dependencies if library is in 'installation error' status:

Convey which dependencies are installed for notebook (created from custom image) if such dependencies have been installed for notebook which is used for custom image creation

2. Show dependencies if library is in 'installed' status:

Convey which dependencies are installed for notebook (created from custom image) if such dependencies have been installed for notebook which is used for custom image creation

3. Don't show library if it is in 'invalid version' status:

Don't convey library for notebook (created from custom image) if such library has been tried to install for notebook which is used for custom image creation",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-29 13:28:08,2
13319933,[Part2]: UI changes for all pages,"# (/) Highlight calendar in 'Billing'/'Audit' pages. The arrow toward From/To is beyond the calendar (/)
 # (/) The horizontal lines are not the same for 15.6 Desktop or if zoom is 125% (/)
 #  (/) if drug window the objects are not minimized in 'List of resource' page as it is in the other pages (/)
 #  (/) Augment more visible information about presence of horizontal scrollbar (/)
 # (/) [Library management]: If highlight all dependencies space shouldn't be white (/)
 # (/) [Library management]: If it's 'installation error' retry icon should be as well (/)
 # (/) [Library management]: Add hint for retry icon if library is in installing process. Add the same hint as it is for 'Install' button (-) (+)
 # (/) [Library management]: Get rid of 'query param artifact' and for java group query should not duplicate (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-29 09:04:34,5
13319732,Augment validation for folder creation,"1. If name folder contains '%' the folder won't be created.

So add '%' as a symbol which is not allowed to create a folder (+)

2. If switch between buckets and bucket name is too long autofocus should be in the beginning (+)

3. Align bucket path and value (+)

4. [List of resource]: Rename 'Select active project' -> 'Select project' (+)

5. All projects should be available if even they have stopped/terminated/terminating/stopping/starting statuses (+)

6. [Billing]: Horizontal lines are not the same (+)

7. [Billing]: Get rid of scrollbar if billing data are absent (+)

8. Add validation for total quota as it is implemented for project one (+)

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-28 11:59:47,5
13319705,Possibility to switch between project has been disappeared,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Go to list of resource page

*Actual result:*

1. Select active project dropdown list is absent

*Expected result:*

1. Select active project dropdown list is present",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-07-28 08:46:43,5
13319689,SSN deployment fails due to some packages are unauthenticated on Jenkins,"*Steps to reproduce:*

1. Deploy SSN via Jenkins job

*Actual result:*

1. SSN deployment fails

*Expected result:*

1.  SSN deployment is successful",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-07-28 07:27:55,3
13319556,Additional packages are not shown ,"*Preconditions:*

1. notebook is created

*Steps to reproduce:*
 # Install 'ruby' from 'Apt/Yum

*Actual result:*
 # Ruby is installed
 # Dependency is not shown

*Expected result:*
 # Ruby is installed
 # Dependency is not shown

 

Make library name case sensitive",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-07-27 14:10:59,5
13319145,[AWS]: Lib installation fails from r/Apt/Yum packages for Data Engine Service,"*Preconditions:*

1. EMR is created

*Steps to reproduce:*

1. Install library for EMR from APT/YUM and R packages

*Actual result:*
 # Library from APT/YUM is not installed
 # Library from r package is not installed (dev-tools is not installed)

*Expected result:*
 # Library from APT/YUM is installed (/)
 # Library from r package is installed (/)

same for dataproc
 * in some cases when apt package has more then 20 lines of dependencies, package installation fails (/)
 * it is stuck in installing status if docker runs with '1'. Libary numba from Py3. Error: 'no such file or directory' (/)
 * 'lam' from r package instead of 'invalid version' should be 'installation error' (see attachment) (/)

----
2. Do not convey lib_name in dependency (htop from 'Apt/Yum' group) (/)

3.  Do not convey <***> in 'Version is not available' (vlan/wget from 'Apt/Yum' group) (/)",AWS Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-07-24 10:50:25,2
13318982,Bucket browser is not defined,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Go to 'Bucket browser'

*Actual result:*

1. Bucket browser is not pulled

*Expected result:*

1. Bucket browser is pulled

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Bug,Critical,2020-07-23 15:11:01,5
13318852,[Project quota]: It is forbidden to set limit which contains more than 10 numbers,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Set project quota '123456789012'

*Actual result:*
 # Error message appears 'Numeric value (12345678901) out of range of int↵ at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 41]↵ at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 1, column: 30] (through reference chain: java.util.ArrayList[0]->com.epam.dlab.backendapi.domain.UpdateProjectBudgetDTO[""budget""])'
 # Indicated quota is not set up

Expected result:
 # Error message does not appear
 # Indicated quota is set up

Show error message:  'Project budget cannot be higher than 1000000000'",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-07-23 07:06:14,5
13318734,It is impossible to create notebook from image,"*Preconditions:*

1. Custom image is created from notebook

*Steps to reproduce:*

1. Create notebook from custom image

*Actual result:*

1. Error appears: 'Could not create exploratory environment Rs-fromami for user vira_vitanska@epam.com: null'

*Expected result:*
 # The error does not appear
 # Notebook is in 'creating' status

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-07-22 15:00:17,6
13318665,If docker runs with '1' the library status is stuck in 'installing' in Web UI,"*Preconditions:*
 # Notebook is created

*Steps to reproduce:*

1. Install library dyNET (Py3) without version indiction

*Actual result:*
 # Docker runs with '1'
 # Library status is still 'installing' in Web UI

*Expected result:*
 # Docker runs with '1'
 # Library status is 'failed' in Web UI",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-07-22 10:11:45,6
13318486,Show error message if user tries update/downgrade library by unreal  version,"# If user installs already installed library but indicates fiction version, the output will be that it is installed the library which has been previously installed (i) the output will be 'invalid version' (/)
 # If any distributive is not available for some library show as library installation 'installation error' and convey error message which we have previously (view attachment) (/)
 # If there is another error during installation except for 'No matching distribution found' show the status 'failed' and convey the particular error
 # If library is failed convey which dependencies have been installed during installation (i) added dependencies to response with failed status, probably some changes should be made on front-end to show dependency list with error message (/)
 # Investigate why some libs are disappeared (in proposed libs) after gaining of available lib list: kabaret.flow-contextaul-dict (Py3), KNN_TextClassifier(Py3), k2flix (Py3) - (i) it{color:#00875a} is not a problem (/){color}
 # It's impossible to install any library from r package for Dataproc. (i) moved to DLAB-1966 (-)",AWS AZURE Debian DevOps GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2020-07-21 12:20:39,2
13318483,[Part1]: UI tasks for library versions,"(/) 1.  Change 'version is not required' -> 'version is optional' +

(/) 2. Add space after format + 

(/) 3. Auto-search does not take into consideration the last letter +

(/) 4. Enter on keyboard should add library if '+' icon is active +

(/) 5. Disable input/'+' icon if group is not fetched +

(/) 6. Library should be shown as installed if user has already installed this library +

(/) 7. if library is in failed status show this library as installed in auto-search +

(/) 8. Add hint for library in grid +

(/) 9. Add bigger spaces between words and between rows in Dependency popup +

(/) 10. Sometimes is not changed lib group if switch between notebook and compute +",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-21 12:13:09,5
13318436,Pages are not taking in consideration for date sort,"*Preconditions:*
 # There are more than 25 notices in audit
 # It is fetched 25 notices per page

*Steps to reproduce:*
 # Update quota
 # Go to Audit page

*Actual result:*

1. Information about update quota is not in the first page

*Expected result:*

1. Information about update quota is in the top of the first page

 

Sort the whole data by date. Now it is sorted only per page, but should not be",AWS AZURE Back-end Debian GCP RedHat,[],DATALAB,Bug,Minor,2020-07-21 08:57:40,6
13318251,[AWS][Jupyter]: Investigate if running code for Data Engine Service uses GPU,"EMR (v5.30.0) with shape p2.xlarge is created for Jupyter.
1.  Running code for TensorFlow (cats/dogs recognition) is not successfully. It requires 'cv2' and it is impossible to install 'cv2' via console, and via Web UI DLab the installation fails

2. If run codes:
- https://weeraman.com/put-that-gpu-to-good-use-with-python-e5a437168c01
- https://www.geeksforgeeks.org/running-python-script-on-gpu/
It is shown that GPU is not used:",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-20 12:52:40,3
13317134,[Azure][Front-end]: Bucket browser,"As a user I want to  use S3 browser so that I can manage my buckets and objects via DLab UI.

 

*Acceptance criteria:*

*1.* User has access endpoint_shared bucket and project bucket (only if he is assigned to this project)

*2.* Another user does not have access to project bucket (if he is not assigned to this project)

*3.* Administrator has access to all buckets

*4.* User can upload and download files to and from bucket

*5.* User can create public URLs to share the files

*6.* User can set Access Control on buckets and files only for project bucket?",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,New Feature,Major,2020-07-16 10:01:02,5
13317103,[Front-end]: Instead of 'Follow link' use 'Open terminal' for audit,Change 'Follow link' -> 'Open terminal' in action column and in name popup,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-07-16 07:13:04,0
13317102,[Back-end]: Instead of 'Follow link' use 'Open terminal' for audit,Change 'Follow link' -> 'Open terminal' in action column and in name popup,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-07-16 07:12:36,1
13316897,[Front-end]: Add available audit period from in header,"Add available audit period from in header in 'Audit' page like it is implemented for 'Billing report' page except for from.

For example:

Available audit period from: Mmm dd, yyyy",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-07-15 10:51:57,0
13316896,[Back-end]: Add available audit period from in header,"Add available audit period from in header in 'Audit' page like it is implemented for 'Billing report' page except for from.

For example:

Available audit period from: Mmm dd, yyyy",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-07-15 10:51:34,1
13316891,"[Front-end]: For copy link should be copy action, not follow","# Rename popup name: 'Follow link' -> 'Copy link'
 # Rename action: 'Follow link' -> 'Copy link'",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-15 10:37:40,0
13316890,"[Back-end]: For copy link should be copy action, not follow","# Rename popup name: 'Follow link' -> 'Copy link'
 # Rename action: 'Follow link' -> 'Copy link'",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-15 10:37:14,1
13316875,[Front-end]: Changes for lib management after update,"1. If lib version is not found during lib installation show 'invalid version' status in Web UI in red color - (/)

2. Show information in Web UI if user chooses 'Other' group: (/)

'Other group contains Python2 and Python3 libraries.'

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-15 09:29:41,5
13316871,[Back-end]: Changes for lib management after update,"1. If lib version is not found during lib installation convey 'invalid version' status to Front-end - (/)

2. Prevent lib installation from pip2 group only for compute - (/)",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-15 09:18:36,6
13316870,Convey 'invalid version' status to back-end,If lib version is not found during lib installation convey 'invalid version' status to Back-end,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-15 09:10:05,2
13316867,Allow one active compute per Notebook,"1.(/)  User should be allowed to create a compute if:
 * Notebook has not any compute 
 * Notebook has compute in 'failed' status
 * Notebook has compute in 'terminated' status

2. (/) User is not allowed to create a compute if:
 * Notebook has compute in 'running' status
 * Notebook has compute in 'creating' status
 * Notebook has compute in 'configuring' status
 * Notebook has compute in 'reconfiguring' status
 * Notebook has compute in 'stopping' status
 * Notebook has compute in 'stopped status
 * Notebook has compute in 'starting' status
 * Notebook has compute in 'terminating' status

3. (/) If user is not allowed to create a compute 'Add compute' action should be disabled. If user hovers the mouse disabled 'Add compute' action a hint should show up: 'Only one compute can be associated with analytical tool at a time' (/)

4. (/) Prevent context menu for Web terminal (/)

5. (/) Assign appropriate name  for hint during copy links: for notebooks/clusters (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-07-15 08:56:47,5
13316861,[Azure][DevOps]: Bucket browser,"As a user I want to  use S3 browser so that I can manage my buckets and objects via DLab UI.

 

*Acceptance criteria:*

*1.* User has access endpoint_shared bucket and project bucket (only if he is assigned to this project)

*2.* Another user does not have access to project bucket (if he is not assigned to this project)

*3.* Administrator has access to all buckets

*4.* User can upload and download files to and from bucket

*5.* User can create public URLs to share the files

*6.* User can set Access Control on buckets and files only for project bucket?
----
 

Fill azureAuthFile: AZURE_AUTH_FILE_PATH

Example:
 authenticationFile: AUTHENTICATION_FILE
# Billing configuration for RateCard API. For more details please see [https://msdn.microsoft.com/en-us/library/mt219004.aspx]
 offerNumber: OFFER_NUMBER
 currency: CURRENCY
 locale: LOCALE
 regionInfo: REGION_INFO
  ",AZURE Debian DevOps RedHat pull-request-available,['DataLab Old'],DATALAB,New Feature,Major,2020-07-15 08:34:22,2
13316658,UI billing changes,"# (/) Decrease font for 'Select ...' in drop down list for Audit/Billing/Environment management/ List of resource pages +
 # (/) Get rid of 'Select' in calendar. it should be 'From date' and 'To date' in one row for billing report/audit pages +
 # (/) Decrease the height of calendar. Try to do the height as it is for 'Export'/'Refresh' buttons +
 # (/) Do horizontal line to the end of the right side under the calendar +
 # (/) Do three dots as a boarder between drop down lists +
 # (/) Align by horizontal drop down list {color:#00875a}from the left side{color} between pair of three dots +
 # Augment more visible information about presence of horizontal scrollbar (i) it will be improve in ticket 

 https://issues.apache.org/jira/browse/DLAB-1971

*Update*

8. (/) Add space between 'Export' and 'Refresh' buttons +

9. (/) The circle icon in 'Refresh' button should be blue +

10. (/)  'No data available' should be the same font as it is Service base name +

11. (/) Add a little bit space between after 'From' 'To' in the Calendar +

12. (/)  Do bigger font for 'Environment name'/ 'User' and the other name of dropdown list +

 

*Audit:*

13.(/)  forward and backward arrows should be blue (if they are active) +

14.(/) If action name is too long - cut them +

15.(/)  Information icon align from the right side +

16. (/)  Should be consistency in fonts +

 

*List of resource page:*

17.  (/)  Align values in grid for detailed billing (view attachment) +

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-14 09:24:32,5
13316653,Argument validation for user adding to group,1. Adding user to group should not be case sensitive,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-14 09:12:44,5
13316650,[Part3]: Set of UI tasks in audit,"# Add copy icon to notebook/cluster links (/)
 # Prevent some actions of context menu: Open link in new tab; Open link in new window; Open link in incognito window; Copy link... (/)
 # Ged rid of full stop on the end of every row for group update in audit (/)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-14 08:53:36,5
13316101,Edge termination deletes image of another endpoint within one project,"This bug was found on AWS. I have not checked for the other clouds. Could you ponder if it is cloud related?

*Preconditions:*
 # Project1 has local endpoint1 on AWS
 # Proejct1 has remote endpoint2 on AWS
 # Any image is created  on endpoint1
 # Any image is created on endpoint2

*Steps to reproduce:*

1. Terminate edge node1 for local endpoint1

*Actual result:*
 # Image is deleted  for endpoint1
 # Image is deleted for endpoint2

*Expected result:*
 # Image is deleted  for endpoint1
 # Image is not deleted  for endpoint2

 ",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-07-10 09:00:28,2
13316093,"[Project termination][Front-end]: Show all resources which will be terminated, not only own","When user terminates a project it is shown only own resources, which will be terminated as well. But also convey the other resources which will be terminated.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-10 08:40:05,0
13316091,"[Project termination][Back-end]: Show all resources which will be terminated, no only own","When user terminates a project it is shown only own resources, which will be terminated as well. But also convey the other resources which will be terminated.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-10 08:39:33,1
13315952,Investigate how we can show following link in Audit in some cases,"Following link is not shown in audit if user refers link via:

- Open link in new tab

- Open link in new window

- Open link in incognito window

Investigate if it can be fixed.  If yes, then fix it.

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-09 17:20:28,5
13315917,Git credentials management in audit,"# If we add git account should be Add in action, NOT Update. Show information icon: 'Add host <host_name>'
 # If we update git account show information icon: Fields -> Previous value -> New value. If user changes a password the password field should be shown, but do NOT show the password. instead of password value you can show six bullets
 # If we decommission git account should be Decommission in action, NOT Update. Show information icon: 'Decommission host <host_name>'",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-09 14:27:13,1
13315915,[Part2][Back-end]: Set of audit tasks,"# Add information icon for compute reconfigure: 'Reconfigure compute <cluster_name>, requested for notebook <notebook_name> (+)
 # Install libs. Change action: Install -> Install libs. Change resource type: Notebook libs -> Notebook  and Computational libs -> Compute (+)
 # Update quota. It is possible to update quota by month. So convey if quota is updated per month or per whole period to front-end (Update quota null->10 change to Update quota null->10/ Update period  total->monthly) (-) | (+)
 # Add information icon if user updates a project by adding/removing group. Show grid: In Action column -> Add group(s)/remove group(s), and in Description column the list of groups. (-) It should be in grid. As it is implemented for lib installation -(i)_*{color:#de350b}(It will be done from frontend side){color}*_
 # Remove '(s)' form File(s) during upload and download files (+)
 # Audit for 'Bucket browser'. In action column change: 

 - for upload - from 'File(s)' to 'Upload file' (-) | (+)
 - for download -from 'File(s)' to 'Download file' (-) | (+)
 - for delete - from 'File(s)' to 'Delete file(s)' (-) | (+)

7. If create folder add action in grid for confirmation icon. 'Upload folder' (+)",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-09 14:18:51,4
13315872,[Part2]: Set of UI tasks in Audit,"(/) 1.  Comma and full stop should not be bold (/)

(/) 2. Endpoint name and project name should be bold (/)

(/) 3.  Change info from 'Stop/terminate/start/reconfigure/create computational resource <cluster_name>, requested for notebook <notebook_name> to 'Stop/terminate/start/reconfigure/create compute <cluster_name>, requested for notebook <notebook_name> (x) (/) 

(/) 4. Spark cluster. Change info: Follow_link computational resource <compute_name>, requested for notebook link. -> Follow compute <compute_name> Master link, requested for notebook <notebook_name>. (/) {color:#de350b}But you should get away double spaces see point 14  (/)  {color}

(/) 5. Data engine Service. Change info: Follow_link computational resource <compute_name>, requested for notebook link. -> Follow compute <compute_name> Hadoop link, requested for notebook <notebook_name>.

(/) 6. Update quota: Rename Update budget -> Update quota (/)

(/) 7. Update quota: Show information about period. Add row 'Period' and values 'Total'/'Monthly' (i) it is waiting for back-end side

(/) 8. Update quote. Move the second column 'Previous value' to the left side in information popup (/)

(/) 9. Web terminal. Change message Open terminal on notebook <notebook_name>. -> Open terminal, requested for notebook <notebook_name>. (x)(/)  remove extra information '2' (/)

(/) 10. Web terminal. Notebook name should be bold (/)

(/) 11. Should be consistency in font style for all information (/)

(/) 12. Action should not be in the past. Change for group update: Removed role(s) -> Remove role(s) and Added role(s) -> Add role(s), and Added user(s) -> Add user(s), and Removed user(s) -> Remove user(s) (x) Now actions are not in the past tense, but the grid is broken when update group or delete objects via bucket browsers (/)

13. Decrease the size for upload/delete/download objects. Add hint for object name. Add three points in the beginning of  object path(/)

(/) 14. Get rid of double spaces Data engine and Data engine service of following link (/)
  
  
  ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-09 11:44:24,5
13315647,"Show information icon for cluster stopping/starting/termination, requested for notebook/project ","Show information icon for computational resource in case of stopping/starting/termination not directly the cluster, but via notebook.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-08 14:36:01,1
13315646,Convey information icon for cluster stopping requested for notebook by scheduler,Convey information icon for stop by scheduler for spark cluster if scheduler is triggered for notebook,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-08 14:31:14,1
13315597,Allow only one active Data Engine Service per notebook,Allow to create EMR per notebook if this notebook has not any DES or DES is in 'terminated'/'failed' statuses.,AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-07-08 10:03:15,5
13315594,"If detailed billing is '0', so convey this value to the 'List of Resource'page","*Preconditions:*
 # Billing is available
 # For notebook detailed billing is 'null'

*Steps to reproduce:*

1. Go to the 'List of resource' page

*Expected result:*

1. Billing is 'N/A' instead of '0'

*Actual result:*

1. Billing is '0'

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-07-08 09:56:34,5
13315121,[AWS][EMR]: It is not shown correct total instance number for EMR,"*Preconditions:*

1. EMR is. created

*Steps to reproduce:*

1. Click on EMR name

*Actual result:*

1. It is not shown correct total instance number

*Expected result:*

1. It is shown correct total instance number

 ",AWS Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-07-06 09:15:01,5
13314931,UI changes for scheduler,"# Show in reminder in which project scheduler will be triggered (/)
 # If Spark cluster inherits notebook scheduler allow or forbid stop and allow terminate action for spark cluster (/)
 # Reminder does not show up for cluster termination - (x) it should be fix by back-end side, the ticket https://issues.apache.org/jira/browse/DLAB-1922
 # Add space after server (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-03 17:37:57,5
13314929,Support audit configurable via administration page,"1. Add audit option in 'Roles'

Audit:

-View audit report

2. We should follow consistency. So, get rid of 'Allow to' in all option",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-07-03 17:29:19,1
13314915,Reminder about notebook stopping continues to show up after scheduler triggering,"*Preconditions:*
 # Notebook is stopped by scheduler

*Steps to reproduce:*
 # Start notebook
 # Refresh page via DLab 'Refresh' button

*Actual result:*

1. Reminder shows up

*Expected result:*

1. Reminder does not show up
----
(i) *Reminder does not show up for cluster termination*",AWS AZURE Debian Front-end GCP Known_issues(release2.4) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-07-03 15:26:47,4
13314902,It is impossible to create any group,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*
 # Go to 'Roles' page
 # Create group

*Actual result:*
 # Appears error message: 'Oops! Group creation failed!'
 # The groups is not created

*Expected result:*
 # Error message is not appear
 # The group is created

 ",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-07-03 14:11:56,4
13314896,Endpoint disconnection should not again terminate project which is already terminated,"*Preconditions:*
 # Project1 is created from endpoint1
 # Project1 is terminated

*Steps to reproduce:*

1. Disconnect endpoint1

*Actual result:*

1.  Project1 changes its status from 'terminated' to 'terminating'

Expected result:

1.  Project1 does not change its status from 'terminated' to 'terminating'

 

 ",AWS AZURE Back-end Debian GCP Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2020-07-03 13:55:49,1
13314877,[Part1][Back-end]: Set of audit tasks,"# Update of total quota is not shown in audit - {color:#de350b}we should wait if we need total quota{color}
 # (/) Change action for endpoint of audit: Create -> Connect, Delete -> Disconnect (/)
 # (/) If we disconnect endpoint all resources are terminated. Show which resources are terminated in audit (/)
 # (/) If we stop notebook all related computational resource except for Data engine service will be terminated. So show terminate action, not stop for DES (/)
 # Show information icon for computational resource in case of stopping/starting/termination not directly the cluster, but via notebook or via project management (action menu) - (i) it will do in ticket https://issues.apache.org/jira/browse/DLAB-1932
 # (/) If create new edge node in existing project add information icon for update project with the following info: 'Create edge node for endpoint <endpoint_name>, requested in project <project_name> (/)
 # If we add git account should be Add in action, NOT Update. Show information icon: 'Add host <host_name>' (i) it will do in the ticket https://issues.apache.org/jira/browse/DLAB-1936
 # If we update git account show information icon: Fields -> Previous value -> New value. If user changes a password the password field should be shown, but do NOT show the password. instead of password value you can show six bullets - (i) it will do in the ticket https://issues.apache.org/jira/browse/DLAB-1936
 # If we decommission git account should be Decommission in action, NOT Update. Show information icon: 'Decommission host <host_name>' (i) it will do in the ticket https://issues.apache.org/jira/browse/DLAB-1936
 # -(x) If user updates quota only for one project do not convey the value of the other project if values is the same as previous- (It will be fixed on UI side)
 # (/) Instead of 'Computational' use 'Compute' in 'Resource type' column (/)
 # (/) If user follows compute link change 'Notebook' to 'Compute' in 'Resource type' column (/)
 # (/) Reconfigure notebook. Change 'Notebook config' -> 'Notebook' in 'Resource type' column (/)
 # (/) Reconfigure spark cluster. Change 'Computational config' -> 'Compute' in 'Resource type' column (/)
 # (/) Reconfigure notebook/spark cluster. Rename action from 'Update' to 'Reconfigure' (/)
 # (/) Date value should be sorted from the newest to the lates in grid (/)
 # (/) Show information if user goes to Web terminal (/)
 # (x) Image creation: Instead of 'Image' use 'Notebook' in 'Resource type' column - (i) it is not a bug, so it will not fix
 # (/) Scheduler set up for notebook/cluster: Instead of 'Notebook scheduler'/'Computational scheduler' use 'Notebook'/'Compute' in 'Resource type' column (/)
 # (/) Rename action for scheduler configuration from 'Create' to 'Configure' or 'Set up' (/)
 # Convey information icon for stop by scheduler for spark cluster if scheduler is triggered for notebook - will do in the other ticket https://issues.apache.org/jira/browse/DLAB-1931
 # (/) If stop notebook by the other user (via environment management), related cluster stopping should also be shown as stopped by the other user, NOT by owner (/)
 # (/) Related stopped cluster are not shown in audit during notebook termination (/)
 # (/) Related stopped notebook/spark cluster are not shown in audit during project termination (/)
 # (/) If the cluster stopped/started/terminated by the scheduler change message Scheduled action -> 'Terminate/stop/start by scheduler, requested for notebook 'notebook_name'.' (/)",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-07-03 10:56:06,6
13314852,Quota is not updated for 'null' (empty value),"*Preconditions:*

1. Manage DLab quotas is set up

*Steps to reproduce:*
 # Set up new value (leave empty)
 # Click 'Apply' button

*Actual result:*

1. New value is not assigned

*Expected result:*

1. New value is assigned

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-07-03 08:32:23,4
13314742,[Part1]: Set of UI tasks in Audit,"# It is too big popup for a such short information. Could you resize popup? (/) (/)
 # If user stop/terminate/start cluster in info show the following: 'Stop/terminate/start/reconfigure/create computational resource <cluster_name>, {color:#de350b}WHICH HAS BEEN{color} requested for notebook <notebook_name> .' (/) (/)
 # Change information for quota update, add column: Field -> Previous value -> New value (/) (/)
 # Change for library installation: Installed libs -> Install libs (/) (/)
 # Follow link. Change: 'User followed Apache Spark Master link' ->'Follow Apache Spark Master link.' (/) (x) will be done in the ticket https://issues.apache.org/jira/browse/DLAB-1934
 # Follow link. Change: 'User followed EMR Master link.' -> 'Follow EMR Master link.' (/) (x) will be done in the ticket https://issues.apache.org/jira/browse/DLAB-1934
 # Follow link. Change: 'User followed RStudio link.' -> 'Follow RStudio link.' Or another notebook (/) (/)
 # Follow link. Change: 'User followed Ungit link.' -> 'Follow Ungit link.' (/) (/)
 # Rename for image creation: 'Image name' -> 'Create image' (/) (/)
 #  Start by scheduler for notebook: Change 'Scheduled action.' -> 'Start by scheduler.'  (/) (/)
 # * Start by scheduler for cluster: Change 'Scheduled action.' -> 'Start by scheduler, requested for notebook 'notebook_name'' (/) (/)
 # Stop by scheduler for notebook: Change 'Scheduled action.' -> 'Stop by scheduler.'(/) (/)
 # * Stop by scheduler for cluster: Change 'Scheduled action.' -> 'Stop by scheduler, requested for notebook 'notebook_name''(/) (/)
 # * Terminate by scheduler for cluster: Change 'Scheduled action.' -> 'Terminate by scheduler, requested for notebook 'notebook_name''(/) (/)
 # Footer should be sticky while scroll up  (/) (/)
 # If user updates quota only for one project do not convey the value of the other project if values is the same as previous  (/) (/)

 

    *   ----  Will work correct after changes from Back End. There need notebook name",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-07-02 16:19:03,5
13314630,[GCP]: Image data is absent in 'Billing report',"*Preconditions:*
 # Notebook is created in GCP
 # Billing is present

*Steps to reproduce:*
 # Go to billing report page
 # Filter by image in 'Resource name' text box

*Actual result:*

1. Image data is absent in grid

*Expected result:*

1. Image data is present in grid

 ",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-07-02 09:08:52,6
13314493,[Billing]: The amount of slave nodes calculate as 'total_number_nodes-1node(master)',"Every computational resource has only one master node and the other nodes are slaves.

We allow user to chose different sizes for master and for slave only for EMR and Dataproc. 

User cannot chose different sizes for master and for slave for Spark cluster.

So calculate the amount of salve nodes for 'Billing report' page:

total_number_nodes - one_node

(i) It is wrong calculation if slave amount for EMR and Dataproc (view attachment).",AWS AZURE GCP,['DataLab Main'],DATALAB,Task,Minor,2020-07-01 14:19:51,6
13314476,Set of UI tasks for 'Billing report' and 'List of resource' pages ,"# 'Refresh' option should not clear data in calendar in 'Billing report' page - (/)
 # Refresh' option should not change the values by place - (x) It will be done by the task https://issues.apache.org/jira/browse/DLAB-1926
 # Refresh' option should not hide the other points in selected drop down list (/)
 # Show node count, version for EMR and Dataproc in computational popup (x) it will be done by the task https://issues.apache.org/jira/browse/DLAB-1916
 # If total number of slaves contains more than one node do not show 'amountxsize' in 'Instance size' drop down list in billing report page, just show 'size' (/)

(x)    For version EMR and Dataproc in the computational popup need changes from Backend",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-07-01 13:19:31,5
13314432,Remote endpoint does not change its status after any action execution,"This bug was found on remote endpoint on AWS. I have not checked for the other clouds.

*Preconditions:*
 # SSN is created on AWS
 # Remote endpoint is created on AWS

*Steps to reproduce:*

1. Create project on remote endpoint

*Actual result:*
 # Docker runs with '0'
 # Endpoint is still in 'creating' status on DLab Web UI

*Expected result:*
 # Docker runs with '0'
 # Endpoint is  in 'running' status on DLab Web UI",AWS AZURE Back-end Debian GCP RedHat,[],DATALAB,Bug,Major,2020-07-01 09:08:24,6
13314423,[AWS]: Billing is not available,"*Preconditions:*
 # Resources are created on local endpoint
 # Billing is present in csv file

*Steps to reproduce:*

1. Go to billing report page or to detailed billing

*Actual result:*

1. Billing is not available in DLab Web UI

*Expected result:*

1. Billing is available in DLab Web UI

 ",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-07-01 07:54:19,6
13314240,[Font-end]: Ability to share own notebook,"As a user I want to share my notebook among the particular user(s) so that user is able to use my own data/ resource for collaboration.
It means that in List of resources page user is able to see own resources and shared resources.
 ",AWS AZURE Debian Front-end GCP Mentor_program RedHat,['DataLab Main'],DATALAB,Improvement,Major,2020-06-30 08:51:30,0
13314239,[Back-end]: Ability to share own notebook,"As a user I want to share my notebook among the particular user(s) so that user is able to use my own data/ resource for collaboration.
It means that in List of resources page user is able to see own resources and shared resources.

 ",AWS AZURE Back-end Debian GCP Mentor_program RedHat,['DataLab Main'],DATALAB,Improvement,Major,2020-06-30 08:51:06,1
13314221,[Front-end]: Support quota per month,As an admin I want to set up quote per month so it allows me to fill limit monthly by demand.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-06-30 07:21:29,5
13314220,[Back-end]: Support quota per month,As an admin I want to set up quote per month so it allows me to fill limit monthly by demand.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2020-06-30 07:20:08,6
13313598,[Audit]: Set of task for browser bucket action,"# Ged rid of 'file'/'size' in upload popup information  (/)
 # Rename 'File name' to 'File(s)' (/)
 # If file name is too long cut it (from the beginning) and add a hint (/)
 # Ged rid of '.' in the end of 'Upload'/'Download'/'Delete' in 'Description' column popup (/)
 # Align audit header by vertical (/)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-06-26 12:49:26,5
13313596,Folder creation (via bucket browser) is not shown in Audit,"*Preconditions:*

1. Bucket browser popup is opened

*Steps to reproduce:*
 # Create empty folder via bucket browser
 # Go to Audit page

*Actual result:*

1. Folder creation is not shown in Audit

*Expected result:*

Folder creation is shown in Audit",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-06-26 12:30:26,6
13313591,It is impossible to delete empty folder via bucket browser,"*Preconditions:*
 # Bucket browser popup is opened
 # Any empty folder is created

*Steps to reproduce:*

1. Delete empty folder

*Actual resul:*

1. Appears error message: 'objects field cannot be empty' 

*Expected result:*

1. Error message does not appear",AWS AZURE Debian Front-end GCP RedHat,[],DATALAB,Bug,Minor,2020-06-26 11:47:35,5
13313578,[Scheduler]: Stopping notebook does not trigger,"*Preconditions:*

1. Notebook is in running status

*Steps to reproduce:*
 # Set up scheduler (vies attachment 'Set up')
 # Click on 'Save' button

*Actual result:*

1. The error message appears: 'Failed to parse string as a date'

*Expected result:*

1. There is not any error message

 ",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-26 10:32:11,6
13313468,DataLab instance status should be synced up with cloud status,"If change instance status from cloud console the DataLab instance status is not updated:
 # Edge node
 # Notebook
 # Cluster (master)

Please investigate the reason",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Critical,2020-06-25 15:32:44,6
13313284,Set of tasks for UI style,"# Filter should be closed by default for all DLab pages - (/)
 # Some billing values of drop down list are absent - (/)
 # Align 'To start working, please, create new environment' in 'Environment management' page - (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2020-06-24 16:02:55,5
13313280,Update mongodb version,"Current version is 3.2.22.

Update to 4.2.0",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-06-24 15:38:50,3
13313073,Web terminal is not always opened successfully,"This bug was found on AWS from branch epm-v2.3.0  (It has not been checked on Azure/GCP). This bug was found on GCP as well, in some minutes (after login) terminal could nod be opened successful ( branch-2.4.0). If user opens terminal right away after login the terminal opens successfully.

*Preconditions:*

1. Notebook is created from branch epm-v2.3.0

*Steps to reproduce:*

1. Open Web terminal

Actual result:
 # Web terminal is not always opened successfully

*Expected result:*
 # Web terminal is always opened successfully

----
This bug was reproduced 29/07/2020 on AWS

This bug was reproduced 07/10/2020 on AWS from develop branch
",AWS Debian RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-06-23 15:16:03,1
13313019,Get requests for lib groups are still gained even after library management popup closing,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*
 # Open 'Library management' popup
 # Oen F12
 # Close 'Library management' popup

*Actual result:*

1. Get requests for lib groups are still gained

*Expected result:*

1. Get requests for lib groups are not gained

 

Merge fix in develop and epm-v2.3.0 branches.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-06-23 10:32:10,5
13313005,[Front-end]: Distinguish action in two columns,"1. Distinguish action in two columns

2. Change column order  (time -> user -> action ...)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-06-23 09:12:54,5
13313002,Refresh option should not clear filter in 'Billing report' page,# Refresh behavior for filter should be the same for all DLab pages,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-06-23 09:02:25,5
13312201,[Part3]: Set of tasks for 'Environment management' page,"# Ged rid of common check box if check box item is absent (view attachment)- (/)
 # 'Reconfiguring' should not overlap stop icon in 'Environment management' page (view attachment) - (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-06-18 11:51:03,5
13312165,Disable access to bucket if endpoint is stopped or termianted,"If endpoint is stopped or terminated and user tries to go to endpoint bucket the process of access trying is permanently in progress. So disable this bucket and convey information message, that endpoint is not active or hide this bucket.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-06-18 08:51:04,5
13311959,Custom image creation fails if image name contains upper case,"*Preconditions:*

1. Notebook name contains upper case

*Steps to reproduce:*

1. Create image from notebook which contains upper case/create image where image name contains upper case or project name

*Actual result:*

1. Custom image creation fails

Expected result:

1. Custom image creation is successful",Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-06-17 12:44:02,2
13311946,[Front-end]: Empty folder is not created in cloud via bucket browser,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create folder via bucket browser

*Actual result:*

1. Empty folder is not created on cloud

*Expected result:*
 # Empty folder is created on cloud

Merge fix into develop and epm-v2.3.0 branches.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-06-17 11:54:22,5
13311912,Token configuration,"# Extend token up to one hour
 # Refresh token in 25 minutes for ending",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-06-17 08:53:47,5
13311910,[Part2]: Set of tasks for 'Environment management' page,"1.  Do not allow to stop/terminate notebook if related cluster is in process status (terminating/stopping/starting/creating/configuring/reconfiguring) - (/)

2. In environment management page cluster name is not clickable, so it should not be blue in this page - (/)

3. If cluster is in failed status- do not convey it in confirmation dialog for stopping/termination - (/)

4. Disable icon action for cluster if notebook is checked off - (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-06-17 08:49:31,5
13311746,Empty folder is not created in cloud via bucket browser,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create folder via bucket browser

*Actual result:*

1. Empty folder is not created on cloud

*Expected result:*
 # Empty folder is created on cloud

Merge fix into develop and epm-v2.3.0 branches.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-16 15:01:26,6
13311522,It is passed default endpoint instead of usable one,"*Preconditions:*
 # Project is created for local end remote endpoints

*Steps to reproduce:*
 # Upload object for remote endpoint

*Actual result:*
 # Upload could be failed or successful
 # It is passed default endpoint

*Expected result:*
 # Upload is successful
 # It is passed usable endpoint

 *Merge this fix into develop and epm-v2.3.0*",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-06-15 16:17:00,5
13311512,Set of tasks for 'Environment management' page,"# Remove common check box if there is not any check box item (/)
 # Grid should not be broken during dragging (/)
 # Do not convey cluster column in confirmation message if cluster list is empty (/)
 # If cancel stop/terminate notebooks previous selected notebooks should remain checked off (/)
 # If name notebook is clickable it should be blue in 'environment management'/'List of resource' pages. Project should not be blue, should be black and bold (/)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-06-15 15:13:24,5
13311471,Add bucket service for remote endpoint,Add this to 'develop' and 'epm-v2.3.0' branches.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-06-15 11:32:15,6
13311396,Bucket browser is not opened for remote endpoint,"*Preconditions:*
 # Project is created for remote endpoint

*Steps to reproduce:*
 # Open bucket browser
 # Go to bucket of remote endpoint via bucket browser

*Actual result:*
 # Bucket of remote endpoint is not opened

*Expected result:*

1. Bucket of remote endpoint is opened

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-15 06:45:39,6
13310660,[Branch-515][Azure][Jupyter]: Playbook airbnb runs with error,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*

1. Run playbook airbnb 

*Actual result:*
 # Playbook runs with error 'attrib() got an unexpected keyword argument 'convert''

*Expected result:*

1.Playbook airbnb successful

 
 ",AWS AZURE Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-06-10 13:15:50,3
13310653,[Branch-515][Azure][RStudio]: Playbook Flight_data_Preparation_R run with error,"*Preconditions:*

1. RStudio is created on  Azure 

*Steps to reproduce:*

1. Run Flight_data_Preparation_R 

*Actual result:*

1. Playbook runs with error:
Error in read.df(bucket_path(""carriers.csv""), ""csv"", header = ""true"",  : 
  could not find function ""read.df""

 *Expected result:*
 # Playbook runs successfully

 
  ",AZURE Debian DevOps Known_issues(release2.4),['DataLab Main'],DATALAB,Bug,Major,2020-06-10 13:10:00,3
13310649,[Branch-515][Azure][Jupyter] :Playbooks Flights_data_Visualization_Python2/python3 run with error ,"*Preconditions:*

1.  Playbook Flights_data_preperation_Python2/Python3 were run on Jupyter

*Steps to reproduce:*

1. Run Playbooks Flights_data_Visualization_Python2/python3 on Jupyter

*Actual result:*

1. Playbooks run with error 'Unsupported class file major version 55'

*Expected result:*
 # Playbooks run successful

 

*Flights_data_Visualization (Azure)*
||Python2 (local)|{color:#de350b}error with running playbook{color}|
||Python3 (local)|{color:#de350b}error with running playbook{color}|
||Python2 (Spark)|{color:#4c9aff}Bloked{color}|
||Python3 (Spark)|{color:#4c9aff}Bloked{color}|",AZURE Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-06-10 13:00:36,3
13310622,[Branch-515][Azure][Jupyter]:  Connection to kernel R/Sala is not successful,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*
 # Run playbook via kernel R (Local SparkR (R-3.4.4, Spark-2.4.4) and remote EMR5.28 SparkR (R-3.4.1, Spark-2.4.4))
 # Run playbook via kernel Scala (Local Apache Toree - Scala (Scala-2.11.12, Spark-2.4.4) and remote EMR 5.28 Apache Toree - Scala (Scala-2.11.12, Spark-2.4.4) )

*Actual result:*
 # Connection to kernel R died
 # Connection to Scala stuck

*Expected result:*

1. Connection to kernel R is successful

2. Connection to Scala is successful


*For Azure*

R/Scala
||R (local)|{color:#de350b}error with connection to kernel{color}|
||R (Spark)|{color:#4c9aff}Blocked{color}|
||Scala (Local)|{color:#de350b}error with connection to kernel {color}|
||Scala (Spark)|{color:#4c9aff}Blocked{color}|",AWS Azure Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Critical,2020-06-10 10:53:53,2
13310584,[Branch-515][Azure]: Data engine creation fails on Jupyter/Rstudio/Zeppelin,"*Precondition:*

1. Jupyter is created

*Steps to reproduce:*

1. Create spark cluster

*Actual result:*

1. Spark cluster creation fails

*Expected result:*
 # Spark cluster is created

(i) On top of that Spark cluster is not created successfully on *RStudio/Zeppelin* on *Azure* (the same *mistake*). 

 ",AZURE Debian DevOps Known_issues(release2.4) pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-06-10 07:50:07,2
13310414,[Branch-515][Ternsorflow with Jupyter][AWS][GCP]: Playbook create_models runs with error,"*Preconditions:*
 # SSN is created on. AWS from branch-515
 # TensorFlow with Jupyter is created

*Steps to reproduce:*

1. Run the playbook create_models

*Actual result:*
 # The playbook runs with error

The error 'module 'tensorflow' has no attribute 'get_default_graph' appears for kernels:

- PySpark (Python-3.6 / Spark-2.4.4)

- Python 2

- Python 3

- Local PySpark (Python-3.6 / Spark-2.4.4)

The error 'No module named tensorflow' appears for kernels:

- PySpark (Python-2.7 / Spark-2.4.4)

- Local PySpark (Python-2.7 / Spark-2.4.4)

 

*Expected result:*

1. The playbook runs successful

 ",AWS Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-06-09 14:46:02,3
13310302,Add check box to 'Environment management' page,"Add possibility to stop/terminate instances via check box in 'Environment management' page.

On top of that support multiple selection.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-06-09 06:16:45,5
13309496,[Branch-515][AWS][GCP][Zeppelin]: Investigate Spark cluster Py3 for correct work,"After Playbook running Flight preparation/airbnb it was impossible to restart kernel and spark cluster termination was stuck (instances were deleted from cloud console, docker was still in executed status and py3 kernel was still available in the list).

This bug is reproduced after running AIRBNB data preparation for Py3 local/remote kernels.",AWS Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2020-06-04 17:25:08,3
13309210,[Branch-515][AWS]: Investigate why any library could not be installed for EMR from r package via DLab UI or terminal,"It was tested on AWS: Jupyter/RStudio/Zeppelin

I tried to install different libraries from r package but every library was in 'Failed' status. The same result was if install library via DLab Web UI or SSH connection on master.

Installation such libraries for the previous EMR was successful: csvread, markdoawn.",AWS Debian DevOps pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-06-03 14:39:44,2
13309209,[Branch-515][AWS]: Library installation fails from python packages on EMR,"It was tested on Jupyter/RStudio/Zeppelin

1. For EMR v. 6.0.0 library installation fails from packages Py2/Others. Library from Py3 is in 'installed' status on Dlab Web UI, docker runs with '0', but after that it is impossible to run the command  'pip3 list' on cluster (via ssh)

2. For EMR v. 5.30.0 library installation fails from packages Py2/Others. Library from Py3 is in 'installed' status on Dlab Web UI, docker runs with '0', but after that it is impossible to run the command  'pip3 list' on cluster (via ssh).

3. During this installation (from packages Py2/Others) docker runs with '1' and  library status is stuck in 'installing' 
----
*Preconditions:*
 # SSN is created from branch-515 on AWS
 # EMR is created on Jupyter/RStudio

*Steps to reproduce:*

1. Install libraries from packages Py2/Py3/Others on EMR

*Actual result:*

1. For EMR v. 6.0.0 library installation fails from packages Py2/Py3/Others

2. For EMR v. 5.30.0 library installation fails from packages Py2/Others

3. On DLab Web UI library has still status 'installing' if docker runs with '1'

*Expected result:*

1. For EMR v. 6.0.0 library installation is successful from packages Py2/Py3/Others

2. For EMR v. 5.30.0 library installation is successful from packages Py2/Others

3. On DLab Web UI library is in status 'failed' if docker runs with '1'

 ",AWS Debian DevOps pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-06-03 14:37:49,2
13309195,[Front-end]: All resources disappear after the last endpoint disconnection,"*Preconditions:*
 1. Environment is created on local endpoint

*Steps to reproduce:*
 1. Disconnect the last endpoint

*Actual result:*
 1. Environment is terminated on local endpoint
 2. All resources are disappeared in 'List of resource' page

*Expected result:*
 1. Environment is terminated on local endpoint
 2. All resources are not disappeared in 'List of resource' page",AWS AZURE Debian Front-end GCP Known_issues(release2.3) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-03 13:09:32,5
13309194,[Library management]: Filtered data are changes in some seconds ,"*Preconditions:*

1.  Any library is installed on notebook

2. Any library is installed on computational resource

3. Library management popup is opened

*Steps to reproduce:*

1.  Filter by destination or resource type or status

*Actual result:*

1. Filtered data are changes in some seconds

*Expected result:*

1. Filtered data are changes in some seconds

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-06-03 13:03:45,5
13308974,[Azure]: It is impossible to create data engine,"*Preconditions:*

1. Any notebook is created on Azure

*Steps to reproduce:*
 # Go to 'List of resource' page
 # click gear action
 # Choose 'Add compute'
 # Fill fields
 # Click 'Create' button

*Actual result:*

1. Data Engine is in creating status

*Expected result:*

1. It is forbidden to create Data Engine",AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-02 15:09:02,6
13308913,[Branch-515][AWS][Jupyter]: Playbooks flights data preparation/visualization R/Scala run with error only for Standalone Spark cluster ,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*
 # Run flights data preparation/visualization R/Scala via Spark kernel

*Actual result:*
 # Playbooks run with error ' Job aborted due to stage failure: Task 0 in stage 3.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3.0 (TID 15, 172.31.50.24, executor 1): org.apache.spark.util.TaskCompletionListenerException: Premature end of Content-Length delimited message body (expected: 43758; received: 16360'

*Expected result:*
 # Playbooks run successful

(i) *For GCP these playbooks run successful* (11/06/2020).

 ",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-06-02 09:13:12,3
13308742,[Branch-515][AWS]: JupyterLab creation fails,"*Preconditions:*

1. SSN is created from branch-515 on AWS

*Steps to reproduce:*

1. Create JupyterLab

*Actual result:*

1. JupyterLab creation fails

*Expected result:*

1. JupyterLab is created successful

 ",AWS Debian DevOps pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-06-01 14:56:51,2
13308731,Shapes for computational resources are absent,"*Preconditions:*

1. Notebook is in running status

*Steps to reproduce:*
 # Go to 'List of resource' page
 # Click 'gear' icon
 # Choose add compute

*Actual result:*

1. Shapes for computational resources are absent

*Expected result:*

1. Shapes for computational resources are present

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-01 14:10:31,6
13308726,It is impossible to start notebook,"*Preconditions:*
 # Notebook is stopped

*Steps to reproduce:*

1. Start notebook

*Actual result:*
 # Notebook is not started
 # Error message appears:  'User not authorized'

*Expected result:*

1. Notebook is in starting process",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-06-01 13:57:54,6
13308359,Limitation for notebook popup in 'Environment management' page,"# (/) Ged rid of Open following URL(s) in your browser to access this box if notebook is still in creating status in 'List of resource' page - {color:#00875a}*verified*{color}
 # (/) Do not allow to open notebook popup if notebook is still in creating/failed statuses in 'Environment management' page - {color:#00875a}*verified*{color}
 # (/) Align header values in 'Environment management' page - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-05-29 15:59:28,5
13308337,[Bucket browser]: Sets of issues with folder creation,"# Do not allow move among the folders/bucket if user does not confirm/cancel folder creation - {color:#00875a}*verified*{color}
 # If user does not confirm/cancel folder creation and closes bucket browser and after that opens bucket browser previous ability of folder creation should not be present - *{color:#00875a}verified{color}*",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-05-29 14:23:50,5
13308076,Write documentation about endpoint creation on Clouds,Write documentation about endpoint creation on Clouds in README.md in Github.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-28 16:14:56,3
13307811,[Branch-515][AWS]: RStudio creation fails,"*Preconditions:*

1. SSN is created on AWS from branch-515

*Steps to reproduce:*

1. Create RStudio

*Actual Result:*

1. Rstudio creation fails

*Expected result:*

1. RStudio creation is successful",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-05-27 17:38:45,3
13307730,Set of tasks concerning download,"1. (/) Do not allow to download the same object if it is still in the download process - {color:#00875a}*verified*{color}

2. (/) Download process should not disappear if user creates a folder during download process - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-05-27 12:59:51,5
13307279,[Branch-515][Azure]: Very often Notebook/compute creation/starting fails due to low level socket,"Very often this case is reproduced for Jupyter and RStudio. However this bug is related to all notebook templates.

The bug was reproduced during compute creation for Jupyter with TensorFlow 15/10/2020

*Preconditions:*

1. SSN is created on Azure

*Steps to reproduce:*
 # Create any notebook
 # Start notebook after previous stopping

*Actual result:*
 # Notebook creation fails
 # Notebook starting fails

*Expected result:*

1. Notebook creation is successful

2. Notebook starting is successful",AZURE Debian DevOps Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-05-25 18:16:33,2
13307275,[Branch-515][Azure][Jupyter][Rstudio][DeepLearning]: The second (via image) Jupyter creation fails ,"This bug was found for Jupyter/Rstudio/DeepLearning on Azure. Perhaps this bug is related for all notebooks.

*Preconditions:*
 # SSN is created form branch-515 on Azure
 # Image is available for Jupyter/Rstudio

*Steps to reproduce:*

1. Create Jupyter/Rstudio via image

*Actual result:*

1. Jupyter'rstudio creation fails

*Expected result:*
 # Jupyter/rstudio creation is successful

",AZURE Debian DevOps Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-05-25 17:52:19,2
13307192,[Front-end][Part5]: Set of improvements for bucket browser,"# (/) It is forbidden to create a folder if bucket is empty - {color:#00875a}*verified*{color}
 # (/) The hint about download should be close to download button - {color:#00875a}*verified*{color}
 # (/) If user has permission to view he is not able to copy path, but should be - *{color:#00875a}verified{color}*
 # (/) If create folder during upload and upload has finished before user hints yes for creation the scrollbar scrolls up and folder creation opportunity is hidden, but should not be hidden. View video in attachment - *{color:#00875a}verified{color}*
 # (/) Azure: get rid of 'Account' from bucket name and add hint for bucket name - *{color:#00875a}verified{color}*
 # (/) If cancel object upload which is in uploading stage it is forbidden to close upload grid - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-05-25 09:29:26,5
13306851,[Branch-515][Azure][RStudio]: RStudio creation fails from image,"This bug was reproduced when image for RStudio was available, so I guess Rstudio creation fails when using general image for RStudio.

*Preconditions:*
 # SSN is created on Azure from branch-515
 # Image for RStudio is available

*Steps to reproduce:*

1. Create RStudio from general image

*Actual result:*

1. RStudio creation fails

*Expected result:*

1. RStudio creation is successful",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-05-22 17:45:30,2
13306802,[Branch-515][Azure]: Deep Learning/Zeppelin/Tensorflow with Jupyter creations fail on stage of disk mount,"*Preconditions:*

1. SSN is created on Azure from branch 515

*Steps to reproduce:*

1. Create Deep Learning/Zeppelin/TensorFlow with Jupyter

*Actual result:*

1. Deep Learning/Zeppelin/TensorFlow with Jupyter creations fail

*Expected result:*

1. Deep Learning/Zeppelin/TensorFlow with Jupyter creations are successful",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-05-22 14:44:42,2
13306766,Argument validation for deployment script,"Validation should:
 * trigger in the beginning of script running
 * check all mandatory field",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-22 12:51:11,3
13306545,Limitation delay should not be if remove object from upload/queue,"*Preconditions:*
 # 11 objects are in upload stage via bucket browser

*Steps to reproduce:*
 # Delete the object which are in upload stage
 # Delete again  the object which are in upload stage

*Actual result:*
 # The object waiting for upload changes the status to uploading with delay
 # It is still forbidden to load new object

Expected result:
 # The object waiting for upload changes the status to uploading without delay
 # It is allowed to load new object",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-05-21 17:01:31,5
13306522,[Branch-515][GCP]: Available lib list is not gained on stage of  signature verification for DES,"*Preconditions:*

1. Dataproc is created

*Steps to reproduce:*
 # Go to 'List of resource' page
 # Click on gear for notebook
 # Choose manage libraries
 # Choose Dataproc in 'Select resource' dropdown list 

*Actual result:*
 # Gaining available list is still in progress from DLab Web UI
 # Docker runs with '1'

*Expected result:*

1.Gaining available list is accomplished

2. Docker runs with '0'

(i) *This bug is still reproduced* (10/06/2020).",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-05-21 15:32:47,3
13306499,[Branch-515][AWS][RStudio]: After successfully library installation from r package the installed library is not found in notebook,"*Preconditions:*

1. RStudio is created on GCP/AWS

*Steps to reproduce:*

1. Install library from r packages

*Actual result:*
 # Library is installed successfully on DLab Web UI
 # Docker runs with '0'
 # Library is not found in notebook via command R -e ""installed.packages()[,c(3:4)]"" | grep <library_name>

*Expected result:*
 # Library is installed successfully on DLab Web UI
 # Docker runs with '0'
 # Library is found in notebook via command R -e ""installed.packages()[,c(3:4)]"" | grep <library_name>

 
----
(i) *This bug is still reproduced* *on AWS* (04/06/2020)

(i) *This bug is not reproduced on GCP* (10/06/2020)",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-05-21 13:27:17,2
13306475,[Branch-515][GCP]: TensorFlow with Jupyter creation fails,"*Preconditions:*

1. SSN is created on GCP from branch-515

*Steps to reproduce:*

1. Create TensorFlow with Jupyter

*Actual result:*

1. TensorFlow with Jupyter creation fails 
what(): boost::filesystem::copy_file: No such file or directory: ""./builds/cuda-toolkit/nvml/example/README.txt"", ""/usr/local/cuda-10.1/nvml/example/README.txt""

*Expected result:*

1. TensorFlow with Jupyter creation is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-05-21 11:22:14,2
13306447,[Branch-515][GCP]: Superset creation fails,"*Preconditions:*

1. SSN is created on GCP

*Steps to reproduce:*

1. Create Superset

*Actual result:*

1. Superset creation fails

*Expected result:*

1. Superset creation is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-05-21 09:47:10,2
13306218,[Branch-515][AWS][GCP][Zeppelin] Flight data visualization Python2/R(py2/3) run with error,"*Preconditions:*
 # Zeppelin is created
 # Playbooks flight preparation Py2/R(py2/py3) was run for local/remote kernels

*Steps to reproduce:*

1. Run playbooks flight visualization Py2/R(py2/py3) for local/remote kernels

*Actual result:* 

1. Playbooks run with error 'u'Unsupported class file major version 55''

*Expected result:*

1. Playbooks run successful
----
On top of that there is a bug if run Flight visualize for R (Py2/3)

*GCP*

Python 2 visualization
||Local|*{color:#de350b}runs with error{color}* (11/06/2020)|
||DP 1.3|{color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.4|{color:#de350b}*runs with error*{color} (11/06/2020)|
||Spark|{color:#de350b}*runs with error*{color} (11/06/2020)|

 Visualization R
||Local (Py2)|{color:#de350b}*runs with error*{color} (11/06/2020)|
||Local (Py3)|*{color:#de350b}runs with error{color}* (11/06/2020)|
||DP 1.3 (Py2)|{color:#de350b}*runs with error*{color} (11/06/2020)|
|| DP 1.3 (Py3)|{color:#de350b}*runs with error* {color}(11/06/2020)|
||DP 1.4 (Py2)|*{color:#de350b}runs with error{color}* (11/06/2020)|
||DP 1.4 (Py3) |{color:#de350b}*runs with error*{color} (11/06/2020) |
||Spark (Py2)|*{color:#de350b}runs with error{color}* (11/06/2020)|
||Spark (Py3) |{color:#de350b}*runs with error*{color} (11/06/2020)|",AWS Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-05-20 15:37:22,3
13306212,[Branch-515][AWS][GCP][Zeppelin]: Playbooks flight/airbnb preparation Py2/Py3/R/Scala run with error for local/remote kernels,"*Preconditions:*

1. Zeppelin is created on AWS/GCP

*Steps to reproduce:*

1. Run playbooks flight preparation Py2/Py3/R/Scala for local/remote kernels

*Actual result:* 

1. Playbooks run with error

*Expected result:*

1. Playbooks run successful

------------------------------------------------------------------------------------------

(i) For GCP Playbook preparation fails only for airbnb see attachments
----
*AWS*

+Flight preparation Py2+
||Local|*{color:#de350b}runs with error{color}* (04/06/2020)|
||EMR 5.30.0|{color:#de350b}*runs with error*{color} (09/06/2020)|
||EMR 6.0.0|{color:#de350b}*runs with error*{color} (09/06/2020)|
||Spark|{color:#de350b}*runs with error*{color} (04/06/2020)|

 +Flight preparation R+
||Local py2|*{color:#de350b}runs with error{color}* (04/06/2020)|
||Local py3|*{color:#de350b}runs with error{color}* (04/06/2020)|
||EMR 5.30.0 py2| *{color:#00875a}verified{color}* (09/06/2020)|
||EMR 5.30.0 py3| {color:#00875a}*verified*{color} (09/06/2020)|
||EMR 6.0.0 py2| *{color:#de350b}runs with error{color}* (09/06/2020)|
||EMR 6.0.0 py3| {color:#de350b}*runs with error*{color} (09/06/2020)|
||Spark py2| {color:#de350b}*runs with error*{color} (04/06/2020)|
||Spark py3| {color:#de350b}*runs with error*{color} (04/06/2020|

+Flight preparation scala+
||Local py2|*{color:#de350b}runs with error{color}* (04/06/2020)|
||Local py3|*{color:#de350b}runs with error{color}* (04/06/2020)|
||EMR 5.30.0 py2| {color:#00875a}*verified*{color} (09/06/2020)|
||EMR 5.30.0 py3| {color:#00875a}*verified*{color} (09/06/2020)|
||EMR 6.0.0 py2| {color:#de350b}*runs with error*{color} (09/06/2020)|
||EMR 6.0.0 py3| *{color:#de350b}runs with error{color}* (09/06/2020)|
||Spark py2|{color:#de350b}*runs with error*{color} (04/06/2020)|
||Spark py3|{color:#de350b}*runs with error*{color} (04/06/2020)|

 +AIRBNB+
||Local py2|*{color:#de350b}runs with error{color}* (04/06/2020)|
||Local py3|*{color:#de350b}runs with error{color}* (04/06/2020)|
||EMR 5.30.0 py2|*{color:#de350b}runs with error{color}* (09/06/2020)|
||EMR 5.30.0 py3|{color:#de350b}*runs with error*{color} (09/06/2020)|
||EMR 6.0.0 py2|*{color:#de350b}runs with error{color}* (09/06/2020) |
||EMR 6.0.0 py3|{color:#de350b}*runs with error*{color} (09/06/2020|
||Spark py2|{color:#de350b}*runs with error*{color} (04/06/2020)|
||Spark py3|{color:#de350b}*runs with error*{color} (04/06/2020)|
----
 *GCP*

+AIRBNB+
||Local py2|{color:#de350b}*runs with error* {color} (11/06/2020)|
||Local py3|{color:#de350b}*runs with error -failed to start pyspark*{color} (11/06/2020)|
||DP 1.3 py2|{color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.3 py3|{color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.4 py2|{color:#de350b}*runs with error*{color} (11/06/2020) |
||DP 1.4 py3|{color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.5 ubuntu py2|{color:#de350b}*runs with error*{color} (16/06/2020)|
||DP 1.5 ubuntu py3|{color:#de350b}*runs with error*{color} (16/06/2020)|
||Spark py2|{color:#de350b}*runs with error*{color}  (11/06/2020)|
||Spark py3|*{color:#de350b}runs with error  -failed to start pyspark{color}* (11/06/2020)|

+Flight preparation Py2+
||Local|{color:#00875a}*verfied*{color} (11/06/2020)|
||DP 1.3|{color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.4|{color:#00875a}*verified*{color} (11/06/2020)|
||DP1.5 ubuntu|{color:#00875a}*runs with error* {color}(16/06/2020)|
||Spark|{color:#00875a}*verified*{color} (11/06/2020)|

 +Flight preparation R+
||Local py2|{color:#00875a}*verified*{color} (11/06/2020)|
||Local py3|{color:#00875a}*verified*{color} (11/06/2020)|
||DP 1.3 py2|{color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.3 py3|{color:#de350b} *runs with error*{color} (11/06/2020)|
||DP 1.4 py2|{color:#00875a}*verified*{color} (11/06/2020)|
||DP 1.4 py3|*{color:#00875a}verified{color}* (11/06/2020)|
||DP 1.5 ubuntu py2|{color:#de350b}*runs with error*{color} (16/06/2020)|
||DP 1.5 ubuntu py3|{color:#de350b}*runs with error* ** {color}(16/06/2020)|
||Spark py2| {color:#00875a}*verified*{color} (11/06/2020)|
||Spark py3|{color:#00875a}*verified* {color}(11/06/2020)|

+Flight preparation scala+
||Local py2|{color:#00875a}*verified*{color} (11/06/2020)|
||Local py3|{color:#00875a}*verified*{color} (11/06/2020)|
||DP 1.3 py2| {color:#de350b}*runs with error*{color} (11/06/2020)|
||DP 1.3 py3|{color:#de350b}*runs with error* {color}(11/06/2020)|
||DP 1.4 py2|*{color:#00875a}verified{color}* (11/06/2020)|
||DP 1.4 py3|{color:#00875a}*verified*{color} (11/06/2020)|
||DP 1.5 ubuntu py2 |{color:#de350b}*runs with erro*{color}r (16/06/2020)|
||DP 1.5 ubuntu py3|{color:#de350b}*runs with erro**r* {color}(16/06/2020)|
||Spark py2|{color:#00875a}*verified*{color} (11/06/2020)|
||Spark py3|{color:#00875a}*verified*{color} (11/06/2020)|",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-05-20 15:13:08,3
13306198,[Branch-515][AWS][GCP][Jupyter]: Playbook airbnb runs with error,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*

1. Run playbook airbnb 

*Actual result:*
 # Playbook runs with error 'attrib() got an unexpected keyword argument 'convert''

*Expected result:*

1.Playbook airbnb successful

 

(i) For GCP Sperk kerner python2 - appears Kernel error.

 *For AWS*

+Prepare data+
||Python2 (local)|{color:#de350b}error with running playbook{color}|{color:#de350b}*runs with error* {color}(02/06/2020)|
||Python3 (local)|{color:#de350b}error with running playbook{color}|{color:#00875a}*verified* {color} (02/06/2020)|
||Python2 (EMR 5.30)|{color:#4c9aff}Blocked{color}|{color:#de350b}*runs with error*{color} (02/06/2020)|
||Python3 (EMR 5.30)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified* {color} (02/06/2020)|
||Python2 (EMR 6.0.0)|{color:#4c9aff}Blocked{color}|{color:#de350b}*runs with error*{color} (03/06/2020)|
||Python3 (EMR 6.0.0)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color}  (03/06/2020)|
||Python2 (Spark)|{color:#4c9aff}Blocked{color}|*{color:#de350b}runs with error{color}* (02/06/2020)|
||Python3 (Spark)|{color:#4c9aff}Blocked{color}|{color:#de350b}*runs with error*{color} (02/06/2020)|

+Visualize data+
||Python2 (local)|{color:#00875a}*verified* {color} (02/06/2020)|
||Python3 (local)|*{color:#00875a}verified{color}*  (02/06/2020)|
||Python2 (EMR 5.30)|{color:#00875a}*verified*{color}  (02/06/2020)|
||Python3 (EMR 5.30)|{color:#00875a}*verified*{color}  (02/06/2020)|
||Python2 (EMR 6.0.0)|{color:#00875a}*verified*{color}  (03/06/2020)|
||Python3 (EMR 6.0.0)|{color:#00875a}*verified*{color}  (03/06/2020)|
||Python2 (Spark)|{color:#de350b}*runs with error* {color}(02/06/2020)|
||Python3 (Spark)|{color:#de350b}*runs with error*{color} (02/06/2020)|
----
*For GCP*

+Prepare data+
||Python2 (local)|{color:#de350b}*runs with error*{color} (11/06/2020)|
||Python3 (local)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python2 (DP 1.3)|{color:#de350b}*runs with error*{color} (11/06/2020)|
||Python3 (DP 1.3)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python2 (DP 1.4)|{color:#de350b}*runs with error*{color} (11/06/2020)|
||Python3 (DP 1.4)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python2 (DP 1.5 ubuntu)|*{color:#de350b}runs with error{color}*{color:#00875a}  {color}(16/06/2020)|
||Python3 (DP 1.5 ubuntu)|{color:#de350b}*runs with error*{color}  (16/06/2020)|
||Python2 (Spark)|{color:#de350b}*runs with error*{color} (11/06/2020)|
||Python3 (Spark)|{color:#00875a}*verified*{color} (11/06/2020)|

+Visualize data+
||Python2 (local)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python3 (local)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python2 (DP 1.3)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python3 (DP 1.3)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python2 (DP 1.4)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python3 (DP 1.4)|{color:#00875a}*verified*{color}  (11/06/2020)|
||Python2 (DP 1.5 ubuntu)|{color:#00875a}*verified*  {color}(16/06/2020)|
||Python3 (DP 1.5 ubuntu)|{color:#00875a}*verified*{color}  (16/06/2020)|
||Python2 (Spark)|*{color:#00875a}verified{color}* (11/06/2020)|
||Python3 (Spark)|{color:#00875a}*verified*{color} (11/06/2020)|",AWS AZURE Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-05-20 13:57:48,3
13306186,[Branch-515][AWS][GCP][RStudio][Rstudio with TensorFlow]: Playbook Flight_data_Preparation_R run with error for local/remote kernels,"*Preconditions:*

1. RStudio/RStudio with TensorFlow are created on AWS/GCP (only RStudio)

*Steps to reproduce:*

1. Run Flight_data_Preparation_R for local/remote kernels

*Actual result:*

1. Playbook runs with error:
 Java version 8 is required for this package; found version: 11.0.7
 *Expected result:*
 # Playbook runs successfully

+AWS+

+Flight_data_Preparation_R+ 
||R (Local)|{color:#de350b}*runs with error*{color} (4/06/2020)|
||R (EMR 5.30.0)|{color:#de350b}*runs with error*{color} (4/06/2020)|
||R (EMR 6.0.0)|*{color:#de350b}runs with error{color}* (4/06/2020)|
||R (Spark)|*{color:#de350b}runs with error{color}* (4/06/2020)|
----
+GCP+

+Flight_data_Preparation_R+ 
||R (Local)|{color:#de350b}*runs with error*{color} (10/06/2020)|
||R (DP 1.3)|{color:#de350b}*runs with error*{color} (10/06/2020)|
||R (DP 1.4)|*{color:#de350b}runs with error{color}* (10/06/2020)|
||R (DP 1.4 ubuntu)|*{color:#de350b}runs with error{color}* (15/06/2020)|
||R (DP 1.5 ubuntu)|*{color:#de350b}runs with error{color}* (15/06/2020|
||R (Spark)|*{color:#de350b}runs with error{color}* (10/06/2020)|",AWS Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-05-20 12:41:53,3
13306127,[Azure]: Show bucket name in one row in notebook name popup,"Path to bucket container@account.blob.core.windows.net

Show this path to bucket in notebook name popup, change 'container'/'account' by value.

 ",AZURE Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-05-20 09:36:46,5
13305890,[Front-end][Part5]: Set of improvements for bucket browser,"# (/) Adjust according to screenshot 'Improvements' - {color:#00875a}*verified*{color}
 # (/) Adjust according to screenshot 'Changes' - {color:#00875a}*verified*{color}
 # {color:#172b4d}(/) Move icons to the left side, because scrollbar covers cancel opportunity - {color:#00875a}*verified*{color}{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-05-19 10:59:35,5
13305885,Upload process should not decrease after increasing,"1. (/) Sometimes upload almost has reached 95% and after that starts from beginning - *{color:#00875a}verified{color}*

On top of that if upload fails the object will appear in some minutes in browser bucket and cloud console.

2. (/) After 95% add 1 percentage every 20 seconds - *{color:#00875a}verified{color}*

3. (/) After deletion the object user should be located in the bucket from what object has been deleted (view video in attachment) - {color:#00875a}*verified*{color}

4. (/)  If user uploads several objects create token and if upload is not finished but user uploads the other objects add the second token - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-05-19 10:40:03,5
13305858,Group uploading should not impact on bucket browser performance,"If objects are in uploading and in waiting for upload it is hard to open bucket browser - preloader is too long. (I have uploaded 12 objects, each object has 162 MB)",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-19 09:11:15,6
13305707,[Branch-515][AWS][GCP][Jupyter]: Playbooks Flights_data_Preparation_Python3/2 run with error,"Preconditions:

1. Jupyter is created

Steps to reproduce:

1. Run playbooks Flights_data_Preparation_Python3/2 on Jupyter for local kernel for AWS

Actual result:

1. Playbooks run with error

Expected result:
 # Playbooks run successfully

*About AWS:*

Flights_data_Preparation
||Python2 (local)|error with running playbook|*{color:#00875a}verified{color}* (01/06/2020)|
||Python3 (local)|error with running playbook|*{color:#00875a}verified{color}* (01/06/2020)|
||Python2 (EMR 5.30)|Blocked|{color:#00875a}*verified*{color} (02/06/2020)|
||Python3 (EMR 5.30)|Blocked|{color:#00875a}*verified*{color} (02/06/2020)|
||Python2 (EMR 6.0.0)|Blocked|{color:#00875a}*verified*{color} (03/06/2020)|
||Python3 (EMR 6.0.0)|Blocked|{color:#00875a}*verified*{color} (03/06/2020)|
||Python2 (Spark)|Blocked|*{color:#de350b}fails on the last block{color}* (02/06/2020)|
||Python3 (Spark)|Blocked|*{color:#de350b}fails on the last block{color}* (03/06/2020)|
----
*About GCP:*

Flights_data_Preparation
||Python2 (local)|{color:#00875a}*verified*{color}|{color:#00875a}*verified*{color} (10/06/2020)|
||Python3 (local)|{color:#00875a}*verified*{color}|*{color:#00875a}verified{color}* (10/06/2020)|
||Python2 (DP1.4)|{color:#00875a}*verified*{color}|*{color:#00875a}verified{color}* (11/06/2020)|
||Python3 (DP1.4)|{color:#00875a}*verified*{color}|*{color:#00875a}verified{color}* (11/06/2020)|
||Python2 (DP1.5 ubuntu)|- |*{color:#00875a}verified{color}* (16/06/2020)|
||Python3 (DP1.5 ubuntu)|-|*{color:#de350b}runs with error{color}{color:#00875a}{color}* (16/06/20)|
||Python2 (DP1.3)|*{color:#de350b}error with running playbook{color}*|*{color:#00875a}verified{color}* (11/06/2020)|
||Python3 (DP1.3)|*{color:#de350b}error with running playbook{color}*|*{color:#00875a}verified{color}* (11/06/2020) |
||Python2 (Spark)|{color:#00875a}*verified*{color}|*{color:#00875a}verified{color}* (11/06/2020)|
||Python3 (Spark)|{color:#de350b}*Error with connection to kernel*{color}|*{color:#00875a}verified{color}* (11/06/2020)|
----
(i) *This bug is not reproduced for GCP* (11/06/2020).",AWS Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-05-18 15:49:58,3
13305705,[Branch-515][AWS][Jupyter] :Playbooks Flights_data_Visualization_Python2/python3 run with error ,"This bug was found for local and remote (for EMR, for spark cluster did not tested, because it was a bug with spark cluster creation) kernels

*Preconditions:*

1.  Playbook Flights_data_preperation_Python2/Python3 were run on Jupyter

*Steps to reproduce:*

1. Run Playbooks Flights_data_Visualization_Python2/python3 on Jupyter

*Actual result:*

1. Playbooks run with error 'Unsupported class file major version 55'

*Expected result:*
 # Playbooks run successful

*Flights_data_Visualization (AWS)*
||Python2 (local)|{color:#de350b}error with running playbook{color}|*{color:#00875a}verified{color}* (01/062020) |
||Python3 (local)|{color:#de350b}error with running playbook{color}|{color:#00875a}*verified* {color}(01/06/2020)|
||Python2 (EMR 5.30)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (02/06/2020)|
||Python3 (EMR 5.30)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (02/06/2020)|
||Python2 (EMR 6.0.0)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (03/06/2020)|
||Python3 (EMR)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (03/06/2020)|
||Python2 (Spark)|{color:#4c9aff}Blocked{color}|*{color:#de350b}runs with error{color}* (02/06/20)|
||Python3 (Spark)|{color:#4c9aff}Blocked{color}|{color:#de350b}*runs with error* {color}(02/06/20)|

 
----
 

*Flights_data_Visualization (GCP)*
||Python2 (local)|{color:#de350b}error with running playbook{color}|*{color:#00875a}verified{color}* (10/06/2020)|
||Python3 (local)|{color:#de350b}error with running playbook{color}|{color:#00875a}*verified* {color}(10/06/2020)|
||Python2 (DP1.3)|{color:#de350b}error with running playbook - another error-view screenshot{color}|*{color:#00875a}verified{color}* (11/06/2020)|
||Python3 (DP1.3)|{color:#de350b}error with running playbook - another error -view screenshot{color}|{color:#00875a}*verified*{color} (11/06/2020)|
||Python2 (DP1.4)|{color:#de350b}error with running playbook{color}|*{color:#00875a}verified{color}* (11/06/2020)|
||Python3 (DP1.4)|{color:#de350b}error with running playbook{color}|*{color:#00875a}verified{color}* (11/06/2020)|
||Python2 (DP1.5 ubuntu)|-|*{color:#00875a}verified{color}* (16/06/2020)|
||Python3 (DP1.5 ubuntu)|- |*{color:#403294}blocked{color}*  (on prepare stage) (16/06/2020)|
||Python2 (Spark)|{color:#de350b}error with running playbook{color}|{color:#00875a}*verified*{color} (11/06/2020)|
||Python3 (Spark)|{color:#de350b}error with running playbook{color}|*{color:#00875a}verified{color}* (11/06/2020)|

 
----
(i) *This bug is not reproduced for GCP* (11/06/2020).",AWS Debian DevOps,[],DATALAB,Bug,Major,2020-05-18 15:44:07,3
13305702,[Branch-515][AWS][GCP][Jupyter]:  Connection to kernel R/Sala is not successful,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*
 # Run playbook via kernel R (Local SparkR (R-3.4.4, Spark-2.4.4) and remote EMR5.28 SparkR (R-3.4.1, Spark-2.4.4))
 # Run playbook via kernel Scala (Local Apache Toree - Scala (Scala-2.11.12, Spark-2.4.4) and remote EMR 5.28 Apache Toree - Scala (Scala-2.11.12, Spark-2.4.4) )

*Actual result:*
 # Connection to kernel R died
 # Connection to Scala stuck

*Expected result:*

1. Connection to kernel R is successful

2. Connection to Scala is successful

*For AWS*

R/Scala
||R (local)|{color:#de350b}error with connection to kernel{color}|*{color:#00875a}verified{color}* (1/06/2020)|n/a|
||R (Spark)|Blocked|{color:#00875a}*verified*{color} (2/06/2020)|n/a|
||R (EMR 5.30)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (2/06/2020)|n/a|
||R (EMR 6.0.0)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (3/06/2020)|n/a|
||Scala (Local)|{color:#de350b}error with connection to kernel {color}|*{color:#00875a}verified{color}* (1/06/2020)|n/a|
||Scala (Spark)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (2/06/2020)|n/a|
||Scala (EMR 5.30)|{color:#4c9aff}Blocked{color}|{color:#00875a}*verified*{color} (2/06/2020)|{color:#00875a}*verified*{color} (22/06/2020)|
||Scala (EMR 6.0.0)|{color:#4c9aff}Blocked{color}|*{color:#de350b}kernel dies{color}* (03/06/20)|*{color:#de350b}kernel dies{color}* (22/06/20)|
----
*For GCP*

R/Scala
||R (local)|{color:#de350b}error with connection to kernel{color}|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||R (Spark)|{color:#de350b}error with connection to kernel{color}|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||R (DP1.3)|{color:#de350b}error with connection to kernel{color}|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||R (DP1.4)|{color:#de350b}error with connection to kernel{color}|{color:#00875a}*verified*{color} (11/06/2020)|n/a|
||R (DP1.5 ubuntu)|-|{color:#00875a}*verified*{color} (16/06/2020)|n/a|
||Scala (Local)|{color:#de350b}error with connection to kernel {color}|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||Scala (Spark)|{color:#de350b}error with connection to kernel{color}|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||Scala (DP1.3)|{color:#de350b}error with connection to kernel{color}|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||Scala (DP1.4)|*{color:#00875a}verified{color}*|*{color:#00875a}verified{color}* (11/06/2020)|n/a|
||Scala (DP1.5 ubuntu)|- |{color:#de350b}*kernel dies*{color} (16/06/20)|{color:#de350b}*kernel dies*{color} (22/06/20)|
----
(i) *This bug is not reproduced for GCP* (11/06/2020).",AWS Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Critical,2020-05-18 15:30:11,3
13305698,[Branch-515][AWS]: Data engine creation fails on Jupyter,"*Precondition:*

1. Jupyter is created

*Steps to reproduce:*

1. Create spark cluster

*Actual result:*

1. Spark cluster creation fails

*Expected result:*
 # Spark cluster is created ",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-05-18 15:15:09,3
13305242,[Branch-1515][AWS][GCP]: DeepLearning creation fails,"*Preconditions:*

1. SSN is created on AWS/GCP from branch-515

*Steps to reproduce:*

1. Create Deep Learning

*Actual result:*

1. Deep Learning creation fails

*Expected result:*

1. Deep Learning is created successful",AWS Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-05-15 16:58:07,2
13305238,[Branch-515][AWS]: TensorFlow with Jupyter creation fails,"*Preconditions:*

1. DLab is deployed on AWS from branch-515

*Steps to reproduce:*

1. Create TensorFlow with Jupyter

*Actual result:*

1. TensorFlow with Jupyter creation fails

*Expected result:*

1. TensorFlow with Jupyter creation is successful 

 ",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2020-05-15 16:27:33,2
13305223,Add hint for notebook creation ,"Convey hint for '+ Create new' button if:
 * user is not assigned to any project
 * user has not any active project

On top of that '+ Create new' button should be disabled.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-05-15 15:36:47,5
13304876,Impersonate Jenkins Job for creation/termination SSN,"1. Impersonate Jenkins Job for creation/termination SSN. It should not be credentials [~omartushevskyi]. It should be like dlab-cicd user. 

2. If terminate SSN via Jenkins job v2.2-GCP the same job triggers also for v2.3-GCP, but it should not be.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-05-14 08:26:43,3
13304765,[Front-end][Part4]: Set of improvements for bucket browser,"# (/) Cut too long tree (path) in upload greed - {color:#00875a}*verified*{color}
 # (/) Folder creation process disappears if upload process has been accomplished - *{color:#00875a}verified{color}*
 # (/) Show PM or AM in grid in 'Last modified' column -  {color:#00875a}*verified*{color}
 # (/) Alter size in confirmation dialog for deletion -  {color:#00875a}*verified*{color}
 # (/) Filter by name should not be case sensitive - {color:#00875a}*verified*{color}
 # (/) Add hint 'You have not permission to open bucket' to 'Bucket browser' button 'Open bucket browser' link if user is not allowed to do that - {color:#00875a}*verified*{color}
 # (/)  Ged rid of the hint  'You have not permission to open bucket' from bucket name - *{color:#00875a}verified{color}*",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-05-13 21:12:04,5
13304762,Augment validation for folder creation,"# (/) It is allowed to create folder with forbidden symbols, not English letters and so on if after that type at least one allowed symbol - {color:#00875a}*verified*{color}
 # {color:#00875a}(/) It is forbidden to create folder with valid name if use 'Delete' button for erasing the last symbol -  *verified*{color}
 # (/) It is forbidden to create folder where name contains only one symbol - *{color:#00875a}verified{color}*",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-13 20:55:16,5
13304683,[AWS]: Object upload fails due to lack of java memory,"*Preconditions:*

1. Environment is created

*Steps to reproduce:*

1. Upload more than 500MB (it was uploaded file.zip)

*Actual result:*

1. Upload fails

*Expected result:*

1. Upload is successful",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-05-13 14:13:21,6
13304619,Upload cancel causes the second appearance of upload window,"*Preconditions:*
 # Object1 is uploaded to Folder1
 # User is in Folder1
 # The same Object1 is chosen for upload (Resolve conflicts)

*Steps to reproduce:*
 # Hit 'Close' button or close icon or beyond the upload window
 # Click on 'Upload files' button

*Actual result:*
 # Window is open for chosen new object
 # The previous 'Resolve conflicts' is open under the window for chosen object

*(/)* *Expected result:*
 # Window is open for chosen new object
 # The previous 'Resolve conflicts' is  not open under the window for chosen object
----
*Related issues:*

*(/)* 1.  Change from 'exist' to 'exists' in 'Resolve conflicts' popup",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-05-13 10:31:50,5
13304577,TensorBoard page is not available (502 error),"*Preconditions:*

1. TensorFlow is created

*Steps to reproduce:*

1. Go to TensoBoard page

*Actual result:*

1. TensorBoard page is not opened successful (502 error)

*Expected  result:*

1. TensorBoard page is opened successful",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-05-13 08:58:05,2
13304354,[Promotion page]: Minor changes in download section,"# Align values of download link
 # Add space in values of release version between number and parentheses",Front-end,['DataLab Main'],DATALAB,Bug,Minor,2020-05-12 12:41:30,5
13304349,[Front-end][Part3]: Set of improvements for bucket browser,"# (/) Increase space between uploading grid and the end of window - *{color:#00875a}verified{color}*
 # (/) 'All affected objects will be deleted.' should be on confirmation message and should not depend on amount of folders. - *{color:#00875a}verified{color}*
 # (/)  Filter by name should trigger if user clicks on 'Name' grid. It should be alike as it is implemented on 'List of resource' page - {color:#00875a}*verified*{color}
 # (/) 'Download'/'Copy path' should be highlighted if hover the mouse and simultaneously the background should be darker - {color:#00875a}*verified*{color}
 # (/) Extend Name for filter field by length - {color:#00875a}*verified*{color}
 # (/) Add padding in the right side to folder - {color:#00875a}*verified*{color}
 # (/) Too long hint should not be cut in 'upload' grid - {color:#00875a}*verified*{color}
 # (/) Shrink the sensitive zone for the first object in grid - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-12 11:58:48,5
13304283,It is impossible to do any action via bucket browser if bucket is not empty,"The bug  is reproduced if there is only one object in a bucket.

*Preconditions:*

1. Upload a file via cloud console

*Steps to reproduce:*

1. Open bucket browser

*Actual result:*

1. Bucket browser is not opened

*Expected result:*

1. Bucket browser is opened
----
*Connected issues:*
 2. If upload via bucket browser any action is not triggered after button clicking
3. Hint is not portray in upload grid",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Critical,2020-05-12 06:17:04,5
13303649,[Obsolete]: Implement queue for load process,"As user I want to have stable work SSN if several users upload simultaneously a lot of objects via bucket browser.
----
If a few users simultaneously upload a lot of objects via bucket browser the SSN will be loaded.
 So implement queue for upload process.

Github issue: [https://github.com/apache/incubator-dlab/issues/733]",AWS AZURE Back-end Debian GCP Github_issue Obsolete RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-08 14:01:40,6
13303627,[Front-end][Part2]: Set of improvements for bucket browser,"# (/) Bucket browser window should be statistic and should not change size during uploading -{color:#00875a}*verified*{color}
 # (/) Get rid of information message about successful/unsuccessful upload - {color:#00875a}*verified*{color}
 # (/) After successful upload  portray 'Uploaded' in green color - {color:#00875a}*verified*{color}
 # (/) After unsuccessful upload portray 'Failed' in red  color - {color:#00875a}*verified*{color}
 # (/) Get rid of 'Close' button - *{color:#00875a}verified{color}*
 # (/) Add 'Refresh' button - {color:#00875a}*verified*{color}
 # (/) Align '-' by the left side for folder - *{color:#00875a}verified{color}*",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-08 12:30:38,5
13303609,Restrict the amount of upload objects per user,"# (/) It should be restricted 10 objects per user, but it is 11 objects per user - {color:#00875a}*verified* {color}
 # (/) Limit up to 50 (10 - in loading, and 40 in queue) - {color:#00875a}*verified* {color}
 # (/) Convey information message if user uploads more than 50 files: 'Only the first fifty objects will be uploaded. Do you want to proceed?' - {color:#00875a}*verified* {color}
 # (/) Convey information message if user uploads the file more than 4GB: 'Only file(s) within 4 GB will be uploaded.' {color:#00875a}- *verified*{color} 
 # (/) Follow the limitation per user. If still 10 files are in uploading stage disable upload button for this user and convey the hint 'Previous upload is still in progress, please wait.' - {color:#00875a}*verified* {color}",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-08 11:22:33,5
13303552,Performance for uploading should be better,"# Uploading performance (I've checked only on AWS and GCP) [~ofuks] look at performance on GCP/AWS:

 

(i) *Uploading performance*

+AWS (30/04/2020)+
||Size||AWS||Bucket browser||
|51MB|1:35|3:15|
|51MB|1:41|0:28|
|51MB|0:33|0:25|
|51MB|0:40|0:22|
||Size||AWS||Bucket browser||
|97.2MB|0:49|0:44|
|97.2MB|1:08|0:39|
|97.2MB|1:14|0:50|
|97.2MB|0:53|0:32|
||Size||AWS||Bucket browser||
|8.6MB|0:16|0:16|
|8.6MB|0:17|0:05|
|8.6MB|0:17|0:05|
|8.6MB|0:17|0:04|

+Via VPN+
||Size||AWS||Bucket browser||
|8.6MB|0:16|0:25|
|8.6MB|4:13|0:29|
|8.6MB|0:34|0:30|
|8.6MB|0:32|0:38|

+AWS (04/05/2020)+
||Size||AWS||Bucket browser||
|51MB|1:02|1:38|
|51MB|0:32|0:28|
|51MB|0:42|0:23|
|51MB|0:28|0:21|
||Size||AWS||Bucket browser||
|97.2MB|1:20|0:34|
|97.2MB|2:44|0:36|
|97.2MB|1:21|3:25|
|97.2MB|0:56|0:43|
----
*Ticket verifying:*
 AWS (13/05/2020)
||Size||AWS||Bucket browser||
|51MB|0:29|0:16|
|51MB|0:18|0:13|
|51MB|0:44|0:12|
|51MB|0:34|0:12|
||Size||AWS||Bucket browser||
|97.2MB|1:22|0:26|
|97.2MB|2:35|0:26|
|97.2MB|0:27|1:58|
|97.2MB|0:46|0:31|

 
----
+GCP+ (30/04/2020)
||Size||GCP||Bucket browser||
|51MB|0:18|1:21|
|51MB|0:12|1:50|
|51MB|0:14|2:20|
|51MB|0:35|2:52|
||Size||GCP||Bucket browser||
|97.2MB|0:47|3:13|
|97.2MB|0:58|more than 8:00|
|97.2MB| | |
|97.2MB| | |
||Size||GCP||Bucket browser||
|8.6MB|0:03|0:04|
|8.6MB|0:03|0:03|
|8.6MB|0:03|0:04|
|8.6MB|0:03|0:08|

+Via VPN+
||Size||GCP||Bucket browser||
|8.6MB|0:05|0:08|
|8.6MB|0:04|0:08|
|8.6MB|0:06|0:17|
|8.6MB|0:04|0:05|

+GCP (04/05/2020)+
||Size||GCP||Bucket browser||
|51MB|0:21|0:19|
|51MB|0:16|0:22|
|51MB|0:15|0:16|
|51MB|0:17|0:19|
||Size||GCP||Bucket browser||
|97.2MB|0:25|1:21|
|97.2MB|0:29|1:19|
|97.2MB|0:37|1:21|
|97.2MB|0:30|0:42|
----
Ticket verifying

+GCP+ (13/05/2020)
||Size||GCP||Bucket browser||
|51MB|0:12|0:17|
|51MB|0:14|0:16|
|51MB|0:14|0:16|
|51MB|0:14|0:15|
||Size||GCP||Bucket browser||
|97.2MB|0:22|0:23|
|97.2MB|0:18|0:23|
|97.2MB| 0:20| 0:25|
|97.2MB| 0:22| 0:26|
||Size||GCP||Bucket browser||
|543MB|2:03|1:26|
|543MB|2:02|1:32|",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-08 06:13:17,6
13303455,Uploading time should be pulled from user PC setting,"It is not time of user PC setting, the difference is 3 hours  between user time and server time

",AWS AZURE Back-end Debian GCP RedHat,[],DATALAB,Task,Minor,2020-05-07 17:16:52,6
13303400,[Azure]: Convey information message if user creates new folder via bucket browser,"For AWS and GCP it is allowed to create empty folder via Cloud console.

But for Azure it is forbidden to create empty folder from cloud console.

If user creates folder  (via cloud console) and does not upload  any object to it the folder will disappear.

So during folder creation (vi bucket browser) convey information message only for Azure: 'If you do not upload any object to the folder, this folder will be removed on MS Azure'",AZURE Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-07 13:33:44,5
13303341,Add possiblity to interrupt (cancel) upload during uploading stage,"# Change icon  for cancel upload from 'x' to 'bin'
 # Do not allow to close general upload grid if upload is in progress",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-05-07 10:05:00,5
13303119,[Front-end]: Augment bucket browser functionality,"# (/) Too long file name should not cut folder tree (at the left section) – *{color:#00875a}verified{color}*
 # (/) Add hint to folder/file name - {color:#00875a}*verified*{color}
 # (/) The hint should appear above the pointed object. Align hint by horizontal *{color:#00875a}- verifie{color}{color:#00875a}d{color}*. For upload we have another ticket 1773
 # (/) It is impossible to uncheck folder/file - {color:#00875a}*verified*{color}
 # (/) If upload several objects simultaneously the same (one) size appears in grid after uploading for all objects - {color:#00875a}*verified*{color}
 # (/) Align 'Bucket browser' 'Add file' buttons according to grid by horizontal in the left side - {color:#00875a}*verified*{color}
 # (/) Shrink distance inside of folder tree by horizontal - *{color:#00875a}verified{color}*
 # (/) The same file is not added to the other folder after previous uploading - *{color:#00875a}verified{color}*
 # (/) Rename confirmation dialog from 'Delete objects' to 'Delete' - {color:#00875a}*verified*{color}
 # (/) In header rename 'Objects' to 'Object' - {color:#00875a}*verified*{color}
 # (/) Align items in 'Object' column by the left side - {color:#00875a}*verified*{color}
 # (/) Add 'Actions' button. Actions button should contain 'Download', 'Copy path' - {color:#00875a}*verified*{color}
 # (/) If bucket is deleted (for example the project bucket during project termination) convey the information: The bucket <name_bucket> is deleted. To start working with data choose available bucket. - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-05-06 14:01:32,5
13302865,[Back-end]: Set of improvements for bucket browser,"# (/) Add possibility to delete folder.
 # -Convey confirmation message for object deletion. Convey grid of folders and objects which will be deleted. Add: 'All affected objects will be deleted. Do you want to proceed?' and add 'No', 'Yes' buttons.  The header of confirmation message 'Delete object(s)'. If user does not delete any folder convey error message 'These objects will be deleted. Do you want to proceed?'-
 # -Add confirmation message if upload the file with exciting name like it is on GCP-
 # (/) Multiple support for upload
 # (/) Multiple support for deletion
 # -Multiple support for download-
 # -{color:#172b4d}Make breadcrumb clickable{color}-
 # (/) 'Download'/'Delete' buttons should be white and should be disabled and if user selects files the buttons should be enabled. Move these buttons on the top of popup and on the bottom leave 'Close' button.
 # (/) The previous uploading process should not disappear if user upload another object or does not cancel and only closes the window or goes to another folder.
 # -Add filter by name-",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2020-05-05 12:50:31,6
13302130,[Front-end]: Support localization ,"As user I want to have formats for dates, currency which are used in my time zone.
----
Using proper local formats for dates, currency for all DLab.

Github issue: [https://github.com/apache/incubator-dlab/issues/732]",AWS AZURE Debian GCP Github_issue RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-04-30 15:17:46,5
13302125,[Bucket browser]: Investigate why it is impossible to review video via cloud console,If type is not conveyed it is impossible to view video from Cloud console (I've checked on AWS/GCP),AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-30 14:54:09,6
13302040,[Back-end]: Set of issues for bucket browser,"# -Allow bucket operation if bucket is empty-   - *{color:#00875a}verified{color}*
 # If delete the last file in folder the folder deletes (if folder has been created via DLab bucket browser) and the folder appears as a file (if folder has been created from Cloud console) {color:#00875a}*verified*{color}
 # -Allow to create empty folder (after creation the folder disappears if nothing is added to the folder).-  - {color:#00875a}*verified*{color}
 ## -Make 'Save' button and add 'Cancel' button and made the option mandatory--
 ## -Do not allow to create folder using not English letter and with the same name, (special characters: <>, <_>, <.> should be allowed - add error message) ? From *-GCP* console it is forbidden </>, <"">, <\>. For *AWS* console it is forbidden </>. For *Azure* console it is forbidden <#>, <?>, </>, <\>. So from DLab UI side do not allow to create using not English letter-- {color:#00875a}*verified*{color}
 # -Allow to manage objects if file or folder contain space- *{color:#00875a}verified{color}*
 # -Do not highlight object if over mouse the file in case if download and deletion are forbidden- *{color:#00875a}verified{color}*
 # -Cut object name if name is too long- *{color:#00875a}verified{color}*
 # -If check off object and than click another folder the 'Download'/'Delete' buttons should be disabled- *{color:#00875a}verified{color}*
 # -After clicking upload  the button 'Upload' should be disabled- *{color:#00875a}verified{color}*
 # -'x' icon should be red if over the mouse during upload and should cancel uploading- *{color:#00875a}verified{color}*
 # -Add header: Name/Size/Last modified-{color:#00875a} *verified*{color}
 # -Do not convey to user size and date/time for folder, instead of it portray <-> symbol-{color:#00875a}*verified*{color}

 
  
  
  ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-30 09:32:11,6
13302022,[Front-end]: Set of issues for bucket browser,"# (/) Allow bucket operation if bucket is empty - {color:#00875a}*Verified*{color}
 # (/)  (06/05/20)(BA)If delete the last file in folder the folder deletes (if folder has been created via DLab bucket browser) and the(/) folder appears as a file (if folder has been created from Cloud console) - {color:#00875a}*Verified*{color}
 # (/)  (06/05/20) (BA)Allow to create empty folder (after creation the folder disappears if nothing is added to the folder). - {color:#00875a}*Verified*{color}
 # (/) Make 'Save' button and add 'Cancel' button and made the option mandatory - {color:#00875a}*Verified*{color}
 # (/) Do not allow to create folder using not English letter and with the same name, (special characters: <->, <_>, <.> should be allowed - add error message) ? From *GCP* console it is forbidden </>, <"">, <\>. For *AWS* console it is forbidden </>. For *Azure* console it is forbidden <#>, <?>, </>, <\>. So from DLab UI side do not allow to create using not English letter -*{color:#00875a}verified{color}* - {color:#de350b}it is not allowed to create with{color} <(>, <)>, <,>, <.>, <]>. For it we have another ticket 1773
 # (BA)Allow to manage objects if file or folder contain space -{color:#00875a}*Verified*{color}
 # (/) Do not highlight object if over mouse the file in case if download and deletion are forbidden - {color:#00875a}*Verified*{color}
 # (/)  Cut object name if name is too long -{color:#00875a} *Verified*{color}
 # (/) If check off object and than click another folder the 'Download'/'Delete' buttons should be disabled - {color:#00875a}*Verified*{color}
 # (/) After clicking upload  the button 'Upload' should be disabled - {color:#00875a}*Verified* {color}
 # (/) 'x' icon should be red if over the mouse during upload - {color:#00875a}*Verified*{color}
 # (/) Add header: Name/Size/Last modified - {color:#00875a}*Verified*{color}
 # (/) Do not convey to user size and date/time for folder, instead of it portray <-> symbol - {color:#00875a}*Verified*{color}
 # (/) In the left section make smaller distance (by vertical) between folders - {color:#00875a}*Verified*{color}
 # (/) Get rid of the hint 'No file chosen' during uploading - {color:#00875a}*Verified*{color}

 
  
  ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-04-30 08:28:04,5
13302005,Bucket browser permission does not trigger as it is selected,"*Preconditions:*
 # User1 is not allowed to delete object from bucket
 # The other permission are allowed for bucket

*Steps to reproduce:*

1.  Go to ** bucket browser

2. Select the object

*Actual result:*

1. User is allowed to delete the object

*Expected result:*
 # User is not allowed to delete the object

This bug is reproduced with any point (Not only delete bucket) for bucket permission.

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-30 07:00:41,6
13301887,[Front-end]: Set of improvements for bucket browser,"# (/) Add possibility to delete folder. - {color:#00875a}*verified*{color}
 # (/) Convey confirmation message for object deletion. Convey grid of folders and objects which will be deleted. Add: 'All affected objects will be deleted. Do you want to proceed?' and add 'No', 'Yes' buttons.  The header of confirmation message 'Delete object(s)'. If user does not delete any folder convey error message 'These objects will be deleted. Do you want to proceed?' - {color:#00875a}*verified*{color}
 # (/) Add confirmation message if upload the file with exciting name like it is on GCP -  {color:#00875a}*verified*{color}
 # (/) Multiple support for upload/deletion/ (-) download - it was discussed multiple support only for upload and deletion. - {color:#00875a}*verified*{color}
 # {color:#172b4d}(/) Make breadcrumb clickable - {color:#00875a}*verified*{color}{color}
 # (/) 'Download'/'Delete' buttons should be white and should be disabled and if user selects files the buttons should be enabled. Move these buttons on the top of popup and on the bottom leave 'Close' button. - {color:#00875a}*verified*{color}
 # (/) The previous uploading process should not disappear if user upload another object or does not cancel and only closes the window or goes to another folder.  *{color:#00875a}- verified{color}*
 # (/) Add filter by name *{color:#00875a}- verified{color}*
 # (/) Add auto-focus during folder creation *- {color:#00875a}verified{color}*
 # (/) Add auto-focus during uploading process -{color:#00875a}*verified*{color}
 # (/) It is impossible to create folder if bucket is empty, but should be - {color:#00875a}*verified*{color}
 # (/) It is impossible to create folder if name contains <(>, <)>, < ]>, <,>, <.> but should be - {color:#00875a}*verified*{color}
 # (/) Add hint for folder in the left section - {color:#00875a}*verified*{color}
 # (/) Hint should be above the pointed object in upload grid. Also align hint - *{color:#00875a}verified{color}*
 # (/) Reduce distance between rows in grid by vertical in confirmation message for deletion - {color:#00875a}*verified*{color}
 # (/) Add  the same icon for all types of  files - {color:#00875a}*verified*{color}
 # (/) Alter text in confirmation message for deletion to: 'The object(s) will be deleted.' - {color:#00875a}*verified*{color}
 # (/) Add hint for confirmation popup for upload (resolve conflicts) - {color:#00875a}*verified*{color}
 # (/) Cut too long name in confirmation dialog for upload (conflicts) - {color:#00875a}*verified*{color}
 # (/) Add padding to name in the right side in confirmation dialog for deletion - {color:#00875a}*verified*{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-04-29 18:20:07,5
13301495,[Library management]: Add scrollbar for error message,If there is too large error message for failing library add vertical scrollbar.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-28 09:07:37,5
13301485,Set of minor improvements for 'Billing report' page,"As user I want to filter resources by endpoint and cloud on billing report page so that I can distinguish charges by clouds and endpoints.
 # (/) Rename 'Environment name' -> 'Resource name'
 # (/) 'Environment name' should be sticky and always present while scrolling right/left
 # Add possibility to filter resource by endpoint and cloud
 # Investigate if it is impossible to shrink distance between SBN and its value in 'Billing report' page",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-28 08:31:20,0
13301295,[Front-end]: Convey Notebook links of other users to administrator,"As an admin I want to see user notebook links, so that I can easily go to users Notebook by demand.
----
Notebook links are portrayed only for own resources. Administrator does not know the Notebook link of other user.

So convey notebook links to administrator in 'Environment management' page:
 # Administrator can view  links of another user
 # User is not able to view links of the other user

Github issue: [https://github.com/apache/incubator-dlab/issues/729]",AWS AZURE Debian Front-end GCP Github_issue RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-27 14:47:25,5
13301281,Administration role could not be edited for already existing group,"*Preconditions:*
 # User1 from group1 is logged as super admin
 # User2 from group2 has administration operation

*Steps to reproduce:*
 # Remove all administration operation for user2

*Actual result:*
 # Operation for super admin is still checked off for user2

*Expected result:*
 # Operation for super admin is unchecked for user2

 ",AWS AZURE Debian Front-end GCP Known_issues(release2.3) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-04-27 13:42:00,5
13301247,[Front-end]: Add option turn on/off billing to administration page,"As a customer I want to have possibility to turn on/off billing, so that I can manage this option by demand.

 

Add possibility to turn on/off billing even for own resources.

For administration page for role add: 'View full billing report for currently logged in use'.

So billing consists of:
 # View full billing report for all users
 # View full billing report for currently logged in user

If user does not select any option - billing is disabled -> 'Billing report' page is not available

If user selects 'View billing report for all users' automatically 'View full billing report for currently logged in user' is selected as well.

If user selects 'View full billing report for currently logged in user' only this item is checked off and billing is available only for own resource.

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-27 12:00:28,0
13301231,Set of task for administration page from UI side,"# Change 'Name' -> 'Type' in the header ('Environment management' page)
 # Add padding in the right side for grid values
 # Comment (Prevent) 'Use shared image' from UI side",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-27 11:08:00,5
13300773,[Compute] Write playbook for  GPU usage,"*Preconditions:*

1. Data Engine is created with shape for GPU

*Steps to reproduce:*

1. Run code that use GPU for Data Engine Service

*Actual result:*

1. GPU is not used

*Expected result:*

1. GPU is used
----
Code which use GPU
 1. [https://weeraman.com/put-that-gpu-to-good-use-with-python-e5a437168c01]
 2. [https://www.geeksforgeeks.org/running-python-script-on-gpu/]
----
(i) Spark cluster creation uses notebook image, so if notebook has been created not on GPU the Spark cluster will be created not on GPU as well, even if you use GPU size for creation.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-24 10:27:37,2
13300753,[DevOps]: Implement audit by DLab level,"As user I want to see history changes, so that I can easily find out what/when/who has made changes.

----


Possibility to find out:

Who has deleted or added user?
Who has created notebook/compute? 
Who has stopped/started/terminated notebook/compute/project?
Who has edited group/project/notebook/compute?
Who has gone to links?
When changes have been made
What changes have been made?
In general everything that has been done via DLab UI should be in history of changes.

History by changes should be in the following consistency:

user → time→ action.

This functionality should be conveyed to a new tab 'Audit'.

{color:#DE350B}Add bell/information icon, where user can view history changes in 'list of resource' page.{color}",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,New Feature,Major,2020-04-24 09:15:35,3
13300741,Automate FQDN during project creation,"As a customer I want to use FQDN instead of IP.
----
FQDN should be assigned during project creation.

Github issue: [https://github.com/apache/incubator-dlab/issues/736]",AWS AZURE Debian DevOps GCP Github_issue RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-24 08:34:33,3
13300740,[Front-end]: Implement audit by DLab level,"As user I want to see history changes, so that I can easily find out what/when/who has made changes.

Possibility to find out:
 * Who has deleted or added user?
 * Who has created notebook/compute? 
 * Who has stopped/started/terminated notebook/compute/project?
 * Who has edited group/project/notebook/compute?
 * Who has gone to links?
 * When changes have been made
 * What changes have been made?

In general everything that has been done via DLab UI should be in history of changes.

History by changes should be in the following consistency:

user → time→ action.

This functionality should be conveyed to a new tab 'Audit'.

{color:#de350b}Add bell/information icon, where user can view history changes in 'list of resource' page.{color}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,New Feature,Major,2020-04-24 08:33:57,5
13300739,[Front-end]: Support access to browser bucket via administration page,"As an admin I want to set permissions to bucket browser per particular users/groups, so that I can distinguish who is able to read/upload/download objects via bucket browser.

*Now we have:*

'Bucket browser':
 * Allow to delete object from the bucket
 * Allow to download object from the bucket
 * Allow to upload object to the bucket
 * Allow to view object in the bucket

*But change to:*

'Bucket browser actions'
 * Allow to delete object via bucket browser
 * Allow to download object via bucket browser
 * Allow to upload object via bucket browser
 * Allow to view object via bucket browser

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-24 08:26:18,5
13300738,[Back-end]: Support access to browser bucket via administration page,"As an admin I want to set permissions to bucket browser per particular users/groups, so that I can distinguish who is able to read/upload/download objects via bucket browser.

*Now we have:*

'Bucket browser':
 * Allow to delete object from the bucket
 * Allow to download object from the bucket
 * Allow to upload object to the bucket
 * Allow to view object in the bucket

*But change to:*

'Bucket browser actions'
 * Allow to delete object via bucket browser
 * Allow to download object via bucket browser
 * Allow to upload object via bucket browser
 * Allow to view object via bucket browser

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-24 08:25:44,6
13300720,It is forbidden to upload more than 50 MB into Bucket,"*Preconditions:*
1. Environment is created

*Steps to reproduce:*
1. Upload into bucket more than 50 MB

*Actual result:*
1. The data are not uploaded

*Expected result:*

1. The data are uploaded
-----
Allow to upload 4096 MB.
",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-04-24 06:23:06,3
13300719,[Create analytical tool]: Get rid of category name if there isn't any instance size in such category,"If do not allow for user to create some instance shapes in some category the category name is still available on notebook creation popup.

",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-04-24 06:16:00,5
13300552,???Support audit by Linux Machine level,"# Implement Squid/NAT audit (network activity monitoring). Perhaps it should be stored in Edge node and only admin can view them after edge connection via ssh. (i)Devops should  discuss.
 # Implement daemon auditd in order to receive audit report for troubleshooting. You can read about auditd in [https://manpages.ubuntu.com/manpages/xenial/en/man8/auditd.8.html#:~:text=auditd%20is%20the%20userspace%20component,the%20ausearch%20or%20aureport%20utilities.&text=The%20audit%20daemon%20itself%20has,admin%20may%20wish%20to%20customize]  (i) Devops should  discuss.",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,New Feature,Minor,2020-04-23 12:20:45,2
13300536,Distinguish DataLab and project shared image,"If admin creates or edits a project he can choose:
 * to use DataLab shared image 
 * to use project shared image.

Now we have only one 'use shared image', that means to use DataLab shared image.

Two radio buttons should be on DataLab UI:
 * use DataLab shared image
 * use project shared image",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Trivial,2020-04-23 11:51:40,0
13300535,[Back-end]: Add option turn on/off billing to administration page,"As a customer I want to have possibility to turn on/off billing, so that I can manage this option by demand.

Add possibility to turn on/off billing even for own resources.

For administration page for role add: 'View full billing report for currently logged in use'.

So billing consists of:
 # View full billing report for all users
 # View full billing report for currently logged in user

If user does not select any option - billing is disabled -> 'Billing report' page is not available

If user selects 'View billing report for all users' automatically 'View full billing report for currently logged in user' is selected as well.

If user selects 'View full billing report for currently logged in user' only this item is checked off and billing is available only for own resource.

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-23 11:34:15,1
13300520,[Front-end]: Support library installation of particular version from DLab UI ,"As user I want to install particular version of library, so that I can easily upgrade or downgrade the library by demand.
 +How it works now:+
 # If library is already installed on instance (this library was previous installed during notebook creation) and user installs the same library via DLab UI. And as result DLab shows installing -> installed, but in fact this library is not installed by DLab, Dlab finds that such library is previously installed and changes status installing -> installed. This case is actual when user want to install upper version of library. So make possible to install library like <library_name==version> via DLab UI. For example, this bug was found with library 'request'.
 # Also if update Python 3 via terminal it shows that it is upgraded but in some minutes it is downgraded again. (It was on Jupyter)
----
*How should it work:*

 - If user types wrong library version the output appears where there is a list of versions for this library
 - If user types right library version this library version should be installed
 - On top of that what dependencies are added during installation should be conveyed to user (?) {color:#de350b}I{color}{color:#de350b}n{color} what way it should be conveyed?
 - The latest installed library should be in the top of the libraries grid.

----
*Issues:*
 - Do not allow to choose library for cluster installation if available lib list is not gained for cluster.

 
----
*Update1:*

1. Template for r package installation <name_lib>, version=<x.x.x>

2. Template for apt/yum installation <package name>=<x.x.x>

3. Template for python installation <name_lib>==<x.x.x>

4. If users types only lib_name with == or = show information about available versions

5. If users types only lib_name without == or = then it is installed the latest version

 

 

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-04-23 10:14:23,5
13300517,[Back-end]: Support library installation of particular version from DLab UI ,"As user I want to install particular version of library, so that I can easily upgrade or downgrade the library by demand.

+How it works now:+
 # If library is already installed on instance (this library was previous installed during notebook creation) and user installs the same library via DLab UI. And as result DLab shows installing -> installed, but in fact this library is not installed by DLab, Dlab finds that such library is previously installed and changes status installing -> installed. This case is actual when user want to install upper version of library. So make possible to install library like <library_name==version> via DLab UI. For example, this bug was found with library 'request'.
 # Also if update Python 3 via terminal it shows that it is upgraded but in some minutes it is downgraded again. (It was on Jupyter)
----
*How should it work:*

 - If user types wrong library version the output appears where there is a list of versions for this library
 - If user types right library version this library version should be installed
 - On top of that what dependencies are added during installation should be conveyed to user (?) {color:#de350b}In what way it should be conveyed?{color}
 - The latest installed library should be in the top of the libraries grid.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-04-23 10:09:09,6
13300515,[DevOps]: Support library installation of particular version from DLab UI ,"As user I want to install particular version of library, so that I can easily upgrade or downgrade the library by demand.

+How it works now:+
 # If library is already installed on instance (this library was previous installed during notebook creation) and user installs the same library via DLab UI. And as result DLab shows installing -> installed, but in fact this library is not installed by DLab, Dlab finds that such library is previously installed and changes status installing -> installed. This case is actual when user want to install upper version of library. So make possible to install library like <library_name==version> via DLab UI. For example, this bug was found with library 'request'.
 # Also if update Python 3 via terminal it shows that it is upgraded but in some minutes it is downgraded again. (It was on Jupyter)
----
*How should it work:*

 - If user types wrong library version the output appears where there is a list of versions for this library
 - If user types right library version this library version should be installed
 - On top of that what dependencies are added during installation should be conveyed to user. (?) {color:#de350b}In what way it should be conveyed?{color}
 - The latest installed library should be in the top of the libraries grid.  ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2020-04-23 10:08:08,2
13300028,SSO is not available for Superset,"*Preconditions:*
1. Superset is created

*Steps to reproduce:*
1. Go to links of Superset

*Actual result:*
1. Links are not opened successfully 

*Expected result:*
1. Links are opened successfully ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-04-21 16:37:19,3
13299894,[Back-end]: Convey Notebook links of other users to administrator,"As an admin I want to see user notebook links, so that I can easily go to users Notebook by demand.
----
Notebook links are portrayed only for own resources. Administrator does not know the Notebook link of other user.

So convey notebook links to administrator in 'Environment management' page:
 # Administrator can view  links of another user
 # User is not able to view links of the other user

Github issue: [https://github.com/apache/incubator-dlab/issues/729]",AWS AZURE Back-end Debian GCP Github_issue RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-21 07:23:46,6
13299170,Billing is not available on environment for video-demo,"Go to environment for video-demo and find out why billing is absent?
After that fix it on the environment.
",AWS Back-end Debian,['DataLab Main'],DATALAB,Bug,Minor,2020-04-17 16:34:17,6
13299124,Total value should be in 'cost' column in file.csv,Total value should be in 'cost' column in file.csv after exporting data from billing report,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-04-17 12:51:45,6
13299107,Filter by options 'all'/'none' do not trigger in 'Billing report' page,"*Preconditions:*

1. Billing is available

*Steps to reproduce:*
 # Go to billing report page
 # Filter by 'all' or 'none'

*Actual result:*

1. Filter does not trigger

*Expected result:*

1.Filter  triggers

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-17 11:53:11,5
13299102,Sort by empty fields triggers not from the first time,"*Preconditions:*

1. Billing is available

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Sort column where is empty field

*Actual result:*

1. Empty fields are not sorted

*Expected result:*

1. Empty fields are sorted

 

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2020-04-17 11:16:00,5
13299033,[Front-end]: Support possibility to add custom bucket,"*On notebook name popup:*
 # *(/)* Bucket name should be not clickable - {color:#00875a}*verified*{color}
 # (/) If hover mouse bucket name the copy icon should appear -{color:#00875a} *verified*{color}
 # (/) If user clicks copy icon the bucket name is copied -  {color:#00875a}*verified*{color}
 # (/) Instead of information message about copy convey hint 'Copied' - {color:#00875a}*verified*{color}
 # (/) Shared endpoint bucket is not copied if user clicks copy icon - *{color:#de350b}does not work -  the point is obsolete{color}*
 # *(/)* Get rid of 'Shared endpoint bucket' from notebook name popup -{color:#00875a}*verified*{color}
 # *(/)* Add clickable link - 'Open bucket browser' - {color:#00875a}*verified*{color}
 # (/) If user clicks on 'Open bucket browser' the bucket browser popup shows up - {color:#00875a}*verified*{color}
 # (/) Upload process should end by 95% - {color:#00875a}*verified*{color}
 # (/) Add button 'Bucket browser' in 'list of resource' page - *{color:#00875a}verified{color}*
 # (/) Bucket tree location in bucket browser (*description view under) - {color:#00875a}verified{color}
 # (/) Change  in hint from 'You have not any buckets' to 'You have not any bucket' - {color:#00875a}*verified*{color}
 # (/) If use 'Bucket browser' button and upload to the second created project the auto-switching triggers after uploading and locate the user in the firs bucket. The same situation is if go to bucket browser via notebook name popup (Project2) and upload in the other bucket ((project1) - {color:#00875a}*verified*{color}
 # (/)  Add space after Project name and before parentheses - {color:#00875a}*verified*{color}
 # (/) Align preloader by vertical - {color:#00875a}*verified*{color}
 # (/) Do not convey project bucket if project is in terminated/terminating/failed statuses - {color:#00875a}*verified*{color}

*Bucket tree location in bucket browser
 How to choose buckets:
 Appropriate bucket should appear after choosing endpoint and project in one dropdown list:
 * Endpoint1 ProjectA
 * Endpoint1 ProjectB
 * Endpoint2 ProjectA
 * Endpoint3 ProjectC

If user has only one endpoint and one project do not convey dropdown list  or expanding menu?
 *The location*
 +Variant1+
 Add dropdown list under 'Upload files' and above 'Bucket path'.
 +Variant2+
 Add expanding menu in the left section with arrows to the left and to the right sides,  where user can choose endpoint. If user clicks on right arrows the right sector is hiding and the left sector is expanding. 
 *The variant2 was agreed.* 
 The tree view also should be in the left section.

 ",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-04-17 09:35:44,5
13299030,For notebook based on GPU allow only GPU computational resource and vice-versa,"For example, for Jupyter do not convey GPU shape for computational resource.

For Tensor do not convey CPU shape for computational resource.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-04-17 09:34:06,5
13299009,[Branch-1571][Azure]: Billing is not updated in 'Billing report' page,"*Preconditions:*
1. Project is created on local endpoint (Azure)
2. Billing is available on cloud console

*Steps to reproduce:*
1. Go to billing report page

*Actual result:*
1. Billing is not updated

*Expected result:*
1. Billing is updated",AZURE Debian Front-end RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-17 08:30:36,5
13298833,[Azure]: Instance in  billing combine as one point,"As a user I want to have clear vision what instances and what statuses do they have in 'Billing report' page.
----
For example, in 'billing report' cost edge consists of three points:
 - Bandwidth
 - Virtual Network
 - Virtual Machine.
 And also all three points have statuses. It confuses a user. Because if only one edge (instance) is running, but in billing report it shows as three instances are running.

Github issue: [https://github.com/apache/incubator-dlab/issues/738]",AZURE Back-end Debian Github_issue RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-16 14:00:32,1
13298813,[Branch-1571]: Local billing is not available for Azure,"*Preconditions:*
1. Billing is available for local endpoint on Azure console

*Steps to reproduce:*
1. Go to 'Billing report' page

*Actual result:*
1. Billing is not available

*Expected result:*
1. Billing is available
",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-04-16 13:23:50,3
13298790,"[Back-end]: Possibility to filter by cloud/endpoint in ""Billing report"" page",As a user I want  to filter resources by endpoint and cloud on billing report page so that I can distinguish charges by clouds and endpoints.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2020-04-16 11:37:42,1
13298769,Template name for 'Jupyter with TensorFlow 1.8.0' should be alike for all cloud,But for Azure we have 'TensorFlow 1.8.0'. Please rename on Azure.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-04-16 10:37:48,3
13298761,[Branch-1571]: Get rid of the information that Super_Admin is not assigned to any project,If 'Allow to execute administration operation' is checked off do not inform users of this group that they are not assigned to any project.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-16 10:03:44,5
13298757,Prevent 'Cloud Endpoint API' and Apache Zeppelin Ungit link from DLab UI side,"Comment 'Cloud Endpoint API' and Apache Zeppelin Ungit link.

So user should not see these information in release 2.3.

Do it when branch for release 2.3 will be ready.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-16 09:53:29,5
13298746,[Branch-1571][Billing report]:Grid is broken if cell contains two values in file .csv,"*Preconditions:*
1. Billing is available for DES

*Steps to reproduce:*
1. Export billing
2. Open file

*Actual result:*
1. Grid is broken

*Expected result:*
1. Grid is not broken

",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-16 09:17:33,6
13298743,[Branch-1571][Billing report]:Get rid of user column if user views only own resources,"If user is allowed only to view his own report, please remove user column form file.csv after billing export.",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-16 09:06:42,6
13298590,Investigate why we have reached  the maximum number of route tables,"As a variant route table leaves after termination or failing.
 Please check it.

(i) Rout table is not delete during edge termination for external endpoint. Please, fix it.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-04-15 14:42:45,2
13298523,[Branch-1571]: Image is present neither in DataLab report nor in S3 (file.csv),"*Preconditions:*
 1. Environment is created on AWS
 2. Shared image per all cloud/shared image per project/custom image are created
 3. Billing is available

*Steps to reproduce:*
 1. Go to ""Billing report"" page
 2. Go to S3 bucket

*Actual result:*
 1. Images are absent in billing report
 2. Images are absent in S3 bucket

*Expected result:*
 1. Images are present in billing report
 2. Images are present in S3 bucket",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-15 10:15:12,3
13298372,[Branch-1571]: Administrative permissions disappear after disconnect the last endpoint,"*Preconditions:*
1. Environment is created on local endpoint

*Steps to reproduce:*
1. Disconnect the last endpoint

*Actual result:*
1. Environment is terminated on local endpoint
2. Administrative permissions disappear

*Expected result:*
1. Environment is terminated on local endpoint
2. Administrative permissions  do not disappear",AWS AZURE Back-end Debian GCP Known_issues(release2.3) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-14 18:49:05,6
13298362,[Branch-1571]: There are double values in 'Instance size' column in 'Billing report' page,"*Preconditions:*
1. Billing is available for remote AWS/Azure and local GCP

*Steps to reproduce:*
1. Go to 'Billing report' page

*Actual result:*
1. There are double values in 'Instance size' column

*Expected result:*
1. There are not double values in 'Instance size' column

-----
There is different behavior during sorting if there is no value.
",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-14 18:17:56,5
13298354,[Branch-1571]: Detailed billing and billing report differ a little bit,"*Preconditions:*
 # Project is created for remote AWS/Azure and local GCP endpoints
 # Billing are available

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Go to detailed billing
 # Compare the same charges on both billings

*Actual result:*

1. Belling report is a little bit higher than detailed billing

*Expected result:*

1. Billing report and detailed billing are equal 

-----
For example:
1. Azure:
Jupyter + Spark = 1.12 (detailed billing)
Jupyter + Spark = 1.16 (billing report)
2. AWS:
Jupyter + Spark + EMR = 1.13 (detailed billing)
Jupyter + Spark + EMR = 1.16 (billing report)
 ",AWS AZURE Back-end Debian GCP Known_issues(release2.3) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-04-14 17:46:00,4
13298349,[Branch-1571]: Issues with billing export,"*Preconditions:*

1. Billing is available for remote AWS/Azure and local GCP endpoint

*Step to reproduce:*
 # Export billing via DLab UI
 # Open file

*Actual result:*
 # Status column is absent in exporting file
 # Instance size is absent in exporting file
 # Value of total is absent
 # All values in ""Resource type"" are in upper case

*Expected result:*
 # Status column is present in exporting file
 # Instance size is present in exporting file
 # Value of total is present in exporting file
 # Only the first letter of the word is in upper case in  ""Resource type"" column
----
[~ofuks] 1. Should we add user column during export if user is allowed only to view his billing?
2. Should we export sorting data if they are sorted  in Billing report? Because now they are exported without sorting.

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-14 17:14:06,6
13298345,[Branch-1571]: Filter does not work by Status/Instance size,"*Preconditions:*

1. Project is created on remote AWS/Azure and local GCP endpoints

*Steps to reproduce:*

1.  Go to ""Billing report"" page

2. Filter by Status or Instance size

*Actual result:*

1. Values are not filtered by Status or Instance size

*Expected result:*

2. Values are filtered by Status or Instance size",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-14 16:53:45,6
13298318,[Branch-1571][AWS][GCP]: Data Engine service value for slave is null in billing report,"*Preconditions:*
1. Billing is available for Data Engine Service on AWS/GCP

*Steps to reproduce:*
1. Go to ""Billing report"" page

*Actual result:*
1. DES slave value is null

*Expected result:*
1. DES slave value is slave size",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-14 15:36:18,6
13298313,"[GCP]: Some values of image are absent in ""Billing report""","*Preconditions:*
 1. Billing is s available in DataLab Web UI
 *Steps to reproduce:*
 1. Go to ""Billing report"" page

*Actual result:*
 1. Values are absent for image

*Expected result:*
 1. Values are present for image

 ",Back-end Debian GCP Known_issues(release2.3) Known_issues(release2.4) Known_issues(release2.5),['DataLab Main'],DATALAB,Bug,Minor,2020-04-14 15:13:59,1
13298294,[Front-end]: User should be notify if project quota is exceeded,"Dear *<User_name>*,
Project cloud infrastructure usage quota has been exceeded. All your analytical environment will be stopped. To proceed working with environment, request increase application quota from DLab administrator.

 

Inform user during his login in case if project quota is exceeded.

Inform only user who is assigned to the project.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-04-14 14:09:57,5
13298293,[Back-end]: User should be notify if project quota is exceeded,"Dear *<User_name>*,
Project cloud infrastructure usage quota has been exceeded. All your analytical environment will be stopped. To proceed working with environment, request increase application quota from DLab administrator.

 

Inform user during his login in case if project quota is exceeded.

Inform only user who is assigned to the project.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-14 14:09:36,6
13298265,SSN/Edge node/Endpoint sizes should be conveyed in configuration file,"The sizes of SSN/Edge node/Endpoint are hard coded, so they are not portrayed on  billing report page. 

Convey these sizes in configuration file.",AWS AZURE Debian DevOps GCP Known_issues(release2.3) Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-14 12:46:15,3
13298252,[Branch-1571]: Most of values are absent for local and remote endpoint,"*Preconditions:*
 # SSN is created on GCP
 # Project is created on  remote AWS/Azure endpoints and local one
 # Billing is available

*Steps to reproduce:*

1. Go to billing report page

*Actual result:*

1. Most of values are absent for local and remote endpoint (AWS)

*Expected result:*

1. All values are present for local and remote endpoint
-----
For GCP bucket is absent in DLab billing and in GCP cloud. Maybe it is wrong tag for bucket?",AWS Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-04-14 12:10:49,3
13298249,[Branch-1571][Azure]: Set of issues on remote endpoint,"*Preconditions:*
 1. Billing is available for remote endpoint on Azure

*Steps to reproduce:*
 1. Go to billing report page

*Actual result:*
 1. -Status for computational resource is absent-
 -2. Values for edge are absent except for 'Product' column-
 3. -All values for shared image per all cloud are absent except for 'Product' column-
 4. -User 'value' is absent for custom image-

*Expected result:*
 1. Status for computational resource is present
 2. Values for edge are present 
 3. All values for shared image per all cloud are present
 4. User 'value' is present for custom image
----
Should we convey 'Shared resource' in 'user' column for image which is shared only per project?",AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-14 11:57:01,6
13298230,[GCP]: Update Dataproc version,"Now we have Dataproc versions 1.3 and 1.4.
Change one of them to the latest stable version, for example 1.5.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2020-04-14 09:45:25,3
13298225,[AWS]: Update EMR version,"Now we have two versions of EMR: 5.28 and 5.19
One of them change to the latest stable version, for example 6.0.0.",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-14 09:34:44,3
13298222,[Back-end]: Implement audit by DLab level,"As user I want to see history changes, so that I can easily find out what/when/who has made changes.

Possibility to find out:
 * Who has deleted or added user?
 * Who has created notebook/compute? 
 * Who has stopped/started/terminated notebook/compute/project?
 * Who has edited group/project/notebook/compute?
 * Who has gone to links?
 * When changes have been made
 * What changes have been made?

In general everything that has been done via DLab UI should be in history of changes.

History by changes should be in the following consistency:

user → time→ action.

This functionality should be conveyed to a new tab 'Audit'.

{color:#de350b}Add bell/information icon, where user can view history changes in 'list of resource' page.{color}",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,New Feature,Major,2020-04-14 09:23:16,6
13298107,Billing is not loaded only for Safari,"*Preconditions:*
1. Billing is available

Steps to reproduce:
1. Go to billing report page or to detailed billing

*Actual result:*
1. Billing is absent

*Expected result:*
1. Billing is present",AWS AZURE Debian Front-end GCP Known_issues(release2.3) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-04-13 19:21:25,5
13298011,[Scheduler]: Time circle should not be cut,"Blue time circle is cut, but should not be.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-04-13 11:49:07,5
13297992,[Front-end]: Project_admin should not be able to remove administrative operation for all Dlab from Super_admin,"If Super_admin and Project_admin are assigned to one project and project_admin makes change in group where is Super_admin (for example,add user), after that administrative operation for all Dlab from Super_admin is removed, but it should be present.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-13 09:41:01,5
13297979,Do not convey to Project_Admin groups of Super_Admin and Project_Admin,"If Super_admin and Project_admin are assigned to one project and project_admin makes change in group where is Super_admin (for example, add user), after that administrative operation for all DLab from Super_admin is removed, but it should be present.

So Project_Admin should not have possibility to alter permissions for Super_Admin and Project_Admin.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-13 08:32:27,6
13297582,Prevent external traffic for squid,Close 3128 port for squid.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-10 09:34:11,3
13296632,[Branch-1571]: Project creation request is not successful,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create a project

*Actual result:*

1. Project creation request is not successful

*Expected result:*

1. Project creation request is successful",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Blocker,2020-04-06 19:02:05,6
13296469,Billing should not be present above report header while scrolling down,"*Preconditions:*

1. Billing is present

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Scroll down

*Actual result:*

1. Billing is present above report header

*Expected result:*

1. Billing is absent above report header",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-06 09:24:29,5
13296085,Data storage on SSN,If endpoint is disconnected related instances disappear in billing report but is still available in 'list of resource' page.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-03 17:55:23,6
13296080,[Branch-1571]: Functionality of exceeding project/total quotas does not trigger,"*Preconditions:*
 # Quotas (total/project) is exceeded 

*Steps to reproduce:*
 # Create resources
 # Start resources

*Actual result:*
 # It is allowed to create notebook/compute/project
 # It is allowed to start notebook/compute/edge node
 # Running resources is still raining

*Expected result:*
 # It is not allowed to create notebook/compute/project
 # It is not allowed to start notebook/compute/edge node
 # Running resources is stopped/terminated (Data Engine Service)",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-03 17:44:45,6
13296068,Billing issues in Branch-1571,"*Preconditions:*
1. SSN is created on AWS
2. Remote endpoint are created on AWS/Azure/Gcp
3. Billing is available on Clouds

*Steps to reproduce:*
1. Go to 'Billing report' page
2.  Filter by calendar
3. Go to 'List of resource' page

*Actual result:*
1. Filter by calendar does not work for GCP
2. Billing detail is unavailable

*Expected result:*
1. Filter by calendar works for GCP
2. Billing detail is availble",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-03 16:47:14,5
13296013,[GCP]: Running code for Data Engine Service does not use GPU,"*Preconditions:*

1. Data Engine Service is created with shape for GPU

*Steps to reproduce:*

1. Run code that use GPU for Data Engine Service

*Actual result:*

1. GPU is not used

*Expected result:*

1. GPU is used
----
Code which use GPU
 1. [https://weeraman.com/put-that-gpu-to-good-use-with-python-e5a437168c01]
 2. [https://www.geeksforgeeks.org/running-python-script-on-gpu/]
----
If use command 'nvidia-smi' on Dataproc - the command is not found
----
Add new parameters in data_engine_service_description.json: Master/Slave GPU type/count.

 ",Debian DevOps GCP Known_issues(release2.3) Known_issues(release2.4),['DataLab Main'],DATALAB,Bug,Major,2020-04-03 11:17:02,3
13295996,[Administration page]: Alter drop down list style for group,Instead of triangle use arrow. ,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-03 09:36:53,5
13295985,Changes for group,"# 'View full billing report for all users' option should trigger for all kinds of users: Admin/Super Admin/User
 # If edge (another cloud) is created after group setting add notebook/compute shapes with the smallest size in the group",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-04-03 09:14:19,6
13295982,Billing is absent for remote endpoint after provisioning restarting,"*Preconditions:*
 1. SSN is created on AWS
 2. Remote endpoints are created on AWS/Azure/GCP

*Steps to reproduce:*
 1. Stop endpoints
 2. Stop SSN
 3. Start SSN
 4. Start endpoints

*Actual result:*
 1. Billing is unavailable for remote endpoint
 Expected result:
 1. Billing is available for remote endpoint",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-03 09:06:13,6
13295794,[Branch-1571]: Notebook management with related Data Engine is forbidden for admin ,"*Preconditions:*

1. Notebook is created by user1
1. Spark cluster is created on notebook

*Steps to reproduce:*
 # Login DLab by user2
 # Go to environment management page
 # Stop Notebook of user1

*Actual result:*
 # Notebook stopping is forbidden

*Expected result:*
 # Notebook stopping is allowed
-----
The same bug is when we terminate notebook.

 

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-02 13:09:02,5
13295785,[Branch-1571]: Group for admin per project does not work if group name contains upper case,"*Preconditions:*
 # Project is created
 # Group name for admin per project contains upper case
 # Admin per project is assigned to the project

*Steps to reproduce:*

1. Go to administration pages

*Actual result:*

1. Any value is not available on administration pages

*Expected result:*

1. Value is available on administration pages",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-02 12:11:08,6
13295778,[Branch-1571]Local billing is not updated,"*Preconditions:*
 # SSN is created on AWS
 # Billing is available on AWS console

*Steps to reproduce:*

1. Go to billing report page

*Actual result:*

1. Billing is not available

*Expected result:*
 # Billing is available
----
 

The bug was found when SSN was created on AWS. Investigate if the same bug will be when SSN will be created on Azure/GCP",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-04-02 12:03:31,3
13295769,[Branch-1571][Front-end]: List of resources page is broken only for user,"*Preconditions:*
 # Notebook is created by 'user1'
 # 'User1' has only user permissions
 # Notebook is created by user1

*Steps to reproduce:*
 # Go to 'List of resource' by user1
 # Stop notebook by user1

*Actual result:*

1. List of resources page is broken

*Expected result:*

1. List of resources page is not broken

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Critical,2020-04-02 11:19:34,5
13295768,[Branch-1571][Back-end]: List of resources page is broken only for user,"*Preconditions:*
 # Notebook is created by 'user1'
 # 'User1' has only user permissions

*Steps to reproduce:*

1. Go to 'List of resource' by user1

*Actual result:*

1. List of resources page is broken

*Expected result:*

1. List of resources page is not broken
----
2. Allow to start notebook only if edge node is in running status.

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Critical,2020-04-02 11:19:09,6
13295751,[Branch-1571][Billing report page]: Header is broken on during extending,"This bug is reproduced on MacBook Pro (13-inch)

*Preconditions:*

1. User is located on Billing report page

*Steps to reproduce:*

1. Extend header

*Actual result:*

1. Header is broken

*Expected result:*

1. Header is not broken",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-02 10:10:14,5
13295732,Apache DLab documentation should be accustomed to new updates,[https://cwiki.apache.org/confluence/display/DLAB/Cloud+name+limitations],AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-04-02 08:35:16,7
13295725,[Azure][Back-end]: Custom image creation fails,"*Preconditions:*
 1. Notebook is created on remote Azure endpoint

*Steps to reproduce:*
 1. Create custom image

*Actual result:*
 1. Custom image creation fails

*Expected result:*
 1. Custom image creation is successful

This bug is reproduced when project is created with 'use shared image' enable or disabled

",AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-04-02 07:55:38,6
13295496,[GCP][Rstudio]: Simultaneously Data Engines creation fails on the same notebook,"*Preconditions:*

1. RStudio is created on GCP

*Steps to reproduce:*

1. Create two Data Engines on RStudio simultaneously

*Actual result:*
 # One Data Engine is created successfully
 # The other Data Engine creation fails

*Expected result:*
 # One Data Engine is created successfully
 # The other Data Engine is created successfully",Debian DevOps GCP Known_issues(release2.3),['DataLab Main'],DATALAB,Bug,Minor,2020-04-01 10:29:21,2
13295019,Set of rules libraries installation,"1. Do not allow to install library while the previous installation is not finished.
2. After library installation  on notebook clear value for library group.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-03-30 15:23:58,5
13294966,Edge node stopping should not stop the other notebooks of different edges in one project,"*Preconditions:*
1. SSN is created on GCP
2. Project is created more then one edge nodes (Remote AWS/Azure/GCP endpoints)
3. Every edge has notebook in running status

*Steps to reproduce:*
1. Stope one edge node

*Actual result:*
1. Only one edge node is stopped
2. All three notebooks are stopped in different edges within one project

*Expected result:*
1. Only one edge node is stopped
2. Only notebook is stopped, which edge has been stopped
",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-30 11:53:30,6
13294543,Prevent edge termination if one of the resource is in processing stage,"1. Do not allow terminate edge node if notebook is in the following status:
- creating
- configuring
- reconfiguring
- stopping
- starting
- creating image
- terminating

2. Do not allow terminate edge node if cluster is in the following status:
- creating
- configuring
- reconfiguring
- stopping
- starting
- terminating",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-03-27 19:20:57,6
13294534,[Back-end]: Instance status is not changed after provserv starting,"(i) {color:#de350b}Do not allow to restart provserv is any instance is in status:{color}
 - Creating
 - Configuring
 - Reconfiguring
 - Creating image
 - Stopping
 - Starting
 - terminating

Confirmation message shows up: 'Oops!
Can not restart provisioning  because one of resource is in processing stage'
Do it as it is implemented for project management.
----
*Preconditions:*
 1. Notebook is in terminating status

*Steps reproduce:*
 1. Run command 'sudo supervisorctl restart all'

*Actual result:*
 1. Docker for notebook termination runs wit '0'
 2. Notebook is still in terminating status on Dlab UI
 3. Notebook is terminated on Cloud console

*Expected result:*
 1. Docker for notebook termination runs wit '0'
 2. Notebook is in terminated status on Dlab UI
 3. Notebook is terminated on Cloud console",AWS AZURE Back-end Debian GCP Known_issues(release2.3) Known_issues(release2.4) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-03-27 18:37:34,1
13294413,Provide mandatory field for library installation ,"# Notebook should be chosen as default value in 'Select resource' drop down list.
 # 'Select group' should be mandatory. Do not allow to type lib name if 'Select group' value is not chosen.
 # When user tries to type library name the error message in red should appear 'Group field is required. Please choose appropriate group.'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-03-27 09:44:15,5
13294144,Notebook creation fails on  due to needs  pip version upgrading,"This bug is not always reproduced. The bug was reproduced for TensorFlow/Zeppelin. Please, take into consideration the other notebooks
*Preconditions:*
1. Project is created

*Steps to reproduce:*
1. Create TensorFlow

*Actual result:*
1. TensorFlow creation fails

*Actual result:*
1. TensorFlow creation is successful",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-26 08:54:16,3
13293906,[Azure]: Custom image creation fails,"*Preconditions:*
 1. Notebook is created on remote Azure endpoint

*Steps to reproduce:*
 1. Create custom image

*Actual result:*
 1. Custom image creation fails

*Expected result:*
 1. Custom image creation is successful

This bug is reproduced when project is created with 'use shared image' enable or disabled

",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-03-25 18:48:57,3
13293883,[Azure][TensorFlow]: Investigate why only one kernel (local/remote) is working Pyspark (Python2.7),"Only one type of kernels work - Local PySpark (Python-2.7 / Spark-2.4.4 ) and PySpark (Python-2.7 / Spark-2.4.4 ) [vi-gcp-23-03-PAwsGcAz1-azure1-de-sp10].

The other type of kernels is dead.",AZURE Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-03-25 17:19:06,7
13293847,[Azure]: Web terminal is not opened only for remote endpoint,"Bug was found when SSN was created on GCP and notebook was created on remote azure endpoint. For now web terminal works for local endpoint for Azure.
*Preconditions:*
1. Notebook is created on Azure

*Steps to reproduce:*
1. Go to web terminal for notebook

*Actual result:*
1. Web terminal is not opened successful

*Expected result:*
1. Web terminal is opened successful",AZURE Debian RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-03-25 15:24:13,3
13293587,[AWS][Azure]: Images are not deleted after project termination,"Take into consideration custom image.

",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-03-24 15:55:59,2
13293586,?Discuss?Shared image per endpoint should be deleted during endpoint termination,Now it is commented and user can not opt for options. And shared image are not created per endpoint. Only image creates per project within the endpoint.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-24 15:50:33,2
13293570,[GCP]: Notebook termination does not delete related Data Engine,"*Preconditions:*
 # SSN is created on GCP
 # Spark cluster is created on any notebook

*Steps to reproduce:*

1. Terminate notebook

*Actual result:*
 # Docker runs with '(0)'
 # Notebook  is terminated on DLab UI and GCP console
 # Spark cluster is not terminated on GCP console and it is terminated on DLab UI

*Expected result:*
 # Docker runs with '(0)'
 # Notebook  is terminated on DLab UI and GCP console
 # Spark cluster is terminated on GCP console and it is terminated on DLab UI",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-03-24 14:14:56,3
13293492,Apply new name limitation from UI side,"1. Project name -10 symbols maximum

2. Endpoint name - 6 symbols maximum

3. Notebook/computational resources names - 14 symbols maximum

4. Alter error message accordingly

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-24 09:37:34,5
13293489,Limit custom image name,"1. Custom image name should be no longer than 10 symbols.

2. The following error message should appear:

'Name cannot be longer than 10 characters and can only contain letters, numbers, hyphens and '_' but can not end with special characters'

 

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-03-24 09:30:36,5
13293357,[AWS]: Notebook creation from custom image does not use custom image,"*Preconditions:*
 # Notebook is created on AWS
 # Custom image is created from Notebook

*Steps to reproduce:*

1. Create Notebook from custom image

*Actual result:*
 # Notebook creation does not use custom image
 # Notebook creation uses default image

*Expected result:*
 # Notebook creation uses custom image
 # Notebook creation does not use default image


  ",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-23 17:43:21,2
13293344,[GCP][RStudio]: Remote kernel is still available in the list after Data Engine termination,"*Preconditions:*
 # RStudio is created on GCP
 # Spark cluster is created on RStudio

*Steps to reproduce:*

1. Terminate spark cluster

*Actual result:*
 # Spark cluster is terminated
 # Remote kernel is still available in the list

*Expected result:*
 # Spark cluster is terminated
 # Remote kernel is not available in the list",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-03-23 16:54:15,2
13293339,"[GCP]: Any playbook is not run due to impossible to connect to bucket for local/remote (spark cluster) kernel, but runs successfully for Data Engine Service kernel)","The bug was found when Notebook was created on GCP

*Preconditions:*

1. RStudio is created on GCP

*Steps to reproduce:*

1. Run Flight data preparation  on RStudio

*Actual result:*

1. Flight data preparation runs with error

*Expected result:*

1. Flight data preparation runs without error
----
The bug is related for all notebooks where we should connect to bucket. The bug is reproduced for local and remote (Data Engine) kernels, but is not reproduced for remote (Data Engine Service) kernel.

 

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-03-23 16:35:38,2
13293287,Add support pandas-profiling installation on instance,It needs higher pandas version.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-23 11:58:49,3
13293273,[GCP]: JupyterLab template is absent for local/remote endpoint,"*Preconditions:*
 # SSN is created on GCP
 # Project is created

*Steps to reproduce:*
 # Go to 'List of resource' page
 # Click '+ Create new' button
 # Choose valid project, endpoint

*Actual result:*

1. JupyterLab template is absent for creation

*Expected result:*

1. JupyterLab template is present for creation

 ",Debian DevOp GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-23 11:09:42,8
13292922,Add possibility to create Odahu cluster in case of cluster termination/fail in the same project,"Existing functionality does not allow add additional ODAHU cluser in same project.

Goal of this task is adding the possibility to create Additional ODAHU cluster, if rest clusters are in FAILED state only.

!image-2020-09-27-16-55-22-133.png!

Other words, user is allowed to create additional ODAHU cluster, if there are no any existing ODAHU intities with not FAILED status.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-03-20 12:10:10,4
13292914,Convey all resources which will be terminated during edge deletion,"User should know which resources will be terminated if delete edge node.

You can convey only notebook list.

And add about clusters: 'All connected computational resources will be terminated as well.'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-03-20 11:41:06,5
13292911,Action for notebook should depend on edge node status.,"If edge node is stopped do not allow user to start notebook.

If user hovers mouse 'run' action the hint should appear 'Unable to start notebook until edge node is stopped'.

Do the same as it is implemented for start notebook during its stopping.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-03-20 11:31:52,5
13292907,Prevent action 'Do not terminate all related resources' for user during endpoint disconnection,"Now user has two options during endpoint disconnection:

1. Do not terminate all related resources

2. Terminate all related resources

Now user should not have the choice. During endpoint disconnection all resources should be terminated.

(!) Do not delete this function, only comment (prevent) for user 'Do not terminate all related resources'",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-03-20 11:18:52,5
13292903,[DevOps]: Add possibility to recreate edge node after edge termination/failing,"As user I want to use my instances in case if edge failing or again to create previously terminated edge in the same project using the same endpoint so that it allows me do not create project with new name and use the previous name.

If we terminate edge (or edge has been failed) we could not create the new edge in the same project and the same endpoint.
----
Edge not could be failed during stopping/starting/creating/terminating.

{color:#de350b}Add retry during stopping/starting edge in case of failing ???{color}

{color:#de350b}If retry does not help convey information message???{color}

!https://kb.epam.com/s/uw6r9b/8401/7d0034e810b0e95b8c0694abfaf748cf5135c15a/_/images/icons/emoticons/information.svg!  One project can contain one endpoint/edge node or more.

*Statuses for recreate*:
 * edge node is terminated from Cloud Web Console - recreate should be

 * edge node is terminated from Web DataLab UI - recreate should be
 * edge node failed during stopping/starting - return cloud status - recreate should NOT be
 * edge node failed during creating - recreate should be 
 * edge node failed during terminating  - recreate should be 

(i) If at least one instance exists - SMART recreate.

(i) If instances do not exist - create all resources.

Github issue: [https://github.com/apache/incubator-dlab/issues/731].

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-20 11:08:56,3
13292663,[AWS]: Not removed project resources block endpoint termination,"Steps to reproduce:
1) Create SSN
2) create separate endpoint on AWS
3) Create project, notebook, cluster on remote endpoint
4) Terminate project
5) Try to terminate endpoint.

Some project resources still exist after project termination on AWS external resources. Please check.",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-19 09:41:17,2
13292513,Unable to go instance link if KeyCloak client contains upper case for remote endpoint,"*Preconditions:*
1. SSN is created on AWS
2. Project is created for remote endpoint for Azure
3. Project name contains upper case
4. Notebook is created on remote Azure endpoint

*Steps to reproduce:*
1. Go to notebook link

*Actual result:*
1. Notebook link is not open successful

*Expected result:*
1. Notebook link is open successful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-18 15:34:21,2
13292500,[Front-end]: Instance management via environment management fails on remote/local endpoints,"*Preconditions:*
 # Notebook/cluster are created on remote/local endpoint

*Steps to reproduce:*
 # Go to environment management page
 # Stop/terminate notebook/cluster

*Actual result:*
 # Appears error message
 # Instances are in previous statuses

*Expected result:*

1. Instances are stopped/terminated

 ----
For FE:
DLAB-1633 branch
 !screenshot-1.png! ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-18 14:58:19,5
13292193,Investigate why notebook template/computational resource are not always available after provisioning starting,"Edge node is created on remote  AWS endpoint. After restarting provisioning not always notebook and computational resources are available. Please, investigate it and fix it.

Especially on remote GCP/Azure endpoints.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Critical,2020-03-17 13:22:11,6
13292180,[Role]: Set of tasks after improvement,"# Appears error in console after group adding
 # Change:

 - 'Computational' -> 'Compute'
 - 'Notebook Shape' -> 'Notebook Shapes'
 - 3. Sort items in 'Role' drop down list

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-03-17 12:01:55,5
13291979,[Scheduler by time/inactivity]: Notebook stop does not trigger,"*Preconditions:*

1. Notebook is created on remote endpoint

*Steps to reproduce:*

1. Set scheduler for notebook for stopping

*Actual result:*

1. Notebook stopping is not triggered

*Expected result:*

1. Notebook stopping is triggered",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-16 13:49:29,6
13291936,Instance management via environment management fails on remote/local endpoints,"*Preconditions:*
 # Notebook/cluster are created on remote/local endpoint

*Steps to reproduce:*
 # Go to environment management page
 # Stop/terminate notebook/cluster

*Actual result:*
 # Appears error message
 # Instances are in previous statuses

*Expected result:*

1. Instances are stopped/terminated

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-16 10:22:55,6
13291634,[Library management]: 'Cancel'/'Install' buttons are cut on 'Library management' popup',"The bug was found on 13-inch monitor.

*Preconditions:*

1. Some libraries are installed on notebook

*Steps to reproduce:*
 # Go to library management popup
 # Choose some libraries to install

*Actual result:*

1. 'Cancel'/'Install' buttons are cut

*Expected result:*

1. 'Cancel'/'Install' buttons are visible",AZURE Debian Front-end GCP RedHat aws pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-03-13 18:26:29,5
13291630,Custom image creation fails on remote AWS endpoint,"*Preconditions:*
 # SSN is created on AWS
 # Notebook is created on remote AWS endpoint

*Step to reproduce:*

1. Create custom image from notebook

*Actual result:*

1. Custom image creation fails

*Expected result:*

1. Custom image creation is successful",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-03-13 18:01:52,2
13291612,Project bucket name should be contain lower-case to FE,Project is created on remote AWS endpoint.,AWS pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-03-13 16:48:44,2
13291518,Project creation fails,"*Preconditions:*
 # SSN is created on Azure/AWS/GCP

*Steps to reproduce:*
 # Create project

*Actual result:*
 # Project creation fails

*Expected result:*
 # Project is created successful ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2020-03-13 09:44:32,3
13291361,[AWS]: Data Engine Service creation fails if project name contains upper case,"*Preconditions:*
1. Project is created on remote AWS endpoint
2. Project name contains upper case

*Steps to reproduce:*
1. Create EMR on notebook

*Actual result:*
1. EMR creation fails

*Expected result:*
EMR is created successful",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-12 15:46:43,2
13291275,[GSP]: Notebook creation fails from image,"*Preconditions:*
 # Project1 is created on GCP with disabled 'use shared image'
 # Notebook1 is created

*Steps to reproduce:*

1. Create notebook of the same template in Project1

*Actual result:*

1. Notebook creation fails

*Expected result:*

1. Notebook creation is successful
----
1. Also Spark cluster creation fails with the same error. 
The bug was  reproduced on Zeppelin notebook. But I think it is not depends on notebook.
2. If project is with enabled 'use shared image' the bug is not reproduced.

",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-12 09:52:59,2
13291271,[GCP]: Notebook termination fails if notebook contains Dataproc,"Dataproc was created on Jupyter. However i think it is related to all notebooks.

*Preconditions:*

1. Dataproc is created

*Steps to reproduce:*

1. Terminate notebook

Actual result:

1. Notebook termination fails

*Expected result:*

1. Notebook termination is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-03-12 09:39:16,2
13291131,[GCP]: Edge node starting fails on  remote  GCP endpoint,"*Preconditions:*
1. Edge is created on GCP remote endpoint
2. Edge node is in stopping status

*Steps to reproduce:*
1. Start edge node on GCP remote endpoint

*Actual result:*
1. Edge node stopping fails

*Expected result:*
1. Edge node stopping is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-11 16:50:27,2
13291128,[GCP]: Notebook stopping fails on remote GCP endpoint,"*Preconditions:*
1. Notebook is created on GCP remote endpoint

*Steps to reproduce:*
1. Stop notebook

*Actual result:*
1. Notebook stopping fails

*Expected result:*
1. Notebook stopping is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-11 16:39:56,2
13291113,[Project management]: Stop/terminate action is not allowed for running edge if another edge is in creating/starting/stopping/terminating statuses in the same project,"*Preconditions:*
 # Project contains more than one edge
 # One edge is in running status
 # The other edge is in creating/starting/stopping/terminating statuses

*Steps to reproduce:*

1. Stop/terminate edge node which is in running status

*Actual result:*

1. Stop/terminate is not allowed

*Expected result:*

1. Stop/terminate is allowed
-----
2. Also stop action should not apply for all edges in one project if it has been chosen only one edge. About terminate I do not know. Investigate it.
3. It should not stop failed edge node.",pull-request-available,['Authentication and authorization'],DATALAB,Bug,Major,2020-03-11 15:55:48,6
13291109,[Azure]: Container name should be lowercase,"*Preconditions:*

1. Remote endpoint is created on Azure

*Steps to reproduce:*

1. Create project with upper case on remote endpoint 

*Actual result:*

1. Edge node creation fails

*Expected result:*

1. Edge node is created successful",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-11 15:22:30,9
13291045,Error with creating edge node on external Azure endpoint,"*Preconditions:*
1. Remote endpoint is created on Azure
2. SSN is created on AWS
*Steps to reproduce:*
1. Create Edge on remote endpoint
*Actual result:*
1. Edge creation fails
*Expected result:*
1. Edge creation is created successful",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-11 10:20:49,9
13290122,Adjust role page according to requirements,"# Changes are not confirmed in role if users column is empty
 # 'Success! Group data successfully updated, but users did't delete from group!' -> 'Success! Group data is updated successfully, but user isn't delete from group!' 

 # 'Success! Group data successfully updated!' -> 'Success! Group data is updated successfully!'

 # If add a new user and don't reload page and delete this added user the confirmation message is absent
 # If confirm a user removing and don't reload page and delete the other user of this group in confirmation message should not contain the previous deleted user

----
If there is any changes should be visible hint for confirmation",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-03-06 16:13:34,5
13290086,[Front-end]: Convey Grafana credentials,"# Label and value for user name should be present for Grafana
 # Label and value for password should be present for Grafana",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-06 14:10:22,5
13290079,[Azure]: Some billing values are absent,"*Preconditions:*
 1. Billing is available on Azure

*Steps to reproduce:*
 1. Go to billing report page
 2. Go to list of resource page
 3. Go to detailed billing

*Actual result:*
 # Value of environment name is absent
 # value of instance size is absent
 # Value of service is absent
 # 'USD' is absent on billing report/detailed report/list of resource

*Expected result:*
 # Value of environment name is present
 # value of instance size is present
 # Value of service is present
 # 'USD' is present on billing report/detailed report/list of resource

----
2. [Notebook name popup]: If there is too long link label should not be cut",AZURE Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-06 13:55:32,5
13289904,[GCP]: Notebook creation fails if project name contains upper case,"*Preconditions:*

1. Project name contains upper case

*Steps to reproduce:*

1. Create notebook

*Actual result:*

1. Notebook creation fails

*Expected result:*

1. Notebook creation is successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-03-05 20:18:21,2
13289549,[Azure][RedHat]: Notebook creation fails,"*Preconditions:*
1. SSN is created on Azure (RedHat)

*Steps to reproduce:*
1. Create any notebook

*Actual result:*
1. Notebook creation fails

*Expected result:*
1. Notebook is created successful
----
For Jupyter/Zeppelin the mistake is the same for RStuduio the mistake differs",AZURE DevOps RedHat,['DataLab Main'],DATALAB,Bug,Critical,2020-03-04 14:22:03,3
13289520,Convey 'Resource Type' value to UI,"1. Value of 'Resource Type' is absent for GCP
2. Get rid of ' Cannot read property 'length' of undefined' while visiting project page for Azure",AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-03-04 12:18:16,5
13289478,If custom_tag is not defined the value should be empty,"It works if create manually notebook.
But not works for auto-test.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2020-03-04 09:41:10,2
13289475,[GCP]: Enable IP forwarding for Edge node,It should be enabled when creating instance using GCP Cloud API,Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2020-03-04 09:30:33,9
13289237,[Front-end]: Cluster name should be unique per project,"Work in branch ""DLAB-1546""",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-03 13:32:19,5
13289236,[Back-end]: Cluster name should be unique per project,"Work in branch ""DLAB-1546""",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-03 13:31:41,6
13289235,Add authentication vars for Jupyterlab plugin,In order to prevent entering credentials every time if user goes to JupyterLab Web UI add  authentication vars for Jupyterlab plugin.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-03 13:26:32,2
13289234,Fix bug with Step certificates on SSN node,"After renewing certificate, we restart only provisioning service instead of all java services
----
*Preconditions:*

1. Notebook is in stopped status

*Steps to reproduce:*

1. Start notebook in next day

*Actual result:*

1. Notebook status is  stuck in 'starting' status on DLab UI

*Expected result:*

1. Notebook status is  in 'running' status on DLab UI",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-03 13:25:14,9
13289155,[Azure][DevOps]: Odahu (Legion) management in DataLab,"Who manages Legion provisioning:
 * Admin or DataLab user?

1. Legion cluster will be deployed per project-endpoint
 2. Under Administration page:
   2.1. Add a page: Legion deployment
   2.2. Page should contain grid with following information:
 - Project Name
 - Endpoint URL
 - Legion cluster Name
 - Legion cluster status
 - Number of Legion cluster nodes
 - Actions column:
 ## Start/Stop?
 ## Terminate?
 ## Scale-down
 ## Scale-up

   2.3. Create Legion cluster popup:
 - Select Project
 - Select Endpoint
 - Select existing k8s cluster (checkbox, once checked input field shows up where you can fill in ""Legion k8s cluster URL"")
 - Select number of shapes
 - Select instance shape
 - Select certificate? +(check with Vitalii Solodinov)+
 ## DataLab passes this as input parameter for Legion provisioning script
 - VPC, Subnet will be auto-propagated from Project (visible = false)
 - NAT gateway
 ## Need Legion team to use Edge IP instead (proxy)
 - Buckets (visible = false)
 ## Bucket for Feedback (use Project bucket)
 ### Legion should access bucket name as parameter
 ## Bucket for State of Terraform (use Project bucket)
 ### Legion should access bucket name as parameter
 - Container registry (pass a parameter?)

 # Security params (keycloak, oauth and ssh key)
 # Repos (docker repo and creds, helm repos)

3. Under List of Resource page
 # Show additional column with Legion icon near every notebook
 # When use clicks on it -> popup shows up containing following information:
 ## URL for feedback storage
 ## URL for Swagger API registry
 ## URL for Grafana
 ## Etc
 # Extend current Actions menu for JupyterLab only with action ""Attach Legion""",Azure Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-03 09:08:44,2
13289153,[AWS][DevOps]: Odahu (Legion) management in DataLab,"Who manages Legion provisioning:
 * Admin or DataLab user?

1. Legion cluster will be deployed per project-endpoint
 2. Under Administration page:
   2.1. Add a page: Legion deployment
   2.2. Page should contain grid with following information:
 - Project Name
 - Endpoint URL
 - Legion cluster Name
 - Legion cluster status
 - Number of Legion cluster nodes
 - Actions column:
 ## Start/Stop?
 ## Terminate?
 ## Scale-down
 ## Scale-up

   2.3. Create Legion cluster popup:
 - Select Project
 - Select Endpoint
 - Select existing k8s cluster (checkbox, once checked input field shows up where you can fill in ""Legion k8s cluster URL"")
 - Select number of shapes
 - Select instance shape
 - Select certificate? +(check with Vitalii Solodinov)+
 ## DataLab passes this as input parameter for Legion provisioning script
 - VPC, Subnet will be auto-propagated from Project (visible = false)
 - NAT gateway
 ## Need Legion team to use Edge IP instead (proxy)
 - Buckets (visible = false)
 ## Bucket for Feedback (use Project bucket)
 ### Legion should access bucket name as parameter
 ## Bucket for State of Terraform (use Project bucket)
 ### Legion should access bucket name as parameter
 - Container registry (pass a parameter?)

 # Security params (keycloak, oauth and ssh key)
 # Repos (docker repo and creds, helm repos)

3. Under List of Resource page
 # Show additional column with Legion icon near every notebook
 # When use clicks on it -> popup shows up containing following information:
 ## URL for feedback storage
 ## URL for Swagger API registry
 ## URL for Grafana
 ## Etc
 # Extend current Actions menu for JupyterLab only with action ""Attach Legion""",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-03 09:06:42,2
13288911,Adjust billing according to multiple support,"# Add USD to cost
 # Get rid of 'Cannot read property 'filter' of undefined' in console",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-02 15:14:15,5
13288906,Action column should be called 'Actions',Action column should be called 'Actions' for resources_list/projects/environment_management pages,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-03-02 14:59:08,5
13288741,[Azure]: Remote endpoint creation fails if VPC/subnet/resource group is not determined,"*Preconditions:*

1. SSN is created on Azure

*Steps to reproduce:*
 # Create remote endpoint on Azure
 # VPC/subnet/resource group fields leave empty

*Actual result:*

1. Remote endpoint creation fails

*Expected result:*

1. Remote endpoint creation is successful",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-03-02 10:27:34,9
13288054,[Azure]: Remote endpoint creation fails,"*Preconditions:*

1. SSN is created on Azure

*Steps to reproduce:*

1. Create remote endpoint on Azure via Jenkins job

*Actual result:*

1. Remote endpoint creation fails

*Expected result:*

1. Remote endpoint is created",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-02-27 13:34:39,2
13287689,[Back-end]: Available lib list is not get due to request entity too large,"*Preconditions:*
 # SSN is created on GCP
 # Endpoint is created on AWS
 # Notebook is created on AWS

*Steps to reproduce:*

1.  Go to 'List of resource' page

2.  Click action menu for notebook

3. Choose manage libraries

4. Choose 'Select resource' in drop down list 

*Actual result:*

1. Available lib list is not get successful

*Expected result:*

1. Available lib list is get successful
----
Available lib list was get successful after 40 minutes of trying.

This bug reproduces especially for RStudio",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-26 09:02:38,6
13287479,[Front-end]: Admin per project,"As an admin I want to assigned/unassigned user to a project for which I am an admin so that I can easily manage of adding users to a group.

*Acceptance criteria:*
 # Admin can assigned/unassigned user to group of which he is admin
 # Admin cannot assigned/unassigned user to group of which he is not admin
----

*NOTE:* 1. Could one group has more than one admin?

2. Could one admin has more than one project?

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-25 12:23:12,5
13287478,Admin per project,"As an admin I want to assigned/unassigned user to a project for which I am an admin so that I can easily manage of adding users to a group.

*Acceptance criteria:*
 # Admin can assigned/unassigned user to group of which he is admin
 # Admin cannot assigned/unassigned user to group of which he is not admin
----

*NOTE:* 1. Could one group has more than one admin?

2. Could one admin has more than one project?

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-25 12:22:11,6
13287451,[Billing report]: Value of service is absent ,"*Preconditions:*
 1. SSN is created in marketplace
 2. Billing is available 
 3. Instances are created

*Steps to reproduce:*
 1. Go to billing report page

*Actual result:*
 1. Value of service is absent

*Expected result:*
 1. Value of service is present",Debian Front-end GCP,['DataLab Main'],DATALAB,Bug,Major,2020-02-25 09:05:38,5
13287445,[Billing report]: Value of instance size is absent for DES,"*Preconditions:*
1. SSN is created from marketplace
2. DES is created
3. Billing is available

*Steps to reproduce:*
1. Go to billing report page

*Actual result:*
1.  Instance size is absent for DES

*Expected result:*
1.  Instance size is present for DES",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-02-25 08:50:09,1
13287344,Adjust images for demo-regime,"*1.* 'It is only demo preview for Dataproc job tracker UI. For real environment you can use all available tools on Dataproc -> It is only demo preview for Hadoop UI. For real environment you can use all available tools on Hadoop.
*2.* 'It is only demo preview for Apache Spark job tracker UI. For real environment you can use all available tools on Apache Spark job tracker UI' -> 'It is only demo preview for Apache Spark Master UI. For real environment you can use all available tools on Apache Spark Master UI'
*3.* Information about demo preview should not be cut for computational resources
*4.* The modal window should have fixed size",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-24 21:02:52,5
13287331,[Branch DLAB-1541]: Stoping/starting/terminating statuses of notebook do not convey to DLab UI,"*Preconditions:*

1. Notebook is created from branch DLAB-1541

*Steps to reproduce:*

1. Stop notebook

*Actual result:*

1. Stopping status is absent on DLab UI

*Expected result:*

1. Stopping status is present on DLab UI
----
The same case is with starting/terminating statuses of notebook.
----
In addition during notebook termination DES does change its status from terminating -> terminated and remained terminating.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-24 19:55:41,6
13287275,[Branch DLAB-1541]: It is forbidden to create computational resources with the same name for second project,"*Preconditions:*
 # Two projects are created
 # Notebook2 with name RS1 are created in two projects
 # Cluster with name Cl1 is created in project1 for RS1

*Steps to reproduce:*

1. Create cluster with name Cl1 for RS1 in project2

*Actual result:*

1. It is forbidden to create a cluster only in project2 (in project1 it is allowed)

*Expected result:*

1. It is allowed to create a cluster",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-24 17:44:13,5
13287197,[Notebook name popup]: Notebook label should  depend on notebook template,"For all notebooks we have label 'Jupyter' despite wich kind of notebook is created.

Notebook label should be depends on notebook template. 

For example for RStudio notebook it should be 'RStudio' and etc.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2020-02-24 12:11:51,1
13287195,Rename IP notebook/ungit/tensorboard/job tracker by demo link,"# notebook_demo_link
 # ungit_demo_link
 # tensorboard_demo_link
 # job tracker_link

It is as example. If you have your ideas you are welcome, be free.",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2020-02-24 12:06:40,1
13287194,Images are not open for job tracker,"*Preconditions:*
 # SSN is created from demo-regime
 # Computational resources are created

*Steps to reproduce:*

1. Clickjob tracker URL of computational resource

*Actual result:*

1. Url is not opened successful

*Expected result:*

1. (/) Url is opened successful
----
2. (/) Get rid of the blue vertical line for Jupyter
 3. (/) Get rid of the red vertical line and grey horizontal column for ungit
 4. For Superset/JupyterLab should not be jupyter link. Now image for superset is still unavailable

5. (/) It is only demo preview for JupyterLab UI. For real environment you can use all available tools on JupyterLab UI.

 ",Debian Front-end GCP,['DataLab Main'],DATALAB,Bug,Major,2020-02-24 11:59:28,5
13287167,[Azure]: Project creation fails on stage of shared storage account/container creation,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create project

*Actual result:*

1. Project creation fails

*Expected result:*

**Project creation ** is successful",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-02-24 09:40:18,2
13286798,[Autotest][Azure]: Change name for SSN,'Azure' name is reserved by MS Azure. So use another name for ssn during testing.,AZURE Debian RedHat,[],DATALAB,Task,Major,2020-02-21 14:04:00,9
13286692,Alter information message if remove user from group," Removing user from group causes that user is not able to see his instances in this project, so convey information message to admin:

'Removing the user  <user_name> from the group may prevent the user to access his/her resources'

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-21 09:25:57,5
13286691,Alter information message if remove group from the project,"Removing a group from project causes that user (from this group) is not able to see his instances in this projects, so convey information message to admin:

'Removing the group  <group_name> from the project may prevent the users from this group to access his/her resources'",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-21 09:25:29,5
13286535,[Azure]: Flight data visualization runs error warning on RStudio,"*Preconditions:*
 # SSN is created on Azure
 # Remote endpoint is created on Azure
 # RStudio is created on remote endpoint

*Steps to reproduce:*

1. Run Flight data visualization on RStudio

*Actual result:*

1. Playbook runs with error

*Expected result:*

1. Playbook runs without error",AZURE Debian DevOps,['DataLab Main'],DATALAB,Task,Minor,2020-02-20 17:47:26,2
13286449,[Environment Management]: Icons should not overlap,"This bug reproduced for 13-inch monitor

*Preconditions:*

1. Notebook is created

*Steps to reproduce:*

1. Go 'Environment Management' page

*Actual result:*

1. Icons are overlapped

*Expected result:*

1. Icons are  not overlapped",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-02-20 11:21:08,5
13286277,Actions for notebook in case if endpoint is disconnected,"If endpoint is disconnected should we fail all action with notebook?

*Case:*
 # Notebook is running
 # endpoint is disconnected

*Steps:*
 # Stop notebook

*Actual result:*
 # Notebook is failed on DLab UI
 # Notebook is running on cloud console
 # Docker for stop is not run",AWS Azure Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-19 16:47:52,6
13286275,[GCP]: Web terminal is not opened in marketplace,"*Preconditions:*

1. Notebook is created in marketpalce

*Steps to reproduce:*

1. Go to web terminal of notebook

*Actual result:*

1. Web terminal is not opened

*Expected result:*

1. Web terminal is opened",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-02-19 16:42:05,1
13286270,[GCP]: Billing is not available in marketpalce,"*Preconditions:*

1. Billing is available for end in GCP console

*Steps to reproduce:*

1. Go to 'Billing report'page in DLab

*Actual result:*

1. Billing is not available

*Expected result:*

1. Billing is  available",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-02-19 16:13:09,3
13286192,Rarely notebook creation fails on stage of 'npm' installation,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create notebook

*Actual result:*

1. Notebook creation fails


*Expected result:*

1. Notebook is created successful",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-02-19 09:40:37,3
13285993,[DataLab+Odahu]: User is not allowed to create computational resource,"*Preconditions:*
 # SSN is created from odahu-integration branch
 # Notebook is in running status

*Steps to reproduce:*

1. Create computational resource

*Actual result:*

1. User is not allowed to create computational resource

Expected result:

1. User is  allowed to create computational resource

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-18 13:56:30,6
13285920,[Billing report page]: Billing issues for user who has not administrative role,"*1.* If user is not admin 'Refresh'/'Export' buttons are not aligned by the right side.

Header is overlapped.

*2.* If user is not admin it is forbidden to view billing, but billing should be allowed",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-02-18 08:41:41,5
13285823,[Azure]: Remote endpoint termination should not delete SSN,"*Preconditions:*
 # SSN is created on Azure
 # Remote endpoint is created on Azure

*Steps to reproduce:*

1. Terminate remote endpoint via Jenkins job

*Actual result:*
 # Remote endpoint is terminated
 # SSN is terminated

*Expected result:*
 # Remote endpoint is terminated
 # SSN is not terminated",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-02-17 17:06:41,9
13285821,Convey billing in demo mode,Billing is available only for SSN.,Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2020-02-17 16:59:26,1
13285820,[Demo-regime]: Disconnection endpoint should not eliminate administration role,"*Preconditions:*

1. Resources (notebook/computational resource) are created on local endpoint
 2. Current user has administrative role

*Steps to reproduce:*

1. Disconnect local endpoint and (do NOT terminate related resources or terminate)

*Actual result:*

1. Administrative role is absent for current user

*Expected result:*

1. Administrative role is absent for current user",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2020-02-17 16:56:57,1
13285809,Portray appropriate image if user clicks notebook/computational resource link,"# For notebook UI link
 # For ungit link
 # For TensorFlow link (only for GPU)
 # For Job-tracker of computational resource link",Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Major,2020-02-17 16:06:23,5
13285807,Change bucket names and notebook/computational resources names,"1. Change bucket names 

2.  Notebook/computational resources names

3. Add the third link for Deep Learning and Jupyter with TensorFlow: 'TensorBoard'

4. Prevent 'cloud Endpoint API'
----
(i) *NOTE* *1.* On Zeppelin/Superset we cannot configure/reconfigure spark ({color:#de350b}I do not remember how about Jupyterlab{color})
 *2.*  GPU notebook has the third url for TensorBoard (deepLearning, TensorFlow/Jupyter with TensorFlow, Rstudio with TensorFlow-AWS)

*3.* Superset/JupyterLab do not have computational resources

*4*.  We cannot instal library on Superset/Jupyterlab
----
*Examples of bucket names:*
 sbn-project_name-endpoint_name-bucket
 sbn-endpoint_name-shared-bucket
  ",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2020-02-17 16:02:25,1
13285800,Web terminal is not opened in demo mode,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*

1. Go to web terminal

*Actual result:*

1. The tab is blank

*Expected result:*

1. Web terminal is in tab",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-02-17 15:21:59,1
13285799,Reconfiguration spark on notebook/spark cluster does not change status,"*Preconditions:*

1. Spark cluster is created on Jupyter

*Steps to reproduce:*

1. Reconfigure spark on spark cluster

*Actual result:*

1. Spark cluster is stuck in 'Reconfiguring' status

*Expected result:*

1. Spark cluster changes its status from  'Reconfiguring' to 'Running'
----
The same issue is with notebook.

 ",Debian GCP,['DataLab Main'],DATALAB,Bug,Minor,2020-02-17 15:15:59,1
13285798,[Gcp]: Not all libraries are pulled in demo mode for pip2/pip3/r package,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*

1. Install matplotlib/pandas from python group, csvread from r package

*Actual result:*

1. These libs are not available

*Expected result:*

1. These libs are available
----
On top of that that is not full list, there are another libs which are available in real environment but are not available in demo-regime.",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Major,2020-02-17 15:03:16,1
13285759,Alter error message for key.pub validation during project creation,"1. If public key is not in name.pub extension or is in such extension but is not in openSSH format alter error message:

 *from* 'key Wrong key format. Key should be in openSSH format' 
 *to* 'Wrong key format. Key should be in openSSH format'
-----


2. If public key is in name.pub extension but is not in openSSH and has too size alter error message:

 *from* 'Project creation failed'
 *to* 'Wrong key format. Key should be in openSSH format'",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-02-17 11:34:58,4
13285727,"Project termination should delete only project bucket, but not endpoint bucket","*SSN is created on AWS:*
 1. If create edge on local endpoint two buckets will be created 
 - {color:#00875a}endpoint-local-shared-bucket{color}
 - {color:#00875a}project-local-bucket{color}

2. If delete edge on local endpoint
 - {color:#00875a}endpoint-local-shared-bucket remains{color}
 - {color:#00875a}project-local-bucket -deletes{color}

*SSN is created on GCP:*
 1. If create edge on local endpoint two buckets will be created
 - {color:#00875a}endpoint-local-shared-bucket{color}
 - {color:#00875a}project-local-bucket{color}

2. If delete edge on local endpoint
 - {color:#00875a}endpoint-local-shared-bucket remains{color}
 - {color:#00875a}project-local-bucket -remains{color} - {color:#4c9aff}verified now the bucket deletes{color}

*SSN is created on AZURE*
 1. If create edge on local endpoint two buckets will be created
 - {color:#00875a}endpoint-local-shared-bucket{color}
 - {color:#00875a}project-local-bucket{color}

2. If delete edge on local endpoint
 - {color:#00875a}project-local-bucket -deletes{color}
 - {color:#00875a}endpoint-local-shared-bucket remains{color}

*Remote endpoint AWS/Azure/GCP*
 1. If create edge on remote endpoint AWS/Azure/GCP two buckets will be created
 - {color:#00875a}project-remote-bucket{color}
 - {color:#00875a}endpoint-remote-shared-bucket{color}

2. If delete edge on remote endpoint (Azure)
 - {color:#00875a}project-remote-bucket deletes {color} - verified
 - {color:#de350b}endpoint-remote-shared-bucket deletes{color} - {color:#4c9aff}verified{color}
 - {color:#de350b} fails the edge (on the same remote endpoint) in the other project{color} - {color:#4c9aff}verified{color}

3. If delete edge on remote endpoint (GCP)
 - {color:#00875a}endpoint-remote-shared-bucket remains{color}
 - {color:#de350b}project-remote-bucket remains{color} - {color:#4c9aff}verified{color}

4. If delete edge on remote endpoint (AWS)
 - {color:#00875a}endpoint-remote-shared-bucket remains{color}
 - {color:#00875a} project-remote-bucket deletes{color}",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-17 09:45:58,3
13285721,[Azure][Back-end]: Bucket browser,"As a user I want to  use S3 browser so that I can manage my buckets and objects via DLab UI.

 

*Acceptance criteria:*

*1.* User has access endpoint_shared bucket and project bucket (only if he is assigned to this project)

*2.* Another user does not have access to project bucket (if he is not assigned to this project)

*3.* Administrator has access to all buckets

*4.* User can upload and download files to and from bucket

*5.* User can create public URLs to share the files

*6.* User can set Access Control on buckets and files only for project bucket?",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,New Feature,Major,2020-02-17 09:09:01,6
13285719,[Front-end]: Bucket browser,"As a user I want to  use S3 browser so that I can manage my buckets and objects via DLab UI.

 

*Acceptance criteria:*

*1.* User has access endpoint_shared bucket and project bucket (only if he is assigned to this project)

*2.* Another user does not have access to project bucket (if he is not assigned to this project)

*4.* User can upload and download files to and from bucket

*5*. User can create folder(

*6*. User can see bucket structure (tree)

*7.* Bucket management functionality is on notebook name popup.

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,New Feature,Major,2020-02-17 09:06:00,5
13285377,[Azure]: Playbook running fails on remote-endpoint-shared-bucket,"*Preconditions:*
 1. Remote endpoint is created on Azure
 2. SSN is created on Azure
 3. Jupyter is created


 *Steps to reproduce:*
 1. Run 'Flights_data_Preparation_Python2-Spark' for remote-endpoint-shared-bucket

*Actual result:*
 1. Playbook running fails

*Expected result:*
 1. Playbook running is successful",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-14 16:57:35,9
13285357,[List of resource]: Prevent lib_groups requests if library managements popup is not opened,"If available lib list is not gained there are often lib_groups requests on 'List of resources' page.

Ger rid of these requests on 'List of resources' page if library managements popup is not open
-----
Also even after notebook termination request are still sending and docker runz with '1'.
[~ofuks] should take a look at it.

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-14 15:26:03,5
13285347,Alter requests for notebook name creation with the same name in different projects,"(/) @DELETE /aws/infrastructure_provision/computational_resources/\{exploratoryName}/\{computationalName}/terminate -> /aws/infrastructure_provision/computational_resources/\{projectName}/\{exploratoryName}/\{computationalName}/terminate

(/) @DELETE /azure/infrastructure_provision/computational_resources/\{exploratoryName}/\{computationalName}/terminate -> /azure/infrastructure_provision/computational_resources/\{projectName}/\{exploratoryName}/\{computationalName}/terminate

(/) @DELETE /gcp/infrastructure_provision/computational_resources/\{exploratoryName}/\{computationalName}/terminate -> /gcp/infrastructure_provision/computational_resources/\{projectName}/\{exploratoryName}/\{computationalName}/terminate

(/) @POST /infrastructure_provision/exploratory_environment/scheduler/\{exploratoryName}/\{computationalName} -> /infrastructure_provision/exploratory_environment/scheduler/\{projectName}/\{exploratoryName}/\{computationalName} [~Price]

(/) /infrastructure_provision/exploratory_environment/\{name}/stop -> /infrastructure_provision/exploratory_environment/\{project}/\{name}/stop

(/) @DELETE /infrastructure_provision/exploratory_environment/\{name}/terminate -> /infrastructure_provision/exploratory_environment/\{project}/\{name}/terminate

(/) @GET /aws/infrastructure_provision/computational_resources/\{exploratoryName}/\{computationalName}/config -> /aws/infrastructure_provision/computational_resources/\{projectName}/\{exploratoryName}/\{computationalName}/config

(/) @GET /azure/infrastructure_provision/computational_resources/\{exploratoryName}/\{computationalName}/config -> /azure/infrastructure_provision/computational_resources/\{projectName}/\{exploratoryName}/\{computationalName}/config

(/) @GET /gcp/infrastructure_provision/computational_resources/\{exploratoryName}/\{computationalName}/config -> /gcp/infrastructure_provision/computational_resources/\{projectName}/\{exploratoryName}/\{computationalName}/config

(/) @POST /infrastructure_provision/exploratory_environment/scheduler/\{exploratoryName} -> /infrastructure_provision/exploratory_environment/scheduler/\{projectName}/\{exploratoryName}

(/) @POST /infrastructure_provision/exploratory_environment/lib_install - added new form parameter ""project_name""

(/) @GET /infrastructure_provision/exploratory_environment/lib_groups - new QueryParam ""project_name""

(/) @POST /infrastructure_provision/exploratory_environment/search/lib_list - added new form parameter ""project_name""

(/) @PUT /infrastructure_provision/exploratory_environment/\{name}/reconfigure -> /infrastructure_provision/exploratory_environment/\{project}/\{name}/reconfigure

(/) @GET /infrastructure_provision/exploratory_environment/\{name}/cluster/config -> /infrastructure_provision/exploratory_environment/\{project}/\{name}/cluster/config

(/) @POST /infrastructure_provision/exploratory_environment/image - added new form parameter ""project_name""

(/) @GET /infrastructure_provision/exploratory_environment/lib_list/formatted - new QueryParam ""project_name""

(/) @GET /infrastructure_provision/exploratory_environment/lib_list - added new form parameter ""project_name""

(/) @GET /infrastructure_provision/exploratory_environment/scheduler/\{exploratoryName} -> /infrastructure_provision/exploratory_environment/scheduler/\{projectName}/\{exploratoryName}

(/) @GET /infrastructure_provision/exploratory_environment/scheduler/\{exploratoryName}/\{computationalName} -> /infrastructure_provision/exploratory_environment/scheduler/\{projectName}/\{exploratoryName}/\{computationalName}

/${exploratory}

(/) @PUT /aws/infrastructure_provision/computational_resources/dataengine/\{exploratoryName}/\{computationalName}/config -> /aws/infrastructure_provision/computational_resources/dataengine/\{projectName}/\{exploratoryName}/\{computationalName}/config

(/) @PUT /azure/infrastructure_provision/computational_resources/dataengine/\{exploratoryName}/\{computationalName}/config -> /azure/infrastructure_provision/computational_resources/dataengine/\{projectName}/\{exploratoryName}/\{computationalName}/config

 (/) @PUT /gcp/infrastructure_provision/computational_resources/dataengine/\{exploratoryName}/\{computationalName}/config -> /gcp/infrastructure_provision/computational_resources/dataengine/\{projectName}/\{exploratoryName}/\{computationalName}/config",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2020-02-14 14:53:31,5
13285024,[AWS]: EMR cluster fails due to wrong tags,Scripts couldn't find EMR ID because of wrong tags,AWS DevOps EMR pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-13 08:37:43,2
13284858,[Azure]: Edge creation fails in case if local and external endpoint are on AZURE in one project,"*Preconditions:*
 # SSN is created on Azure
 # External endpoint1 is created on Azure

*Steps to reproduce:*

1. Create project with 2 endpoints: local (Azure), external (Azure)

*Actual result:*
 # One edge creation is successful
 # The other creation fails

*Expected result:*
 # One edge creation is successful
 # The other creation is successful too",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-12 16:09:27,9
13284778,[Notebook]: Need to change behavior of Show Active button,"As a user I want to be notified about failing notebook in 'show active' mode so that I do not need to switch to 'show all' mode.

 

*Case:*

1. on resources_list page 'show active' button is chosen
 2. notebook is creating
 3. notebook creation fails

It should be possibility to 'notify' user about failed computational resource.

Because in 'show active' mode this notebook will not be portrayed until a user change mode to 'show all'.

 

*Acceptance criteria:*

1. user should receive notify about unsuccessful notebook creation in 'show active' mode on resource list page

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Minor,2020-02-12 10:33:34,5
13284772,[Billing report]: Value of total sum is not sticky in Firefox browser,"# (/) Total sum should be sticky when scrolling only for Firefox v.72.0.2 (64-bit)
 # (/) Add horizontal lines for Service Charges column for Edge browser v. 44.18362.387.0
 # (/) If project is not still created all pages are blocked after visiting a 'Billing report' page",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-02-12 10:05:00,5
13284767,[Back-end]: Add possibility to recreate edge node after edge termination/failing,"As user I want to use my instances in case if edge failing or again to create previously terminated edge in the same project using the same endpoint so that it allows me do not create project with new name and use the previous name.

If we terminate edge (or edge has been failed) we could not create the new edge in the same project and the same endpoint.

Github issue: [https://github.com/apache/incubator-dlab/issues/731].

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2020-02-12 09:51:16,6
13284689,It should be possible to create notebook with existing name in another project for the same user,"*Preconditions:*

1. Notebook with name 'Notebook1' is created in Project1 by user1

*Steps to reproduce:*

1. Create notebook with name 'Notebook1' in Project2 by user1

*Actual result:*

1. Notebook creation is forbidden

*Expected result:*

1. Notebook creation is allowed",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-11 22:50:22,6
13284676,[GCP][Project creation]: Subnet search should be by CIDR according to VPC,"*Preconditions:*
 # SSN is created on GCP
 # Remote endpoint is created on GCP

*Steps to reproduce:*

1. Create project using external endpoint fromGCP

*Actual result:*

1. Project creation fails

*Expected result:*

1. Project creation is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-02-11 21:56:38,3
13284635,[GCP]: Value of user_tag should contain username but not random generated value,"Example:
For user - demo1_test1@epam.com the value for user tag should be 'user : demo1-test1'.
See attachment",Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-02-11 17:57:53,2
13284628,[Azure]: Project creation fails,"*Preconditions:*

1. SSN is created on Azure

*Steps to reproduce:*

1. Create project

*Actual result:*

1. Project creation fails
Failed to generate variables dictionary. Exception: name 'edge_conf' is not defined""}

*Expected result:*

1. Project creation is successful",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-02-11 17:13:25,2
13284583,Different bucket behavior during edge termination,"*SSN is created on AWS:*
 1. If create edge on local endpoint two buckets will be created 
 - endpoint-local-shared-bucket
 - project-local-bucket

2. If create edge on remote endpoint (GCP) only one bucket will be created 
 - project-remote-bucket

Endpoint-remote-shared-bucket is created during endpoint creation. For this we have task https://issues.apache.org/jira/browse/DLAB-1536 *{color:#00875a}It is fixed now{color}*

 3. If delete edge on local endpoint
 - endpoint-local-shared-bucket remains
 - project-local-bucket -deletes

4. If delete edge on remote endpoint (GCP):
 - endpoint-remote-shared-bucket remains
 - project-remote-bucket remains
----

*SSN is created on GCP:*
 1. If create edge on local endpoint two buckets will be created
 - endpoint-local-shared-bucket
 - project-local-bucket

2. If create edge on remote endpoint (AWS) only one bucket will be created
 - project-remote-bucket

Endpoint-remote-shared-bucket is created during endpoint creation. For this we have task https://issues.apache.org/jira/browse/DLAB-1536 *{color:#00875a}It is fixed now{color}*

3. If delete edge on local endpoint 
 - endpoint-local-shared-bucket remains
 - project-local-bucket -remains

4. If delete edge on remote endpoint (AWS)
 - endpoint-remote-shared-bucket remains
 - project-remote-bucket deletes (but then appears name in S3)
----

*SSN is created on AZURE*

1. If create edge on local endpoint two buckets will be created
 - {color:#00875a}endpoint-local-shared-bucket{color}
 - {color:#00875a}project-local-bucket{color}

2. If create edge on remote endpoint
 - {color:#00875a}project-remote-bucket{color}
 - {color:#00875a}endpoint-remote-shared-bucket{color}

3. If delete edge on local endpoint 
 * {color:#00875a}+project-local-bucket -deletes+{color}
 * {color:#00875a}endpoint-local-shared-bucket remains{color}

4. If delete edge on remote endpoint (Azure)
 - {color:#00875A}project-remote-bucket deletes {color}
 - {color:#DE350B}endpoint-remote-shared-bucket deletes{color}
 - {color:#DE350B}fails the edge (on the same remote endpoint) in other project{color}

5. If delete edge on remote endpoint (GCP)
 - endpoint-remote-shared-bucket remains
 - project-remote-bucket remains

6. If delete edge on remote endpoint (AWS)
 - {color:#00875a}endpoint-remote-shared-bucket remains{color}
 - {color:#00875a}project-remote-bucket deletes{color}",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-11 13:32:55,9
13284582,Remote endpoint-shared-bucket should be created during edge creation,"Now remote endpoint-shared-bucket is created during remote endpoint creation, but should be during edge creation.

This implementation should be pushed in develop and in branch for terraform.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-11 13:30:54,3
13284578,[Billing report]: The same project should not be portrayed as two projects if contains upper case,"If project name contains upper case in billing report page appears two different projects (see attachment)

It should be fixed in develop and internal release branch.",Back-end Debian GCP RedHat,[],DATALAB,Task,Major,2020-02-11 13:23:46,6
13284576,[Front-end]: Add possibility to recreate edge node after edge termination/failing,"As user I want to use my instances in case if edge failing or again to create previously terminated edge in the same project using the same endpoint so that it allows me do not create project with new name and use the previous name.
----
If we terminate edge (or edge has been failed) we could not create the new edge in the same project and the same endpoint.

 

Github issue: [https://github.com/apache/incubator-dlab/issues/731.|https://github.com/apache/incubator-dlab/issues/731]",AWS AZURE Debian Front-end GCP Github_issue RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-11 13:16:22,5
13284574,[List of resource]: Notebook is not convey to DLab UI in Microsoft Edge browser,"*Preconditions:*

1. Notebook is created by user1

Steps to reproduce:

1. Login by user1 in Microsoft Edge browser

*Actual result:*

1. Notebook1 is not in List of resource page

*Expected result:*

1. Notebook1 is in List of resource page",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-11 13:12:33,5
13284330,[Azure]: Add endpoint name to container,"In Azure bucket consists of account and container.

Add endpoint name to container.

Now we have:
 # sbn-project_name-container
 # sbn-shared-container

But should be:
 # sbn-project_name-endpoint_name-container
 # sbn-endpoint_name-shared-container",Azure Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-02-10 14:37:33,2
13284297,Bucket should be tagged according to template,"(!)  *AWS*
 # [Remote endpoint]: 'Shared endpoint bucket'  does not have endpoint_tag as it has 'Project bucket'
----

(!) *Azure*
 # [Local endpoint]: Shared endpoint bucket'  does not have endpoint_tag as it has 'Project bucket'
----

(!) *GCP*
 # All buckets should contain endpoint_tag, product, name, {color:#de350b}*?sbn?*{color}
 # What does 'vi-aws-10...' tag mean. Do we need it?
 # In addition 'Project bucket' should contain project_tag.

About GCP consult with [~omartushevskyi]
----
*AWS*
 1. From shared_bucket remove project_tag. Shared-bucket has 4 tags, project bucket has 5 tags.
 *AZURE*
 1. Added endpoint_tag to shared-bucket
 2. Name -> sbn-tag for two buckets
 *GCP*
 1. to shared bucket added three tags (tag-name, endpoint-tag, project:dlab, sbn-tag)
 2. to project bucket added project-tag",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-02-10 12:36:38,2
13283920,[AWS]: Endpoint_tag is not added to EMR,"*Preconditions:*

1. EMR is created

*Steps to reproduce:*

1. Go to EMR console

*Actual result:*

1. Endpoint_tag is absent

*Expected result:*

1. Endpoint_tag is present

 ",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-02-07 12:02:20,2
13283904,Computational resources should also contain endpoint name,Also take into consideration cloud name limitation. Maybe we should limit endpoint name.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-07 10:24:21,9
13283899,[Azure][GCP] Rarely notebook/ssn are not created successful from the first attempt,"This bug was found on Azure/GCP for notebooks Zeppelin (Azure)/Jupyter (Azure)/DeepLearning (GCP)/TenzorFlow (Azure) during auto-test. The second attempt was successful by manually creation.

*Preconditions:*

1. SSN is created on Azure

*Steps to reproduce:*

1. Create Jupyter

*Actual result:*

Jupyter creation fails:

Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)

*Expected result:*

Jupyter creation is successful
----
Also spark cluster is not created (Jupyter) on Azure.
----
(!) This bug was reproduced for DeepLearning on GCP (29/10/2020, 23/11/2020, 18/12/2020).

(!) Such kind of bug was reproduced for DeepLearning on AWS (23/11/2020, 18/02/2021. 25/02/2021).

(i) A new bug showed up. See attachment 'Azure-npm-16-04-2021'
{code:java}
[datalab-user@52.183.7.69] out: E: The package omsagent needs to be reinstalled, but I can't find an archive for it.
[datalab-user@52.183.7.69] sudo: cat /tmp/apt-get.log
[datalab-user@52.183.7.69] out: 
[datalab-user@52.183.7.69] sudo: npm config set unsafe-perm=true
[datalab-user@52.183.7.69] out: /bin/bash: npm: command not found
{code}
(i) Azure 07/06/2021. SSN creation fails on command apt-get -y install nodejs 2>&1.",AWS AZURE Debian DevOps GCP Known_issues(release2.3) Known_issues(release2.4) pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-02-07 10:02:44,2
13283889,Alter Jenkins job according to the second endpoint creation for the same SSN and cloud,"*Preconditions:*
 # SSN is created on AWS
 # Remote endpoint1 is created on GCP

*Steps to reproduce:*
 # Create remote endpoint2 on GCP from Jenkins job

*Actual result:*
 # Remote endpoint1 is deleted on GCP
 # Remore endpoint2 creation fails on GCP

*Expected result:*
 1. Remote endpoint1 is still available on GCP

2. Remore endpoint2 creation is successful on GCP
----
Ponder if it is cloud related? 

Will the same situation be on Azure/AWS during the second endpoint creation? If yes, then fix that on all clouds.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-02-07 09:39:23,9
13283463,[Connect endpoint]: Add validation for empty field ,"If endpoint name or url are empty and user sens regrets convey error message
 * 'Endpoint name field cannot be empty' 
 * 'Endpoint url field cannot be empty' ",AWS AZURE Back-end Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-02-05 12:25:27,6
13283308,[Azure]: Service_name in Nginx configuration should be dns name instead of IP address,"*Preconditions:*
 # SSN is created on Azure
 # 'Endpoint1' is created on AWS
 # 'Endpoint2' is created on GCP

*Steps to reproduce:*

1. Create one  project with 'local', 'Endpoint1', 'Endpoint2' endpoints

*Actual result:*
 # Docker for 3 edge nodes run with '0'
 # 'Local' endpoint has running status on DLab UI
 # 'Endpoint1' has still creating status on DLab UI
 # 'Endpoint1' has still creating status on DLab UI

*Expected result:*
 # Docker for 3 edge nodes run with '0'
 # 'Local' endpoint has running status on DLab UI
 # 'Endpoint1' has running status on DLab UI
 # 'Endpoint1' has running status on DLab UI

 ",AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-04 18:37:00,9
13283299,[Front-end]: Notebook name should be unique per project,"If the same name is already in the project convey error message:

'This name already exists in the project.'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-02-04 18:10:04,5
13283298,[Back-end]: Notebook name should be unique per project,"If the same name is already in the project convey error message:

'This name already exists in the project.'",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-04 18:09:41,6
13283296,[Back-end]: Remove actions from quota popup,"# Change 'Manage environment' -> 'Manage DLab quotas' on popup and button name
 # Remove stop/terminate action from popup
 # If all edge nodes are in terminated status in the same project do not convey this project to 'Manage DLab quotas' popup. Ponder about failed edge node****",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-02-04 17:42:11,6
13283200,Report footer should be sticky and always present while scrolling down,Report footer (total and its value) should be sticky and always present while scrolling down.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Minor,2020-02-04 09:17:40,5
13283004,[GCP]: Edge creation fails in case if local and external endpoint are on GCP in one project,"*Preconditions:*
 # SSN is created on GCP
 # External endpoint1 is created on AWS
 # External endpoint2 is created on GCP

*Steps to reproduce:*

1. Create project with 3 endpoints: local (GCP), external (AWS), external (GCP)

*Actual result:*
 # Local creation fails
 # External (AWS) creation is successful
 # External (GCP) creation fails

*Expected result:*
 # External (AWS) creation is successful
 # Local creation  is successful
 # External (GCP) creation  is successful

 ",Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-02-03 15:44:44,9
13282993,Available lib list is not get due to request entity too large,"*Preconditions:*
 # SSN is created on GCP
 # Endpoint is created on AWS
 # Notebook is created on AWS

*Steps to reproduce:*

1.  Go to 'List of resource' page

2.  Click action menu for notebook

3. Choose manage libraries

4. Choose 'Select resource' in drop down list 

*Actual result:*

1. Available lib list is not get successful

*Expected result:*

1. Available lib list is get successful
----
Available lib list was get successful after 40 minutes of trying.

This bug reproduces especially for RStudio",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-02-03 14:51:02,7
13282977,[AWS][Project creation]: Subnet search should be by CIDR according to VPC,"*Preconditions:*
 # SSN is created on GCP
 # Endpoint is created on AWS
 # Notebook is created on AWS

*Steps to reproduce:*

1. Create any cluster on AWS

*Actual result:*

1. Cluster creation fails

*Expected result:*

1. Cluster creation is successful",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-02-03 12:51:19,3
13282951,Investigate why some templates disappear after endpoint/SSN stopping only for remote GCP endpoint,"*Preconditions:*
 # SSN is created on AWS
 # Externel endpoint is created on GCP
 # One project has edge node on AWS (Local) the other edge on GCP (external endpoint)
 # All instances including SSN are stopped

*Steps to reproduces:*
 # Start SSN
 # Start endpoint
 # Change status endpoint in Mongo
 # Start edge node
 # Look available template for external endpoint on GCP

*Actual result:*
 # Jupyter is available
 # RStuduo is available
 # Deeplearning is available
 # Zeppelin is not available
 # Superset is not available
 # JupyterLab is not available
 # Jupyter with TensorFlow is not available

*Expected result:*
 # Jupyter is available
 # RStuduo is available
 # Deeplearning is available
 # Zeppelin is available
 # Superset is available
 # JupyterLab is available
 # Jupyter with TensorFlow is  available

 ",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-02-03 10:28:18,6
13282645,Investigate why DLab instance status are not synced up with cloud status,"If change instance status from cloud console the Dlab instance status is not updated:
 # Edge node
 # Notebook
 # Cluster (master)

Please investigate the reason of that.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-01-31 15:22:06,6
13282595,[Libary management]: 'Formatted exploratory name ' request should not be repetitive in list of resources page,"This bug reproduces for local and external endpoints

*Preconditions:*
 # SSN is created in multiple cloud
 # notebook is created

*Steps to reproduce:*

1. Install any kind of libs for notebook

*Actual result:*

'Formatted exploratory name ' request is duplicated on  list of resources page

*Expected result:*

'Formatted exploratory name ' request is not on  list of resources page
----
This request should appear if open 'Manage libraries' popup
----
Display 'Close'/'Install' buttons lower.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-01-31 12:10:25,5
13282591,[Project page]: Combine the similar action for a project,"# Remove 'Delete project' from action
 # Stop edge node popup:  change 'Select the items you want to stop' -> 'Select the edge nodes you want to stop'
 # Terminate edge node popup:  change 'Select the items you want to terminate' -> 'Select the edge nodes you want to terminate'
 # Add 'Select all' for stop/terminate edge node popup",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-31 11:28:30,5
13282570,Augment the project page,"# Stop edge node action should stop all its resources as well. It should be the same functionality as it was in 'Manage environment' popup

Do not forget that stopping edge node depends on instance statuses.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-31 09:55:08,6
13282564,Remove actions from quota popup,"# Change 'Manage environment' -> 'Manage DLab quotas' on popup and button name
 # Remove stop/terminate action from popup.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-31 09:29:40,5
13282550,[Azure]: Project creation fails,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create project

*Actual result:*
 # Project creation fails:

Failed to generate variables dictionary. Exception: 'default_endpoint_name'

*Expected result:*

Project creation is successful",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-01-31 08:19:38,2
13282438,[List of resources]: Endpoint disconnection should not cause disappearing previously available instance,"*Preconditions:*
 # SSN is created in AWS
 # External endpoint is created on GCP
 # One project is created for two clouds
 # notebooks are created for each cloud

*Steps to reproduce:*
 # Go to 'Mange environment' popup
 # Click 'Endpoints' button
 # Click disconnect external endpoint with resources termination
 # Go to list of resource

*Actual result:*

1. Any notebooks are not available in list of resource

*Expected result:*

1. All notebooks are not available in list of resource

 ",AWS Azure Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-01-30 16:50:45,5
13282437,[List of resources]: Endpoint disconnection should not cause disappearing previously available instance,"*Preconditions:*
 # SSN is created in AWS
 # External endpoint is created on GCP
 # One project is created for two clouds
 # notebooks are created for each cloud

*Steps to reproduce:*
 # Go to 'Mange environment' popup
 # Click 'Endpoints' button
 # Click disconnect external endpoint with resources termination
 # Go to list of resource

*Actual result:*

1. Any notebooks are not available in list of resource

*Expected result:*

1. All notebooks are not available in list of resource

 ",AWS Azure Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-01-30 16:50:16,6
13282433,Project termination fails in multiple cloud,"*Preconditions:*
 # SSN is created in AWS
 # External endpoint is created in GCP
 # One project is created for two clouds
 # Notebook is created in AWS

*Steps to reproduce:*
 # Go to 'Environment management' popup
 # Click 'Mange environment' button
 # Click 'Terminate' action

*Actual result:*

1. Project termination fails

*Expected result:*

1. Project termination is successful",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-01-30 16:35:02,9
13282427,[Environment management]: Cluster issues during notebook stopping/termination,"# (/) Convey cluster size during notebook stopping/terminating on pop up
 # (/) Remove grid header during stopping/termination notebook if DE is terminated/failed
 # (/) Convey DES which will be terminated during notebook stopping/termination.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-30 16:18:53,5
13282406,Status instance should be not in progress in case of failing to open result.json,"The bug reproduces if terminate project via 'Manage environment' popup. 

In case of termination project fails the status notebook is still 'terminating'.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-01-30 15:16:34,6
13282388,Create script to clear all created resources via DLab on external endpoint ,"Bucket, images, vpc, subnet are left after endpoint/ssn termination via Jenkins job, however they should be cleared.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-01-30 14:37:34,9
13282367,[AWS]: Notebook creation fails on external endpoint in the same cloud where SSN is deployed,"*Preconditions:*
 # SSN is created on AWS
 # External 'endpoint1' is created on AWS

*Steps to reproduce:*

1. Create notebook on External 'endpoint1' 

*Actual result:*

1. Notebook creation fails

*Expected result:*
 # Notebook is created successfully

",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-01-30 12:23:28,9
13282365,Notebook name should also contain endpoint name,Also take into consideration cloud name limitation. maybe we should limit endpoint name.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-30 12:11:01,2
13282334,Noteboks/links are not loaded successfully if project contains more than one edge,"*Preconditions:*
 # SSN is created on AWS
 # External endpoint is created on GCP
 # Notebook1/cluster1 are created on GCP external endpoint
 # Notebook2/cluster2 are created on AWs external endpoint

*Steps to reproduce:*
 # Refer the notebook1/cluster1 link
 # Refer the notebook2/cluster2 link
 # 

*Actual result:*

1. Links for one notebook/cluster are loaded successfully

2. Links for the other notebook/cluster are not loaded successfully

*Expected result:*

1. All notebook/cluster links are loaded successfully 

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-01-30 08:15:56,3
13282216,Add Superset and Jupyterlab template,"# Superset for GCP
 # JupyterLab for AWS/GCP
 # TensorFlow with Rstudio for AWS
----
Name difference:
Jupyter with TensorFlow on AWS/GCP, but on Azure - TensorFlow.

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-29 15:29:39,9
13282202,Administrative page appear for user who has not permission,"*Preconditions:*
 # User1 is not allowed to do administrative operations
 # User1 one is located on 'Resource list' page

*Steps to reproduce:*
 # Go to SSN via ssh
 # Run a command 'sudo supervisorctl restart all'
 # Open web where user1 is logged in DLab
 # Click 'Refresh' button

*Actual result:*

1. Administrative page appears

*Expected result:*

1. Administrative page does not appear",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-29 14:45:56,5
13282195,Roles does not trigger by group (keycloak),"*Preconditions:*
 # 'Project1' is created
 # User1  from group1 is assigned to 'Project1' 
 # '$anyuser' is allowed only to do administrative operation
 # '$anyuser' is not assigned to 'Project1' 

*Steps to reproduce:*
 # Go to 'List of resources' by user1
 # Click '+Create new' button
 # Select project
 # Select endpoint
 # Select template

*Actual result:*

1. 'Select template' is disabled

*Expected result:*

1. 'Select template' is enabled",AWS AZURE Back-end Debian GCP Known_issues(release2.3) Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-29 14:17:10,1
13282168,Notebooks are not conveyed to 'Resource list' page,"*Preconditions:*
 # SSN is created
 # Edhe node1 is created on AWS
 # Edge node2 is created on GCP

*Steps to reproduce:*

1. Create any notebook in any project

*Actual result:*

1. Notebook is not portrayed on 'Resource list' page

*Expected result:*

1. Notebook is portrayed on 'Resource list' page",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-01-29 12:28:50,6
13282155,Project status is not auto-updated," 
----
*Preconditions:*

1. At least one project is created <Project1>

*Steps to reproduce:*

1. Create <Project2>

*Actual result:*

1. 'Project2' does not appear in grid until hitting 'Refresh' button

*Expected result:*

1.  'Project2'  appears in grid automatically
----
2. Project status does not auto-change if stop/start/terminate project
-----


3. Previously checked off edge should be cleared after closing popup 'Stop edge node'/'Terminate edge node'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-01-29 11:07:31,5
13282133,Shared endpoint bucket should be created during project creation,"Now shared endpoint bucket is created during SSN deployment. And during project creation only project bucket is created.

However should be:
 # Remove shared endpoint bucket during SSN deployment
 # During project creation the following buckets should be created:

 * shared endpoint bucket
 * project bucket",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-29 09:13:27,3
13282126,[GCP]: It is impossible to create Dataproc,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*
 # Select cluster type 'Dataproc'
 # Go to console (F12)

*Actual result:*

1. The error appears ""n.DICTIONARY[t.context.$implicit.image] is undefined'

*Expected result:*

There is not any error",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-01-29 08:07:55,5
13281990,SSN creation deletes predefined VPC/subnet in case of SSN creation fails,"*Steps to reproduce:*
 # Create SSN with predefined VPC/subnet
 # Fail SSN creation

*Actual result:*
 # Predefined VPC is deleted
 # Predefined subnet is deleted

*Expected result:*
1. Predefined VPC is not deleted

2. Predefined subnet is not deleted

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-28 16:41:15,3
13281898,Job should check also inactive endpoint status,"Now we have job which checks endpoint active status. However if endpoint changes its status <Active> -> <Inactive> the job will not  more check the status.

But the job should also check  inactive endpoint status.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-01-28 09:45:44,6
13281889,[Azure]: Notebooks/Data engine links are not loaded su,"*Preconditions:*

1. Data Engine is created on Notebook

Steps to reproduce:

1. Refer the Notebook/Data Engine links

*Actual result:*
 # Notebook link opens with error
 # Data Engine link opens with error

*Expected result:*
 # Notebook link opens successfully
 # Data Engine link opens successfully",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-01-28 09:12:09,3
13281704,[AWS]: Notebook/cluster links are not opened successfully,"*Preconditions:*

1. Cluster is created on notebook

*Steps to reproduce:*

1. Go to notebook link

*Actual result:*

1.  Links is not opened successfully

*Expected result:*

1.  Links is  opened successfully
----
The same issue appears if go links in cluster job tracker.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2020-01-27 12:14:20,3
13281699,After some minutes of logging web terminal is not opened successfully,"*Preconditions:*

1. Notebook is in running status

*Steps to reproduce:*
 # Log in DLab
 # Wait 10 minutes
 # Go to web terminal via DLab UI

*Actual result:*

1. Web terminal is not opened successfully

*Expected result:*

1. Web terminal is opened successfully

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-27 12:03:16,6
13281698,Set of tasks connected with with cloud specific and statuses,"1. (/) Resource list page: remove header during Notebook stopping if Data Engine is terminated

2. (/) Project page: do not convey failed edge node during project termination

3. (/) Project page: if hove the mouse endpoint name arrow-cursor should not appear

4 .(/) Change hint 'Select endpoint' -> 'Select template' in 'Select template' drop down list

5. (/) Cloud provider is not defined, so it is impossible to create cluster on external endpoint

6. (/) Remove 'Create AMI' action for GCP",AWS AZURE Debian Front-end GCP RedH pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-27 12:02:02,5
13281656,[Rstudio][Zeppelin][Jupyter][Rstudio-TensorFlow]: Notebook creation fails due to 'devtools' absence,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Create Zeppelin/RStudio/Jupyter/RStudio with TensorFlow

*Actual result:*

1. Zeppelin/RStudio/Jupyter/RStudio with TensorFlow creation fails

*Expected result:*

1. Zeppelin/RStudio/Jupyter/RStudio with TensorFlow are created successfully",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-01-27 08:27:47,9
13281401,Keycloak configuration fails during Edge creation,"*Preconditions:*

SSN is created

*Steps to reproduce:*

1. Create project

*Actual result:*

1. Keycloak configuration fails

*Expected result:*

1. Keycloak configuration  is successful

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2020-01-24 14:50:05,9
13281190,Implement demo mode for marketplace,"As a potential user I want try DLab usage in order to study it better and to know if it is acceptable for me, so that I do not waste money and time for deployment for cognitive purpose.
 Github issue: [https://github.com/apache/incubator-dlab/issues/739]

 

(i) This ticket is done, however there are a lot of tasks/bugs/improvement which are related to this ticket and top of that this ticket is in github issue.

Branch for this ticket DLAB-1476. ",Back-end Debian GCP Github_issue,['DataLab Main'],DATALAB,Task,Major,2020-01-23 13:20:41,1
13281177,[GCP]: Billing is not correct updated for period containing two years,"*Preconditions:*
 # SSN is created in previous year
 # Billing is available on cloud for two years
 # User is logged in DLab

*Steps to reproduce:*

1. Go to billing report page/detailed billing

*Actual result:*
 # Detailed billing is available only for current year for GCP

 *Expected result:*
 # Detailed billing is available only for two years for GCP

 ",Back-end Debian GCP Known_issues(release2.3) RedHat,['DataLab Main'],DATALAB,Bug,Major,2020-01-23 12:10:25,6
13281170,[AWS]: Investigate why web terminal is opened not always successfully,"*Preconditions:*
 1. Notebook is in running status in AWS
 2. User is located on 'Resources list' page

*Steps to reproduce:*
 1. Click action menu for notebook
 2. Chose 'Open terminal'

*Actual result:*
 1. Web terminal is not opened successfully 
 Error 401 for tunnel connection

*Expected result:*
 Web terminal is opened successfully without error 
----
The frequency of reproducing this bug is approximately once from five attempts.",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2020-01-23 11:27:27,6
13280994,Values of drop down list are unavailable only for Safari/MS edge browsers,"*Preconditions:*
 # User is logged in DLab

*Steps to reproduce:*
 # Go to 'Roles' page
 # Click 'Roles' drop down list

*Actual result:*

1. Values of drop down list are unavailable

*Expected result:*

1. Values of drop down list are present",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-01-22 15:36:03,5
13280972,[GCP]: Superset creation fails on stage of keycloak configuring,"*Preconditions:*

1. Project is created on GCP

*Steps to reproduce:*

1. Create Superset

*Actual result:*

1. Superset creation fails 

*Expected result:*

1. Superset is created successful

 ",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2020-01-22 14:02:17,3
13280965,[Git credentials]: Validation are not triggered after 'Password' altering,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1.  Clock 'Git credential' button
 2.  Put <1111111> in 'Password' text box
 3. Put <1111111> in 'Confirm password' text box
 4. Remove one symbol from 'Password' text box

*Actual result:*

1. There is no error message

*Expected result:*

There is an error message:
 'Passwords don't match.'

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-01-22 12:42:32,5
13280943,Fix issue with cloud properties for endpoint,"1. Fix issue with KEYCLOAK_AUTH_SERVER_URL
2. Fix issue with VPC_ID and SUBNET_ID",AWS AZURE Debian DevOps GCP RedHat pull-request-available,[],DATALAB,Task,Major,2020-01-22 10:49:02,9
13280925,[Environment management][Library management]: Set of minor issues,"# (/) Environment management page: header should not move to left during extending.
 # (/) Library management pop up : installation status is not auto-updated
 # (/) Computational resources pop up: Hint for slave is absent",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-01-22 09:28:57,5
13280775,[Computational resource]: Integration tests in Gherkin,Write integration tests for compute management in Gherkin.,AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-21 14:20:03,7
13280736,Embed cloud provider into 'local' endpoint during deployment,"Implement changes in multiple-cloud branch.

Cloud provider should be conveyed in upper-case.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-01-21 11:19:22,9
13280735,Convey 'local' endpoint and self-service url in self-service collection during deployment,Implement changes in multiple-cloud branch,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2020-01-21 11:15:59,9
13280734,Set <SSN_INSTANCE_SIZE> in self-service.yml,Implement changes in multiple-cloud branch,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-21 11:11:20,9
13280571,[Library management]: Integration tests in Gherkin new,Write integration tests for project management in gherkin.,AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-20 14:52:44,7
13280557,Prevent stop action if notebook is in reconfiguring status,Remove stop action from action menu if notebook is in reconfiguring status.,AWS AZURE Debian DevOps GCP RedHat,[],DATALAB,Task,Minor,2020-01-20 12:57:11,5
13280534,UI issues on 'Manage library' page,"*Preconditions:*

1. Notebook is in running status

*Steps to reproduce:*

1. Go to 'manage library' page

*Actual result:*
 # (/) Notebook name is in #718ba6 color
 # (/) Header grid is broken
 # (/) Action filter is in oval if over the mouse
 # Installation status is not auto-updated

*Expected result:*

 1. Notebook name is in #455c74 color

2. Header grid is not broken

3. Action filter is in circle if over the mouse

4. Installation status is  auto-updated",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2020-01-20 11:15:03,5
13279855,[Administration page]: Remove 'Allow to' from 'Roles' drop down list,Change 'Allow to use Standard_NC6 instance shape for notebook' -> 'Use Standard_NC6 instance shape for notebook''Roles' drop down list.,AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-01-16 15:29:02,6
13279835,Role sort auto-triggered after group updating,"*Preconditions:*

1. At least three groups are created

*Steps to reproduce:*
 # Click 'Roles' drop down list of any group
 # Uncheck the topper item in 'Roles' drop down list
 # Confirm changes

*Actual result:*

Editable group changes its position

*Expected result:*

 Editable group does not changes its position",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2020-01-16 13:57:45,5
13279790,[MS Edge browser]: UI style should be consistent,"*Resource list page:*

1. Header horizontal line is absent for MS Edge browser
----
 

Microsoft Edge v.44.18362.387.0

 

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-01-16 09:50:09,5
13279777,Change link to maven repository,"Any notebook is not created due to: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.3/hadoop-azure-2.7.3.jar]
 out: Connecting to 172.31.0.5:3128... connected.
 out: Proxy request sent, awaiting response... 501 Not Implemented
----
For more information read this article: https://support.sonatype.com/hc/en-us/articles/360041287334",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-16 08:51:50,3
13279548,Add documentation for existing Keycloak,Add Keycloak in pre-requisites and describe parameters in README.md.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-15 09:56:50,2
13279536,Endpoint url/name should be unique,"1. Make Endpoint url unique per DLab.

If such endpoint is already added convey error message :

'This endpoint url  already exists.'

2.  Endpoint name and endpoint url should be case insensitive.

3. Add error message for name 'This endpoint url  already exists' 

4. Extend validation for endpoint url.",AWS AZURE Back-end Debian Front-end GCP pull-request-available,[],DATALAB,Task,Minor,2020-01-15 09:13:55,5
13279535,[List of Resource][Environment Management]: Report header should be sticky and always present while scrolling down,"As a user I want always to see List of Resource/Environment Management header on the top of window during scrolling down so that I will be able to see titles of grid values.

 

*Acceptance criteria:*

1. List of Resource/Environment Management header is always present on the top of window during scrolling down",AZURE Debian Front-end RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Minor,2020-01-15 08:58:11,5
13279377,Cloud endpoint API,"# As far as I know only Endpoint and projects were done. However endpoint is absent
 # Calling project API does not work
 # Do we need bohdan_hliva@epam.com in '../assets/endpoint-api.json' and bhliva in servers address?
 # Do we need Contact the developer?

 

 ",AWS AZURE Back-end Debian GCP RedHat,['Cloud endpoint'],DATALAB,Task,Minor,2020-01-14 14:57:00,1
13279361,Endpoint url should be unique,"1. Make Endpoint url unique per DLab.

If such endpoint is already added convey error message :

""Endpoint url with this address already exists in system""

2.  Endpoint name and endpoint url should be case insensitive.

3. Change error message for name 'Endpoint with passed name already exist in system' -> 'Endpoint with this name already exists in system'",AWS AZURE Back-end Debian GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2020-01-14 13:55:59,6
13279355,UI tasks for cloud endpoint API,"1. Rename 'Cloud endpoint API' -> 'Cloud Endpoint API'

2. If main menu is expended it should be only 'Cloud Endpoint API' and without icon

3. If open in new tab/window 'Administration'/'Billing report ' should be present as well

4. [Shrunken menu]: Cloud Endpoint API icon should change color if over the mouse

5.  Adjust active aria for Cloud Endpoint API according to its limits",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-14 13:40:08,5
13279042,Endpoint status is not conveyed to front-end,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Go to Project page

*Actual result:*

1. Endpoint status is absent

*Expected result:*

1. Endpoint status is present",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-13 10:59:31,9
13279038,Drop down list structure should disappear simultaneously with other items,Drop down list structure disappear with delay (see attachment).,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-01-13 10:37:25,5
13278726,[AWS]: Billing data updates with delay for 2020 year,"*Preconditions:*
 # Environment is created in 2020 year
 # Billing is loaded in S3 bucket

*Steps to reproduce:*

1. Go to Billing report page in DLab

*Actual result:*
 # Billing data is unavailable in DLab
 # Billing data is available in S3 buckect
 # Billing will be available in DLab only for the next day

*Expected result:*

1. Billing data is available in DLab
----
 It  tries to load billing data only for 2019 year step by step via scheduling, and after that it tries to load for 2020 year.",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-10 14:31:01,6
13278689,[Branch-515][AWS]: R_templ.r playbook runs with error on RStudio with TensorFlow template,"*Preconditions:*

1. Rstudio-TensorFlow is created

*Steps to reproduce:*

1. Run R_templ.r playbook on Rstudio-TensorFlow

*Actual result:*

1. Playbook runs with error 

*Expected result:*

Playbook runs successfully

 
----
The playbook still runs with  another error:
 Error in eval(lhs, parent, parent) : object 'model' not found
 (09/06/2020).
----
The bug is still reproduced from dev (10/08/2020):

Error in eval(lhs, parent, parent) : object 'model' not found

 ",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Minor,2020-01-10 10:46:39,3
13278485,Remove extra arrow if some object is disabled only for Mac,"See attachment, only for Mac.

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2020-01-09 15:35:01,5
13278144,UI style should be consistent,"*Resource list page:*
 1. (/) All horizontal lines should have the same style, but the second line from the top is bolder

2.  (/) If there is not any instances the filter confirmation icons should have the same style, but the tick is lighter

3.(/) Extra upper dots in dropdown list for notebook and computational resources

4. (/)  Extend area for forbidden icon for create analytical tool pop up

5. (/) Header horizontal line is absent for Firefox
----
 Firefox v.72.0 (64-bit)

Microsoft Edge v.44.18362.387.0

Chrome v. 79.0.3945.88

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2020-01-08 11:44:06,5
13278127,[Back-end]: Support a multiple cloud functionality,"As a user I want to use more than one endpoints for a project so that one project can have environment on AWS, Azure, GCP.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2020-01-08 10:28:18,6
13277381,[Project management]: Investigate how to display pen icon for current tab,"1. When user is located on current tab the 'pen' icon should be displayed. However the icon displays only during the first visiting the tab. However the 'pen' icon should be displayed during the following visiting as well.
2. Endpoint header should be sticky and does not change a position while scrolling down (see attachment)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-03 14:40:24,5
13277349,[Connection testing]: Some characters are not considered in the end if endpoint url is correct,"Preconditions:

1. SSN is created

*Steps to reproduce:*
 # Put endpoint url https://localhost:8084-_:,.!$&*()=+;'oLeh/
 # Fill other text-box by valid data
 # Click 'Test Endpoint' button

*Actual result:*

1. Test endpoint is successful

*Expected result:*

1. Test endpoint is not successful

If endpoint_url is corect and contains any of other symbols -_:,.!$&*()=+;'oLeh the testing is successful, but should be failed.
Request URL: http://ec2-44-230-8-93.us-west-2.compute.amazonaws.com/api/endpoint/url/https%3A%2F%2Flocalhost%3A8084-_%3A%2C.!%24%26*()%3D%2B%3B'~oLeh%2F?noCache=1578051124758",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2020-01-03 11:08:29,6
13277338,Add test functionality for already added endpoint (endpoint list),"Now it is implemented  testing endpoint for connection during endpoint adding, but we cannot check it among already added endpoint.

So add test functionality for already added endpoint in endpoint list.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2020-01-03 09:57:44,5
13276892,Alter action menu for project management,"#  (/) Do not convey project name in action menu. It should be just a project -> 'Edit project', 'Delete project'
 #   Stop or terminate edge node:

 * (/) There is too space between item list and confirmation question. As a result ""No"" ""Yes"" buttons have very bottom  position in the model.
 * (/) Disable button 'Yes' if user does not fetch any item

3.  (/)  Give icon from notebook termination for terminate edge node

4.  (/) Extend aria for action menu. For example if user hovers mouse close to margin of action the action is still highlighted but after hitting it the action is not triggered.

5. (/) Change message on confirmation dialog: '<Project_name> will be disconnected.' -> 'Project <project_name> will be decommissioned.'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-12-30 14:28:20,5
13276651,[Project management]: Integration tests in Gherkin,Write integration tests for project management in gherkin.,AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-27 20:20:00,7
13276578,Error message is not appeared if project quota is more than total one,"*Preconditions:*
 # Environment is created
 # 'Manage environment' popup is opened

*Steps to reproduce:*

1. Set project quota less than total one

*Actual result:*
 # Error messages are not appear
 # 'Apply' button is enable

*Expected result:*
 # Error messages appear
 # 'Apply' button is disabled

 

(!) And after workaround this case appears another issue - error messages have different fonts (see attachments)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-12-27 09:53:58,5
13276572,UI changes for project management,"# Do not highlight tab in which user is located
 # Icon pen should on page  in which user is located",AWS AZURE GCP pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-12-27 09:18:32,5
13276449,Images are not included in billing report,"*Preconditions:*
 # Notebook is created
 # Billing is available for deployed DLab

*Steps to reproduce:*

1. Go to billing report page

*Actual result:*

1. Image is not absent in billing report page

*Expected result:*

1. Image is present in billing report page
----
(!) Images are not included  even in billing on cloud console. 

Check if images are correctly tagged.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-12-26 10:50:15,2
13275745,Investigate why some java-dependencies are not found from DLab UI though they are available in maven repository ,"This case was admited on Zeppelin v 0.8.2 fro Dataproc v.1.4 on GCP

{""code"":404,""message"":""Artifact with id=bus-crypto, groupId=org.aoju and version 5.3.4 not found""}

or
 # 
Request URL:
http://34.83.125.54/api/infrastructure_provision/exploratory_environment/search/lib_list/maven?artifact=com.github.database-rider:rider-core:1.9.1&noCache=1576854537136
 # 
Request Method:
GET
 # 
Status Code:
502 Bad Gateway",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Minor,2019-12-20 15:10:57,6
13275703,[AWS][?Obsolete?]: Spark-submit --version runs with error for EMR v.5.19.0,"*Preconditions:*

1. EMR v.5.19.0 is created

*Steps to reproduce:*
 # Go to EMR via ssh
 # Run a command 'spark-submit --version'

*Actual result:*

1. Error appears: log4j:ERROR Either File or DatePattern options are not set for appender [DRFA-stderr].
 log4j:ERROR setFile(null,true) call failed.
 java.io.FileNotFoundException: /stdout (Permission denied)

*Expected result:*
 # There is no any error

(i) Obsolete: If we update EMR version this ticket will be obsolete.

 ",AWS Debian DevOps Known_issues(release2.3),['DataLab Main'],DATALAB,Bug,Trivial,2019-12-20 10:37:52,3
13275674,[AWS][Azure]: SSN creation fail on RedHat,"*Steps to reproduce:*

1. Deploy SSN on RedHat for AWS/Azure via Jenkins job

*Actual result:*

1. SSN deployment fails

Failed to export image: failed to create image: failed to get layer sha256:f9ea30538fad29e8707380ef43e38ed198d52c06b68e4d2330b2cf95c65580a4: layer does not exist

*Expected result:*

1. SSN deployment is successful",AWS AZURE DevOps RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-12-20 07:46:58,9
13275510,[Back-end][Part2]: Embed shared image functionality on DLab UI,"As an admin, I want to turn on/off of using shared image in project setting so that I can easily switch without changing configuration files via SSH.
Pass the _conf_shared_image_enabled_ parameter to provisioning during exploratory/computational creation.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-19 13:58:48,6
13275503,Scala version is not actual on Jupyter UI for DES/Jupyter,"*Preconditions:*

1. Dataproces v1.3 and v.1.4 are created on Jupyter

*Steps to reproduce:*
 # Go to Jupyter UI
 # Go to kernel

*Actual result:*
 # For Dataproc v.1.3 scala version is 2.12.8
 # For Dataproc v.1.4 scala version is 2.12.8

*Expected result:*
 # For Dataproc v.1.3 scala version is 2.11.8
 # For Dataproc v.1.4 scala version is 2.11.12

----
(!) *For EMR v.5.19.0/v.5.28.0*
 1. Scala version should be 2.11.8/2.11.12

2. R version should be 3.4.1/3.4.1 but is 3.4.4
----
 *For Jupyter* 

Scala version should be 2.11.12, but it is 2.12.8",AWS Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-12-19 13:35:25,3
13275248,Add validation for endpoint disconnection in case if saving related resources,If admin does not terminate resource and  related instance is in status -ing (except for running) convey error message that it is impossible to terminate instance. Error message should appear after a user hint 'Yes' button.,AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-18 12:22:56,6
13275221,Job termination fails if use predefined vpc/subnet,"*Preconditions:*

1. Environment  is created with predefined vpc/subnet

*Steps to reproduce:*

1. Run Jenkins job for SSN termination

*Actual result:*
 # SSN termination fails on step of subnets removing (see job termination from develop (2.3) #418)
 # Buckets are still available

*Expected result:*

1. SSN termination is successful

2. Buckets are removed",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-12-18 10:22:54,8
13275058,Simultaneously DES creation fails per one note and for the same user,"*Preconditions:*

1. Jupyter is created

*Steps to reproduce:*

1. Create simultaneuosly two DES on the same jupyter

*Actual result:*
 # One DES creation fails
 # The other DES creation is successful

*Expected result:*

1. Two DES creation is successful

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-12-17 17:16:29,3
13275054,Simultaneously Data Engine Service creation fails in the same project even if users are different,"*Preconditions:*
 # User1 is assigned to Project1
 # User2 is assigned to Project1

*Steps to reproduce:*
 # Create DES1 for user1
 # Create DES2 for user2

*Actual result:*
 # One DES is created successfully
 # The other DES creation fails

*Expected result:*

1. DES1 is created successfully

2. DES2 is created successfully",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-12-17 16:52:42,3
13275042,[Endpoint disconnection][Front-end]: Convey edge node on confirmation dialog,Also convey related edge node as resource which will be terminated.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-17 15:44:18,5
13275041,[Endpoint disconnection][Back-end]: Convey edge node on confirmation dialog,"Also convey related edge node as resource which will be terminated.

Create endpoint to retrieve all related resource by endpoint (with edge node information).",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-17 15:42:27,6
13275020,Alter information message for endpoint management,"(/) 1.  DLab does not create endpoint, so change information message -> '*Success! Endpoint connected successfully!'*

(/) 2. Change confirmation dialog -> '*Endpoint <Endpoint_name> will be disconnected. Do you want to proceed?'*

(/) 3. '*Success! Endpoint successfully disconnected. All related resources are terminating!*'

(/) 3. If endpoint does not have any resource -> '*Success! Endpoint successfully disconnected*.'",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-17 14:23:34,5
13274978,Extra vertical scrollbar during pages switching,"*Preconditions:*
 # Environment is created
 # User is located in 'resource list' page

*Steps to reproduce:*

1. Go to Billing report page

*Actual result:*

1. Vertical scrollbar appears on the right side

*Expected result:*

1. Vertical scrollbar does not appear on the right side",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2019-12-17 12:10:45,5
13274974,UI issues depending on browser,"*Billing report page:*
 * (/) Horizontal lines are absent for Edge browser (see attachments 'Billing report*')

*Environment management page:*
 * (/) Horizontal line is absent For Edge/Firefox browsers  (see attachments 'Env-management*')

*Connect endpoint:*
 *  (/)  Horizontal line is absent for Edge/Firefox browsers (see attachments 'connect endpoint*')

*Manage Git Credentials:*
 *  (/) Extra scrollbar for Firefox browser after clicking '+Create new' button (see attachment 'Manage git*')",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-12-17 11:59:27,5
13274960,Reconfiguration spark is absent for Data Engine,"*Preconditions:*

1. Data Engine is in running status

*Steps to reproduce:*

1. Click Data Engine name

*Actual result:*

1. Cluster configuration check box is absent on popup

*Expected result:*

1. Cluster configuration check box is present on popup",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-12-17 10:17:03,10
13274952,Add logs into integration tests,Provide logs for slf4j.,AWS AZURE Back-end GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-17 09:48:57,6
13274951,Setup time out for http client,Timeout should be on property file.,AWS AZURE Back-end GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-17 09:44:35,6
13274782,[Notebook name popup]: Cancel' button does not trigger for Cluster configurations,"*Preconditions:*

1. Notebook is in 'running' status

*Steps to reproduce:*
 # Go to 'resource list' page
 # Click Notebook name
 # Check off 'Cluster configurations' check box
 # Click 'Cancel' button

*Actual result:*

1. Notebook name popup is not closed

*Expected result:*

1. Notebook name popup is closed

 ",AWS Azure Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-12-16 15:07:41,5
13274774,[Connect endpoint popup]: Endpoint grid breaks after visiting of manage libraries popup,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*
 # Go to library management popup
 # Close library management popup
 # Go to environment management page
 # Click 'Endpoint' button
 # Go to 'Endpoint list'

*Actual result:*

1. Endpoint grid is broken

*Expected result:*

1. Endpoint grid is  not broken",AWS Azure Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-12-16 14:25:11,5
13274748,[Project page][Back-end]: All actions collect in action menu for every project,"As an admin I want to use action menu for per project.

Action menu contains:
 * start edge node
 * stop edge node
 * terminate edge node
 * edit project

(?) If there is more than one edge node in popup admin can fetch which edge will be stop/start/terminate",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2019-12-16 12:37:11,6
13274402,[Project page]: All actions collect in action menu for every project,"As an admin I want to use action menu for per project.

Action menu contains:
 * start edge node
 * stop edge node
 * terminate edge node
 * edit project

(?) If there is more than one edge node in popup admin can fetch which edge will be stop/start/terminate",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Minor,2019-12-13 12:03:08,5
13274400,[Front-end]: Add Endpoint Status on project page,"As an admin I want to see connection status to endpoint so that I can easily determine if this endpoint is connected to a project.
----
*1.* Endpoint status  <{color:#de350b}*Inactive*>{color}/<{color:#00875a}*Active*{color}>

*1.1.* If endpoint status is *{color:#00875a}Active{color}*:  (?) *endpoint* -><*Name_endpoint*> *Active/Inactive*,

 *Edge node status* -> /*creating*/*stopped*/*stopping*/*running*/*starting*/

*1.2.* If endpoint status is *{color:#de350b}Inactive{color}*: (?) *endpoint* ->*Edge node status* -> *terminating/terminated/failed*

----

For future if endpoint is deleted *endpoint status* -> *N/A*, *Edge node status* -> *N/A*

 
----
----
----
If stop/start/terminate edge node in information message should be edge node  stop/start/terminate but don't  endpoint connect/disconnect (see attachment)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-13 11:43:49,5
13274394,[Back-end]: Add Endpoint Status on project page,"As an admin I want to see connection status to endpoint so that I can easily determine if this endpoint is connected to a project.
----
*1.* Endpoint status  <{color:#de350b}*Inactive*>{color}/<{color:#00875a}*Active*{color}>

*1.1.* If endpoint status is *{color:#00875a}Active{color}*:  (?) *endpoint* -><*Name_endpoint*> *Active/Inactive*,

 *Edge node status* -> /*creating*/*stopped*/*stopping*/*running*/*starting*/

*1.2.* If endpoint status is *{color:#de350b}Inactive{color}*: (?) *endpoint* ->*Edge node status* -> *terminating/terminated/failed*

----

For future if endpoint is deleted *endpoint status* -> *N/A*, *Edge node status* -> *N/A*

 
----
----
----
If stop/start/terminate edge node in information message should be edge node  stop/start/terminate but don't  endpoint connect/disconnect (see attachment)",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-13 11:21:55,6
13274375,Extra warning after assigning user to a project,"*Preconditions:*

1. User1 is not assigned to any project

*Steps to reproduce:*
 # Assigned user1 to Project1
 # Go to any page

*Actual result:*

1. Information message appears, that user1 is not assigned to any project

*Expected result:*

1. Information message does not appear, that user1 is not assigned to any project

 ",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2019-12-13 10:34:11,5
13274347,[GCP]: Dataproc creation fails ,"*Preconditions:*

1. Jupyter/Rstudio/Apache Zeppelin are created

*Steps to reproduce:*

1. Create Dataproc

*Actual result:*
 # Dataproc creation fails
HttpError: <HttpError 403 when requesting https://dataproc.googleapis.com/v1/projects/or2-msq-epmc-dlab-t1iylu/regions/us-west1/clusters?alt=json returned ""Required 'compute.projects.get' permission for 'projects/or2-msq-epmc-dlab-t1iylu'"">

*Expected result:*

1. Dataproc creation is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-12-13 09:05:09,3
13274207,[Libraries management][Part2]: Issues with libraries filter,"1.  (/)   Filter auto-clears after 3 seconds in case of 'No matches found' (at least one library has failed status)

2.  (/)   !image-2019-12-12-19-09-52-982.png! icon is absent for status

3.  (/)   Filter does not trigger for 'v.'. It works for name and for version which contains numbers and dots

4.  (/)   If there is no any value in grid 'No group'/'No resource'/'No resource type'/'No Status' should have color #4A5C89

5.  (/)  Why do we need request for installed library on notebook on resources_list/roles/projects/environment_management/billing_report pages? (at least one library has failed status) (see attachment). If we do not need get rid of them. 

 ",AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-12-12 17:26:44,5
13274117,Name convention for tags should be documented,"Create document which contains example all tags for instances for all clouds in KB.

We should obey consistency in tag names.",AWS AZURE Back-end GCP,['DataLab Main'],DATALAB,Task,Trivial,2019-12-12 11:56:03,1
13274112,[DLab info]: Change link for release notes,"If refer the link of release notes the release notes v2.1 is opned, but should be the release notes v2.2.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-12 11:37:07,6
13273912,[Azure]: Failed to manage git credentials due to 'ssn_storage_account_name' is not defined,"*Preconditions:*
 # Environment is created
 # User is located on 'resources_list' page

*Steps to reproduce:*
 # Click 'Git credentials' button
 # Click 'Add account' button
 # Put valid data
 # Click 'Assign' button
 # Click 'Apply changes' button

*Actual result:*

1. Docker runs with '1'
NameError: name 'ssn_storage_account_name' is not defined

*Expected result:*

1. Docker runs with '0'",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-12-11 15:24:34,2
13273871,[Libraries management][Part1]: Issues with libraries filter,"# (/) Filter auto-clears after 3 seconds
 # (/) Change 'Select resourceType' -> 'Select resource type'
 # (/) Accept and clear icons should change color if hover the mo_emphasized text_use. Accept -> green, clear -> read
 # (/)'Close', 'Install' buttons are hidden if there are too many libraries in grid  
 # (/) In 'Group' drop down list value should be:  Apt/Yum, Python 2, Python 3, Java, R packages, Others
 # (/) Drop down values should not have blue color. The style should be the similar as it is  on the other pages DLab
 # (/) 'No data available' should have center alignment in 'Billing Report'/'List of Resources' pages. However after visiting library management modal window it changes from  center to left alignment
 (/) It should filter by version not only by name in name text box
 # (/) Remove extra information message 'Cannot retry to reinstall failed libraries: Exploratory Jup1 is not running' if notebook is in 'stopped'/'starting'/'Creating Image'/'stopping'/'terminating'/'terminated'/'reconfiguring' statuses
 # (/) If there is no any value in grid the style of drop dawn values should be as it is in 'billing report' page (see attachment)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-12-11 12:49:53,5
13273839,[Promotion page]: Initial slider is not loaded if open links by turn in one browser,"The bug is reproduced onle in Chrome browser (incognito mode), Firefox browser, Microsoft Edge browser.

*Steps to reproduce:*
 # Refer the link [dlab.incubator.apache.org|http://http//dlab.incubator.apache.org] in Firefox browser
 # Refer the link [dlab.apache.org|http://dlab.apache.org/] in new tab of Firefox browser

*Actual result:*

1. Initial slider is not loaded until user uses scrollbar in new tab

*Expected result:*

1. Initial slider is loaded without additional action in new tab

 ",Front-end,['DataLab Main'],DATALAB,Bug,Trivial,2019-12-11 10:31:03,5
13273804,[GCP]: SSN creation fails on stage of setting  role to service_account,"*Steps to reproduce:*

1. Deploy DLab on GCP

*Actual result:*

DLab deployment fails

*Expected result:*

DLab deployment is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Blocker,2019-12-11 08:54:47,3
13273641,Roles are not synced up with groups/users,"*Preconditions:*
 # grp1 is not allowed to create computational resources
 # grp1 is assigned to a project1
 # grp2 is assigned to a project1
 # grp2 is allowed to create computational resources
 # '$anyuser' group is deleted

*Steps to reproduce:*
 # User from grp1 creates computational resource in project1

*Actual result:*
 # User1 is allowed to create computational resources

*Expected result:*
 # User1 is not allowed to create computational resources

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-12-10 15:32:43,6
13273613,[Promotion page]: Initial slider is not loaded until user uses scrollbar,"The bug is  reproduced only in Chrome browser (incognito mode),  Firefox, Microsoft Edge browsers.

*Steps to reproduce:*

1. refer the link [promotion page|http://dlab.apache.org]

*Actual result:*

1. Initial slider is not loaded until user uses scrollbar

*Expected result:*

1. Initial slider is loaded without additional action

 ",Front-end,['DataLab Main'],DATALAB,Bug,Minor,2019-12-10 12:59:34,5
13273559,Edge creation accompanies with error,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create Project

*Actual result:*

1. The error appears:
 Endpoint id:
 ('Failed to attach Project tag to Endpoint', 'An error occurred (InvalidParameterValue) when calling the CreateTags operation: Value ( null ) for parameter resourceId is invalid. Null/empty value for resourceId is invalid')

*Expected result:*
 1. There is not error ",AWS GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-10 09:04:49,9
13272779,The next Superset creation does not used shared image from the first creation,"*Preconditions:*
 # Shared image is enabled
 # Superset is created

*Steps to reproduce:*

1. Create the second Superset

*Actual result:*
 # Superset is created
 # Shared image is not used from previous creation

*Axpected result:*
 # Superset is created
 # Shared image is used from previous creation

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-06 15:21:42,8
13272774,Hint should not be overlapped in 'Select user groups' drop down list,"If create a new project  'Select user groups' hint is overlapped, but should not be.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2019-12-06 15:03:11,5
13272725,Permission for user should be considered during instances creation,"* Preconditions:*
 1. Group1 and Group2 are assigned to a project
 2. User1 from Group1 is not allowed to create resource1
 3. User2 from Group1 is allowed to create resource1
 4. User1 is located on 'resources_list' page

*Steps to reproduce:*
 1. User1 clicks '+Create new' button
 2. Choose available project
 3. Choose available endpoint
 4. Click 'Select template' drop dawn list

*Actual result:*
 1. Resource1 is present in 'Select template' drop dawn list

*Expected result:*
 1. Resource1 is absent in 'Select template' drop dawn list

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-12-06 11:15:37,6
13272719,[JupyterLab][Superset]: Disable 'gear' icon if any action is not available,"If Superset/JupyterLab are in status failed/terminated/terminating/starting/stopping.

 

(?) I was wondering  if the bottom line could be removed in stopped status?",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-12-06 10:56:10,10
13272716,Add information message if user is not allowed to create computational resource,"If user is not allowed to create any of computational resource, convey information message about this.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-06 10:48:23,5
13272707,[AWS][Part2]: Alter billing values according to updates on billing page,"# -'Image' should be used in  'Resource type' column for all images-
 #  User_name should be used in 'User'column if it is custom image
 # -Project_name should be in 'Project' column if it is custom image-
 # -'Shared resource' should be in 'Project' column if it is shared image-
 # -Filter does not work by 'Shared resource' in 'Project' column-
 # -Filter does not work by 'Service' in 'Service' column-",2.3_old AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-06 10:23:27,6
13272545,[AWS]: User:tag for image should also contain resource name,"*Now user:tag value for image is:*
 sbn:sbn  -> dem-1299-aws:dem-1299-aws
 *But should be:*
 sbn:resource_name -> dem-1020aws:dem-1020aws-local-zeppelin-notebook-image

",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-12-05 16:06:31,2
13272439,[GCP][Roles page]: Add Superset in 'Roles' drop down list,Add Create Notebook Superset in 'Roles' drop down list only for GCP.,Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-05 09:53:08,6
13272433,Tag value should be 'null' if tag is not used for resource,"For some resource if tag is not used tag_value is '{}', but should be 'null'.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-05 09:32:28,1
13272430,Rename bucket labels,"1. Shared bucket -> Project bucket

2. Shared project bucket -> Shared endpoint bucket

3. Prevent tag column in 'Billing page'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-12-05 09:21:34,10
13272423,Migrate to Python3,"# -Retrieve available lib list group -error -ModuleNotFoundError: No module named 'xmlrpclib'- -{color:#00875a} fixed 04/02/2021.{color}
 # -Get rid of Python2 during SSN deployment, Notebook creation.-
 *SSN:*

{code:java}
Step 4/28 : RUN apt-get update &&     apt-get -y upgrade &&     apt-get -y install python3-pip python-dev groff vim less git wget nano libssl-dev libffi-dev libffi6 &&     apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
 ---> Running in e349ec65c115 fixed 26/03/2021{code}
(i) {color:#00875a}Fixed{color} in '-tep 4/28 : RUN apt-get update &&'- (03/02/2021).
{code:java}
sudo: cd /opt/datalab/sources/infrastructure-provisioning/src/; docker build --build-arg OS=debian --build-arg SRC_PATH= --file general/files/gcp/base_Dockerfile -t docker.datalab-base:latest fixed 26/03/2021
{code}
(i) {color:#00875a}Fixed{color} in -'sudo: cd /opt/datalab/sources/'- (03/02/2021).
{code:java}
sudo: apt-get -y install supervisor 2>&1 | tee /tmp/tee.tmp; if ! grep -w -E ""(frontend is locked|locked)"" /tmp/tee.tmp > /tmp/apt.log; then echo """" > /tmp/apt.log;fi fixed 26/03/2021{code}
(i) {color:#00875a}Fixed{color} -in 'apt-get -y install supervisor-' (03/02/2021).

(-) {color:#57d9a3}Done{color} -Script for Keycloak client creation should use Python3.- 

*{color:#57d9a3}Done{color} -Jupyter-:*
{code:java}
sudo: apt-get -y build-dep python-matplotlib 2>&1 | tee /tmp/tee.tmp; if ! grep -w -E ""(frontend is locked|locked)"" /tmp/tee.tmp > /tmp/apt.log; then echo """" > /tmp/apt.log;fi - fixed 26/03/2021
{code}
{code:java}
sudo: apt-get -y install nodejs 2>&1 | tee /tmp/tee.tmp; if ! grep -w -E ""(frontend is locked|locked)"" /tmp/tee.tmp > /tmp/apt.log; then echo """" > /tmp/apt.log;fi - fixed 26/03/2021{code}
*{color:#57d9a3}Done{color} -RStudio-:*
{code:java}
sudo: apt-get -y install nodejs 2>&1 | tee /tmp/tee.tmp; if ! grep -w -E ""(frontend is locked|locked)"" /tmp/tee.tmp > /tmp/apt.log; then echo """" > /tmp/apt.log;fi - fixed 26/03/2021
{code}
(i) -Py2 is still available in 'apt-get -y install nodejs'- - {color:#00875a}fixed{color} (26/032021).

*{color:#57d9a3}Done{color} -Zeppelin-:*
{code:java}
sudo: apt-get -y build-dep python-matplotlib 2>&1 | tee /tmp/tee.tmp; if ! grep -w -E ""(frontend is locked|locked)"" /tmp/tee.tmp > /tmp/apt.log; then echo """" > /tmp/apt.log;fi - fixed 26/03/2021
{code}
{code:java}
 
sudo: apt-get -y install nodejs 2>&1 | tee /tmp/tee.tmp; if ! grep -w -E ""(frontend is locked|locked)"" /tmp/tee.tmp > /tmp/apt.log; then echo """" > /tmp/apt.log;fi - fixed 26/03/2021
{code}
(i) -Py2 Is still available in 'apt-get -y install nodejs-' {color:#00875a}fixed{color} (26/032021).

 *3.* -Performance for SSN deployment. It has to take up to 40-50 minutes-. But now it takes 2 hours. *{color:#00875a}Fixed{color}, now ~40m*

*4.1.* -It's impossible to retrieve available lib list for any group. Docker runs with '1'- (26/03/2021): {color:#00875a}F{color}*{color:#00875a}ixed{color} 30.03.2021*
{code:java}
No idea what 'list_libs' is!
Traceback (most recent call last):
  File ""/root/entrypoint.py"", line 154, in <module>
    subprocess.run(""/bin/list_libs.py"", shell=True, check=True)
  File ""/usr/lib/python3.8/subprocess.py"", line 512, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '/bin/list_libs.py' returned non-zero exit status 1.
{code}
*4.2.* Available lib list retrieving: 

+RStudio:+
||Apt/Yum|Still in process: is too long, > 50 min.|
||Py3|Passed|
||R packages|Docker runs with 1,  Error: package or namespace load failed for ‘SparkR’:
 package ‘SparkR’ was installed before R 4.0.0: please re-install it|
||Others|Passed|

+Zeppelin:+
||Apt/Yum|Still in process: is too long, > 50 min. |
||Py3| Passed|
||R packages| Passed|
||Others| Passed|

*5.* Lib installation:
 +RStudio:+
||Apt/Yum|Blocked by lib list retrieving, but if open new window it can be installed|
||Py3|Passed|
||R packages|Failed by lib list retrieving|
||Others|Passed|

+Zeppelin:+
||Apt/Yum|Blocked by lib list retrieving, but if open new window it can be installed|
||Py3|Passed|
||R packages|Passed|
||Java dependency|Passed|
||Others|Passed|

 *6.* Get rid of kernel python2 for notebooks.

*7.* Docker of image creation runs with '1': *fixed 30.03.2021*
{code:java}
No idea what 'create_image' is!
Traceback (most recent call last):
  File ""/root/entrypoint.py"", line 160, in <module>
    subprocess.run(""/bin/create_image.py"", shell=True, check=True)
  File ""/usr/lib/python3.8/subprocess.py"", line 512, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '/bin/create_image.py' returned non-zero exit status 1.
{code}
 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-05 09:08:02,2
13272419,Update content on promotion page,"Links  to promotion page:

 [https://dlab.apache.org/]

[https://dlab.incubator.apache.org/]",Front-end,['DataLab Main'],DATALAB,Task,Major,2019-12-05 08:46:32,10
13272268,[AWS][GCP][SSO for notebook]: Investigate unstable working on AWS and non-working on Azure,"On *AWS* about 70-80 % does not work: links are not opened successful; even if link is opened successful any file could be uploaded.

None of my attempts were not successful to go through the notebook links on *Azure*.
 Maybe it is caused by https://issues.apache.org/jira/browse/DLAB-584",AWS AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-04 15:29:00,3
13272212,[Azure]: Alter billing values according to updates on billing page,"# -'Bucket' should be in 'Resource type' column for all buckets-
 # -Project_name should be in 'Project' column if it is project_bucket.-
 # -'Shared resource' should be in 'Project' column if it is ssn_bucket.-
 # -Images are not in billing report page-
 # -'Image' should be used in  'Resource type' column for all images-
 #  User_name should be used in 'User'column if it is custom image
 # -Project_name should be in 'Project' column if it is custom image.-
 # -'Shared resource' should be in 'Project' column if it is shared image.-
 # Instead of <SSN_INSTANCE_SIZE> should be its value
 # -There are three storages (sbn-ssn-storage/sbn-pr1-local-storage/sbn-local-shared-storage), but in DLab billing we have only one (sbn-pr1-local-storage)-
 # If project name contains upper case in billing report page appears two different projects (see attachment)
 # -Filter does not work by 'Shared resource' in 'Project' column-
 # -Filter does not work by 'Resource Type' in 'Resource Type' column-
 # -Not all values are rounded-",2.3_old AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-04 12:51:46,6
13272211,[GCP]: Alter billing values according to updates on billing page,"# -'Bucket' should be in 'Resource type' column for all buckets-
 # -Project_name should be in 'Project' column if it is project_bucket.-
 # -'Shared resource' should be in 'Project' column if it is ssn_bucket.-
 # -Images are not in billing report page-
 # -'Image' should be used in  'Resource type' column for all images-
 #  -User_name should be used in 'User'column if it is custom image-
 # -Project_name should be in 'Project' column if it is custom image.-
 # 'Shared resource' should be in 'Project' column if it is shared image.
 # -If project name contains upper case in billing report page appears two different projects (see attachment)- - This task will be fixed in https://issues.apache.org/jira/browse/DLAB-1535
 # -Filter does not work by 'Shared resource' in 'Project' column-
 # -Spark cluster volume should NOT be marked as shared resource because it is project resource and should contain 'Volume'  in 'Resource type' column-
 # Size is absent for Dataproc in 'Instance size' column
 #  If filter by 'n1-standard-2' empty values should not be shown
 # Everywhere should be 'USD' not '$'",2.3_old Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-12-04 12:51:01,6
13272194,Investigate if we need 'Resource tag ID' in 'Billing' page,If the 'Resource tag ID' is not needed please comment it,AWS AZURE Back-end Debian GCP RedHat,[],DATALAB,Task,Minor,2019-12-04 11:41:38,6
13272183,Often Superset creation fails ,"*Preconditions:*
1. Project is created on GCP
*Steps to reproduce:*
1. create Superset

*Actual result:*
1. Superset creation fails

Expected result:
1. Superset creation is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-04 10:27:40,3
13272162,[Project page] :Adjust confirmation questions for edge node,"# If terminate one edge node the following question should be on confirmation dialog:
 'Edge node in endpoint <endpoint_name> will be terminated for <project_name> 
 Do you want to proceed?
 # If stop one edge node the following question should be on confirmation dialog:
 'Edge node in endpoint <endpoint_name> will be stopped for <project_name> 
 Do you want to proceed?
 # If terminate all edge nodes in one project simultaneously rename :
'Endpoint' -> 'Edge node in endpoint'

 

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-04 09:12:49,10
13272145,[Front-end]: Odahu (Legion) management in DataLab ,"Who manages Legion provisioning:
 * Admin or DataLab user?

1. Legion cluster will be deployed per project-endpoint
 2. Under Administration page:
   2.1. Add a page: Legion deployment
   2.2. Page should contain grid with following information:
 - Project Name
 - Endpoint URL
 - Legion cluster Name
 - Legion cluster status
 - Number of Legion cluster nodes
 - Actions column:
 ## Start/Stop?
 ## Terminate?
 ## Scale-down
 ## Scale-up

   2.3. Create Legion cluster popup:
 - Select Project
 - Select Endpoint
 - Select existing k8s cluster (checkbox, once checked input field shows up where you can fill in ""Legion k8s cluster URL"")
 - Select number of shapes
 - Select instance shape
 - Select certificate? +(check with Vitalii Solodinov)+
 ## DataLab passes this as input parameter for Legion provisioning script
 - VPC, Subnet will be auto-propagated from Project (visible = false)
 - NAT gateway
 ## Need Legion team to use Edge IP instead (proxy)
 - Buckets (visible = false)
 ## Bucket for Feedback (use Project bucket)
 ### Legion should access bucket name as parameter
 ## Bucket for State of Terraform (use Project bucket)
 ### Legion should access bucket name as parameter
 - Container registry (pass a parameter?)

 # Security params (keycloak, oauth and ssh key)
 # Repos (docker repo and creds, helm repos)

3. Under List of Resource page
 # Show additional column with Legion icon near every notebook
 # When use clicks on it -> popup shows up containing following information:
 ## URL for feedback storage
 ## URL for Swagger API registry
 ## URL for Grafana
 ## Etc
 # Extend current Actions menu for JupyterLab only with action ""Attach Legion""",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-04 08:08:25,5
13272143,[Back-end]: Odahu (Legion) management in DataLab,"Who manages Legion provisioning:
 * Admin or DataLab user?

1. Legion cluster will be deployed per project-endpoint
 2. Under Administration page:
   2.1. Add a page: Legion deployment
   2.2. Page should contain grid with following information:
 - Project Name
 - Endpoint URL
 - Legion cluster Name
 - Legion cluster status
 - Number of Legion cluster nodes
 - Actions column:
 ## Start/Stop?
 ## Terminate?
 ## Scale-down
 ## Scale-up

   2.3. Create Legion cluster popup:
 - Select Project
 - Select Endpoint
 - -Select existing k8s cluster (checkbox, once checked input field shows up where you can fill in ""Legion k8s cluster URL"")-
 - -Select number of shapes-
 - -Select instance shape-
 - -Select certificate? +(check with Vitalii Solodinov)+-
 ## DataLab passes this as input parameter for Legion provisioning script
 - VPC, Subnet will be auto-propagated from Project (visible = false)
 - NAT gateway
 ## Need Legion team to use Edge IP instead (proxy)
 - Buckets (visible = false)
 ## Bucket for Feedback (use Project bucket)
 ### Legion should access bucket name as parameter
 ## Bucket for State of Terraform (use Project bucket)
 ### Legion should access bucket name as parameter
 - Container registry (pass a parameter?)

 # Security params (keycloak, oauth and ssh key)
 # Repos (docker repo and creds, helm repos)

3. Under List of Resource page
 # Show additional column with Legion icon near every notebook
 # When use clicks on it -> popup shows up containing following information:
 ## URL for feedback storage
 ## URL for Swagger API registry
 ## URL for Grafana
 ## Etc
 # Extend current Actions menu for JupyterLab only with action ""Attach Legion""",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-04 08:07:22,6
13271982,Update RStudio notebook version,"Now we use v.1.1.463

Link to release:

[https://support.rstudio.com/hc/en-us/articles/200716783-RStudio-Release-History] ",AWS AZURE Debian DevOps GCP RedHat,[],DATALAB,Task,Major,2019-12-03 15:43:13,3
13271967,Update Jupyter notebook version,"Now we use v.5.7.4

Link to releases [https://github.com/jupyter/notebook/releases] ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-03 14:41:51,3
13271965,Update Dataproc version,"Now we use v.1.2, v.1.3

Link to release versions [https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.4]",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-03 14:37:27,3
13271963,Update Spark version,"Now we use v.2.3.2

Link to release notes [https://spark.apache.org/news/spark-2-3-1-released.html]",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-03 14:32:42,3
13271940,Add Superset and JupyterLab templates in dev mode,"Now JupyterLab is implemented for AWS and GCP.

Superset only for GCP",AWS GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-03 12:46:45,6
13271938,Update EMR version,"Now we use v.5.19.0

Link to EMR release notes: [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-components.html]",AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-03 12:41:51,3
13271686,[GCP][DevOps]: Odahu (Legion) management in DataLab ,"Who manages Legion provisioning:
 * Admin or DataLab user?

1. Legion cluster will be deployed per project-endpoint
 2. Under Administration page:
   2.1. Add a page: Legion deployment
   2.2. Page should contain grid with following information:
 - Project Name
 - Endpoint URL
 - Legion cluster Name
 - Legion cluster status
 - Number of Legion cluster nodes
 - Actions column:
 ## Start/Stop?
 ## Terminate?
 ## Scale-down
 ## Scale-up

   2.3. Create Legion cluster popup:
 - Select Project
 - Select Endpoint
 - Select existing k8s cluster (checkbox, once checked input field shows up where you can fill in ""Legion k8s cluster URL"")
 - Select number of shapes
 - Select instance shape
 - Select certificate? +(check with Vitalii Solodinov)+
 ## DataLab passes this as input parameter for Legion provisioning script
 - VPC, Subnet will be auto-propagated from Project (visible = false)
 - NAT gateway
 ## Need Legion team to use Edge IP instead (proxy)
 - Buckets (visible = false)
 ## Bucket for Feedback (use Project bucket)
 ### Legion should access bucket name as parameter
 ## Bucket for State of Terraform (use Project bucket)
 ### Legion should access bucket name as parameter
 - Container registry (pass a parameter?)

 # Security params (keycloak, oauth and ssh key)
 # Repos (docker repo and creds, helm repos)

3. Under List of Resource page
 # Show additional column with Legion icon near every notebook
 # When use clicks on it -> popup shows up containing following information:
 ## URL for feedback storage
 ## URL for Swagger API registry
 ## URL for Grafana
 ## Etc
 # Extend current Actions menu for JupyterLab only with action ""Attach Legion""",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-02 11:57:54,8
13271674,[Back-end]: Support localization ,"As user I want to have formats for dates, currency which are used in my time zone.
----
Using proper local formats for dates, currency for all DLab.

Github issue: [https://github.com/apache/incubator-dlab/issues/732]

 ",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-12-02 11:33:01,6
13271657,Distinguish endpoint and edge node  on DLab UI,"# On 'Project' page should be:
 1.1 'Edge node status' instead of 'Endpoint status' labels
 1.2. Change the following edge edge node status:
 connecting - creating
 connected - running 
 disconnecting - stopping 
 disconnected - stopped
 terminated - terminated
 terminating - terminating
 # Change confirmation dialog on 'Connect endpoint' popup:
 *from:* '<endpoint_name> will be decommissioned.
 Do you want to proceed?'
 *to:* Endpoint<endpoint_name> will be disconnected.
 Do you want to proceed?",2.3_old AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-12-02 09:25:18,10
13271647,Get rid of some actions for JupyterLab template,"*Prevent the following actions:*
1. add computational resources
2. library management
3. Spark configuration/reconfiguration
4. Custom image creation

*Leave the following actions:*
1. stop/start/terminate
2. scheduler
3. web-terminal",2.3_old Debian GCP,['DataLab Main'],DATALAB,Task,Major,2019-12-02 09:03:42,10
13271646,Superset starting fails (previous it was stopped),"*Preconditions:*

1. Superset is stopped

*Steps to reproduce:*

1. Start Superset from DLab UI

*Actual result:*

1. Superset starting fails

*Expected result:*

1. Superset starting is successful",Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-12-02 09:02:33,3
13271447,Enhancement for 'List of resource'/'Project' pages,"# Get rid of 'To start working, please, create new environment' if all resources are terminated in 'Show active' mode. It is visible during switching between the modes
 # If terminate project on confirmation dialog edge_node values should have left alignment
 # Icons for computational resources should be aligned
 # Get rid of data labels delay on:

 - Environment Management page-> Manage environment popup -> stop/terminate a project
 - roles page -> delete roles
 - resources_list page -> stop/terminate computational resource
 - Environment Management page -> stop/terminate computational resource",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2019-11-29 16:06:15,5
13271432,[AWS][GCP][Azure]: It is not filtered by service/product/resource type and by shared resource in Project column,"*Preconditions:*
 # Billing is availabe in DLab

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Filter by values of service(AWS)/product(GCP)/resource type (Azure)
 # click icon aply filter

*Actual result:*

1. Billing report is not filtered by  values of service/product

*Expected result:*

1. Billing report is not filtered by  values of service/product
----
 (!)  Also filter does not work by 'Shared resource' in Project column for all clouds.",2.3_old AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-29 15:03:22,6
13271406,Create playbook for JupyterLab,Working playbook should be created in order to verify Jupyterlab's kernel.,2.3_old AWS Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-29 13:13:02,2
13271398,[Manage environment popup]: Error handling improvement for project actions,"Convey error handling only if resource is in progress:

1.  if notebook/computational resources are in:

1.1. creating

1.2. configuring

1.3. starting

1.4. reconfiguring

1.5 stopping???

1.7 terminating???

 

2.  if edge_node status is in:

2.1. connecting

2.2. disconnecting

 

Action for terminate?

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-29 12:25:55,6
13271338,Ungit does not work with two-factor authentication,"If two-factor authentication is enabled on git server it is impossible to do any action via ungit (clone/push/pull). Password is required for each action, even after putting a correct password the password is required again.
 # Investigate if ungit can work with two-factor authentication
 # Investigate if it is possible to make ungit working with two-factor authentication in DLab

 ",2.3_old AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-29 08:22:17,11
13271263,Minor links altering,"# Get rid of a delay for notebook link
 # Align Notebook link by left side
 # Investigate if it is possible to make link copy-able and leave a hint?",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-28 16:27:38,5
13271255,[back-end]: Get rid of necessary log in RStudio/Rstudio with TensorFlow,"In the scope of SSO implementation (https://issues.apache.org/jira/browse/DLAB-1105).
In order to rid of necessary log in RStudio	some changes should be made:
etc/systemd/system/rstudio-server.service
[Service]
Environment=USER=dlab-user
ExecStart=/bin/bash -c ""export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/cudnn/lib64:/usr/local/cuda/lib64; /usr/lib/rstudio-server/bin/rserver --auth-none 1""",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-28 15:49:39,6
13271252,[Billing page]: Data engine is marked as shared resource,"*Preconditions:*

1. Billing is available for spark clustaer

*Steps to reproduce:*

1. Go to 'Billing report' page

*Actual result:*

Data Engine is marked as shared resource

*Expected result:*

Data Engine is marked as user's resource

 ",2.3_old AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-11-28 15:32:28,6
13271190,Jupyter/DeepLearnin/TensorFlow creation fails,"*Preconditions:*

*1. Project is created*

Steps to reproduce:

1. Create Jupyter/DeepLearnin/TensorFlow

*Actual result:*

Jupyter/DeepLearnin/TensorFlow creation fails

*Expected result:*

Jupyter/DeepLearnin/TensorFlow creation is successful",AWS Azure Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-11-28 11:20:28,3
13271061,"JupyterLab creation fails in the  other project, not in the first project where it was created","*Preconditions:*
 # *Shared image is enable*
 # *Jupyterlab is created in Project1*

*Steps to reproduce:*

1. Create JupyterLab2 in project2

*Actual result:*

1. JupyterLab2 creation fails on stage Starting Jupyter container

*Expected result:*
 # JupyterLab2 creation is successful

The bug also is related to Superset",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Blocker,2019-11-27 17:33:36,8
13271056,[Project page]: Convey project_endpoint_name on confirmation dialog,"Alter confirmation dialog for project endpoint on Project page:
 # ""Project endpoint <endpoint_name> will be stopped. Do you want to proceed?""
 # ""Project endpoint <endpoint_name> will be  will be decommissioned. Do you want to proceed?""",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-27 17:18:40,5
13271055,[Billing][GCP]: SSN type is not pulled from GCP,"*Preconditions:*

1. Billing is available for DLab

*Steps to reproduce:*

1. Go to 'Billing page'

*Actual result:*

1. SSN type is t2.medium

Expected result:

1. SSN type is n1-standard-2",2.3_old Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Minor,2019-11-27 17:08:01,6
13270977,If custom image contains dependency Notebook creation fails,"*Preconditions:*
 # Library is installed on Notebook1 via DLab UI
 # Create custom image from Notebook1

*Steps to reproduce:*
 # Go to 'Resource list' page
 # Create Notebook2 from custom image 

*Actual result:*
 # Notebook is not created
 # Appears error message

*Expected result:*
 # Notebook is created
 # There is not error message",AWS AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-27 12:38:35,6
13270970,[Endpoint disconnection][Front-end]: Convey all resources which will be terminated on confirmation dialog,"Disconnection endpoint does not terminate existing resource if user does not check off the appropriate checkbox.

Confirmation dialog:
 '*<Endpoint_name> will be disconnected.* '
 Convey all related notebooks with their projects. 
 checkbox '*Do not* *terminate all related resources*'

'*All connected computational resources will be terminated as well*'

 *Do you want to proceed*?'

 ",2.3_old AWS AZURE Debian Front-end GCP RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-11-27 12:12:35,5
13270969,[Endpoint disconnection][Back-end]: Convey all resources which will be terminated on confirmation dialog,"Disconnection endpoint does not terminate existing resource if user does not check off the appropriate checkbox.

If user checks off the checkbox all related resources will be terminated.

If related instance is in status -ing (except for running) convey error message that it is impossible to terminate instance. Error message should appear after a user hint 'Yes' button.

 ",2.3_old AWS AZURE Back-end Debian GCP RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-11-27 12:11:23,6
13270963,[Environment management]: Consistency should be in alignment,"If it is only one cluster on notebook action items for Notebook and Cluster are not on the same level: Actions for cluster is higher located than 'gear' for notebook, but should on the same level.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Task,Trivial,2019-11-27 11:31:17,5
13270938,Alter DLab login page scaling,"DLab login page has some wrong scaling, especially it is catchy in macOS, 13.3-inch (2560 x 1600), intel Iris Plus Graphics 640 1536 MB graphics",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2019-11-27 10:06:46,5
13270742,"[Manage environment popup]: Stop/terminate project uses api/project, but should use api/project/managing","*Preconditions:*
 # More than one project are created
 # All projects are in active statuses

*Steps to reproduce:*
 # Go to manage environment popup
 # Choose stop Project1

*Actual result:*

1. api/project is used

2. All icons  are disabled for another projects on Manage environment popup

*Expected result:*

1. api/project/managing

2. All icons  are enable for another projects according to their statuses

 

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-26 15:16:37,10
13270737,[Manage environment popup]: Project_edge status/notebook status of reconfiguring should be taken into account,"# If notebook is in 'reconfiguring' status do not allow to stop project
 # If project_edge status is in 'disconnecting'/'connecting' statuses do not allow to stop project
 # If project_edge status is in 'connected' status and none of resource is in active status - allow to stop project",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-11-26 14:59:11,6
13270729,[Manage Environment popup]: Not active icons (stop/terminate) should not change color if hover mouse,Not active icons (stop/terminate) should have grey color if hover mouse on Manage Environment popup,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2019-11-26 14:23:55,10
13270505,[Manage environment popup]: Edge_node status should not change from stopped->stopping->stopped if it is already in stopped status,"*Preconditions:*
 # Notebook is running
 # Edge node is stopped

*Steps to reproduce:*
 # Go to 'Manage environment' popup
 # Click 'Stop'project

*Actual result:*

1. Edge_node changes status stopped->stopping->stopped on environment management page

*Expected result:*

Edge_node status is stopped on environment management page

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2019-11-25 17:37:44,6
13270502,[Superset termination]: Docker executes with '1' nevertheless Superset is terminated on DLab UI and GCP console,"*Preconditions:*
1. Superset is created

*Steps to reproduce:*
1. Terminate Superset from DLab UI

*Actual result:*
1. Superset is terminated on DLab UI
2. Superset is terminated on GCP console
3. Docker runs with '1'
*cannot access '/response/notebook_PR1_8a625fed-7dd4-4f95-9c85-be2dac9ebfaf.json': No such file or directory
*
*Expected result:*
1. Superset is terminated on DLab UI
2. Superset is terminated on GCP console
3. Docker runs with '0'",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Minor,2019-11-25 17:26:23,3
13270495,Change hint if none of resources is not active,"If project is terminated or stopped the following hint should be conveyed :

'Unable to terminate project because all resources are already terminated'

'Unable to stop project because all resources are already stopped''",2.3_old AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-11-25 17:06:07,5
13270486,[List of Resources]: In drop down list computational resources value should start from upper case,"Preconditions:
1. Computational resource is created

*Steps to reproduce:*
1. Go to 'List of Resources' page
1. Click ' Computational resource' drop down list

*Actual result:*
1. Computational resources value start from upper case

*Expected result:*
1. Computational resources value start from lower case",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2019-11-25 16:49:28,5
13270421,Issues on promotion page,"*Preconditions:*
1. User is located on https://apache.github.io/incubator-dlab/#

*Steps to reproduce:*
1.  Click 'Get Started' button
2. Click 'Play' button
3. Click '+' icons for 'Data access problem/'Problems with tools/'Increased IT handholding'/'Lack of security'
4. Click any icon features

*Actual result:*
1. 'Get Started' button does not work - nothing is happened
2. 'Play' button does not work - nothing is happened
3.  Icons for 'Data access problem/'Problems with tools/'Increased IT handholding'/'Lack of security' do not work nothing - is happened
4. Any icon features  do not work nothing - is happened

*Expected result:*
1. 'Get Started' button - works
2. 'Play' button - works - video is running
3.  Icons for 'Data access problem/'Problems with tools/'Increased IT handholding'/'Lack of security' work
4. Any icon features  work - it is switched by features

*************************************************************
Also add 'GCP' in 'Flexible deployment architecture'
",Front-end pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-11-25 11:19:34,5
13270399,[DevOps]: Get rid of necessary log in RStudio,"In the scope of SSO implementation (https://issues.apache.org/jira/browse/DLAB-1105).
In order to rid of necessary log in RStudio	some changes should be made:
etc/systemd/system/rstudio-server.service
[Service]
Environment=USER=dlab-user
ExecStart=/bin/bash -c ""export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/cudnn/lib64:/usr/local/cuda/lib64; /usr/lib/rstudio-server/bin/rserver --auth-none 1""",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-25 08:55:04,8
13270398,[Front-end]: Get rid of necessary log in RStudio,In the scope of SSO implementation (https://issues.apache.org/jira/browse/DLAB-1105).,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-25 08:54:23,10
13270116,[Superset]: Web terminal connection is unsuccessful,"*Preconditions:*
1. Superset is created

*Steps to reproduce:*
1. Go to 'resources list' page
2. Click action menu for superset notebook
3. Choose  web terminal

*Actual result:*
1. Web terminal connection is unsuccessful

*Expected result:*
Web terminal connection is successful",2.3_old Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-11-22 15:30:31,3
13270106,Get rid of some actions for Superstet template,"*Prevent the following actions:*
1. add computational resources
2. library management
3. Spark configuration/reconfiguration

*Leave the following actions:*
1. stop/start/terminate
2. scheduler
3. web-terminal",Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-11-22 15:13:32,10
13270083,[Azure][AWS]: Image is not used for Data Engine creation on Apache Zeppelin/RStudio/TensorFlow/DeepLearning,"*Preconditions:*
1. Apache Zeppelin/RStudio/TensorFlow/DeepLearning are created

Steps to reproduce:
1. Create  Data Engine on Zeppelin/RStudio/TensorFlow/DeepLearning

Actual result:
1. Notebook image is not used for Data Engine creation

Expected result:
1. Notebook image is used for Data Engine creation",AWS Azure Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-22 12:24:20,8
13270066,[List of resource]: Grid values do not match with grid header,"*Preconditions:*
1. Notebooks are created

Steps to reproduce:
1. Go to 'List of resource' page

*Actual result:*
1. Grid values do not match with grid header

*Expected result:*
1. Grid values matches with grid header",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-22 10:13:14,10
13269873,A-z sorting for notebook templates,Notebook template should be sorted by a-z on 'Create analytical tool' popup,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2019-11-21 15:09:00,5
13269865,[Manage environment popup]: Extend validation for project stopping/termination,"If al resources are stopped or terminated (including edge) the icons are available for stopping/termination, but should be disabled. 

If  icons are disabled add a hint 'There is not any active instance'. 
PS you are free to change the contest hint.
GET /api/project/managing",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-21 14:41:55,10
13269837,JupyterLab creation fails,"*Preconditions:*
1. Project is created

S*teps to reproduce:*
1. Create Jupyterlab

*Actual result:*
1. JupyterLab creation fails: 'Failed to configure Jupyterlab'

*Expected result:*
1. JupyterLab is created successful
",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-11-21 13:02:56,8
13269789,Login DLab fails only from the first attempt after SSN creation or starting,"*Steps to reproduce:*
1. Deploy SSN via Jenkins
2.  Refer to the link for login
3. Put valid username and password

*Actual result:*
1. User is not logged in DLab
2. Appears an error message: 'Http failure response for http://35.203.179.142/api/oauth/refresh/eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI1ODNkMzJlZi1iNjYzLTQ2NWEtYjgyOS1lYzAzYTZhOTBhYTIifQ.eyJqdGkiOiIzNmMzNTU1Zi1kODk4LTQxNWUtYmRkMy05ZjQxOGM1OWM3NDMiLCJleHAiOjE1NzQyNjgyOTMsIm5iZiI6MCwiaWF0IjoxNTc0MjY2NDkzLCJpc3MiOiJodHRwOi8vNTIuMTEuNDUuMTE6ODA4MC9hdXRoL3JlYWxtcy9kbGFiIiwiYXVkIjoiaHR0cDovLzUyLjExLjQ1LjExOjgwODAvYXV0aC9yZWFsbXMvZGxhYiIsInN1YiI6IjViMGFhMDJmLWE1N2UtNDBjNC05ODg0LTQ5ZmJlOThlYjM1OCIsInR5cCI6IlJlZnJlc2giLCJhenAiOiJ2aXQtMjAxMW4tdWkiLCJhdXRoX3RpbWUiOjAsInNlc3Npb25fc3RhdGUiOiI2MjljYzc2OC1jOWQ5LTQ5ZTUtYTljMi0zYzM5MTEwYzA1NmUiLCJyZXNvdXJjZV9hY2Nlc3MiOnsiYWNjb3VudCI6eyJyb2xlcyI6WyJtYW5hZ2UtYWNjb3VudCIsIm1hbmFnZS1hY2NvdW50LWxpbmtzIiwidmlldy1wcm9maWxlIl19fSwic2NvcGUiOiJlbWFpbCBwcm9maWxlIn0.CR0DVXoEMRS4uvnrkTLztsxEdV6Y5rqN_RdcjRabztY?noCache=1574322906802: 400 Bad Request'

*Expected result:*
1. User is logged in DLab
2. There is not an error message: ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-21 10:05:40,10
13269618,[AWS]: Pointed zone is not used during DLab deployment,"*Steps to reproduce:*
1. Deploy SSN via Jenkins indicated zone a ( region; us-west-2)

*Actual result:*
Dlab is deployed in zone c ( region; us-west-2)

*Expected result:*
Dlab is deployed in zone a ( region; us-west-2)",AWS Debian,['DataLab Main'],DATALAB,Bug,Minor,2019-11-20 15:37:17,8
13269555,[AWS]: Project creation fails on stage of 'get_ami_id',"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create a project

*Actual result:*
 # Project creation fails
 NoCredentialsError: Unable to locate credentials

*Expected result:*
 # Project creation is successful

 ",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Blocker,2019-11-20 10:07:47,8
13269544,[List of Resource]: Extend instance statuses in 'Show active' mode,"Show active operation should convey instances which are in the following statuses:

-running

-stopping

-stopped

-creating

-configuring

-starting

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-20 09:25:15,10
13269339,Custom image status is not updated if use the same name in different projects for one user,"*Preconditions:*
 # User1 is assigned to Project1 and to Project2
 # RStudio1 is created in Project1
 # RStudio2 is created in Project2
 # Custom image with name <ami1> is created from RStudio1

*Steps to reproduce:*

1. Create custom image  with name <ami1> from RStudio2

*Actual result:*

1.  Custom image 'ami1' is not available  on DLab UI for RStudio2

*Expected result:*

Custom image 'ami1' is available  on DLab UI for RStudio2",AWS AZURE Back-end DevOps Known_issues(release2.2) pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-11-19 15:23:33,6
13269298,[Azure]: SSN termination fails on parameter  ssn_conf['region']),"*Preconditions:*

1. Project (Edge) is created

*Steps to reproduce:*
 # Terminate DLab via Jenkins job

*Actual result:*

DLab termination fails

*Expected result:*

DLab is terminated successful",AZURE Debian Known_issues(release2.2) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-19 11:36:47,9
13269067,After pressing 'Esc' DLab menu disappears,"*Preconditions:*

1. Enviroment is created

*Steps to reproduce:*
 # Go to any page
 # Press 'Esc' from key board
 # Click menu bar

*Actual result:*

1. Main menu disappears

*Expected result:*

1. Main menu does not disappear

 

 ",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-18 12:50:14,5
13269051,[Role]: Add 'Create Notebook JupyterLab',Add 'Create Notebook JupyterLab' in 'Roles' drop down list,AWS Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-11-18 11:22:52,6
13269047,[Azure]: Detailed billing is not available,"*Preconditions:*

1. Billing is available for notebook on 'Billing report' page

*Steps to reproduce:*

1. Go to 'List of resource' page

*Actual result:*

1. Detailed billing is absent

*Expected result:*

1. Detailed billing is present",AZURE Debian Known_issues(release2.2) RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-18 11:05:20,6
13269041,[Azure]: Total sum/resource type is absent in exporting file,"*Preconditions:*
 # Billing is available

*Steps to reproduce:*
 # Go to 'Billing report page'
 # Click 'Export' button
 # Open exporting file

*Actual result:*

1. Total sum is absent in exporting file

*Expected result:*

1. Total sum is present in exporting file
-----
Resource type is absent as well
 ",2.3_old AZURE Debian RedHat,['DataLab Main'],DATALAB,Bug,Trivial,2019-11-18 10:41:45,6
13269040,[Azure]: Billing data is not available after calendar filter using,"*Preconditions:*

1. Billing is available

*Steps to reproduce:*

1.  Go to 'Billing report'

2. Click calendar

3. Click 'Last Month'

4. Click 'This year'

*Actual result:*

1. Billing data is not available for this year

*Expected result:*

1. Billing data is not available for this year",2.3_old AZURE Back-end Debian Known_issues(release2.2) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-18 10:36:48,6
13269036,[Azure]: Custom image creation fails and deletes existed Notebook,"*Preconditions:*

1. Notebook is in running status

*Steps to reproduce:*

1.  Create custom image from running notebook

*Actual result:*
 # Custom image creation fails
 # Notebook is deleted from Azure console

*Expected result:*
 # Custom image is created successfully
 # Notebook is not deleted from Azure console",AZURE Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-18 10:20:23,9
13268843,Can not get token if stop the whole project or stop/terminate notebook of another user,"*Preconditions:*
 # Resources  are created by user2 in Project1
 # User 2 is logged out
 # User1 has administrative role

*Steps to reproduce:*
 # Login by user1
 # Go to 'environment management' page
 # Click stop or terminate notebook of user2

*Actual result:*
 # It is not allowed to stop or terminate notebook
 # Error message appears 'Can not get token '

*Expected result:*
 # It is allowed to stop or terminate notebook
 # Error message is absent

The same error appears if stop project which contains notebook of another user",AWS AZURE Back-end Debian GCP Known_issues(release2.2) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-16 13:09:02,6
13268842,User1 is auto logged out DLab on cloud1 after manually logout DataLab on cloud2 only in the same browser,"*Preconditions:*
 # User1 is logged in DataLab1 on cloud1 on Chrome browser
 # User1 is logged in DataLab2 on cloud2 on Chrome browser

*Steps to reproduce:*

1. User1 logs out  DataLab1 on cloud1 on Chrome browser

*Actual result:*
 # User1 is auto logged out DataLab2 on cloud2 on Chrome browser

*Expected result:*
 # User1 is still logged in DataLab2 on cloud2 on Chrome browser",2.3_old AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-16 12:34:59,5
13268788,[Web terminal]: Investigate how to set copy/paste tools,"# In web terminal copy is left mouse click, paste is right mouse click. But it works very rarely.
 # If I copy a command not from web terminal I will not able to paste it in web terminal.",2.3_old AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-16 11:30:06,1
13268651,[Data Engine]:  Abbreviation for slave and master should not be in billing report,"*Preconditions:*
 # Data Engine is created on GPU instance
 # Billing is available for Data Engine

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Look at data for Data Engine

*Actual result:*

1.  Project value is absent

2. Resource type is absent

3. 's1', 'm' are present in 'environment name'

*Expected result:*

1.  Project value is present

2. Resource type is present

3. 's1', 'm' are absent in 'environment name'

 ",2.3_old Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Minor,2019-11-15 19:05:18,6
13268626,[Scheduler by time]: Starting is not triggered only for spark cluster,"*Preconditions:*

1. Spark cluster is stopped on running notebook

*Steps to reproduce:*

1. Set scheduler for starting spark cluster by time

*Actual result:*

1. Spark cluster is started

*Expected result:*

1. Spark cluster is not started",AWS AZURE Back-end Debian GCP Known_issues(release2.2) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-15 17:03:04,6
13268615,[List of resource][Computational resource]: 'Show active' does not check for existing name in not active resource,"*Preconditions:*

1. Cluster1 is terminated on running Notebook1

*Steps to reproduce:*
 # Go to 'Resource list'
 # Click 'Show active' button
 # Enter cluster with name 'Cluster1' on  Notebook1
 # Enter valid data in the other fields
 # Click 'save' button

*Actual result:*

1. Error message appears (see attachment)

*Expected result:*
 # After entering cluster name appears error message 'This cluster name already exists.'

Also it allows to create cluster with the same name in 'Show active' mode. Example: 'CLUSter1' and 'cluster1' (see attachment 'the same name')

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-15 16:39:42,10
13268602,[Scheduler]: 400 error causes auto log out,"*Preconditions:*
 # Notebook/cluster are created
 # Scheduler pop up is open

*Steps to reproduce:*
 # Turn on scheduler by time
 # Click 'Save' button

*Actual result:*

1. User is auto logged out

*Expected result:*

1. User is not auto logged out",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-15 15:54:02,10
13268578,Environment stopping should take into consideration instance status,"# Do not allow to stop environment if project endpoint is not in connected or disconnected statuses.
 # Do not allow to stop environment if notebook is in creating, starting, creating image, reconfiguring statuses
 # Do not allow to stop environment  if computational resource is in creating, starting, configuring, reconfiguring statuses.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-11-15 13:50:12,6
13268566,Environment management fails if manage computational resource of another user,"*Preconditions:*

1. Computational resource is created by user1

*Steps to reproduce:*
 # User1 is not logged in Dlab
 # Login by user2
 # Go to 'environment management' page
 # Click stop or terminate icon for computational resource of users1

*Actual result:*

1. It is not allowed to terminate computational resource

*Expected result:*

1. Computational resource is terminated",AWS AZURE Back-end Debian GCP Known_issues(release2.2) RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-15 12:48:51,6
13268535,[Scheduler]: Add timezone parameter to scheduler select ,'Select offset' label -> 'Select time zone' label,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-15 10:05:43,10
13268518,Extra scrollbar during login in Chrome browser,"*Preconditions:*

1.  Environment is created

*Steps to reproduce:*

1. Go to link from Jenkins job to login

*Actual result:*

There is vertical scrollbar on the right side

*Expected result:*

There is not vertical scrollbar on the right side",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2019-11-15 08:34:57,5
13268061,[Billing Report]: Project column is not exported,"*Preconditions:*

1. Billing is available

*Steps to reproduce:*
 # Go to 'Billing report' page
 # Click 'Export' button
 # Open exported file

*Actual result:*

1. Project column is absent in exported file

*Expected result:*

1. Project column is present in exported file

 ",2.3_old AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-13 15:34:27,6
13268016,[GCP]: SSN termination fails due to 'pre_defined_vpc' is not defined,"*Preconditions:*

1. Environment is created on GCP

*Steps to reproduce:*

1. Run Jenkins job for SSN termination

*Actual result:*

1. SSN is not terminated

*Expected result:*

1. SSN is terminated",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-11-13 12:32:12,12
13268013,[AWS]: Notebook image/snapshot are not deleted during SSN termination,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*

1. Run Jenkins job for SSN termination

*Actual result:*
 # Notebook image is still on AWS console
 #  Notebook snapshot is still on AWS console

*Expected result:*
 # Notebook image is terminated on AWS console
 #  Notebook snapshot is terminated on AWS console",AWS Debian DevOps Known_issues(release2.2) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-13 12:26:37,8
13268000,[Group adding]: Role drop down list extends for all monitor width,"*Preconditions:*

1. Environment is created

*Step to reproduce:*
 # Go to 'Roles' page
 # Click 'add group' button
 # Enter valid group name in text box
 # Click 'Next >' button
 # Click 'Next >' button again
 # Choose any available role in drop down list
 # Click 'Role' drop down list

*Actual result:*

1. Role drop down list extends for all monitor width

*Expected result:*

1. role drop down list is not extended by width

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-13 11:44:41,10
13267992,[Project termination]: Computational resource status is not updated,"*Preconditions:*

1. Computational resource is created

*Steps to reproduce:*
 # Go to 'environment management' page 
 # Click 'Manage environment' button
 # Click terminate environment where computational resource is

*Actual result:*
 # All environment resources are terminated
 # Only status of computational resource is 'terminating' on DLab UI

*Expected result:*
 # All environment resources are terminated
 # All environment resources have terminated statuses

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-13 11:14:59,6
13267967,[Back-end][List of Resource]: Switching between project is not available for user (not admin),"*Preconditions:*

1. User (not admin) is logged in DLab

*Steps to reproduce:*

1. Go to 'List of Resource' page

*Actual result:*

1. 'Select active project' drop down list is absent

*Expected result:*
 # 'Select active project' drop down list is present

************************************************************************

2.  In 'select active project' should be available for user only assigned his project. However not it is available all project.

FE should use GET /api/project/me",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-13 09:39:12,6
13267946,Failed to manage git credentials,"*Preconditions:*
 # Notebook Is created
 # 
h4. Git Credentials is added

*Steps to reproduce:*
 # Go to ungit notebook link
 # Clone repository

*Actual result:*
 # Clone/push/ commands are forbidden

*Expected result:*
 # Clone/push/ commands are allowed

 

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-13 08:29:42,6
13267792,Scheduler for stopping is not trigger in case of  total quota depleting,"*Preconditions:*

1. Billing is available in Dlab

*Steps to reproduce:*

1.  Set total quota more than real used total quota

*Actual result:*

1. All resources are still running

*Expected result:*

1. All resources are stopped (DES - terminated)",2.3_old AWS AZURE Back-end Debian GCP Known_issues(release2.2) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-12 18:08:35,6
13267783,It is allowed to start notebook if project quota is depleted,"*Preconditions:*

1. Billing is available for project

*Steps to reproduce:*

1.  Set project quota more than real used project quota

2. Start notebook

*Actual result:*
 # Notebook is starting
 # There is not error message

*Expected result:*
 # Notebook is stopped
 # Error message appears: 'Oops! Operation can not be finished. Resource quote is reached'",AWS AZURE Back-end Debian GCP Known_issues(release2.2) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-12 17:23:40,6
13267731,[Front-end]: Custom image should be unique per project,"Now custom image should be unique per user, but it should be per project.

If custom image name already exists an error message should appear: 'This name already exists in project'",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-11-12 14:40:43,10
13267730,[Back-end]: Custom image should be unique per project,"Now custom image should be unique per user, but it should be per project.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-11-12 14:39:07,6
13267671,[AWS]: Billing report export fails due to 500 error,"*Preconditions:*

1. Billing is available for AWS

*Steps to reproduce:*
 # Go to 'Billing report' page
 #  Click 'Export' button

*Actual result:*

1. Billing export fails

*Expected result:*

1. Billing export is successful

 ",AWS Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-12 11:19:27,6
13267645,[Endpoint list]: Endpoint header values should not change their position,If endpoint url is a little longer the values of endpoint header should not change their position. ,AWS AZURE Debian GCP RedHat,['Cloud endpoint'],DATALAB,Task,Minor,2019-11-12 09:36:50,5
13267642,[AWS]: Limitation for name of computational resource,"1. Name of computational resource should not exceed 10 symbols. If name of computational resource exceeds more than 10 symbols it should be forbidden to create cluster. 

2. Add an error message in case of computational resource name exceeds 10 symbols.

'Cluster name cannot be longer than 10 characters and can only contain letters, numbers, hyphens and '_' but can not end with special characters.'",AWS Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-12 09:29:36,5
13267524,[Billing report]: Alter values in grid,"1. Column values from grid should not be visible partially under its drop down list

2. There is too small space between values of environment name and user",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-11-11 18:02:07,5
13267521,[AWS][Environment management]: It is impossible to stop notebook,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*
 # Go to 'Environment management' page
 # Click action menu for notebook
 # Choose 'Stop' action
 # Click 'Yes' button

*Actual result:*
 # Notebook is not stopped
 # Error message appear: 'Can not get token'

*Expected result:*

1. Notebook is stopped",pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-11 17:46:43,6
13267516,Script for deploying Keycloak,"You should fetch from two options:
 # should it be a script which deploys resource on cloud and configures keycloak
 OR
 # Should it deploy keycloak on already existing instance

Could you jot down in comments which option will be implemented?",2.3_old AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-11 17:12:16,3
13266868,Add hint for instance link,"Please, add hint for link of notebook/computational resource.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Trivial,2019-11-07 17:08:57,5
13266820,Libraries are not installed due to permission absence to the directory,"*Preconditions:*
 # DLab is deployed on K8S
 # Notebook is created

*Steps to reproduce:*
 # Go to 'resources_list' page
 # Click action menu for notebook
 # Choose 'manage libraries'
 # Choose available resource in 'Select resource' drop down list
 # Select apt/Yum/Python/Others groups
 # Choose available libraries
 # Click installed

*Actual result:*
 # Docker runs with '0'
 # Libraries installation failed

*Expected result:*
 # Docker runs with '0'
 # Libraries installation is successful

For java and r package groups I could not check because there were issues not from our side.",2.3_old AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-07 12:20:13,3
13266817,[GCP]: Secondary image is not created for notebook,"*Preconditions:*

1. Environment is created on GCP

*Steps to reproduce:*

1. Create notebook

*Actual result:*
 # Secondary image is not created for notebook

*Expected result:*
 # Secondary image is created for notebook",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-07 11:59:57,8
13266816,[Resource list][FireFox]: Bottom header line is not drawn for all grid in case of sticky columns,"*Preconditions:*
 # Environment is created
 # User is logged in DLab via FireFox browser

*Steps to reproduce:*

1. Go to 'Resource list' 

*Actual result:*

1. Bottom header line is not drawn for all grid

Expected result:

1. Bottom header line is drawn for all grid (from left to right)",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Trivial,2019-11-07 11:50:13,5
13266813,Fix issue with SSL certs expiration,"SSL certificates configured for being renewed in 24 hours. If instance is stopped at this time, SSL certificate will be expired and unable to be renewed.
Also fix path to keystore",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-11-07 11:26:46,9
13266788,Alter error message for starting notebook,"If project (project endpoint) is not active and user tries start notebook, please change error message to the following:

*Oops!*

*Could not exploratory environment <name> start. Connection to <xx.xxx.xxx.xxx:xxxx> failed*",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-11-07 10:11:18,13
13266616,[Resources_List][Billing_Report:] Extra line appears if value is not available or matches are not found,"*Preconditions:*
 1. Env is created
 2. Billing is not available
 3. Any resource is not created in a project

*Steps to reproduce:*
 1. Go to list of resource
 2. Choose project in which any there is not any resource

*Actual result:*
 1. Extra line appears until press 'F5'

*Expected result:*
 1. There is not spare line",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-11-06 15:54:21,5
13266612,'Roles' drop down list is broken only in Microsoft Edge browser,"*Preconditions:*
1. Env is created

*Steps to reproduce:*
1. Go to 'Roles' page

*Actual result:*
1. 'Roles' drop down list is broken

*Expected result:*
1. 'Roles' drop down list is not extended for all page
",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-11-06 15:43:01,5
13266582,[GPC]: Predefined VPC/subnet name are not used during project creation,"*Preconditions:*
1. DLab is deployed with predefined VPC name

*Steps to reproduce:*
1. Create Project

Actual result:
1. Predefined VPC name is not used (instead of it it is used SBN)

*Expected result:*
Predefined VPC name is used

The same case is with subnet name",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-06 13:10:09,8
13266577,Available lib list is not getting due to large json,"*Preconditions:*
1. Notebook is created

*Steps to reproduce:*
1. Go to 'resources_list' page
2. Click action menu for notebook
3. Choose 'Manage libraries'
4. Choose available resource in 'Select resource' drop down list

*Actual result:*
1. Available lib list getting is stuck on UI
2. User is not able to install library
 HTTP 413 Request Entity Too Large

*Expected result:*
1. User is able to install library

",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-06 12:48:53,9
13266569,Provisioning service have no access to response files,"*Preconditions:*
1. Notebook is created 

*Steps to reproduce:*
1. Go to 'resources_list' page
2. Click action menu for notebook
3. Choose 'Manage libraries'
4. Choose available resource in 'Select resource' drop down list

*Actual result:*
1. Available lib list getting is stuck on UI
2. User is not able to install library

*Expected result:*
1. User is able to install library


Could not find file because it finds from dlab-user, but this json is available for root.
",2.3_old AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-11-06 12:10:24,3
13266364,Swagger UI integration in DataLab,"Currently DataLab has endpoints that return API description for 2 kind of APIs:

 - DataLab api (for third party integrations: ui, mobile devices etc)

-  Endpoint API (api for environment provisioning that is used by DataLab itself)

 

There are 2 endpoint that expose configurations for swagger ui in open api format:
 * /assets/endpoint-api.json
 * /assets/dlab-api.json

On DataLab UI it should be a btn that will open API description inside swagger UI.

 

Links: 

[https://github.com/swagger-api/swagger-ui]

 ",pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2019-11-05 15:23:12,1
13266131,[GCP]: Bucket should be tagged,"The following label should be added for project bucket:

 ""\{sbn}-\{project_name} -\{endpoint_name}-bucket""

 ",Debian DevOps GCP,[],DATALAB,Task,Major,2019-11-04 14:55:30,12
13266121,Provisioning is not started due to value absence for sharedImageEnabled,"*Steps to reproduce:*

1. Deploy DLab via Jenkins job

*Actual result:*

Provserv is stuck (status is 'starting')

*Expected result:*

Provserv is  in 'running' status",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-11-04 13:49:44,8
13266108,Add cloud parameter in provisioning,"Add new cloud parameter in cloudProperties for provisioning.

<imageEnabled>: <true> or <false>

Pass the parameter to docker as conf_image_enabled.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-11-04 12:33:56,6
13266093,[Create analytical tool]: Value is not rewrote according to project/endpoint,"*Preconditions:*

1.  JupyterA is created in ProjectA

2. JupyterB is created in ProjectB

3. Custom_Image1 is created from JupyterA

4. User is located on 'resources_list' page

*Step to reproduce:*

1.  Click '+ Create new' button

2. Choose ProjectA in 'Select project' drop down list

3. Choose available endpoint in 'Select endpoint' drop down list

4. Choose Jupyter in 'Select template' drop down list

5. Choose 'Custom_Image1'

6. Change project from ProjectA to ProjectB

*Actual result:*

1. 'Custom_Image1' is still available for ProjectB

*Expected result:*

'Custom_Image1' is absent for ProjectB",AWS AZURE Debian GCP RedH pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-04 11:10:39,10
13266079,[Keycloak]: 'Service Accounts' should be switched on by default,Parameter  'Service Accounts Enabled' should be switched on by default in Keycloak.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-11-04 10:02:43,9
13266074,[Quota]: Project endpoint (Edge) should be stopped in case of quota exceeding,If quota is exceeded Project endpoint (Edge) should be stopped too.,AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-11-04 09:52:39,6
13265769,Shrink computational resource name on confirmation dialog,If it is too long name for computational resource - cut it on confirmation dialog for stopping/termination.,AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-11-01 14:01:01,5
13265765,[Azure]: Cannot update billing information,"*Preconditions:*

1. Environment is created

*Steps to reproduce:*

1. go to billing.log

*Actual result:*
 # There is an error:
 *com.epam.dlab.billing.azure.BillingSchedulerAzure$CalculateBilling: Cannot update billing information*

*Expected result:*

1. there is not any error in billing.log",AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-11-01 13:46:00,6
13265740,'$anyuser' group is not taken into consideration for resource creation ,"*Preconditions:*

1. '$anyuser' group is assigned to Project1

*Steps to reproduce:*

1.  Go to 'resources_list' page

2. Click '+ Create new button'

3. Click 'Select project' drop down list

 

*Actual result:*

1. Project1 ** is absent in drop down list

*Expected result:*

1. Project1 ** is present in drop down list",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-11-01 10:49:54,6
13265550,[AWS]: Notebook stopping fails if it has active computational resources,"*Preconditions:*
 # EMR is in running status
 # Spark cluster is in running status

*Steps to reproduce:*
 # Go to 'resources_list' page
 # Click action menu for notebook
 # Choose 'Stop' notebook

*Actual result:*
 # EMR is in terminated status on Dlab UI
 # EMR is in running status on AWS console
 # Spark cluster is in failed status on Dlab UI
 # Spark cluster is in running status on AWS console
 # Notebook is in failed status on Dlab UI
 # Notebook is in running status on AWS console

*Expected result:*
 # EMR is in terminated status on Dlab UI
 # EMR is in terminated status on AWS console
 # Spark cluster is in stopped status on Dlab UI
 # Spark cluster is in stopped status on AWS console
 # Notebook is in stopped status on Dlab UI
 # Notebook is in stopped status on AWS console",AWS Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-10-31 14:36:37,6
13265529,[Azure]: Failed ssn creation is still running on Azure console,"*Preconditions:*

1. It is impossible to connect to pkg.jenkins.io

*Steps to reproduce:*
 # Deploy DLab via Jenkins job

*Actual result:*

 1.  SSN deployment fails

2. SSN is running on Azure console

*Expected result:*

 1.  SSN deployment fails

2. SSN is terminated on Azure console

Please, see Jenkins job v2.2 #223/#222
----
(i) Now SSN creation was failed on the other stage, however SSN was still running on Azure console.",2.3_old AZURE Debian DevOps,['DataLab Main'],DATALAB,Bug,Minor,2019-10-31 13:06:56,3
13265527,[Azure][Back-end]: Reconfiguration spark on Notebook fails,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*

1. Reconfigure Spark on Notebook

*Actual result:*

1. Notebook reconfiguration fails

*Expected result:*

1. Notebook reconfiguration is successful

 

For BE: Add parameter  endpoint_name for Notebook reconfiguration",AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-10-31 12:31:34,6
13265321,[AWS][Part1]: Alter billing values according to updates on billing page,"# (/) 'Bucket' should be in 'Resource type' column for all buckets
 # (/) Project_name should be in 'Project' column if it is project_bucket.
 # (/) 'Shared resource' should be in 'Project' column if it is ssn_bucket.",2.3_old AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-10-30 14:53:41,6
13265219,[AWS]: EMR termination fails due to absence of 'endpoint_name' parameter,"*Preconditions:*
 # EMR is created on notebook
 # User is located on 'resources_list' page

*Steps to reproduce:*

1.  Click terminate icon for EMR

2. Confirm terminate by clicking 'Yes' button

*Actual result:*

1. EMR termination fails

*Expected result:*

1. EMR is terminated successfully

 ",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-10-30 07:42:25,6
13265032,Add new cloud parameter in provisioning,"Add new cloud parameter in cloudProperties for provisioning.

<sharedAmiUsed>: <true> or <false>

Convey the parameter for the following:
 # create/terminate notebook
 # create/terminate computational resources
 # create/terminate project",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-10-29 13:07:18,6
13265016,[AWS]: Notebook creation does not use custom image,"*Preconditions:*
 # Custom image is created

*Steps to reproduce:*

1. Create notebook from custom image

*Actual result:*

1. Notebook is created from default image

Expected result:

1. Notebook is created from custom image",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-10-29 12:21:42,8
13264996,Update images for promotion page,Update images on website [https://dlab.opensource.epam.com/] according to new features:,2.3_old Front-end,['DataLab Main'],DATALAB,Task,Major,2019-10-29 09:10:41,10
13264993,[Azure][Back-end]: Configuration and reconfiguration spark on Data Engine fails,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*

1. Create custom Data Engine

*Actual result:*

1. Data Engine creation fails

*Expected result:*

1. Data Engine creation is successful

 

Also Data Engine reconfiguration fails 

 

For BE: Add parameter  endpoint_name for Data Engine reconfiguration",AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-10-29 08:58:57,6
13264827,User is auto logged out in case of wrong java dependency format,"*Preconditions:*
 # Notebook is in running status
 # User is located on 'resources_list page

*Steps to reproduce:*
 # Click action menu for notebook
 # Choose 'manage libraries'
 # Choose type in  'Select resource' drop down list
 # Choose 'Java' in 'Select group' drop down list
 # put wrong <org.jline » jline-terminal-jansi » 3.13.1> format 

*Actual result:*
 1. User is logged out

*Expected result:*
 1. User is logged in DLab",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['Cloud endpoint'],DATALAB,Bug,Major,2019-10-28 14:13:44,10
13264770,[Front-end]: Pass user/project/endpoint/custom tags on 'Billing Report' page,As a user I want to see all types of tags on 'Billing report' page so that I can swiftly determine the descriptions which are associated with tags.,AWS AZURE Ba Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-10-28 09:27:42,10
13264476,[List of Resource]: Switching between project is not available for user (not admin),"*Preconditions:*

1. User (not admin) is logged in DLab

*Steps to reproduce:*

1. Go to 'List of Resource' page

*Actual result:*

1. 'Select active project' drop down list is absent

*Expected result:*
 # 'Select active project' drop down list is present

************************************************************************

2.  In 'select active project' should be available for user only assigned his project. However not it is available all project.

FE should use GET /api/project/me",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-10-25 12:01:39,10
13264473,[Manage environment][dialog]: Add parameter project_name for stopping,"POST 

api/project/managing/stop/\{name}",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-10-25 11:36:42,10
13264454,[Web terminal]: After some time inactivity appears 401 Unauthorized error,"*Preconditions:*
 # Notebook is created
 # User is located on 'resources_list' page

*Steps to reproduce:*
 # Click action menu on notebook
 # Choose 'Open terminal'
 # Do not move mouse more than 2 minutes

*Actual result:*

1. 401 error appears in console

*Expected result:*
 # 401 error does not appear in console

*********************************************************************

*2.* Recovery the previous zoom due to non-coinciding cursor and mouse

 

 

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-10-25 09:36:53,10
13264442,[GCP]: Dataproc creation fails due to  Edge node is unavailable,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*

1. Create dataproc on notebook

*Actual result:*

Dataproc creation fails

*Expected result:*

Dataproc is created succesfully",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-10-25 08:30:29,12
13264274,[GCP]: Alter an error message for computational resources,"# Computational resource name should not exceed 7 symbols.
 # Alter an error message in case of computational resource name exceeds 7 symbols.

'Cluster name cannot be longer than 7 characters and can only contain letters, numbers, hyphens and '_' but can not end with special characters.'",Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Minor,2019-10-24 14:28:45,5
13264272,[Environment management]: Align names of computational resources,Names of computational resources should be aligned according to the header of computational resources and 'no details',AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-10-24 14:25:19,5
13264260,Filter actions should be in one row in 'environment_management' page,"Chrome Version 77.0.3865.90

Firefox 69.0.2 (64-біт)

It is reproduced even if zoom is 100%

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-10-24 13:31:50,5
13264237,[Project page]: Issues with endpoint status,"1. Disable endpoint removing from project during 'connecting' 

2. Value of endpoint status should have left alignment

3. Show active should not show 'Failed' endpoint status

4. 'No details'   should have left alignment in 'Endpoint' column

5. Disable endpoint removing from project if endpoint is in 'failed' status",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-10-24 11:11:16,10
13263471,Fix scheduler issue with token expiration,Fix scheduler issue with token expiration,pull-request-available,[],DATALAB,Bug,Major,2019-10-21 07:21:23,6
13262841,Add required parameters for billing (Azure),"Please add the following parameters required for *_billing.yml:_*
 # AZURE_SSN_STORAGE_ACCOUNT_TAG
 # AZURE_SHARED_STORAGE_ACCOUNT_TAG
 # AZURE_DATALAKE_TAG

Please add the following parameter required for *_self-service.yml:_*
 # SSN_INSTANCE_SIZE",2.3_old,[],DATALAB,Task,Major,2019-10-17 11:19:03,3
13261799,[GCP]: Add support of Spark cluster AMI,Spark cluster should be created from prepared AMI as developed for AWS/Azure.,pull-request-available,[],DATALAB,Task,Major,2019-10-11 13:00:52,8
13261043,Add validator to key,Add validator to the end of key for symbols < \n>.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-10-08 06:40:02,13
13260888,[Endpoint status]: Info message should be consistent,"Change messages:
 # from 'Processing! Endpoint start is in progress' to 'Processing! Endpoint connect is in progress'
 # from 'Processing! Endpoint stop is in progress' to 'Processing! Endpoint disconnect is in progress'",AWS AZURE Back-end Debian GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2019-10-07 12:59:09,6
13260871,Values of endpoint status should have left alignment,Align values of  endpoint status  by left side.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-10-07 11:49:44,10
13260855,[GCP]: SSN creation fails,"*Steps to reproduce:*

Deploy DLab SSN via Jenkins from develop

*Actual result:*
 # SSN creation fails. Error 'Unable to generate cert and copy to java keystore'

*Expected result:*

1. SSN is created succesfully",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Blocker,2019-10-07 10:31:53,9
13260853,[AWS]: Billing report is not found,"*Preconditions:*
 # Environment is created
 # Billing is available in S3 bucket

*Steps to reproduce:*
 # Go to 'Billing report' page

*Actual result:*

1. Billing is absent

*Expected result:*

1. Billing is present",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-10-07 10:21:50,6
13260577,[Billing]: Switching option 'View full billing report for all users' does not trigger,"*Preconditions:*
 1. Env is created
 2. User1 is logged in DLab

*Steps to reproduce:*
 1. Go to 'Role' page
 2. Click 'Roles' drop down list
 3. Uncheck 'View full billing report for all users' for user1
 4. Confirm action
 5. Go to 'Billing' page

*Actual result:*
 1. 'User' column is present

*Expected result:*
 1. 'User' column is absent",AWS AZURE Debian GCP RedH,['DataLab Main'],DATALAB,Bug,Major,2019-10-04 13:53:24,7
13260528,[Billing]: Some values are absent for edge,"*Preconditions:*
1. Billing is available in DLab

*Steps to reproduce:*
1. Go to billing report page

*Actual result:*
1. Resource type and status are absent for edge

Expected result:
1. Resource type and status are present for edge
",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-10-04 08:12:45,6
13260327,[Azure]: Edge-project termination fails if contains at least one resource ,"*Preconditions:*
1. Notebook is created in Project1

*Steps to reproduce:*
1. Terminate Edge-project1

*Actual result:*
1. Notebook is not terminated

Expected result:
1. Notebook is terminated
2. All related resources are terminated as well",AZURE Debian Front-end RedH,['DataLab Main'],DATALAB,Bug,Major,2019-10-03 14:57:47,9
13260313,[Azure]: Remote kernels are still available in list after Spark cluster termination on Jupyter/Zeppelin/DeepLearning/TensorFlow,"*Preconditions:*
1. Spark cluster is created on Jupyter/Zeppelin/TensorFlow/

*Steps to reproduce:*
1. Terminate spark cluster
2. Go to Notebook UI

*Actual result:*
1. Remote kernels are available in list for terminated spark cluster

*Expected result:*
1. Remote kernels are available in list for terminated spark cluster
 ",2.3_old AZURE Debian Known_issues(release2.2) RedH,['DataLab Main'],DATALAB,Bug,Minor,2019-10-03 14:12:06,3
13260292,[Azure]: Sometimes internal Jupyter terminal is not open successfully,"*Preconditions:*
 1. Jupyter is in running status

*Steps to reproduce:*
 # Go to Jupyter UI
 #  Click 'New' drop down list
 # Choose 'Terminal'

*Actual result:*
 Terminal is not open successfully

*Expected result:*
 Terminal is open successfully",2.3_old AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-10-03 11:31:51,3
13260288,[Azure]: Billing is not available,"*Preconditions:*
 # Env is created
 # Billing is available in Azure console

*Steps to reproduce:*
 # Go to billing page

*Actual result:*

1. Billing is not available in DLab

*Expected result:*

1. Billing is  available in DLab

 ",AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-10-03 11:09:53,6
13260285,[Azure]: Issues on billing page,"*Preconditions:*

1. Env is created

*Steps to reproduce:*
 # Go to billing page
 # Click header grid

*Actual result:*
 # Calendar, buttons are not aligned
 # Header grid is not extended

*Expected result:*
 # Calendar has center alignment
 # Buttons have left alignment
 # Header grid is extended
 # There is an error in console",AZURE Debian Front-end RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-10-03 11:01:23,10
13260125,[Azure]: Reconfiguration spark on Data Engine fails,"*Preconditions:*

1. Notebook is created

*Steps to reproduce:*
 # Create Data Engine
 # Reconfigure Data Engine

*Actual result:*

1. Data Engine reconfiguration fails

*Expected result:*

1. Data Engine reconfiguration is successful

 

 ",AZURE Debian DevOps Known_issues(release2.2) RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-10-02 14:54:14,3
13260094,[Azure]: SSN creation fails with DataLake,"*Steps to reproduce:*

1. Deploy DLab on Azure with DataLake

*Actual result:*

1.  SSN creation fails

*Expected result:*

1. SSN is created succesful",2.3_old AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-10-02 11:19:16,3
13260087,Users and groups should be case insensitive ,"*Preconditions:*
 # Environment is created
 # grp1/user1 are in lower case in LDAP

*Steps to reproduce:*
 # Create role where Grp1/User1 are in upper case
 # Assign Grp1/User1 to Project1

*Actual result:*

1. Grp1/User1 are forbidden to create resource in  Project1

*Expected result:*

1. 1. Grp1/User1 are allowed to create resource in  Project1

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-10-02 10:45:35,13
13259933,[GCP][Billing][Terraform][KeyCloak]: Investigate name convention in grid header,"In GCP console instance shape it is machine type.

In DLab grid we have 'Type'  in grid header and 'Select shape' in drop down list. However we should use the the same one, not different.

 

UPD: We use the 'shape' key, not like a placeholder, it has a more functional role in grid data filtering and selection inside filtering control. 

Need to be updated constants inside the billing filtering using the 'DICTIONARY' (by clouds) catalog for filtering data models.",2.3_old Debian Front-end GCP,['DataLab Main'],DATALAB,Bug,Minor,2019-10-01 14:47:06,5
13259922,[Terraform][KeyCloak][GCP]: Project value is absent in grid  on 'Environment management' page,"*Preconditions:*

1. Environment is cretaed

*Steps to reproduce:*

1. Go to 'Environment management' page

*Actual result:*

1. Project value is absent in grid

*Expected result:*

1. Project value is present in grid",GCP,['DataLab Main'],DATALAB,Bug,Major,2019-10-01 14:08:37,13
13259618,[Front-end]:Change values for 'Endpoint status',"# Change values for 'Endpoint status'

creating - connecting
running - connected
stopping - disconnecting
stopped - disconnected
terminated - terminated
terminating - terminating",AWS AZURE Debian Front-end GCP RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-09-30 08:57:14,10
13259617,[Back-end]:Change values for 'Endpoint status',"# Change values for 'Endpoint status'

creating - connecting
starting - connecting
 running - connected
 stopping - disconnecting
 stopped - disconnected
 terminated - terminated
 terminating - terminating",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['Cloud endpoint'],DATALAB,Task,Major,2019-09-30 08:55:53,6
13259255,[EQF][Environment management]: Can not get token on stage of cluster/notebook termination/stopping,"*Preconditions:*

1. Cluster or notebook are created

Steps to reproduce:
 # Go to 'Environment management' page
 # Stop or terminate a cluster/notebook

*Actual result:*

Cluster/notebook are not stopped or terminated

*Expected result:*

Cluster/notebook are stopped or terminated",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-27 12:16:04,6
13259253,[EQF][Project management]: Checkbox should be non-editable for previously added endpoint,Unchecking previously added endpoint (during creating or updating) should be forbidden,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-27 12:05:59,10
13259250,[EQF][Manage environment]: Project stopping/termination should depend on instance status,"These actions are related on 'Manage environment' popup
 # If all edges are terminated stopping/termination actions should be disabled
 # If at least one instance is in running status  ** stopping/termination actions should be enabled",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-27 11:53:39,10
13259248,[EQF][Environment management]: Extend filter,"# In 'Type' drop down list it is filtered only by type, but should be also by name on 'environment_management' page
 # Value in 'user' drop down list should be in lower case in 'environment_management'page
 # Gear icon should not be cut in 'environment_management' page
 # Gear icon should be aligned by top value in 'environment_management' page
 # Filter actions should be in one row in 'environment_management' page
 # Align grid names with grid values on 'resources_list' page",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-27 11:42:01,10
13259056,[DLAB-terraform][KeyCloak]: Custom image creation fails,"*Preconditions:*
1. Any notebook is in running status
*Steps to reproduce:*
1. Create custom image from Dlab UI
*Actual result:*
Custom image is not created
*Expected result:*
1. Custom image is created",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-09-26 13:29:04,8
13259029,[Billing page]: Drop down list is under (crossed by) grid,"*Preconditions:*

1. Billing is not available

*Steps to reproduce:*
 # Go to Billing page
 # Click any drop down list

*Actual result:*

1. Drop down list are under grid

*Expected result:*

1. Drop down list are over grid",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-26 12:02:01,10
13259018,Automate Self Signed certificate generation,"During DLab deployment certificates are pulled from bucket, where they have been put before by manually.

However it should be automated when UI and endpoints are signed by the same certificate.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-26 11:13:19,9
13258995,[EQF]: Extend token validation,"Now we have token validation up to 5 minutes.

Extend it up to one hour.

It should be done only for EQF manually.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-26 08:38:27,9
13258781,[DataLab-terraform][KeyCloak]: Convey SBN value during deploy SSN,"DLAB-terraform branch.
",2.3_old AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-25 11:51:40,3
13258776,Add active instances filtering,"Add 'Show all/show Active' buttons on 'Project' page;
It is should the same functional for the projects as it is implemented for instances on 'resources_list' page",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-25 11:26:48,10
13258739,[List or resource page]: Tag tool tip should contain <name tag: value tag> ,If mouse over a tag a tool tip should have the following structure '<name tag: value tag'>,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-25 09:05:39,10
13258737,[Role]: Change order creation for group adding,"Currently the order is 'Group name'-> 'Roles'-> 'Users'

But should be: 'Group name'->'Users'-> 'Roles'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-09-25 08:52:01,10
13258727,"Remove 'Backup', 'SSN Monitor' buttons","Do not display 'Backup', 'SSN Monitor' buttons on DLab UI.

It is caused by the following::

1. Backup does not work

2. SSN Monitor is useless information now",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-25 08:29:15,10
13258600,[EQF]: Project termination fails,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. terminate project from DLab UI

*Actual result:*
 # Project termination fails

*Code 500: message 'HTTP 405 method not allowed'*

*Expected result:*

1. Project termination is successful",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-24 17:17:15,13
13258593,[EQF][Manage environment]: Project status should not be considered for environment management,We rid of a project status. that's why we should not take into consideration  a project status during environment management,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-24 16:31:53,10
13258592,[EQF]: Backup execution runs with 500 error,"*Preconditions:*

1. Environment is created

*Steps to reproduce:*
 # Go to 'Environment management' page
 # Click 'Backup' button
 # Click 'Apply' button

*Actual result:*

1. Backup execution fails

*Expected result:*

1. Backup execution is successful",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-24 16:16:45,13
13258581,[EQF]: Add support of multiple endpoints within one project,"Multiple endpoints functionality is implemented for whole DLab, however it does not work within one project (see attachments).

Add possibility to support multiple endpoints within one project.

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-24 15:34:21,12
13258572,[EQF]: Project editing causes 500 error,"*Precondirions:*

1. Project is created

*Steps to reproduce:*
 # Click 'edit' icon for project
 # Go to 'Groups'
 # Add new group
 # Click 'Update' button

*Actual result:*

1. Project is not updated

*Expected result:*

1. Project is updated",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-24 14:36:12,10
13258566,[EQF]: Project issues,"# If all edges are  terminated  icons terminate should be disabled. 
 # Endpoint which is assigned to a project should be marked and NOT be editable in edit project popup
 # If it is only one edge on project should be only one icon for terminate
 # Name edge column 'Endpoint status'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-24 14:24:26,10
13258535,[EQF]: 400 error for java dependency (library instalation),"*Preconditions:*

1. Notebook or cluster is in running status

*Steps to reproduce:*

1. Install any library for notebook/cluster from java group

*Actual result:*

1. Library is not installed installed. 400 error after typing library name

*Expected result:*

1. Library is installed",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-09-24 11:54:14,6
13258501,[GCP]: Limit quantities for computational resource name,"According to 'IAM Entity Character Limits' role name cannot exceed 63 characters.

1. Limit quantities for computational resource name up to 9 symbols only for GCP
 2. Alter an error message in case of computational resource name exceeds 9 symbols.

'Cluster name cannot be longer than 9 characters and can only contain letters, numbers, hyphens and '_' but can not end with special characters.'

 ",Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Major,2019-09-24 10:07:09,10
13258498,[EQF]: Terminated project is proposed for notebook creation,"*Preconditions:*

1.  User1 is assigned to a terminated notebook

2. User1 is located on 'List of resources' page

*Steps to reproduce:*
 # Click '+Create new' button
 # Click 'Select project' drop down list

*Actual result:*

Terminated project is in drop down list

*Expected result:*

Terminated project is not in drop down list

 

*NOTE:* If Edge status is stopped/terminated/creating/stopping/terminating in a project (and no one edge status is NOT in running status) do not convey this project in  'Select project' drop down list.",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Major,2019-09-24 10:01:45,13
13258477,Add Apache Superset template as a docker container,"For research you can use the following information:

1. [https://superset.incubator.apache.org/]

2. [https://superset.incubator.apache.org/installation.html#google-bigquery]

3. [https://superset.incubator.apache.org/installation.html#start-with-docker]",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-24 08:18:03,3
13258292,[DevOps]: Support a multiple cloud functionality,"As a user I want to use more than one endpoints for a project so that one project can have environment on AWS, Azure, GCP.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-23 13:23:25,9
13258291,[Front-end]: Support a multiple cloud functionality,"As a user I want to use more than one endpoints for a project so that one project can have environment on AWS, Azure, GCP.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-23 13:21:52,5
13258251,[GCP]: Billing issues,"*Preconditions:*
 # Billing is available on env

*Steps to reproduce:*
 # Go to detailed billing/billing page

*Actual result:*
 # Detailed billing are not  rounded 
 # Values of  'resource type' are absent only for volume of computational resources
 #  Volume of computational resources are marked as 'shared resource'
 # Total calculation  does not include rounded values

*Expected result:*
 # Detailed billing should be rounded to the hundreds
 #  Values of  'resource type' of computational resources contains 'Volume'
 #  Volume of computational resources are marked by user who has created it
 # Total calculation includes rounded values


 ",Back-end Debian GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-09-23 09:22:13,6
13257963,[Generate key]: 'Generate' should have the same style and size as 'Upload' button,"*Preconditions:*
1. Only SSN is created

*Steps to reproduce:*
1. Go to a 'Project' page

*Actual result:*
1. 'Upload' and 'Generate' button have different style

*Expected result:*
1. 'Generate' button has the same size as 'Upload' button
2. 'Generate' button has the same color as 'Upload' button
",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-09-20 14:36:01,10
13257962,[Generate key]: Only format of key.pub should be stored on instances,"*Preconditions:*
1. SSN is created

*Steps to reproduce:*
1. Click 'Generate' key

*Actual result:*
1. user_name.pem is generated per user
2. user_name.pub is not stored on project
3. Appears error message: 'Wrong key format. Key should be in openSSH format'

*Expected result:*
1. user_name.pem is generated per user
2. user_name.pub is stored on project",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-20 14:18:54,13
13257887,Web terminal usability,"1. Web terminal should be aligned by center
2. Copy/paste tools should be allowed in web terminal
3. Font should be more readable, likewise as it is in internal terminal in Jupyter UI. Also more characters should be contained in one row in web terminal.

See screenshots in attachment for comparing between DLab UI terminal and Jupyter UI (internal) terminal.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-20 08:40:17,10
13257085,[Front-end]: Add 'Test' button to endpoints,As an admin I want to check if it is connection to endpoint so that I can be convinced that endpoint instance is correct and works.,2.3_old AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2019-09-17 11:20:51,5
13257084,[Back-end]: Add 'Test' button to endpoints,As an admin I want to check if it is connection to endpoint so that I can be convinced that endpoint instance is correct and works.,2.3_old AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2019-09-17 11:20:01,6
13257079,[Endpoint]: Alter error message,"Change error message:

*from* 'Endpoint url can only contain letters, numbers, hyphens and '_' but can not end with special characters'

*to* 'Endpoint url should end with slash.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-17 10:36:16,10
13257076,[KeyCloak]: Prevent all buttons/actions which do not work,"Prevent all actions which are marked by red arrows (see attachment).

These actions do not work",AWS Debian DevOps,['DataLab Main'],DATALAB,Task,Major,2019-09-17 10:13:57,9
13256903,[KeyCloak][Azure]: Project creation fails,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create a project

*Actual result:*
 # Project is not created
 # Project status is in 'creating' on DLab UI
 # Docker for Project creation is not executed

*Expected result:*
 # Project is created
 # Project status is in 'Active' on DLab UI
 # Docker for Project creation is executed with '0'

 ",AZURE Debian RedHat,['DataLab Main'],DATALAB,Task,Blocker,2019-09-16 14:34:42,13
13256819,[Front-end]: Embed shared image functionality on DLab UI,As an admin I want to turn on/off of using shared image in project setting so that I can easily switch without changing configuration files via SSH.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-16 08:39:01,10
13256816,[Back-end][Part1]: Embed shared image functionality on DataLab UI,As an admin I want to turn on/off of using shared image in project setting so that I can easily switch without changing configuration files via SSH.,AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-16 08:36:44,6
13256500,Scheduler by time does not check out running jobs on Spark Standalone,"*Preconditions:*

1. Notebook is in running status

*Steps to reproduce:*

1. Set up  notebook scheduler by time for stopping 

2. Check off 'In case of running...' check box

!Setting.png!

3. Set the same time when the playbook was been running (local/remote kernels)

*Actual result:*

Notebook stop scheduler is triggered

*Expected result:*

Notebook stop scheduler is not triggered",2.3_old AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-13 10:14:55,3
13256477,[Azure]: Add parameter of shared image (notebook/spark cluster) in deploy script,"Add parameter of using shared image during the notebook or spark cluster creation in deploy script.

 ",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:31:32,8
13256476,[GCP]: Add parameter of shared image (notebook) in deploy script,"Add parameter of using shared image during the notebook creation in deploy script.

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:30:40,8
13256475,[AWS]: Add parameter of shared image (notebook/spark cluster) in deploy script,"Add parameter of using shared image during the notebook or spark cluster creation in deploy script.

 ",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:30:00,8
13256472,Implement SSO for notebooks,Ponder for using keycloak-gatekeeper.,AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:16:20,3
13256470,[GCP][DevOps]: Support a multiple endpoints functionality,"As a user I want to use more than one endpoints for one project so that one project can has environment on AWS, Azure, GCP.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:08:37,12
13256469,[Front-end]: Support a multiple endpoints functionality,"As a user I want to use more than one endpoints for one project so that one project can has environment on AWS, Azure, GCP.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:06:56,10
13256468,[Back-end]: Support a multiple endpoints functionality,"As a user I want to use more than one endpoints for one project so that one project can has environment on AWS, Azure, GCP.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-09-13 08:05:17,13
13256369,[Scheduler]: 'Default timezone_offset' is rewritten if switching scheduler types for notebook,"*Preconditions:*

1. Notebook is created (in 'running' or 'stopped' statuses)

*Steps to reproduce:*
 # Turn on scheduler by inactivity for notebook
 # Click 'Save' button
 # Turn on scheduler by time

*Actual result:*

1. Value 'Select offset' is absent ('Z' in F12)

*Expected result:*

1. Value 'Select offset' is default value of your browser",2.3_old AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-09-12 15:08:46,5
13256359,[GCP]: Docker tensor-rstudio fails due to 'No JSON object could be decoded',"*Steps to reproduce:*
 # Create SSN on GCP
 # Go to SSN via SSH

*Actual result:*

1.  Docker dlab-tensor-rstudio:latest runs with '1'

*Expected result:*

1. Docker dlab-tensor-rstudio:latest runs with '0'",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Minor,2019-09-12 14:42:12,3
13256336,Avert a notebook creation for users who are in a deleted group,"*Preconditions:*
 1. User1 is in Group1
 2. Group1 is assigned to a Project1

*Steps to reproduce:*
 1. Delete Group1 on 'Roles' page

*Actual result:*
 1. User1 is still allowed to create notebook in Project1

*Expected result:*
 1. User1 is not allowed to create notebook in Project1

 

*NOTE:*

This case reproduces only for group which are in LDAP group or for <$anyuser>.

So if admin deletes a group which are assigned to a project an error message should appear:

'Group can not be removed because it is used in some project'. An error message should not appear for a group which is assigned to a  project which is in deleted/deleting statuses",AWS pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-12 11:56:08,6
13256329,[Front-end]: Add reminder for computational resources termination,"As a user I want to be informed in 15 minutes about computational resources termination so that I can reschedule termination in case of working with this cluster.

It should be implemented likewise it is in reminder for instances stopping.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-12 11:40:02,0
13256328,[Back-end]: Add reminder for computational resources termination,"As a user I want to be informed in 15 minutes about computational resources termination so that I can reschedule termination in case of working with this cluster.

It should be implemented likewise it is in reminder for instances stopping.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-12 11:39:02,13
13256279,[Environment Management page]: Align 'gear' according to other top icons,All icons should be on the same top level in grid on 'Environment Management' page.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-12 07:22:16,10
13256152,[Create analytical tool]: Prevent extra scrollbar,"Extra scrollbar appears if custom image is available for notebook template in MS Windows [Version 10.0.18362.239] in browsers:

1.  Google Chrome [76.0.3809.132]
2. Opera [63.0.3368.71]



 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-11 16:53:40,10
13256126,[Keycloak]: Issues connected with quota,"# *+Any instance is not stopped in case of total quota exceeding+*

*Preconditions:*

1. Instances are in running status

*Steps to reproduce:*
 # Set quota total less than it is used in real

*Actual result:*

Instances are NOT stopped

*Expected result:*

Instances are  stopped

*****************************************************************************************

+*2. If project quota is exceeded it is allowed to start notebooks*+ 

*Preconditions:*
 # Quota is exceeded for Project1
 # Notebook is in 'stopped' status 

*Steps to reproduce:*

1. Start notebook

*Actual result:*

1. Notebook is starting

*Expected result:*
 # User is not allowed to start notebook
 # Error message appears ""Operation can not be finished. Resource quote is reached""

 *************************************************************************************************

+*3. If total quota is exceeded it is allowed to create a new project*+

*Preconditions:*
 # Total quota is exceeded

*Steps to reproduce:*

1. Create a new project

*Actual result:*

1. It is  allowed to create a new project

*Expected result:*
 # It is NOT allowed to create a new project
 # Error message appears ""Operation can not be finished. Resource quote is reached""

********************************************************

*+4. Project is not stopped in case of project quota exceeding+*

*Preconditions:*

1. Project1 is in active status

*Steps to reproduce:*
 # Set quota for project1 less than it is used in real

*Actual result:*

Project1 is NOT stopped

*Expected result:*

Project1 is stopped

 ",AWS Back-end Debian,[],DATALAB,Bug,Major,2019-09-11 14:32:19,13
13255868,[Computational resources]: Do not break link in several rows,If link is too long cut it in order to locate the link in one row.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-10 12:12:39,10
13255672,[Spark cluster]: 'Sync_start-required' should not be rewrite after turning on scheduler by inactivity ,"*Preconditions:*
1. Spark cluster is created

*Steps to reproduce:*
1. Turn on  scheduler by inactivity for Spark cluster
2. Turn on scheduler by time for Spark cluster

*Actual result:*
1. Choose start date is not editable
2. Choose start time is not editable
3. Select offset is not editable

*Expected result:*
1. Choose start date is editable
2. Choose start time is editable
3. Select offset is editable",AWS AZURE Back-end Debian GCP RedHat,[],DATALAB,Bug,Major,2019-09-09 15:17:55,13
13255629,[AWS]: RStudio with TensorFlow creation fails due to 'conf_service_base_name' is not defined,"*Preconditions:*
1. Env is created

*Steps to reproduce:*
1. Create RStudio with TensorFlow

*Actual result:*
RStudio with TensorFlow creation fails

*Expected result:*
1. RStudio with TensorFlow is created
",AWS Debian pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-09-09 13:07:40,3
13255253,[Azure]: Notebook connection via SSH github/gitlab fails,"*Preconditions:*
1. Notebook is in running status

*Steps to reproduce:*
1. Go to notebook terminal
2. Run a command *ssh-keygen -t rsa*
3. Add genereted key.pub to github/gitlab (in your settings)
4. Run a command *git clone <SSH_path>*

*Actual result:*
1. Repository is not cloned on notebook
nc: Proxy error: ""HTTP/1.1 503 Service Unavailable""
ssh_exchange_identification: Connection closed by remote host
fatal: Could not read from remote repository

*Expected result:*
1. Repository is cloned on notebook",2.3_old AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-06 12:37:34,3
13255074,[Debian]: Notebook/spark cluster creations stuck using shared image,"*Preconditions:*
1. Shared AMI is available 

*Steps to reproduce:*
1. Created the second notebook from shared AMI

*Actual result:*
1. Notebook creation is stuck

*Expected result:*
1. Notebook is created",AWS AZURE Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Critical,2019-09-05 20:02:01,3
13254989,[Resource list]: Grid is broken due to cannot read  property 'resources' ,"Please, see attachment",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-05 12:11:34,10
13254734,[Scheduler on idle]: Inactivity time is not updated after job running for local/remote Sparks,"*Preconditions:*
 # Notebook/Spark cluster is created

*Steps to reproduce:*

1. Run job for remote/local kernels

*Actual result:*

1. Inactivity time is not updated

*Expected result:*

1. Inactivity time is updated

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-09-04 15:28:45,12
13254732,[Scheduler on idle]: Does not trigger by inactivity for Spark Standalone cluster,"*Preconditions:*

1. Spark cluster is in running status

*Steps to reproduce:*

1. Actual inactivity time is more than  inactivity time for Spark in scheduler

*Actual result:*

Spark standalone is rinning

*Expected result:*

1. Spark standalone is stopped",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-09-04 15:12:23,6
13254708,[AWS][DLAB-terraform][KeyCloak]: Billing is not updated on DLab UI,"*Preconditions:*

1.  Env is created

2. Billing is loaded in S3

*Steps to reproduce:*

1. Go to 'Billing report' page

*Actual result:*
 # Billing is not available on 'Billing report' page
 # Detailed billing is not available

*Expected result:*
 # Billing data is available on DLab UI
 # Detailed billing is available

*******************************************
 # Shape is absent for edge node on 'billing page'
 #  Start/End dates are absent on detailed billing",AWS,['DataLab Main'],DATALAB,Bug,Critical,2019-09-04 12:55:45,13
13254682,[Front-end]: Add possibility to generate key pairs during project creation,"As an admin I want to have possibility to generate a pair of key during project creation, so that I should not create key pair by myself beforehand.

1.  key.pub is put on instances (project/notebook/cluster)

2. key.pem is offered to save in PC

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-04 10:47:39,10
13254677,[Back-end]: Add possibility to generate key pairs during project creation,"As an admin I want to have possibility to generate a pair of key during project creation, so that I should not create key pair by myself beforehand.

1.  key.pub is put on instances (project/notebook/cluster)

2. key.pem is offered to save in PC

 ",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-04 10:31:48,6
13254409,[Backup]: Jars files are absent,"*Preconditions:*

Env is created

*Steps to reproduce:*

Execute backup for jars

*Actual result:*

Jars files are absent

*Expected result:*

Jars files are present",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-09-03 14:19:12,12
13254391,[EQF]:Stop project simultaneously should affect related instances,"*Preconditions:*

Environment is created

Steps to reproduce:
 # Go to 'Manage environment' popup
 # Click stop project

*Actual result:*
 # Any instance is  not stopped
 # Related instances are in running status

*Expected result*
 # Edge is stopped
 # Related instances are stopped

*Notes:*
 * Stop project should take into consideration instances in 'creating/configuring/starting/reconfiguring/Creating Image/' statuses. If any instance is in these statuses do not allowed admin to stop/terminate project.
 In this case convey error message: ""Can not stop environment because on of user resource is in status CREATING or STARTING""
 * If at least one instance is in running status and edge is stopped and the other instances as well - allow admin to stop project

 ",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-09-03 13:08:03,6
13254260,[Manage libraries]: Remove extra expansion of the library list on installing process,"Please, see attachment",AWS Azure Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-02 14:47:22,10
13254247,[GCP]: Spark cluster creation fails if project name contains upper case,"*Preconditions:*
 # Project name contains upper case
 # Notebook is in running status

*Steps to reproduce:*

1. Create a spark cluster

*Actual result:*

Spark cluster creation fails:

*Expected result:*

Spark cluster is created

 ",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-02 13:37:46,12
13254188,"[Notebook/cluster creation]: Do not accept <->, <_> for unique name validation","Add error message: 'This name already exists.'

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-09-02 08:51:39,10
13254181,Group name should be case insensitive,"*Preconditions:*
1. Admin is located on 'Roles' page



*Tasks:*
1. Group name should be case insensitive. Add error message: 'Group name already exists.'",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-02 08:35:20,10
13254171,[GCP][Resource list]: Align actions icon according to gear,Align actions icon according to gear in grid (see attachment),Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-02 07:56:32,10
13254161,[Endpoint]: Align endpoint value by vertical,Align endpoint value by vertical (see attachments),AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-09-02 07:19:09,10
13254159,[Quota]: Prevent putting some values and align error message,"1.  Error message overlaps on text box

2. Prevent putting non-integer value or add error message:

'Budget can contain only integer value'. What non-integer values are allowed depends on kind of browser (see attachment)",2.3_old AWS AZURE Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-09-02 06:59:46,5
13253984,[Azure]: Project starting fails,"*Preconditions:*

1. Project is in 'Active' status

*Steps to reproduce:*

1. Start project from DLab UI

*Actual result:*
 # Project is in 'Not failed' status
 # Docker runs with '1'

*Expected result:*
 # Project is in 'Not Active' status
 # Docker runs with '0'

 ",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-08-30 16:53:34,3
13253983,[Azure]: Project termination fails,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Terminate a project

*Actual result:*
 # Docker executes with '0'
 # There is an error in docker logs
 # Project status is 'Deleted' on Dlab UI
 # Project status is 'running' on Azure console

*Expected result:*
 # Docker executes with '0'
 # There is no error in docker logs
 # Project status is 'Deleted' on Dlab UI
 # Project is not on Azure console",AZURE Debian DevOps RedHat,[],DATALAB,Bug,Major,2019-08-30 16:47:04,3
13253982,[Azure]: Any notebook is not created successfully,"*Preconditions:*
 # User is assigned to a project

*Steps to reproduce:*
 # Create any notebook

*Actual result:*

1. Notebook creation fails (see attachments)

*Expected result:*

1. notebook is created successfully",AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Blocker,2019-08-30 16:39:34,8
13253924,[Keycloak]: Ldap group/users are not mapped to DLab role,"*Preconditions:*
 # User1 is not allowed to do administrative operation

*Steps to reproduce:*
 # Login by user1
 

*Actual result:*
 # User1 is allowed to do administrative operation

*Expected result:*

1. User1 is not allowed to do administrative operation

 

 ",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-30 12:30:28,13
13253746,[EQF][List of resources]: Add possibility to switch between projects,"As a user I want to have two have two choices: 1. to work in current project or to work in all projects  on 'List of resources' page, so that it will be more user-friendly during instance creation.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,New Feature,Major,2019-08-29 15:51:37,10
13253739,[Billing report]: Add project data to reporting grid,As a user I want to filter all resources by project so that it will be easy to work with billing data.,AWS AZURE Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-29 15:33:21,10
13253729,Custom image should be fetched depending on project/endpoints,"*Preconditions:*
 # User1 is assigned to Project1
 # User1 is assigned to Project2
 # Custom image is created from Jupyter1 in Project1

*Steps to reproduce:*

1. Click '+ Create new' button

2. Choose Project2

3. Choose Jupyter notebook template

 

*Actual result:*
 # Custom image is from Jupyter1 is available

*Expected result:*
 # Custom image is from Jupyter1 is not available",AWS AZURE Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-08-29 14:51:53,6
13253727,Do not convey custom_tag in case of its value is empty,Custom_tag should not be portrayed on 'list of resources' page if custom_tag value is empty.,AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-08-29 14:40:18,10
13253725,Notebook action should consider only the statuses of related clusters,"*Preconditions:*
 # Notebook1 is created in Project1
 # Any cluster1 is created on Notebook1
 # Notebook2 is created in Project1
 # Any cluster2 is created on Notebook2

*Steps to reproduce:*
 # Stop cluster1
 # Go to action menu for Notebook2

*Actual result:*

It is forbidden to stop/terminate Notebook2

*Expected result:*

It is allowed to stop/terminate Notebook2",pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-08-29 14:22:35,10
13253723,Cluster name should be unique per notebook,"*1.* Add check for unique *computational resource* name per notebook.
If computational resource name is not unique add *error message*: 'This name already exists.'

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-29 14:06:27,10
13253513,[Azure]: Project status is not updated from creating to active,"*Preconditions:*

1. SSN is created

*Steps to reproduce:*

1. Create a project

*Actual result:*
 # Project is created (docker executed with '0')
 # Project status is still in 'creating' status

*Expected result:*
 # Project is created (docker executed with '0')
 # Project status is in 'active' status",AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-08-28 14:11:26,3
13253495,[Terraform]: Jenkins job for termination does not delete images and snapshots,"*Preconditions:*

1. Environment is created

*Steps to reproduce:*

1. Run jenkins job for terination

*Actual result:*

1. Images and snapshots are NOT deleted

*Expected result:*

1. All related resources are deleted",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Major,2019-08-28 13:14:04,12
13253441,[Terraform]: Add possibility to login DataLab via 443 port,"As an user I want to use TLS/SSL during login DataLab,  so that I can be provided by communications security.

Currently user can login DataLab via 80 port.

Please, add possibility to login via 443 port.",2.3_old AWS Debian DevOps,['DataLab Main'],DATALAB,Task,Major,2019-08-28 08:47:08,3
13253282,[Terraform][AWS]: Investigate IAM checking during login,"Previously it was checked if user is an IAM user. In terraform this checking is absent. 

Please. investigate how to add it.",AWS Back-end Debian,[],DATALAB,Task,Major,2019-08-27 15:08:49,13
13253280,[Terraform]: Issues connected with quota,"# *+Any instance is not stopped in case of quota exceeding+*

*Preconditions:*

1. Instances are in running status

*Steps to reproduce:*
 # Set quota (total/project) less than it is used in real

*Actual result:*

Instances are NOT stopped

*Expected result:*

Instances are stopped

+*2. If project quota is exceeded it is allowed to create new instance*+

*Preconditions:*
 # Project quota is exceeded

Steps to reproduce:

1. Create a notebook/cluster

*Actual result:*

Notebook/cluster is created

*Expected result:*

User is not allowed to create notebook/cluster

 ",AWS Back-end Debian,[],DATALAB,Bug,Major,2019-08-27 15:03:49,13
13253276,Augment tooltip to project name on  Manage environment popup,"The project names have the same names (after cutting) if project names are too long and have the same beginning.

Please, add tooltip to project name on 'Manage environment' popup",AWS AZURE Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-08-27 14:48:17,10
13253272,[Terraform][Quota]: Convey error message during cluster creation in case of quota exceeding,"*DLAB-terraform branch*

As a user I want to know why I cannot create a cluster so that I can resolve the situation.

 

If quota is exceed a user is not allowed to create cluster. Please, inform user about it via error message:

'Operation can not be finished. Resource quote is reached'",AWS Debian Front-end pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-27 14:34:52,10
13253255,[Terraform]: Backup is not stored in /opt/dlab/tmp/result,"*Preconditions:*

1. Admin is located on 'environment_management' page

*Steps to reproduce:*
 # Click 'Backup' button
 # Click  'Apply' button

*Actual result:*

Backup.tar.gz is NOT in /opt/dlab/tmp/result

Backup.tar.gz destination is unknown

*Expected result:*

Backup.tar.gz is in /opt/dlab/tmp/result

 

Also Backup execution needs fabric v.1.14.0 to be installed.
 *Even after moving backup.py to /opt/dlab/conf and fabric installation the following issues appear:*
 1. Backup execution fails on certs/db/logs
 2. Jars directory is empty
 3. Backup_name.tar.gz does not contain date and time of backup execution",AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Minor,2019-08-27 12:37:52,12
13253232,[Terraform]: It is impossible to do any git actions via ungit,"*Preconditions:*
 # User is located on ungit page
 # Git account is created in DLab

*Steps to reproduce:*
 # Put in </home/dlab-user> in ungit search
 # Paste https path to git repository in ungit
 # Click 'clone' button

*Actual result:*
 # For every git actions username and password are required in ungit
 # required credentials are so frequent, that it is impossible to do git actions 

*Expected result:*
 # Git action is done without credential requiring in ungit

Previously git credentials were stored in notebook in <.netrc>, but now this file is absent now",2.3_old AWS Debian DevOps,['DataLab Main'],DATALAB,Bug,Minor,2019-08-27 10:56:57,3
13252702,EMR creation fails only from the first attempt on the same notebook,"*Preconditions:*

1. Apache Zeppelin/Jupyter is in running status

*Steps to reproduce:*

1. Create EMR on Apache zeppelin

*Actual result:*

EMR configuration fails

*Expected result:*

EMR is created

From the second and the other attempts EMR is configuring successfully ONLY on the same notebook.

Please, find out if the bug is present in develop

 ",AWS Debian DevOps,['DataLab Main'],DATALAB,Task,Major,2019-08-23 15:11:03,12
13252695,Scrollbar consistency,Scrollbar should have the same style.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Trivial,2019-08-23 14:48:04,10
13252692,[Apache Zeppelin]: Error message overlaps on 'Create analytical tool' popup,"*Preconditions:*
 # Project is created
 # *User is located on 'Resource_list' page*

*Steps to reproduce:*

1.  Click '+Create new' button

2. Choose Apache Zeppelin

3. put invalid data for notebook name <zep-> 

*Actual result:*

Error message overlaps on 'name' text box 

*Expected result:*

Error message is under 'name' text box 

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-08-23 14:43:56,10
13252680,[Terraform]: Notebook creation fails in case of allocating the same CIDR to different subnets,"This bug is reproduced if the same CIDR is allocated to different subnets

*Preconditions:*
 # Two SSNs are deployed
 # Project1 in SSN1 has CIDR which has project2 has in SSN2

*Steps to reproduce:*
 # Create notebook for project1 

*Actual result:*

Notebook is not created

*Expected result:*

Notebook is created",AWS Debian DevOps pull-request-available,[],DATALAB,Bug,Major,2019-08-23 14:01:58,12
13252672,Pass user/project/endpoint/custom tags to FE on 'Billing Report' page,As a user I want to see all types of tags on 'Billing report' page so that I can swiftly determine the descriptions which are associated with tags.,AWS AZURE Ba Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-23 13:30:03,6
13252649,Notebook custom image is not used during creation,"*Preconditions:*

1.  Project name contains upper case

2. Notebook is running

*Steps to reproduce:*
 # Create custom AMI from running notebook
 # Create notebook from custom AMI

*Actual result:*

Notebook is created from default AMI

*Expected result:*

Notebook is created from custom AMI

 ",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-08-23 12:15:29,12
13252611,Add tag to 'list of resources' page,As a user I want to see all type of tags which are assigned to resources so than I can quickly turn out resources with my description in tags. ,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-23 09:14:20,10
13252417,[AWS]: EMR creation fails in case of the othe EMR creation/termination simultaneously,"*Preconditions:*

1.  two EMRs are running on notebook1 in project1

3. one EMR is running on notebook2 in Project2

*Steps to reproduce:*
 # Create one EMR on  on notebook1 in project1
 # Create one EMR on  on notebook3 in project2
 # 

*Actual result:*
 # Three EMR is terminated
 # EMR (on  on notebook1 in project1) is failed
 # EMR (on  on notebook3 in project2) is created

*Expected result:*
 # Three EMR is terminated
 # EMR (on  on notebook1 in project1) is created
 # EMR (on  on notebook3 in project2) is created",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-22 15:28:58,12
13252414,EMR creation does not use spot instance,"*Preconditions:*

1. Zeppelin/Jupyter/RStudio is created

*Steps to reproduce:*

1. Create EMR with spot instance

*Actual result:*

EMR is created without spot instance

*Expected result:*

EMR is created without spot instance

 

Add parameter *emr_slave_instance_spot* and value for it should be true or false

 

 ",AWS Debian Front-end RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-22 15:13:50,10
13252370,[Detailed billing]: Name value overlaps,"If there is too long name for instance, the name value overlaps.

Extend the grid by height",AWS AZURE Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-08-22 12:07:07,10
13252324,[Terraform]: Investigate why K8S was not starting successfully,"Steps to reproduce:
 # Stop master/workers/endpoint instances
 # Start master/workers/endpoint instances

In this case instances were started, but kube_cluster did not.

Please, also fix it.",AWS Debian DevOps,['DataLab Main'],DATALAB,Task,Major,2019-08-22 08:48:40,3
13251880,[Create analytical tool]: Buttons should not be cutted in Firefox/Edge browsers,"*Preconditions:*

1. Environment is created

*Steps to reproduce:*
 # Click '+ Create new' button
 # Enter valid values
 # Check off 'Spark configurations '

*Actual result:*

'Create'/'Cancel' buttons are cutted

*Expected result:*

'Create'/'Cancel' buttons are not cutted",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-08-20 16:31:17,10
13251864,DLab role is not mapped with LDAP group or users," 

*Preconditions:*
 1. user1/group1 is allowed to do administration operations
 2. user2/group2 is not allowed to do administration operations

*Steps to reproduce:*
 1. Login Dlab with user1
 2. Login DLab with user2

*Actual result:*
 1. User1/group1 is allowed to do administration operations
 2. User2/group2 is not allowed to do administration operations

*Expected result:*
 1. User1group1 is allowed to do administration operations
 2. User2/group2 is not allowed to do administration operations

And it is forbidden to login DLab (AWS)

 ",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-20 15:19:40,9
13251841,[Terraform branch]: Project stopping/termination fail,"*Preconditions:*

1. Project is created

*Steps to reproduce:*

1. Stop/terminate project

*Actual result:*

Project stopping/termination fails due to parameter *conf_service_base_name*

*Expected result:*

Project stopping/termination is successful",AWS Back-end Debian,['DataLab Main'],DATALAB,Bug,Major,2019-08-20 13:41:45,13
13251597,[Keycloak]: Auto logout triggers every 6 minutes,"*[DLAB-terraform|https://github.com/apache/incubator-dlab/tree/DLAB-terraform]* branch

*Preconditions:*

1. user1 is logged in DLab

*Steps to reproduce:*
 # Wait 6 minutes after login
 # Click any action on DLab page (except for logout)

*Actual result:*

User1 is logged out

*Expected result:*

User1 is logged in DLab",AWS Back-end Debian,['DataLab Main'],DATALAB,Bug,Major,2019-08-19 14:12:05,13
13251576,[Azure]: Shared AMis for both per project and per endpoint,As a user I want to use shared AMI per project and per endpoints so that I can decrease resources (time/finance) for the next instance creation.,AZURE Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-19 12:40:44,3
13251575,[GCP]: Shared AMis for both per project and per endpoint,As a user I want to use shared AMI per project and per endpoints so that I can decrease resources (time/finance) for the next instance creation.,Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-19 12:38:34,8
13251571,[Keycloak]: Project creation is not mapped by LDAP group ( it works only by LDAP user),"*Preconditions:*
 # Environment is created  by terraform on K8S
 # User1 is from grp1 (LDAP)
 # grp1 is assigned to project1 creation
 # Project1 is in active status

*Steps to rerpoduce:*
 # Loggin by user1
 # Go to list of resource page

Actual result:

'+ Create new' button is disabled

Expected result:

'+ Create new' button is available",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-19 12:15:56,13
13251565,[Terraform branch][KeyCloak]: It is impossible to logout,"*Preconditions:*

1. User1 is logged in DLab

*Steps to reproduce:*

1. Click logout button

*Actual result:*

User1 is NOT logged out

*Expected result:*

User1 is logged out",AWS Back-end Debian pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-08-19 11:57:11,6
13251562,Add params validation to python wrapper script,"# limit SBN by 12 symbols
 # only <a-z>, <->, <1-0> are acceptable for SBN
 # limit endpoint_ID by 12 symbols",AWS Debian,['DataLab Main'],DATALAB,Task,Major,2019-08-19 11:48:10,14
13251307,[AWS]:  It is forbidden to login," 

 

*Preconditions:*
 1. SSN is created on AWS



*Steps to reproduce:*
 1. Login Dlab with user1

*Actual result:*
 1. User1/group1 is not logged in DLab



*Expected result:*
 1. User1group1 is logged in DLab

!image-2019-08-16-21-09-11-929.png!",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-08-16 18:08:22,13
13251303,Prevent service account and IAM role termination in name coincidence,"*Preconditions:*
1. SSN 'demo-project' is created
2. SSN 'ptoject-demo' is created

*Steps to reproduce:*
1. Run Jenkins job for terminatio for 'demo-project'

*Actual result:*
1. SSN 'demo-project' is terminated
2.  IAM role of 'ptoject-demo' is terminated
3. Service account of 'ptoject-demo' is terminated

*Expected result:*
1. SSN 'demo-project' is terminated

The bug was found on GCP. Investigate if it connected with the other clouds.",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-08-16 17:35:13,3
13251296,SBN should be unique during cutting,"SBN lengs limitted depends on cloud.
SBN should be unique in case of cutting.
Case:
Admin1 create SBN 'project-demo-new-feature'
Admin2 create SBN 'project-demo-dlab'
On GCP it is limitted by 12 symbols, so for admin1 and admin2 it is the same name 'project-demo'

This case was found on GCP. Also investigate if it is related to all clouds.",AWS AZURE Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Major,2019-08-16 17:19:19,3
13251235,[Billing]: Prevent error in console for Cost Detailed Dialog,See attachment,AWS Azure Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-16 11:58:51,10
13251182,Clear grid value during wrong filter,"If filter is wrong the value of grid should be cleared.
And change information error from *To start working, please, create new environment* to *No matches found*",AWS AZURE Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-16 07:35:48,10
13251003,Prevent notebook action depending on status of computational resource,"Prevent notebook action (stop/terminate) if at least one of related computational resource is in '-ing' status:
- creating
- configuring
- starting
- stopping
- terminating
- reconfiguring",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-15 11:49:37,10
13251001,Project request is absent on project page until refresh the page (F5),"*Preconditions:*
1. SSN is created

*Steps to reproduce:*
1. Create project
2. Go to another page
3. Return to project page

*Actual result:*
Project is not visible until refresh the page (F5)

*Expected result:*
Project is visible

This bug is reproduced randomly",AWS pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-08-15 11:44:00,10
13250529,Extend validation for project creation,It should be forbid to accept two symbols and more <-> <_> successively during project creation from DLab UI.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-13 12:04:53,10
13250523,[Azure]: Add 'azure_resource_group_name' parameter for project termination,"Project termination fails due to 'key error  'azure_resource_group_name'. 
Please add this parameter.",AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-13 11:50:00,13
13250490,[GCP]: Add endpoint_tag and project_tag for images,Add for notebook-primary-image and notebook-secondary-image endpoint_tag and project_tag,Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-13 09:51:57,3
13250297,[AWS][IAM role]: Endpoint_tage should be present for Project/notebook/Computational resources,Convey endpoint_tage for Project/notebook/Computational resources in IAM role,2.3_old AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-12 14:23:55,3
13250295,[AWS][IAM role]: Add user:tag for SSN/Project/Notebook/Computational resources,"In AIM role for instances  user:tag is not conveyed.
Please, convey it.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-12 14:18:45,8
13250292,[AWS]: sg_tag should be consistent,Alias 'sg' should be in lower case.,AWS Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-08-12 14:05:36,3
13250287,[AWS]: Add endpoint_tag for SG for EMR,"Security Group for EMR does not have endpoint_tag.
Please, convey endpoint_tag for EMR on AWS console.",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-12 13:43:04,12
13250286,Custom_tag is not conveyed for instances,"*Preconditions:*
1. Notebook is created with custom_tag
2. Computational resources are created

*Steps to reproduce:*
1. Go to cloud console

*Actual result:*
1. Custom-tag is absent for notebook 
2. Custom-tag is absent for computational resources

*Expected result:*
1. Custom-tag is present for notebook 
2. Custom-tag is present for computational resources",2.3_old AWS AZURE Debian DevOps GCP Known_issues(release2.4) RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-08-12 13:33:20,3
13250284,Support multi endpoint_tags to instances,"As an Admin I want to see all endpoint_tags which are assigned to a project so that I can quikly filter by endpoint_tag.

Project can contain several endpoints.
But only one endpoint_tag is conveyed on cloud console for project.
Please, convey all endpoint_tags, which are assigned to a projet.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-12 13:19:50,12
13250254,[Azure]: Notebooks are not visible on 'resources_list' page,"Preconditions:
1. Notebook is created

Steps to reproduce:
1. Go to 'resources_list' page

*Actual result:*
 Notebooks are not visible on 'resources_list' page

*Expected result:*
 Notebooks are visible on 'resources_list' page",AZURE Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-12 11:01:34,13
13250246,Custom error message overlaps check box of Spark configuration,"*Preconditions:*
1. User is located in 'resource list' page

*Steps to reproduce:*
1. Click '+ Create new button'
2. Put valid value for all fields except for Custom tag <tag->

*Actual result:*
Error message overlaps the check box of Spark configuration

*Expected result:*
Error message does not overlap the check box of Spark configuration
",AWS AZURE DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2019-08-12 10:43:14,10
13249884,[Azure]: Project status is not updated,"*Preconditions:*
SSN is created

Steps to reproduce:
1. Create a project

*Actual result:*
1. Project is created
2. Project status is not changed from 'Creating' to 'Active'

*Expected result:*
1. Project is created
2. Project status is changed from 'Creating' to 'Active'
",AZURE Debian RedHat,['DataLab Main'],DATALAB,Task,Critical,2019-08-09 12:14:05,13
13249876,[AWS]: Limit SBN length,Limit length for SBN by 12 symbols,AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-09 11:32:01,3
13249606,[Quota]: Add checking of project status before stopping,"If quota is exceeded the resources will be stopped. But a project status is not checked. And if project is stopped it tries to stop the project which is already in 'stopped' status.
Please, add checking of project status before stopping",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-08-08 08:56:55,13
13249390,Convey error message in case of project starting,"I quota is exceeded and admin starts a project the error message should appear:
'Operation can not be finished. Resource quote is reached'",AWS Debian GCP RedHat pull-request-available,[],DATALAB,Task,Major,2019-08-07 15:48:15,10
13249327,Data Engine should use notebooks's AMI during cration,"Previously Data Engine used notebook's AMI during creation. And Data Engine creation continued aproximately 10 minutes.
But now  Data Engine creation can not find AMI and its creation continues approximately 40 minutes.

So Data Engine should use notebooks's AMI during cration.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-08-07 11:05:12,3
13249134,[GCP]: Dataproc termination fails,"*Preconditions:*
Dataproc is created

*Steps to reproduce:*
1. Terminate Dataproc from DLab UI

*Actual result:*
Dataproc termination failed

*Expected result:*
Dataproc is terminated successful",Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-08-06 13:45:01,3
13249108,Data Engine template should be available for adding to GPU instances,For GPU instances only spark standalone cluster should be available for adding.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-06 12:31:20,10
13248874,Filter columns should be marked,Mark column which is filtered in case if filter dropdown is hidden,AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-08-05 12:58:20,10
13248565,Forbid instance starting/creation if PROJECT quota is exceeded and project creation if TOTAL quota is exceeded ,"User should not be allowed to create and start instances if PROJECT quota is exceeded.
Admin should not be allowed to create new project if TOTAL quota is exceeded.",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-02 11:25:50,13
13248560,Resources are not stopped after total/project quotas exceeding,"*Preconditions:*
1. Billing is available

*Steps to reproduce:*
1. Put quota per project/total less than it is currently used

*Actual result:*
1. Instances are not stopped
2. Data Engine service is not terminated

*Expected result:*
1. Instances are stopped
2. Data Engine service is terminated",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-02 11:06:23,13
13248557,[Termination confirmation dialog]: Prevent name grid if all computational resources are in failed/terminated statuses,If there are no stopped/running computational resources on notebook do not portray name grid for failed/terminated cluster in case of notebook termination on 'resources_list' and 'environment_management' pages.,AWS Debian Front-end GCP RedHat pull-request-available,[],DATALAB,Task,Minor,2019-08-02 10:35:03,10
13248552,Convey billingProjectQuotaUsed instead of billingUserQuoteUsed,"BE does not convey information about used quota per project to FE, so there are absent information:
*1.* when project quota is close to limit (equal and less 30%)
*2.* when project quota is above the limit

*Tasks:*
*1.* convey this information to FE
*2.* replace user quota by project quota",AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-02 10:18:44,13
13248544,[Quota]: Add validation and align error message,"1. Add validation for <.>, <->, <+> 
2. Align error message

Please, see attachments",AWS Debian Fron RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-08-02 09:39:34,10
13248535,Prevent 500 error if project is in 'creating' or 'failed' status,"*Steps:*
1. Project is in status 'creating' or 'failed'
2. Go to billing report page
3. open 'console' through F12",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-02 08:41:08,13
13248375,Convey list of installed libraries to UI in case of notebook creatoion from custom AMI,"*Steps:*
1. Create notebok1
2. Install libs on notebook1
3. Create custom AMI from notebook1
4. Create notebook2 from custom AMI
5. Go to library management of notebook2

If notebook is created from custom AMI previous installed libs are not visible on DLab UI, although libs are installed on notebook.",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-08-01 15:37:16,13
13248342,Computation resources creation fails,"*Preconditions:*
 # Notebook is in running status

*Steps to reproduce:*
 1. Create Data Engine or Data Engine Service

*Actual result:*
Data Engine or Data Engine Service creation fails
 
 *Expected result:*
Data Engine or Data Engine Service are created successful

 ",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-08-01 13:37:29,13
13248297,Prevent 500 error for health status checking,!500 error.png!,pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-08-01 09:46:09,10
13248294,Project public key is absent on notebooks and computational resources,"*Preconditions:*
Computationals resources are in running status

*Steps to reproduce:*
1. Go to notebook/cluster node
2. Check keystore file 

*Actual result:*
Project public key is absent 

*Expected result:*
Project public key is absent ",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-08-01 09:28:17,3
13248062,[GCP]: Dataproc creation fails,"*Preconditions:*
1. Jupyter/RStudio/Zeppelin is created

*Steps to reproduce:*
1. Create Dataproc from DLab UI

*Actual result:*
Dataproc creation fails with error:
 File ""/root/scripts/dataengine-service_configure.py"", line 147, in <module> dataproc_conf['projcet_name'],
KeyError: 'projcet_name'

*Expected result:*
Dataproc creation is successful",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Critical,2019-07-31 09:18:40,3
13247851,[Front-end]: Adjust project and endpoint tags,"Project tag should contain only the value of project name.
 Endpoint tag should contain onle the value of endpoint name.


*UI tasks:*
*1.* Rid of 'dlab-' in 'Project tag' text box
*2.* Make 'Project tag' text box not editable
*3.*  'Endpoint tag' text box should contain the value of 'Name' text box by default
4. 'Endpoint tag' should be not editable",AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-07-30 13:21:58,10
13247849,[Back-end]: Adjust product and endpoint tags,"Project tag should contain only the value of project name.
Endpoint tag should contain onle the value of endpoint name.
Project tag and Endpoint tag should be prevented from DLab UI and not convey from BE to DevOps.",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-30 13:18:43,13
13247841,[K8S]: Fix issue with Keycloak HA,"When Keycloak replica is more than 1, Keycloak isn't deployed successfully in all cases.
The issue is that Keycloak nodes trying to join the cluster after starting. Our startup script is also executed after startup. ",2.3_old DevOps,[],DATALAB,Task,Major,2019-07-30 12:04:49,3
13247794,[AWS]: Shared AMis for both per project and per endpoint,As a user I want to use shared AMI per project and per endpoints so that I can decrease resources (time/finance) for the next instance creation.,AWS Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-07-30 08:07:45,12
13247790,Any actions are blocked on project page,"*Preconditions:*
1. SSN is created
2. Admin is logged in DLab

*Steps to reproduce:*
1. Go to project page
2. Hit any available button

*Actual result:*
Any actions are blocked
 
*Expected result:*
Actions are available",AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-07-30 07:30:52,10
13247673,[Manage environment popup]: It is impossible to stop/terminate project,"*Preconditions:*
1. At least one of resource in project is in 'running' status
2. Admin is located on 'environment management' page

*Steps to reproduce:*
1. Click 'Manage environment' popup
2. Click stop or terminate action

*Actual result:*
Error 400

*Expected result:*
Stop or terminate is proceed
******************************************************************************************************
2. Convey project name on confirmation dialog
 ",AWS Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-07-29 14:42:16,10
13247671,Prevent conveying failed project on 'Manage environment' popup,Do not convey failed project on 'Manage environment' popup,AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-29 14:25:38,13
13247664,Double command execution (for now 'cd' ) causes closing web terminal or logging out from DLab,"Double execution the *cd* command causes closing web terminal or logging out in Firefox/Chrome/Opera/Microsoft Edge browsers.
Please, see the video in the attachment.",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-29 13:42:39,13
13247656, [Scheduler]: Support the same style for non active check boxes,"*Steps to reproduce:*
1. Turn on 'Scheduler by time'/'Scheduler by inactivity'
2. Turn on 'Start all spark clusters...'
3. Turn on 'In case of...'
4. Turn off 'Scheduler by time'/'Scheduler by inactivity'



",AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-07-29 13:05:30,10
13247650,Web terminal should not be opened with a part of DLab menu,"*Preconditions:*
Notebook is in 'Active status'

*Steps to reproduce:*
1. Click action menu for notebook
2. Choose 'Open terminal'

*Actual result:*
Opne terminal is opned together with a part of DLab menu

*Expected result:*
Only window of terminal is opened in new tab",AWS Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Major,2019-07-29 12:45:47,10
13247599,Jupyter creation fails due to  unrecognized arguments,"*Preconditions:*
1. Project is in 'Active' status

*Steps to reproduce:*
1. Create jupyter from DLab UI

*Actusl result:*
Jupyter creation fails (see attachment)

*Expected result:*
Jupyter creation is successful",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-29 08:26:38,12
13247340,[Notebook]: Investigate how it is allocated memory for spark,"{color:#de350b}(i) This allocation is obsolete because of task https://issues.apache.org/jira/browse/DATALAB-1985{color}

*Previously it was:*
 1. the value of spark memory is equal 75% from total (if instance shape is up to 8 GB)
 2. the value of spark.executor.memory is equal formula value: total value minus 3.5 GB (if instance shape is over than 8 GB).

*Currently it is:*
 +Case 1:+ Rstudio [4 GB]
 3.4*0.75*1024=3072MB (by theory)
 But by practical:

!image-2019-07-26-17-55-04-023.png!

 +Case2:+
 Rstudio [122 GB]
 (122-3.5)*1024= 121344
 But by practical: (by theory)

!image-2019-07-26-17-54-18-544.png!

Please, investigate how memory should be allocated",2.3_old AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-26 14:56:01,3
13247308,[GCP]: Custom spark cluster creation fails,"*Preconditions:*
1. Notebook is in 'running' status

*Steps to reproduce:*
Create spark cluster with custom parameter

*Actual result:*
Spark cluster creation fails

*Expected result:*
Spark cluster is created",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-07-26 12:01:32,3
13247287,[AWS][GCP]: Reconfiguration spark parameters for notebook/spark cluster failed,"*Preconditions:*
1. Spark cluster is in running status

*Steps to reproduce:*
1. Reconfigure spark parameter for notebook or spark cluster

*Actual result:*
Docker is executed with '1'

*Expected result:*
Docker is executed with '0'",AWS Debian DevOps GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-26 11:45:38,3
13247284,Notebook and spark cluster statuses are not updated after  spark parameter reconfiguration,"*Preconditions:*
1. Spark cluster is in running status

*Steps to reproduce:*
1. Reconfigure spark parameter for notebook or spark cluster

*Actual result:*
1. Docker is executed
2. Notebook or spark cluster status is still in 'Reconfiguring' status

*Expected result:*
1. Docker is executed
2. Notebook or spark cluster status is updated (failed or running)

In this case docker runs with '1', so instance status should be 'failed' on DLab UI.",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-26 11:33:29,13
13247073,[Billing report page]: Convey some values for billing grid,"1. Convey 'Bucket' for all buckets in 'resource type' drop down
2. Convey 'Edge Node' for all edge nodes in 'resource type' drop down
3. Convey volume for all edge node volumes in 'resource type' drop down
4. Convey shape for all edge nodes in 'shape' drop down

Please, see attachment",AWS Back-end Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Minor,2019-07-25 14:04:22,13
13247038,[Billing report page][Firefox][Microsoft Edge]: Grid lines are absent for environment name and service charges columns,"*Preconditions:*
1. Billing is available

*Steps to reproduce:*
1. Go to 'Billing report' page

*Actual result:*
Grid lines are absent only for environment name and service charges columns

*Expected result:*
Grid lines are present for all columns",AWS Debian RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-07-25 11:41:28,10
13247031,[AWS]: Add tags to AMIs,Add project and endpoint tag to AMis,AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-25 11:10:46,3
13247029,[K8S]: Change container OS of docker images for DataLab Java services,"Currently, we are using Ubuntu 16.04. Size of such image is very large.
We should change it to something like Alpine or CoreOS",DevOps,[],DATALAB,Task,Major,2019-07-25 10:58:53,9
13247027,[Administrative page]: Stop/terminate computational resources fails,"*Preconditions:*
1. Data Engine/Data Engine Service is created
2. Admin is located on 'Environment Management' page

*Steps to reproduce:*
1. Click stop or terminate icon for computational resource
2. Confirm stop or terminate

*Actual result:*
1. Appears error message 'Oops! Environment management failed!'
2. Computational resource is not stopped or terminated

*Expected result:*
1. There is no error message
2. Computational resource is stopped or terminated

Note: Data Engine admin can stop or terminate, but Data Engine Service only terminate",AWS Debian GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-07-25 10:51:55,10
13246809,[AWS][EMR]: Spot instance should be turned on by default,Check box for spot instance should be checked off and contain value of '50' by default.,AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-24 12:41:08,10
13246805,[Billing]: Align values in grid,"*1.* Actions for filter should be aligned from the left  side according to 'Service Charges'
*2.* Total sum should have tha same space from the left side as other charde values
*3.* Align vertically all values in rows.",AWS Debian RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-07-24 12:24:26,10
13246582,Fix Endpoint user-tag,Fix Endpoint user-tag.,AWS Debian,['Cloud endpoint'],DATALAB,Task,Major,2019-07-23 14:22:43,12
13246567,Add possibility to set quota more than for one project,As an Admin I want to set quota for several project (not only for one) so that I can control finance budget more than for one project.,AWS,['DataLab Main'],DATALAB,Task,Major,2019-07-23 13:23:46,13
13246565,[Group termination]: Group name should be on the same level with confirmation dialog,"*Preconditions:*
1. Admin is logged in DLab on 'roles' page
2. Group is created

*Steps to reproduce:*
1. Click terminate icon for group

*Actual result:*
Group name is not on the same level with confirmation dialog

*Expected result:*
Group name is on the same level with confirmation dialog",AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-07-23 13:19:41,10
13246564,[Roles]: Disable 'Create' button in case of invalid group name,"It is possible to add group if group name contains banned characters or already exists with the same name. 
Plase disable 'Create' button if:
1. group name contains banned characters
2. group name already exists with the same name",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-07-23 13:07:34,10
13246561,[GCP][Dataproc]: It is impossible to create dataproc,"*Preconditions:*
1. Jupyter/RStudio/Zeppelin is in running status
2. User is located on 'resources_list' page

*Steps to reproduce:*
1. Click action menu for notebook
2. Choose 'Add compute'
3. Choose Dataproc cluster in 'Select cluster type' drop down list
4. put valid values for Dataproc creation
5. Click 'Create' button

*Actual result:*
Dataproc is not created

*Expected result:*
Dataproc is created",Debian Front-end GCP pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2019-07-23 12:59:00,10
13246558,[GCP][Notebook creation]: Notebook name text box is overlapped by error message,"*Preconditions:*
1. Project is in 'Active' status
2. User1 is assigned to project1
3. User1 is on 'resources_list' page

*Steps to reproduce:*
1. Click '+ Create new' button
2. Put not valid value in 'name' text box <@>

*Actual result:*
Notebook name text box is overlapped by error message

*Expected result:*
Error message is between 'Name' and 'Instance type' text boxes",Debian Front-end GCP,['DataLab Main'],DATALAB,Bug,Minor,2019-07-23 12:44:53,10
13246518,[Project termination]: Convey resource list wich be deleted on confirmation dialog,On confirmation dialog for project termination convey list of resources which will be terminated as well,AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-07-23 08:40:22,5
13246339,Add filter error message ,"If filter fields do not match, please add error message: 'No matches found' on ",AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-22 14:52:05,10
13246335,[EMR creation]: Disable 'create' button if mandatory fields are not filled,"If 'cluster alias', 'Total instance number', 'Master instance shape' are filled the 'Create' button becomes enable. But in this case this button should be disable, because 'Slave instance shape' is not filled.
Please, disable 'Create' button for EMR creation if not all of the following text boxes are filled:
1. Select cluster type
2. Select template
3. Cluster alias
4. Total instance number
5. Master instance shape
5. Slave instance shape",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-22 14:47:16,10
13246290,[GCP]: Project deactivation does not trigger,"*Preconditions:*
1. Project is created

*Steps to reproduce:*
1. Stop project from DLab UI

*Actual result:*
1. Project remains in 'Deactivating' status
2. Project is NOT stop

*Expected result:*
1. Project changes his status from 'Active' to 'Not Active'",Debian DevOps GCP,['DataLab Main'],DATALAB,Bug,Major,2019-07-22 10:39:11,3
13246281,Url endpoints mapping,As an admin I want to use url of previously endpoint creation during project creation.,AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-22 09:30:49,13
13246262,Related instance statuses are not updated on DLAB UI if project is deleted,"*Preconditions:*
1. Clusters and notebooks are created in project1

*Steps to reproduce:*
1. Delete project1

*Actual result:*
1. Project are deleted both on AWS console and on DLab UI
2. Notebook and clusters are deleted onle on AWS console
3. Notebook and clusters are not deleted on  DLab UI
4. Docker for project termination executes with '0'

*Expected result:*
1. Project are deleted both on AWS console and on DLab UI
2. Notebook and clusters are deleted onle on AWS console
3. Notebook and clusters are deleted on  DLab UI
4. Docker for project termination executes with '0'
",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-22 08:48:35,13
13246254,Create Cloud endpoint on GCP,Create Cloud endpoint on GCP using Terraform templates.,Debian GCP,['Cloud endpoint'],DATALAB,Task,Major,2019-07-22 08:09:46,3
13245971,[Demo-dlab-legion]: Investigate why not all of tempales are visible for notebook creation,"Not all of templates are visible for notebook after edge creation until rebuild dockers and restart services.

Please investigate this case, and fix it.",AWS Debian RedHat,['DataLab Old'],DATALAB,Task,Major,2019-07-19 10:57:00,8
13245966,Prevent actions for project depending on notebook/cluster status,If notebooks or cluster are in status which ends by 'ing' do not allow to stop a project.,AWS,['DataLab Main'],DATALAB,Task,Major,2019-07-19 10:38:00,10
13245965,[List of resources]: Reconcile grid name and  drop down list value at one horizontal level,"Please, align text from the left side in drop down list or change some style in name grid, because  visually it looks like that grid name and drop down list are not at the same horizontal level.",AWS Front-end GCP,['DataLab Main'],DATALAB,Task,Minor,2019-07-19 10:34:19,10
13245959,[Endpoints]: Alter button and popup names for endpoints,"Rename 'Create endpoint' to 'Connect to endpoint'.
Rename 'Create' to 'Connect' button",AWS Debian GCP RedHat pull-request-available,['Cloud endpoint'],DATALAB,Task,Minor,2019-07-19 09:58:00,10
13245740,[GCP]: Project status is not updated after creation,"*Preconditions:*
1. SSN is creted on GCP

*Steps to reproduce:*
1. Create project

*Actual result:*
1. Project is created on GCP console
2. Project creation docker executes with '0'
3. Project status is 'Creating' on DLab UI

*Expected result:*
1. Project is created on GCP console
2. Project creation docker executes with '0'
3. Project status is 'Active' on DLab UI",Back-end Debian GCP,['DataLab Main'],DATALAB,Bug,Critical,2019-07-18 10:28:07,13
13245735,'$anyuser' group does not trigger for project,"*Preconditions:*
1. '$anyuser' group  is assigned to a project1
2. Project1 is created
3. Project1 is in 'Active' status

*Steps to reproduce:*
1. Go to 'resources_list' page

*Actual result:*
1. '+ Create new' button is disabled
2. User is not allowed to create notebook

*Expected result:*
1. '+ Create new' button is unabled
2. User can create notebook
",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-18 10:17:54,13
13245726,User is not allowed to create any notebook if administration operations are turned off,"*Preconditions:*
1. user1 i assigned to a project
2. user1 is not supposed to execute administration operations
3. user1 is located on 'resources_list' page

*Steps to reproduce:*
1. Click '+ Create new' button

*Actual result:*
User1 is logged out
User1 is NOT able to create notebook

*Expected result:*
User1 is logged in DLab
User1 is able to create notebook",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-18 10:08:06,13
13245723,Sometimes template list for notebook is empty,"The bug is reprodued not always.

*Preconditions:*
1. The env is stopped

*Steps to reproduce:*
1. Start SSN
2. Start project
3. Go to resource list
4. Click '+Create new' button
5. Select project in 'Select project' drop down list
6. Select endpoint in Select endpoint drop down list

*Actual result:*
Select template is disabled

*Expected result:*
Select template is enabled
",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-07-18 09:50:33,13
13245437,[GCP]: Look into performance of SSN deploying,"Previously SSN have been deployed approximately during 30 minutes on GCP, but now it takes  an hour.
Please, investigate  the reason of time increasing and fix it.",Debian DevOps GCP,['DataLab Main'],DATALAB,Task,Minor,2019-07-17 10:14:52,3
13245409,Change python version,Change python version of provisioning-script from 2.7 to 3.7.,AWS Debian Endpoint,['Cloud endpoint'],DATALAB,Task,Major,2019-07-17 09:03:27,12
13245172,Investigate why project/notebook/cluster creation fails from time to time,"Sometimes project/notebook/cluster creation fails on process of python package installation (see attachment)

The same error was in previous DLab version 2.1.1 (without project) during Edge creation",AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-16 10:02:07,12
13245000,Project status is not updated after activation,"*Preconditions:*
1. Project is in 'Not Active' (stopped) status

*Steps to reproduce:*
1. Activate project

Actual result:
1. Docker run with '0'
2. Instance project is running on AWS console
3. Project is in 'Activating' status on DLab UI

*Expected result:*
1. Docker run with '0'
2. Instance project is running on AWS console
3. Project is in 'Active' status on DLab UI",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-15 14:58:31,13
13244997,It is impossible to edit project if project is in 'Not Active' status,"*Preconditions:*
Project is in 'Not Active' status

*Steps to reproduce:*
1. Go to 'Project' page
2. Click project edit icon

*Actual result:*
Project editing popup is not open

*Expected result:*
Project editing popup is open",AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-15 14:46:32,10
13244994,[Environment management]: Resources are not visible if project status is 'Not Active',"*Preconditions:*
1. Notebooks are created in project1

*Steps to reproduce:*
1. Stop project1
2. Go to 'Environment management' page

*Actual result:*
Notebooks from project1 is not visible

*Expected result:*
Notebooks from project1 is visible",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-15 14:38:29,13
13244988,[AWS]: EMR configuration fails,"The bug was found in AWS, on Jupyter, but it sounds that the defect is actual for all notebooks.

*Preconditions:*
1. Notebook is created

*Steps to reproduce:*
1. Create EMR on notebook

*Actual result:*
EMR configuration fails

*Expected result:*
EMR creation is successful",AWS Debian DevOps RedHat,[],DATALAB,Bug,Critical,2019-07-15 14:10:23,3
13244984,[AWS]: Spark cluster configuration fails,"The bug was found in AWS, on Jupyter, but it sounds that the defect is actual for all notebooks.

*Preconditions:*
1. Notebook is created

*Steps to reproduce:*
1. Create spark standalone cluster on notebook

*Actual result:*
Spark cluster configuration fails 

*Expected result:*
Spark cluster creation is successful",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Critical,2019-07-15 14:01:03,3
13244905,[AWS user identification]: Change error message,"If user has not an AWS account and awsUserIdentificationEnabled is true, change error massage from 'Username or password are not valid' to 'Please contact AWS administrator to create corresponding IAM User'",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-15 09:02:04,13
13244137,[Resorce list]: Rebuild exploratory filtering according to new data format,As a user I want to filter notebook on 'Resource list' page,AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-07-10 15:40:17,10
13244127,Notebook creation from custom image fails,"*Preconditions:*
1. Custom image is created for notebook

*Steps to reproduce:*
1. Create notebook from custom image

*Actual result:*
Notebook creation fails

*Expected result:*
Notebook creation is created succesful",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-10 14:58:05,3
13244122,Docker of resource status executes with '1',"*Preconditions:*
1. Notebook is created

*Steps to reproduce:*
1. Go to SSN via ssh

*Actual result:*
Docker of resource status is executed with '1' (view 'Available lib list.png' in attachment)

*Expected result:*
Docker of resource status is executed with '0'",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-10 14:45:10,3
13243882,EMR creation fails on process of uploading user key,"*Preconditions:*
1. Jupyter/rsudio/zeppelin is created

*Steps to reproduce:*
1. Create EMR with valid data

Actual result:
EMR creation fails on process of uploading user key


*Expected result:*
EMR creation is successful",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-09 12:21:11,3
13243602,[AWS]: User can not create Data Engine Service,"*Preconditions:*
1. Notebook (Zeppelin/Rstudio or Jupyter) is created
2. EMR popup creation is open

*Steps to reproduce:*
1. Put valid data for EMR creation
2. Click 'Create' button

*Actual result:*
EMR creation popup is not closed
EMR is not  created

*Expected Result:*
EMR creation popup is closed
EMR is created",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-08 08:31:15,13
13243358,Remove extra scrollbar from spark cluster name popup,"*Preconditions:*
1. Spark is created and in running status
2. Spark cluster popup is open
",AWS Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-05 12:20:01,10
13243351,Convey project name  to BE in case of spark cluster starting,Start cluster does not start without value of project name.,AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-05 11:54:36,10
13243342,Adding/removing group from project should not change project status,"*Preconditions:*
1. Project is created
2. Admin is located on 'project' page

*Steps to reproduce:*
1. Click edit action for project
2. Add or remove group
3. Click update

*Actual result:*
Project status changes from 'Active' to 'Creating'

*Expected result:*
Project status does not changed",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-05 11:24:48,13
13243332,Align action icons for projects,"*Pregonditions:*
1. Several projects are created
2. At least one project is failed

Project actions should be aligned in the same column.",AWS AZURE Debian Front-end GCP RedHat,[],DATALAB,Task,Minor,2019-07-05 10:30:41,10
13243322,[Instance creation]: Instance shapes should be in ascending order for notebook and cluster in dropdown list,"Sort  instance shapes in ascending order of size in the dropdown list.
Look in attachment in order to find out the size.",AWS Debian Front-end RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-07-05 10:08:21,5
13243215,Investigate if shared AMI is used per project,"There is  'No pre-configured image found. Using default one: ami-08692d171e3cf02d6' in logs if create the same type of notebook (it is not the first creation) even for the same user.
Please investigate if shared AMi is used.",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-07-04 13:05:09,12
13243200,Scheduler for instances does not trigger,"*Preconditions:*
1. Notebook/cluster is created

*Steps to reproduce:*
1. Set valid data for notebook/cluster scheduler

*Actual result:*
Scheduler does not trigger

*Expected result:*
Scheduler triggers, notebook/cluster is stopped or running according to setting",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-04 12:11:24,13
13243192,Alter the message for project creation,"Change message *from* 'Success! 'Project created successfully!' *to* 'Project creation is processing!'
",AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-04 11:36:24,10
13243178,Remove extra scrollbar from time scheduler,"Please, remove horizontal scrollbar on time scheduler.",AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Minor,2019-07-04 10:50:57,10
13243156,Prevent actions for notebook depending on project status,"If project is in active status (activated) allow user all action for his notebook.
 If project is in NOT active (creating, failed, deleted, deleting, deactivating, deactivated) status allow user 'Manage libraries' action ???",AWS,['DataLab Main'],DATALAB,Task,Major,2019-07-04 08:39:14,10
13243146,[Front-end]: Notebook name should be unique per user,"*1.* Add check for unique *notebook* name per user.
 If notebook name is not unique add *error message*: 'This name already exists.'

*2.* Convey error message for *Custom tag* on 'Create analytical tool' popup: 'Custom tag can only contain letters, numbers, hyphens and '_' but can not end with special characters'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-07-04 07:52:07,10
13243145,[Back-end]: Notebook name should be unique per user,"Add check for unique notebook name per user.
If notebook name is not unique add error message: 'This name already exists.'",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-04 07:50:01,13
13243018,[Front-end]: Convey shared and shared_project buckets,"As a user I want to see shared and shared_project buckets on DLab UI, so that I can collaborate with other users.

*Acceptance criteria:*

1. There is 'Shared bucket' end its value on notebook name popup
2. There is 'Shared project bucket' end its value on notebook name popup",AWS Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-07-03 14:31:03,10
13243016,[Back-end]: Convey shared and shared_project buckets,"As a user I want to see shared and shared_project buckets on DLab UI, so that I can collaborate with other users.

*Acceptance criteria:*

1. Convey to FE the value of shared bucket
2. Convey to FE the value of shared_project buckets",AWS Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-03 14:26:11,13
13243008,User is not allowed to create computational resources,"*Preconditions:*
1. Notebook is created for user1
2. User1 has permission for computational resources creation
3. User1 is located on 'Resources list' page

*Steps to reproduce:*
1. Click action menu for neotebook
2. Choose 'Add compute'

*Actual result:*
1. Message appears : 'Computational resource creations are not available.
Please, check your permissions.'
2. It is not allowed to create computational resources

*Expected result:*
It is allowed to create computational resources",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-03 14:08:05,13
13242981,Obtaining of resource status/available lib list and custom image creation fails,"*Preconditions:*
1. Notebook is created

*Steps to reproduce:*
1. Go to SSN via ssh

*Actual result:*
Docker of  resource status is executed with '1' (view 'Available lib list.png' in attachment)
*Expected result:*
Docker of  resource status is executed with '0'
******************************************************************************************************************
The same error appears if get available lib list (view 'Resource status.png' in attachment) and custom image creation (view 'Custom image.png' in attachment)",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-03 11:48:14,13
13242954,Start/stop notebook is not successful,"*Preconditions:*
1. Notebook is created in running status

*Steps to reproduce:*
1. Stop a notebook

*Actual result:*
1. Notebook is in 'stopping' status on DLab UI
2. Notebook is in 'running' status on cloude console
3. Docker is executed with '1' status

*Expected result:*
1. Notebook is stopped on Dlab UI and cloude console
2. Notebook is stopped on cloude console
3. Docker is executed with '0' status",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-07-03 09:39:27,13
13242742,[Front-end]: Add activate/deactivate actions per project,"As an admin I want to activate and deactivate so that I can thrift financial resources.
Allow admin to edit project if project is in 'Not active' status.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-02 12:57:41,10
13242736,[Back-end]: Add activate/deactivate actions per project,As an admin I want to activate and start deactivate so that I can thrift financial resources.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-02 12:39:02,13
13242450,[Project status]: Convey project status on DLab UI,"Convey project status on 'projects' page:

CREATING,
ACTIVE,
FAILED,
DELETED,
DELETING,
DEACTIVATING,
ACTIVATING,
NOT_ACTIVE",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-01 08:59:54,10
13242447,[Project status]: Convey project status to FE,"Convey project status:

CREATING,
ACTIVE,
FAILED,
DELETED,
DELETING,
DEACTIVATING,
ACTIVATING,
NOT_ACTIVE",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-07-01 08:56:01,13
13242444,Limit quantities of characters for project name,"Accord to 'IAM Entity Character Limits' role name cannot exceed 64 characters [https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-limits.html]
 currently it is limited by 50 characters, but it is not enough.
 Please, limit depending on cloud:
 1. 40 for AWS
 2. 30 for Azure
 3. 10 for GCP

So:
 1. PROJECT name cannot exceed *XX** *characters*
 2. If project name exceed *XX** *characters* add error message: 'Project name cannot exceed XX* characters.'

*Note: numbers of characters depends on cloud.

Please, see 'Cloud limitation' in attachments.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-07-01 08:49:34,10
13241979,[Project creation]: 'Clear' button does not clear label of previous uploading key,"*Preconditions:*
1. Admin is logged in DLab
2. At least one endpoint is created
3. 'Create new project' popup is open

*Steps to reproduce:*
1. Click 'Upload' button
2. Chose key.pub
3. Click 'Open' button
4. Click 'Clear' button

*Actual result:*
Clear button does not clear  key.pub value
Please, view 'Clear button.png' in attachment

*Expected result:*
Clear button clears  key.pub value

",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-06-27 15:54:47,10
13241975,Expand validation for group name,"*Preconditions:*
1. Admin is located on 'Roles' page
2. At least one endpoint is created

*Tasks:*
1. Group name should be case insensitive (view 'Case insensitive.png' in attachments). Add error message: 'This group name already exists.'
2. Cut group name on confirmation dialog for termination (view 'Group name termination.png' in attachments)
3. Scrollbar should have the same style  (view 'Scrollbar.png' in attachments)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-27 15:35:19,10
13241514,[Front-end]: Allow to add endpoint to existing project,"It is impossible to add new endpoint to existing project.

Please implement possibility ONLY to add (not remove) endpoint to existing project.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-06-25 15:18:11,10
13241512,[Back-end]: Allow  to add endpoint to existing project,"It is impossible to add new endpoint to existing project.

Please implement possibility ONLY to add (not remove) endpoint to existing project.",AWS AZURE BackSlash Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-25 15:16:18,13
13241508,[Billing][Environment management]: Do not convey grid if any resources are not created,"Remove grid if any resources are not created on 'Billing' and 'Environment Management' pages


*************************************************************************************************************************************************************************

Add error message for *project tag:*

'Project tag can only contain letters, numbers, hyphens and '_' but can not end with special characters'",AWS AZURE Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-06-25 15:04:34,10
13241492,[Project Edge Node]: Replace 'key upload' component during the first  project creation,"*Preconditions:*
1. Admin is logged in DLab
2. Any project is not created
3. Go to 'Project' page

Task:
1.  On stage of key upload replace component  by the component which is on 'Create new project' popup

",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-25 14:25:54,10
13241484,Extra scrollbar on 'Create new project' popup,"*Preconditions:*
At leats one project is created

*Steps to reproduce:*
1. Click '+ create new' button

*Actual result:*
There is extra vertical scrollbar

*Expected result:*
There is no extra vertical scrollbar

",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Bug,Minor,2019-06-25 13:43:00,10
13241474,Adjust validation for endpoint url,"Endpoint url text box should not eccept:
- not English letters
- space 

Error message should appear if user puts not allowed symbols:
'Endpoint url can only contain English letters, numbers, characters and can not contain space'",AWS AZURE Debian Front-end GCP RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-06-25 13:18:24,10
13241457,[Admin]: Investigate why sometimes administration page is denied,"This case is reproduced random.
Sometimes after project creation.
Please investigate this case and fix it.",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-06-25 11:34:02,13
13241438,Forbid to choose project for user who is not assigned to it,"*Preconditions:*
1. User is logged in DLab
2. There is at leats one project which is not assigned to current user

*Steps to reproduce:*
1. Go to 'Resources list' page
2. Click ' + Create new' button
3. Click 'Select project' drop down list

*Actual result:*
User can choose project which are NOT assigned to him

*Expected result:*
User can choose project which are ONLY assigned to him

",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-06-25 10:04:47,13
13239244,[Front-end]: Ability to set/update project quota,"As an Admin I want to set/update limit per project and general limit, so that I can prevent exceeding determined budget.

*Preconditions:*
 Admin is logged-in DLab
 Admin is located in ‘Manage environment’ popup tab on ‘Administration’ page

*Description:*
 Admin is able to set/udate limit for:
 Project in text box for project quota 
 DLab in text box for DLab for total quota
 Textbox accept only numbers and only integer. 
 If Admin clicks ‘Apply’ button the quota is confirmed and ‘Manage environment’ popup is closed.
 If Admin clicks ‘Cancel’ button the quota is not set/update and ‘Manage environment’ popup is closed.

*Acceptance criteria:*
 1. Admin is able to set/update quota per project
 2. Admin is able to set/update total Dlab quota
 3. DLab TOTAL quota prevails (P1)
 4. Project quota has second priority (P2). Such that Pr1 + Pr2 <= TOTAL Dlab",Debian GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-06-13 10:42:53,10
13239048,[Billing report][Resources list]: Add possibility to filter  by tag ,"Clarify what kind of types should we filter:
- DLab /SBN
- endpoint
- project
- user
- custom?
",AWS Azure Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-12 14:18:58,10
13238851,"[User tag]: Once Notebooks, Clusters are created - pass User tag to DevOps","* *_user_tag:_*

o    _Autogenerated_

o    _format: [user_name@domain.com]_

o    _assigned to notebooks, clusters, including Service data engines_

 
 # _Admin created Endpoint1 with *endpoint_tag:Endpoint1*_
 # _All resources within Endpoint (notebook, cluster) obtain*endpoint_tag:Endpoint1*_
 # _Admin created Project1 with *project_tag:Project1* and assign Endpoint1 to that project_
 # _All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project1*_
 # _User1 created notebook within Project1 with custom*custom_tag:Custom1*_
 # _That notebook obtains: *endpoint_tag:Endpoint1, project_tag:Project1, user_tag:[User_Name1@domain.com|mailto:User_Name1@domain.com], custom_tag:custom1*_
 # _Admin created Project 2 with *project_tag:Project2* and assign Endpoint1 to that project_
 # _All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project2*_
 # _User1 creates notebook within Project2 with custom*custom_tag:Custom2*_
 # _That notebook obtains: *endpoint_tag:Endpoint1, project_tag:Project2, user_tag:[User_Name1@domain.com|mailto:User_Name1@domain.com], custom_tag:custom2*_",AWS Back-end Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-11 19:33:06,13
13238575,[Cloud endpoint tag]: Once endpoint is created - pass endpoint_tag to DevOps,"  *Endpoint_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

 

Admin created Endpoint1 with *endpoint_tag:Endpoint1:*
 - All resources within Endpoint (notebook, cluster) obtain *endpoint_tag:Endpoint1.*",AWS Debian RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-06-10 16:39:25,13
13238564,[Custom tag]: Add ability to add custom tag during notebook creation on Web UI,"# Ability to add Custom tag from Notebook creation popup
 # Once added, on list of resources screen - user should see additional column ""Custom tag""",AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-10 16:08:13,10
13238555,"[Cloud endpoint tag]: Once endpoint_tag is gained - tag all notebooks, clusters with endpoint_tag","  *Endpoint_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

 

Admin created Endpoint1 with *endpoint_tag:Endpoint1:*
 - All resources within Endpoint (notebook, cluster) obtain *endpoint_tag:Endpoint1.*",AWS Debian RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-06-10 15:42:27,8
13238552,[Project tag]: Once new Endpoint is associated with the project - assign project tag to newly associated Endpoint,"*Project_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

project tag can not be changed once project got created

 

Instances are created per project. Add project_tag to all instances which are created in this project.

So that once project is created - tag all project infrastructure with project tag.

 

Example:

Admin created Project1 with *project_tag:Project1* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project1.***

Admin created Project 2 with *project_tag:Project2* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project2.*

 ",AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-10 15:37:06,13
13238550,[Cloud endpoint tag]: Add ability to define Cloud endpoint tag on Web UI,"  *Endpoint_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

 

Admin created Endpoint1 with *endpoint_tag:Endpoint1:*
 - All resources within Endpoint (notebook, cluster) obtain *endpoint_tag:Endpoint1.*",AWS Debian RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-06-10 15:30:28,10
13238549,[Project tag]: Once Cloud Endpoint is associated with the project - assign project tag to Cloud Endpoint,"*Project_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

project tag can not be changed once project got created

 

Instances are created per project. Add project_tag to all instances which are created in this project.

So that once project is created - tag all project infrastructure with project tag.

 

Example:

Admin created Project1 with *project_tag:Project1* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project1.***

Admin created Project 2 with *project_tag:Project2* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project2.*

 ",AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-06-10 15:25:58,12
13238545,[Project tag]: Add ability to define project tag on Web UI,"# Project tag - required field
 # Project tag should be defined by admin on Web UI
 # Project tag, by default should the same as Project name, but admin can change default value
 # Project tag can not be changed after project got created",AWS Debian Front-end RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-10 14:58:12,10
13238544,[Project tag]: Once Project is created - pass project tag to DevOps,"Once Project is created - pass project tag to DevOps.

 

*Project_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

project tag can not be changed once project got created

 

Instances are created per project. Add project_tag to all instances which are created in this project.

So that once project is created - tag all project infrastructure with project tag.

 

Example:

Admin created Project1 with *project_tag:Project1* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project1.***

Admin created Project 2 with *project_tag:Project2* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project2.*

 ",AWS Debian RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-10 14:56:13,13
13238511,Cloud Endpoint API,Need to be decupelled,AWS Debian RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-06-10 12:00:39,15
13237954,[Project tag]: Once Project is created - tag all project infrastructure with project tag,"*Project_tag:*

o    manual input from Web UI

o    assigned to notebooks, clusters, including Service data engines

project tag can not be changed once project got created

 

Instances are created per project. Add project_tag to all instances which are created in this project.

So that once project is created - tag all project infrastructure with project tag.

 

Example:

Admin created Project1 with *project_tag:Project1* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project1.***

Admin created Project 2 with *project_tag:Project2* and assign Endpoint1 to that project
 - All resources within Endpoint (notebook, cluster) obtain additional tag *project_tag:Project2.*

 ",AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-06 12:47:41,3
13237701,Define API for endpoint,Please define data contract between BE and Cloud Endpoint using Open API 3.0 spec.,AWS AZURE Back-end Debian GCP RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-06-05 10:28:04,13
13237695,[AWS]: Fix billing,Investigate how to billing is working (in terms of integration with athena),AWS Back-end Debian RedHat,['DataLab Old'],DATALAB,Task,Major,2019-06-05 10:16:22,13
13237688,Update documentation,"As a user I want to have clear vision how I Should DLab deploy.

Update readme.
Divide cloud related documentations.",DevOps pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-06-05 09:34:54,3
13237674,[Project Edge Node]: DevOps,"As as user I want to create instance when I am assigned on a project.

Now subnet, edge are created per user during edge creation. 
And user can create notebook only after his edge node creation.
Remove edge node per user and add posibility to create notebook if user is assigned at least per one project.
Subnet, and edge node, and shared_project_bucket should be created per project.
Shared_project_bucket is shared within this project
Shared_bucket is created on step of SSN creation. Shared_bucket is shared between the projects.",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-06-05 08:18:26,3
13237505,Introduce Edge node per project,"As as user I want to create instance when I am assigned on a project. 

Now subnet, edge are created per user during edge creation. 
And user can create notebook only after his edge node creation.
Remove edge node  per user and add posibility to create notebook if user is assigned at least per one project.
Subnet, and edge node, and shared_project_bucket should be created per project.
Shared_project_bucket is shared within this project
Shared_bucket is created on step of SSN creation. Shared_bucket is shared between the projects.",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-06-04 14:05:19,13
13237434,Roles are not synched up with users or groups,"*Preconditions:*
 # grp1 is allowed to to do administration operation
 # '$anyuser' group is deleted

*Steps to reproduce:*

1. Login by user from grp1

*Actual result:*

it is forbidden for user to do adminiastration operations

*Expected result:*

It is allowed for user to do adminiastration operations",AWS AZURE Back-end Debian GCP RedHat,['DataLab Main'],DATALAB,Bug,Major,2019-06-04 09:07:57,13
13236884,[Project]: Rename label from 'Users' to 'Groups' on 'Project' page,"*Preconditions:*
 # Admin is logged in DLab
 # Admin is located on 'Project' page

*Steps to reproduce:*
 # Click 'Next' on step1

*Actual result:*
 # Label 'Users' is on step1/2

*Expected result:*

       1. Label 'Groups' is on step1/2",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-05-31 14:57:21,10
13236882,[Projects]: Reuse library mngmt component for endpoints and groups,"1. Add for endpoints and groups in grid the same component which we have for library

2. Cut too long values

3. Add scrollbar in grid in case if there are more values

4. Cut project name on confirmation dialog for termination (view video in attachments)

5. Add error message for project name text box: 'Project name can only contain letters, numbers, hyphens and '_' but can not end with special characters'

6. 'Select user groups' hint is cut in the down (view 'Select user groups.png' in the attachments)

7. Project name should be case insensitive (view 'Project case insensitive.png' in attachments)",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-05-31 14:51:25,10
13236879,[Project][Role][Resource list]: List of items should have the same length as dropdown list,"*Preconditions:*
 # Admin is logged in DLab
 # Admin is located on 'Project' page

*Steps to reproduce:*
 # Click 'Select endpoint' drop down list

*Actual result:*
 # List of items has smaller length than the length of dropdown list (on step1 and step2)

*Expected result:*

1. List of items has the same length as dropdown list (on step1 and step2)

 

+Note: The same case is with dropdown list on 'Roles' page/'Create analitical tool' popup and should be *on the same level*.+",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Minor,2019-05-31 14:43:58,5
13236852,Endpoint UI issues,"*Preconditions:*
 # Admin is logged in DLab
 # Admin is located in 'Create Endpoint' popup on 'Environment management' page

*Steps to reproduce:*
 # Go to 'endpoint list' tab

*Actual result:*
 # 'Cancel' button is cut on  'Create Endpoint' popup (view 'Cancel is cut.png' in attachments)

*Expected result:*
 # 'Cancel' button is NOT cut  'Create Endpoint' popup

_________________________________________________________________________

 

*Preconditions:*
 # Admin is logged in DLab
 # Admin is located in 'Create Endpoint' popup on 'Environment management' page
 # At least one endpoint is created wit too long values

*Steps to reproduce:*
 # Go to 'endpoint list' tab

*Actual result:*
 # Endpoint values (name/url/account) are not cut (view 'Endpoint Cut Values.png', 'Endpoint Cut Values2.png' in attachments)

*Expected result:*
 # Endpoint values (name/url/account) are cut

_________________________________________________________________________

 

*Preconditions:*
 # Admin is logged in DLab
 # Admin is located in 'Create Endpoint' popup on 'Environment management' page
 # At least one endpoint is created wit too long values

*Steps to reproduce:*
 # Go to 'endpoint list' tab
 # Click terminate icon for endpoint with too long name

*Actual result:*
 # Endpoint name is not cut on confirmation dialog (view 'Too long name on confirmation dialog.png' in attachments)

*Expected result:*
 # Endpoint name should be cut on confirmation dialog

___________________________________________________________________

 

*Preconditions:*
 # Admin is logged in DLab
 # Admin is located in 'Create Endpoint' popup on 'Environment management' page

*Steps to reproduce:*
 # Put 'test-' in name text box
 # Put 'url_' in url text box
 # Put '111-' in account text box

*Actual result:*
 # 'Create' button is enabled (view 'Endpoint validation.png' in attachment)
 # User can create endpoint with such values (view 'Too long name on confirmation dialog.png' in attachments)

*Expected result:*
 # 'Create' button is disabled
 # User cannot create endpoint with such values 
 # Error message appears: 
3.1 'Endpoint name can only contain letters, numbers, hyphens and '_' but can not end with special characters' for name text box
3.2. 'Endpoint url can only contain letters, numbers, hyphens and '_' but can not end with special characters' url text box
3.3. 'Endpoint account can only contain letters, numbers, hyphens and '_' but can not end with special characters' account text box",AWS AZURE Debian Front-end GCP RedHat,['Cloud endpoint'],DATALAB,Bug,Major,2019-05-31 13:28:32,10
13236830,Add validation for endpoint,"# Endpoint name should be case insensitive (view 'Endpoint Case insensitive.png' in attachments)
 # Endpoint url text box should accept characters: 
 2.1. reserved  !*'();:@&=+$,/?#[]
 2.2. unreserved A-Za-z0-9_.~-%
 # Endpoint url text box should NOT accept:
 3.1. space
 3.2. not Eglish letters
 # Account text box should accept only A-Za-z0-9",AWS AZURE Back-end Debian Front-end GCP RedHat,['Cloud endpoint'],DATALAB,Task,Major,2019-05-31 12:26:50,13
13234687,Prevent 'Open terminal 'action for not in running instance status,"'Open terminal' action should be enabled only for notebook which is in running status.

Please, prevent 'Open terminal' action for instances, which are in statuses:
 * creating
 * configuring
 * stopping
 * stopped
 * starting
 * terminating
 * terminated",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Task,Major,2019-05-21 18:03:30,10
13234644,Convey IP notebook in separate field,Back-end should convey  to Front-end an IP notebook in separate field.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-05-21 15:12:50,13
13234577,Project and Endpoint API documentation,"DLab endpoint and project API should be documented using OpenAPI 3.0 spec:

[https://github.com/OAI/OpenAPI-Specification]

 ",Back-end Debian GCP,['DataLab Main'],DATALAB,Task,Major,2019-05-21 11:07:00,13
13234480,Implement terraform infrastructure,"* Create Dlab terraform architecture
 * Implement terraform state storage
 * Implement terraform scripts versioning
 * Implement terraform execution job",AWS Debian RedHat,['DataLab Old'],DATALAB,Task,Major,2019-05-21 05:31:16,15
13234470,[Azure]: Implement terraform templates for notebooks,Implement creation of exploratory envinroment and computational resources with deployment performed by terraform.,AZURE Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-05-21 05:21:30,8
13234469,[GCP]: Implement terraform templates for Notebooks,Implement creation of exploratory envinroment and computational resources with deployment performed by terraform.,Debian DevOps GCP pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-05-21 05:20:20,8
13234467,[AWS]: Implement terraform templates for notebooks,Implement creation of exploratory envinroment and computational resources with deployment performed by terraform.,AWS Debian DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Task,Major,2019-05-21 05:18:23,8
13234297,[Project selector]: Introduce project selector on Notebook creation popup,"As a User I want to create analytical tools so that I can use and manage them for analytical calculations.

 

*Preconditions:*
 # User is logged-in DLab
 # User is located in ‘List of Resources’ page

 

*Description:*

If User clicks ‘+ Create new’ button ‘Analytical tool’ popup appears which consists of:
 * ‘Select template’ - drop down list
 * ‘Name’ - textbox it should be unique within endpoint and user
 * ‘Instance shape’ (AWS)/‘Virtual machine size’ (Azure) drop down list
 * ‘Project’ - drop down list
 * ‘Endpoint’ - drop down list
 * two buttons ‘Cancel’ and ‘Create’

All fields are mandatory.

If User clicks ‘Create’ button the analytical tool is creating and ‘Analytical tool’ popup is closed.

If User clicks ‘Cancel’ button the analytical tool is not created and ‘Analytical tool’ popup is closed.

 

*Acceptance criteria:*
 # User is able to create analytical tool
 # User is able to choose in what project analytical tool will be created
 # During the first notebook template creation shared AMI is created. Shared AMI is per Endpoint and per Cloud
 # User can create notebook from custom AMI. Custom AMI is per user and per Cloud
 # It is not allowed for  other user to access to notebooks within same project on identity provider level - [(v1) - SHORT TERM]
 # Users works under his own identity. All actions are logged from that username *[(v2) - LONG TERM]*",Debian GCP,['DataLab Main'],DATALAB,Task,Major,2019-05-20 12:51:14,10
13234293,"[Back-end]: Disable Add New button for a user, who is not assigned to any project","As a User I want to be informed why I am not allowed to create analytical tool, so that I can ask the appropriate person to give me opportunity for creation.

 

*Preconditions:*
 # User is logged-in DLab
 # User is located in ‘List of Resources’ page

 

*Description:*

If user is not assigned to any project for him it is forbidden to create analytical tool. ‘+ Create new ’ button is disabled.

If user goes to ‘List of Resources’ page error message appears ‘You are not assigned to any project. Please contact DLab administrator to add you to a project’

 

*Acceptance criteria:*
 # User is  not allowed to create analytical tool if he is not assigned at least to one project
 # Error message appears if User goes ‘List of Resources’ page",Debian GCP,['DataLab Main'],DATALAB,Task,Major,2019-05-20 12:42:09,13
13234275, [Admin]: Extension of Administration page,"As an Admin I want to have all necessary instruments so that I can  do administrative operations.

 

*Preconditions:*
 # Admin is logged-in DLab
 # Admin is located on ‘Administration’ page

 

*Acceptance criteria**:*

‘Administration’ page consists of navigation menu in the left side:

1. ‘List of Resources’

2. ‘Administration’:

2.1. 'Endpoint' [(v2) - LONG TERM]

2.2. 'Projects'

2.3. 'Environment management'

3. ‘Billing Report’

There are the following buttons on 'Environment management' page:
 - ‘SSN monitor’
 - ‘Manage environment’
 - ‘Backup’
 - ‘Refresh’",Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Major,2019-05-20 11:10:10,10
13234272,[Back-end]: Ability to set/update project quota,"As an Admin I want to set/update limit per project and general limit, so that I can prevent exceeding determined budget.

*Preconditions:*
 Admin is logged-in DLab
 Admin is located in ‘Manage environment’ popup tab on ‘Administration’ page

*Description:*
 Admin is able to set/udate limit for:
 Project in text box for project quota 
 DLab in text box for DLab for total quota
 Textbox accept only numbers and only integer. 
 If Admin clicks ‘Apply’ button the quota is confirmed and ‘Manage environment’ popup is closed.
 If Admin clicks ‘Cancel’ button the quota is not set/update and ‘Manage environment’ popup is closed.

*Acceptance criteria:*
 1. Admin is able to set/update quota per project
 2. Admin is able to set/update total Dlab quota
 3. DLab TOTAL quota prevails (P1)
 4. Project quota has second priority (P2). Such that Pr1 + Pr2 <= TOTAL Dlab",Debian GCP,['DataLab Main'],DATALAB,Task,Major,2019-05-20 11:05:39,13
13234264,[Front-end]: Ability to terminate a project for admin user,"As an Admin I want to terminate project(s), so that I can rid of unnecessary resources.

 

*Preconditions:*
 # Admin is logged-in DLab
 # Admin is located in ‘Project’  tab on ‘Administration’ page
 # ‘Manage project’ popup is open

 

*Description:*

If Admin clicks termination icon in action column confirmation dialog appears ‘Project name’ will be decommissioned. Do you want to proceed?

If Admin clicks ‘Yes’ a project is terminated, confirmation dialog is closed.

If Admin clicks ‘No’ a project is not terminated, confirmation dialog is closed.

*Acceptance criteria:*
 # Admin is able to terminate a project
 # Terminated project is absent among the other project
 # All resources are killed except for VPC (or check if any other project exists in same VPC, if not - delete VPC) and bucket.",Debian Front-end GCP,['DataLab Main'],DATALAB,Task,Major,2019-05-20 10:42:55,10
13234261,[Back-end]: Ability to terminate a project for admin user,"As an Admin I want to terminate project(s), so that I can rid of unnecessary resources.

 

*Preconditions:*
 # Admin is logged-in DLab
 # Admin is located in ‘Project’  tab on ‘Administration’ page
 # ‘Manage project’ popup is open

 

*Description:*

If Admin clicks termination icon in action column confirmation dialog appears ‘Project name’ will be decommissioned. Do you want to proceed?

If Admin clicks ‘Yes’ a project is terminated, confirmation dialog is closed.

If Admin clicks ‘No’ a project is not terminated, confirmation dialog is closed.





*Acceptance criteria:*
 # Admin is able to terminate a project 
 # Terminated project is absent among the other project
 # All resources are killed except for VPC (or check if any other project exists in same VPC, if not - delete VPC) and bucket.",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,New Feature,Major,2019-05-20 10:40:05,13
13234258,[Admin]: Ability to edit a project,"As an Admin I want to edit project(s), so that I can add or remove user(s).

 

*Preconditions:*
 # Admin is logged-in DLab
 # Admin is located in ‘Project’  tab on ‘Administration’ page
 # At least one project is created

 

*Description:*

If Admin clicks action menu for related project ‘Manage project’ popup appears.

Admin can remove the user from a project hitting on removing icon for appropriate user and in ‘users’ column and confirm the action.

Admin can add user typing the username in users column and confirm the action.

 

*Acceptance criteria:*
 # Admin is able to remove user from a project 
 # Admin is able to add user to a project",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,New Feature,Major,2019-05-20 10:34:54,16
13234250,[Admin]: Ability to create a project,"As an Admin I want to create project(s) and view them, so that I can add group/user(s) per project, assign roles per project, view what group/users are in the project.

 

*Preconditions:*
 # Admin is logged-in DLab
 # Admin is located in ‘Project’  tab on ‘Administration’ page
 # At least one endpoint is created

 

*Description:*

Current functionality of  'Manage roles' moves to 'Project' tab and renames ‘Manage project’.

Admin can create  a project clicking 'Create project' button. ‘Manage project’ popup appears which consists of:

*Step 1 (mandatory)*

- Project name - should be unique within all DLab [editable]

- Endpoint name - [editable]

- Endpoint - dropdown (AWS Account 1, AWS Account 2, GCP Account 1) [part of config file]

- Project Tag: autogenerated, based on project name: dlab_project_name (validation if tag already exists within projects in same account) [ editable]

*Step 2* 
 *  Add a group (mandatory)

*Step 3* 
 *  Assign shape/template (mandatory)

*Step 4* 
 *  Add user(s) (optional)
 * ‘Back’ button
 * ‘Cancel’ button
 * ‘Create’ button

If Admin clicks ‘Back’ button he returns to Step 3

If Admin clicks ‘Cancel’ button the project is not added and ‘Manage project’ popup is open.

If Admin clicks ‘Create button the project is added, appears in grid among the other project, and ‘Manage project’ popup is open.

Grid on ‘Manage project’ consists of:
 * Progect_name
 * Group_name
 * Roles
 * Users
 * Action

 

*Acceptance criteria:*
 # Admin is able to create a project
 # Admin is able to  return to the previous step on the project creation step if admin has not clicked ‘Create’ button.
 # Admin is able to view all existing project(s)
 # The project_shared_bucket is created on Project creation step->
 # User has his home directory, BUT this folder will be accessible for everyone [(v1) - SHORT TERM]
 # User should have proper permission scheme applied later *[(v2) - LONG TERM]*
 # Example: project_bucket: home/user1, home/user2, shared
 # Cloud Role is assigned per project NOT per user
 # Roles per project is created - automatically
 # It is  forbidden to  access to other user notebooks within same project on identity provider level - [(v1) - SHORT TERM]
 # Users works under his own identity. All actions are logged from that username - *[(v2) - LONG TERM]*
 # All edge nodes within project subnet - [(v1) - SHORT TERM]
 # VPC is created - manually

13.1. Multiple projects per 1 VPC

13.2.  Rare case: 1 Project ! VPC
 # Subnet:

14.1. We define Subnet CIDR via WebUI - automatically

14.2. Subnet per 1 project -> Subnet CIDR to be defined on UI",AWS Debian GCP RedHat,['DataLab Main'],DATALAB,New Feature,Major,2019-05-20 09:58:22,16
13231286,[Convolution notebook in docker image][GCP]: Add provision script for Debian,Add provision script for Debian OS family on GCP cloud provider.,Debian DevOps GCP pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-05-02 15:11:13,8
13231282,[Convolution notebook in docker image][Azure]: Add provision script for Debian,Add provision script for Debian OS family on Azure cloud provider,2.3_old AZURE Debian DevOps pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-05-02 15:06:57,8
13231278,[Convolution notebook in docker image][AWS]: Add provision script for Debian,Add provision script for debian os family on aws cloud provider.,AWS DevOps debian,['DataLab Old'],DATALAB,Task,Major,2019-05-02 15:02:30,8
13231274,[Edge]: Extra question on confirmation dialog,"*Preconditions:*
Environment is created

*Steps to reproduce:*
1. Go to 'Environment management' page
2. Click action menu for Edge
3. Choose Stop action

*Ectual result:*
There are two questions on confirmation dialog:
- Do you want to proceed?
- Are you sure you want to continue?
 !Edge.png! 

*Expected result:*
There is one question on confirmation dialog:
- Do you want to proceed?


The same case for notebooks (stop/terminate) on 'environment_management' page
 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-05-02 14:50:26,10
13231266,[Scheduler]: Add validation if time termination is chosen but date - is not fetched,"User can choose time for termination without date and save the scheduler in such setting. But if not choose the date for termination the scheduler for termination will not tigger. 
the best resolution is to add possibility to choose time termination after the date choosing or to add error message.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-05-02 14:19:16,10
13231250,UI changes for scheduler,"*1.* Adjust error messages or disable 'Save' button if mandatory fields are not filled or filled by invalid data:
 !inactivity dot.png! 
 !inactivity empty.png! 
 !scheduler by time date emty.png! 

*2.* Rename radio button 'In case of running jobs on Spark standalone, notebook stop scheduler will not be trigger 'In case of running jobs on Spark standalone, notebook stop scheduler will not be triggered'

*3.* Choose start date/choose finish date/radio button 'Use even...' are disabled if scheduler by time was set before.
+Preconditions:+
Scheduler by time is turn on
+Steps to reproduce:+
1. Open scheduler
Please, view video ''Scheduler.mp4 in attachments",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-05-02 12:53:48,10
13230909,[Data Engine Service]: Front-end,"*Preconditions:*

Data Engine Service is running

*Steps to reproduce:*
 # Go to resource list page

*Actual result:*

It is impossible to set a scheduler for Data Engine Service

*Expected result:*

It is impossible to set a scheduler for Data Engine Service",AWS Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Sub-task,Major,2019-04-30 13:15:00,10
13229846,Create a script for java dependency installation (not from UI) on earlier DLab version ,"Write a cript for  java dependency installation on earlier version DLab ( in which  java dependency feature has not been implemented yet).

For example, add possibility for installation of this lib https://github.com/vegas-viz/Vegas",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-04-24 11:22:34,17
13229577,[Azure]: Spaces are absent between labels and values on Notebook popup,"*Precondirions:*
 # SSN is created on Azure
 # Any notebook is created

*Steps to reproduce:*
 # Go to 'resource list' page
 # Click on Notebbok name

*Actual result:*

Spaces are present between labels and values on Notebook popup

*Expected result:*

Spaces are absent between labels and values on Notebook popup",AZURE Front-end pull-request-available,['DataLab Old'],DATALAB,Bug,Trivial,2019-04-23 10:52:58,5
13229381,Instance management should not depend on Edge status,"*Preconditions:*
 # Computational resource is created for user1
 # Edge is not created for user2. user2 is Admin

*Steps to reproduce:*

1. Login by user2

*Actual result:*

User2 cannot manage by notebook of user1

*Expected result:*

User2 can manage be notebook of user1",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-22 14:04:41,10
13229035,[Azure][Data Lake]: It is impossible to login DLab,"*Preconditions:*
1. SSN is created with Data Lake

*Steps to reproduce:*
1. Click button 'Login with Azure' 

*Actual result:*
User is  not logged in DLab

*Expected result:*
User is logged in DLab

Also the bug is reproduced if enter valid credentials and click 'Login' button.",AZURE Debian Front-end RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-19 11:51:06,10
13229008,Username should be case insensitive,"*Preconditions:*
1. Environment for user1 is created
2. Edge for user1 is topped

*Steps to reproduce:*
1. Login by user1 using upper and lower case for username

*Actual result:*
1. For current user start icon is disabled for edge
 !Case insensitive.png! 

*Expected result:*
1. For current user start icon is enabled for edge
",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-04-19 07:05:06,10
13228552,[Scheduler]: Change error message for Data Engine Service termination,In case if neither terminate date nor terminate time are not chosen and user hits 'Save' butt change error message  from 'The request body Start days or stop days should be filled for scheduler' to 'The request body Terminate days and Terminate times should be filled for scheduler',Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Minor,2019-04-17 09:33:12,6
13228549,Edge node does not appear after step creation till refresh ,"*Preconditions:*
1. SSN is created

*Steps to reproduce:*
1. Cretae an edge node

*Actual result:*
1. Edge node is created
2. Edge node does not appear on DLab UI till refresh

Expected result:
1. Edge node is created
2. Edge node appears on DLab UI",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-17 09:15:53,10
13228351,[AWS][2 VPCs]: Notebook creation fails on stage of configure creation,"*Preconditions:*
SSN is created with two VPCs

*Steps to reproduce:*
1. Create any notebook

*Actual result:*
Notebook creation fails

*Expected result:*
Notebook creation is successful",AWS DevOps pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-16 14:06:38,17
13228121,Convey Edge IP on DLab UI,"If  a notebook is not created user can not find out an Edge IP from Dlab UI.

Please convey IP for Edge on Dlasb UI.",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Minor,2019-04-15 14:45:50,10
13227661,Wrong login error message on DLAB UI,"*Preconditions:*

SSN is created.

*Steps to reproduce:*
 # Go to Dlab UI
 # Enter wrong username and password

*Expected result:*

Error message is: “Username or password are not valid”. 

*Actual result:*

Error message is: “There was an error processing you request. It has been logged (ID d61e47ae31e5030)”. ",Back-end GCP pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-12 09:07:40,13
13227403,[AWS]: Tag 'product: dlab' is absent for some resources,"*Preconditions:*

1. Environment is created

 

*Steps to reproduce:*
 # go to AWS console
 # go to IAM roles
 # go to SGs for SSN

 

*Actual result:*
 # Tag 'product: dlab'  is absent in IAM roles for all resorces
 # Tag 'product: dlab'  is absent  in SGs for SSN

 

*Expected result:*
 # Tag 'product: dlab'  is present in IAM roles for all resorces
 # Tag 'product: dlab'  is present in SGs for SSN",AWS Debian RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-11 08:53:20,9
13227215,Add possibility to choose  zone during DLab deployment,Add new zone,AWS DevOps pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-04-10 15:40:29,17
13227214,[AWS][GCP]:Convey cluster version on Data Engine Service popup,"As a user I want to have a possibility to view a version of Data Engine Service which is deployed from DLab UI on AWS and GCP

 

*Acceptance criteria:*

1.  there is a version of Data Engine Service in 'Cluster type' textbox on cluster popup

AWS EMR cluster, v.X.X.X",Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Minor,2019-04-10 15:34:06,10
13226949,GCP_Terminate_environment job fails,"*Preconditions:*

SSN is created on GCP.

*Steps to reproduce:*

Terminate SSN with “GCP_Terminate_environment-develop” Jenkins Job.

*Expected result:*

Job is finished successfully and environment is terminated. 

*Actual result:*

Job is failed.",Debian DevOps GCP pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-04-09 13:05:01,9
13226878,Reconfiguration spark cluster (on step creation) breaks license header structure,"*Preconditions:*

Notebook is created

 

*Steps toreproduce:*

Reconfigure spark cluster on step creation

 

*Actual result:*

License header structure is broken

!Spark cluster reconfiguration on step creation.PNG!

*Expected result:*

License header structure is  not broken",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Trivial,2019-04-09 08:10:42,9
13226763,Reconfiguration local spark reduces spark driver memory,"*Preconditions:*

Notebook is created

 
 *Steps to reproduce:*

Reconfigure local spark by valid parameters

 

*Actual result:*

Spark driver memory is less than before reconfiguration

 
 *Expected result:*

Spark driver memory is the same as before reconfiguration",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-04-08 18:31:14,3
13226684,[AWS]: Add tags to resources,"Some DLab resources don't have all tags:

- VPC
 * Subnets
 * IAM roles
 * SGs
 * Route tables
 * VPC peering
 * Elastic IPs

It should be fixed",pull-request-available,[],DATALAB,Bug,Major,2019-04-08 12:07:28,9
13226374,Check if instance profiles/iamInstance profiles are deleted during SSN termination,"Verify if instance profiles/iamInstance profiles are deleted, because it is impossible to create Edge",AWS Debian DevOps RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-04-05 13:58:33,9
13226345,[GCP]: Upgrade Dataproc to 1.2 and 1.3,"*Preconditions:*

Dataproc v.1.1 is created on Jupyter

 

*Steps to reproduce:*

1. Go to Jupyter UI

2. Chose SparkR kernel for 

 

*Actual result:*

SparkR kernel dies

!SparkR kernel on Jupyter.PNG!

 

*Expected result:*

SparkR kernel is ready for running

 ",Debian GCP pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-04-05 11:20:43,17
13226114,Buckets and some VPC are not deleted after deleting SSN on GCP by Jenkins Job,"*Preconditions:*

SSN is created.

*Steps to reproduce:*
 # Terminate SSN with “GCP_Terminate_environment-develop” Jenkins Job.
 # Verify absence of VPC in GCP Console.

*Expected result:*

Corresponding to SSN VPC are absent. 

*Actual result:*

Corresponding to SSN VPC are present. ",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Major,2019-04-04 12:58:37,8
13226108,[AWS][GCP]: Sometimes docker return not actual edge status,"The bug was reproduce by one time on AWS [Debian].

*Preconditions:*

1. Environment is created for two users

 

*Steps to reproduce:*

For second user edge is in running status

 

*Actual result:*

On DLab UI Edge is in terminated status

On AWS console Edge is in running status

 

*Expected result:*

On DLab UI Edge is in runing status

On AWS console Edge is in running status

 ",Debian DevOps GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Blocker,2019-04-04 12:35:03,17
13225527,Adjust Zeppelin status after computational resource configuration,"After computational resource configuration, Zeppelin stops and starts.

Please, convey the actual Zeppelin status on UI. And in this case if Zeppelin is in stopping/starting status the other actions for Zeppelin is disabled from DLab UI.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Minor,2019-04-02 14:11:53,2
13225490,[Azure]: Investigate reverse proxy performance after instance creation/starting,"Link downloads too long time where reverse proxy is used in case:
- after notebook creation
- after notebook starting.

Please, investigate it.",AZURE Debian RedHat,['DataLab Old'],DATALAB,Task,Major,2019-04-02 13:30:42,9
13225237,[Front-end]: Add DLab version on UI,Convey the link for release note as well.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-04-01 10:33:26,10
13225236,Back-end,Convey the link for release note as well.,AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Sub-task,Major,2019-04-01 10:32:16,13
13225235,[Back-end]: Add DLab version on UI,"As a user I want to view what version of DLab I am using.

 

*Acceptance criteria:*
 # 'About us' button is added on 'resources list' page
 # If user clicks 'About us' button a popup appears, on which version of deployed DLab  is pointed",Debian RedHat,[],DATALAB,New Feature,Major,2019-04-01 10:30:58,13
13224816,Label should be one for check box preemptible node ,"If preemptible nodes is cheked off:
 * remove the label 'Preemptible nodes' 
 * leave the label Preemptible node count

If preemptible nodes is uncheked leave as it is:
 * only one label 'Preemptible nodes' is displayed

Do the same as it is implemented for AWS:",Front-end GCP pull-request-available,['DataLab Old'],DATALAB,Task,Minor,2019-03-29 13:14:32,10
13224808,[Manage roles]: Remove extra scrollbar from users column,"If a user it is added for users column (only one row) appears extra scrollbar:

!Manage roles.PNG!

Please, remove the extra scrollbar for one row.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Trivial,2019-03-29 12:45:10,10
13224801,[Manage Git Credentials]: Add succeess notification after creating/updating existance account,"If user create new account or update existing one for 'Git credential' success notification should be appear to order to inform a user.

For example:

'Account is created successful'

'Account is updated successful'",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Minor,2019-03-29 12:01:06,10
13224256,Spark reconfiguration fails on Azure,Issue with parameters azure_client_id and azure_user_refresh_token,pull-request-available,[],DATALAB,Bug,Major,2019-03-27 10:26:17,9
13224078,Change response code in case of failing edge on step creation,"*Preconditions:*

SSN is created

 

*Steps to reproduce:*
 # Edge creaion fails

 

Actual result:

1. 'Upload key' popup is offered for user

2. 'Initial infrastructure' popup is still available until user click at any place of 'resources list' page

*Expected result:*

'Upload key' popup is offered for user

 ",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-03-26 16:11:10,13
13224072,Implement ntpdate client for setting system time,"Check that the server clock is synchronized.
 If the clock is delayed, can cause this error:
 AWS was not able to validate the provided access credential

Add  ntpdate client for all nodes",AWS Debian DevOps RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Major,2019-03-26 15:51:18,9
13223339,[GCP]: Notebook stopping/termination fails,"*Preconditions:*

1. Notebook is created

 

*Steps to reproduce:*

2. Stop or terminate notebook from DLab UI

 

*Actual result:*

Notebook stopping/termination fails

It is failed on step of Dataproc termination although any cluster was not created on notebook

!GCP stop notebook.png!

!GCP terminate notebook.png!

 

*Expected result:*

Notebook stopping/termination is successful",Debian DevOps GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,Bug,Critical,2019-03-22 13:39:13,17
13223291,[GCP]: Image creation fails if create notebooks the same template simultaneously,"*Preconditions:*

1. Edge is created on GCP

 

*Steps to reproduce:*

1. Create two notebooks the same template simultaneously

 

*Actual result:*
 # One of notebook creation is failed
 # The second notwbook creation is successful
 # Images is deleted (rollback works)

*!GCP image creation.PNG!*

 

*Expected result:*

1. The first ** notebook is successful created

2. The second ** notebook is successful created

3. Image is created during the first notebook creation

 

 ",Debian GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-03-22 09:50:07,17
13223101,Front-end,"*1.* 'Start' button is not rendered for Edge:

!Start edge after stopping.png!

*2.* If user (not admin) stop his edge confirmation dialog should be for edge not for notebook. Now it is 'Notebook server will be stopped.'

But should be 'Edge node will be stopped. You will need to start it later to proceed working with DLAB.' :

!Edge_stopping.PNG!

 

 

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Sub-task,Major,2019-03-21 15:07:46,10
13223099,Back-end,"*Preconditions:*
 # User (not Admin) is logged in Dlab
 # 'Environment management' page is open

 

*Steps to reproduce:*
 # Click action menu for edge
 # Choose stop action
 # Hit 'Yes' on confirmation dialog

 

*Actual result:*
 # User is logged out:

!401(F12).PNG!

!User not authorized.PNG!

 

*Expected result:*
 # Edge status is 'stopping'
 # User is in 'Environment management' page",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Sub-task,Major,2019-03-21 15:06:53,10
13222566,Playbooks don’t run successfully on Zeppelin Spark-cluster,"*Preconditions:*
 Zeppelin Spark-cluster is created and running. “Python2_data_preparation.json” has run successfully.

*Steps to reproduce:*
 # Open notebook URL in browser 
 # Upload files “R_data_preparation.json”, “Scala_data_preparation.ipynb”, “Python2_data_visualization.json”.
 # Choose appropriate kernel for Spark-cluster.
 # Correct bucket and protocol parameters if needed.
 # Run playbooks.

*Expected result:*
 Playbooks are run successfully. 

*Actual result:*
 Playbooks don’t run successfully.",AWS DevOps,['DataLab Old'],DATALAB,Bug,Major,2019-03-19 12:44:39,9
13222549,[AWS][Debian]: Notebook creation (based on GPU) fails,"*Preconditions:*

1. SSN is created

 

*Steps to reproduce:*

2. Create Notebook (based on GPU)

 

*Actual result:*

Notebook creation fails

!Notebook GPU.PNG!

 

*Expected result:*

Notebook is created",AWS Debian DevOps,['DataLab Old'],DATALAB,Bug,Major,2019-03-19 11:42:32,8
13222358,Connection to EMR v.5.19 kernel PySpark (Python-3.4 / Spark-2.3.2) can't be established on Jupyter,"*Preconditions:*
 EMR v.5.19 for Jupyter is created and running.

*Steps to reproduce:*
 # Choose EMR kernel PySpark (Python-3.4 / Spark-2.2.1) for running

*Actual result:*
 EMR kernel is NOT started successfully

*Expected result:*
 EMR kernel is started successfully

 

EMR kernel is not started only for EMR v.5.19. 

And on the other Jupyter for EMR[5.19] it is the same glitch with starting.",2.3_old AWS DevOps Known_issues(release2.2) Known_issues(release2.3),['DataLab Old'],DATALAB,Bug,Minor,2019-03-18 16:18:48,3
13222074,Manage role is not updated in FireFox browser,"*Preconditions:*
1. DLab s open in FireFox
2. Manage role popu[ is open

*Steps to reproduce:*
1. Add or remove manage role or new user for existing group
2. Click confirm icon
3. Close popup
4. Open manage role popup

*Actual result:*
1. After changes confirmation any request is not sent (f12)
2. After opening manage role popup the group is not updated

*Expected Result:*
1. After changes confirmation appropriate request is sent (f12)
2. After opening manage role popup the group is updated",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-03-16 08:54:37,10
13222062,[GCP]: Disable 'Create' button in case of invalid output data on Data Engine Service creation popup,"*Preconditions:*

1. Dataproc creation popup is open

 

*Steps to reproduce:*
 # Put invalid data for preemtible node count <0.2>
 # Click 'Save' button

 

*Actual result:*

1. 'Save' button is enabled

!DES.png!

 

*Expected result:*

Save' button is disabled

 

************************************************

Decrease distance between preemtible node count/preemtible nodes check box and text box

One label - 'preemtible node count' should be the same as it is implemented for EMR

 

 ",Debian GCP GCP_demo front-end,['DataLab Old'],DATALAB,Bug,Major,2019-03-16 07:24:02,10
13222056,Shell interpreter is absent for some Apache Zeppelin shapes,"*Preconditions:*
 1. Apache Zeppelin is created

Steps to reproduce:
 1. Go to Zeppelin interpreter

*Actual result:*
 Shell interpreter is absent

*Expected result:*
 Shell interpreter is present

Even if Shell interpreter is present the command running fails using the shell for example on GCP:

!Shell interpreter.png!  ",2.3_old AWS AZURE Debian DevOps GCP Known_issues(release2.2) RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Minor,2019-03-16 06:58:47,2
13222043,[AWS]: SSN creation fails,"*Steps to reproduce:*
1. Run Jenkins job for SSN deploying on AWS [Debian]

*Actual result:*
SSN creation fails
 !image-2019-03-16-04-01-03-806.png! 
job/aws_vitv/4

*Expected result:*
SSN creation is successful
",AWS Debian DevOps pull-request-available,['DataLab Old'],DATALAB,Bug,Blocker,2019-03-16 02:04:47,8
13221938,[GCP]: Data engine job failed on RStudio and stuck on Apache Zeppelin,"*{color:#333333}Preconditions:{color}*{color:#333333}
 {color}RStudio Data engine{color:#333333} is created and running.{color}

*{color:#333333}Steps to reproduce:{color}*{color:#333333}
 1. Upload “carriers.csv”, “airports.csv”, “2008.csv.bz2” in storage.{color}

{color:#333333}2. Upload “Flight_data_Preparation_R-Spark.R” on  RStudio.{color}

{color:#333333}3. Correct bucket and protocol parameters if needed.{color}

{color:#333333}4. Choose Data engine for running.{color}

{color:#333333}5. Run playbook.{color}

*{color:#333333}Expected result:{color}*{color:#333333}
 {color}{color:#24292e}Playbook is run successfully.{color}{color:#333333} {color}

*{color:#333333}Actual result:{color}*{color:#333333}
 {color}{color:#24292e}Data engine job has not accepted any resources.{color}

{color:#24292e} {color}

{color:#24292e}*************************************************{color}

{color:#24292e}For Apache Zeppelin any of preparation/visualization playbooks stuck for Data Engine{color}

 

{color:#24292e}*************************************************{color}

{color:#24292e}For RStudio and Apache Zeppelin the bug is reproduced if:{color}
 * {color:#24292e}Notebook deployed on shape ""Size"": ""S"", ""Description"": ""n1-standard-2"", ""Type"": ""n1-standard-2"",""Ram"": ""7.50 GB"",""Cpu"": ""2""{color}
 * Data Engine deployed on shape ""Size"": ""XS"", ""Description"": ""n1-standard-1"", ""Type"": ""n1-standard-1"",""Ram"": ""3.75 GB"",""Cpu"": ""1""}

If deploy Data Engine on larger shape - ""n1-standard-2 -> the will not be reproduced

{color:#24292e} {color}",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Critical,2019-03-15 15:47:15,8
13221684,[Azure]: TensorFlow/Zeppelin/Jupyter creations fail in installing Matplotlib,"*Preconditions:*

1. Edge is created

 

*Steps to reproduce:*

1. Create TensorFlow (Jupyter with tensorFlow) or Jupyter or Apache Zeppelin

 

*Actual result:*

TensorFlow creation fails

!TensorFlow.PNG!

*Expected result:*

TensorFlow creation is successful",AZURE Debian DevOps pull-request-available,['DataLab Old'],DATALAB,Bug,Blocker,2019-03-14 14:48:02,8
13221681,[GCP]: Jupyter does not start successfully after Data Engine Service creation,"*Preconditions:*
 # Data Engine Service is running on Jupyter

 

*Steps to reproduce:*
 # Stop Jupyter from DLab UI
 # Start Jupyter after stopping fron DLab Ui
 # Go to Jupyter UI

*Actual result:*
 # It is impossible to go to Notebook UI

!Jupyter.PNG!

*Expected result:*

It is possible to go to Notebook

 ",Debian DevOps GCP_demo pull-request-available,[],DATALAB,Bug,Major,2019-03-14 14:38:25,17
13221677,[Azure]: Rarely  template Apache Zeppelin is not available after SSN creation ,"SSN was created on Azure (debian) without Data Lake

*Preconditions:*

1. Edge is created

 

Steps to reproduce:
 # 'Resource list' page is open
 # Click 'Create new' button

 

*Actual result:*

Template Apache Zeppelin is absent

 

*Expected result:*

Template Apache Zeppelin is present

 

*** Template Apache Zeppelin will be enabled if restart UI

 

 ",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Task,Minor,2019-03-14 14:18:40,13
13221427,[Azure]: Billing is not loaded for Notebook and Data Engine,"*Preconditions:*

1. Billing is available

 

*Steps to reproduce:*
 # Go to billing report page or billing detail

 

*Aactual result:*

1. Billing is not loaded for Notebook and Data Engine

!Billing.PNG!

 

*Expected result:*

1. Billing is loaded for Notebook and Data Engine

 ",AZURE Back-end Debian RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-03-13 14:47:01,13
13221389,???[Java library]: Add support of repository for spark-packages.org,"Add support for repository of spark-packages.org in order to install bundle libraries from DLab UI

Spark-packages.org link: [https://spark-packages.org/] 

[https://dl.bintray.com/spark-packages/]

 ",AWS AZURE Back-end GCP,['DataLab Old'],DATALAB,Task,Minor,2019-03-13 12:04:15,13
13221387,[Java libraries]:Add possibility to find bundle-lib from maven repository ,"As a user I want to install bundle library on instances from DLab UI

 

*Acceptance criteria:*
 # If user enter bundle library the library is found and it is suggested to install
 # if user installs bundle library the bundle library is installed on instance(s) * ** ***

 

Spark-packages.org link: [https://spark-packages.org/] 

 

For now library apache avro 1.8.2 is not found from DLab UI

[https://mvnrepository.com/artifact/org.apache.avro/avro/1.8.2]

!bundle library.png!

 ",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-03-13 11:53:54,13
13221153,[GCP]: Data Engine creation fails on DeepLearning,"*Preconditions:*

1. DeepLearning is created

 

*Steps to reproduce:*
 # Create Data Engine on DeepLerning

 

*Actual result:*

Data Engine creation fails

!Data Engine.PNG!  

 

*Expected result:*

Data Engine is created

 ",Debian DevOps GCP GCP_demo,['DataLab Old'],DATALAB,Bug,Major,2019-03-12 15:24:37,17
13220211,Wrong marking for shared resources,"*Preconditions:*

*Biiling is available*

 

Steps to reproduce:
 # Go to billing report page

 

*Actual result:*
 # Value for user 'Shared resource' is marked for user's own values

 

*Expected result:*

1. Value for user 'user@epam.com' is marked for user's own values",AZURE Back-end Debian RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-03-07 13:36:01,13
13220179,Migration to ubuntu 18.04 for all clouds,"For now ubuntu version for AWS is 16.04.01 and for GCP is 16.04.02
 # Use one version 18.04 for AWS, GCP, Azure
 # Get rid of kernel Py 2.7
 # Leave kernels python 3.5 ??? and 3.7 (Default value of ubuntu 18.04 is Python 3.6.9???)

 ",AWS AZURE DevOps GCP,['DataLab Old'],DATALAB,Task,Critical,2019-03-07 12:18:16,3
13220133,[Quota]: Value for 'limit' text box should be only integer,"*Preconditions:*
 # environment is created

 

*Steps to reproduce:*
 # Click 'Manage environment'
 # Put in limit text box <.>, <->, <+>

 

*Actual result:*
 # There is no error message
 # 'Apply' button is enable

 

*Expected result:*
 # There is  error message. For example: 'The value should be an integer and can not contain letters, characters'
 # 'Apply' button is disabled",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-03-07 09:19:49,10
13219906,[GCP]: Instance connection is unsuccessful via GCP console due to failed to read key,"*Preconditions:*
 # Edge is created
 # GCP console is open

 

*Steps to reproduce:*
 # Click SSH drop down list related SSN or Edge
 # Choose 'open in browser window using provided private SSH key'
 # Choose your key.pem

 

*Actual result:*

1. Connection is not successful

2. Error message appears 'Select a private ECDSA or RSA key file to sign into the VM or connect with a generated SSH key.'

 

*Expected result:*

1. Connection is successful",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Minor,2019-03-06 13:16:46,2
13219528,[GCP][Azure]: Data Engine reconfiguration fails,"The bug was found on Jupyter, RStudio. Bit it seems it will be reproduced for all notebooks and clouds.

*Preconditions:*
 Data Engine is created on Notebook

*Steps to reproduce:*
 # Stop notebook with related cluster
 # Start notebook with related cluster
 #  Reconfigure Data Engine with valid parameters

*Actual result:*
 Data Engine reconfiguration fails on running cp -f command
 !Data Engine Reconfiguratio.png!

*Expected result:*
 Data Engine reconfiguration is successful",AZURE Debian DevOps pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-03-05 11:24:00,9
13219361,[GCP][Data Engine]: Spark Configuration fails on process of  installing  kernels,"The bug was found on notebooks: Jupyter, RStudio, Apache Zeppelin

 

*Preconditions:*

1. Notebook is created

 

*Steps to repdoduce:*
 # Create Data Engine on Notebook

 

*Actual result:*

Data Engine configuration fails:

!Spark configuration.PNG!

 

*Expected result:*

Data Engine is created successfully",Debian DevOps GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,Bug,Critical,2019-03-04 18:38:57,17
13219251,UI issues after scheduler modification,"*1.* Radio button should not overlap if it is error message 'At least one of time range fields should be selected':

!Overlapping  radio button.PNG!

 

*2.* 'Save' button should be disabled if mandatory fields are not filled or filled by invalid data for scheduler by time/scheduler by inactivity:

!Disabled button.PNG!

 

*3.* Float, negative numbers (using '.' or ',') are accepted  in scheduler by inactivity text box:

!Float.PNG!

 

*4.* Label 'Scheduler by inactivity, min' changes its position to down if appears error message:

!No error.PNG!

!With error.PNG!

 

*5.* Turning off scheduler by inactivity does not turn off scheduler for instance. To reproduce this case:
 +Preconditions:+ 
 Scheduler by inactivity is turn on
 +Steps to reproduce:+
 1. disable scheduler by inactivity
 2. Close scheduler 
 3. Open scheduler

 

*6.* Choose start date/choose finish date/radio button 'Use even...' are disabled if scheduler by time was set before 

!disabled texbox.PNG!

 

*7*. Rename radio button 'Use even some jobs are ran on computational resources' by 'In case of running jobs on Spark standalone, notebook stop scheduler will not be trigger'

 

*8.* Radio button 'In case of running jobs on Spark standalone, notebook stop scheduler will not be trigger' should be for 'Scheduler by time' and 'Scheduler by inactivity' 

 

*9.* Radio button 'In case of running jobs on Spark standalone, notebook stop scheduler will not be trigger'  should be checked by default

 

*10.* In order to turn off scheduler:

*10.1.* [scheduler by time]: disable 'scheduler by time' -> click 'Save' button

*10.2*. [scheduler by inactivity]: disable 'scheduler by inactivity' -> click 'Save' button

 

*11.* Scheduler for Spark cluster. Radio button 'Inherit notebook schedule settings' dissapers if switch from scheduler inactivity to scheduler by time:

!Scheduler for Spark.PNG!",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-03-04 09:56:07,10
13219236,[Autotest][GCP][Data Engine Service]: The python script execution wasn`t successful on Jupyter,"*Steps to reproduce:*

1. Run autotest for Data Engene Service for Jupyter

 

*Actual result:*

Autotest execution fails:

+AutoTests_GCP/130/+

 

*Expected result:*

Autotest execution is successful

 

Playbook running for Data Engene Service for Jupyter is successful if run by manually",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Minor,2019-03-04 08:26:23,8
13219021,During DLab page open replace 'Loading...',Investigate what image to give or move 'Loading...' in the center of page,Front-end,[],DATALAB,Task,Major,2019-03-01 21:19:43,10
13219020,Notebook creating is not in active filter,"*Preconditions:*
1. Edge is created

*Steps to reproduce:*
1. Create the first notebook in the environment

*Actual result:*
Notebook is NOT shown in active filter though its status is 'creating'

*Expected result:*
Creating notebook is shown in active filter

",Front-end pull-request-available,[],DATALAB,Bug,Major,2019-03-01 21:12:27,10
13218926,Playbook running fails on TensorFlow with RStudio,"*Preconditions:*

TensorFlow with RStudio is created on AWS [RedHat]

 

*Steps to reproduce:*

Run Playbook r_templ.r on TensorFlow with RStudio

 

*Actual result:*

Plybook runs with error:

!TensorFlowRstudio.PNG!

 

*Expected result:*

Playbook runs successfully

 

 ",AWS Debian DevOps RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-03-01 16:34:11,17
13218912,[Environment management]: Add possibility to filter items by values ,"As an admin I want to filter by values on 'Environment management' page, so that I can quickly find necessary information.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Main'],DATALAB,Improvement,Major,2019-03-01 15:56:53,10
13218897,Docker check in activity should be terminated after execution,"docker run -i replace by docker run --rm -i

 Inactivity integration branch",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-03-01 14:30:59,13
13218683,RStudio_TensorFlow creation fails in case of creating from user’s AMI,"*{color:#333333}Preconditions:{color}*{color:#333333}
 {color}RStudio with TensorFlow notebook {color:#333333}is created with Spark-cluster. {color}

{color:#333333}Notebook is running.{color}

 

*{color:#333333}Steps to reproduce:{color}*{color:#333333}
 1. Create AMI for notebook.{color}

{color:#333333}2. Create new notebook from created AMI{color}.

 

*{color:#333333}Expected result:{color}*{color:#333333}
 {color}New notebook is created{color:#24292e} successfully.{color}{color:#333333} {color}

 

*{color:#333333}Actual result:{color}*

{color:#333333}Creation of new notebook {color}{color:#24292e}failed.{color}{color:#333333} {color}",AWS Debian DevOps,['DataLab Old'],DATALAB,Bug,Major,2019-02-28 16:24:46,9
13218337,[Azure]: Inactivity file does not have ID for Data Engine Cluster,"*Preconditions:*

Data Engine Cluster is created

 

*Steps to reproduce:*
 # Go to /opt/inactivity
 # Run ls -la

 

*Actual result:*

File name for Data Engine Cluster is '_inactivity'

 

*Expected result:*

File name for Data Engine Cluster is 'ID_inactivity'

 

_________________________________________________________________________

The bug was found if start data Engine Cluster fron DLab UI (previously Data Engine Cluster was stopped from DLab UI)

*Data Engine cluster starting fails (it was previously stopped):*

!Data Engine Starting.PNG!

 

 

 ",AZURE Debian DevOps RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-27 13:21:14,17
13218134,Warning concerning limit is absent,"*Preconditions:*
 # Billing is available

 

*Steps to reproduce:*
 # Set limit from 75% to 99% of actually used
 # Logout
 # Login

 

*Actual result:*

Warning concerning limit (that user is closed to quota ) is absent

 

*Expected result:*

Warning concerning limit (that user is closed to quota ) is present",AZURE Debian Front-end RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-26 17:02:57,10
13217965,Reconfiguration spark breaks license header structure,"*Preconditions:*
 Any notebook/cluster is created

*Steps to reproduce:*
 1. Reconfigurate local spark with valid parametters

*Actual result:*
 License header structure is broken in spark-default file
 !reconfiguration.png!

*Expected result:*
 License header structure is NOT broken in spark-default file

!spark-default.png!

*For remote spark header structure is broken even if remote spark is NOT reconfigured*. Please view 'Remote spark.txt' in attachments",AWS AZURE Debian DevOps GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Trivial,2019-02-26 01:25:18,9
13217918,[Azure][GPC]: SSN creation fails,"Steps to reproduce:
 1. Create SSN on GCP or Azure from branch 'inactivity_integration'

*Actual result:*
 SSN creation fails with error *File ""/usr/lib/python2.7/UserDict.py"", line 40, in __getitem__
 raise KeyError(key) KeyError: 'ldap_hostname'*

feature-branches/job/azure_vitv/24
feature-branches/job/gcp_vitv/18

*Expected result:*
 SSN is created successfully",AZURE Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Blocker,2019-02-25 21:11:17,17
13217819,[GCP][Data Engine Service]: Kernels R/Pyspark (Py2) of dataproc 1.2 are not run successfully on Zeppelin 0.8.0,"*Preconditions:*
 Dataproc 1.2 is created on Apache Zeppelin 0.8.0

*Steps to reproduce:*
 1. Run r data preparation for remote kernel Py2 or Py3

*Actual result:*
 Playbook runs with error

*Expected result:*
 Playbook runs successfully

***If Dataproc 1.2 is created (running/stopped) local kernel (Py2/Py3) is not run successfully. It is the same error as for remote kernel. If Dataproc 1.2 is terminated local kernel (Py2/Py3) is run successfully.

***Dataproc 1.3 runs successfully.

--------------------------------------------------------------------------------------------------------------------

If runs Py2 data preparation on dataproc 1.2 a playbook runs successfully but wit exception. For local kernel it is the same. If Dataproc 1.2 is terminated local kernel (Py2) is run without exception.

 

Please, view attachments.",2.3_old Debian DevOps GCP Known_issues(release2.2) RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-25 13:14:57,7
13217812,Add possibility to terminate computational resources by scheduler,"As a user I want to terminate computational resources via scheduler by time.

*Acceptance criteria:*
 *1.* scheduler for computational resources should be the same as for Spark Standalone cluster except for:
 *1.1*. instead of finish use terminate

*1.2.* scheduler by inactivity is absent for computational resources
 *1.3.* cut start date/time
 *1.4.* cut the next radio button:
 !Scheduler.PNG!",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2019-02-25 12:45:27,13
13217436,Issues after updating angular and dependencies,"# +*Manage roles:*+

 * Add space between buttons 'Back/'Cancel/'Next' and 'Back/'Cancel/'Create'
 !1.png!

 * Fonts for user's label (manage roles page ) and library's label (manage libraries page) should be the same
 !1a.PNG!

 * Extra scrollbar for users column:
 !3.png!

* Decrease the popup size in case if user is nit allowed to create clusters:
 !cluster creation.png! 

2. +*Manage Libraries:*+
 * Add space before 'Enter library name'
 !8.png! 
 !9.png!
 *  Add space between library name and version

!Library.PNG!

3. +*Login:*+
 * Align Login in button 'Login'
 !4.png!

 * Convey error message: 'Username or password are not valid'
 !5.png!

 * Convey error message: 'Please contact AWS administrator to create corresponding IAM User'
 !5a.PNG!

 * Convey error message: 'Please contact AWS administrator to activate your Access Key'

4. +*EMR Cluster:*+
 * error message for spot instance is beyond of popup
 !13.png!

 * add space between Apache Spark Master/ EMR Master and url
 !15.png!

5. +*Align confirmation dialog, add header and delete icon for:*+
 * group deletion (manage roles)
 * spark/emr stopping/deletion on resource list page and environment management page
 * account deletion for manage git credentials
 * user's environment stop/termination on Manage environment popup

5. +*Filter shape in dtopdown list shoul start from lower case on 'resources list' and 'billing report' pages:*+
 !6.png! 
 !11.png!",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-22 12:56:43,10
13217411,Add possibility of filtering list of installed libraries ,"As a user I want to have possibility to filter the list of installed libraries.

 

*Acceptance criteria:*

There is possibility to filter installed libraries list by Name, Group, Destination, Resource type and Status.",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Main'],DATALAB,Improvement,Major,2019-02-22 10:56:25,5
13217395,Fix max_body_size for Nginx,"*Preconditions:*
 1. Jupyter UI page is open via reverse proxy

*Steps to reproduce:*
 1. Upload file.csv with size more than 1 MB

*Actual result:*
 File uploading is blocked

*Expected result:*
 File is uploaded

_Increase max_body_size to 50 MB_",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-22 09:29:09,9
13217389,Add parameter SSN size in DataLab deploy script,"As a user I want to choose the size of SSN during DataLab deploying.

Add parameter which responses for size of SSN during DataLab deploying.

*Acceptance criteria:*

1. There is parameter [key: value] for SSN size in deploy script",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Trivial,2019-02-22 09:09:26,2
13217205,[Scheduler by inactivity]: Job running for Spark standalone cluster updates last activity for notebook,"*Preconditions:*
1. Spark Standalone cluster is created on Notebook
2. Scheduler by inactivity is set for Spark Standalone cluster

*Steps to reproduce:*
1. run job for Spark Standalone cluster
2. Time by inactyvity is passed for Spark Standalone cluster

*Actual result:*
1. Last activity is NOT updated for Spark Standalone cluster
2. Last activity is updated fot notebook

*Expected result:*
1. Last activity is updated for Spark Standalone cluster
2. Last activity is NOT updated fot notebook
",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-21 13:47:18,13
13217148,[Spark Standalone Cluster]: Scheduler by inactivity does not work for cluster if it is more than one spark clusters on the same notebook,"*Preconditions:*
1. Two Spark Standalone clusters are created on the same notebook

*Steps to reproduce:*
1. Set schedule by inactivity for the second created  spak cluster

*Actual result:*
Schedule by inactivity does not work for the second created  spak cluster (spark cluster is running after passing inactivity time)

*Expected rsult:*
Schedule by inactivity works for the second created  spak cluster (spark cluster is stopped after passing inactivity time)",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-21 09:55:38,13
13217005,Scheduler by inactivity does not work for notebook if spark standalone cluster is not created yet,"*Preconditions:*
1. Apache Zepelin is created

*Steps to reproduce:*
1. Set scheduler by inactivity time
2. Inactivity time is passed

*Actual result:*
1. Apache Zeppelin is not stopped

*Expected result:*
1. Apache Zeppelin is stopped

",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-20 18:42:43,13
13216986,Visualization_Python2-Spark doesn’t run successfully on Jupyter EMR,"*Preconditions:*
 Jupyter EMR-cluster is created and running.

*Steps to reproduce:*
 1. Open notebook URL in browser 
 # Upload file “Flights_data_Visualization_Python2-Spark.ipynb”.
 # Choose appropriate kernel for EMR-cluster.
 # Correct bucket and protocol parameters if needed.
 # Run playbook.

 *Expected result:*
 Playbook is run successfully. 

 ** *Actual result:*
 Playbook doesn’t run successfully.",AWS Debian EMR Nexus,['DataLab Old'],DATALAB,Bug,Major,2019-02-20 17:08:40,9
13216985,Preparation_Scala-Spark doesn’t run successfully on Jupyter EMR,"*Preconditions:*
 Jupyter EMR-cluster is created and running.

*Steps to reproduce:*
 1. Open notebook URL in browser 
 # Upload file “Flights_data_Preparation_Scala-Spark.ipynb”.
 # Choose appropriate kernel for EMR-cluster.
 # Correct bucket and protocol parameters if needed.
 # Run playbook.

 *Expected result:*
 Playbook is run successfully. 

 ** *Actual result:*
 Playbook doesn’t run successfully.",AWS Debian EMR Nexus,['DataLab Old'],DATALAB,Bug,Major,2019-02-20 17:04:22,9
13216972,Preparation_Python2-Spark doesn’t run successfully on Spark-cluster,"*Preconditions:*
 Jupyter Spark-cluster is created and running.

*Steps to reproduce:*
 # Open notebook URL in browser.
 # Upload file “Flights_data_Preparation_Python2-Spark.ipynb”.
 # Choose appropriate kernel for Spark-cluster.
 # Correct bucket and protocol parameters if needed.
 # Run playbook.

 *Expected result:*
 Playbook is run successfully. 

 ** *Actual result:*
 Playbook doesn’t run successfully.",AWS Debian Nexus,['DataLab Old'],DATALAB,Bug,Major,2019-02-20 16:27:51,9
13216855,EMR creating fails in case of creating with spot instances,"*{color:#333333}Preconditions:{color}*{color:#333333}
 Zeppelin is created and running.{color}

 

*{color:#333333}Steps to reproduce:{color}*{color:#333333}
 1. Add computational resources.{color}

{color:#333333}2. Choose cluster type “AWS EMR cluster”.{color}

{color:#333333}2. SELECT any computational parameters.{color}

{color:#333333}3. Check off “Spot instances”.{color}

{color:#333333}4. Press “Create” button.{color}

{color:#333333} {color}

*{color:#333333}Expected result:{color}*

{color:#24292e}EMR is created.{color}{color:#333333} {color}

 *{color:#333333}{color}* 

{color:#333333}*Actual result:*
 {color}{color:#24292e}EMR creation is failed.{color}",AWS Debian EMR Nexus,['DataLab Old'],DATALAB,Bug,Major,2019-02-20 09:30:30,9
13216682,Installation of  CensusData library from Python2 group fails on Zeppelin without error information,"*{color:#333333}Preconditions:{color}*{color:#333333}
 Zeppelin is created and running.{color}

 

*{color:#333333}Steps to reproduce:{color}*{color:#333333}
 1. Manage libraries for notebook.{color}

{color:#333333}2. Chose {color}CensusData library from Python2 group

{color:#333333}3. Press “Install” button{color}

 

*{color:#333333}Expected result:{color}*{color:#333333}
 {color}{color:#24292e}CensusData library is installed.{color}{color:#333333} {color}

 

{color:#333333}*Actual result:*
 {color}{color:#24292e}CensusData library is not installed.{color}{color:#333333} Information about error is not displayed.{color}",AWS Debian EMR Nexus,['DataLab Old'],DATALAB,Bug,Major,2019-02-19 16:06:05,9
13216678,List of available libraries for Zeppelin EMR notebook isn’t loaded ,"*Preconditions:*
 Zeppelin is created and running. EMR is running.

*Step to reproduce:*
 Manage libraries for EMR.

*Expected result:*
 After a while list of all available libraries is received. 

*Actual result:*
 Loading process does not reach the end.",AWS Debian EMR Nexus,['DataLab Old'],DATALAB,Bug,Major,2019-02-19 15:51:14,9
13216668,[Billing]: Filter by shared resource does not work,"*Preconditions:*
1. Billing is available
2. Billing report page is opened

*Steps to reproduce:*
1. Filter by shared resource

*Actual result:*
Filter output is emty
 !Shared resource.png! 

*Expected result:*
Filter output includes only values for shared resource

_______________________________________________

Shared resource in dropdown list should start from upper case",AZURE Debian Front-end RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-19 15:08:29,13
13216586,"[AWS][Autotest][Apache Zeppelin]: Version numpy is 1.7.1, but should be 1.14.3","*Preconditions:*
1. Environment is created by autotest 

*Steps to reproduce:*
1. Go to Apache Zeppelin UI
2. Run a command '%pyspark
import numpy; print(numpy.__version__) '

*Actual result:*
Version numpy is 1.7.1

*Expected result:*
Version numpy is 1.14.3
",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-19 10:39:46,17
13216408,[AWS][RedHat]: Notebook creation fails due to AWS was not able to validate the provided access credentials,"*Preconditions:*
1. Edge is created

*Steps to reproduce:*
1. Create any template of Notebook

*Actual result:*
Notebook creation fails:
 !Notebook creation.PNG! 

*Expected result:*
Notebook is succesfully created
",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Blocker,2019-02-18 14:06:47,17
13216394,?[AWS]: IAM resources are not deleted during SSN termination by  Jenkins job,"*Preconditions:*
1. Environment is created

*Steps to reproduce:*
1. Terminate SSN by Jenkins Job

*Actual result:*
IAM resources are not deleted 

*Expected result:*
All IAM resources are deleted
",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-18 12:57:00,2
13216021,Front-end,"Remove radio button in case if Spark Standalone cluster is not absent or is in status 'terminated/terminating'' or any spak cluster is not created yet.
 !Scheduler.PNG!",Front-end,[],DATALAB,Sub-task,Major,2019-02-15 15:31:23,10
13216015,Modification of exploratory scheduler,"1. Add check box for for notebook scheduler 'Use even some jobs are ran on computational resources':
1.1. check box is checked off means scheduler for notebook is more prioritative. Scheduler for notebook will be executed (stop) even if jobs are running on related computational resources at that moment  and scheduler by inactivity is set on cluster. 
1.2. check box is unchecked means if scheduler is set for notebook before stopping it will verify if some jobs are running on computational resource. If jobs are running scheduler for notebook will not stop by inactivity until this jobs are running on clusters.
",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-15 15:18:55,13
13215984,[Scheduler on idle]: Reminder for scheduled by Inactivity does not appears before 15 minutes of stopping,"*Steps to reproduce:*
1. Set valid value for shcedule by inactivity for instance more than actual inactivity
2. Click 'Save' button

*Actual result:*
Scheduler reminder appears

*Expected result:*
Scheduler reminder does not  appear
",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Minor,2019-02-15 14:12:20,13
13215782,"[Scheduler on idle]: Prevent scheduler by inactivity execution in case if computational resource are in statuses 'creating', 'terminating', 'configuring'","*Preconditions:*
 Computational resource is in status creating/configuring/terminating

*Steps to reproduce:*
 1. Execute scheduler by inactivity for notebook

*Actual result:*
 Computational resource is in failed status on DLab UI

*Expected result:*
 Prevent scheduler by inactivity execution",AWS AZURE Back-end Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-14 17:02:19,13
13215773,[Scheduler on idle]:Spark Standalone cluster is stopped earlier due to data is written in MongoDB with delay,"*Preconditions:*
1. Scheduler by Inactivity is set 10 minutes for Spark Standalone Cluster

*Steps to reproduce:*
1. Run job for Spark Standalone Cluster

*Actual result:*
1. Spark cluster is in 'stopping' status but running of job is not finished

*Expected result:*
1. Spark cluster will stop only in 10 minutes after finishing job for Spark Standalone Cluster",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-14 16:37:25,17
13215765,[Scheduler on idle]: Inactivity scheduler for notebook should check if job is running on remote kernels (Data Engine/ Data Engine Service),"*Preconditions:*
1. Data Engine or Data Engine Service is created
2. Job is running on remote kernel
3. Job on local kernel was run more than 10 minutes

*Steps to reproduce:*
1. Set scheduler by inactivity only for notebook for 10 minutes

*Actual result:*
1. Notebok is stoping despite job is running on remote kernel

*Expected result:*
1. Notebook is running
2.  Notebook will stop if any jobs are not running on remote kernel
",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-14 15:47:57,13
13215719,[Scheduler on idle]: Update last time inactivity after restarting by starting time,"If start an instance (previously it was stopped) the last time of inactivity is not updated. It is used that last time at what job was running before instance stopping.

So if instance starts after stopping we should update the previous last time by starting time",AWS AZURE Debian GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-02-14 11:08:34,17
13215519,[Scheduler by inactivity time]: Include 'max_inactivity' in request,"The value of  'max_inactivity' is not included in request, so the latest changes for acheduler inactivity are not portrayed on UI side.

Now the feature for inactivity is available  in branch  inactivity_integration",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-13 14:42:07,10
13215489,[GCP]: SSN creation fails due to error in modifying conf files,"*Steps to reproduce:*
1. Create SSN

*Actual result:*
SSN creation fails 
job/GCP_Create_environment-develop/120/console
 !GCP_SSN.PNG! 

*Expected result:*
SSN is created successfully",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Blocker,2019-02-13 11:57:35,17
13215286,[Inactivity time]: UI changes for scheduler,"*1.*  Remove note: *'NOTE: to enable the scheduler at least one weekday should be specified.'*
*2.* Remove error handling if scheduler is not set:
 !Error handling.PNG! 
*3*. The latest unchecked day should have the color as unselected one:
 !Color days.PNG! 
*3.* After clicking 'Save' button for  'Inactive time' scheduler popup should be closed
*4.* 'Save' button should be enabled only after filing of mandatory field???
*5.* Rename label 'Inactivity time' by 'Scheduler by inactivity, min'
*6.* Rename label 'Enable scheduler' by 'Scheduler by time'
*7.* If 'inactivity time' is enable the setting scheduler by manual should not be visible (hide it)
*8.* The default value for Inactivity time should be 120
*9.* If items are disabled they should have the same color. For now value of 'select offset' has another color: 
 !Select offset disabled.PNG! ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-12 16:52:40,10
13215255,[GCP][Apache Zeppelin][Dataproc]: Playbook running fails due to spark.submit.deployMode is not specified,"*Preconditions:*
1. Dataproc is created on Apache Zeppelin

*Steps to reproduce:*
1. Run any playbook on Apache Zeppelin with Dataproc

*Actual result:*
Playbook running fails with error:
master is set as yarn, but spark.submit.deployMode is not specified
 !Zeppelin DP py2.PNG! 

*Expected result:*
Playbook running is successful
",Debian DevOps GCP pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-12 14:02:46,17
13215245,[Billing]: Value for shared resources should be named in user dropdown list,"There empty values in user dropdown list for SBN, SSN-bucket, shared-bucket.
Value of 'Shared resource' should be in this empty values.
 !Billing.PNG! ",AZURE Back-end Debian RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Minor,2019-02-12 13:15:20,13
13215225,[GCP]: Notebook starting fails (previously it was stopped),"First of all the bug was found during starting RStudio. And now the bug reproduced for Jupyter, Zeppelin randomly. Notebook after starting fails or if starting is successful it is impossible to go to UI Notebook

*Preconditions:*
 RStudio is in 'stopped' status

*Steps to reproduce:*
 1. Start RStudio

*Actual result:*
 RStudio starting fails with error
 *fabric.exceptions.NetworkError: Timed out trying to connect to 172.31.16.14 (tried 100 times)*
 !RStudio starting.PNG!

*Expected result:*
 RStudio starting is successful",Debian DevOps GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,Bug,Blocker,2019-02-12 12:29:19,8
13214989,[Front-end]: Improvements for manage role, Group by shape/template in 'Roles' drop down list,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Minor,2019-02-11 13:13:32,5
13214986,[Front-end]: Add scheduler management on 'Environment Management' page,"As an Admin I want to manage users scheduler so that I can monitor and manage of stopping/termination instances via scheduler.

Acceptance criteria:
1. only admin can see/manage user scheduler
2. if admin changes user scheduler a user sees appropriate message after opening a scheduler

Options",AWS AZURE Debian Front-end GCP RedHat,[],DATALAB,Improvement,Minor,2019-02-11 12:58:39,0
13214985,[Back-end]: Add scheduler management on 'Environment Management' page,"As an Admin I want to manage users scheduler so that I can monitor and manage of stopping/termination instances via scheduler.

*Acceptance criteria:*
1. only admin can see/manage user scheduler
2. if admin changes user scheduler a user sees appropriate message after opening a scheduler",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Minor,2019-02-11 12:57:58,1
13214691,[Generate key]: 'Initial infrastructure creation' popup does not appear until clicking 'Refresh' button,"*Preconditions:*
1. User is logged in DLAB

Steps to reproduce:
1. Click 'Generate' button
2. Click 'Save' button

*Actual result:*
1. Initial infrastructure creation' popup does not appear until clicking 'Refresh' button

*Expected result:*
Initial infrastructure creation' popup  appears",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-02-08 18:44:47,10
13214584,Add name of active page to DLab menu bar,"The name of active page should be displayed on DLab menu bar.

!DLab_menu_bar.png!

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Improvement,Minor,2019-02-08 11:16:49,10
13214571,[Admin page]: Merge 'Health status' and 'Environment Management' pages,"As an admin I want to have administration page in one place.

1. *Move 'Health status' page to 'Environment Management' pages adding buttons on the right top:*
 1.1. 'Manage roles'
 1.2. 'SSN Monitor'
 1.3. 'Manage environment'
 1.4. 'Backup'

2. *If Admin click on icon 'health status' the 'Environment Management' page opens with the following sorted:*
 2.1 by current user and edge
 2.2. reupload key/ recreate edge should be only for current user (the actions should not be displays for other user if even Admin is logged in)

3. *If user is logged in (Not Admin) can see:*
 3.1. only own environment NOT other users
 3.2. only user's edge status with appropriate actions: stop/start/recreate/reupload key",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Major,2019-02-08 10:08:50,13
13214557,[Manage roles]: Group name should be mandatory field,"It is impossible to add new group if group name text box leave empty, but user is not notified about this.

*To do:*
 # group name should be mandatory field
 # if leave empty group name text box error message should appears ""Group name should not be empty""",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-08 09:01:45,10
13214390,Add header and close-window buttons to popups,"# Add header to SSN Monitor popup.
 # Add close button to SSN Monitor popup.

!SSN monitor.PNG!

3. Add close button to library installation error popup.

!library_installation_error_popup.png!",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-07 14:36:29,10
13214381,"Change actions block position in ""Manage Git Credentials"" popup","1. Buttons ""Cancel"" and ""Apply changes"" in the ""Manage Git Credentials"" popup should be in the bottom of popup in case there is information about some users in the table with users.

2. If put buttons in the bottom of popup should NOT be extra space between account and buttons.

!man_git_button_to_bottom.png!

3. Buttons ""Cancel"" and ""Apply changes"" in the ""Manage Git Credentials"" popup should not be displayed in popup in case the table with users are empty.

4. Add 'Add account' button in the left of top popup as it is done for 'Manage roles'

5. If click 'Apply changes' after creating or updating existance error handling should apper 
",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-02-07 14:24:01,10
13214191,Replace error handling component ,"Current error handling components are not supported.
Replace Current error handling components by another (which are supported).",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-06 15:49:37,10
13214188,Public.key name should not be beyond of 'Create initial infrastructure' popup,"*Preconditions*:

1. User is logged into DLab Web Application

2. ""Create initial infrastructure"" popup is open

 

*Steps to reproduce:*
 # Click 'Upload' button
 # Choose  public.key with too long name
 # Click 'Create' button

*Actual result*: 

Public.key is shown beyond of 'Create initial infrastructure' popup.

*Expected result*: 

Public.key is not shown in 'Create initial infrastructure' popup.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-02-06 15:42:49,10
13214175,Fix issue with Route Table for user subnet,"DLab create new route table for each user subnet. 

This route table doesn't have route to VPC Peering",AWS Debian DevOps,['DataLab Old'],DATALAB,Task,Major,2019-02-06 15:23:27,9
13214174,Replace picker date component on 'Billing report' page,"Current picker date component has been not supported yet.
Find new one and replace picker date component by another.",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-06 15:20:26,10
13214160,Upgrade Angular version and related dependency,For now angular version is 5.2.0. Update angular to v. 7.x.x.,AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Task,Major,2019-02-06 14:06:05,10
13213862,[AWS]: Investigate why extra localhost.localdomain is added on Notebook/Edge,"On Notebook/Edge extra localhost.localdomains are added and duplicated on AWS:
 !extra local.png! 

But should be as on other clouds:
 !as expected.png! 
",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-05 10:33:39,9
13213686,Implement possibility to interrupt library installation on instance from DLab UI,"As a user I want to interrupt library installation so that I can install the other library in case of hanging of previous library installation.

Acceptance criteria:
1. There is ""Cancel"" button near ""Install"" one.
2. If user hit ""Cancel"" button the installation will be interrupted and the next library installation will be enabled.",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2019-02-04 13:16:48,13
13213676,Add sparkmagic to Jupyter,"As a user I want to use spark magic so that I can interactively running Spark code in multiple languages, as well as some kernels that I can use to turn Jupyter into an integrated Spark environment.
----
View the following link about sparkmagic:
 [https://github.com/jupyter-incubator/sparkmagic/blob/release/examples/Spark%20Kernel.ipynb]

 

Github issue: [https://github.com/apache/incubator-dlab/issues/750]

 ",AWS AZURE Debian DevOps GCP Github_issue RedHat,['DataLab Old'],DATALAB,New Feature,Major,2019-02-04 12:47:13,2
13213661,Deploy all DLab instances of the same SSN in one availability Zone,"SSN and edge instances have Availability Zone ""us-west-2c"" while notebook instances have Availability Zone ""us-west-2a"".",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-02-04 11:16:22,17
13213641,Add waiter for Nexus service,"In order to not use try - except, waiter should be added to get status of Nexus service",AWS Debian DevOps,['DataLab Old'],DATALAB,Task,Major,2019-02-04 09:41:16,9
13212818,Unable to resolve host localhost.localdomain on notebook,"The bug was found on Jupyter, Apache Zeppelin/Rstudio notebooks on Azure. Sometimes the bug is produced after notebook starting (if notebook was previously stopped)

*Preconditions:*
 Apache Zeppelin is stopped

*Steps to reproduce:*
 1. Start Apache Zeppelin
 2. Go to Apache Zeppelin UI

*Expected result:*
 Apache Zeppelin UI is available

*Actual result:*
 Apache Zeppelin UI is not available

!fault1.png!

 

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-30 13:35:09,9
13212601,Upload key is offered for previous login user NOT current,"*Preconditions:*
 1. for user1 edge is created
 2. for user2 edge is not created

*Steps to reproduce:*
 1. log in with user2
 2. log out with user2
 3. log in with user1
  

*Actual result:*
 upload key is offered for user1
  

*Expected result:*
 upload key is NOT offered for user1, because edge is already created for the user1

 

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-29 16:48:39,10
13211847,[Volodymyr][Documentation]: Solution architecture diagram for GCP,Assign to Volodymyr Veres,DevOps GCP GCP_demo,['DataLab Old'],DATALAB,Task,Major,2019-01-25 14:01:11,11
13211815,[GCP]: Dataproc creation fails on process of downloading jars on Jupyter and RStudio,"*Preconditions:*
1. Jupyter or RStudio is created

*Steps to reproduce:*
1. create Dataproc on Jupyter or Rstudio

*Actual result:*
Dataproc creation fails 

!DataProc.PNG!

*Expected result:*
Dataproc creation is successful",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Major,2019-01-25 12:10:28,17
13211813,[Autotest]: Playbook running for Apache Zeppelin fails due to using of wrong name of EMR kernel,"A bug was found on AWS [RedHat]

 

*Steps to reproduce:*

1. Run Autotest for Apache Zeppelin using EMR

 

*Actual result:*

Playbook running fails

!Autotest.PNG!

*Expected result:*

Playbook runs successfully

 ",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-25 12:04:46,8
13211810,[Tech debt]: Refactor a code for appropriate Apache Toree kernel configuring for Jupyter notebook,"1. Delete toree from sources:

1.1./infrastructure-provisioning/src/general/files/os/toree_kernel.tar.gz

1.2./infrastructure-provisioning/src/general/files/os/toree-assembly-0.2.0.jar

 

2. Use toree from public repository

 

 

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-25 11:58:31,8
13211787,[GCP][Spark Standalone cluster]: Playbook running fails using spark cluster kernel due to 'Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found',"*Steps to reproduce:*
 # Run autotest on GCP for Data Engine or Create Spark cluster on Jupyter/Zeppelin/Rstudio and run playbook by manual

 

*Actual result:*

playbook running fails with error:

!GCP_autotest.PNG!

[AutoTests_GCP/91/console|http://35.166.222.81/view/AutoTests/job/AutoTests_GCP/91/console]

*Expected result:*

Autotest runs successfully",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Critical,2019-01-25 10:25:42,8
13211760,After reconfiguration local spark (Notebook) spark.jars should not be duplicated,"*Preconditions:*
Notebook is created

*Steps to reproduce:*
 1. reconfigure local spark with any valid values

*Actual result:*
 the same spark jars are duplicated

!jars.PNG!

 

*Expected result:*
 the same spark jars are not duplicated

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-25 08:50:16,9
13211709,[GCP]: DeepLearning creation Fails on configure.py process,"*Preconditions:*

1. Edge is created

 

*Steps to reproduce:*

1. Create Deeplearning

 

*Actual result:*

Deeplearning creation fails with error:

!DeepLearning creation.PNG!

*Expected result:*

DeepLearning is created

 

 

Note: Now by date 14-01-2019 Deeplerning creation fails on stage:

!DeepL.png!",Debian DevOps GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2019-01-25 01:54:48,17
13211701,[GCP]: Autotest fails due to can not uploading data to personal bucket,"*Steps to reproduce:*
1. Run autotest for GCP

*Actual result:*
Autotest fails with error:
*Fatal error: local() encountered an error (return code 1) while executing 'gsutil -m cp /tmp/airports.csv gs://it63-dlab-test-bucket/rstudio_dataset/*

[job/AutoTests_GCP/111/|http://35.166.222.81/job/AutoTests_GCP/111/console]

*Expected result:*
 Autotest runs successfully

 ",Debian DevOps GCP,['DataLab Old'],DATALAB,Bug,Major,2019-01-25 01:27:38,8
13211700,First rendering of notification dialogs works with delay,"A bug reproduces only at first time or after clear browsing data.

*Precondirions:*
 1.Edge is running
 2. 'Manage environment' popup is open

*Steps to reproduce:*
 1. Click 'stop' or 'terminate' for running environment

*Actual result:*
 Name of user environment displays with delay 
 Please, see attachment

!Environment.PNG!

*Expected result:*
 Name of user environment displays simultaneously with dialog window

!Environment 2.PNG!
 _____________________________________________________________________

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-25 01:19:31,10
13211699,Create image should be disabled during installing lib,"Please, prevent possibility to create custom image in case if library is in installing status.",AWS AZURE Front-end GCP RedHat debian pull-request-available,['DataLab Old'],DATALAB,Task,Minor,2019-01-25 01:13:25,0
13211693,[Azure][Integration test]: Add '+1' in SBN for every new created SSN,"The SSN will NOT be created if run integration test and SSN exists from previous integration test. SSN is not created due to using previously SBN.

 ",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-25 00:59:56,13
13211692,Increase rating of DLab promotion page,[http://dlab.opensource.epam.com/],AWS AZURE Front-end GCP,['DataLab Old'],DATALAB,Task,Major,2019-01-25 00:57:26,10
13211691,[Manage roles]: Add checkbox for check off/uncheck all boxes for role items,"As a user I want to have possibility to check off/uncheck all boxes for role, so that I can do that zction by one click onstaed of to check off/uncheck every boxes.

 

*Acceptance criteria:*

*1.* extra check box should be for hecking off/unchecking all boxes

*2.* if user checks off all boxes are checked off for role

*3.* if user unchecks off all boxes are unchecked for role",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Improvement,Minor,2019-01-25 00:55:27,10
13211687,"[Add computational resources]: Extra vertical scrollbar if check off the ""Cluster configurations"" check box and error message is overlapped","*Preconditions:*
 1. ""Add computational resources"" popup is open

*Steps to reproduce:*
 1. choose Spark Standalone in ""Select cluster type"" drop down list
 2. check off ""Cluster configurations"" check box

*Actual result:*
 During expanding a text box appears/disappears extra vertical scrollbar

*Expected result:*
 There is no extra vertical scrollbar during expanding a text box 
 ------------------------------------------------------------------------------------------------------

*Preconditions:*
 1. ""Add computational resources"" popup is open

*Steps to reproduce:*
 1. choose AWS EMR cluster in ""Select cluster type"" drop down list
 2. uncheck ""Spot instance"" check box
 3. put not valid cluster alias in text box ""test 1""

*Actual result:*
 error message overlaps ""Spot instance"" check box
 !EMR error.PNG!

*Expected result:*
 error message does not overlap ""Spot instance"" check box.

For example, you can increase distance between ""Cluster alias"" text box and ""Spot instance"" check box
 *OR*
 to put error message in one row

 ",Debian Front-end RedHat,[],DATALAB,Bug,Major,2019-01-25 00:49:00,10
13210984,[Autotest][AWS][GCP][Zeppelin]: Autotest running fails on step of establish a new connection,"*Steps to reproduce:*
 # Run integration test for Zeppelin on AWS/GCP

 

*Actual result:*

Integration test fails with error

('Error!', ""HTTPConnectionPool(host='172.31.99.176', port=8080): Max retries exceeded with url: /api/notebook/import (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1740919b10>: Failed to establish a new connection: [Errno 111] Connection refused',))"")

AutoTests_AWS/299
AutoTests_AWS/278

AutoTests_GCP/122

*Expected result:*

Integration test runs successfully",Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-22 14:20:24,8
13210918,[Manage libraries]: Adjust error message for Java dependencies,"*Preconditions:*

 1. Environment is created

2. 'Manage libraries' popup is open

 

*Steps to reproduce:*

 1. Choose 'java' package

2.  Put wrong library format in text box: ch.exense. <step:selenium-plugin-DEf::3.8.0>

3. Put library which is not available: <ch.exense.step:selenium-plugin-def:3.3.0>

 

*Actual resul*t:

1. *query param artifact Wrong library name format. Should be <groupId>:<artifactId>:<versionId>. E.g. io.dropwizard:dropwizard-core:1.3.5*

2. *Artifact with id=selenium-plugin-DEf, groupId=ch.exense.step and version 3.8.0 not found""*

 

 

 

*Expected result:*
 # *Wrong library name format. Should be <groupId>:<artifactId>:<versionId>*
 # *No matches found*",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-22 09:33:31,6
13210388,[Azure]: Playbook running on Apache Zeppelin fails due to impossible connection to blob via wasbs protocol,"*Preconditions:*
 Zeppelin is created

*Steps to reproduce:*
 1. run playbook on Apache Zeppelin

*Actual result:*
 playbook running fails due to impossible connection to blob via wasbs protocol

 

*Expected result:*
 playbook runs successfully",2.3_old AZURE Debian DevOps Known_issues(release2.2) Known_issues(release2.3) Known_issues(release2.4) RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-18 14:05:29,2
13210153,The warning updates for user only after second login if admin adjusts the quota,"*Preconditions:*
1. two users have environments
2. quota is set for budjet (user has already used his quota)

*Steps to reproduce:*
1. update quota (increase limit)

*Actual result:*
1. after the first login the warning is not updated (it is previous quota value)
2. after the second login the warning is updated

!warning.PNG!

*Expected result:*
after the first login the warning is updated

*****************************************************************************
Rename button from 'Create' to 'Apply' on 'Manage environment' popup",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-17 13:13:51,10
13210112,[Spark reconfiguration]: Add validation for Spark parameters from UI side,"During Spark reconfiguration user can enter wrong parameters for spark, please investigate how it can be validated.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,New Feature,Major,2019-01-17 09:56:19,0
13210111,[GCP]: Add possibility to create Notebook/ Data engine from images,"As a user I want to create Notebook/Spark cluster standalone from image on GCP

*Acceptance criteria:*
1. during notebook creation creates shared image which can be used to create the next notebook of the same template
2. Spark cluster standalone creation use Notebook's image",Debian DevOps GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,New Feature,Major,2019-01-17 09:52:34,17
13210110,Investigate usage of Elastic IPs on AWS account,"We have limited Elastic IPs on AWS account. 

Investigate how increase the quantity.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-17 09:51:40,9
13210090,[New UI] for 'Manage Roles' page,"1. Components for user ('Manage Roles' page) should be look like components for libraries ('Manage Libraries' page)

2. 'Role' drop down list should be above adding users

3. Values of 'Role' drop down list is offset to left side, but should be equally under 'Role' drop down list

4. In ""Role"" drop down list value is cropped. Add a hint if hover the mouse on value in ""Role"" drop down list.

5. If user is not allowed to create computational resource an information message appears on pop-up. ""Cancel"" and ""Create"" buttons are extra buttons in this case, please remove them from pop-up.",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-17 08:41:49,10
13210089,"[Quota]: Error message overlays on ""Stop"" ""Terminate"" icons only for Microsoft Edge and Firefox browsers","*Preconditions:*
 ""Manage environment"" popup is open in Microsoft Edge or Firefox browsers

*Steps to reproduce:*
 1. put negative value in text box, for example <-9>

*Actual result:*
 error message overlays on ""Stop"" ""Terminate"" icons only for Microsoft Edge and Firefox browsers

*Expected result:*
 error message is under text box (as in Chrome browser )",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-17 08:37:48,10
13210087,[Manage environment]: Investigate how to convey that user's environment is terminated or disable icon for termination in this case,If all user's instances are in terminated/terminating statuses the icon for termination is still active but should be disabled on Manage environment popup.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Minor,2019-01-17 08:35:11,13
13210086,[Manage Libraries]: Should appear the vertical scroll bar if increase zoom starting from 100% and more,"*Preconditions:*
 1. ""Manage Libraries"" popup is open
 2. Dispay setting is the next :
!image-2019-01-17-10-32-31-571.png!
!image-2019-01-17-10-32-40-240.png!
  

*Steps to reproduce:*
 1. Increase zoom startin from 100%

*Actual result:*
 1. ""Close"", ""Create"" buttons are beyond of display

*Expected result:*
 Appears vertical scroll bar in the right side in order to scroll down to ""Close"", ""Create"" buttons",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-17 08:34:03,10
13210085,Possibility to execute SS schedulers in cluster mode,"MERGED INTO DEV, but not yet tested because currently we have just 1 ssn node",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-17 08:31:42,13
13210084,API for updating user group,Update API for user group,AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-17 08:29:41,13
13210083,[Zeppelin]: Memory should be allocated for spark based on notebook instance shape,"*Preconditions:*
Spark standalone is created on zeppelin

*Steps to reproduce:*
1. run playbook using remote/local spark

*Actual result:*
1024 MB is used per memory executor (is less than 75% from total)

*Expected result:*
75% are used per memory executor
",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-17 08:27:00,9
13210081,[Manage Environment]: Extra space between user and total budget,"*Preconditions:*
1. Edge for one user is created
2. ""Environment Health Status"" page is open

*Steps to reproduce:*
1. click ""Manage Environment"" button

*Actual result:*
distance between user and total budget is too extra

!Manage environment.PNG!

*Expected result:*
decrease distance between user and total budget",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-17 08:23:43,10
13209966,[Scheduler reminder]: Extra space in word,"*Preconditions:*
Schedulers are set more than for one notebook fro stopping at tne same time

*Steps to reproduce:*
1. wait untill remain 15 minutes for stopping

*Actual result:*
Appears Scheduler reminder on which the word servers is written ""server s""


*Expected result:*
Appears Scheduler reminder on which the word servers is written ""servers""",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-16 16:38:53,10
13209965,[Manage roles]:Adding user to group fails if any user has not been added yet to the group,"*Preconditions:*
 1. group is created without adding users
 2. ""manage roles"" popup is open

*Steps to reproduce:*
 1. add user to existing group

*Actual result:*
 1. user is not added to group

!image-2019-01-16-18-35-32-505.png!

*Expected result:*
 1. user i added to group",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-16 16:36:28,13
13209964,[Manage roles]: Distances between buttons should be equal Create/Cancel/Back,"*Preconditions:*
1. ""Manage roles"" popup is open

*Steps to reproduce:*
1. click ""Add group"" button
2. click iscon ""3 users""

*Actual result:*
Distances between buttons Create/Cancel/Back are different 
 !image-2019-01-16-18-34-02-709.png! 

Expected result:
Distances between buttons Create/Cancel/Back are equal",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-16 16:34:28,10
13209963,[Apache Zeppelin]: Add info concerning change of spark default configuration from DLab UI,"Currently Spark default configuration can be change directly through Apache Zeppelin interpreter menu NOT from DLab UI.

Please, add this infor nearby configuration textbox fro Apache Zeppelin

Link for release notes: https://docs.google.com/document/d/15iJkog3v4SEX76Y2ODaIQ1LtDclKNxUYxBEnXFFen5o/edit#",Debian Front-end RedHat,[],DATALAB,Task,Major,2019-01-16 16:32:43,10
13209961,Correct setting of executor and driver memory for Zeppelin,"For local sparks we need to set driver memory and for remote sparks - executor memory

Also, we should open port ranges 4040-4045 in order to see several spark UIs",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-16 16:31:45,9
13209960,Allow port range 4040-4045 for notebooks,"For every Spark context, we have Spark UI located on ports 4040 - ...

In order to have ability to access UIs, we should add appropriate rules to SG",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-16 16:30:48,9
13209959,[AWS][GCP][Data Engine Service]: RStudio stopping fails on process of removing dataengines kernel,"*Preconditions:*
 1. EMR is created on RStudio

*Steps to reproduce:*
 1. stop Rstudio

*Actual result:*
 1. EMR is terminated
 2. RStudio is failed on DLab UI but from AWS console is running !image-2019-01-16-18-28-49-559.png!

!image-2019-01-16-18-28-49-559.png!

*Expected result:*
 1. EMR is terminated
 2. RStudio is stopped

The same situation is on GCP if Dataproc is created on RStudio",Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-16 16:29:39,9
13209958,[Confirmation dialog]: Prevent cluster header in case of notebook termination and no cluster is available on notebook,"*Preconditions:*
1. notebook is in running status
2. cluster is not deployed on notebook or is only in terminated status

*Steps to reproduce:*
1. click action menu for notebook
2. choose terminate notebook

*Actual result:*
confirmation dialog appears with cluster head
 !image-2019-01-16-18-24-40-464.png! 

*Expected result:*
confirmation dialog appears without cluster head",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2019-01-16 16:26:01,10
13209956,[Zeppelin]: Warning info concerning change of Spark default configuration should be only in some steps of actions for Zeppelin/clusters,"h4. *For Apache Zeppelin*
Warning info concerning change of Spark default configuration remove if:
 - Zeppelin is in creating/terminating/terminated/stopping/stopped/creating Image statuses

*For Data Engine*
Warning info concerning change of Spark default configuration remove if:
 - Data Engine is in in creating/terminating/terminated/stopping/stopped statuses

*For Data Engine Service*
Warning info concerning change of Spark default configuration remove if:
 - Data Engine Service is in creating/terminating/terminated/running statuses",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-16 16:23:09,10
13209955,[AWS]: Add parameter CIDR for the second VPC,"Currently parameter CIDR for the second VPC is absent.
Please add it.",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-16 16:20:59,17
13209954,Deeplearning creation fails on step of installing cntk,"*Preconditions:*
Edge is created

*Steps to reproduce:*
1. create deep learning

*Actual result:*
creation Deep learning fails on step of installing cntk==2.6
  !image-2019-01-16-18-18-20-337.png! 


*Expected result:*
Deep Learning is created",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2019-01-16 16:18:48,17
13209953,[Billing Report]: 'Creating Image' should be the same style as on the other pages,"*Preconditions:*
Notebook is created

*Steps to reproduce:*
1. create image from Notebook
2. go to 'Billing reports'

*Actual result:*
 !image-2019-01-16-18-13-02-712.png! 
 !image-2019-01-16-18-13-39-451.png! 

*Expected result:*
 !image-2019-01-16-18-13-29-517.png! 

 ",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Trivial,2019-01-16 16:15:04,13
13209951,[AWS][EMR]: Add link to AWS documentation,"Add the following text
To view example JSON of configurations refer for AWS official documentation [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html]",AWS Debian Front-end RedHat,['DataLab Old'],DATALAB,Task,Major,2019-01-16 16:11:16,10
13205907,Update  Apache Zeppelin version,"Now we use v.0.8.0

Link to release notes: [https://issues.apache.org/jira/secure/ConfigureReleaseNote.jspa?projectId=12316221&version=12341035]

*******************************

By date 25/01/2019 version of Apache Zeppelin version is 8.0.0

Update to version 8.1.0 (in this version job url track is fixed)",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-21 09:14:20,3
13205903,Reverse proxy for Notebook,"As a user I want to go to notebook UI via reverse proxy so that I can not open tunnel for Edge and set foxyproxy on MS Azure/GCP.

 

*Acceptance criteria:*
 # It should be link for notebook UI using reverse proxy
 # if user clicks a link for notebook UI using reverse proxy a new tab will open and user have to put DLab username and password. If DLab username and password are correct notebook UI page will open",AZURE Debian GCP_demo RedHat,['DataLab Old'],DATALAB,New Feature,Major,2018-12-21 09:06:59,9
13205724,[Manage Libraries]: Should appear the vertical scroll bar if increase zoom starting from 100% and more,"*Preconditions:*
1. ""Manage Libraries"" popup is open
2. Dispay setting is the next :

!image-2018-12-20-15-37-29-223.png!

!image-2018-12-20-15-37-54-325.png!

 

 

*Steps to reproduce:*
1. Increase zoom startin from 100%

*Actual result:*
1. ""Close"", ""Create"" buttons are beyond of display

*Expected result:*
Appears vertical scroll bar in the right side in order to scroll down to ""Close"", ""Create"" buttons",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-20 13:38:51,10
13205675,[Need discussion][DevOps]: Add support of local repository for DataLab,"As a user I want to create instance from local repository, so that I can prevent some random issues during creation.
----
1. It should be the single source from which changes will be performed.

2. Go and distribute all traffic through this sandbox

Github issue: [https://github.com/apache/incubator-dlab/issues/737]
----
[~vveres] should we store just our docker-images & jars or all software and libraries we need for deployments?",AWS AZURE Debian DevOps GCP Github_issue RedHat,['DataLab Old'],DATALAB,New Feature,Major,2018-12-20 09:27:28,2
13205673,[New UI] for 'Manage Roles' page,"*Preconditions:*
 # Environment is created
 # 'Manage Roles' page is open

*Steps to reproduce:*

1.  Click '$anyuser' group

2. Click 'Select role' drop down list

*Actual result:*

Values are not grouped by roles

!image-2018-12-20-11-21-35-824.png!

*Expected result:*

Values are grouped by roles

For example:

notebook instance shape management
 notebook management
 cluster instance shape management
 cluster management
 admin page management",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-20 09:22:26,10
13205672,[Quota]: After updating quota values the corresponding warning appears or does not appear only after the second the log in,"*Preconditions:*
1. environment is created
2. billing is available
3. values for quota are set

*Steps to reproduce:*
1. open ""Manage environment popup""
2. set new values for quota. If quota is reached set values more than reached quota

*Actual result:*
warning appears from the first attemt of login
warning does not appear from the second attempt of login and the next logins

*Expected result:*
warning does not appear from any attempt of login",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 09:20:03,10
13205671,[Quota]: After logging warning is absent if overquota (per user and total budget) is 200% and more,"*Preconditions:*
1. environment is created
2. billing is available
3. manage environment popup is open

*Steps to reproduce:*
1. Set such value for user Quota or total billing which is more than 200% of used amount

*Actual result:*
warning is absent

!image-2018-12-20-11-18-09-202.png!

*Expected result:*
appears warning

!image-2018-12-20-11-18-29-372.png!",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 09:18:42,10
13205669,Add a local git hook script for checking commit message,"As a DLab contributor I want to use local git hook script in order to restrict commits different than the format proposed

Acceptance criteria:

1. local git hook script is created
2. instruction is written how to install script
3. if commits are different than the format proposed a user see appropriate error message and commits are not allowed to do
4. if commits are the same as the format proposed so user is allowed to commit",AWS AZURE Back-end GCP,['DataLab Old'],DATALAB,Task,Major,2018-12-20 09:12:08,13
13205668,[Software Versions upgrade]: Update 3-d party software to new stable versions,"As a user I want to use stable and one of the latest tools versions.

 

Please, review, investigate and update necessary versions:
1. *TensorFlow:* used TensorFlow v.1.4 / updated TensorFlow v.1.5 
2. *Jupyter:* used Jupyter v.5.2.0 / updated Jupyter v.5.5.0 
    2.1. *Jupyter with TensorFlow* used Jupyter v.5.2.0, TensorFlow v.1.4
3. *RStudio:* used RStudio v.1.1.383 / updated RStudio v.1.1.423 
    3.1 *Rstudio with TensorFlow* used RStudio v. 1.1.383 and TensorFlow 1.8
4. *Zeppelin:* used Zeppelin v.0.7.3 / updated Zeppelin v.0.8.0
5. *EMR:* used EMR v.5.6.0  and EMR v.5.12.0 / updated EMR v.5.15.0

6. *Spark* used v.2.2.1 / updated v.2.3.1
7. *Scala:* used Scala v.2.11.8 / updated Scala v.2.12.16
8. *CNTK:* used CNTK v.2.1.0 / updated CNTK v.2.5.1
9. *Keras:* used Keras v. 2.0.8 / updated Keras v. 2.2.0
10. *Spark* used v. 2.2.1 / updated v. 2.3.0

11. *MXNET*    used v.1.0.0 / updated v.1.2.0

12. *Theano* used v.0.9.0 / updated v.1.0.2

13. Update *ungit*

 

You can use this documentation: *[https://docs.google.com/spreadsheets/d/1HqxiRI9IE8Emzyi0xxSrCkuzbEWRxS_OLqC_McCO6yk/edit#gid=0]*",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-20 09:08:13,17
13205666,[Quota]: percentages for user quota and total budget are calculated wrong (percentages are less than actually),"*Preconditions:*
1. environment is created
2. billing is available

*Steps to reproduce:*
1. look what is used billing per user
2. set quota per user
3. calclulate manually using formula used (per user billing /(divide) set quota per user)*100%
4. logout DLab
5. login DLab using F12

*Actual result:*
in f12 used quota per user (in percentage) is less than your percentage (calculated by manually)

*Expected result:*
in f12 used quota per user (in percentage) is equal your percentage (calculated by manually)",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-20 09:05:47,13
13205664,[Quota]: Add error message for Notebook/Spark Standalone cluster starting if resource quote is reached,"*Preconditions:*
1. user's environment is stopped except for Edge node
2. user's quato/total budget is reached
3. user is logged in DLab

*Steps to reproduce:*
1. go to ""resource list"" page
2. click action menu for notebook
3. choose start action

*Actual result:*
Error message is absent

*Expected result:*
Appears error message:
Should be the same error message as for Edge, except for ""Edge""
""Oops!
Instance running failed""

*or*

""Oops!
Instance running failed. Resource quote is reached""

*or*

""Oops!
Operation can not be finnished. Resource quote is reached""

Currently error message is absent for starting notebook and computational resources",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-20 09:02:10,10
13205662,[Quota]: error message should be in one style,"*Preconditions:*
environment is created

*Steps to reproduce:*
1. click ""Manage environment"" button
2. put <4> for user quota
3. put <2> for total budget
4. put <3> for user quota

*Actual result:*
error messages have different style

*Expeted result:*
error messages have the same style

Error messages should be not bold",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:59:56,10
13205656,[Billing Report]: User's value in dropdown list should be in lower case,"*Preconditions:*
1. environment is created
2. billing is available
3. ""Billing Report"" page is open

Steps to reproduce:
1. click ""select user"" drop down list

*Actual result:*
User's value is in lower and upper case

!image-2018-12-20-10-48-01-520.png!

 

*Expected result:*
User's value is in lower case",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-20 08:48:07,10
13205655,"[GCP]: Error message is beyond of ""Add computational resources"" popup","*Preconditions:*
1. Notebook is created
2. ""Add computational resources"" popup is opne

*Steps to reproduce:*
1. choose Dataproc cluster in ""Select cluster type"" drop down list
2. check off ""Preemptible node count"" check box
3. enter not valid vulue for ""Preemptible node count""? for example 23

*Actual result:*
error message ""Only integer values greater than or equal to 0 and less than 11 are allowed"" beyond of ""Add computational resources"" popup

!image-2018-12-20-10-39-28-981.png!

*Expected result:*
error message ""Only integer values greater than or equal to 0 and less than 11 are allowed"" appers under ""Preemptible node count"" text box",Debian Front-end GCP,['DataLab Old'],DATALAB,Bug,Minor,2018-12-20 08:46:17,10
13205653,[Azure]: Custom parameters are not written in spark-defaults.conf if create custom Spark Standalone (NOT via regonfiguration),"The bug was found on Azure on Jupyter.

Preconditions:
1. Notebook is created

Steps to reproduce:
1. Create custom Spark Standalone:
[
{
""Classification"": ""spark-defaults"",
""Properties"":

{ ""spark.reducer.maxSizeInFlight"": ""48m"" }

}
]

*Actual result:*
Custom Spark Standalone is created as default Spark Standalone

!image-2018-12-20-10-37-08-978.png!

*Expected result:*
Custom Spark Standalone is created with custom parameters",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:37:26,13
13205652,Reconfiguration local spark on notebook deletes paths to jars in spark-defaults.conf,"*Preconditions:*
1. Spark Standalone is created on Notebook

*Steps to reproduce:*
1. reconfigure local spark on Notebook:
""spark.maxRemoteBlockSizeFetchToMem"": ""Int.MaxValue - 512""

*Actual result:*
paths to jars are deleted in spark-defaults.conf

!image-2018-12-20-10-35-10-904.png!

!image-2018-12-20-10-35-25-732.png!

*Expected result:*
paths to jars are NOT deleted in spark-defaults.conf",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:35:27,9
13205651,Spark Standalone reconfiguration fails if spark name is a part of another spark name on one notebook,"h4. Description
When there more than 1 spark cluster created with similar name (e.g. spa, spark) on on notebook action reconfigure for spark with name ""spa"" fails

*Preconditions:*
1. two Spark Standalones are created on one notebook with names ""spa"" ""spark""

*Steps to reproduce:*
1. reconfigure Spark Standalone with name ""spa"": ""spark.maxRemoteBlockSizeFetchToMem"": ""Int.MaxValue - 512""

*Actual result:*
Spark Standalone reconfiguration fails

*Expected result:*
Spark Standalone reconfiguration is successful",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:33:23,9
13205649,"[Firefox][Microsoft Edge]: ""Resources list"" page is blocked if choose any actions ""Create new""/""Add compute""/""Scheduler""/""Create AMI""/""Manage libraries""/""Git credentials""...","*Preconditions:*
Dlab main page is open in FireFox  or Microsoft Edge browsers

*Steps to reproduce:*
1. Click ""create new"" button
 
*Actual result:*
1. ""Create analytical tool"" popup is not open
 2. ""Resources list"" page is blocked 

!image-2018-12-20-10-31-50-462.png!

*Expected result:*
""Create analytical tool"" popup is open",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:31:54,10
13205648,Spark Standalone creation (custom/default) fails on Zeppelin,"A bug was found on AWS/azure during Spark Standalone creation on Zeppelin

*Preconditions:*
1. Zeppelin is created
2. ""Resources list"" page is open

*Steps to reproduce:*
1. click action menu for Zepplin
2. choose ""Add compute""
3. choose Spark Standalone in ""Select cluster type"" drop down list
4. enter valid name for Spark Standalone in ""Cluster alias"" text box
5. click ""Create"" button

*Actual result:*
Spark Standalone creation fails:

!image-2018-12-20-10-28-26-651.png!

*Expected result:*
Spark Standalone creation is successful

Repeat steps from 2-5 for creting custom Spark Standalone",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:28:31,9
13205647,"Autofocus should be on Cluster configuration template, JSON"" after checking off ""cluster configurations"" check box""","As a user I want to have autofocus on ""Cluster configuration template, JSON"" text box if I check off ""custer configuration"" check box

*Acceptance criteria:*

1. if user checks off ""custer configuration"" check box a scrollbar goes down by automatically in front of ""Cluster configuration template, JSON"" text box",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-20 08:26:53,10
13205646,[Azure][400: Bad Request]: It is impossible to login DLab only via HTTPS and HOSTNAME,"*Preconditions:*
SSN is created on Azure

*Steps to reproduce:*
1. use link to login DLAB via HHTPS and HOSTNAME

*Actual result:*
1. user is NOT logged in DLab
2. appears ""400: Bad Request""

!image-2018-12-20-10-25-38-481.png!

*Expected result:*
user is logged in DLab",AZURE Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:25:40,17
13205645,Creation only the first notebook of every template fails due to AMI_ID does not exist,"A bug was found on AWS. It is reproduced if create only the first notebook of every template. The next Notebook creations of such template will be successful.

*Preconditions:*
1. Edge is created
2. ""Resources list"" page is open

*Steps to reproduce:*
1. click ""Create new"" button
2. select any notebook from template drop down list
3. put valid name for notebook in name text box
4. click ""Create"" button

*Actual result:*
Notebook creation fails with eror: *""(InvalidAMIID.NotFound) when calling the DescribeImages operation: The image id '[ami-082df6e756a82ab43]' does not exist""

!image-2018-12-20-10-22-41-581.png!

*Expected result:*
Notebook is created",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:22:43,17
13205644,"""Apply"" button is not active after reopening ""Backup options"" popup if all backup options were unchecked previously","*Preconditions:*
1. environment is created
2. ""health status page"" is open

*Steps to reproduce:*
1. click ""Backup"" button
2. uncheck all options
3. Close ""Backup options""
4. click ""Backup"" button again

*Actual result:*
""Apply"" button is not active

*Expected result:*
""Apply"" button is active",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:20:37,10
13205643,[Azure]: Login DLab fails,"*Preconditions:*
 # Edge is created
 # DLab Login page is open

 

*Steps to reproduce:*
 # enter valid user name in text box for username
 # enter valid  password in tex box for password
 # hit ""Login"" button

 

*Actual result:*
 # user is not logged into DLab
 # error message appears:

!image-2018-12-20-10-19-12-977.png!

 

!image-2018-12-20-10-19-33-703.png!

 

*Expected result:*

user is logged in DLAB

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Blocker,2018-12-20 08:19:35,9
13205642,"Information message should be center aligned on ""Manage git credentials"" popup","*Preconditions:*
1. environment is created
2. git credential is not set up in Dlab

*Steps to reproduce:*
1. click ""Git credentials"" button
2. click ""List"" tab

*Actual result:*
""You have no related accounts"" is left aligned

!image-2018-12-20-10-17-31-101.png!

*Expected result:*
""You have no related accounts"" center aligned",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-20 08:17:32,10
13205641,Apache Zeppelin template is absent on DLab UI,"*Preconditions:*
1. Edge is created
2. ""Resource list"" page is open

*Steps to reproduce:*
1. click ""Create new"" button
2. click ""Select template"" drop down list

*Actual result:*
Apache Zeppelin template is absent

!image-2018-12-20-10-15-22-905.png!

*Expected result:*
Apache Zeppelin template is present",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-20 08:15:25,13
13205640,EMR configuration fails due to of limited quantity nginx restart per 10 seconds,"*Preconditions:*

Environment is created

 

*Steps ro reproduce:*

1. Create EMR with seven or more nodes

 

*Actual result:*

EMR configuration fails

 

*Expected result:*

EMR is created",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-20 08:13:46,18
13205635,"Investigate how to set up quota for user if all users instances are not in ""running"" status","As an admin I want to configure quota for user whose instance is not in ""running"" status, so I can limited the particular existed user in spite of instances' status.

 

*Acceptance criteria:*

*1.* admin can configure quota for existed user  whose instance is not in ""running"" status",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-20 08:10:08,13
13205634,[Azure][AWS]: Integration test fails to execute goal org.apache.maven.plugins:maven,"*Steps to reproduce:*

1. Run autotest for Azure

 

*Actual result:*

Autotest fails with error:
*Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.17:test failed*
[http://35.166.222.81/job/AutoTests_Azure/196/console]

[http://35.166.222.81/job/AutoTests_AWS/265/console]

 

*Expected result:*

1. Autotest runs successfully

 ",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:08:14,13
13205633,Extra VPC addes during SSN creation with parameters of one VPC,"*Steps to reproduce:*

1. Create SSN with one VPC

 

*Actual result:*

Two VPCs are created

 

*Expected result:*

Only one VPC is created",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-20 08:07:07,17
13205581,UI issues for 'SSN Monitor' page,"*Preconditions:*
 # Environment is created
 # 'SSN Monitor' page is open

 

*Steps to reproduce:*

1.  Click 'HDD' tab

 

*Actual result:*

Values are not grouped by roles

Header name is 'Disk 1'

*Expected result:*

Header name is 'Storage'

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 22:19:36,10
13205578,[Java dependencies]: Add validation for installed library,Validation should be only for name of  installed library not for different versions.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 22:15:18,10
13205576,[Azure][GCP]: Provide mock/sub for creating two VPCs,"Now, implemented 2 VPC's for AWS, so that SSN creation fails for GCP/Azure.

 

[http://35.166.222.81/view/v2.1-Azure/job/Azure_Create_environment-develop/87/consoleFull]",Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 22:13:44,17
13205574,Investigate what action should be for user if edge is in 'failed' status,"Case:
 # Edge status is 'failed'
 # any instance is not in 'running' status

In this case user is blocked, so that he can not delete or recreate edge via DLab UI.

Please, investigate what action should be for user if edge is in 'failed' status",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 22:11:30,13
13205573,[Admin page]: Ability to choose certain instance shapes for computational resources per group via Dlab UI,As an admin I want to add/remove certain user in/from group or particular group in order to choose certain instance shapes for computational resources per group via Dlab UI.,AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 22:10:06,13
13205570,[AWS]: Use private IP address instead of public for notebooks proxy,"*Steps to reproduce:*

1. Deploy SSN in private subnet

2. Create notebook

 

*Actual result:*

Notebook creation fails due to ising public IP

 

*Expected result:*

Notebook is successfully created",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 22:06:50,18
13205569,[Azure]:SSN creation fails on process of network interface creating,"*Steps to reproduce:*
 # Create SSN

 

*Actual result:*

SSN creation fails:

!image-2018-12-20-00-03-47-676.png!

[http://35.166.222.81/view/v2.1-Azure/job/Azure_Create_environment-develop/81/console]

*Expected result:*

SSN is created",AZURE Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Blocker,2018-12-19 22:03:49,19
13205567,It should be impossible to stop Data Engine during notebook image creating on 'environment management',"A bug was found during notebook image creating on environment management page

 

*Preconditions:*
 # Notebook is created
 # Spark cluster on notebook is created

 

*Steps to reproduce:*
 # Create custom image from notebook
 # Go to environment management page

*Actual result:*

The icon to stop spark cluster on notebook is enabled

*Expected result:*

The icon to stop spark cluster on notebook is disabled",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 22:01:47,10
13205566,Jupyter creation fails on process of installing Toree-Scala kernel,"*Preconditions:*

Edge is created

 

*Steps to reproduce:*
 # Create Jupyter notebook

 

*Actual result:*

Jupyter creation fails:

!image-2018-12-20-00-00-08-709.png!

 

*Expected result:*

Jupyter is created",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 22:00:11,18
13205563,[Manage roles]: Name group is cropped,"*Preconditions:*
 # environment is created
 # 'resource list' page is open

 

*Steps to reproduce:*
 # click 'Roles' button
 # click 'Add group' button
 # put too long group name <testtoolongnameofgrouptoverify>
 # click 'Create' button

 

*Actual result:*

Name group is cropped

 

*Expected result:*

A hint appears if hover mouse on group name

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 21:58:13,10
13205562,[Manage roles]: Add API for deleting user group from role,DELETE /group/ID,AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 21:54:43,13
13205561,[Azure]: Sometimes Images/instances creation fails due to one IP is allocated to two instances simultaneously,"*Steps to reproduce:*

1. Run autotest on Azure

 

*Actual result:*

Creation image fails with error:

!image-2018-12-19-23-53-04-831.png!

[http://35.166.222.81/job/AutoTests_Azure/189/consoleFull]

*Expected result:*

Autotest runs successfully",AZURE Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 21:53:08,19
13205559,[GCP]: 'View Full billing Report For All Users' value should be absent in 'role' drop down list on 'Manage Roles' popup,"*Preconditions:*
 # SSN is created
 # 'Manage roles' popup is open

 

*Steps to reproduce:*
 # Click 'role' drop down list

 

*Actual result:*

'View Full billing Report For All Users'  is present in drop down list

*Expected result:*

'View Full billing Report For All Users'  is absent in drop down list",Back-end Debian GCP,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 21:49:50,13
13205558,[AWS]: Shape 'C4.xlarge' for computational resource is absent in 'role' drop down list on 'Manage roles' popup,"*Preconditions:*
 # SSN is created on AWS
 # 'Manage roles' popup is open

 

*Steps to reproduce:*

1. Click 'role' drop down list

 

*Actual result:*

Shape 'C4.xlarge' is absent in 'role' drop down list

*Expected result:*

Shape 'C4.xlarge' is present in 'role' drop down list

 ",AWS Bak-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:47:58,13
13205557,DEX support on UI,"As a user I want to have a possibility to login to DLAB using DEX identity provider.

 

*Acceptance criteria:*
 # New button on DLab login page - Login with DEX
 # Textboxes for login and password should be removed
 # After clicking on Login with DEX btn user is redirected to dex login page and after receiving identity is logged in to DLab (similar to Azure SSO)
 # Feature should be enabled in case when dexIdentityProviderEnabled self-service property is true.

 ",AWS Debian Front-end,['DataLab Old'],DATALAB,Task,Major,2018-12-19 21:46:06,10
13205553,Instance shapes should be filtered by clouds,"*Preconditions:*
 # Edge is created on AWS
 # 'Environment health status' page is open

 

*Steps to reproduce:*
 # Click 'Roles' button
 # Click  '$anyuser' drop down list
 # Click 'Role' drop down list

 

*Actual result:*

Appears shapes for AWS, Azure, GCP

!image-2018-12-19-23-43-13-404.png!

 

*Expected result:*

Appears shapes for AWS",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:43:14,13
13205551,Value of used space is not true on HDD tab,"A bug was found on AWS. 

 

*Preconditions:*

1. Edge is created

2. 'Health status' page is open

 

*Steps to reproduce:*
 # Click 'SSN Monitor' button
 # Go to HDD tab 

 

*Actual result:*
1. Disk value of 'used space' is more than 'total space'

!image-2018-12-19-23-37-20-249.png!

!image-2018-12-19-23-37-39-528.png!

*Expected result:*

1. Disk value of 'used space' is less than 'total space'

2. Disk value of 'used space' matches the result value to command *df*",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:37:41,13
13205549,[R package]: Library installation has failed but status is still 'installing' on DLab UI,"A bug was found on all clouds and notebooks but only for *'R package'* group

 

*Preconditions:*
 # Rstudio is created
 # 'Resources list' page is open

 

*Steps to reproduce:*

1.  Click action menu for RStudio

2. Choose 'Manage libraries'

3. Choose  notebook in 'Select resource'  drop down list

4. Choose r packages in 'Select group'

5. Enter library name *spduration* in text box for library name

6. Choose entered library in autosearch

7. Click 'Install' button

 

*Actual result:*
 # Docker executed with 0, but library is not installed due to some dependencies
 # Library status is 'installing' on DLab UI

 

*Expected result:*

1. Library status is failed on DLab UI",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:34:07,13
13205548,[Azure]: Error handling is absent for wrong username/password,"*Preconditions:*

SSN is created

 

*Steps to reproduce:*

1. Login DLab with wrong username or password

 

*Actual result:*

Error handling is absent

 

*Expected result:*

Error handling is present",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:31:58,13
13205547,Write instruction how to update certificates on DLab,Instruction add on KB page.,AWS AZURE Back-end GCP,['DataLab Old'],DATALAB,Task,Minor,2018-12-19 21:30:25,13
13205545,[Autotest]: Edge creation fails due to not valid of username or password,"*Steps to reproduce:*

1. Run integration test on AWS/Azure/GCP

 

*Expected result:*

Edge creation fails:

[http://35.166.222.81/job/AutoTests_AWS/256/consoleFull]

[http://35.166.222.81/job/AutoTests_Azure/186/consoleFull]

[http://35.166.222.81/job/AutoTests_GCP/96/consoleFull]

 

*Actual result:*

Autotest runs successfully",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:28:55,13
13205543,Rename columns Swap Page In/Out in SSN Monitor,"# 'Swap Page In' should be renamed into 'Pages Paged In' on ;SSN Monitor' ('Memory' tab)
 # 'Swap Page Out' should be renamed into 'Pages Paged Out' on ;SSN Monitor' ('Memory' tab)
[https://unix.stackexchange.com/questions/284315/how-to-obtain-counters-for-swap-in-swap-out-on-linux]
 # Remove row ' Page Size' from 'Memory' grid, because its value does not match the value of page size (getconf PAGESIZE) or define what value is portrayed in ' Page Size' and rename it accordingly",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Minor,2018-12-19 21:26:07,13
13205541,Error message for existing image name is absent during creation image on the other Notebbok,"*Preconditions:*
1. Two different Notebooks are created

*Steps to reproduce:*
1. For one Notebook create image with valid name 'ami1'
2. For the second Notebook create image with the same name 'ami1'

*Actual result:*
Error message about concerning message is absent

*Expected result:*
Error message about concerning message is Present",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:19:49,10
13205534,It is possible to stop/terminate notebook if related computational resource has terminating status in 'show active' mode on 'resource list' page,"*Preconditions:*

Computational resource has terminating status

 

*Steps to reproduce:*
 # Switch to 'show active' mode on 'resource list' page
 # Click on action menu for notebook
 # Choose stop/terminate notebook

 

*Actual result:*

Stop/terminate button is enable

Computational resource has status 'Failed'

 

*Expected result:*

Stop/terminate button is disabled for notebook",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 21:10:16,10
13205532,[BE]: Attribution and proper use of Apache Software Foundation trademarks,"Over the last few months I’ve been facing with a significant number of slide decks and white papers, prepared by our experts for the clients, and used in the pre-sales and other meetings. While not in 100% of the cases, but yet a common scheme is to mis-attribute the FOSS projects and technologies used in our solution. This is specifically noticeable in the case of Apache Software Foundation projects, because they are the most widely used technologies in our immediate area of expertise.

it is pretty important to use the proper attribution and trademarked names. The rule of thumb is to always prepend the project name with Apache i.e. it is always Apache Hadoop, Apache Parquet, Apache Hive, Apache Spark, Apache Zeppelin and so on. The more information and helpful resource could be found at [1]

[1] [https://kb.epam.com/display/EPMCBDCC/Trademark+attributions]",AWS AZURE Back-end GCP,['DataLab Old'],DATALAB,Task,Major,2018-12-19 21:08:10,20
13205530,Admin should be informed about successfully or unsuccessfully backup execution,"As an admin I want to know if backup execution is successful or unsuccessful.

 

*Acceptance criteria:*
 # If backup execution is successful appears message ""backup execution is successful""
 # If backup execution is unsuccessful appears message ""backup execution failed""

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Improvement,Minor,2018-12-19 21:02:43,10
13205528,[Java]: Add Swagger tool to DLab,"As a user I want use Swagger in order to call API DLab instae of using DLab UI.

 

*Acceptance criteria:*
 # to apply swagger to DLab user uses the next link  <IPssn/api/swagger#/> as example

Official documentation: [https://swagger.io|https://swagger.io/]",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,New Feature,Minor,2018-12-19 20:54:36,13
13205527,Extra scrollbar during tabs switching from left to right on SSN monitor page,"*Preconditions:*
 # Edge is created
 # SSN monitor page is oen

 

*Steps to reproduce:*

1. Switch between tabs CPU/Memory/HDD from left to right

 

*Actual result:*

Appears extra scrollbar

Please, see video in attachmnet

*Expected result:*

Extra scrollbar is absent

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 20:53:03,10
13205526,Page is blocked during uploading a key,"*Preconditions:*
 # Only SSN is created
 # User is logged in DLab

 

*Steps to reproduce:*
 # Go to 'Billing Report'  or 'Environment Management' page
 # Click beyond of reuploading key popup

 

*Actual result:*

'Billing Report'  or 'Environment Management' page is blocked during

 

*Expected result:*

'Billing Report'  or 'Environment Management' page isn't blocked during",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 20:51:29,10
13205525,[Environment management]: Size value is absent for computational resources on confirmation dialog,"*Preconditions:*
 # Computational resource is runnnig or stoppped
 # 'Environment management' page is open 

 

*Steps to reproduce:*

1. Click action menu for notebook (where comoutational resource is running ) to stop or terminate it

 

*Actual result:*

Size value is absent for computational resource on confirmation dialog

!image-2018-12-19-22-49-31-233.png!

 

*Expected result:*

Size value is present for computational resource on confirmation dialog",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 20:49:32,10
13205523,Role UI issues,"*Preconditions:*

'Manage roles' popup is open

 

*Steps to reproduce:*
 # Look at 'Manage roles' popup
 # Click 'Add group'
 # Click 'Roles'

 

*Actual result:*
 # 'Add group' button and dropdown lists are not aligned
 # 'Select roles' drop down list is cropped from the right side

*Expected result:*

1. 'Add group' button and dropdown lists should be aligned",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 20:44:34,10
13205522,It should be disabled to stop edge via 'environment management' if notebook/computational resource is in creating/configuring status,"*Preconditions:*

1. Notebook is in creating status

 

*Steps to reproduce:*
 # Go to 'Environment management' menu
 # Click action menu for edge
 # Click stop action

 

*Actual result:*

Stop action is enabled

 

*Expected result:*

Stop action is disabled

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 20:41:47,10
13205519,Rendering issues on 'Reporting' page (only in Firefox browser),"*Preconditions:*
 # Environment is creted more than one day ago

*Steps to reproduce:*
 # Got to 'Reporting' page

*Actual result:*

Two vertical lines are absent for 'status' column

!image-2018-12-19-22-40-08-460.png!

*Expected result:*
Two vertical lines are present for 'status' column",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 20:40:11,10
13205518,Persist/Restore reply to self-service from provisioning-service in case self-service is unavailable,Health check should be used to verify availability of self-service,AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 20:37:09,13
13205517,Review documentation concerning DLab deployment on AWS,"As a user I want easily and quickly deploy DLab  by myself on AWS using appropriate documentation:
 - [https://github.com/epam/DLab/blob/master/README.md#Configuration_files]

 - [https://kb.epam.com/display/EPMCBDCC/Deployment+checklist]

 

*Acceptance criteria:*

1. All prerequisites should be counted which are needed before DLab deploy",AWS Back-end,['DataLab Old'],DATALAB,Task,Major,2018-12-19 20:35:38,13
13205516,Investigate updating node version js and dependencies,Investigate possibility to update node version js.If it can be implemented please update node version js and dependencies.,AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 20:30:39,10
13205515,Information for not existing page and not having permission to page access,"Information which user sees in case:
 - If page does not exist user sees 'page not found'
 - If page exists but user has not access to it user sees 'Permission denied'",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 20:29:04,10
13205514,All DLab resources should be tagged as product:DLab,"Now we have (key) user:tag.

New tag should be added: Product:DLab",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 20:25:54,17
13205513,[AWS] RStudio with TensorFlow creation fails on executing command sudo: /bin/bash /home/dlab-user/NVIDIA-Linux-x86_64-390.48.run -s --dkms,"*Preconditions:*

Environment is  created

 

*Steps to reproduce:*

1. Create RStudio with TensorFlow

 

*Actual result:*

RStudio with TensorFlow creation fails on executing command:

*sudo: /bin/bash /home/dl ab-user/NVIDIA-Linux-x86_64-390.48.run -s --dkms*

 

*Expected result:*

RStudio with TensorFlow is created",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 20:24:46,17
13205512,[AWS][Azure][RedHat]: Notebook creation fails on process of sparkR installation,"*Preconditions:*
Edge/project is created

*Steps to reproduce:*
1. create jupyter/zeppelin/rstudio/tensorflof with rstudio notebook

*Actual result:*
Jupyter/zeppelin/rstudio/tensorflof with rstudio creation fails wit error:

*Requested: cd /usr/local/spark/R/lib/SparkR; R -e ""devtools::install('.')""*
*Executed: sudo -S -p 'sudo password:' /bin/bash -l -c ""cd /usr/local/spark/R/li b/SparkR; R -e \""devtools::install('.')\""""*

*Expected result:*
Jupyter/zeppelin/rstudio/tensorflof with rstudio is created



 The bug is reproduced in develop/feature/projects/roject_tmp branches",AWS AZURE DevOps RedHat pull-request-available,['DataLab Main'],DATALAB,Bug,Critical,2018-12-19 20:23:09,9
13205511,[AWS]: EMR creation fails,"*Preconditions:*

Environment is created

 

*Steps to reproduce:*

1. Create EMR

 

*Actual result:*

EMR creation fails:

*File ""/usr/lib/python2.7/UserDict.py"", line 40, in __getitem__*
*raise KeyError(key)*

 

*Expected result:*

EMR is created",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 20:21:12,17
13205510,URL should be in one row on Notebook name popup,"*Preconditions:*

Notebook with long name is created <Rstudio-tensorflow>

 

*Steps to reproduce:*
 # Click Notebook name popup

 

*Actual result:*

Ungit link is portrayed in two rows

!image-2018-12-19-22-19-27-389.png!

*Expected result:*

1. ungit link should not be splitted in 2 lines (ungit link is portrayed only in one row)/
2. url is cutted",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 20:19:29,10
13205509,Prevent Java library installation for RStudio and RStudio with TensorFlow templates,From UI remove possibility to  install Java library for Rstudio and RStudio with TensorFlow templates,AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 20:17:15,10
13205508,[AWS]: SSN creation fails due to object has no attribute '__getitem__',"*Steps to reproduce:*

1. create SSN

 

*Actual result:*

SSN fails with error:

*'Object has no attribute '__getitem__''*

[http://35.166.222.81/job/Amazon_Create_environment-develop/146/console]

 

*Expected result:*

SSN is created",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Bug,Blocker,2018-12-19 20:15:55,17
13205507,Remove header in case of absent computational resources on confirmation dialog,"Only notebook is created

 

*Steps to reproduce:*
 # Click action menu of Notebook
 # Choose stop or terminate action

 

*Actual result:*

Header for computational resources is present

*Expected result:*

""Stop notebook: name_notebook"" is present
Message ""Notebook server will be stopped."" is present
Header for computational resources is absent
Message ""You have no active resources"" is present
Message ""Do you want to proceed?"" is present
""No"" and ""Yes"" buttons are present
""Notebook server will be stopped"" should be center aligned",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 20:14:09,10
13205506,TensorFlow-Jupyter/Deep Learning creation fails on process of nvidia-installer,"*Steps to reproduce:*

1. Create DeepLearning or TensorFlow with Jupyter

 

*Actual result:*

DeepLearning or TensorFlow with Jupyter creation fails

!image-2018-12-19-22-12-02-893.png!

*Expected result:*

DeepLearning or TensorFlow with Jupyter is created

 ",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 20:12:06,17
13205504,Change in build procedure: build UI and BE after SSN is deployed,We don't need all git repository inside of docker container except SSN,AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 20:09:24,9
13205503,[Manage roles]: Back-end issues,"*Preconditions:*
""Manage roles"" popup is open
 
*Steps to reproduce:*
1. click ""Role"" drop down list
2. look through 
 
*Actual result:*
1. use [some size] instance shape 
2. use [some size] instance shape for computational
 
*Expected result:*
1. use [some size] instance shape for notebook
2. use [some size] instance shape for cluster",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 20:06:48,13
13205501,[Add computational resources]: Alter and prevent passing some values to front-end side,"# Remove descriptions: ""Apache Spark standalone cluster"", ""Image for EMR provisioning""
 # Rename templates names from ""Apache Spark cluster"" to ""Apache Spark standalone cluster"" and from ""EMR cluster"" to ""AWS EMR cluster""",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 20:05:06,13
13205500,[Add computational resources]: UI improvements,"*1.* Rename label from ""Your bid for spot instance, %"" to ""Spot instance bit, %""

*2.* ""Configurations"" check box rename to ""Cluster configuration template, JSON""

*3.* Divide 'Add computational resources' into two columns:
*3.1.* for Spark cluster move ""Total node number"" and ""Node shape"" into the second (right) side. ""Cluster configuration template, JSON"" check box with input value should be above all drop downs list 
*3.2.* for EMR cluster move ""Total instance number"", ""Master instance shape"" and ""Slave instance shape""into the second (right) side. ""Cluster configuration template, JSON"" check box with input value should be above all drop downs list and ""Spot instance bit, %""",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 20:03:24,10
13205499,"Extra vertical scrollbar for six users on Manage popup""","*Preconditions:*
Six users has at least one running instance

*Steps to reproduce:*
1. Go to Health status page
2. Click on 'Manage enironment button

*Actual result:*
Extra vertical scrollbar appears

*Expected result:*
There is no extra vertical scrollbar",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Trivial,2018-12-19 20:01:48,10
13205498,Notebook termination should convey the future status of previously stopped cluster on confirmation dialog,"*Preconditions:*
1. spark standalone cluster is in stopped status
2. ""Resources list"" page is open

*Steps to reproduce:*
1. click action menu for notebook
2. choose terminate notebook

*Actual result:*
In confirmation dialog is absent that stopped cluster will be terminated

*Expected result:*
In confirmation dialog informst that stopped cluster will be terminated",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:59:48,10
13205496,UI Issues with EMR configuration,"*Preconditions:*
 # Notebook (Jupyter/Zeppelin/Rstudio) is created
h4.  

 # 'Add computational resources' is open

 

*Steps to reproduce:*
 # choose EMR cluste
 # check off 'configuration' box
 # uncheck 'configuration' box

 

*Actual result:*

Height of popup  changes

 

*Expected result:*

Height of popup does not change ",Debian Front-end RedHat,[],DATALAB,Bug,Minor,2018-12-19 19:56:14,10
13205490,Prevent scheduler submit on turning off scheduler. Error handling should not appear if turn off scheduler,"*Preconditions:*
1. Notebook is created
2. ""Resources list"" page is open

*Steps to reproduce:*
1. click action menu for notebook
2. choose scheduler
3. scheduler is disabled
4. click ""save button""

*Actual result:*
1. appears error handling 
2. scheduler popup is open

*Expected result:*
1. scheduler popup is close
2. scheduler is disabled",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:45:46,10
13205489,The same style should be on all DLab pages,"Billing is avalable

 

*Steps to reproduce:*
 # Go to 'resources_list' page
 # Go to 'billing_report' page
 # Go to 'environment_management' page

 

*Actual result:*

There are different styles on all pages

*Expected result:*

There is the same style on all pages
You can use style which is on ""resources list"" page

 ",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Minor,2018-12-19 19:44:07,10
13205488,"[Health Status page ]: Convey current instance status on ""Manage environment"" popup","As an admin I want to see current instance status on ""Manage environment"" popup so that I will be informed and can easily manage them.

*Acceptance criteria:*

*1.* if at least one instance of user's environment has running status - the status user's environment should be running
*2.* if all instances of user's environment have terminated status - the status user's environment should be terminated
*3.* if all instances of user's environment have stopped status - the status user's environment should be stopped
4. if admin stops user's environment via ""Manage environment"" - the status user's environment should be stopping
5. if admin terminates user's environment via ""Manage environment"" - the status user's environment should be terminatining",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 19:41:56,13
13205487,User is not Admin by default after creating SSN,"A bug was found on AWS.

*Preconditions:*
Edge is create

*Steps to reproduce:*
1. Go to ""Environment health status"" page

*Actual result:*
1. Buttons (""Manage roles"", ""SSN"", ""Backup"", ""Manage environment"") for executing administration actions are absent
2. ""Environment management"" page is absent in main menu

*Expected eresult:*
1. Buttons (""Manage roles"", ""SSN"", ""Backup"", ""Manage environment"") for executing administration actions are present
2. ""Environment management"" page is present in main menu",AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 19:37:27,13
13205486,[Azure]: Billing is not available,"*Preconditions:*
Environment is created a day ago

*Steps to reproduce:*
1. go to ""Billing report""

*Actual result:*
Billing information is absent on ""Billing report"" page and ""Billing detail"" popup

*Expected result:*
Billing information is present on ""Billing report"" page and ""Billing detail"" popup",AZURE Back-end Debian RedHat,['DataLab Old'],DATALAB,Bug,Critical,2018-12-19 19:35:42,13
13205485,If update the configurations for Spark Standalone the value of Memory per Executor is less than 75% from total,"*Preconditions:*
1. Notebook is created

*Steps to reproduce:*
1. Create custom Spark standalone with parameters:
[
{
""Classification"": ""spark-defaults"",
""Properties"":

{ ""spark.shuffle.sort.bypassMergeThreshold"": ""200"" }

}
]
2. Update custom Spark standalone with parameters:
[
{
""Classification"": ""spark-defaults"",
""Properties"":

{ ""spark.shuffle.sort.bypassMergeThreshold"": ""250"" }

}
]
3. run a playbook on remote kernel
4. go to Spark standalone (master) UI

*Actual result:*
13% is allocated for Memory per Executor from total

*Expected result:*
75% is allocated for Memory per Executor from total",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:33:42,9
13205484,"[Chrome]: After creating edge any actions is not available (except for ""Git credentials"") until press ""f5""","*Preconditions:*
1. SSN is created
2. DLab is open in Chrome

*Steps to reproduce:*
1. create edge node
2. click ""Creaye new"" button

*Actual result:*
""Create analytical tool"" popup does not appear

*Expected result:*
""Create analytical tool"" popup appears",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:31:41,10
13205483,[GCP]: Prevent adjusting total budget/user limit and cluster configurations for Dataproc,"Currently billing is not available on GCP. 
1. Please, remove possibility of managing quota for DLab per user and total.
2. Remove ""Cluster configuration"" for Dataproc",Debian Front-end GCP,['DataLab Old'],DATALAB,Task,Critical,2018-12-19 19:29:46,10
13205482,[Manage environment]: Action does not submit because of not correct value passing,"*Preconditions:*
1. Edge is created (in running status)
2. ""Environment health status"" page is open

*Steps to reproduce:*
1. click ""Manage environment"" button
2. click ""Stop"" or ""Terminate"" icon

*Actual result:*
appears message ""Environment of [object Object] will be stopped"" on confirmation dialog

*Expected result:*
1. appears message ""Environment of <username> will be stopped"" on confirmation dialog . Instead of <username> should be value of username whose environment will be stopped or terminated",AWS AZURE Debian Front-end GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:27:58,10
13205480,Cost details dialog fixes. Date should be in one line on billing detail popup,"*Preconditions:*
 # Billing is available
 # 'Resources_list' page is open

 

*Steps to reproduce:*

1. Click billing detail of notebook

 

*Actual result:*

Start date is in two rows

End date is in one row

 

*Expected result:*

Start date is in one row (increase column width for start date)

End date is in one row 
Column widths of start/end date are equal
 ",AZURE Debian Front-end RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:24:18,10
13205478,Add LDAP authorization in Squid,"As a user I want to use LDap authorization in Squid so that it optimises the data flow between client and server to improve performance.

 

[https://github.com/epam/DLab/pull/676/files]",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-12-19 19:21:38,9
13205475,Remove 'add compute' button for user who is not allowed to create computational resources,"*Preconditions:*
 # environment is created
 # user is not allowed to create computetional resources
 # notebook is created

 

*Steps to reproduce:*
 # Go to 'resources list' page
 # Click action menu for notebook
 # Choose 'add compute'

 

*Actual result:*

Appears popup and it is impossible to close it

 

*Expected result:*

'Add compute' button is absent

 ",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-19 19:15:23,13
13205470,Add support of installing DLab into two VPCs,"As a user I want to deploy DLab using two VPCs.

 

*Acceptance criteria:*

1. the first VPC for SSN and EDGE

2. the second for Notebooks, computational resources",AWS Debian RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 19:01:09,16
13205413,"[Admin page]: Implement SSN Load Monitor: CPU, Memory, HDD","As an admin I want to monitor and manage ssn HDD and Memory load so that I can see showing Memory and HDD usage of SSN.

 Create new REST service to perform next actions:
 - Popup showing Memory and HDD usage of SSN

 

*Acceptance criteria:*
 # there is 'SSN monitor' button on 'health status' page
 # if user clicks 'SSN monitor' button 'SSN monitor' popup is opened
 # there are three tabs on  'SSN monitor' popup: CPU, HDD, Memory

 ",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,New Feature,Major,2018-12-19 14:54:02,16
13205409,Do not allow to create AMI's with same name: add extra checks by Status during AMI creation,"We should filter AMI which is in status 'created' and 'creating' so that the name of creating AMI will be checked and will be not to allowed to create AMI with the same name.

 

*Acceptance criteria:*

1.If custom AMI has 'creating' status and user puts the same name (as in 'creating' AMI) appears error message concerning using this name

 ",AZURE,['DataLab Old'],DATALAB,Task,Major,2018-12-19 14:49:34,16
13205401,Add support of Java dependencies from DLab web UI,"As a user I want to install Java dependencies via DLab web UI in order to quickly, usability, easily installation on instance.

 

*Acceptance criteria:*
 # It should be new field in dropdown list 'Java'
 # In text box user can put Java dependencies for installation and install them

 ",AWS AZURE GCP,['DataLab Old'],DATALAB,New Feature,Major,2018-12-19 14:41:57,18
13205398,[TechDebt] Generalized communication protocol(error handling) between UI and BE,"As a user I want to see error handling for some action in order to be informed about process.

 

*Acceptance criteria:*

1. error handling should appear in case of successful/unsuccessful backup

2.  error handling should appear in case of uploading key of wrong content

3.  error handling should appear in case of edge stopping/terminating during if one of the instance is in creating/starting status via  environment management",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 14:37:53,13
13205395,[Admin page]: Ability to choose certain instance shapes for Notebooks per group via Dlab UI," As an admin I want to add/remove certain user in/from group or particular group in order to choose certain instance shapes for Notebooks per group via Dlab UI.

 

*Acceptance criteria:*
 # only Admin can add a user in a group
 # only Admin can  remove a user from a group
 # only Admin can add new group
 # only Admin can delete a group
 # only Admin can choose certain instance shapes for Notebooks per group

       ",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 14:33:39,13
13205394,Stop user env when reaching user qoute,"In case when user billing quote is reached, user env should be stopped even if application quote is not reached yet",Back-end,[],DATALAB,Sub-task,Major,2018-12-19 14:28:58,13
13205389,[Cost limitation]: Implement ability to manage per user quota for DLab,"As a user I want to view notification concerning of using  financial DLab resources so that I can know available financial resources.

 

*Acceptance criteria:*

*1.*  A user sees notification during login in DLab:

_'You used 55% of available quota._

_Attention: if it reaches 100% all instances will be terminated.'_

*2.* if total amount of financial using equals 100% all instances will be terminated  -?<SSN>?-

*3.* a  button  'Quota' will be on 'Health status page' where user can see amount of financial using",AZURE Debian RedHat,['DataLab Old'],DATALAB,New Feature,Major,2018-12-19 14:23:30,16
13205386,API for getting spark configuration,API needed for getting spark configurations (exploratory and computational),AWS AZURE Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,Sub-task,Major,2018-12-19 14:19:02,13
13205359,Tune Spark/EMR parameters from web UI on instance creation step,"As a user I want to tune Spark/EMR parameters from web UI.

 

*Acceptance criteria:*

1. it should be check box for configuration on computational creation popup

2. if user checks off configuration check box parameters a text box appears in the right side of computational creation popup

3. in parameters a text box a user can put Spark/EMR parameters

 

_You can view example JSON of configurations using this link [https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html]_ ",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 11:55:02,16
13205352,[Scheduler on idle]: Ability to configure scheduler for resources stopping/termination on exceeding idle time,"As a user I want to have a scheduler for Clusters to be stopped (Spark) or terminated (EMR) after XXX minutes of inactivity, so that I can reduce charges during non-working period with computational resources.

 

*Acceptance criteria:*

*1.* it should be default option

*2.* radio button should be on scheduler dialog which has this option checked and set to 120 minutes       

      *2.1.* in case it was configured to 120 min and on 119 minute some activity started we do not stop/terminate anything. BUT postpone stopping/termination for next 120 min

*3.* it should NOT be two schedulers at the same time (it should be configured by pattern and inactivity one)",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-12-19 11:27:05,13
13205348,[Scheduler on timer]: Add a reminder after user logs in notifying that corresponding resources are about to be stopped/terminated,"As a user I want to see reminder in 15 minutes before a scheduler begins to stop machine(s) so that I will be informed about my current schedule and can change schedule if I need to work with instance.

 

*Acceptance criteria:*
 # Reminder should appear in 15 minutes before a scheduler begins to stop machine(s).

 

_*** Please, investigate if reminder should be by email or appears on user browser (in this case we need autorefresh on UI side)_",AWS AZURE Debian GCP RedHat,['DataLab Old'],DATALAB,New Feature,Major,2018-12-19 11:20:30,13
13202554,[RStudio]: Terminating spark cluster behaves wrong for resources with similar name,"When there more than 1 spark cluster created with similar name (e.g. cluster, cluster1) on RStudo action terminating for cluster also removed remote kernels for cluster1 from kernels list)

*Preconditions:*
1. two spark clusters are created on RStudio with names ""spark"" and ""spark1""

*Steps to reproduce:*
1. terminate cluster with name ""spark""

*Actual result:*
1. cluster with name ""spark"" is terminated
2. remote kernel of cluster with name ""spark"" is deleted from kernels list 
3. cluster with name ""spark1"" is running
4. remote kernel of cluster with name ""spark1"" is deleted from kernels list

*Expected result:*
1. cluster with name ""spark"" is terminated
2. remote kernel of cluster with name ""spark"" is deleted from kernels list 
3. cluster with name ""spark1"" is running
4. remote kernel of cluster with name ""spark1"" is available in kernels list",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-12-05 14:24:37,18
13198308,[AWS][GCP][Back-end]: Bucket browser,"As a user I want to  use S3 browser so that I can manage my buckets and objects via DLab UI.

 

*Acceptance criteria:*

*1.* User has access endpoint_shared bucket and project bucket (only if he is assigned to this project)

*2.* Another user does not have access to project bucket (if he is not assigned to this project)

*3.* Administrator has access to all buckets

*4.* User can upload and download files to and from bucket",AWS Back-end Debian GCP RedHat,['DataLab Old'],DATALAB,New Feature,Major,2018-11-14 12:22:26,6
13196614,[AWS][SSN performance optimization]: Refactor pricing retrieval logic to optimize RAM usage on SSN,"*Preconditions:*

1. Environment is created

 

*Steps to reproduce:*

1. Create EMR with spot

 

*Actual result:*

EMR creation fails

 

*Expected result:*

EMR is created

 

Currently method _*get_ec2_price*_ uses downloading of big json file (~484 Mb) during creating EMR cluster with sport instances.

Python required ~ _*3.2Gb + 484*_ Mb on SSN node for 1 launch, which can cause memory error on small shapes (like _*t2.medium*_).

Suggestion: rewrite method using python client to avoid such resource (memory/cpu) overheads.

Links:

[https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSpotPriceHistory.html]

[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.Client.describe_spot_price_history]

[https://aws.amazon.com/blogs/aws/new-aws-price-list-api/]

[https://aws.amazon.com/blogs/aws/aws-price-list-api-update-new-query-and-metadata-functions/]",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Bug,Major,2018-11-06 15:32:35,18
13196598,[GCP]: Add TensorFlow-RStudio template,"As a user I want to have a TensorFlow-RStudio template on GCP.

 

*Acceptance criteria:*

1. in Notebook creation popup it should be TensorFlow-Rstudio template in drop down list

2. playbook  for RStudio should be run successfully on TensorFlow-Rstudio template  (via local and remote kernels) 

3. playbook  which use GPU should be run successfully on TensorFlow-Rstudio template (via local kernel).

For GCP is template is prevented because some issues there were.",2.3_old Debian DevOps GCP GCP_demo pull-request-available,['DataLab Old'],DATALAB,New Feature,Minor,2018-11-06 15:11:34,2
13196593,Report header should be sticky and always present while scrolling down,"As a user I want always to see report header on the top of window during scrolling down so that I will be able to see titles of grid values.

 

*Acceptance criteria:*

1. report header is always present on the top of window during scrolling down",AZURE Debian Front-end RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Minor,2018-11-06 15:04:22,5
13196572,Add possibility to clean docker logs on SSN,"As a user I want to manage of cleaning docker on SSN so that it prevent do not load of SSN.

 

*Acceptance criteria:*
1. it should be possibility to turn on/off a function of cleaning docker logs on SSN

2. only executed docker with '0' should be cleaned after determined period, for example, after three days???",AWS AZURE Debian DevOps GCP RedHat,['DataLab Old'],DATALAB,Task,Major,2018-11-06 14:34:20,1
13196560,[Multiple versions]: Add support for previous Spark 3.x.x versions,"As a user I want to choose from DLAB UI what version of Spark cluster can be deployed on notebook, so that I am not connected only with one version of Spark cluster.
----
*Acceptance criteria:*
 # in drop down list for spark cluster  on create computational resources popup there are more than one spark cluster for choice
 # user can deploy different version of Spark cluster on one notebook

 

_***Currently we support only 1 version of spark cluster_

Github issue: _[https://github.com/apache/incubator-dlab/issues/744]_",AWS AZURE Debian DevOps GCP Github_issue RedHat,['DataLab Old'],DATALAB,Improvement,Minor,2018-11-06 14:12:42,2
13196555,[GCP][Notebooks]: Add  custom images,"As a user I want to create Notebook using custom image, so that I can reduce time of libraries installation on a new notebook.

*Acceptance criteria:*
 1. User is able to create image from running notebook
 2. User is able to create notebook from custom image",Debian DevOps GCP pull-request-available,['DataLab Old'],DATALAB,Task,Major,2018-11-06 14:02:19,2
13196552,[GCP]: Failed edge removes a shared bucket,"A bug was found on GCP.  Edge creation fails on a process creating ssh user. 
Failing edge deletes shared bucket. After that it is impossible to create a new edge for user.

!Edge creation again.PNG!

*Preconditions:*
1. SSN is created

 

*Steps to reproduce:*
1. Fail SSN on stage of creating ssh user

 

*Actual result:*
Shared bucket is deleted

!Shared bucket.PNG!

*Expected result:*
Shared bucket is not deleted

 ",Debian DevOps GCP GCP_demo,['DataLab Old'],DATALAB,Bug,Critical,2018-11-06 13:56:31,9
13196513,[Azure]: Implement DLab instalation via private IP,"As a user I want to install DLab via private IP

 

*Acceptance criteria:*

1. All instances should have only private IP",AZURE Debian DevOps RedHat,['DataLab Old'],DATALAB,Improvement,Major,2018-11-06 12:21:20,21
13196509,[AWS]: Project's public key should not be stored on s3 ssn bucket after EMR creation,"*Preconditions:*

EMR is created

 

*Steps to reproduce:*
 # Go to AWS console S3 SSN bucket

 

*Actual result:*

Public key is stored on s3 ssn bucket

!Public key aws.PNG!

 

*Expected result:*

Public key is not stored on s3 ssn bucket",2.3_old AWS Debian DevOps RedHat,['DataLab Main'],DATALAB,Bug,Minor,2018-11-06 12:15:06,12
13196508, [Azure]: SSO supporting for RedHat,"As a user I want to have SSO on DLAB for AWS for RedHat, so that I be able  to login to DLab through it.

 Now project creation on RedHat fails with error:

ImportError: cannot import name install_nginx_lua

!image-2020-01-09-18-18-13-086.png!",Azure DevOps RedHat,['DataLab Old'],DATALAB,Task,Major,2018-11-06 12:12:49,3
13196507,[AWS]: SSO supporting for RedHat,"As a user I want to have SSO on DLAB for AWS for RedHat, so that I be able  to login to DLab through it.

 Now project creation on RedHat fails with error:

ImportError: cannot import name install_nginx_lua

 ",AWS Debian DevOps RedHat,['DataLab Old'],DATALAB,Task,Minor,2018-11-06 12:11:35,3
13196499,[Data Engine]: Add Spot instances support for Standalone Spark cluster,"As a user I want to use spot functionality during Spark cluster creation so that I can reduce financial charges on AWS/GCP/MS Azure.
----
*Acceptance criteria:*

1. it should be check box for spot instance on Spark cluster creation popup

2. check box for Low spot instance should be checked off by default

3. if user unchecks check box for spot instance and hits 'Create' button a Spark cluster will be created without spot price

4. if user checks off check box for spot instance and hits 'Create' button a Spark cluster will be created with spot price

Github issue: [https://github.com/apache/incubator-dlab/issues/742]",AWS AZURE Debian GCP Github_issue RedHat,['DataLab Old'],DATALAB,New Feature,Minor,2018-11-06 11:42:44,2
13196484,[GCP]: Fix issue with re-using GCP Service accounts and roles,"After recreating project with the same name, roles and service accounts are not valid. To fix it, we should add something unique to names of roles and service accounts.

 ",2.3_old Debian DevOps GCP pull-request-available,['DataLab Old'],DATALAB,Bug,Major,2018-11-06 11:00:48,2
13196481,[GCP]: Billing,"As a user I want to see charges of instances so that I can monitor financial situation.

 

*Acceptance criteria:*
 # it should be billing report page 
 # user can filter values on billing report page
 # user can export data from billing report page 
 # user can see billing details

 ",Back-end Debian GCP,['DataLab Main'],DATALAB,New Feature,Major,2018-11-06 10:57:12,13
13196480,Need to change behavior of Show Active button,"As a user I want to be notified about failing computational resources in 'show active' mode so that I do not need to switch to 'show all' mode.

 

*Case:*

1. on resources_list page 'show active' button is chosen
2. computational resource is creating
3. creating computational resource fails

It should be possibility to 'notify' user about failed computational resource.

Because in 'show active' mode this computational resource will not be portayed untill a user change mode to 'show all'.

 

*Acceptance criteria:*

1. user should receive notify about unsuccessful computational resources creating in 'show active' mode on resource list page

 ",AWS AZURE Debian Front-end GCP RedHat pull-request-available,['DataLab Old'],DATALAB,Improvement,Minor,2018-11-06 10:55:28,5
13196453,[Front-end]: Flexible disk size for instances,"(i) As it was said it was done by DevOps side. We should check it.

As a user I want to choose volume type, size, lops during Notebook/computational resource creation or change the parameters of already existed Notebook so that I can add more space by necessary on GCP.

 

*Acceptance criteria:*

1. 'Custom configuration' should be on notebook creation popup.

2. 'Custom configuration' consists of:
 * Additional disk((main/mounted))/storage?
 * Spark configurations

3. On top of that user can change disk on already exciting notebook. 'Custom configuration should be on notebook name popup'

Additional disk should be only for notebook or for computational resource as well?

 
----
(i) Also it was mentioned to give dlab-user access to opt. For now only root has access.",AWS AZURE Debian Front-end GCP RedHat WPM,['DataLab Old'],DATALAB,New Feature,Minor,2018-11-06 09:29:48,0
13196449,[Back-end]: Flexible disk size for instances,"(i) As it was said it was done by DevOps side. We should check it.

As a user I want to choose volume type, size, lops during Notebook/computational resource creation or change the parameters of already existed Notebook so that I can add more space by necessary on Azure/GCP/AWS.

 

*Acceptance criteria:*

1. 'Custom configuration' should be on notebook creation popup.

2. 'Custom configuration' consists of:
 * Additional disk((main/mounted))/storage?
 * Spark configurations

3. On top of that user can change disk on already exciting notebook. 'Custom configuration should be on notebook name popup'

Additional disk should be only for notebook or for computational resource as well?

 
----
(i) Also it was mentioned to give dlab-user access to opt. For now only root has access.",AWS AZURE Back-end Debian GCP RedHat WPM,['DataLab Old'],DATALAB,New Feature,Minor,2018-11-06 09:21:04,1
13196439,[DevOps]: Flexible disk size for instances,"(i) As it was said it was done by DevOps side. We should check it.

As a user I want to choose volume type, size, lops during Notebook/computational resource creation or change the parameters of already existed Notebook so that I can add more space by demand on AWS/GCP/Azure.

 

*Acceptance criteria:*

1. 'Custom configuration' should be on notebook creation popup.

2. 'Custom configuration' consists of:
 * Additional disk(main/mounted)/storage?
 * Spark configurations

3. On top of that user can change disk on already exciting notebook. 'Custom configuration should be on notebook name popup'

Additional disk should be only for notebook or for computational resource as well?

 
----
(i) Also it was mentioned to give dlab-user access to opt. For now only root has access.
----
 ",AWS AZURE Debian DevOps GCP RedHat WPM,['DataLab Old'],DATALAB,New Feature,Minor,2018-11-06 09:04:45,3
