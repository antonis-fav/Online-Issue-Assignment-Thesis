id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13391423,[C++] Slice of FixedSizeList fails ValidateFull,"{code:python}
>>> arr = pa.array([[""a"", ""b""], None, [""c"", ""d""]], pa.list_(pa.string(), 2))
>>> arr.validate(full=True)
>>> arr.slice(0, 1).validate(full=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/array.pxi"", line 1219, in pyarrow.lib.Array.validate
  File ""pyarrow/error.pxi"", line 97, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Values length (6) is not equal to the length (1) multiplied by the value size (2)
{code}
Came up while looking at ARROW-13222. I'm not sure of the check here is necessarily valid, given slicing; it should perhaps only check that values length >= length * value_size, not that they're exactly equal.",pull-request-available,['C++'],ARROW,Bug,Major,2021-07-22 18:20:24,0
13391283,[C++][Flight] -lssl is missing with bundled gRPC and system shared OpenSSL,"This causes Apache Arrow GLib verification failure:
https://github.com/apache/arrow/pull/10768
https://github.com/ursacomputing/crossbow/runs/3125198200?check_suite_focus=true

{noformat}
 g-ir-scanner: link: x86_64-linux-gnu-gcc -pthread -o /tmp/arrow-5.0.0.PYIBz/apache-arrow-5.0.0/c_glib/build/tmp-introspectijhws295/ArrowFlight-1.0 /tmp/arrow-5.0.0.PYIBz/apache-arrow-5.0.0/c_glib/build/tmp-introspectijhws295/ArrowFlight-1.0.o -L. -Wl,-rpath,. -Wl,--no-as-needed -L./arrow-flight-glib -L/tmp/arrow-5.0.0.PYIBz/apache-arrow-5.0.0/c_glib/build/arrow-glib -Wl,-rpath,/tmp/arrow-5.0.0.PYIBz/apache-arrow-5.0.0/c_glib/build/arrow-glib -L/tmp/arrow-5.0.0.PYIBz/install/lib -Wl,-rpath,/tmp/arrow-5.0.0.PYIBz/install/lib -L/tmp/arrow-5.0.0.PYIBz/install/lib -Wl,-rpath,/tmp/arrow-5.0.0.PYIBz/install/lib -larrow-flight-glib -larrow_flight -larrow -lgobject-2.0 -lglib-2.0 -lgirepository-1.0 -lgio-2.0 -lgobject-2.0 -Wl,--export-dynamic -lgmodule-2.0 -pthread -lglib-2.0
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ENGINE_set_default'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ERR_error_string_n'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ENGINE_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `X509_NAME_dup'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_EncryptInit_ex'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_get_peer_cert_chain'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_next_protos_advertised_cb'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `BIO_read'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ENGINE_by_id'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `HMAC_CTX_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `BIO_new'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_DigestSignFinal'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `CRYPTO_get_ex_new_index'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `OPENSSL_init_crypto'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_sha256'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_new'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_MD_CTX_new'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_set_SSL_CTX'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `BIO_ctrl'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_use_PrivateKey'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `X509_STORE_set_flags'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_PKEY_set1_RSA'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `HMAC_Init_ex'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_session_id_context'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_MD_CTX_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `BIO_new_bio_pair'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_options'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `X509_NAME_ENTRY_get_data'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_get_error'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `PEM_read_bio_PrivateKey'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_ctrl'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `X509_STORE_add_cert'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ENGINE_init'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_cipher_list'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `OPENSSL_init_ssl'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_session_reused'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_EncryptUpdate'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_set_connect_state'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_set_bio'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SHA256_Update'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `PEM_read_bio_RSAPrivateKey'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_use_certificate'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `BIO_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_CIPHER_CTX_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_state_string_long'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_do_handshake'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `OPENSSL_sk_push'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `PEM_read_bio_X509_AUX'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `i2d_SSL_SESSION'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_aes_256_gcm'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ENGINE_load_private_key'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_ex_data'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_ctrl'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SHA256_Final'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `TLS_method'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `OPENSSL_sk_num'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_read'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ERR_print_errors'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_SESSION_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_DigestUpdate'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_get_peer_certificate'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `X509_get_ext_d2i'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ERR_clear_error'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `BIO_write'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_get_ex_data'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `ERR_get_error'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `d2i_SSL_SESSION'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `PEM_read_bio_X509'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_get_SSL_CTX'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_client_CA_list'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `X509_NAME_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `CRYPTO_free'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_DecryptInit_ex'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `PEM_write_bio_X509'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_EncryptFinal_ex'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EC_KEY_new_by_curve_name'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_get0_next_proto_negotiated'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_get_rbio'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `SSL_CTX_set_alpn_select_cb'
/usr/bin/ld: /tmp/arrow-5.0.0.PYIBz/install/lib/libarrow_flight.so: undefined reference to `EVP_DigestSignInit'
collect2: error: ld returned 1 exit status
{noformat}
",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2021-07-22 08:06:37,1
13391162,[Dev][Archery] Archery import pandas which imports pyarrow,"Just got this error when trying to run benchmarks on a PR:
{code}
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/bin/archery"", line 33, in <module>
    sys.exit(load_entry_point('archery', 'console_scripts', 'archery')())
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/antoine/arrow/dev/dev/archery/archery/cli.py"", line 634, in benchmark_diff
    no_counters, ren_counters)
  File ""/home/antoine/arrow/dev/dev/archery/archery/cli.py"", line 650, in _format_comparisons_with_pandas
    import pandas as pd
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/__init__.py"", line 50, in <module>
    from pandas.core.api import (
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/core/api.py"", line 29, in <module>
    from pandas.core.arrays import Categorical
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/core/arrays/__init__.py"", line 20, in <module>
    from pandas.core.arrays.string_arrow import ArrowStringArray
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pandas/core/arrays/string_arrow.py"", line 65, in <module>
    import pyarrow.compute as pc
  File ""/home/antoine/arrow/dev/python/pyarrow/compute.py"", line 18, in <module>
    from pyarrow._compute import (  # noqa
ImportError: cannot import name 'ProjectOptions' from 'pyarrow._compute' (/home/antoine/arrow/dev/python/pyarrow/_compute.cpython-37m-x86_64-linux-gnu.so)
{code}

Since Archery is a tool for developing Arrow, current PyArrow may be broken or incompatible with the currently available Arrow C++.",pull-request-available,"['Archery', 'Benchmarking', 'Developer Tools']",ARROW,Bug,Major,2021-07-21 16:05:38,2
13391158,[C++] conda-forge benchmark library rejected,"It seems that our detection routine for the C++ benchmark library got broken recently:
{code}
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:235 (find_package):
  Could not find a configuration file for package ""benchmark"" that is
  compatible with requested version ""0.0.0"".

  The following configuration files were considered but not accepted:

    /home/antoine/miniconda3/envs/pyarrow/lib/cmake/benchmark/benchmarkConfig.cmake, version: 1.5.4

Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:1852 (resolve_dependency)
  CMakeLists.txt:515 (include)

{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2021-07-21 15:55:33,1
13390759,[C++][Compute] Document out-of-source addition to the FunctionRegistry,"The FunctionRegistry class provides support for consumers of the library to define their own compute functions and add them to the registry for usage in filter expressions, projections, etc. We don't have any documentation on how to do this, and it'd be worthwhile to add an example too.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-19 18:27:09,0
13390748,[C++] Specify minimum required zstd version in cmake,"This causes the build to fail as ZSTD_minCLevel() can't be found [https://github.com/ursacomputing/crossbow/runs/3105438383] 
{noformat}
[ 54%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/csv/converter.cc.o
cd /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/src/arrow && /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/build-apache-arrow/Library/Homebrew/shims/mac/super/clang++ -DARROW_EXPORTING -DARROW_HAVE_RUNTIME_AVX2 -DARROW_HAVE_RUNTIME_AVX512 -DARROW_HAVE_RUNTIME_BMI2 -DARROW_HAVE_RUNTIME_SSE4_2 -DARROW_HAVE_SSE4_2 -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR="""" -DARROW_MIMALLOC -DARROW_WITH_BACKTRACE -DARROW_WITH_LZ4 -DARROW_WITH_RE2 -DARROW_WITH_SNAPPY -DARROW_WITH_TIMING_TESTS -DARROW_WITH_UTF8PROC -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DURI_STATIC_BUILD -DUTF8PROC_STATIC -I/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/src -I/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/cpp/src -I/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/cpp/src/generated -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/cpp/thirdparty/flatbuffers/include -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/jemalloc_ep-prefix/src -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/include -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/rapidjson_ep/src/rapidjson_ep-install/include -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/xsimd_ep/src/xsimd_ep-install/include -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/re2_ep-install/include -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/build/utf8proc_ep-install/include -isystem /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/cpp/thirdparty/hadoop/include -Qunused-arguments -fcolor-diagnostics -O3 -DNDEBUG  -Wall -Wno-unknown-Note-option -Wno-pass-failed -stdlib=libc++ -msse4.2  -DNDEBUG -isysroot /Applications/Xcode_12.4.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX11.1.sdk -mmacosx-version-min=10.15 -fPIC -std=c++11 -o CMakeFiles/arrow_objlib.dir/csv/converter.cc.o -c /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/cpp/src/arrow/csv/converter.cc
/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp/apache-arrow-20210719-2645-6nb4zw/cpp/src/arrow/util/compression_zstd.cc:231:59: error: use of undeclared identifier 'ZSTD_minCLevel'; did you mean 'ZSTD_maxCLevel'?
  int minimum_compression_level() const override { return ZSTD_minCLevel(); }
                                                          ^~~~~~~~~~~~~~
                                                          ZSTD_maxCLevel
/private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/build-apache-arrow/include/zstd.h:142:25: note: 'ZSTD_maxCLevel' declared here
ZSTDLIB_API int         ZSTD_maxCLevel(void);               /*!< maximum compression level available */
                        ^
1 error generated.
make[2]: *** [src/arrow/CMakeFiles/arrow_objlib.dir/util/compression_zstd.cc.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [src/arrow/CMakeFiles/arrow_objlib.dir/all] Error 2
make: *** [all] Error 2==> Formula
Path: /Users/runner/work/crossbow/crossbow/arrow/r/check/arrow.Rcheck/00_pkg_src/arrow/tools/apache-arrow.rb
==> Configuration
HOMEBREW_VERSION: >=1.7.1 (shallow or no git repository)
ORIGIN: (none)
HEAD: (none)
Last commit: never
Core tap ORIGIN: https://github.com/autobrew/homebrew-core
Core tap HEAD: 73b60196102b25b6a91197210cad74ae7833b871
Core tap last commit: 8 weeks ago
HOMEBREW_PREFIX: /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/build-apache-arrow
HOMEBREW_REPOSITORY: /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/build-apache-arrow
HOMEBREW_CELLAR: /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/build-apache-arrow/Cellar
HOMEBREW_CACHE: /var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/
HOMEBREW_TEMP: /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/hbtmp
HOMEBREW_CASK_OPTS: --no-quarantine
HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS: 3650
HOMEBREW_GIT: git
HOMEBREW_NO_ANALYTICS: 1
HOMEBREW_NO_AUTO_UPDATE: 1
CPU: 3-core 64-bit ivybridge
Homebrew Ruby: 2.3.7 => /private/var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T/build-apache-arrow/Library/Homebrew/vendor/portable-ruby/2.3.7/bin/ruby
Clang: 12.0 build 1200
Git: 2.24.3 => /Applications/Xcode_12.4.app/Contents/Developer/usr/bin/git
Curl: 7.64.1 => /usr/bin/curl
Java: 14.0.2, 13.0.2, 12.0.2, 11.0.11, 1.8.0_292
macOS: 10.15.7-x86_64
CLT: 12.4.0.0.1.1610135815
CLT headers: 12.4.0.0.1.1610135815
Xcode: 12.4 => /Applications/Xcode_12.4.app/Contents/Developer
Error: Error: apache-arrow HEAD-91a6ea4 did not build {noformat}",pull-request-available,['C++'],ARROW,Bug,Blocker,2021-07-19 16:48:21,3
13390731,[C++] Aggregation over scalars fails autobrew R job,"[https://github.com/ursacomputing/crossbow/runs/3091873413#step:7:488]
{noformat}
 *** caught illegal operation ***
address 0x109dc30cc, cause 'illegal opcode'Traceback:
 1: compute__CallFunction(function_name, args, options)
 2: call_function(FUN, a, options = list(na.rm = na.rm, na.min_count = na.min_count))
 3: scalar_aggregate(""sum"", ..., na.rm = na.rm)
 4: sum.ArrowDatum(<environment>, na.rm = FALSE)
 5: eval_bare(expr, quo_get_env(quo))
 6: quasi_label(enquo(object), arg = ""object"") {noformat}
I would guess at first glance the compiler is autovectorizing something more than necessary?",pull-request-available,['C++'],ARROW,Bug,Blocker,2021-07-19 14:51:05,0
13390072,[R] bindings for sign(),Following ARROW-12861,pull-request-available,['R'],ARROW,New Feature,Major,2021-07-16 14:52:42,4
13390044,[Documentation] Build failing with sphinx.util.cfamily.DefinitionError,"Might need someone to dig into the sphinx internals to try to get more info about the failure.
{noformat}
Sphinx parallel build error:
sphinx.util.cfamily.DefinitionError: Invalid C++ declaration: Expected identifier in nested name. [error at 8]
  typename...
  --------^
2
Error: `docker-compose --file /home/vsts/work/1/s/arrow/docker-compose.yml run --rm -e SETUPTOOLS_SCM_PRETEND_VERSION=5.0.0.dev445 ubuntu-docs` exited with a non-zero exit code 2, see the process log above.The docker-compose command was invoked with the following parameters: {noformat}
[https://dev.azure.com/ursacomputing/crossbow/_build/results?buildId=8115&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181]",pull-request-available,['Documentation'],ARROW,Bug,Major,2021-07-16 12:40:37,0
13390043,[C++] Valgrind failure in case_when kernel,"https://github.com/ursacomputing/crossbow/runs/3083584283

{noformat}
[ RUN      ] TestCaseWhen.FixedSizeBinary
==11732== Conditional jump or move depends on uninitialised value(s)
==11732==    at 0x41153FD: arrow::TestInitialized(arrow::ArrayData const&) (gtest_util.cc:665)
==11732==    by 0x75BE16: arrow::compute::(anonymous namespace)::ValidateOutput(arrow::ArrayData const&) (test_util.cc:231)
==11732==    by 0x75C3F6: arrow::compute::ValidateOutput(arrow::Datum const&) (test_util.cc:262)
==11732==    by 0x7609EA: arrow::compute::(anonymous namespace)::CheckScalarNonRecursive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::Datum const&, arrow::compute::FunctionOptions const*) (test_util.cc:52)
==11732==    by 0x764317: arrow::compute::CheckScalar(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::Datum, arrow::compute::FunctionOptions const*) (test_util.cc:106)
==11732==    by 0x675AA7: arrow::compute::TestCaseWhen_FixedSizeBinary_Test::TestBody() (scalar_if_else_test.cc:798)
==11732==    by 0x5BBB98D: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBBBE0: testing::Test::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBBF0E: testing::TestInfo::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBC035: testing::TestSuite::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBC5EB: testing::internal::UnitTestImpl::RunAllTests() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBC858: testing::UnitTest::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x41FB07E: main (in /opt/conda/envs/arrow/lib/libgtest_main.so)
==11732== 
{
   <insert_a_suppression_name_here>
   Memcheck:Cond
   fun:_ZN5arrow15TestInitializedERKNS_9ArrayDataE
   fun:_ZN5arrow7compute12_GLOBAL__N_114ValidateOutputERKNS_9ArrayDataE
   fun:_ZN5arrow7compute14ValidateOutputERKNS_5DatumE
   fun:_ZN5arrow7compute12_GLOBAL__N_123CheckScalarNonRecursiveERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorINS_5DatumESaISB_EERKSB_PKNS0_15FunctionOptionsE
   fun:_ZN5arrow7compute11CheckScalarENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorINS_5DatumESaIS8_EES8_PKNS0_15FunctionOptionsE
   fun:_ZN5arrow7compute33TestCaseWhen_FixedSizeBinary_Test8TestBodyEv
   fun:_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS_4TestEvEET0_PT_MS4_FS3_vEPKc
   fun:_ZN7testing4Test3RunEv
   fun:_ZN7testing8TestInfo3RunEv
   fun:_ZN7testing9TestSuite3RunEv
   fun:_ZN7testing8internal12UnitTestImpl11RunAllTestsEv
   fun:_ZN7testing8UnitTest3RunEv
   fun:main
}
==11732== Conditional jump or move depends on uninitialised value(s)
==11732==    at 0x41153FD: arrow::TestInitialized(arrow::ArrayData const&) (gtest_util.cc:665)
==11732==    by 0x411544E: arrow::TestInitialized(arrow::Array const&) (gtest_util.cc:646)
==11732==    by 0x75BFAF: arrow::compute::(anonymous namespace)::ValidateOutput(arrow::ChunkedArray const&) (test_util.cc:237)
==11732==    by 0x75C3C0: arrow::compute::ValidateOutput(arrow::Datum const&) (test_util.cc:265)
==11732==    by 0x765301: arrow::compute::CheckScalar(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::Datum, arrow::compute::FunctionOptions const*) (test_util.cc:170)
==11732==    by 0x675AA7: arrow::compute::TestCaseWhen_FixedSizeBinary_Test::TestBody() (scalar_if_else_test.cc:798)
==11732==    by 0x5BBB98D: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBBBE0: testing::Test::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBBF0E: testing::TestInfo::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBC035: testing::TestSuite::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBC5EB: testing::internal::UnitTestImpl::RunAllTests() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x5BBC858: testing::UnitTest::Run() (in /opt/conda/envs/arrow/lib/libgtest.so)
==11732==    by 0x41FB07E: main (in /opt/conda/envs/arrow/lib/libgtest_main.so)
==11732== 
{
   <insert_a_suppression_name_here>
   Memcheck:Cond
   fun:_ZN5arrow15TestInitializedERKNS_9ArrayDataE
   fun:_ZN5arrow15TestInitializedERKNS_5ArrayE
   fun:_ZN5arrow7compute12_GLOBAL__N_114ValidateOutputERKNS_12ChunkedArrayE
   fun:_ZN5arrow7compute14ValidateOutputERKNS_5DatumE
   fun:_ZN5arrow7compute11CheckScalarENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorINS_5DatumESaIS8_EES8_PKNS0_15FunctionOptionsE
   fun:_ZN5arrow7compute33TestCaseWhen_FixedSizeBinary_Test8TestBodyEv
   fun:_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS_4TestEvEET0_PT_MS4_FS3_vEPKc
   fun:_ZN7testing4Test3RunEv
   fun:_ZN7testing8TestInfo3RunEv
   fun:_ZN7testing9TestSuite3RunEv
   fun:_ZN7testing8internal12UnitTestImpl11RunAllTestsEv
   fun:_ZN7testing8UnitTest3RunEv
   fun:main
}
[       OK ] TestCaseWhen.FixedSizeBinary (171 ms)
[ RUN      ] TestCaseWhen.DispatchBest
[       OK ] TestCaseWhen.DispatchBest (36 ms)
[----------] 6 tests from TestCaseWhen (889 ms total)

[----------] Global test environment tear-down
[==========] 771 tests from 180 test suites ran. (50602 ms total)
[  PASSED  ] 771 tests.

  YOU HAVE 1 DISABLED TEST

==11732== 
==11732== HEAP SUMMARY:
==11732==     in use at exit: 10,082 bytes in 160 blocks
==11732==   total heap usage: 3,142,063 allocs, 3,141,903 frees, 1,148,265,868 bytes allocated
==11732== 
==11732== LEAK SUMMARY:
==11732==    definitely lost: 0 bytes in 0 blocks
==11732==    indirectly lost: 0 bytes in 0 blocks
==11732==      possibly lost: 0 bytes in 0 blocks
==11732==    still reachable: 10,082 bytes in 160 blocks
==11732==         suppressed: 0 bytes in 0 blocks
==11732== Reachable blocks (those to which a pointer was found) are not shown.
==11732== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==11732== 
==11732== Use --track-origins=yes to see where uninitialised values come from
==11732== For lists of detected and suppressed errors, rerun with: -s
==11732== ERROR SUMMARY: 18 errors from 2 contexts (suppressed: 401021 from 59)

      Start 32: arrow-compute-expression-test
{noformat}",pull-request-available,['C++'],ARROW,Bug,Major,2021-07-16 12:38:53,0
13389992,[Python][CI] conda-python-3.7-pandas-0.24 nightly build failing in test_extract_datetime_components,"Due to using {{astype(""Int64"")}} (with a capital, to mean a nullable integer type), which doesn't work yet in pandas 0.24 to then convert that to pyarrow",pull-request-available,['Python'],ARROW,Test,Major,2021-07-16 08:50:48,5
13389889,[C++] Remove compile time parsing from EnumType,"EnumType doesn't *need* to parse a string at compile time, remove that logic to ensure minimal build time overhead",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-15 17:42:53,6
13389670,[C++] Segfault in arrow-compute-plan-test ExecPlanExecution.SourceScalarAggSink,"https://github.com/apache/arrow/pull/10608/checks?check_run_id=3068468137
On ""AMD64 MacOS 10.15 C++""

Looks like a state wasn't initialized somehow?

{noformat}
[ RUN      ] ExecPlanExecution.SourceScalarAggSink
Found core dump, printing backtrace:
(lldb) target create --core ""core.arrow-compute-pl.19192""
Core file '/Users/runner/work/arrow/arrow/build/cpp/build/test-work/arrow-compute-plan-test/core.arrow-compute-pl.19192' (x86_64) was loaded.(lldb) thread backtrace all -e true
error: exec_plan.cc.o 0x0004f7d7: DW_TAG_inheritance failed to resolve the base class at 0x0004f8fb from enclosing type 0x0004f7ce. 
Please file a bug and attach the file at the start of this error message
* thread #1, stop reason = signal SIGSTOP
  * frame #0: 0x000000010776df15 libarrow.500.0.0.dylib`std::__1::unique_ptr<arrow::compute::KernelState, std::__1::default_delete<arrow::compute::KernelState> >::get(this=0x0000000000000000) const at memory:2596:19
    frame #1: 0x000000010779c268 libarrow.500.0.0.dylib`arrow::compute::ScalarAggregateNode::DoConsume(this=0x00007f8f514731c0, batch=0x00007ffeef01ea38, thread_index=0) at exec_plan.cc:665:51
    frame #2: 0x000000010779aae8 libarrow.500.0.0.dylib`arrow::compute::ScalarAggregateNode::InputReceived(this=0x00007f8f514731c0, input=0x00007f8f51470880, seq=1, batch=ExecBatch @ 0x00007ffeef01ea38) at exec_plan.cc:683:17
    frame #3: 0x000000010777e65e libarrow.500.0.0.dylib`arrow::compute::SourceNode::StartProducing(this=0x00007f8f51473c78, batch=0x00007f8f514739c8)::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&)::operator()(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&) const at exec_plan.cc:287:38
    frame #4: 0x000000010777e383 libarrow.500.0.0.dylib`std::__1::enable_if<((!(std::is_void<nonstd::optional_lite::optional<int> >::value)) && (!(is_future<nonstd::optional_lite::optional<int> >::value))) && ((!(arrow::Future<nonstd::optional_lite::optional<int> >::is_empty)) || (std::is_same<nonstd::optional_lite::optional<int>, arrow::Status>::value)), void>::type arrow::detail::ContinueFuture::operator(this=0x00007ffeef01ec28, next=Future<nonstd::optional_lite::optional<int> > @ 0x00007ffeef01eb68, f=0x00007f8f51473c78, a=0x00007f8f514739c8)<arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&, nonstd::optional_lite::optional<int>, arrow::Future<nonstd::optional_lite::optional<int> > >(arrow::Future<nonstd::optional_lite::optional<int> >, arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&)&&, nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&) const at future.h:138:23
    frame #5: 0x000000010777e199 libarrow.500.0.0.dylib`void arrow::detail::ContinueFuture::IgnoringArgsIf<arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::Future<nonstd::optional_lite::optional<int> >, nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&>(this=0x00007ffeef01ec28, (null)=std::__1::false_type @ 0x00007ffeef01eb98, next=0x00007f8f51473c98, f=0x00007f8f51473c78, a=0x00007f8f514739c8)::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&)&&, nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&) const at future.h:181:5
    frame #6: 0x000000010777e016 libarrow.500.0.0.dylib`arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing(this=0x00007f8f51473c78, result=0x00007f8f514739c0)::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)>::operator()(arrow::Result<nonstd::optional_lite::optional<arrow::compute::ExecBatch> > const&) && at future.h:589:25
    frame #7: 0x000000010777df62 libarrow.500.0.0.dylib`arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::WrapResultyOnComplete::Callback<arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing(this=0x00007f8f51473c78, impl=0x00007f8f51474040)::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)> >::operator()(arrow::FutureImpl const&) && at future.h:486:9
    frame #8: 0x000000010777decb libarrow.500.0.0.dylib`arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::FnImpl<arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::WrapResultyOnComplete::Callback<arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing(this=0x00007f8f51473c70, a=0x00007f8f51474040)::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)> > >::invoke(arrow::FutureImpl const&) at functional.h:152:42
    frame #9: 0x0000000107433d55 libarrow.500.0.0.dylib`arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::operator(this=0x00007ffeef01ee30, a=0x00007f8f51474040)(arrow::FutureImpl const&) && at functional.h:140:17
    frame #10: 0x0000000107433729 libarrow.500.0.0.dylib`arrow::ConcreteFutureImpl::RunOrScheduleCallback(this=0x00007f8f51474040, callback_record=0x00007ffeef01ee30, in_add_callback=true) at future.cc:295:7
    frame #11: 0x000000010742de87 libarrow.500.0.0.dylib`arrow::ConcreteFutureImpl::AddCallback(this=0x00007f8f51474040, callback=arrow::FutureImpl::Callback @ 0x00007ffeef01eec0, opts=(should_schedule = Never, executor = 0x0000000000000000))>, arrow::CallbackOptions) at future.cc:248:7
    frame #12: 0x000000010742dd9c libarrow.500.0.0.dylib`arrow::FutureImpl::AddCallback(this=0x00007f8f51474040, callback=arrow::FutureImpl::Callback @ 0x00007ffeef01ef60, opts=(should_schedule = Never, executor = 0x0000000000000000))>, arrow::CallbackOptions) at future.cc:383:28
    frame #13: 0x000000010777da47 libarrow.500.0.0.dylib`void arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::AddCallback<arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing(this=0x00007ffeef01f0c8, on_complete=ThenOnComplete<(lambda at /Users/runner/work/arrow/arrow/cpp/src/arrow/compute/exec/exec_plan.cc:279:23), (lambda at /Users/runner/work/arrow/arrow/cpp/src/arrow/compute/exec/exec_plan.cc:290:23)> @ 0x00007ffeef01efd8, opts=(should_schedule = Never, executor = 0x0000000000000000))::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)>, arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::WrapResultyOnComplete::Callback<arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)> > >(arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)>, arrow::CallbackOptions) const at future.h:537:12
    frame #14: 0x000000010777d033 libarrow.500.0.0.dylib`arrow::Future<nonstd::optional_lite::optional<int> > arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::Then<arrow::compute::SourceNode::StartProducing(this=0x00007ffeef01f0c8, on_success=(anonymous class) @ 0x00007ffeef01f028, on_failure=(anonymous class) @ 0x00007ffeef01f018, options=(should_schedule = Never, executor = 0x0000000000000000))::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&), arrow::Future<nonstd::optional_lite::optional<arrow::compute::ExecBatch> >::ThenOnComplete<arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&)>, arrow::Future<nonstd::optional_lite::optional<int> > >(arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(nonstd::optional_lite::optional<arrow::compute::ExecBatch> const&), arrow::compute::SourceNode::StartProducing()::'lambda'()::operator()() const::'lambda'(arrow::Status const&), arrow::CallbackOptions) const at future.h:651:5
    frame #15: 0x000000010777cb90 libarrow.500.0.0.dylib`arrow::compute::SourceNode::StartProducing(this=0x00007f8f51472558)::'lambda'()::operator()() const at exec_plan.cc:278:39
    frame #16: 0x00000001077800c6 libarrow.500.0.0.dylib`arrow::Future<int> arrow::Loop<arrow::compute::SourceNode::StartProducing(this=0x00007f8f51472558, maybe_control=0x00007f8f5141d520)::'lambda'(), nonstd::optional_lite::optional<int>, int>(arrow::compute::SourceNode::StartProducing()::'lambda'())::Callback::operator()(arrow::Result<nonstd::optional_lite::optional<int> > const&) && at future.h:900:26
    frame #17: 0x0000000107780072 libarrow.500.0.0.dylib`arrow::Future<nonstd::optional_lite::optional<int> >::WrapResultyOnComplete::Callback<arrow::Future<int> arrow::Loop<arrow::compute::SourceNode::StartProducing(this=0x00007f8f51472558, impl=0x00007f8f51473a10)::'lambda'(), nonstd::optional_lite::optional<int>, int>(arrow::compute::SourceNode::StartProducing()::'lambda'())::Callback>::operator()(arrow::FutureImpl const&) && at future.h:486:9
    frame #18: 0x000000010777ffdb libarrow.500.0.0.dylib`arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::FnImpl<arrow::Future<nonstd::optional_lite::optional<int> >::WrapResultyOnComplete::Callback<arrow::Future<int> arrow::Loop<arrow::compute::SourceNode::StartProducing(this=0x00007f8f51472550, a=0x00007f8f51473a10)::'lambda'(), nonstd::optional_lite::optional<int>, int>(arrow::compute::SourceNode::StartProducing()::'lambda'())::Callback> >::invoke(arrow::FutureImpl const&) at functional.h:152:42
    frame #19: 0x0000000107433d55 libarrow.500.0.0.dylib`arrow::internal::FnOnce<void (arrow::FutureImpl const&)>::operator(this=0x00007ffeef01f3c0, a=0x00007f8f51473a10)(arrow::FutureImpl const&) && at functional.h:140:17
    frame #20: 0x0000000107433729 libarrow.500.0.0.dylib`arrow::ConcreteFutureImpl::RunOrScheduleCallback(this=0x00007f8f51473a10, callback_record=0x00007ffeef01f3c0, in_add_callback=true) at future.cc:295:7
    frame #21: 0x000000010742de87 libarrow.500.0.0.dylib`arrow::ConcreteFutureImpl::AddCallback(this=0x00007f8f51473a10, callback=arrow::FutureImpl::Callback @ 0x00007ffeef01f450, opts=(should_schedule = Never, executor = 0x0000000000000000))>, arrow::CallbackOptions) at future.cc:248:7
    frame #22: 0x000000010742dd9c libarrow.500.0.0.dylib`arrow::FutureImpl::AddCallback(this=0x00007f8f51473a10, callback=arrow::FutureImpl::Callback @ 0x00007ffeef01f4f0, opts=(should_schedule = Never, executor = 0x0000000000000000))>, arrow::CallbackOptions) at future.cc:383:28
    frame #23: 0x000000010777cc87 libarrow.500.0.0.dylib`void arrow::Future<nonstd::optional_lite::optional<int> >::AddCallback<arrow::Future<int> arrow::Loop<arrow::compute::SourceNode::StartProducing(this=0x00007ffeef01f5a0, on_complete=Callback @ 0x00007ffeef01f560, opts=(should_schedule = Never, executor = 0x0000000000000000))::'lambda'(), nonstd::optional_lite::optional<int>, int>(arrow::compute::SourceNode::StartProducing()::'lambda'())::Callback, arrow::Future<nonstd::optional_lite::optional<int> >::WrapResultyOnComplete::Callback<arrow::Future<int> arrow::Loop<arrow::compute::SourceNode::StartProducing()::'lambda'(), nonstd::optional_lite::optional<int>, int>(arrow::compute::SourceNode::StartProducing()::'lambda'())::Callback> >(arrow::compute::SourceNode::StartProducing()::'lambda'(), arrow::CallbackOptions) const at future.h:537:12
    frame #24: 0x000000010777c79a libarrow.500.0.0.dylib`arrow::Future<int> arrow::Loop<arrow::compute::SourceNode::StartProducing(iterate=(anonymous class) @ 0x00007ffeef01f5d0)::'lambda'(), nonstd::optional_lite::optional<int>, int>(arrow::compute::SourceNode::StartProducing()::'lambda'()) at future.h:928:15
    frame #25: 0x000000010777c182 libarrow.500.0.0.dylib`arrow::compute::SourceNode::StartProducing(this=0x00007f8f51470880) at exec_plan.cc:270:17
    frame #26: 0x0000000107769ab1 libarrow.500.0.0.dylib`arrow::compute::(anonymous namespace)::ExecPlanImpl::StartProducing(this=0x00007f8f51417b50) at exec_plan.cc:96:18
    frame #27: 0x0000000107769926 libarrow.500.0.0.dylib`arrow::compute::ExecPlan::StartProducing(this=0x00007f8f51417b50) at exec_plan.cc:198:61
    frame #28: 0x0000000100bf1caa arrow-compute-plan-test`arrow::compute::(anonymous namespace)::StartAndCollect(plan=0x00007f8f51417b50, gen=arrow::AsyncGenerator<util::optional<ExecBatch> > @ 0x00007ffeef01ff00)>) at plan_test.cc:241:3
    frame #29: 0x0000000100bfaa66 arrow-compute-plan-test`arrow::compute::ExecPlanExecution_SourceScalarAggSink_Test::TestBody(this=0x00007f8f5141cf00) at plan_test.cc:442:3
    frame #30: 0x0000000101bd1e44 libgtestd.dylib`void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(object=0x00007f8f5141cf00, method=21 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00, location=""the test body"")(), char const*) at gtest.cc:2433:10
    frame #31: 0x0000000101b9ae2b libgtestd.dylib`void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(object=0x00007f8f5141cf00, method=21 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00, location=""the test body"")(), char const*) at gtest.cc:2469:14
    frame #32: 0x0000000101b9ad63 libgtestd.dylib`testing::Test::Run(this=0x00007f8f5141cf00) at gtest.cc:2508:5
    frame #33: 0x0000000101b9bf67 libgtestd.dylib`testing::TestInfo::Run(this=0x00007f8f514179f0) at gtest.cc:2684:11
    frame #34: 0x0000000101b9ce4c libgtestd.dylib`testing::TestSuite::Run(this=0x00007f8f51417110) at gtest.cc:2816:28
    frame #35: 0x0000000101ba96ed libgtestd.dylib`testing::internal::UnitTestImpl::RunAllTests(this=0x00007f8f514163c0) at gtest.cc:5338:44
    frame #36: 0x0000000101bd7504 libgtestd.dylib`bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(object=0x00007f8f514163c0, method=f0 92 ba 01 01 00 00 00 00 00 00 00 00 00 00 00, location=""auxiliary test code (environments or event listeners)"")(), char const*) at gtest.cc:2433:10
    frame #37: 0x0000000101ba908b libgtestd.dylib`bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(object=0x00007f8f514163c0, method=f0 92 ba 01 01 00 00 00 00 00 00 00 00 00 00 00, location=""auxiliary test code (environments or event listeners)"")(), char const*) at gtest.cc:2469:14
    frame #38: 0x0000000101ba8f5d libgtestd.dylib`testing::UnitTest::Run(this=0x0000000101bf0360) at gtest.cc:4925:10
    frame #39: 0x0000000101b7eed1 libgtest_maind.dylib`RUN_ALL_TESTS() at gtest.h:2473:46
    frame #40: 0x0000000101b7eeb0 libgtest_maind.dylib`main(argc=1, argv=0x00007ffeef0205f8) at gtest_main.cc:45:10
    frame #41: 0x00007fff6b0d9cc9 libdyld.dylib`start + 1
    frame #42: 0x00007fff6b0d9cc9 libdyld.dylib`start + 1
  thread #2, stop reason = signal SIGSTOP
    frame #0: 0x00007fff6b21d882 libsystem_kernel.dylib`__psynch_cvwait + 10
    frame #1: 0x00007fff6b2e2425 libsystem_pthread.dylib`_pthread_cond_wait + 698
    frame #2: 0x00007fff683b4592 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
    frame #3: 0x00000001074a46f2 libarrow.500.0.0.dylib`arrow::internal::WorkerLoop(state=std::__1::shared_ptr<arrow::internal::ThreadPool::State>::element_type @ 0x00007f8f51470d38 strong=5 weak=1, it=std::__1::list<std::__1::thread, std::__1::allocator<std::__1::thread> >::iterator @ 0x000070000d16feb8) at thread_pool.cc:192:16
    frame #4: 0x00000001074a41ff libarrow.500.0.0.dylib`arrow::internal::ThreadPool::LaunchWorkersUnlocked(this=0x00007f8f51472418)::$_2::operator()() const at thread_pool.cc:336:7
    frame #5: 0x00000001074a414d libarrow.500.0.0.dylib`decltype(__f=0x00007f8f51472418)::$_2>(fp)()) std::__1::__invoke<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2>(arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2&&) at type_traits:3545:1
    frame #6: 0x00000001074a40b5 libarrow.500.0.0.dylib`void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2>(__t=size=2, (null)=__tuple_indices<> @ 0x000070000d16ff58)::$_2>&, std::__1::__tuple_indices<>) at thread:273:5
    frame #7: 0x00000001074a3806 libarrow.500.0.0.dylib`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2> >(__vp=0x00007f8f51472410) at thread:284:5
    frame #8: 0x00007fff6b2e2109 libsystem_pthread.dylib`_pthread_start + 148
    frame #9: 0x00007fff6b2ddb8b libsystem_pthread.dylib`thread_start + 15
  thread #3, stop reason = signal SIGSTOP
    frame #0: 0x00007fff6b21d882 libsystem_kernel.dylib`__psynch_cvwait + 10
    frame #1: 0x00007fff6b2e2425 libsystem_pthread.dylib`_pthread_cond_wait + 698
    frame #2: 0x00007fff683b4592 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
    frame #3: 0x00000001074a46f2 libarrow.500.0.0.dylib`arrow::internal::WorkerLoop(state=std::__1::shared_ptr<arrow::internal::ThreadPool::State>::element_type @ 0x00007f8f51470d38 strong=5 weak=1, it=std::__1::list<std::__1::thread, std::__1::allocator<std::__1::thread> >::iterator @ 0x000070000d1f2eb8) at thread_pool.cc:192:16
    frame #4: 0x00000001074a41ff libarrow.500.0.0.dylib`arrow::internal::ThreadPool::LaunchWorkersUnlocked(this=0x00007f8f51604328)::$_2::operator()() const at thread_pool.cc:336:7
    frame #5: 0x00000001074a414d libarrow.500.0.0.dylib`decltype(__f=0x00007f8f51604328)::$_2>(fp)()) std::__1::__invoke<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2>(arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2&&) at type_traits:3545:1
    frame #6: 0x00000001074a40b5 libarrow.500.0.0.dylib`void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2>(__t=size=2, (null)=__tuple_indices<> @ 0x000070000d1f2f58)::$_2>&, std::__1::__tuple_indices<>) at thread:273:5
    frame #7: 0x00000001074a3806 libarrow.500.0.0.dylib`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::$_2> >(__vp=0x00007f8f51604320) at thread:284:5
    frame #8: 0x00007fff6b2e2109 libsystem_pthread.dylib`_pthread_start + 148
    frame #9: 0x00007fff6b2ddb8b libsystem_pthread.dylib`thread_start + 15
~/work/arrow/arrow/build/cpp/src/arrow/compute/exec {noformat}",pull-request-available,['C++'],ARROW,Bug,Major,2021-07-14 16:40:09,6
13389389,[Archery] Validate docker compose configuration,The previous validation command wasn't actually executing the validation.,pull-request-available,['Archery'],ARROW,Improvement,Major,2021-07-13 13:02:00,3
13389187,[C++][Compute] Add ScalarAggregateNode,Provide an ExecNode which wraps ScalarAggregateFunctions,pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-12 16:27:04,6
13389144,[C++] Use reflection-based enums for compute options,This will reduce boilerplate and give us consistent naming. To be done after ARROW-13296.,compute pull-request-available,"['C++', 'Python', 'R']",ARROW,Improvement,Major,2021-07-12 13:24:17,0
13388727,[C++] Implement hash_aggregate any/all Boolean kernels,These would mimic Pandas's [DataFrameGroupBy.all/DataFrameGroupBy.any|https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.all.html].,pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-09 18:53:21,0
13388698,[C++] Provide reflection-compatible enum replacement,"There's significant boilerplate associated with using enumerations in arrow, since we need to expose them to the bindings which involves a mapping to/from strings per binding. That's not ideal and we could do better with a metaprogramming utility which provides enum semantics but supports basic reflection (min/max value, number of values, to/from string).",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-09 16:20:04,6
13388690,[C++] Implement hash_aggregate mean/stdev/variance kernels,"We have scalar aggregate kernels for these already that can serve as the basis for a hash aggregate implementation.

Depends on ARROW-12759 as that refactors how hash aggregate kernels are implemented.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-09 15:34:54,0
13388466,[GLib][CI] Require gobject-introspection 3.4.5 or later,It's needed for Flight tests.,pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Improvement,Major,2021-07-08 19:45:01,1
13388444,[C++] Log functions don't have int kernels,"I've been writing R bindings for the log functions implemented in this PR: [https://github.com/apache/arrow/pull/10567]

They work when the inputs are floats, but when the input is an int32 I get the following error:
{code:java}
NotImplemented: Function ln_checked has no kernel matching input types (scalar[int32]){code}
",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-08 18:14:25,0
13388258,[CI] Require docker-compose 1.27.0 or later,"We need it for ""extends"".

See also:

  * https://issues.apache.org/jira/browse/ARROW-13199
  * https://github.com/apache/arrow/pull/10611
  * https://github.com/docker/compose/pull/7588",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2021-07-08 00:18:29,1
13388216,[Developer Tools] Support passing through memory limits in archery docker run,"It can be useful to limit the memory available to one of our Docker containers to more accurately reproduce CI conditions. docker-compose doesn't directly support this (except in 'swarm' deployment mode) but since Archery can directly invoke the docker CLI, we could support this ourselves, either by adding custom metadata into docker-compose.yml or exposing a manual CLI parameter.",pull-request-available,"['Archery', 'Developer Tools']",ARROW,Improvement,Major,2021-07-07 15:51:53,0
13388045,[C++] Don't use .pc only in CMake paths for Requires.private,Because they can't be found by raw pkg-config usage.,pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-07 00:21:39,1
13387688,[Python] Improve the repr of ParquetFileFragment,"Compare with the legacy version:

{code}
In [5]: d1 = pq.ParquetDataset(""test_partitioned"")

In [6]: d2 = pq.ParquetDataset(""test_partitioned"", use_legacy_dataset=False)

In [7]: d1.pieces[0]
<ipython-input-30-45c15fc36b93>:1: DeprecationWarning: ParquetDataset.pieces attribute is deprecated
  d1.pieces[0]
Out[7]: ParquetDatasetPiece('test_partitioned/B=a/1f73a8652e6242b58de7582b1a514907.parquet', row_group=None, partition_keys=[('B', 0)])

In [8]: d2.fragments[0]
Out[8]: <pyarrow._dataset.ParquetFileFragment at 0x7fec0f88c710>
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2021-07-05 10:03:54,5
13387394,[C++][FlightRPC] Segfault when sending record batch >2GB,"When sending a record batch > 2GiB, the server will segfault. Although Flight checks for this case and returns an error, it turns out that gRPC always tries to increment the refcount of the result buffer whether the serialization handler returned successfully or not:
{code:cpp}
// From gRPC 1.36
Status CallOpSendMessage::SendMessagePtr(const M* message,
                                         WriteOptions options) {
  msg_ = message;
  write_options_ = options;
  // Store the serializer for later since we have access to the message
  serializer_ = [this](const void* message) {
    bool own_buf;
    // TODO(vjpai): Remove the void below when possible
    // The void in the template parameter below should not be needed
    // (since it should be implicit) but is needed due to an observed
    // difference in behavior between clang and gcc for certain internal users
    Status result = SerializationTraits<M, void>::Serialize(
        *static_cast<const M*>(message), send_buf_.bbuf_ptr(), &own_buf);
    if (!own_buf) {
      // XXX(lidavidm): This should perhaps check result.ok(), or Serialize should
      // unconditionally initialize send_buf_
      send_buf_.Duplicate();
    }
    return result;
  };
  return Status();
}
{code}
Hence when Flight returns an error without initializing the buffer, we get a segfault.

Originally reported on StackOverflow: [https://stackoverflow.com/questions/68230146/pyarrow-flight-do-get-segfault-when-pandas-dataframe-over-3gb]",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2021-07-02 19:46:37,0
13387262,[Java][CI] Consistent timeout in the Java JNI build,The JNI build consistently gets killed on github actions.,pull-request-available,"['Continuous Integration', 'Java']",ARROW,Bug,Major,2021-07-02 11:07:35,3
13387120,[C++] Improve decimal random generation,"Right now, random Decimal128 generation in {{testing/random.cc}} does not support precisions greater than 18. Also, Decimal256 generation is not implemented.",pull-request-available,['C++'],ARROW,Task,Major,2021-07-01 19:26:03,2
13387057,[C++][Dataset][Compute] Substitute ExecPlan impl for dataset scans,"ARROW-11930 grew too large to include substitution of an ExecPlan for existing scan machinery, but this still needs to happen",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-01 13:28:06,6
13387044,[Python] Improve repr of pyarrow.compute.FunctionOptions,"ARROW-13025 added a ToString which is now used in the Python repr. This adds the specified parameters nicely, but I would be useful to still include the name of the class as well:

{code}
In [9]: pc.TrimOptions(""a"")
Out[9]: {characters=""a""}
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2021-07-01 12:39:49,0
13387042,[C++] Make type_name equal to options class name for all FunctionOptionTypes,Follow up from ARROW-13025. This will let us get rid of the explicit lookup table in _compute.pyx. Also fix ArithmeticOptions.,pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-01 12:27:32,0
13387026,[C++] Add string padding option to determine which side the extra space goes on,"I'm trying to implement R bindings for the compute function ""utf8_center"" but in cases where there are an odd number of spaces in total, the extra space is added to the left. However, in R, the extra space is added to the right.

Please can we have another string pad option to determine which side the extra space goes, so I can make the functionality match?",pull-request-available,['C++'],ARROW,Improvement,Major,2021-07-01 11:02:09,0
13386954,Add CSV Writer documentation,"The new CSV writer in C++, Python and possibly other languages does not have much documentation. A user who checks out our site would probably still believe that we don't have an CSV writer in C++.",pull-request-available,['Documentation'],ARROW,Improvement,Major,2021-07-01 05:27:06,0
13386898,[Python] Add a general purpose cython trampolining utility,"Currently, invoking python from C++ requires a lot of boilerplate. For example, see any class named {{/\w+Vtable/}} which holds a bundle of function pointers and a single OwnedRef to a python object accessed by those functions. This is unnecessarily baroque for adding simple callbacks and we could instead have a general purpose utility similar to std::bind which can bring a python object into closure.",pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-30 20:45:41,6
13386857,[C++][CI] Fix thread sanitizer failures,See nightly build failure in https://github.com/ursacomputing/crossbow/runs/2949298540,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2021-06-30 16:23:58,2
13386828,[C++] Add a 'choose' kernel/scalar compute function,Emulate SQL's choose or (a very limited subset of) NumPy's choose.,pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-30 15:12:06,0
13386822,[C++][GLib] Demote/deprecate CompareOptions,"CompareOptions is not properly a FunctionOptions but it has bindings in GLib that expect it to be a FunctionOptions. After ARROW-13025, we should make CompareOptions a bare struct (instead of a FunctionOptions subclass) and deprecate it in favor of just specifying the comparison operator directly.",pull-request-available,"['C++', 'GLib']",ARROW,Improvement,Major,2021-06-30 14:56:37,1
13386506,[C++][CI] Remove outdated Github Actions ARM builds,"We used to have ARM self-hosted workers configured for GHA. These builds are permanently set to disabled, we should rather remove those jobs to reduce the noise both in the configuration file and the PR checks.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2021-06-29 11:47:21,3
13386498,[Python][CI] Fix vcpkg caching mechanism for the macOS wheels,"There is a cache hit despite that it has been working for a long time. Perhaps some of the plugin's dependencies may have been updated since then because the generated cache keys are identical, so there should be a cache hit.

Currently installing the vcpkg packages take around ~50m which makes its maintenance really time consuming.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2021-06-29 10:44:12,3
13386193,[Java][Document] Create prose document about Java algorithms,"The document should include the basics of comparing vector elements, searching values in a vector, and sorting vectors. ",pull-request-available,['Java'],ARROW,New Feature,Major,2021-06-28 07:20:05,7
13385905,[c++][python] Possibly memory not deallocated when reading in CSV,"When one reads in a table from CSV in pyarrow version 4.0.1, it appears that the read-in table variable is not freed (or not fast enough). I'm unsure if this is because of pyarrow or because of the way pyarrow memory allocation interacts with Python memory allocation. I encountered it when processing many large CSVs sequentially.

When I run the following piece of code, the RAM memory usage increases quite rapidly until it runs out of memory.

{code:python}
import pyarrow as pa
import pyarrow.csv

# Generate some CSV file to read in
print(""Generating CSV"")
with open(""example.csv"", ""w+"") as f_out:
    for i in range(0, 10000000):
        f_out.write(""123456789,abc def ghi jkl\n"")


def read_in_the_csv():
    table = pa.csv.read_csv(""example.csv"")
    print(table)  # Not strictly necessary to replicate bug, table can also be an unused variable
    # This will free up the memory, as a workaround:
    # table = table.slice(0, 0)


# Read in the CSV many times
print(""Reading in a CSV many times"")
for j in range(100000):
    read_in_the_csv()
{code}
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2021-06-25 16:52:59,2
13385577,[Python] Fix repr and contains of StructScalar with duplicate field names,"Broken off from ARROW-9997

When having duplicate fields, the repr fails:

{code}
In [28]: s = pa.scalar([('a', 1), ('b', 2), ('a', 3)], pa.struct([('a', 'int64'), ('b', 'int64'), ('a', 'int64')]))

In [29]: 0 in s
Out[29]: True

In [30]: s
....
KeyError: 'a'
{code}

In addition, the contains ({{in}}) operation also shouldn't accept integers (this is also the case for non-duplicate fields)",pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-24 07:59:58,5
13385549,[C++] Add find_substring_regex kernel and implement ignore_case for find_substring,"The find_substring compute function uses the MatchSubstringOptions Options class. However, when I try to set ignore_case to TRUE, I get the following error:
{code:java}
 Error: NotImplemented: find_substring with ignore_case
{code}
R code to replicate the error is below, though depends on a currently unmerged branch:
{code:java}
df <- tibble(x = c(""Foo and Bar"", ""baz and qux and quux""))

df %>%
      Table$create() %>%
      mutate(x = arrow_find_substring(x, options = list(pattern = ""b"", ignore_case = TRUE))) %>%
      collect()
{code}

Since case-insensitive search will be implemented using RE2, this is also an opportunity to add a {{find_substring_regex}} compute function.",pull-request-available,['C++'],ARROW,New Feature,Major,2021-06-24 05:09:49,0
13385377,[Dev][Archery] Crossbow build submission fails,"When trying to submit Crossbow jobs, either using the CLI ({{archery crossbow submit JOB}}) or from the Github comment interface ({{\@github-actions crossbow submit JOB}}}, it fails with the following error:
{code}
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/bin/archery"", line 33, in <module>
    sys.exit(load_entry_point('archery', 'console_scripts', 'archery')())
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/decorators.py"", line 27, in new_func
    return f(get_current_context().obj, *args, **kwargs)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/cli.py"", line 127, in submit
    head=arrow_sha, version=arrow_version)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/core.py"", line 712, in from_repo
    version = get_version(repo.path)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/core.py"", line 651, in get_version
    major, minor, patch = map(int, match.groups())
AttributeError: 'NoneType' object has no attribute 'groups'
{code}

This is because the tag returned by {{git describe...}} is ""5.0.0.dev0"", which doesn't match the regex.",pull-request-available,"['Archery', 'Developer Tools']",ARROW,Bug,Critical,2021-06-23 09:49:15,2
13385342,[Java] Respect the rounding policy when allocating vector buffers,"According to the current implementation, the default ""next power of two"" rounding policy is assumed when allocating buffers for a vector. 

In particular, for fixed width vectors, this policy is applied for the validity and data buffers, and for variable width vectors, this policy is applied for the validity and offset buffers. 

However, this default policy is not always used for the allocator. When an alternative policy is in use, the buffers allocated assuming the default policy will have inappropriate capacities, which may lead to waste of memory spaces. 
",pull-request-available,['Java'],ARROW,Improvement,Major,2021-06-23 07:35:34,7
13385122,[C++][Python] HadoopFileSystem: automatically set CLASSPATH based on HADOOP_HOME env variable?,"In the ""legacy"" python-specific HadoopFileSystem implementation, we have a {{_maybe_set_hadoop_classpath}} function which has some logic to set the {{CLASSPATH}} environment variable based on {{HADOOP_HOME}} or the hadoop executable: https://github.com/apache/arrow/blob/c43fab3d621bedef15470a1be43570be2026af20/python/pyarrow/hdfs.py#L134-L149

This is also mentioned in the documentation of the new HadoopFileSystem (https://arrow.apache.org/docs/python/filesystems.html#hadoop-file-system-hdfs ):

> If CLASSPATH is not set, then it will be set automatically if the hadoop executable is in your system path, or if HADOOP_HOME is set.

However, this sentence was probably simply copied over from the docs about the legacy filesystem. And for the new HadoopFileSystem implementation, we don't have this logic to automatically set up {{CLASSPATH}}. 

Do we want to add this logic to the new implementation as well? (in cython, or actually in C++?) Or if not, we should update the docs to clarify that {{CLASSPATH}} is actually required.

cc [~apitrou]",filesystem hdfs pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2021-06-22 09:28:26,5
13384941,"[C++] Add a ""coalesce"" variadic scalar kernel","Add a variadic scalar compute kernel that works like the SQL {{coalesce}} function. It should take arbitrarily many inputs of the same or compatible types and process them row-wise, returning the first non-null value in each row, or returning{{null}} if there are no non-null values in the row.

For example, in the case of 3 integer-type input arrays, this would take inputs:
{code:java}
Array<int32>         Array<int32>         Array<int32>
[                    [                    [
  null,                2,                   3,
  4,                   null,                6,
  null,                null,                9,
  null                 null                 null
]                    ]                    ]
{code}
and return output:
{code:java}
Array<int32>
[ 
  2,
  4,
  9,
  null
] 
{code}
This should accept scalars and recycle their values.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-21 15:53:43,0
13384928,[C++] Fix Status propagation in END_PARQUET_CATCH_EXCEPTIONS,It seems {{END_PARQUET_CATCH_EXCEPTIONS}} misses the logic to propagate a {{Status}} stored in a {{ParquetStatusException}}.,pull-request-available,['C++'],ARROW,Bug,Minor,2021-06-21 14:18:45,2
13384422,[Python] Pyarrow 4.0.0 crashes upon import on macOS 10.13.6,"Our Jenkins worker that we use for building `snowflake-connector-python` has the following setup:


{code:java}
$ uname -a
Darwin imac.local 17.7.0 Darwin Kernel Version 17.7.0: Fri Jul  6 19:54:51 PDT 2018; root:xnu-4570.71.3~2/RELEASE_X86_64 x86_64
$ python --version --version
Python 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
$ pip list
Package    Version
---------- -------
Cython     0.29.23
numpy      1.19.5
pip        21.1.2
pyarrow    4.0.0
setuptools 57.0.0
wheel      0.36.2
{code}
This is in a completely new venv.

Then after installing these dependencies see the issue here:
{code:java}
$ python -c ""import pyarrow""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/Shared/Jenkins/Home/workspace/BuildPyConnector-Mac/venv-36/lib/python3.6/site-packages/pyarrow/__init__.py"", line 63, in <module>
    import pyarrow.lib as _lib
ImportError: dlopen(/Users/Shared/Jenkins/Home/workspace/BuildPyConnector-Mac/venv-36/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin
  Referenced from: /Users/Shared/Jenkins/Home/workspace/BuildPyConnector-Mac/venv-36/lib/python3.6/site-packages/pyarrow/libarrow.400.dylib
  Expected in: /usr/lib/libSystem.B.dylib
 in /Users/Shared/Jenkins/Home/workspace/BuildPyConnector-Mac/venv-36/lib/python3.6/site-packages/pyarrow/libarrow.400.dylib
{code}
I'm sorry I'm not too sure what could be causing this, but please see what Uwe said here: [https://github.com/snowflakedb/snowflake-connector-python/pull/762#issuecomment-863531840]



I'd be happy to help you test a potential fix if you don't have a machine with such an old MacOS version



",pull-request-available,['Python'],ARROW,Bug,Major,2021-06-17 20:29:04,3
13384377,[C++] ByteStreamSplit implementation uses invalid pointer cast,"This code in {{src/parquet/encoding.cc}}:
{code:c++}
template <>
void ByteStreamSplitEncoder<FloatType>::PutArrowArray(const ::arrow::Array& values) {
  DirectPutImpl<::arrow::FloatArray>(values,
                                     reinterpret_cast<::arrow::BufferBuilder*>(&values_));
}

template <>
void ByteStreamSplitEncoder<DoubleType>::PutArrowArray(const ::arrow::Array& values) {
  DirectPutImpl<::arrow::DoubleArray>(
      values, reinterpret_cast<::arrow::BufferBuilder*>(&values_));
}
{code}

Casts a {{TypedBufferBuilder<T>\*}} to a {{BufferBuilder<T>\*}}. Apparently it works by chance...",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-17 14:55:33,2
13384196,[Dev][Archery] Reorganize docker submodule to its own subpackage,Making it easier to extend the docker related functionality and modularize the root CLI configuration.,pull-request-available,"['Archery', 'Developer Tools']",ARROW,Improvement,Major,2021-06-16 16:09:41,3
13384195,[C++] Provide a simple reflection utility for {{struct}}s,"In cases such as ARROW-13025 it's advantageous to avoid boilerplate when dealing with objects which are basic structs of data members. A simple reflection utility (get/set the value of a data member, print the name of a member to string) would allow writing functionality generically in terms of a tuple of properties, greatly reducing boilerplate.

See a sketch of one such utility here https://gist.github.com/bkietz/7899f477e86df49f21ab17201c518d74",pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-16 16:01:06,6
13384191,[C++] Implement logarithm compute functions,"ln, log, log2?",beginner pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-16 15:51:18,0
13384190,[C++] Implement trigonometric compute functions,"sin, cos, asin, acos, tan, atan, cotan, atan2",beginner pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-16 15:50:12,0
13384150,[C++] CreateDir should fail if the target exists and is not a directory,As discussed in https://github.com/apache/arrow/pull/10540#issuecomment-862284472 .,pull-request-available,['C++'],ARROW,Improvement,Minor,2021-06-16 11:43:38,2
13384126,[Python] Test failure with ffspec 2021.6.0,"These failures have started showing up in AppVeyor and can be reproduced locally:

{code}
pyarrow/tests/test_fs.py ........sx.x..s....sx.x..s.....s.....s....s.....s....s....Fs....s.....s....s.....s....s.....s....s.....s.....s.....s....s.....s....s..... [ 40%]
s....s.....s................ssss....................ssss....s.....s................ssss....................ssss................ssss....................ssss....s.. [ 88%]
...s......s.............................                                                                                                                           [100%]

================================================================================ FAILURES ================================================================================
______________________________________________ test_get_file_info[PyFileSystem(FSSpecHandler(fsspec.filesystem(""memory"")))] ______________________________________________
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_fs.py"", line 616, in test_get_file_info
    fs.create_dir(aaa)
  File ""pyarrow/_fs.pyx"", line 460, in pyarrow._fs.FileSystem.create_dir
    check_status(self.fs.CreateDir(directory, recursive=recursive))
  File ""pyarrow/_fs.pyx"", line 1054, in pyarrow._fs._cb_create_dir
    handler.create_dir(frombytes(path), recursive)
  File ""/home/antoine/arrow/dev/python/pyarrow/fs.py"", line 266, in create_dir
    self.fs.mkdir(path, create_parents=recursive)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/implementations/memory.py"", line 99, in mkdir
    raise FileExistsError
FileExistsError
{code}",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2021-06-16 09:59:26,2
13383930,[CI] Forward R argument to ubuntu-docs build,The R version set from environment variable is not propagated to the linux-apt-docs.dockerfile,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2021-06-15 11:43:49,3
13383902,[Release] Generate the API docs in ubuntu 20.10,I noticed that the ubuntu build variable is passed as a runtime environment variable to the container: https://github.com/apache/arrow/blob/master/dev/release/post-09-docs.sh#L49,pull-request-available,['Packaging'],ARROW,Bug,Major,2021-06-15 09:26:07,3
13383774,[Python] Expose C data interface API for pyarrow.Field,The C data interface has not been exposed for Field objects on the python side.,pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-14 17:40:45,3
13383773,[Python] Start with deprecating ParquetDataset custom attributes,"As a first step for ARROW-9720, we should start with deprecating attributes/methods of {{pq.ParquetDataset}} that we would definitely not keep / are conflicting with the ""dataset API"". 

I am thinking of the {{pieces}} attribute (and the {{ParquetDatasetPiece}} class), the {{partitions}} attribute (and the {{ParquetPartitions}} class). 

In addition, some of the keywords are also exposed as properties (memory_map, read_dictionary, buffer_size, fs), and could be deprecated.",pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-14 17:38:07,5
13383738,[C++] Add bitwise arithmetic compute functions,"Implement bitwise operators (and/or/not/xor) and shifts.

Shifts will require some consideration with casting (e.g. we will not want to promote unsigned LHS to signed as that's a different operation - or, perhaps preferably, we should implement separate logical and arithmetic shift operators) and overflow",pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-14 14:24:36,0
13383599,[GLib][Dataset] Change prefix to gadataset_ from gad_,Because we choose gaflight_ for Apache Arrow Flight GLib.,pull-request-available,['GLib'],ARROW,Improvement,Major,2021-06-12 23:13:15,1
13383508,"[C++] Add a general ""if, ifelse, ..., else"" kernel (""CASE WHEN"")","ARROW-10640 added a ternary {{if_else}} kernel. Add another kernel that extends this concept to an arbitrary number of conditions and associated results, like a vectorized {{if-ifelse-...-else}} with an arbitrary number of {{ifelse}} and with the {{else}} optional. This is like a SQL {{CASE}} statement.

How best to achieve this is not obvious. To enable SQL-style uses, it would be most efficient to implement this as a variadic kernel where the even-number arguments (0, 2, ...) are the arrays of boolean conditions, the odd-number arguments (1, 3, ...) are the corresponding arrays of results, and the final argument is the {{else}} result. But I'm not sure if this is practical. Maybe instead we should implement this to operate on listarrays, like NumPy's {{[np.where|https://numpy.org/doc/stable/reference/generated/numpy.where.html]}} or {{[np.select|https://numpy.org/doc/stable/reference/generated/numpy.select.html]}}.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-11 21:28:15,0
13383349,[C++] S3FileSystem fails moving filepaths containing = or +,"Hi Arrow team,

we have the very common use-case of having partitioned parquet tables on S3, written by Spark. These include equals (=) to denote the partition value per folder.



In trying to use PyArrows S3FileSystem `move` function, it's not possible to move these objects in the bucket underneath a path which contains `=` somewhere:
{code:java}
OSError: When copying key 'table/date=202007/part-00000-e39069c2-0ea6-4a62-85ea-8011047cd4f4.c000.snappy.parquet' in bucket 'bucket' to key 'table2/date=202007/part-00000-e39069c2-0ea6-4a62-85ea-8011047cd4f4.c000.snappy.parquet' in bucket 'bucket': AWS Error [code 133]: The specified key does not exist.{code}

It is also not possible to move, using preemptively URL-quoted paths, like these:


{code:java}
OSError: When copying key 'table/date%3D202007/part-00000-e39069c2-0ea6-4a62-85ea-8011047cd4f4.c000.snappy.parquet' in bucket 'bucket' to key 'table2/date%3D202007/part-00000-e39069c2-0ea6-4a62-85ea-8011047cd4f4.c000.snappy.parquet' in bucket 'bucket': AWS Error [code 133]: The specified key does not exist.{code}


The source object does definitely exist, it has in fact been returned by a FileSelector from PyArrow itself and is just passed to move.


Is there any configuration option to be set, or special quoting to be used?

Thanks in advance.
Joerg





",filesystem pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-06-11 08:19:28,0
13383295,[Packaging][RPM][deb] Don't install system utf8proc if it's old,See also: ARROW-13002,pull-request-available,['Packaging'],ARROW,Improvement,Major,2021-06-11 01:48:02,1
13383272,[C++] Automatic checks that kernels don't leave uninitialized data in output,"To minimize the risk of issues such as ARROW-13041, perhaps our compute kernel test harness should include a check that allocated data is always initialized (using Valgrind).",pull-request-available,['C++'],ARROW,Task,Major,2021-06-10 20:28:16,2
13383269,[C++] Unary kernels can leave uninitialized data under null entries,"In {{ScalarUnaryNotNullStateful}}, not all null value visitors are careful to initialize the underlying data deterministically.",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-10 20:03:09,2
13383254,[R] Fix error message handling,"When propagating a Status error up to R, we call {{cpp11::stop}} which actually has a printf-style syntax.

When our error message contains {{'%'}} characters, this messes formatting and can trigger Valgrind errors (since the formatting tries to read additional arguments from the C stack).",pull-request-available,['R'],ARROW,Bug,Major,2021-06-10 17:47:45,2
13383172,[Python][Docs] Update outdated examples for hdfs/azure on the Parquet doc page,"From https://github.com/apache/arrow/issues/10492

- The chapter ""Writing to Partitioned Datasets"" still presents a ""solution"" with ""hdfs.connect"" but since it's mentioned as deprecated no more a good idea to mention it.
- The chapter ""Reading a Parquet File from Azure Blob storage"" is based on the package ""azure.storage.blob"" ... but an old one and the actual ""azure-sdk-for-python"" doesn't have any-more methods like get_blob_to_stream(). Possible to update this part with new blob storage possibilities, and also another mentioning the same concept with Delta Lake (similar principle but since there are differences ...)

",pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-10 12:17:16,5
13382999,"[C++][Compute] Enhance FunctionOptions with equality, debug representability, and serializability","Currently the {{FunctionOptions}} interface is entirely opaque. It would be useful to add 

- equality comparability {code}
    bool FunctionOptions::Equals(const FunctionOptions& other) const
    {code}
- debug representation {code}
    std::string FunctionOptions::ToString() const
    {code}
- serializability {code}
    Status FunctionOptions::Serialize(io::OutputStream*) const

    Result<std::unique_ptr<FunctionOptions>>
        FunctionOptions::Deserialize(io::InputStream*)
    {code} (or similar)

These are already implemented for common instances of {{FunctionOptions}} in expression.cc, since {{Expression}} has the above capabilities and may contain a {{FunctionOptions}}. Making these explicit virtual functions will formalize this ad-hoc
code and expose it for direct unit testing.

As an added bonus, if options can serialize themselves to JSON then that can be used by Python and other bindings to generate wrappers instead of the current hand-written listing of wrapper classes for each {{FunctionOptions}} subclass",pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-09 16:27:10,0
13382918,[C++][Docs] Use consistent terminology for nulls (min_count) in scalar aggregate kernels,"For example, the current `pc.sum` docstring has

{code}
Null values are ignored. Minimum count of non-NA
values can be set and NAN is returned if too few are present.
This can be changed through ScalarAggregateOptions.
{code}

which uses both NA and NAN in addition to ""null"".",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-09 10:01:14,5
13382714,[C++] Deprecation warning when compiling minimal example,"{code}
/io/example.cc: In function 'arrow::Status {anonymous}::RunMain(int, char**)':
/io/example.cc:46:75: warning: 'static arrow::Result<std::shared_ptr<arrow::csv::TableReader> > arrow::csv::TableReader::Make(arrow::MemoryPool*, arrow::io::IOContext, std::shared_ptr<arrow::io::InputStream>, const arrow::csv::ReadOptions&, const arrow::csv::ParseOptions&, const arrow::csv::ConvertOptions&)' is deprecated: Use MemoryPool-less variant (the IOContext holds a pool already) [-Wdeprecated-declarations]
   46 |                                     arrow::csv::ConvertOptions::Defaults()));
      |                                                                           ^
{code}",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Trivial,2021-06-08 13:02:27,2
13382557,[C++] Datasets needs dependency on xsimd,"Because of ARROW-12644 now partition.cc includes utf.8 which in turn includes xsimd. There's no dependency exposed to CMake so if Ninja et. al. schedule the build 'badly', it'll build partition.cc before building xsimd, causing a missing header error.

Example: [https://github.com/apache/arrow/pull/10470/checks?check_run_id=2767312685]",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-07 19:35:28,0
13382526,[R] Fix tests that assume UTC local tz,"Here's the problem I detected while  triaging tickets. 

This was run locally after merging from apache/arrow at commit 8773b9d and re-building both Arrow library and Arrow R package.

{code:r}
library(arrow)
#> See arrow_info() for available features
#> 
#> Attaching package: 'arrow'
#> The following object is masked from 'package:utils':
#> 
#>     timestamp
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
library(testthat)
#> 
#> Attaching package: 'testthat'
#> The following object is masked from 'package:dplyr':
#> 
#>     matches
#> The following object is masked from 'package:arrow':
#> 
#>     matches

tstring <- tibble(x = c(""08-05-2008"", NA))
tstamp <- tibble(x = c(strptime(""08-05-2008"", format = ""%m-%d-%Y""), NA))

expect_equal(
  tstring %>%
    Table$create() %>%
    mutate(
      x = strptime(x, format = ""%m-%d-%Y"")
    ) %>%
    collect(),
  tstamp,
  check.tzone = FALSE
)
#> Error: `%>%`(...) not equal to `tstamp`.
#> Component ""x"": Mean absolute difference: 14400
{code}

We can see that the dates are different by exact 4 hours by removing the expectation:

{code:r}
library(arrow)
#> See arrow_info() for available features
#> 
#> Attaching package: 'arrow'
#> The following object is masked from 'package:utils':
#> 
#>     timestamp
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
library(testthat)
#> 
#> Attaching package: 'testthat'
#> The following object is masked from 'package:dplyr':
#> 
#>     matches
#> The following object is masked from 'package:arrow':
#> 
#>     matches

tstring <- tibble(x = c(""08-05-2008"", NA))
tstamp <- tibble(x = c(strptime(""08-05-2008"", format = ""%m-%d-%Y""), NA))

tstring %>%
  Table$create() %>%
  mutate(
    x = strptime(x, format = ""%m-%d-%Y"")
  ) %>%
  collect()
#> # A tibble: 2 x 1
#>   x                  
#>   <dttm>             
#> 1 2008-08-04 20:00:00
#> 2 NA

tstamp
#> # A tibble: 2 x 1
#>   x                  
#>   <dttm>             
#> 1 2008-08-05 00:00:00
#> 2 NA
{code}

_Created on 2021-06-07 by the [reprex package|https://reprex.tidyverse.org] (v2.0.0)_
",pull-request-available,['R'],ARROW,Task,Major,2021-06-07 16:17:06,4
13382517,[Python] Address boundary error with invalid Feather file and stackprinter,"I'm trying to read a 0 byte file, probably erroneously written. This results in ""ipython terminated by signal SIGSEGV (Address boundary error)"", which could probably be more gracefully handled



https://ml-pull.s3.eu-central-1.amazonaws.com/job_desc_address_boundary_error.feather",pull-request-available,['Python'],ARROW,Bug,Major,2021-06-07 15:55:42,2
13382504,[CI] Travis ARM builds often crash,"Those builds often crash with a compiler crash, for example:
https://travis-ci.com/github/apache/arrow/jobs/511663690

{code}
FAILED: CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o 

/usr/bin/c++  -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -DHAVE_ZLIB -I. -I/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src -fdiagnostics-color=always -ggdb -O0 -g -fPIC   -fdiagnostics-color=always -ggdb -O0 -g -fPIC   -std=c++11 -MD -MT CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o -MF CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o.d -o CMakeFiles/libprotobuf.dir/build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc.o -c /build/cpp/protobuf_ep-prefix/src/protobuf_ep/src/google/protobuf/descriptor.cc

c++: fatal error: Killed signal terminated program cc1plus
{code}

It seems this is a problem with the Travis-CI ARM execution environment.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2021-06-07 15:18:30,1
13382486,"[CI] ""Dev PR"" jobs undully cancelled","The cancellation scheme that was checked in by ARROW-12895 seems to only work properly when a Github Actions workflow is triggered by a ""pull_request"" or ""push"" event, but not ""pull_request_target"", as the group key {code}${{ github.repository }}-${{ github.ref }}-${{ github.workflow }}{code} ends up as {code}apache/arrow-refs/heads/master-Dev PR{code}.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2021-06-07 14:08:19,2
13382482,[CI] The kartothek nightly integration build is failing (test_update_dataset_from_ddf_empty),"The nightly ""kartothek"" integration builds are failing.

More specifically, the {{test_update_dataset_from_ddf_empty}} is failing with:

{code}
=================================== FAILURES ===================================
___________________ test_update_dataset_from_ddf_empty[True] ___________________

store_factory = functools.partial(<function get_store_from_url at 0x7f1434733050>, 'hfs:///tmp/pytest-of-root/pytest-0/test_update_dataset_from_ddf_e0/store')
shuffle = True

    @pytest.mark.parametrize(""shuffle"", [True, False])
    def test_update_dataset_from_ddf_empty(store_factory, shuffle):
        with pytest.raises(ValueError, match=""Cannot store empty datasets""):
            update_dataset_from_ddf(
>               dask.dataframe.from_delayed([], meta=((""a"", int),)),
                store_factory,
                dataset_uuid=""output_dataset_uuid"",
                table=""core"",
                shuffle=shuffle,
                partition_on=[""a""],
            ).compute()

tests/io/dask/dataframe/test_update.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

dfs = [], meta = (('a', <class 'int'>),), divisions = None
prefix = 'from-delayed', verify_meta = True

    @insert_meta_param_description
    def from_delayed(
        dfs, meta=None, divisions=None, prefix=""from-delayed"", verify_meta=True
    ):
        """"""Create Dask DataFrame from many Dask Delayed objects
    
        Parameters
        ----------
        dfs : list of Delayed
            An iterable of ``dask.delayed.Delayed`` objects, such as come from
            ``dask.delayed`` These comprise the individual partitions of the
            resulting dataframe.
        $META
        divisions : tuple, str, optional
            Partition boundaries along the index.
            For tuple, see https://docs.dask.org/en/latest/dataframe-design.html#partitions
            For string 'sorted' will compute the delayed values to find index
            values.  Assumes that the indexes are mutually sorted.
            If None, then won't use index information
        prefix : str, optional
            Prefix to prepend to the keys.
        verify_meta : bool, optional
            If True check that the partitions have consistent metadata, defaults to True.
        """"""
        from dask.delayed import Delayed
    
        if isinstance(dfs, Delayed):
            dfs = [dfs]
        dfs = [
            delayed(df) if not isinstance(df, Delayed) and hasattr(df, ""key"") else df
            for df in dfs
        ]
        for df in dfs:
            if not isinstance(df, Delayed):
                raise TypeError(""Expected Delayed object, got %s"" % type(df).__name__)
    
>       parent_meta = delayed(make_meta)(dfs[0]).compute()
E       IndexError: list index out of range

/opt/conda/envs/arrow/lib/python3.7/site-packages/dask/dataframe/io/io.py:591: IndexError
{code}

(from https://github.com/ursacomputing/crossbow/runs/2756067090)

Not directly sure if this is a kartothek issue or a pyarrow issue. But also created an issue on their side: https://github.com/JDASoftwareGroup/kartothek/issues/475",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Minor,2021-06-07 13:29:58,5
13382429,[Python][Packaging] Unable to install pygit2 in the arm64 wheel builds,pygit2==1.6 ships aarch64 wheels though we have pinned pygit2 to the earlier 1.5 version since it broke the crossbow comment bot.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2021-06-07 10:50:10,3
13382295,[C++][Python] Converter::Extend gets stuck in infinite loop causing OOM if values don't fit in single chunk,"_Apologies if this is a duplicate, I haven't found anything related_

When creating an arrow table via the python api, the following code runs out of memory after using all the available resources on a box with 512GB of ram. This happens with pyarrow 4.0.0 and 4.0.1. However when running the same code with pyarrow 3.0.0, the memory usage only reaches 5GB (which seems like the appropriate ballpark for the table size).
 The code generates a table with a single string column with 1m rows, each string being 3000 characters long.

Not sure whether the issue is python related or not, I haven't tried replicating it from the C++ api.


{code:python}
import os, string
import numpy as np
import pyarrow as pa

print(pa.__version__)
np.random.seed(42)

alphabet = list(string.ascii_uppercase)

_col = []
for _n in range(1000):
  k = ''.join(np.random.choice(alphabet, 3000))
  _col += [k] * 1000

table = pa.Table.from_pydict({'col': _col})
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-05 12:37:02,0
13382163,[C++] match_substring doesn't match empty needle to empty haystack,"{noformat}
>>> import pyarrow as pa, pyarrow.compute as pc
>>> pa.__version__
'4.0.0'
>>> pc.match_substring(["""", ""a""], """")
<pyarrow.lib.BooleanArray object at 0x7fd8a77888e0>
[
  false,
  true
]
>>> """" in """"
True {noformat}
Also confirmed with PyArrow 4.0.1.",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-04 14:36:42,0
13382136,[Python] Expose Python binding for ElementWiseAggregateOptions,Follow-up on ARROW-12751,pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-04 12:39:59,5
13381991,[GLib][Ruby] Add Arrow:Scalar,"In ruby trying to use compute kernals.  For the kernels that take a scalar there is no way to make on in GLib or ruby 
ar= Arrow::Array.new([0,1,1,1,1,2,3,4,2,6,7])
fun = Arrow::Function.find('add')

needed = Arrow::Scalar.new(3)

sum = fun.execute([Arrow::ArrayDatum.new(ar)]).value
",pull-request-available,"['GLib', 'Ruby']",ARROW,Improvement,Major,2021-06-03 19:53:49,1
13381974,[C++] MSVC issues warning building PyArrow on Windows,"[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/39448469/job/96ja5h1b984ln09d]

While it doesn't block anything, there's a warning (looks like because a parameter was misdeclared in the pxd file):
{noformat}
[33/36] Building CXX object CMakeFiles\_compute.dir\_compute.cpp.obj
_compute.cpp(18101): warning C4244: 'argument': conversion from 'int64_t' to 'uint32_t', possible loss of data
{noformat}",pull-request-available,['Python'],ARROW,Improvement,Major,2021-06-03 18:14:18,0
13381951,[C++] Fix crash on Parquet file (OSS-Fuzz),See https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=34521,pull-request-available,['C++'],ARROW,Bug,Major,2021-06-03 16:34:25,2
13381934,[C++] Add regex count kernel,"Returns the number of times the regular expression pattern occurs in the string, like {{len(re.findall(...))}} in Python and {{regexp_count()}}in some SQL dialects",pull-request-available,['C++'],ARROW,New Feature,Major,2021-06-03 15:23:36,0
13381932,[C++] Refactor StringTransform,"In {{compute/kernels/scalar_string.cc}, currently there is a non-trivial inheritance relationship between {{StringTransform}} and its derived implementation classes. Also, the derived classes have to be templated on the actual string type even though their code doesn't care, making generated code size probably bigger than it should be.

Instead, there could be a {{StringTransformExec}} class that delegates to an independent class for per-kernel specifics (mostly: the {{MaxCodeunits}} and {{Transform}} methods).",pull-request-available,['C++'],ARROW,Task,Minor,2021-06-03 15:17:50,2
13381931,[C++] Add substring count kernel,"Returns the number of times the specified substring occurs in the string, like Python's{{str.count()}}",pull-request-available,['C++'],ARROW,New Feature,Major,2021-06-03 15:14:09,0
13381928,[C++] Add string starts-with/ends-with kernels,"Like Python {{str.startswith()}}and {{str.endswith()}}
",pull-request-available,['C++'],ARROW,New Feature,Major,2021-06-03 15:03:01,0
13381920,[C++] Add string slice replace kernel,"This should implement substring positional replacement, like {{[pandas.Series.str.slice_replace|https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.slice_replace.html]}}",pull-request-available,['C++'],ARROW,New Feature,Major,2021-06-03 14:46:59,0
13381729,[C++] Allow specifying default metadata for new S3 files,"The S3 filesystem recognizes some metadata keys with {{OpenOutputStream}}, such as {{Content-Language}} or {{ACL}} (see ARROW-12719). It would be useful, especially for ACLs, to be able to set default values at the filesystem level itself.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-06-02 15:18:38,2
13381688,[C++][CI] Compiler error on some clang versions,"My bad: I merged ARROW-12859 without noticing that there was a compilation error on some CI configurations.
{code}
/Users/runner/work/arrow/arrow/cpp/src/arrow/ipc/json_simple_test.cc:1362:63: error: implicit conversion from 'double' to 'bool' changes value from -0 to false [-Werror,-Wliteral-conversion]
  AssertJSONScalar<DoubleType, bool>(float64(), ""-0.0"", true, -0.0);
  ~~~~~~~~~~~~~~~~                                            ^~~~
{code}
",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Critical,2021-06-02 12:08:56,2
13381480,[C++] Build errors with Visual Studio 16.10.31321.278,"The {{test-build-vcpkg-win}} CI job uses the current{{windows-2019}}GHA runner image, so it often catches build errors caused by Visual Studio/MSVC updates.

The Visual Studio version in this image was just updated from*16.**9.31229.75* to*16.10.31321.278*and the MSVC runtime was updated from*14.**28.29914*to *14.29.30037*as shown at[https://github.com/actions/virtual-environments/pull/3452/files)]. This seems to have triggered a new error:
{code:java}
C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30037\include\xutility(1232,54): error C2794: 'reference': is not a member of any direct or indirect base class of 'std::iterator_traits<_Iter>' [D:\a\crossbow\crossbow\arrow\cpp\build\src\arrow\arrow-stl-test.vcxproj]
          with
          [
              _Iter=arrow::stl::ArrayIterator<arrow::BaseBinaryArray<arrow::BinaryType>,arrow::stl::detail::DefaultValueAccessor<arrow::BaseBinaryArray<arrow::BinaryType>>>
          ] {code}
Full log: [https://github.com/ursacomputing/crossbow/runs/2716032508]

Log from the same CI job just before this update, showing no errors:[https://github.com/ursacomputing/crossbow/runs/2708195132]

There are no apache/arrow commits or vcpkg port updates changes that would explain this error so I strongly suspect it was caused by the Visual Studio update.",pull-request-available,['C++'],ARROW,Bug,Major,2021-06-01 15:09:37,0
13381466,[C++][R][pyarrow] Failure importing some decimal types using the C data interface,"In my R notebook, I try to read data from a Db2 database (""SELECT CAST (15 as decimal(5,0)) FROM sysibm.sysdummy1"") into an R dataframe, leveraging arrow/flight.
integers, varchars, etc can be loaded without issues, but when I use a decimal type, an error is thrown.



Here is the code I'm running:


{code:java}
library(""reticulate"")
library(""arrow"")
itcfs <- import(""..."")
readClient <- itcfs$get_flight_client()
Manual_data_request = ....
flightInfo <- itcfs$get_flight_info(readClient,data_request=Manual_data_request)

tables <- itcfs$read_tables(readClient, flightInfo)
...

{code}


{{The itcfs package is implemented in Python, and here is the read_tables method:}}


{color:#0000ff}def{color}{color:#000000} read_tables(read_client, flight_info):{color}
{color:#a31515}""""""Read a list of pyarrow.Table""""""{color}
{color:#000000} tables = []{color}
{color:#0000ff}for{color}{color:#000000} endpoint {color}{color:#0000ff}in{color}{color:#000000} flight_info.endpoints:{color}
{color:#000000} reader = read_client.do_get(endpoint.ticket){color}
{color:#000000} batches = [b.data {color}{color:#0000ff}for{color}{color:#000000} b {color}{color:#0000ff}in{color}{color:#000000} reader]{color}
{color:#000000} tables.append(pa.Table.from_batches(batches)){color}
{color:#0000ff}return{color}{color:#000000} tables{color}

This is the erro message:


{code:java}
Error: Invalid: Invalid or unsupported format string: 'd:5,0'
Traceback:
1. itcfs$read_tables(readClient, flightInfo)
2. py_to_r(result)
3. py_to_r.python.builtin.list(result)
4. lapply(converted, function(object) {
 . if (inherits(object, ""python.builtin.object"")) 
 . py_to_r(object)
 . else object
 . })
5. FUN(X[[i]], ...)
6. py_to_r(object)
7. py_to_r.pyarrow.lib.Table(object)
8. maybe_py_to_r(x$columns)
9. x$columns
10. `$.python.builtin.object`(x, ""columns"")
11. py_get_attr_or_item(x, name, TRUE)
12. py_maybe_convert(object, py_has_convert(x))
13. py_to_r(x)
14. py_to_r.python.builtin.list(x)
15. lapply(converted, function(object) {
 . if (inherits(object, ""python.builtin.object"")) 
 . py_to_r(object)
 . else object
 . })
16. FUN(X[[i]], ...)
17. py_to_r(object)
18. py_to_r.pyarrow.lib.ChunkedArray(object)
19. ChunkedArray$create(!!!maybe_py_to_r(x$chunks))
20. ChunkedArray__from_list(list2(...), type)
21. list2(...)
22. maybe_py_to_r(x$chunks)
23. x$chunks
24. `$.python.builtin.object`(x, chunks)
25. py_get_attr_or_item(x, name, TRUE)
26. py_maybe_convert(object, py_has_convert(x))
27. py_to_r(x)
28. py_to_r.python.builtin.list(x)
29. lapply(converted, function(object) {
 . if (inherits(object, ""python.builtin.object"")) 
 . py_to_r(object)
 . else object
 . })
30. FUN(X[[i]], ...)
31. py_to_r(object)
32. py_to_r.pyarrow.lib.Array(object)
33. ImportArray(array_ptr, schema_ptr)
{code}




In a pure python envrionment, decimal data can be read without issues.",pull-request-available,"['C++', 'Python', 'R']",ARROW,Bug,Major,2021-06-01 14:39:21,2
13380860,[Python] Cython fails builds on some machines with a CResult[shared_ptr[const Foo]]] declaration,"On the Conbench machines, the Cython code is failing to build:
{noformat}
Error compiling Cython file:
------------------------------------------------------------
...
        @staticmethod
        CStatus Open(const shared_ptr[CRandomAccessFile]& file,
                     CMemoryPool* pool,
                     unique_ptr[ORCFileReader]* reader)
 
        CResult[shared_ptr[const CKeyValueMetadata]] ReadMetadata()
                                ^
------------------------------------------------------------
 
pyarrow/_orc.pxd:42:33: Expected ']', found 'CKeyValueMetadata'{noformat}
Cython appears not to like the {{CResult[shared_ptr[const CKeyValueMetadata]]]}}, though this seems to only happen on some machines, even if using the same Cython version. Declaring a Status-returning variant seems to work around it.",pull-request-available,['Python'],ARROW,Bug,Major,2021-05-27 20:50:21,0
13380852,[Release][C#] Package upload script is broken,"* Download URL is wrong
* Downloaded packages aren't removed",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-05-27 20:11:12,1
13380816,"[CI] Use ""concurrency"" setting on Github Actions","We're currently using a dedicated Github Actions to cancel previous jobs when a new job is queued. It seems this now can be done better using the ""concurrency"" setting (unfortunately in beta):
https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#concurrency
",pull-request-available,['Continuous Integration'],ARROW,Task,Minor,2021-05-27 15:49:32,2
13380759,[C++][Compute][Dataset] Extract subtree pruning logic to compute::,"Subtree pruning need not reference fragments or anything else in dataset::, so it should be extracted to the compute namespace and be unit tested alongside Expression",pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-27 12:14:51,0
13380599,[CI] AppVeyor pip install failure during setup,"Last successful build:
https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/39249718
First failed build:
https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/39255805

{{pip install}} for clcache fails with a certificate verification error:
https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/39255805/job/e96cn9q9ohib5fuw#L523

For some reason, I cannot reproduce this on my fork (with the exact same CI configuration, presumably).",pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2021-05-26 17:45:52,2
13380124,[C++] Add ScalarFromJSON for easier testing,"We have ArrayFromJSON and various others but not DatumFromJSON which would make things a little easier.

(Originally this suggested making a DatumFromJSON but then it's ambiguous whether something might be, for instance, an array or a list scalar, as Antoine pointed out.)",pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-24 13:37:40,0
13380115,[C++] hash_aggregate_test not building on master,The test was recently updated but looks like it needed a rebase,pull-request-available,['C++'],ARROW,Bug,Major,2021-05-24 13:07:24,0
13380067,[Dev][Release] Windows wheel verification script fails to download artifacts,See the windows build at https://github.com/apache/arrow/pull/10374#issuecomment-846007281,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2021-05-24 10:26:42,3
13379762,[Release] Mail template points to 404,"The email template to send to the dev for vote yields

{code:java}
[4]: https://apache.jfrog.io/artifactory/arrow/centos-rc/4.0.1-rc1
[5]: https://apache.jfrog.io/artifactory/arrow/debian-rc/4.0.1-rc1
[6]: https://apache.jfrog.io/artifactory/arrow/python-rc/4.0.1-rc1
[7]: https://apache.jfrog.io/artifactory/arrow/ubuntu-rc/4.0.1-rc1
{code}

however, these are always 404, as the distros are directory-based. This should probably be without `/4.0.1-rc1`, since the 4.0.1-rc1 is split between the different sub-directories.
",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-05-21 12:55:27,1
13379627,[C++][Compute] Add is_inf kernel for floating point arrays,Add a scalar kernel {{is_inf}}that returns true at the positions of a floating point number array that have positive or negative infinite values ({{inf}} and {{-inf}}) and false at other positions.The implementation of this should be similar to{{is_nan}}.,pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-21 02:20:37,0
13379585,[Java][FlightRPC] Error metadata from FlightStatusException is not propagated to client,"It works if you use gRPC's StatusRuntimeException directly, but not if you use FlightStatusException, since the metadata isn't copied over in StatusUtils.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2021-05-20 21:04:46,0
13379514,[C++] Implement case insenstive match in match_substring(_regex) and match_like,See discussion in [https://github.com/apache/arrow/pull/10356] - we can add a (default-case-sensitive) flag for case insensitive matching for these kernels. Any kernel which doesn't implement the option should raise an error if set.,pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-20 13:46:15,0
13379506,[Python][Dataset] Allow enabling I/O coalescing/pre-buffer for IPC in Python,The option exists in C++ but not Python. The IPC benchmark will need updating too.,dataset datasets pull-request-available,['Python'],ARROW,Improvement,Major,2021-05-20 12:53:34,0
13379145,[C++] [Dataset] Review error pass-through in the datasets API,"There is at least one (and I think there are actually several) places in the datasets API where we are bubbling up errors without attaching the necessary context. For example, in the discussion here [https://github.com/apache/arrow/pull/10326#pullrequestreview-662095548] a call to ""DatasetFactory::Create"" (where the user incorrectly assigned a default file format of parquet) is returning ""Parquet magic bytes not found in footer"" instead of something like ""Dataset creation failed. The fragment '/2019/July/myfile.csv' did not match the expected 'parquet'' format: Parquet magic bytes not found in footer""",pull-request-available,['C++'],ARROW,Improvement,Minor,2021-05-18 21:04:48,0
13378859,[Packaging][Java] Improve JNI jars build,"- to better align with the manylinux scripts
- also build the pure java packages
- add dynamic dependency check functionality to archery",pull-request-available,"['Java', 'Packaging']",ARROW,Improvement,Major,2021-05-17 17:56:15,3
13378854,[Python] Run tests with AWS_EC2_METADATA_DISABLED=true,This explains why some tests are so slow. There's already a few tests that work around this.,pull-request-available,['Python'],ARROW,Improvement,Major,2021-05-17 17:42:28,0
13378836,[C++] Fix merge conflicts with Future refactor/async IPC,ARROW-12004 and ARROW-11772 conflict with each other (they merge cleanly but the result doesn't build),pull-request-available,['C++'],ARROW,Bug,Major,2021-05-17 15:05:45,0
13378821,[Python] test_write_to_dataset_filesystem missing a dataset mark,From https://stackoverflow.com/questions/67526288/modulenotfounderror-no-module-named-pyarrow-dataset,pull-request-available,['Python'],ARROW,Improvement,Major,2021-05-17 13:44:35,5
13378467,[Python] Cannot read from HDFS with blanks in path names,"I have a Hadoop FS with blanks in path and filenames.

Running this

{{hdfs = fs.HadoopFileSystem('namenode', 8020)}}
{{files = hdfs.get_file_info(fs.FileSelector(""/"", recursive=True))}}

throws a

{{pyarrow.lib.ArrowInvalid: Cannot parse URI: 'hdfs://namenode:8020/data/Path with Blank'}}

How can I avoid that?

Strangely enough, reading a file with

{{hdfs.open_input_file(csv_file)}}

works just fine regardless of the blanks?",filesystem hdfs pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-05-14 11:06:31,2
13378366,[CI][C++] MinGW builds failing when trying to build Gandiva,"clang is failing to run because of a missing DLL. The trick is to use strace which will tell you the exact missing package, in this case it happens to be libxml2.

[https://github.com/msys2/MINGW-packages/issues/544#issuecomment-802725054]",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2021-05-13 20:35:24,0
13378359,[Python][FlightRPC] Flight server segfaults with certain data,"This do_get RPC handler segfaults when invoked:
{code:python}
def do_get(...):
    schema = pa.schema([])
    return flight.GeneratorStream(schema, itertools.repeat(schema.empty_table()))
{code}
A similar one for do_exchange also segfaults.

Confirmed using the Linux Conda package for 4.0.0.",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2021-05-13 19:18:31,0
13378304,[Archery][Integration] Fix decimal case generation in write_js_test_json,"The integration build has started to fail on master: https://github.com/apache/arrow/runs/2575265526#step:9:4265

I don't entirely understand the reason why we see this error, in order to call that function we would need to pass {{--write_generated_json}} to the archery command, but we don't. The implementation is clearly wrong though.",pull-request-available,"['Archery', 'Continuous Integration']",ARROW,Bug,Major,2021-05-13 13:16:21,3
13378290,[CI] Merge script test fails due to missing dependency,"{{six}} is missing, perhaps github has updated the virtual environment of the hosted actions machines.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2021-05-13 12:12:53,3
13378208,[Python] Negative out of range slices yield invalid arrays,"Tested on pyarrow 2.0 and pyarrow 4.0 wheels. The errors are slightly different between the 2.0. Below is a script from 4.0



This is taken from the result of test_slice_array

{{}}
{{ >>> import pyarrow as pa}}
{{ >>> pa.array(range(0,10))}}
{{ <pyarrow.lib.Int64Array object at 0x7f59b8bdab20>}}
{{ [}}
{{ 0,}}
{{ 1,}}
{{ 2,}}
{{ 3,}}
{{ 4,}}
{{ 5,}}
{{ 6,}}
{{ 7,}}
{{ 8,}}
{{ 9}}
{{ ]}}
{{ >>> a=pa.array(range(0,10))}}
{{ >>> a[-9:-20]}}
{{ <pyarrow.lib.Int64Array object at 0x7f59b8bdaa00>}}
{{ []}}
{{ >>> len(a[-9:-20])}}
{{ Traceback (most recent call last):}}
{{ File ""<stdin>"", line 1, in <module>}}
{{ SystemError: <built-in function len> returned NULL without setting an error}}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-05-12 23:11:05,5
13378193,[CI] Fix arguments in Conda Windows builds,"azure-conda-win-vs2017-py36-r36 (and azure-conda-win-vs2017-py37-r40, azure-conda-win-vs2017-py38, azure-conda-win-vs2017-py39) are affected by this new error

_Error: Got unexpected extra arguments (D:\bld\win-64\arrow-cpp-proc-3.1.0.dev834-cpu.tar.bz2 D:\bld\win-64\parquet-cpp-1.5.1-h57928b3_0.tar.bz2 D:\bld\win-64\pyarrow-3.1.0.dev834-py39hf9247be_0_cpu.tar.bz2 D:\bld\win-64\pyarrow-tests-3.1.0.dev834-py39hf9247be_0_cpu.tar.bz2)_",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2021-05-12 20:52:01,3
13378164,[C++][Python][R] S3FileSystem: IO thread parallelism limited to 8 threads,"On Arrow 4, I'm not able to get more than 8 thread parallelism (for example when N threads call fs.ls() with different paths) for S3FileSystem created with pyarrow, because the default IO context is created with 8 thread limit (hardcoded), and I don't see a way to change this or pass a custom IO context through pyarrow APIs. I believe this affects other Arrow filesystem implementations as well.",filesystem performance pull-request-available,"['C++', 'Python', 'R']",ARROW,Improvement,Major,2021-05-12 19:01:10,0
13378143,"[Dev][Archery] Warning about RUST variable in ""archery docker run""","This spurious warning is being displayed:
{code}
$ archery docker run conda-python-hdfs bash
WARNING: The RUST variable is not set. Defaulting to a blank string.
Pulling conda ... done
WARNING: The RUST variable is not set. Defaulting to a blank string.
Pulling conda-cpp ... done
[...]
{code}
",pull-request-available,['Archery'],ARROW,Bug,Trivial,2021-05-12 16:43:30,2
13378105,[C++] Add variadic row-wise min/max kernels (least/greatest),"Add a pair of variadic functions equivalent to SQL's {{least}}/{{greatest}} or R's {{pmin}}/{{pmax}}. Should take 0, 1, 2, ... same-length numeric arrays as input and return an array giving the minimum/maximum of the values found in each position of the input arrays. For example, in the case of these 2 input arrays:
{code:java}
Array<double>        Array<double>
[                    [
  1,                   2,
  4                    3
]                    ]
{code}
{{least}} would return:
{code:java}
Array<double>
[ 
  1,
  3
] 
{code}
and {{greatest}} would return
{code:java}
Array<double>
[ 
  2,
  4
] 
{code}
The returned array should have the same data type as the input arrays, or follow promotion rules if the numeric types of the input arrays differ.

Should also accept scalar numeric inputs and recycle their values.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-12 13:50:49,0
13377932,[CI] Configure GitHub Token for Nightly Builds,"this shall avoid the errors ""403 API rate limit exceeded for installation ID 123456789"" seen in both github-wheel-osx-high-sierra-cp36 (2021-05-11) and github-wheel-osx-mavericks-cp37 (2021-05-10)

",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2021-05-11 17:41:34,3
13377911,"[CI] [Gandiva] Nightly build error in azure-conda-osx-clang-py38 (and py39, py*-r*)","These are all failing because of a mismatch in LLVM version, they all have some variation on the following when referencing the precompiled gandiva

{code}
Unknown attribute kind (70) (Producer: 'LLVM12.0.0' Reader: 'LLVM 10.0.1')
{code}

It _looks like_ the precompiled gandiva llvm version might be [coming from the .env file|LLVM=12]

Examples:
https://dev.azure.com/ursacomputing/crossbow/_build/results?buildId=6219&view=logs&j=cf796865-97b7-5cd1-be8e-6e00ce4fd8cf&t=88ee2fb8-46fd-5c68-fde3-1c8d31ba2a5f&l=1069
https://dev.azure.com/ursacomputing/crossbow/_build/results?buildId=6250&view=logs&j=cf796865-97b7-5cd1-be8e-6e00ce4fd8cf&t=88ee2fb8-46fd-5c68-fde3-1c8d31ba2a5f&l=1046

For some of these, the build phase will pass (even though it did not succeed and the error is then: ""File ... does not exist"". This is a red herring and the build problem above is the real issue.",pull-request-available,"['C++ - Gandiva', 'Continuous Integration']",ARROW,Task,Major,2021-05-11 16:33:05,8
13377887,[C++] Eliminate unnecessary copy in FieldPath::Get(),"FieldPath::Get() uses RecordBatch::column_data which returns a vector<shared_ptr<>> by _value_ instead of by _reference_ - so with wide schemas, we end up copying and then destroying a lot of shared_ptrs. When done in a tight loop - as with dataset projection - this is a major performance pessimization.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-11 15:13:19,0
13377682,[R] Use InMemoryDataset for Table/RecordBatch in dplyr code,"This lets us consolidate our Expression handling code and prepares us for more query evaluation in the near future. As a bonus, it should also simplify our dplyr NSE function definition and make it easier to add and test them going forward.",pull-request-available,['R'],ARROW,New Feature,Major,2021-05-10 20:10:27,4
13377656,[C++][Python] pyarrow.fs.S3FileSystem pass extra kwargs i.e ACL,"Is there a way to set specific ACL args to the new written files via the S3FileSystem? We have a situation where the writer/read role groups do not have access unless specified during the write process.

Example of how we update with `s3fs`:
{code:python}
s3fs.S3FileSystem(
 s3_additional_kwargs={'ACL': 'bucket-owner-full-control'}
){code}
or with `boto`:
{code:python}
extra_args.update(\{""ACL"": ""bucket-owner-full-control""})
boto3.s3.transfer.S3Transfer(...).upload_file(extra_args=extra_args)
{code}
or
{code:python}
s3.Object(my_bucket_name, my_key_path).put(Body=my_body, ACL='bucket-owner-full-control')
{code}",pull-request-available,"['C++', 'Python']",ARROW,New Feature,Minor,2021-05-10 16:16:20,2
13377650,[C++] Substring find position kernel,"Return the integer position of a substring within a string, like SQL {{instr}}/{{locate}}or Python string {{find}}/{{index}}",pull-request-available,['C++'],ARROW,New Feature,Major,2021-05-10 15:27:14,0
13377649,[C++] Left/right/center string padding kernels,Similar to SQL {{lpad}}/{{rpad}}or Python {{ljust}}/{{rjust}},beginner pull-request-available,['C++'],ARROW,New Feature,Major,2021-05-10 15:15:24,0
13377648,[C++] SQL-style glob string match kernel,"Similar to the SQL {{LIKE}} and {{ILIKE}}operators, with wildcard{{_}} matching a single character and wildcard {{%}} matching multiple characters,and an option controlling case sensitivity",pull-request-available,['C++'],ARROW,New Feature,Major,2021-05-10 15:08:52,0
13377637,[C++] Add variadic string join kernel,"Similar to SQL's {{concat}} and {{concat_ws}}. Should take 0, 1, 2, ... string arrays and an optional separator (default empty string) and concatenate them together, returning a string array.

For example, in the case of 2 input arrays and with the separator {{""-""}}, this would take inputs:
{code}
Array<string>        Array<string>
[                    [
  ""foo"",               ""bar"",
  ""push""               ""pop""
]                    ]
{code}
and return output:
{code}
Array<string>
[ 
  ""foo-bar"",
  ""push-pop""
] 
{code}

Should also accept scalar strings and recycle their values.
",pull-request-available,['C++'],ARROW,New Feature,Major,2021-05-10 14:01:30,0
13377323,[R][CI] rtools35 job failing on 32-bit build tests,"See https://github.com/apache/arrow/actions/workflows/r.yml?query=branch%3Amaster, this started when ARROW-9697 (CountRows for Scanner) merged. It's only failing on rtools35 (aka gcc 4.9), and only on the 32-bit build (i386). Since there's no output about what failed, it's probably a segfault. The easiest way to get more information is to flip this {{if: false}} to true and let it print detailed output about where it was when it died https://github.com/apache/arrow/blob/master/.github/workflows/r.yml#L186",pull-request-available,"['C++', 'R']",ARROW,New Feature,Major,2021-05-07 23:56:29,0
13377291,[R] Implement ArrowArrayStream C interface,"See https://github.com/apache/arrow/commit/97879eb970bac52d93d2247200b9ca7acf6f3f93, which adds it and also adds Python bindings. ",pull-request-available,['R'],ARROW,New Feature,Major,2021-05-07 17:04:28,4
13377289,[C++][Python][Dataset] Support C Data Interface with Scanner,"There are lots of ways we could do this:
 * Direct support on the Scanner
 * A generic exporter for any RecordBatchIterator
 * A way to wrap a Scanner and/or RecordBatchIterator as RecordBatchReader",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2021-05-07 17:00:21,0
13377287,[C++][Python][FlightRPC] Support export_to_c in DoGet/inherit from RecordBatchReader,{{MetadataRecordBatchReader}} should probably inherit from RecordBatchReader to make reuse easier and to support common APIs like the C Data Interface.,pull-request-available,"['C++', 'FlightRPC', 'Python']",ARROW,Improvement,Major,2021-05-07 16:56:49,0
13377075,"[C++] Segfault casting result of ""fill_null()"" (not bitmap but unknown null_count)","The array returned by {{pa.Array.fill_null()}} is unusable.

{code}
>>> import pyarrow as pa
>>> arr = pa.array([None], pa.bool_()).fill_null(False)
>>> arr.cast(pa.int8())
Segmentation fault (core dumped)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2021-05-06 18:17:17,5
13377070,[C++] extract_regex gives bizarre behavior after nulls or non-matches,"After a non-match, the *subsequent* string may match ... but its data is in the wrong array element.

{code}
>>> pa.compute.extract_regex(pa.array([""a"", ""b"", ""c"", ""d""]), pattern=""(?P<x>[^b])"")
<pyarrow.lib.StructArray object at 0x7f80de918ee0>
-- is_valid:
  [
    true,
    false,
    true,
    true
  ]
-- child 0 type: string
  [
    ""a"",
    """",
    """",
    ""c""
  ]
{code}

Same if trying to match after {{null}}:

{code}
>>> pa.compute.extract_regex(pa.array([""a"", None, ""c"", ""d"", ""e""]), pattern=""(?P<x>[^b])"")
<pyarrow.lib.StructArray object at 0x7f80de918ee0>
-- is_valid:
  [
    true,
    false,
    true,
    true,
    true
  ]
-- child 0 type: string
  [
    ""a"",
    """",
    """",
    ""c"",
    ""d""
  ]
{code}

Workaround: 1) filter out non-matches; 2) extract only the matching strings; 3) interpolate nulls:

{code:python}
def _extract_regex_workaround_arrow_12670(
    array: pa.StringArray, *, pattern: str
) -> pa.StructArray:
    ok = pa.compute.match_substring_regex(array, pattern=pattern)
    good = array.filter(ok)
    good_matches = pa.compute.extract_regex(good, pattern=pattern)

    # Build array that looks like [None, 1, None, 2, 3, 4, None, 5]
    # ... ok_nonnull: [False, True, False, True, True, True, False, True]
    # (not ok.fill_null(False).cast(pa.int8()) because of ARROW-12672 segfault)
    ok_nonnull = pa.compute.and_kleene(ok.is_valid(), ok)
    # ... np_ok: [0, 1, 0, 1, 1, 1, 0, 1]
    np_ok = ok_nonnull.cast(pa.int8()).to_numpy(zero_copy_only=False)
    # ... np_index: [0, 1, 1, 2, 3, 4, 4, 5]
    np_index = np.cumsum(np_ok, dtype=np.int64) - 1
    # ...index_or_null: [None, 1, None, 3, 4, 5, None, 5]
    valid = ok_nonnull.buffers()[1]
    index_or_null = pa.Array.from_buffers(
        pa.int64(), len(array), [valid, pa.py_buffer(np_index)]
    )

    return good_matches.take(index_or_null)
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-05-06 17:39:20,2
13377048,[C++][Dataset] CountRows occasionally segfaulting,"[https://github.com/apache/arrow/pull/9656/checks?check_run_id=2518312525]
{noformat}
Start test: dim() correctly determine numbers of rows and columns on arrow_dplyr_query object

 *** caught segfault ***
address 0x7ff7cf2cf8f8, cause 'invalid permissions'

Traceback:
 1: dataset___Scanner__CountRows(self)
 2: scanner$CountRows()
 3: dim.arrow_dplyr_query(.)
 4: dim(.)
 5: ds %>% filter(chr == ""a"") %>% dim()
 6: eval_bare(expr, quo_get_env(quo))
 7: quasi_label(enquo(object), label, arg = ""object"")
 8: expect_identical(ds %>% filter(chr == ""a"") %>% dim(), c(2L, 7L))
 9: eval(code, test_env)
10: eval(code, test_env)
11: withCallingHandlers({    eval(code, test_env)    if (!handled && !is.null(test)) {        skip_empty()    }}, expectation = handle_expectation, skip = handle_skip, warning = handle_warning,     message = handle_message, error = handle_error)
12: doTryCatch(return(expr), name, parentenv, handler)
13: tryCatchOne(expr, names, parentenv, handlers[[1L]])
14: tryCatchList(expr, names[-nh], parentenv, handlers[-nh])
15: doTryCatch(return(expr), name, parentenv, handler)
16: tryCatchOne(tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),     names[nh], parentenv, handlers[[nh]])
17: tryCatchList(expr, classes, parentenv, handlers)
18: tryCatch(withCallingHandlers({    eval(code, test_env)    if (!handled && !is.null(test)) {        skip_empty()    }}, expectation = handle_expectation, skip = handle_skip, warning = handle_warning,     message = handle_message, error = handle_error), error = handle_fatal,     skip = function(e) {    })
19: test_code(desc, code, env = parent.frame(), reporter = reporter)
20: testthat::test_that(what, {    skip_if(getOption(""..skip.tests"", TRUE), ""arrow C++ library not available"")    code})
21: test_that(""dim() correctly determine numbers of rows and columns on arrow_dplyr_query object"",     {        skip_if_not_available(""parquet"")        ds <- open_dataset(dataset_dir, partitioning = schema(part = uint8()))        expect_identical(ds %>% filter(chr == ""a"") %>% dim(),             c(2L, 7L))        expect_equal(ds %>% select(chr, fct, int) %>% dim(),             c(20L, 3L))        expect_identical(ds %>% select(chr, fct, int) %>% filter(chr ==             ""a"") %>% dim(), c(2L, 3L))    })
22: eval(code, test_env)
23: eval(code, test_env)
24: withCallingHandlers({    eval(code, test_env)    if (!handled && !is.null(test)) {        skip_empty()    }}, expectation = handle_expectation, skip = handle_skip, warning = handle_warning,     message = handle_message, error = handle_error)
25: doTryCatch(return(expr), name, parentenv, handler)
26: tryCatchOne(expr, names, parentenv, handlers[[1L]])
27: tryCatchList(expr, names[-nh], parentenv, handlers[-nh])
28: doTryCatch(return(expr), name, parentenv, handler)
29: tryCatchOne(tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),     names[nh], parentenv, handlers[[nh]])
30: tryCatchList(expr, classes, parentenv, handlers)
31: tryCatch(withCallingHandlers({    eval(code, test_env)    if (!handled && !is.null(test)) {        skip_empty()    }}, expectation = handle_expectation, skip = handle_skip, warning = handle_warning,     message = handle_message, error = handle_error), error = handle_fatal,     skip = function(e) {    })
32: test_code(NULL, exprs, env)
33: source_file(path, child_env(env), wrap = wrap)
34: FUN(X[[i]], ...)
35: lapply(test_paths, test_one_file, env = env, wrap = wrap)
36: force(code)
37: doWithOneRestart(return(expr), restart)
38: withOneRestart(expr, restarts[[1L]])
39: withRestarts(testthat_abort_reporter = function() NULL, force(code))
40: with_reporter(reporters$multi, lapply(test_paths, test_one_file,     env = env, wrap = wrap))
41: test_files(test_dir = test_dir, test_package = test_package,     test_paths = test_paths, load_helpers = load_helpers, reporter = reporter,     env = env, stop_on_failure = stop_on_failure, stop_on_warning = stop_on_warning,     wrap = wrap, load_package = load_package)
42: test_files(test_dir = path, test_paths = test_paths, test_package = package,     reporter = reporter, load_helpers = load_helpers, env = env,     stop_on_failure = stop_on_failure, stop_on_warning = stop_on_warning,     wrap = wrap, load_package = load_package, parallel = parallel)
43: test_dir(""testthat"", package = package, reporter = reporter,     ..., load_package = ""installed"")
44: test_check(""arrow"", reporter = arrow_reporter)
An irrecoverable exception occurred. R is aborting now ...{noformat}
The test also seems to give the wrong results sometimes ([https://github.com/apache/arrow/pull/9656/checks?check_run_id=2518312803])
{noformat}
== Failed tests ================================================================
-- Failure (test-dataset.R:148:3): dim() correctly determine numbers of rows and columns on arrow_dplyr_query object --
ds %>% filter(chr == ""a"") %>% dim() not identical to c(2L, 7L).
1/2 mismatches
[1] 1 - 2 == -1
 {noformat}",pull-request-available,['C++'],ARROW,Bug,Major,2021-05-06 14:20:03,0
13376844,"[C++][Compute] Support SimplifyWithGuarantee(is_null(foo), is_valid(foo))","Simplifying a predicate using is_null with a guarantee using invert(is_null()) doesn't lead to any simplification.

Once supported, tests should be added for Datasets to exercise row group filtering (and to generate the necessary clauses from statistics).",dataset datasets expression expressions pull-request-available,['C++'],ARROW,Improvement,Major,2021-05-05 18:06:12,6
13376563,[C++][FlightRPC] Allow using TLS in benchmark,Allow enabling TLS in the Flight benchmark so we can evaluate its impact.,pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2021-05-04 13:20:30,0
13376554,[C++][CI][Packaging][Python] Bump vcpkg version to its latest release,"Need to update the vcpkg.patch as well.
https://github.com/apache/arrow/pull/10236",pull-request-available,"['C++', 'Continuous Integration', 'Packaging', 'Python']",ARROW,Test,Major,2021-05-04 12:27:12,3
13376551,[Python] Fix numpydoc validation,We haven't started to use the numpydoc validation feature of archery yet. We should start to utilize it so we can ensure all parameters are covered with docstrings. ,pull-request-available,['Python'],ARROW,Improvement,Major,2021-05-04 12:18:12,3
13376524,[C++][Dataset] Support reading date/time-partitioned datasets accounting for URL encoding (Spark),"I'm using Spark (3.1.1) to write a dataframe to a partitioned parquet dataset (using delta.io) which is partitioned by a timestamp field.

The relevant Spark code:
{code:java}
// code placeholder
(
  df.withColumn(
                ""Date"",
                sf.date_trunc(
                    ""DAY"",
                    sf.from_unixtime(
                        (sf.col(""MyEpochField"")),
                    ),
                ),
            )
    .write.format(""delta"")
    .mode(""append"")
    .partitionBy(""Date"")
    .save(""..."")

{code}
This gives a structure like following:
{code:java}
// code placeholder
/tip
/tip/Date=2021-05-04 00%3A00%3A00
/tip/Date=2021-05-04 00%3A00%3A00/Time=2021-05-04 07%3A27%3A00
/tip/Date=2021-05-04 00%3A00%3A00/Time=2021-05-04 07%3A27%3A00/part-00000-8846eb80-a369-43f6-a715-fec9cf1adf95.c000.snappy.parquet

{code}
Notice the : character is (url?) encoded because of fs protocol violation.

When i try to open this dataset using delta-rs ([https://github.com/delta-io/delta-rs)]which uses Arrow below the hood, then an error is raised trying to parse the Date (folder) value.
{code:java}
// code placeholder
pyarrow.lib.ArrowInvalid: error parsing '2021-05-03 00%3A00%3A00' as scalar of type timestamp[ns]
{code}
It seems this error is raised inScalarParseImpl =>ParseValue =>StringConverter<TimestampType>::Convert =>ParseTimestampISO8601

The mentioned parse method does support for format:
{code:java}
// code placeholder
static inline bool ParseTimestampISO8601(const char* s, size_t length,
                                         TimeUnit::type unit,
                                         TimestampType::c_type* out) {
  using seconds_type = std::chrono::duration<TimestampType::c_type>;  // We allow the following formats for all units:
  // - ""YYYY-MM-DD""
  // - ""YYYY-MM-DD[ T]hhZ?""
  // - ""YYYY-MM-DD[ T]hh:mmZ?""
  // - ""YYYY-MM-DD[ T]hh:mm:ssZ?""
<...>{code}
But may not support (url?) decoding the value upfront?

Questions we have:
 * Should Arrow support timestamp fields when used as partitioned field?
 * Where to decode?



Some more information from the writing side.

The writing is initiated usingFileFormatWriter.write that eventually uses aDynamicPartitionDataWriter (passing in the partitionColumns through the job description).

Here the actual ""value"" is rendered and concatennated.
{code:java}
// code placeholder
  /** Expression that given partition columns builds a path string like: col1=val/col2=val/... */
  private lazy val partitionPathExpression: Expression = Concat(
    description.partitionColumns.zipWithIndex.flatMap { case (c, i) =>
      val partitionName = ScalaUDF(
        ExternalCatalogUtils.getPartitionPathString _,
        StringType,
        Seq(Literal(c.name), Cast(c, StringType, Option(description.timeZoneId))))
      if (i == 0) Seq(partitionName) else Seq(Literal(Path.SEPARATOR), partitionName)
    })

{code}
Where the encoding is done in:

[https://github.com/apache/spark/blob/v3.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala#L66]

If i understand correct, then Arrow should provide the equivalent ofunescapePathName for fields used as partitioned columns.

",dataset datasets delta parquet pull-request-available spark,['C++'],ARROW,Bug,Major,2021-05-04 09:31:50,0
13376467,Add documentation for experimental repos,"See https://lists.apache.org/x/thread.html/r73d09033466e4fe2fbff8d4d81bb63e3aa9d16796f4b342f61a30815@%3Cdev.arrow.apache.org%3E for details

",pull-request-available,['Documentation'],ARROW,Task,Major,2021-05-04 02:26:05,9
13376444,[C++] Provide thread id accessors,"Sometimes, a worker thread must identify its thread id within a ThreadPool (for example, to support safe concurrent access to per-thread state). We could use {{std::this_thread::get_id()}} for this, but this is less convenient for indexing into a container of per-thread state.",pull-request-available,['C++'],ARROW,New Feature,Major,2021-05-03 20:43:57,6
13376435,[CI][Archery] Archery build fails to create branch,"The release script tests use the commits from the main branch, so it must be fetched before running the release submodule's tests. 

",pull-request-available,"['Archery', 'Continuous Integration']",ARROW,Bug,Major,2021-05-03 19:48:37,3
13376398,[C++][Dataset][Compute] Add support for dictionary_encode to Expression,"dictionary_encode should be usable in the context of ExecuteScalarExpression, but is not currently supported because it requires mutable state (the hash table). Currently scanning assumes that Expression state will not be mutated so only one instance is initialized and is shared between all threads of execution. Supporting dictionary_encode will require adding support for multiple states to Expression and usage of that by dataset scans.",dataset,['C++'],ARROW,New Feature,Major,2021-05-03 15:35:00,6
13376377,[Python] pyarrow.dataset.write_table should accept a Scanner to write,"Assume you open a dataset and want to write it back with some projected columns. Currently you need to actually materialize it to a Table or convert it to an iterator of batches, for being able to write the dataset:

{code:python}
import pyarrow.dataset as ds

dataset = ds.dataset(pa.table({'a': [1, 2, 3]}))

# write with projected columns
projection = {'b': ds.field('a')}

# this works but materializes full table
ds.write_dataset(dataset.to_table(columns=projection), ""test.parquet"", format=""parquet"")

# this requires the exact schema, which is a bit annoying as you need to construct that manually
ds.write_dataset(dataset.to_batches(columns=projection), ""test.parquet"", format=""parquet"", schema=...<projected schema>...)
{code}

You could expect to do the following?

{code}
ds.write_dataset(dataset.scanner(columns=projection), ""test.parquet"", format=""parquet"")
{code}

cc [~lidavidm] do you think this logic is correct?

(encountered this while trying to reproduce ARROW-12620 in Python)",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2021-05-03 12:54:39,5
13376376,[Dev][Integration] conda-integration docker build fails,"{code}
$ archery docker run conda-integration
[...]
WARNING: The RUST variable is not set. Defaulting to a blank string.
+ source_dir=/arrow/rust
+ export 'RUSTFLAGS=-C debuginfo=1'
+ RUSTFLAGS='-C debuginfo=1'
+ export ARROW_TEST_DATA=/testing/data
+ ARROW_TEST_DATA=/testing/data
+ export PARQUET_TEST_DATA=/cpp/submodules/parquet-testing/data
+ PARQUET_TEST_DATA=/cpp/submodules/parquet-testing/data
+ rustup show
Default host: x86_64-unknown-linux-gnu
rustup home:  /root/.rustup

stable-x86_64-unknown-linux-gnu (default)
rustc 1.51.0 (2fd73fabe 2021-03-23)
+ pushd /arrow/rust
/arrow/ci/scripts/rust_build.sh: line 37: pushd: /arrow/rust: No such file or directory
{code}",pull-request-available,"['Developer Tools', 'Integration']",ARROW,Bug,Major,2021-05-03 12:50:14,3
13376268,[C++] Experiment with -Bsymbolic-functions,"The linker option {{-Bsymbolic-functions}} may improve library load times (and binary sizes?) in addition to {{-fno-semantic-interposition}}:

https://lore.kernel.org/lkml/20210501235549.vugtjeb7dmd5xell@google.com/
",pull-request-available,['C++'],ARROW,Task,Minor,2021-05-02 11:48:06,2
13376210,[Python] Segfault when reading CSV inside Flight server,"Using pyarrow.csv.read_csv inside a Flight server results in a segfault. This did not happen in pyarrow 3.0.0.

The [CI build of a library we're building failed|https://github.com/timeseer-ai/kukur/runs/2467203316] and made us aware of the issue.

Attached, a CSV and Python server/client can be found that demonstrates the problem.
 * Run the server with `python crash.py server`.
 * Run the client with `python crash.py client`. The server segfaults with 'Segmentation fault (core dumped)'.

The crash does not happen when just reading the CSV (`python crash.py`).

This is the stacktrace generated by `coredumpctl debug` of a debug build of commit 2746266addddf71d20a4fe49381497b894c4d15c:
{code:java}
#0 0x00007f9275cffedc in __gnu_cxx::__atomic_add (__val=1, __mem=0x10) at /usr/include/c++/10.2.0/ext/atomicity.h:55

#1 __gnu_cxx::__atomic_add_dispatch (__val=1, __mem=0x10) at /usr/include/c++/10.2.0/ext/atomicity.h:96

#2 std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_add_ref_copy (this=0x8)

 at /usr/include/c++/10.2.0/bits/shared_ptr_base.h:142

#3 0x00007f9275cfe0a5 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::__shared_count (this=0x7f92735a2778, 
 __r=...) at /usr/include/c++/10.2.0/bits/shared_ptr_base.h:740

#4 0x00007f9275cfd01f in std::__shared_ptr<arrow::StopSourceImpl, (__gnu_cxx::_Lock_policy)2>::__shared_ptr (

 this=0x7f92735a2770) at /usr/include/c++/10.2.0/bits/shared_ptr_base.h:1181

#5 0x00007f9275cfd045 in std::shared_ptr<arrow::StopSourceImpl>::shared_ptr (this=0x7f92735a2770)

 at /usr/include/c++/10.2.0/bits/shared_ptr.h:149

#6 0x00007f9275cfd06b in arrow::StopToken::StopToken (this=0x7f92735a2770)

 at /home/jeroen/dev/python/apache-arrow/dist/include/arrow/util/cancel.h:57

#7 0x00007f9275ce96f7 in __pyx_pf_7pyarrow_4_csv_read_csv (__pyx_self=0x0, __pyx_v_input_file=0x7f929e9f28b0, 
 __pyx_v_read_options=0x7f929f49ee80 <_Py_NoneStruct>, __pyx_v_parse_options=0x7f929f49ee80 <_Py_NoneStruct>, 
 __pyx_v_convert_options=0x7f929f49ee80 <_Py_NoneStruct>, __pyx_v_memory_pool=0x7f929f49ee80 <_Py_NoneStruct>)

 at /home/jeroen/dev/python/apache-arrow/arrow/python/build/temp.linux-x86_64-3.8/_csv.cpp:14208

#8 0x00007f9275ce8b92 in __pyx_pw_7pyarrow_4_csv_1read_csv (__pyx_self=0x0, __pyx_args=0x7f929ea64be0, __pyx_kwds=0x0)

 at /home/jeroen/dev/python/apache-arrow/arrow/python/build/temp.linux-x86_64-3.8/_csv.cpp:14036

#9 0x00007f929f22cf98 in ?? () from /usr/lib/libpython3.8.so.1.0

#10 0x00007f929f22d5f8 in _PyObject_MakeTpCall () from /usr/lib/libpython3.8.so.1.0

{code}
Based on my limited understanding of the code, it looks like the error is here:
[https://github.com/apache/arrow/blob/master/python/pyarrow/_csv.pyx#L799]
{code:java}
    with SignalStopHandler() as stop_handler:
                io_context = CIOContext(
                    maybe_unbox_memory_pool(memory_pool),
                    (<StopToken> stop_handler.stop_token).stop_token)
{code}
Where `stop_token` is null, because the `SignalStopHandler` had an empty list of signals on creation ([https://github.com/apache/arrow/blob/master/python/pyarrow/error.pxi#L191).|https://github.com/apache/arrow/blob/master/python/pyarrow/error.pxi#L191]
{code:java}
        if (signal_handlers_enabled and
                threading.current_thread() is threading.main_thread()):
            self._signals = [
                sig for sig in (signal.SIGINT, signal.SIGTERM)
                if signal.getsignal(sig) not in (signal.SIG_DFL,
                                                 signal.SIG_IGN, None)]
        if not self._signals.empty():
            self._stop_token = StopToken()
            self._stop_token.init(GetResultValue(
                SetSignalStopSource()).token())
            self._enabled = True
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2021-05-01 13:00:34,0
13376163,[C++] Dataset writing can only include projected columns if input columns are also included,"I discovered this while working on https://github.com/apache/arrow/pull/10191. You can project new columns when writing a dataset, but only if they are derived from columns that are included in the output. Here's an R-based example:

{code}
# Simple function to write and re-open the new dataset
write_then_open <- function(ds, path, ...) {
  write_dataset(ds, path, ...)
  open_dataset(path)
}

tab <- Table$create(a = 1:5)

tab %>% 
  write_then_open(ds_dir) %>%
  collect()

# # A tibble: 5 x 1
#       a
#   <int>
# 1     1
# 2     2
# 3     3
# 4     4
# 5     5

# If you rename a column, it's all nulls
tab %>%
  select(b = a) %>%
  write_then_open(ds_dir) %>%
  collect()

# # A tibble: 5 x 1
#       b
#   <int>
# 1    NA
# 2    NA
# 3    NA
# 4    NA
# 5    NA

# If you derive a new column and keep the original, it works
tab %>%
  mutate(b = a) %>%
  write_then_open(ds_dir) %>%
  collect()

# # A tibble: 5 x 2
#       a     b
#   <int> <int>
# 1     1     1
# 2     2     2
# 3     3     3
# 4     4     4
# 5     5     5

# transmute() only keeps the added columns, so it also illustrates the failure
tab %>%
  transmute(b = a) %>%
  write_then_open(ds_dir) %>%
  collect()

# # A tibble: 5 x 1
#       b
#   <int>
# 1    NA
# 2    NA
# 3    NA
# 4    NA
# 5    NA
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2021-04-30 23:36:36,0
13376145,[Python] pyarrow sdist should not require git,"{noformat}
FROM ubuntu:20.04

RUN apt update && apt install -y python3-pip

RUN pip3 install --no-binary pyarrow pyarrow==4.0.0
{noformat}

{noformat}
$ docker build .
...
Step 3/3 : RUN pip3 install --no-binary pyarrow pyarrow==4.0.0
 ---> Running in 28d363e1c397
Collecting pyarrow==4.0.0
  Downloading pyarrow-4.0.0.tar.gz (710 kB)
  Installing build dependencies: started
  Installing build dependencies: still running...
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
    Preparing wheel metadata: started
    Preparing wheel metadata: finished with status 'error'
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python3 /tmp/tmp5rqecai7 prepare_metadata_for_build_wheel /tmp/tmpc49gha3r
         cwd: /tmp/pip-install-or1g7own/pyarrow
    Complete output (42 lines):
    Traceback (most recent call last):
      File ""/tmp/tmp5rqecai7"", line 280, in <module>
        main()
      File ""/tmp/tmp5rqecai7"", line 263, in main
        json_out['return_val'] = hook(**hook_input['kwargs'])
      File ""/tmp/tmp5rqecai7"", line 133, in prepare_metadata_for_build_wheel
        return hook(metadata_directory, config_settings)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 166, in prepare_metadata_for_build_wheel
        self.run_setup()
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 258, in run_setup
        super(_BuildMetaLegacyBackend,
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 150, in run_setup
        exec(compile(code, __file__, 'exec'), locals())
      File ""setup.py"", line 585, in <module>
        setup(
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup
        return distutils.core.setup(**attrs)
      File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup
        _setup_distribution = dist = klass(attrs)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 434, in __init__
        _Distribution.__init__(self, {
      File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__
        self.finalize_options()
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 743, in finalize_options
        ep(self)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 750, in _finalize_setup_keywords
        ep.load()(self, ep.name, value)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools_scm/integration.py"", line 24, in version_keyword
        dist.metadata.version = _get_version(config)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools_scm/__init__.py"", line 173, in _get_version
        parsed_version = _do_parse(config)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools_scm/__init__.py"", line 119, in _do_parse
        parse_result = _call_entrypoint_fn(config.absolute_root, config, config.parse)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools_scm/__init__.py"", line 54, in _call_entrypoint_fn
        return fn(root)
      File ""setup.py"", line 546, in parse_git
        return parse(root, **kwargs)
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools_scm/git.py"", line 115, in parse
        require_command(""git"")
      File ""/tmp/pip-build-env-d53awzo4/overlay/lib/python3.8/site-packages/setuptools_scm/utils.py"", line 142, in require_command
        raise OSError(""%r was not found"" % name)
    OSError: 'git' was not found
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python3 /tmp/tmp5rqecai7 prepare_metadata_for_build_wheel /tmp/tmpc49gha3r Check the logs for full command output.
The command '/bin/sh -c pip3 install --no-binary pyarrow pyarrow==4.0.0' returned a non-zero code: 1
{noformat}",pull-request-available,['Python'],ARROW,Improvement,Major,2021-04-30 20:47:55,3
13376124,[Python] pyarrow.orc.write_table signature reverses that of pyarrow.parquet.write_table,"
The following signatures are inverted and for consistency it would be good for them to align:

{code:python}
pyarrow.parquet.write_table(table, where)
pyarrow.orc.write_table(where, table)
{code}

{code:python}
In [1]: import pyarrow.parquet as pa

In [2]: import pyarrow.orc as po

In[3]: pa.write_table?
Signature:
pa.write_table(
    table,
    where,
    row_group_size=None,
    version='1.0',
    use_dictionary=True,
    compression='snappy',
    write_statistics=True,
    use_deprecated_int96_timestamps=None,
    coerce_timestamps=None,
    allow_truncated_timestamps=False,
    data_page_size=None,
    flavor=None,
    filesystem=None,
    compression_level=None,
    use_byte_stream_split=False,
    data_page_version='1.0',
    use_compliant_nested_type=False,
    **kwargs,

In [11]: po.write_table?
Signature: po.write_table(where, table)
{code}
",pull-request-available,['Python'],ARROW,Bug,Minor,2021-04-30 17:32:43,5
13376086,[C++][Compute] Revert support for Tables in ExecuteScalarExpression,ARROW-11929 added untested support for evaluating expressions against tables to ExecuteScalarExpression,pull-request-available,['C++'],ARROW,Bug,Major,2021-04-30 14:42:50,6
13376067,[C++][Compute] Add Expression to type_fwd.h,Fixes the r-minimal build.,pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-30 13:35:28,0
13376059,[CI][Python] Nightly test-conda-python-pandas-0.24 is failing due to numpy compat issue,"See eg https://github.com/ursacomputing/crossbow/runs/2473244858

The reason is that the build is installing the latest numpy 1.20.2, which doesn't work with pandas 0.14. So for an older version of pandas, we also need to install an older version of numpy.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Test,Major,2021-04-30 13:00:39,5
13376009,[C++] Add split_pattern_regex function,"Currently the split_pattern compute kernel can split strings by a given pattern, but it'd be really helpful if the specified pattern could be a regular expression",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-30 09:23:37,0
13375911,[Documentation] Repair line numbers in dataset.rst,ARROW-11929 broke the line endings in dataset.rst,pull-request-available,['Documentation'],ARROW,Bug,Minor,2021-04-29 21:09:30,0
13375860,[CI] Push docker images from crossbow tasks,For better caching.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2021-04-29 14:53:25,3
13375847,[C++][Dataset] Implement row-count for CSV or allow selecting 0 columns from CSV,"For ARROW-9697 file formats can implement a fast path to count rows in a fragment. For CSV this isn't implemented. We could do the equivalent of {{wc -l}} for CSV (using the lexing boundary finder as needed) and adjust the row count based on options for the header, or we could change the CSV reader options to allow selecting no columns (right now, passing no columns to the reader implies you want to read all columns). The former is likely faster but the latter will be more robust/less work.",dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-29 13:38:07,0
13375829,[C++] Implement OptionalParallelForAsync,"In ARROW-11843 we found that we still can't enable parallel column conversion for Parquet datasets because the Arrow Parquet reader uses OptionalParallelFor, which does a parallel wait. We should provide an async version to avoid nested parallelism.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-29 12:02:59,0
13375498,[C++][FlightRPC] Benchmark compression with real data,"Flight benchmark program is using random test data as payload. Random data is hard to compress. Per my test, the compressed payload (zstd default level) is actually a bit larger than the original uncompressed payload. So compression is a pure loss in current benchmark.

It's better to use real world test set for a more reasonable benchmark about how compression influences FlightRPC performance. Perhaps the benchmark (both client/server) could accept a path to an IPC file as an option.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2021-04-28 05:07:48,0
13375466,[Python] Pyarrow 4.0.0 dependency numpy 1.19.4 throws errors on Apple silicon/M1 compilation,"Hi team! I've been unable to install older numpy versions (including 1.19.4 as specified here f[or aarch64 machine|https://github.com/apache/arrow/blob/master/python/requirements-wheel-build.txt]s) on my Apple Silicon machine because the build process throws a number of numpy compilation errors. I've been able to successfully install numpy 1.20.2 however - is it possible to bump up the numpy acceptable version number to enable Apple silicon installations?

Thanks!",build-failure,['Python'],ARROW,Bug,Minor,2021-04-28 01:02:03,3
13375158,Allow duplicates in the value_set for compute::is_in  ,"In the arrow release-4.0.0 branch, the `compute::is_in` operation rejects duplicate values in the `value_set` [1]. This was not the case in arrow 2.0 >=.

I was wondering if this strict restriction is required? Because ultimately, a hash set would be created from the value_set values, and there's no harm in having duplicates while doing so, isn't it?
PS: I understand that the param name ""value_set"" indicates that the values need to be unique, but in the useability perspective, this can be relaxed IMO. ex: Pandas isin [2].


[1] [https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/kernels/scalar_set_lookup.cc#L53]
[2] [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html]",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2021-04-26 20:40:57,2
13375071,[CI][Python] Failing conda-python-3.9 Nightly Build,"The build segfaults after running the Python tests.
Very similar to https://issues.apache.org/jira/browse/ARROW-11146",pull-request-available,"['Continuous Integration', 'Packaging', 'Python']",ARROW,Bug,Major,2021-04-26 14:09:40,5
13375062,[Docs] Improve styling/readability of tables in the new doc theme,"In the new doc theme, tables don't have a border / different background as the rest of the body. We can add some striping to let them stand out more.",pull-request-available,['Documentation'],ARROW,Improvement,Major,2021-04-26 13:20:26,5
13374840,[C++] Remove Buffer::mutable_data_ member and use const_cast on data_ only if is_mutable_ is true,"Proposed new implementation of mutable_data() by [~apitrou]

{code}
  uint8_t* mutable_data() {
     return is_mutable() ? const_cast<uint8_t*>(data()) : nullptr;
   }
{code}

This will help avoid various classes of bugs (initializing Buffer subclasses incorrectly) and make the object smaller on the heap",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-24 22:03:00,2
13374724,"[C++] Implement asynchronous/""lazy"" variants of ReadRangeCache","Currently ReadRangeCache performs both readahead and coalescing. Also, it exposes primarily a blocking API. Two improvements would be useful for implementing async-generator versions of file readers:
 * A method to get a Future<> for a set of read ranges, so that you can asynchronously wait for ranges to be read instead of attempting to read and getting blocked
 * A way to make the cache not perform readahead, so that data is fetched only when requested. (Then, consumers could handle readahead by making multiple requests to the cache.)

The cache would still act as an actual cache and would still coalesce. (A further improvement might be to allow discarding cache entries. For the purpose of getting AsyncGenerator<RecordBatch>, we don't need a range more than once, so the cache is just wasting memory.)

This makes it straightforward to adapt synchronous readers into asynchronous ones so long as you know the read ranges up front; you can then cache all the ranges, call WaitFor<>, then hand the buffer to the existing synchronous reader.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-23 19:55:36,0
13374618,[Python] Expose Parquet statistics has_null_count / has_distinct_count,"This was added to C++ in PARQUET-1566, could be exposed in Python as well",pull-request-available,['Python'],ARROW,Improvement,Major,2021-04-23 10:05:03,5
13374474,[C++][Dataset] Implement CSV writing support,"Now that there's a CSV writer, we should hook it up to Datasets.

It seems some refactoring will be needed to expose a full writer class for CSV so that Datasets can write batches incrementally.",csv dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-22 17:53:51,0
13374437,[CI] Remove duplicated cron/nightly builds,"There are builds duplicated between the GHA cron jobs and crossbow nightlies.
",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2021-04-22 15:20:42,3
13374310,"[C++] Ensure using ""lib/"" for jemalloc's library directory","I have checked [https://arrow.apache.org/docs/r/articles/install.html#package-installed-without-c-dependencies|https://arrow.apache.org/docs/r/articles/install.html#package-installed-without-c-dependencies,]and that none of the known issues apply to me.

So then it's telling me to issue `[arrow::install_arrow(verbose = TRUE)|https://arrow.apache.org/docs/r/reference/install_arrow.html]`, which I did. Here's the output:


----



{code}
> arrow::install_arrow(verbose = TRUE)
 Installing package into /data2/bers/opt/R/4.0/library
 (as lib is unspecified)
 trying URL 'https://cran.r-project.org/src/contrib/arrow_3.0.0.tar.gz'
 Content type 'application/x-gzip' length 344814 bytes (336 KB)
 ==================================================
 downloaded 336 KB
 * installing *source* package arrow ...
 ** package arrow successfully unpacked and MD5 sums checked
 ** using staged installation
 trying URL 'https://dl.bintray.com/ursalabs/arrow-r/libarrow/bin/opensuse-15/arrow-3.0.0.zip'
 Error in download.file(from_url, to_file, quiet = quietly) : 
 cannot open URL 'https://dl.bintray.com/ursalabs/arrow-r/libarrow/bin/opensuse-15/arrow-3.0.0.zip'
 *** No C++ binaries found for opensuse-15
 trying URL 'https://dl.bintray.com/ursalabs/arrow-r/libarrow/src/arrow-3.0.0.zip'
 Error in download.file(from_url, to_file, quiet = quietly) : 
 cannot open URL 'https://dl.bintray.com/ursalabs/arrow-r/libarrow/src/arrow-3.0.0.zip'
 trying URL 'https://www.apache.org/dyn/closer.lua?action=download&filename=arrow/arrow-3.0.0/apache-arrow-3.0.0.tar.gz'
 Content type 'application/x-gzip' length 8200790 bytes (7.8 MB)
 ==================================================
 downloaded 7.8 MB

 * 
 ** 
 *** Successfully retrieved C++ source
 *** Building C++ libraries
 *** Building with MAKEFLAGS= -j2
 **** arrow with SOURCE_DIR=""/tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp"" BUILD_DIR=""/tmp/RtmpOXZGhl/file1568178b21a6"" DEST_DIR=""libarrow/arrow-3.0.0"" CMAKE=""/data2/bers/opt/cmake/bin/cmake"" CC=""gcc"" CXX=""g++ -std=gnu++11"" LDFLAGS=""-L/usr/local/lib64"" ARROW_S3=ON ARROW_MIMALLOC=ON
 ++ pwd
 + : /tmp/RtmppXbaGR/R.INSTALL155322509007/arrow
 + : /tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp
 + : /tmp/RtmpOXZGhl/file1568178b21a6
 + : libarrow/arrow-3.0.0
 + : /data2/bers/opt/cmake/bin/cmake
 ++ cd /tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp
 ++ pwd
 + SOURCE_DIR=/tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp
 ++ mkdir -p libarrow/arrow-3.0.0
 ++ cd libarrow/arrow-3.0.0
 ++ pwd
 + DEST_DIR=/tmp/RtmppXbaGR/R.INSTALL155322509007/arrow/libarrow/arrow-3.0.0
 + '[' '' = '' ']'
 + which ninja
 + '[' FALSE = false ']'
 + ARROW_DEFAULT_PARAM=OFF
 + mkdir -p /tmp/RtmpOXZGhl/file1568178b21a6
 + pushd /tmp/RtmpOXZGhl/file1568178b21a6
 /tmp/RtmpOXZGhl/file1568178b21a6 /tmp/RtmppXbaGR/R.INSTALL155322509007/arrow
 + /data2/bers/opt/cmake/bin/cmake -DARROW_BOOST_USE_SHARED=OFF -DARROW_BUILD_TESTS=OFF -DARROW_BUILD_SHARED=OFF -DARROW_BUILD_STATIC=ON -DARROW_COMPUTE=ON -DARROW_CSV=ON -DARROW_DATASET=ON -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_FILESYSTEM=ON -DARROW_JEMALLOC=ON -DARROW_MIMALLOC=ON -DARROW_JSON=ON -DARROW_PARQUET=ON -DARROW_S3=ON -DARROW_WITH_BROTLI=OFF -DARROW_WITH_BZ2=OFF -DARROW_WITH_LZ4=OFF -DARROW_WITH_SNAPPY=OFF -DARROW_WITH_UTF8PROC=OFF -DARROW_WITH_ZLIB=OFF -DARROW_WITH_ZSTD=OFF -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_LIBDIR=lib -DCMAKE_INSTALL_PREFIX=/tmp/RtmppXbaGR/R.INSTALL155322509007/arrow/libarrow/arrow-3.0.0 -DCMAKE_EXPORT_NO_PACKAGE_REGISTRY=ON -DCMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY=ON -DCMAKE_UNITY_BUILD=ON -G 'Unix Makefiles' /tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp
 -- Building using CMake version: 3.19.5
 -- The C compiler identification is GNU 7.5.0
 -- The CXX compiler identification is GNU 7.5.0
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /usr/bin/gcc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /usr/bin/g++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- Arrow version: 3.0.0 (full: '3.0.0')
 -- Arrow SO version: 300 (full: 300.0.0)
 -- clang-tidy not found
 -- clang-format not found
 -- Could NOT find ClangTools (missing: CLANG_FORMAT_BIN CLANG_TIDY_BIN)
 -- infer not found
 fatal: not a git repository (or any of the parent directories): .git
 -- Could NOT find Python3 (missing: Python3_EXECUTABLE Interpreter)
 Reason given by package: 
 Interpreter: Cannot use the interpreter ""/data2/bers/opt/pyenv/shims/python3.9""

 Found cpplint executable at /tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp/build-support/cpplint.py
  System processor: x86_64
  Performing Test CXX_SUPPORTS_SSE4_2
  Performing Test CXX_SUPPORTS_SSE4_2 - Success
  Performing Test CXX_SUPPORTS_AVX2
  Performing Test CXX_SUPPORTS_AVX2 - Success
  Performing Test CXX_SUPPORTS_AVX512
  Performing Test CXX_SUPPORTS_AVX512 - Success
  Arrow build warning level: PRODUCTION
 Using ld linker
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
  Build Type: RELEASE
  Using BUNDLED approach to find dependencies
  ARROW_ABSL_BUILD_VERSION: 0f3bb466b868b523cf1dc9b2aaaed65c77b28862
  ARROW_AWSSDK_BUILD_VERSION: 1.8.90
  ARROW_AWS_CHECKSUMS_BUILD_VERSION: v0.1.5
  ARROW_AWS_C_COMMON_BUILD_VERSION: v0.4.59
  ARROW_AWS_C_EVENT_STREAM_BUILD_VERSION: v0.1.5
  ARROW_BOOST_BUILD_VERSION: 1.71.0
  ARROW_BROTLI_BUILD_VERSION: v1.0.7
  ARROW_BZIP2_BUILD_VERSION: 1.0.8
  ARROW_CARES_BUILD_VERSION: 1.16.1
  ARROW_GBENCHMARK_BUILD_VERSION: v1.5.2
  ARROW_GFLAGS_BUILD_VERSION: v2.2.2
  ARROW_GLOG_BUILD_VERSION: v0.4.0
  ARROW_GRPC_BUILD_VERSION: v1.33.2
  ARROW_GTEST_BUILD_VERSION: 1.10.0
  ARROW_JEMALLOC_BUILD_VERSION: 5.2.1
  ARROW_LZ4_BUILD_VERSION: v1.9.2
  ARROW_MIMALLOC_BUILD_VERSION: v1.6.4
  ARROW_ORC_BUILD_VERSION: 1.6.2
  ARROW_PROTOBUF_BUILD_VERSION: v3.13.0
  ARROW_RAPIDJSON_BUILD_VERSION: 1a803826f1197b5e30703afe4b9c0e7dd48074f5
  ARROW_RE2_BUILD_VERSION: 2019-08-01
  ARROW_SNAPPY_BUILD_VERSION: 1.1.8
  ARROW_THRIFT_BUILD_VERSION: 0.12.0
  ARROW_THRIFT_BUILD_MD5_CHECKSUM: 3deebbb4d1ca77dd9c9e009a1ea02183
  ARROW_UTF8PROC_BUILD_VERSION: v2.5.0
  ARROW_ZLIB_BUILD_VERSION: 1.2.11
  ARROW_ZSTD_BUILD_VERSION: v1.4.5
  Looking for pthread.h
  Looking for pthread.h - found
  Performing Test CMAKE_HAVE_LIBC_PTHREAD
  Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
  Check if compiler accepts -pthread
  Check if compiler accepts -pthread - yes
  Found Threads: TRUE 
  Looking for __SIZEOF_INT128__
  Looking for __SIZEOF_INT128__ - found
  Boost include dir: /tmp/RtmpOXZGhl/file1568178b21a6/boost_ep-prefix/src/boost_ep
  Boost libraries: boost_system_static;boost_filesystem_static
  Found OpenSSL: /usr/lib64/libcrypto.so (found suitable version ""1.1.1d"", minimum required is ""1.0.2"") 
  Found OpenSSL Crypto Library: /usr/lib64/libcrypto.so
  Building with OpenSSL (Version: 1.1.1d) support
 Building Apache Thrift from source
  Building (vendored) jemalloc from source
  Building (vendored) mimalloc from source
  Building RapidJSON from source
  Building ZLIB from source
  Building RE2 from source
  Found hdfs.h at: /tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp/thirdparty/hadoop/include/hdfs.h
 Building AWS C++ SDK from source
  Found CURL: /usr/lib64/libcurl.so (found version ""7.72.0"") 
  Found AWS SDK headers: /tmp/RtmpOXZGhl/file1568178b21a6/awssdk_ep-install/include
  Found AWS SDK libraries: aws-cpp-sdk-identity-management;aws-cpp-sdk-sts;aws-cpp-sdk-cognito-identity;aws-cpp-sdk-s3;aws-cpp-sdk-core;AWS::aws-c-event-stream;AWS::aws-checksums;AWS::aws-c-common
  All bundled static libraries: thrift::thrift;jemalloc::jemalloc;mimalloc::mimalloc;ZLIB::ZLIB;re2::re2;aws-cpp-sdk-identity-management;aws-cpp-sdk-sts;aws-cpp-sdk-cognito-identity;aws-cpp-sdk-s3;aws-cpp-sdk-core;AWS::aws-c-event-stream;AWS::aws-checksums;AWS::aws-c-common
  CMAKE_C_FLAGS: -O3 -DNDEBUG -Wall -fno-semantic-interposition -msse4.2 
  CMAKE_CXX_FLAGS: -Wno-noexcept-type -Wno-subobject-linkage -fdiagnostics-color=always -O3 -DNDEBUG -Wall -fno-semantic-interposition -msse4.2 
  Looking for backtrace
  Looking for backtrace - found
  backtrace facility detected in default set of libraries
  Found Backtrace: /usr/include 
  Creating bundled static library target arrow_bundled_dependencies at /tmp/RtmpOXZGhl/file1568178b21a6/release/libarrow_bundled_dependencies.a
  ---------------------------------------------------------------------
  Arrow version: 3.0.0
  
  Build configuration summary:
  Generator: Unix Makefiles
  Build type: RELEASE
  Source directory: /tmp/RtmpOXZGhl/file156868fe52ec/apache-arrow-3.0.0/cpp
  Install prefix: /tmp/RtmppXbaGR/R.INSTALL155322509007/arrow/libarrow/arrow-3.0.0
  
  Compile and link options:
  
  ARROW_CXXFLAGS="""" [default=""""]
  Compiler flags to append when compiling Arrow
  ARROW_BUILD_STATIC=ON [default=ON]
  Build static libraries
  ARROW_BUILD_SHARED=OFF [default=ON]
  Build shared libraries
  ARROW_PACKAGE_KIND="""" [default=""""]
  Arbitrary string that identifies the kind of package
  (for informational purposes)
  ARROW_GIT_ID="""" [default=""""]
  The Arrow git commit id (if any)
  ARROW_GIT_DESCRIPTION="""" [default=""""]
  The Arrow git commit description (if any)
  ARROW_NO_DEPRECATED_API=OFF [default=OFF]
  Exclude deprecated APIs from build
  ARROW_USE_CCACHE=ON [default=ON]
  Use ccache when compiling (if available)
  ARROW_USE_LD_GOLD=OFF [default=OFF]
  Use ld.gold for linking on Linux (if available)
  ARROW_USE_PRECOMPILED_HEADERS=OFF [default=OFF]
  Use precompiled headers when compiling
  ARROW_SIMD_LEVEL=SSE4_2 [default=NONE|SSE4_2|AVX2|AVX512]
  Compile-time SIMD optimization level
  ARROW_RUNTIME_SIMD_LEVEL=MAX [default=NONE|SSE4_2|AVX2|AVX512|MAX]
  Max runtime SIMD optimization level
  ARROW_ARMV8_ARCH=armv8-a [default=armv8-a|armv8-a+crc+crypto]
  Arm64 arch and extensions
  ARROW_ALTIVEC=ON [default=ON]
  Build with Altivec if compiler has support
  ARROW_RPATH_ORIGIN=OFF [default=OFF]
  Build Arrow libraries with RATH set to $ORIGIN
  ARROW_INSTALL_NAME_RPATH=ON [default=ON]
  Build Arrow libraries with install_name set to @rpath
  ARROW_GGDB_DEBUG=ON [default=ON]
  Pass -ggdb flag to debug builds
  
  Test and benchmark options:
  
  ARROW_BUILD_EXAMPLES=OFF [default=OFF]
  Build the Arrow examples
  ARROW_BUILD_TESTS=OFF [default=OFF]
  Build the Arrow googletest unit tests
  ARROW_ENABLE_TIMING_TESTS=ON [default=ON]
  Enable timing-sensitive tests
  ARROW_BUILD_INTEGRATION=OFF [default=OFF]
  Build the Arrow integration test executables
  ARROW_BUILD_BENCHMARKS=OFF [default=OFF]
  Build the Arrow micro benchmarks
  ARROW_BUILD_BENCHMARKS_REFERENCE=OFF [default=OFF]
  Build the Arrow micro reference benchmarks
  ARROW_TEST_LINKAGE=static [default=shared|static]
  Linkage of Arrow libraries with unit tests executables.
  ARROW_FUZZING=OFF [default=OFF]
  Build Arrow Fuzzing executables
  ARROW_LARGE_MEMORY_TESTS=OFF [default=OFF]
  Enable unit tests which use large memory
  
  Lint options:
  
  ARROW_ONLY_LINT=OFF [default=OFF]
  Only define the lint and check-format targets
  ARROW_VERBOSE_LINT=OFF [default=OFF]
  If off, 'quiet' flags will be passed to linting tools
  ARROW_GENERATE_COVERAGE=OFF [default=OFF]
  Build with C++ code coverage enabled
  
  Checks options:
  
  ARROW_TEST_MEMCHECK=OFF [default=OFF]
  Run the test suite using valgrind --tool=memcheck
  ARROW_USE_ASAN=OFF [default=OFF]
  Enable Address Sanitizer checks
  ARROW_USE_TSAN=OFF [default=OFF]
  Enable Thread Sanitizer checks
  ARROW_USE_UBSAN=OFF [default=OFF]
  Enable Undefined Behavior sanitizer checks
  
  Project component options:
  
  ARROW_BUILD_UTILITIES=OFF [default=OFF]
  Build Arrow commandline utilities
  ARROW_COMPUTE=ON [default=OFF]
  Build the Arrow Compute Modules
  ARROW_CSV=ON [default=OFF]
  Build the Arrow CSV Parser Module
  ARROW_CUDA=OFF [default=OFF]
  Build the Arrow CUDA extensions (requires CUDA toolkit)
  ARROW_DATASET=ON [default=OFF]
  Build the Arrow Dataset Modules
  ARROW_FILESYSTEM=ON [default=OFF]
  Build the Arrow Filesystem Layer
  ARROW_FLIGHT=OFF [default=OFF]
  Build the Arrow Flight RPC System (requires GRPC, Protocol Buffers)
  ARROW_GANDIVA=OFF [default=OFF]
  Build the Gandiva libraries
  ARROW_HDFS=OFF [default=OFF]
  Build the Arrow HDFS bridge
  ARROW_HIVESERVER2=OFF [default=OFF]
  Build the HiveServer2 client and Arrow adapter
  ARROW_IPC=ON [default=ON]
  Build the Arrow IPC extensions
  ARROW_JEMALLOC=ON [default=ON]
  Build the Arrow jemalloc-based allocator
  ARROW_JNI=OFF [default=OFF]
  Build the Arrow JNI lib
  ARROW_JSON=ON [default=OFF]
  Build Arrow with JSON support (requires RapidJSON)
  ARROW_MIMALLOC=ON [default=OFF]
  Build the Arrow mimalloc-based allocator
  ARROW_PARQUET=ON [default=OFF]
  Build the Parquet libraries
  ARROW_ORC=OFF [default=OFF]
  Build the Arrow ORC adapter
  ARROW_PLASMA=OFF [default=OFF]
  Build the plasma object store along with Arrow
  ARROW_PLASMA_JAVA_CLIENT=OFF [default=OFF]
  Build the plasma object store java client
  ARROW_PYTHON=OFF [default=OFF]
  Build the Arrow CPython extensions
  ARROW_S3=ON [default=OFF]
  Build Arrow with S3 support (requires the AWS SDK for C++)
  ARROW_TENSORFLOW=OFF [default=OFF]
  Build Arrow with TensorFlow support enabled
  ARROW_TESTING=OFF [default=OFF]
  Build the Arrow testing libraries
  
  Thirdparty toolchain options:
  
  ARROW_DEPENDENCY_SOURCE=BUNDLED [default=AUTO|BUNDLED|SYSTEM|CONDA|BREW]
  Method to use for acquiring arrow's build dependencies
  ARROW_VERBOSE_THIRDPARTY_BUILD=OFF [default=OFF]
  Show output from ExternalProjects rather than just logging to files
  ARROW_DEPENDENCY_USE_SHARED=ON [default=ON]
  Link to shared libraries
  ARROW_BOOST_USE_SHARED=OFF [default=ON]
  Rely on boost shared libraries where relevant
  ARROW_BROTLI_USE_SHARED=ON [default=ON]
  Rely on Brotli shared libraries where relevant
  ARROW_BZ2_USE_SHARED=ON [default=ON]
  Rely on Bz2 shared libraries where relevant
  ARROW_GFLAGS_USE_SHARED=ON [default=ON]
  Rely on GFlags shared libraries where relevant
  ARROW_GRPC_USE_SHARED=ON [default=ON]
  Rely on gRPC shared libraries where relevant
  ARROW_LZ4_USE_SHARED=ON [default=ON]
  Rely on lz4 shared libraries where relevant
  ARROW_OPENSSL_USE_SHARED=ON [default=ON]
  Rely on OpenSSL shared libraries where relevant
  ARROW_PROTOBUF_USE_SHARED=ON [default=ON]
  Rely on Protocol Buffers shared libraries where relevant
  ARROW_THRIFT_USE_SHARED=ON [default=ON]
  Rely on thrift shared libraries where relevant
  ARROW_UTF8PROC_USE_SHARED=ON [default=ON]
  Rely on utf8proc shared libraries where relevant
  ARROW_SNAPPY_USE_SHARED=ON [default=ON]
  Rely on snappy shared libraries where relevant
  ARROW_UTF8PROC_USE_SHARED=ON [default=ON]
  Rely on utf8proc shared libraries where relevant
  ARROW_ZSTD_USE_SHARED=ON [default=ON]
  Rely on zstd shared libraries where relevant
  ARROW_USE_GLOG=OFF [default=OFF]
  Build libraries with glog support for pluggable logging
  ARROW_WITH_BACKTRACE=ON [default=ON]
  Build with backtrace support
  ARROW_WITH_BROTLI=OFF [default=OFF]
  Build with Brotli compression
  ARROW_WITH_BZ2=OFF [default=OFF]
  Build with BZ2 compression
  ARROW_WITH_LZ4=OFF [default=OFF]
  Build with lz4 compression
  ARROW_WITH_SNAPPY=OFF [default=OFF]
  Build with Snappy compression
  ARROW_WITH_ZLIB=ON [default=OFF]
  Build with zlib compression
  ARROW_WITH_ZSTD=OFF [default=OFF]
  Build with zstd compression
  ARROW_WITH_UTF8PROC=OFF [default=ON]
  Build with support for Unicode properties using the utf8proc library
  (only used if ARROW_COMPUTE is ON)
  ARROW_WITH_RE2=ON [default=ON]
  Build with support for regular expressions using the re2 library
  (only used if ARROW_COMPUTE or ARROW_GANDIVA is ON)
  
  Parquet options:
  
  PARQUET_MINIMAL_DEPENDENCY=OFF [default=OFF]
  Depend only on Thirdparty headers to build libparquet.
  Always OFF if building binaries
  PARQUET_BUILD_EXECUTABLES=OFF [default=OFF]
  Build the Parquet executable CLI tools. Requires static libraries to be built.
  PARQUET_BUILD_EXAMPLES=OFF [default=OFF]
  Build the Parquet examples. Requires static libraries to be built.
  PARQUET_REQUIRE_ENCRYPTION=OFF [default=OFF]
  Build support for encryption. Fail if OpenSSL is not found
  
  Gandiva options:
  
  ARROW_GANDIVA_JAVA=OFF [default=OFF]
  Build the Gandiva JNI wrappers
  ARROW_GANDIVA_STATIC_LIBSTDCPP=OFF [default=OFF]
  Include -static-libstdc++ -static-libgcc when linking with
  Gandiva static libraries
  ARROW_GANDIVA_PC_CXX_FLAGS="""" [default=""""]
  Compiler flags to append when pre-compiling Gandiva operations
  
  Advanced developer options:
  
  ARROW_EXTRA_ERROR_CONTEXT=OFF [default=OFF]
  Compile with extra error context (line numbers, code)
  ARROW_OPTIONAL_INSTALL=OFF [default=OFF]
  If enabled install ONLY targets that have already been built. Please be
  advised that if this is enabled 'install' will fail silently on components
  that have not been built
  Outputting build configuration summary to /tmp/RtmpOXZGhl/file1568178b21a6/cmake_summary.json
  Configuring done
  Generating done
  Build files have been written to: /tmp/RtmpOXZGhl/file1568178b21a6
 + /data2/bers/opt/cmake/bin/cmake --build . --target install
 Scanning dependencies of target jemalloc_ep
 Scanning dependencies of target boost_ep
 [ 0%] Creating directories for 'jemalloc_ep'
 [ 1%] Creating directories for 'boost_ep'
 [ 2%] Performing download step (download, verify and extract) for 'jemalloc_ep'
 [ 3%] Performing download step (download, verify and extract) for 'boost_ep'
  jemalloc_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-download-*.log
 [ 4%] No update step for 'jemalloc_ep'
 [ 5%] Performing patch step for 'jemalloc_ep'
 [ 6%] Performing configure step for 'jemalloc_ep'
  boost_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/boost_ep-prefix/src/boost_ep-stamp/boost_ep-download-*.log
 [ 7%] No update step for 'boost_ep'
 [ 8%] No patch step for 'boost_ep'
 [ 8%] No configure step for 'boost_ep'
 [ 9%] No build step for 'boost_ep'
 [ 9%] No install step for 'boost_ep'
 [ 10%] Completed 'boost_ep'
 [ 10%] Built target boost_ep
 Scanning dependencies of target rapidjson_ep
 [ 11%] Creating directories for 'rapidjson_ep'
 [ 12%] Performing download step (download, verify and extract) for 'rapidjson_ep'
  rapidjson_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/src/rapidjson_ep-stamp/rapidjson_ep-download-*.log
 [ 13%] No update step for 'rapidjson_ep'
 [ 13%] No patch step for 'rapidjson_ep'
 [ 14%] Performing configure step for 'rapidjson_ep'
  rapidjson_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/src/rapidjson_ep-stamp/rapidjson_ep-configure-*.log
 [ 15%] Performing build step for 'rapidjson_ep'
  rapidjson_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/src/rapidjson_ep-stamp/rapidjson_ep-build-*.log
 [ 16%] Performing install step for 'rapidjson_ep'
  rapidjson_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/src/rapidjson_ep-stamp/rapidjson_ep-install-*.log
 [ 16%] Completed 'rapidjson_ep'
 [ 16%] Built target rapidjson_ep
 Scanning dependencies of target aws_c_common_ep
 [ 16%] Creating directories for 'aws_c_common_ep'
 [ 17%] Performing download step (download, verify and extract) for 'aws_c_common_ep'
  aws_c_common_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_common_ep-prefix/src/aws_c_common_ep-stamp/aws_c_common_ep-download-*.log
 [ 18%] No update step for 'aws_c_common_ep'
 [ 19%] No patch step for 'aws_c_common_ep'
 [ 20%] Performing configure step for 'aws_c_common_ep'
  aws_c_common_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_common_ep-prefix/src/aws_c_common_ep-stamp/aws_c_common_ep-configure-*.log
 [ 20%] Performing build step for 'aws_c_common_ep'
  jemalloc_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-*.log
 [ 20%] Performing build step for 'jemalloc_ep'
  aws_c_common_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_common_ep-prefix/src/aws_c_common_ep-stamp/aws_c_common_ep-build-*.log
 [ 21%] Performing install step for 'aws_c_common_ep'
  aws_c_common_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_common_ep-prefix/src/aws_c_common_ep-stamp/aws_c_common_ep-install-*.log
 [ 22%] Completed 'aws_c_common_ep'
 [ 22%] Built target aws_c_common_ep
 Scanning dependencies of target aws_checksums_ep
 [ 23%] Creating directories for 'aws_checksums_ep'
 [ 24%] Performing download step (download, verify and extract) for 'aws_checksums_ep'
  aws_checksums_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_checksums_ep-prefix/src/aws_checksums_ep-stamp/aws_checksums_ep-download-*.log
 [ 24%] No update step for 'aws_checksums_ep'
 [ 25%] No patch step for 'aws_checksums_ep'
 [ 26%] Performing configure step for 'aws_checksums_ep'
  aws_checksums_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_checksums_ep-prefix/src/aws_checksums_ep-stamp/aws_checksums_ep-configure-*.log
 [ 27%] Performing build step for 'aws_checksums_ep'
  aws_checksums_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_checksums_ep-prefix/src/aws_checksums_ep-stamp/aws_checksums_ep-build-*.log
 [ 28%] Performing install step for 'aws_checksums_ep'
  aws_checksums_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_checksums_ep-prefix/src/aws_checksums_ep-stamp/aws_checksums_ep-install-*.log
 [ 29%] Completed 'aws_checksums_ep'
 [ 29%] Built target aws_checksums_ep
 Scanning dependencies of target zlib_ep
 [ 30%] Creating directories for 'zlib_ep'
 [ 30%] Performing download step (download, verify and extract) for 'zlib_ep'
  zlib_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/zlib_ep-prefix/src/zlib_ep-stamp/zlib_ep-download-*.log
 [ 31%] No update step for 'zlib_ep'
 [ 32%] No patch step for 'zlib_ep'
 [ 33%] Performing configure step for 'zlib_ep'
  zlib_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/zlib_ep-prefix/src/zlib_ep-stamp/zlib_ep-configure-*.log
 [ 34%] Performing build step for 'zlib_ep'
  zlib_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/zlib_ep-prefix/src/zlib_ep-stamp/zlib_ep-build-*.log
 [ 35%] Performing install step for 'zlib_ep'
  zlib_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/zlib_ep-prefix/src/zlib_ep-stamp/zlib_ep-install-*.log
 [ 36%] Completed 'zlib_ep'
 [ 36%] Built target zlib_ep
 Scanning dependencies of target mimalloc_ep
 [ 37%] Creating directories for 'mimalloc_ep'
 [ 37%] Performing download step (download, verify and extract) for 'mimalloc_ep'
  Downloading...
 dst='/tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep-prefix/src/v1.6.4.tar.gz'
 timeout='none'
 inactivity timeout='none'
  Using src='https://github.com/microsoft/mimalloc/archive/v1.6.4.tar.gz'
  [download 100% complete]
  [download 1% complete]
  [download 6% complete]
  [download 14% complete]
  [download 30% complete]
  [download 52% complete]
  [download 64% complete]
  [download 85% complete]
  [download 100% complete]
  Downloading... done
  extracting...
 src='/tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep-prefix/src/v1.6.4.tar.gz'
 dst='/tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep-prefix/src/mimalloc_ep'
  extracting... [tar xfz]
  extracting... [analysis]
  extracting... [rename]
  extracting... [clean up]
  extracting... done
 [ 38%] No update step for 'mimalloc_ep'
 [ 39%] No patch step for 'mimalloc_ep'
 [ 40%] Performing configure step for 'mimalloc_ep'
  The C compiler identification is GNU 7.5.0
  The CXX compiler identification is GNU 7.5.0
  Detecting C compiler ABI info
  Detecting C compiler ABI info - done
  Check for working C compiler: /usr/bin/gcc - skipped
  Detecting C compile features
  Detecting C compile features - done
  Detecting CXX compiler ABI info
  Detecting CXX compiler ABI info - done
  Check for working CXX compiler: /usr/bin/g++ - skipped
  Detecting CXX compile features
  Detecting CXX compile features - done
  
  Library base name: mimalloc
  Build type : release
  Install directory: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6
  Build targets : static
  
  Configuring done
  Generating done
 CMake Warning:
 Manually-specified variables were not used by the project:

CMAKE_EXPORT_NO_PACKAGE_REGISTRY
 CMAKE_FIND_PACKAGE_NO_PACKAGE_REGISTRY

 Build files have been written to: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep-prefix/src/mimalloc_ep-build
 [ 40%] Performing build step for 'mimalloc_ep'
 Scanning dependencies of target mimalloc-static
 [ 7%] Building C object CMakeFiles/mimalloc-static.dir/src/stats.c.o
 [ 14%] Building C object CMakeFiles/mimalloc-static.dir/src/random.c.o
 [ 21%] Building C object CMakeFiles/mimalloc-static.dir/src/os.c.o
 [ 28%] Building C object CMakeFiles/mimalloc-static.dir/src/arena.c.o
 [ 35%] Building C object CMakeFiles/mimalloc-static.dir/src/region.c.o
 [ 42%] Building C object CMakeFiles/mimalloc-static.dir/src/segment.c.o
 [ 50%] Building C object CMakeFiles/mimalloc-static.dir/src/page.c.o
 [ 57%] Building C object CMakeFiles/mimalloc-static.dir/src/alloc.c.o
 [ 64%] Building C object CMakeFiles/mimalloc-static.dir/src/alloc-aligned.c.o
 [ 71%] Building C object CMakeFiles/mimalloc-static.dir/src/alloc-posix.c.o
 [ 78%] Building C object CMakeFiles/mimalloc-static.dir/src/heap.c.o
 [ 85%] Building C object CMakeFiles/mimalloc-static.dir/src/options.c.o
 [ 92%] Building C object CMakeFiles/mimalloc-static.dir/src/init.c.o
 [100%] Linking C static library libmimalloc.a
 [100%] Built target mimalloc-static
 [ 41%] Performing install step for 'mimalloc_ep'
 [100%] Built target mimalloc-static
 Install the project...
  Install configuration: ""RELEASE""
  Installing: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/libmimalloc.a
  Installing: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/include/mimalloc.h
  Installing: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/include/mimalloc-override.h
  Installing: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/include/mimalloc-new-delete.h
  Installing: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/cmake/mimalloc-config.cmake
  Installing: /tmp/RtmpOXZGhl/file1568178b21a6/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/cmake/mimalloc-config-version.cmake
 [ 42%] Completed 'mimalloc_ep'
 [ 42%] Built target mimalloc_ep
 Scanning dependencies of target re2_ep
 [ 43%] Creating directories for 're2_ep'
 [ 44%] Performing download step (download, verify and extract) for 're2_ep'
  re2_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/re2_ep-prefix/src/re2_ep-stamp/re2_ep-download-*.log
 [ 45%] No update step for 're2_ep'
 [ 46%] No patch step for 're2_ep'
 [ 46%] Performing configure step for 're2_ep'
  re2_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/re2_ep-prefix/src/re2_ep-stamp/re2_ep-configure-*.log
 [ 47%] Performing build step for 're2_ep'
  jemalloc_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-build-*.log
 [ 48%] Performing install step for 'jemalloc_ep'
  jemalloc_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-install-*.log
 [ 49%] Completed 'jemalloc_ep'
 [ 49%] Built target jemalloc_ep
 Scanning dependencies of target arrow_dataset_objlib
 [ 50%] Building CXX object src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/Unity/unity_1_cxx.cxx.o
 [ 51%] Building CXX object src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/Unity/unity_0_cxx.cxx.o
  re2_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/re2_ep-prefix/src/re2_ep-stamp/re2_ep-build-*.log
 [ 51%] Performing install step for 're2_ep'
  re2_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/re2_ep-prefix/src/re2_ep-stamp/re2_ep-install-*.log
 [ 52%] Completed 're2_ep'
 [ 52%] Built target re2_ep
 Scanning dependencies of target thrift_ep
 [ 52%] Creating directories for 'thrift_ep'
 [ 53%] Performing download step (download, verify and extract) for 'thrift_ep'
  thrift_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-download-*.log
 [ 54%] No update step for 'thrift_ep'
 [ 55%] No patch step for 'thrift_ep'
 [ 56%] Performing configure step for 'thrift_ep'
 [ 56%] Built target arrow_dataset_objlib
 Scanning dependencies of target aws_c_event_stream_ep
 [ 57%] Creating directories for 'aws_c_event_stream_ep'
 [ 57%] Performing download step (download, verify and extract) for 'aws_c_event_stream_ep'
  aws_c_event_stream_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_event_stream_ep-prefix/src/aws_c_event_stream_ep-stamp/aws_c_event_stream_ep-download-*.log
 [ 58%] No update step for 'aws_c_event_stream_ep'
 [ 59%] No patch step for 'aws_c_event_stream_ep'
 [ 60%] Performing configure step for 'aws_c_event_stream_ep'
  aws_c_event_stream_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_event_stream_ep-prefix/src/aws_c_event_stream_ep-stamp/aws_c_event_stream_ep-configure-*.log
 [ 60%] Performing build step for 'aws_c_event_stream_ep'
  aws_c_event_stream_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_event_stream_ep-prefix/src/aws_c_event_stream_ep-stamp/aws_c_event_stream_ep-build-*.log
 [ 61%] Performing install step for 'aws_c_event_stream_ep'
  aws_c_event_stream_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/aws_c_event_stream_ep-prefix/src/aws_c_event_stream_ep-stamp/aws_c_event_stream_ep-install-*.log
 [ 62%] Completed 'aws_c_event_stream_ep'
 [ 62%] Built target aws_c_event_stream_ep
 Scanning dependencies of target awssdk_ep
 [ 63%] Creating directories for 'awssdk_ep'
 [ 64%] Performing download step (download, verify and extract) for 'awssdk_ep'
  thrift_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-configure-*.log
 [ 64%] Performing build step for 'thrift_ep'
  awssdk_ep download command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/awssdk_ep-prefix/src/awssdk_ep-stamp/awssdk_ep-download-*.log
 [ 65%] No update step for 'awssdk_ep'
 [ 65%] No patch step for 'awssdk_ep'
 [ 66%] Performing configure step for 'awssdk_ep'
  awssdk_ep configure command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/awssdk_ep-prefix/src/awssdk_ep-stamp/awssdk_ep-configure-*.log
 [ 67%] Performing build step for 'awssdk_ep'
  thrift_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-build-*.log
 [ 68%] Performing install step for 'thrift_ep'
  thrift_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-install-*.log
 [ 69%] Completed 'thrift_ep'
 [ 69%] Built target thrift_ep
  awssdk_ep build command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/awssdk_ep-prefix/src/awssdk_ep-stamp/awssdk_ep-build-*.log
 [ 70%] Performing install step for 'awssdk_ep'
  awssdk_ep install command succeeded. See also /tmp/RtmpOXZGhl/file1568178b21a6/awssdk_ep-prefix/src/awssdk_ep-stamp/awssdk_ep-install-*.log
 [ 70%] Completed 'awssdk_ep'
 [ 70%] Built target awssdk_ep
 Scanning dependencies of target toolchain
 Scanning dependencies of target arrow_bundled_dependencies
 [ 70%] Built target toolchain
 [ 70%] Bundling /tmp/RtmpOXZGhl/file1568178b21a6/release/libarrow_bundled_dependencies.a
 Scanning dependencies of target arrow_dependencies
 /usr/bin/ar: /tmp/RtmpOXZGhl/file1568178b21a6/jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a: No such file or directory
 make[2]: *** [src/arrow/CMakeFiles/arrow_bundled_dependencies.dir/build.make:80: release/libarrow_bundled_dependencies.a] Error 9
 make[1]: *** [CMakeFiles/Makefile2:1388: src/arrow/CMakeFiles/arrow_bundled_dependencies.dir/all] Error 2
 make[1]: *** Waiting for unfinished jobs....
 [ 70%] Built target arrow_dependencies
 gmake: *** [Makefile:160: all] Error 2
 + popd
 /tmp/RtmppXbaGR/R.INSTALL155322509007/arrow
 ------------------------- NOTE ---------------------------
 See [https://arrow.apache.org/docs/r/articles/install.html]
 for help installing Arrow C++ libraries
 ---------------------------------------------------------
 * 
 ** libs
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c array.cpp -o array.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c array_from_vector.cpp -o array_from_vector.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c array_to_vector.cpp -o array_to_vector.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c arraydata.cpp -o arraydata.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c arrowExports.cpp -o arrowExports.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c buffer.cpp -o buffer.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c chunkedarray.cpp -o chunkedarray.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c compression.cpp -o compression.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c compute.cpp -o compute.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c csv.cpp -o csv.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c dataset.cpp -o dataset.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c datatype.cpp -o datatype.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c expression.cpp -o expression.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c feather.cpp -o feather.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c field.cpp -o field.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c filesystem.cpp -o filesystem.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c imports.cpp -o imports.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c io.cpp -o io.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c json.cpp -o json.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c memorypool.cpp -o memorypool.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c message.cpp -o message.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c parquet.cpp -o parquet.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c py-to-r.cpp -o py-to-r.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c recordbatch.cpp -o recordbatch.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c recordbatchreader.cpp -o recordbatchreader.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c recordbatchwriter.cpp -o recordbatchwriter.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c scalar.cpp -o scalar.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c schema.cpp -o schema.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c symbols.cpp -o symbols.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c table.cpp -o table.o
 g++ -std=gnu++11 -I""/usr/lib64/R/include"" -DNDEBUG -I'/data2/bers/opt/R/4.0/library/cpp11/include' -I/usr/local/include -fpic -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector-strong -funwind-tables -fasynchronous-unwind-tables -fstack-clash-protection -c threadpool.cpp -o threadpool.o
 g++ -std=gnu++11 -shared -L/usr/lib64/R/lib -L/usr/local/lib64 -o arrow.so array.o array_from_vector.o array_to_vector.o arraydata.o arrowExports.o buffer.o chunkedarray.o compression.o compute.o csv.o dataset.o datatype.o expression.o feather.o field.o filesystem.o imports.o io.o json.o memorypool.o message.o parquet.o py-to-r.o recordbatch.o recordbatchreader.o recordbatchwriter.o scalar.o schema.o symbols.o table.o threadpool.o -L/usr/lib64/R/lib -lR
 installing to /data2/bers/opt/R/4.0/library/00LOCK-arrow/00new/arrow/libs
 ** R
 ** inst
 ** byte-compile and prepare package for lazy loading
 ** help
 *** installing help indices
 ** building package indices
 ** installing vignettes
 ** testing if installed package can be loaded from temporary location
 ** checking absolute paths in shared objects and dynamic libraries
 ** testing if installed package can be loaded from final location
 ** testing if installed package keeps a record of temporary installation path
 * DONE (arrow)
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2021-04-22 07:22:46,1
13374245,[C++][Dataset] Consolidate similar tests for file formats,"Between CSV/Parquet/IPC we have a number of very similar or in some cases essentially identical tests. As we're doing more refactoring and development it would be nice to consolidate these tests so that we can ensure all formats behave consistently and get the same level of testing. For instance, ARROW-11772 now adds more comprehensive tests for scanning IPC which don't yet apply to Parquet/CSV.

This sort of consolidation may also be nice to do in Python.",dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-21 21:43:16,0
13374152,[C++][Dataset] Ensure Scanner tests fully cover async,"Some of the tests for scanners don't fully cover the async scanner as they scan a single fragment, which isn't supported by AsyncScanner.",dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-21 14:31:42,0
13374145,[C++][Python] NumPy buffer sets is_mutable_ to true but does not set mutable_data_ when the NumPy array is writable,"Bug is evident

{code}
NumPyBuffer::NumPyBuffer(PyObject* ao) : Buffer(nullptr, 0) {
  PyAcquireGIL lock;
  arr_ = ao;
  Py_INCREF(ao);

  if (PyArray_Check(ao)) {
    PyArrayObject* ndarray = reinterpret_cast<PyArrayObject*>(ao);
    data_ = reinterpret_cast<const uint8_t*>(PyArray_DATA(ndarray));
    size_ = PyArray_SIZE(ndarray) * PyArray_DESCR(ndarray)->elsize;
    capacity_ = size_;

    if (PyArray_FLAGS(ndarray) & NPY_ARRAY_WRITEABLE) {
      is_mutable_ = true;
    }
  }
}
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-04-21 13:58:41,2
13374143,[C++] ORC adapter fails to compile on GCC 4.8 ,Centos 7 packaging build failed during the release https://github.com/ursacomputing/crossbow/runs/2400255864,pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-21 13:57:49,3
13373935,[C++][Dataset] ScanBatches() hangs if there's an error during scanning,"Errors during scanning aren't properly reported, causing the iterator to hang forever.

This affects ScanBatches() and anything built on top of it (Python to_batches, TakeRows, etc)

Verified on the 4.0.0 RC",dataset datasets pull-request-available,['C++'],ARROW,Bug,Blocker,2021-04-20 19:58:54,0
13373911,[Doc][Python] Mention CSVStreamingReader pitfalls with type inference,"Looks like Arrow infer type for the first batch and apply it for all subsequent batches. But information might be not enough to infer the type correctly for the whole file. For our particular case, Arrow infers some field in the schema as date32 from the first batch but the next batch has an empty field value that cant be converted to date32.

When I increase the batch size to have such a value in the first batch Arrow set string type (not sure why not nullable date32) for such a field since it cant be converted to date32 and the whole file is read successfully.

This problem can be easily reproduced by using the following code and attached dataset:
{code:java}
import pyarrow as pa
import pyarrow._csv as pa_csv
import pyarrow._fs as pa_fs

read_options: pa_csv.ReadOptions = pa_csv.ReadOptions(block_size=5_000_000)
parse_options: pa_csv.ParseOptions = pa_csv.ParseOptions(newlines_in_values=True)
convert_options: pa_csv.ConvertOptions = pa_csv.ConvertOptions(timestamp_parsers=[''])
with pa_fs.LocalFileSystem().open_input_file(""dataset.csv"") as file:
 reader = pa_csv.open_csv(
 file, read_options=read_options, parse_options=parse_options, convert_options=convert_options
 )
 for batch in reader:
 table_batch = pa.Table.from_batches([batch])
 table_batch
{code}
Error message:
{code:java}
 for batch in reader:
 File ""pyarrow/ipc.pxi"", line 497, in __iter__
 File ""pyarrow/ipc.pxi"", line 531, in pyarrow.lib.RecordBatchReader.read_next_batch
 File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
 pyarrow.lib.ArrowInvalid: In CSV column #23: CSV conversion error to date32[day]: invalid value ''
{code}

 When we useblock_size `10_000_000` file can be read successfully since we have the problematic value in the first batch.

An error occurs when I try to attach dataset, so you can download it from Google Drive[here|https://drive.google.com/file/d/1Vt1yN02dyVumsou_kFs7GTnKT46eE6ja/view?usp=sharing]",CSVParser CSVReader pull-request-available,"['Documentation', 'Python']",ARROW,Bug,Major,2021-04-20 17:14:46,2
13373622,[C++][Gandiva] Add support for LLVM12,MacOS CI builds have started failing recently as Homebrew has updated LLVM to version 12.,pull-request-available,['C++ - Gandiva'],ARROW,Improvement,Major,2021-04-19 13:33:49,3
13373444,[RUST] [CI] Remove Rust and point integration tests to arrow-rs repo,"Goals:

* Make integration tests run against arrow-rs@master
* Remove Rust from apache/arrow

Tasks:

* Remove Rust from CI
* Remove rust/
* git clone apache/arrow-rs@master in integration tests
* remove rust from Archery Lint
* remove rust from PR labeler
* remove rust from detect_changes.py

",pull-request-available,['Rust'],ARROW,Task,Major,2021-04-19 03:50:18,9
13373401,[CI] Set job timeouts on GitHub Actions,"The default timeout for a single job in Github Actions is 6 hours:

[https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#jobsjob_idtimeout-minutes]

While our jobs normally do not exceed 1 hour of runtime (and most of them are far quicker), sometimes some network issues may lead a job to take up the full 6 hours before timing out. Not only does this contribute to our own build queue growing unnecessarily, but it also impedes other Apache projects, since the number of jobs which can be run in parallel is capped at the organization level.

We should therefore configure job timeouts which reflect our expectation of the overall runtime for each job. 1 hour should be a safe value for most of them, and would already dramatically reduce the impact of network issues.",pull-request-available,['Continuous Integration'],ARROW,Task,Critical,2021-04-18 15:45:14,2
13373389,"[Release] Various packaging, release script and release verification script fixes",Fixes for issues surfaced during the preparation of 4.0.0-RC0,pull-request-available,"['Developer Tools', 'Packaging']",ARROW,Bug,Major,2021-04-18 13:54:32,3
13373250,[Rust] [Ballista] Ballista plans must not include RepartitionExec,Ballista plans must not include RepartitionExec because it results in incorrect results. Ballista needs to manage its own repartitioning in a distributed-aware way later on. For now we just need to configure the DataFusion context to disable repartition.,pull-request-available,['Rust - Ballista'],ARROW,Bug,Major,2021-04-17 22:02:09,10
13373239,[Rust] [Ballista] Show executed plans with metrics,Show executed plans with metrics to help with debugging and performance tuning,pull-request-available,['Rust - Ballista'],ARROW,New Feature,Major,2021-04-17 16:47:23,10
13373231,[Rust] Builds failing due to new flatbuffer release introducing const generics,I filed [https://github.com/google/flatbuffers/issues/6572] but for now we should pin the dependency to 0.8.3,pull-request-available,['Rust'],ARROW,Bug,Blocker,2021-04-17 16:04:32,10
13373229,[Rust] [DataFusion] Add metrics for SortExec,Add metrics for SortExec,pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2021-04-17 15:37:23,10
13373123,[C++] MergedGeneratorTestFixture is incorrectly instantiated,"[https://gist.github.com/kou/868eaed328b348e45865747044044272#file-source-cpp-txt]

Looks like the base class was accidentally instantiated instead of the actual test",pull-request-available,['C++'],ARROW,Bug,Major,2021-04-16 22:39:35,0
13373100,[Python] pyarrow.parquet.read_* should use pre_buffer=True,"If the user is synchronously reading a single file, we should try to read it as fast as possible. The one sticking point might be whether it's beneficial to enable this no matter the filesystem or whether we should try to only enable it on high-latency filesystems.",pull-request-available,['Python'],ARROW,Improvement,Major,2021-04-16 18:08:00,0
13373010,[Rust] [DataFusion] topk_query test fails in master,"{code:java}
     Running target/debug/deps/user_defined_plan-6b63acb904117235running 3 tests
test topk_plan ... ok
test topk_query ... FAILED
test normal_query ... okfailures:---- topk_query stdout ----
thread 'topk_query' panicked at 'assertion failed: `(left == right)`
  left: `[""+-------------+---------+"", ""| customer_id | revenue |"", ""+-------------+---------+"", ""| paul        | 300     |"", ""| jorge       | 200     |"", ""| andy        | 150     |"", ""+-------------+---------+""]`,
 right: `[""++"", ""||"", ""++"", ""++""]`: output mismatch for Topk context. Expectedn
+-------------+---------+
| customer_id | revenue |
+-------------+---------+
| paul        | 300     |
| jorge       | 200     |
| andy        | 150     |
+-------------+---------+Actual:
++
||
++
++
', datafusion/tests/user_defined_plan.rs:133:5
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
 {code}",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-04-16 13:03:23,10
13373009,[C++/Dataset] Reading null columns as dictionary not longer possible,"Reading a dataset with a dictionary column where some of the files don't contain any data for that column (and thus are typed as null) broke with https://github.com/apache/arrow/pull/9532. It worked with the 3.0 release though and thus I would consider this a regression.

This can be reproduced using the following Python snippet:

{code}
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds

table = pa.table({""a"": [None, None]})
pq.write_table(table, ""test.parquet"")
schema = pa.schema([pa.field(""a"", pa.dictionary(pa.int32(), pa.string()))])
fsds = ds.FileSystemDataset.from_paths(
    paths=[""test.parquet""],
    schema=schema,
    format=pa.dataset.ParquetFileFormat(),
    filesystem=pa.fs.LocalFileSystem(),
)
fsds.to_table()
{code}

The exception on master is currently:

{code}
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-14-5f0bc602f16b> in <module>
      6     filesystem=pa.fs.LocalFileSystem(),
      7 )
----> 8 fsds.to_table()

~/Development/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Dataset.to_table()
    456         table : Table instance
    457         """"""
--> 458         return self._scanner(**kwargs).to_table()
    459 
    460     def head(self, int num_rows, **kwargs):

~/Development/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Scanner.to_table()
   2887             result = self.scanner.ToTable()
   2888 
-> 2889         return pyarrow_wrap_table(GetResultValue(result))
   2890 
   2891     def take(self, object indices):

~/Development/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()
    139 cdef api int pyarrow_internal_check_status(const CStatus& status) \
    140         nogil except -1:
--> 141     return check_status(status)
    142 
    143 

~/Development/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()
    116             raise ArrowKeyError(message)
    117         elif status.IsNotImplemented():
--> 118             raise ArrowNotImplementedError(message)
    119         elif status.IsTypeError():
    120             raise ArrowTypeError(message)

ArrowNotImplementedError: Unsupported cast from null to dictionary<values=string, indices=int32, ordered=0> (no available cast function for target type)
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-16 13:01:16,3
13372712,[Rust] Add Builder interface for adding Arrays to record batches,"
Use case:

While writing tests (both in IOx and in DataFusion) where I need a single `RecordBatch`, I often find myself doing something like this:

```
        let schema = Arc::new(Schema::new(vec![
            ArrowField::new(""float_field"", ArrowDataType::Float64, true),
            ArrowField::new(""time"", ArrowDataType::Int64, true),
        ]));

        let float_array: ArrayRef = Arc::new(Float64Array::from(vec![10.1, 20.1, 30.1, 40.1]));
        let timestamp_array: ArrayRef = Arc::new(Int64Array::from(vec![1000, 2000, 3000, 4000]));

        let batch = RecordBatch::try_new(schema, vec![float_array, timestamp_array])
            .expect(""created new record batch"");
```

This is annoying because the information that `float_field` is a float is encoded both in the Schema and the `Float64Array`

I would much rather rather be able to construct RecordBatches a a builder style to avoid the the redundancy and reduce the amount of typing / redundancy:


```

        let float_array: ArrayRef = Arc::new(Float64Array::from(vec![10.1, 20.1, 30.1, 40.1]));
        let timestamp_array: ArrayRef = Arc::new(Int64Array::from(vec![1000, 2000, 3000, 4000]));

        let batch = RecordBatch::empty()
          .append(""float_field"", timestamp_array).unwrap()
          .append(""time"", float_array).unwrap;

```

The proposal is to add a method to `RecordBatch` like

```
impl RecordBatch {
...
  fn append(self, field_name: &str, field_values: ArrayRef) -> Result<Self>
}
```

That would append the a field name to the current schema, returning an error if field_name was already present.

The nullability of the field would be set based on the actual null count of the field_values
",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-04-15 18:08:03,11
13372665,[R] Delete Scan() bindings,The deprecation suppression causes an R CMD check failure,pull-request-available,['R'],ARROW,Bug,Blocker,2021-04-15 14:50:13,0
13372663,[Python] Deprecation warning when building PyArrow,"{code}
/home/antoine/arrow/dev/python/build/temp.linux-x86_64-3.7/_dataset.cpp:40723:115: warning: 'Scan' is deprecated: Deprecated in 4.0.0 for removal in 5.0.0. Use ScanBatches(). [-Wdeprecated-declarations]
  __pyx_t_1 = arrow::py::GetResultValue<arrow::dataset::ScanTaskIterator>(__pyx_cur_scope->__pyx_v_self->scanner->Scan()); if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 2817, __pyx_L1_error)
                                                                                                                  ^
/home/antoine/miniconda3/envs/pyarrow/include/arrow/dataset/scanner.h:259:3: note: 'Scan' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Deprecated in 4.0.0 for removal in 5.0.0. Use ScanBatches()."")
  ^
/home/antoine/miniconda3/envs/pyarrow/include/arrow/util/macros.h:107:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
{code}
",pull-request-available,['Python'],ARROW,Bug,Trivial,2021-04-15 14:46:36,0
13372645,[Rust] [DataFusion] Implement SQL metrics framework,"As a user, I would like the ability to inspect metrics for an executed plan to help with debugging and performance tuning.",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2021-04-15 13:32:56,10
13372586,[Python][Docs] Clarify serialization docstrings about deprecated status,"Currently the docstring itself says nothing about it, there is only the warning.",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2021-04-15 09:18:20,5
13372443,[C++][CI] Conda nightly jobs fail due to not bundling xsimd,"We don't bundle xsimd if the compile-time SIMD level is NONE, but we still try to build code that uses xsimd if the runtime SIMD level is not NONE. The bundle check needs to account for both. SeeARROW-11993.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2021-04-14 16:05:10,0
13372344,[C++][CI] Thread sanitizer failure in SerialExecutor,"Another ThreadSanitizer failure was found possibly resulting from ARROW-12208

Log: [https://github.com/ursacomputing/crossbow/runs/2340248774]",pull-request-available,['C++'],ARROW,Bug,Major,2021-04-14 14:17:35,2
13372297,[Dev] archery trigger-bot should use logger.exception,"[https://github.com/apache/arrow/blob/9c85e5465a5738f5ba9a2455d1b566948f89d0f3/dev/archery/archery/bot.py#L145-L150]

We should use {{logger.exception}} not {{logger.error}} to get the full traceback, else it's harder to tell what happened.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-04-14 12:38:32,0
13372293,[Release] Remove rebase post-release scripts,"We're going to release from a maintenance branch from now on, so we won't rebase neither the master branch nor the pull requests again.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-04-14 12:28:44,3
13372224,[CI][C++][cron] Use Ubuntu 20.04 instead of 16.04,"Because we dropped support for Ubuntu 16.04.
",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2021-04-14 07:28:04,1
13372067,[C++] Stop producing when PushGenerator was destroyed,"When the consuming-facing {{PushGenerator}} is lost, the {{Producer}} should expose that information so that any background-running producing code can abort.

Perhaps this may be combined with StopToken...",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-13 14:45:37,2
13372043,[Rust] [DataFusion] Allow users to override physical optimization rules,As a user of DataFusion (in Ballista) I would override the list of physical optimization rules. It is currently possible to add new rules but not to remove existing rules.,pull-request-available,['Rust - Ballista'],ARROW,Improvement,Major,2021-04-13 13:12:04,10
13372039,[Dev] Unify source code formatting of Archery ,"Using black, isort and flake8.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-04-13 12:50:56,3
13372017,"[Archery] Error running ""crossbow submit ...""","{code:python}
$ archery crossbow submit -g cpp
/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.4) or chardet (3.0.4) doesn't match a supported version!
  RequestsDependencyWarning)
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/bin/archery"", line 33, in <module>
    sys.exit(load_entry_point('archery', 'console_scripts', 'archery')())
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/click/decorators.py"", line 27, in new_func
    return f(get_current_context().obj, *args, **kwargs)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/cli.py"", line 113, in submit
    config = Config.load_yaml(config_path)
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/core.py"", line 1060, in load_yaml
    params={})
  File ""/home/antoine/arrow/dev/dev/archery/archery/crossbow/core.py"", line 122, in _render_jinja_template
    loader = jinja2.FileSystemLoader(searchpath)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/jinja2/loaders.py"", line 163, in __init__
    self.searchpath = list(searchpath)
TypeError: 'PosixPath' object is not iterable
{code}
",pull-request-available,['Archery'],ARROW,Bug,Major,2021-04-13 11:25:29,2
13371908,[Packaging][deb] Rename -archive-keyring to -apt-source,"Because lintian recommends that a package that puts files to
/etc/apt/sources.list.d/ uses -apt-source suffix.

See also: https://lintian.debian.net/tags/package-installs-apt-sources

This also changes repository URL to
https://apache.jfrog.io/artifactory/ from https://apache.bintray.com/ .",pull-request-available,['Packaging'],ARROW,Improvement,Major,2021-04-13 01:32:29,1
13371888,[CI][R][Windows] Remove needless workaround for MSYS2,"repo.msys2.org is alive. sf.net may be fragile than repo.msys2.org.

See also ARROW-10202.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2021-04-12 21:32:14,1
13371886,[CI][Ruby] Use ruby/setup-ruby instead of actions/setup-ruby,"Because actions/setup-ruby is deprecated:

{quote}
Please note: This action is deprecated and should no longer be used. The team at GitHub has ceased making and accepting code contributions or maintaining issues tracker. Please, migrate your workflows to the ruby/setup-ruby, which is being actively maintained by the official Ruby organization.
{quote}

https://github.com/actions/setup-ruby#setup-ruby",pull-request-available,"['Continuous Integration', 'Ruby']",ARROW,Improvement,Major,2021-04-12 21:26:47,1
13371842,[Packaging] Fix tabulation in crossbow templates for submitting nightly builds,We upload gemfury artifacts from the nightly builds only checking arrow's branch we submit the builds against. The jinja macro produced wrong yml configurations.,pull-request-available,['Packaging'],ARROW,Bug,Major,2021-04-12 16:47:04,3
13371160,[Rust] [Ballista] Bump DataFusion version,Update Ballista to use latest DataFusion version,pull-request-available,['Rust - Ballista'],ARROW,Task,Major,2021-04-11 22:23:12,10
13371158,[Rust] [Ballista] Aggregate queries producing incorrect results,"I just ran benchmarks for the first time in a while and I see duplicate entries for group by keys.



For example, query 1 has ""group by l_returnflag, l_linestatus"" and I see multiple results with l_returnflag = 'A' and l_linestatus = 'F'.",pull-request-available,['Rust - Ballista'],ARROW,Bug,Major,2021-04-11 21:48:14,10
13370822,[Rust] [Ballista] Add README,We did not bring a README over in the donation and need to write a new one anyway now this is part of Arrow,pull-request-available,['Rust - Ballista'],ARROW,Task,Major,2021-04-10 14:32:19,10
13370775,[C++] Avoid needless c-ares detection,"If we use system gRPC, we don't need to detect c-ares explicitly.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-10 04:39:22,1
13370710,[C++] Switch default memory allocator from jemalloc to mimalloc on macOS,"Benchmarking shows that mimalloc seems to be faster on real workflows (at least on macOS, still collecting data on Ubuntu). We could switch the default memory pool cases so that mimalloc is preferred. 

cc [~jonkeane] [~apitrou]",pull-request-available,['C++'],ARROW,New Feature,Major,2021-04-09 17:35:50,2
13370682,[Python] pq.read_pandas with use_legacy_dataset=False does not accept columns as a set (kartothek integration failure),"The kartothek nightly integration builds started to fail(https://github.com/ursacomputing/crossbow/runs/2303373464), I assume because of ARROW-11464 (https://github.com/apache/arrow/pull/9910).

It seems that in the new ParquetDatasetV2 (what you get with {{use_legacy_dataset=False}}), the handling of the columns argument is slightly different.


Example failure:

{code}
_____________________ test_add_column_to_existing_index[4] _____________________

store_factory = functools.partial(<function get_store_from_url at 0x7faf12e9d0e0>, 'hfs:///tmp/pytest-of-root/pytest-0/test_add_column_to_existing_in1/store')
metadata_version = 4
bound_build_dataset_indices = <function build_dataset_indices at 0x7faf0c509830>

    def test_add_column_to_existing_index(
        store_factory, metadata_version, bound_build_dataset_indices
    ):
        dataset_uuid = ""dataset_uuid""
        partitions = [
            pd.DataFrame({""p"": [1, 2], ""x"": [100, 4500]}),
            pd.DataFrame({""p"": [4, 3], ""x"": [500, 10]}),
        ]
    
        dataset = store_dataframes_as_dataset(
            dfs=partitions,
            store=store_factory,
            dataset_uuid=dataset_uuid,
            metadata_version=metadata_version,
            secondary_indices=""p"",
        )
        assert dataset.load_all_indices(store=store_factory()).indices.keys() == {""p""}
    
        # Create indices
>       bound_build_dataset_indices(store_factory, dataset_uuid, columns=[""x""])

kartothek/io/testing/index.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/envs/arrow/lib/python3.7/site-packages/decorator.py:231: in fun
    return caller(func, *(extras + args), **kw)
kartothek/io_components/utils.py:277: in normalize_args
    return _wrapper(*args, **kwargs)
kartothek/io_components/utils.py:275: in _wrapper
    return function(*args, **kwargs)
kartothek/io/eager.py:706: in build_dataset_indices
    mp = mp.load_dataframes(store=ds_factory.store, columns=cols_to_load,)
kartothek/io_components/metapartition.py:150: in _impl
    method_return = method(mp, *method_args, **method_kwargs)
kartothek/io_components/metapartition.py:696: in load_dataframes
    date_as_object=dates_as_object,
kartothek/serialization/_generic.py:122: in restore_dataframe
    date_as_object=date_as_object,
kartothek/serialization/_parquet.py:302: in restore_dataframe
    date_as_object=date_as_object,
kartothek/serialization/_parquet.py:249: in _restore_dataframe
    table = pq.read_pandas(reader, columns=columns)
/opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/parquet.py:1768: in read_pandas
    source, columns=columns, use_pandas_metadata=True, **kwargs
/opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/parquet.py:1730: in read_table
    use_pandas_metadata=use_pandas_metadata)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyarrow.parquet._ParquetDatasetV2 object at 0x7faee1ed9550>
columns = {'x'}, use_threads = True, use_pandas_metadata = True

    def read(self, columns=None, use_threads=True, use_pandas_metadata=False):
        """"""
        Read (multiple) Parquet files as a single pyarrow.Table.
    
        Parameters
        ----------
        columns : List[str]
            Names of columns to read from the dataset. The partition fields
            are not automatically included (in contrast to when setting
            ``use_legacy_dataset=True``).
        use_threads : bool, default True
            Perform multi-threaded column reads.
        use_pandas_metadata : bool, default False
            If True and file has custom pandas schema metadata, ensure that
            index columns are also loaded.
    
        Returns
        -------
        pyarrow.Table
            Content of the file as a table (of columns).
        """"""
        # if use_pandas_metadata, we need to include index columns in the
        # column selection, to be able to restore those in the pandas DataFrame
        metadata = self.schema.metadata
        if columns is not None and use_pandas_metadata:
            if metadata and b'pandas' in metadata:
                # RangeIndex can be represented as dict instead of column name
                index_columns = [
                    col for col in _get_pandas_index_columns(metadata)
                    if not isinstance(col, dict)
                ]
>               columns = columns + list(set(index_columns) - set(columns))
E               TypeError: unsupported operand type(s) for +: 'set' and 'list'

/opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/parquet.py:1598: TypeError
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2021-04-09 14:26:22,5
13370672,[Rust] [Ballista] Benchmark documentation out of date,The scheduler/executor were refactored and the documentation for the benchmarks now needs updating. I plan on fixing this over the weekend.,pull-request-available,['Rust - Ballista'],ARROW,Bug,Major,2021-04-09 13:47:44,10
13370583,[Java] ValueVector#getObject should support covariance for complex types,"Currently, the {{ValueVector#getObject}} API supports covariance for primitive types.
For example, {{IntVector#getObject}} returns {{Integer}} while {{BitVector#getObject}} returns {{Boolean}}.

For complex types, we should also support covariance. For example, {{ListVector#getObject}} should return a {{List}}

This will help reduce unnecessary casts, and enforce type safety. ",pull-request-available,['Java'],ARROW,Improvement,Major,2021-04-09 07:45:54,7
13370393,[Python] pq.write_to_dataset does not recognize S3FileSystem,"{code:java}
    pq.write_to_dataset(pa.concat_tables(pa_tables),
  File ""C:\venv\*\lib\site-packages\pyarrow\parquet.py"", line 1914, in write_to_dataset
    fs, root_path = legacyfs.resolve_filesystem_and_path(root_path, filesystem)
  File ""C:\venv\*\lib\site-packages\pyarrow\filesystem.py"", line 474, in resolve_filesystem_and_path
    filesystem = _ensure_filesystem(filesystem)
  File ""C:\venv\*\lib\site-packages\pyarrow\filesystem.py"", line 457, in _ensure_filesystem
    raise OSError('Unrecognized filesystem: {}'.format(fs_type))
OSError: Unrecognized filesystem: <class 'pyarrow._s3fs.S3FileSystem'>
{code}

Creating the S3FileSystem these two ways produced the above error when invoking parquet.write_to_dataset with filesystem=s3_filesystem:

{code:java}
    s3_filesystem = file_system.S3FileSystem(region='us-east-1')
    s3_filesystem, path = file_system.FileSystem.from_uri(""s3://{0}"".format(PARQUET_BUCKET))
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2021-04-08 13:34:42,5
13370179,"[Rust][DataFusion]Use Timestamp(Nanosecond, None) for SQL TIMESTAMP Type","# Rationale
Running the query `CREATE EXTERNAL TABLE .. (c TIMESTAMP)` today in DataFusion will result in a data type pf ""Date64"" which means that anything more specific than the date will be ignored.

This leads to strange behavior such as

{code}
echo ""Jorge,2018-12-13T12:12:10.011"" >> /tmp/foo.csv
echo ""Andrew,2018-11-13T17:11:10.011"" > /tmp/foo.csv

cargo run -p datafusion --bin datafusion-cli
    Finished dev [unoptimized + debuginfo] target(s) in 0.23s
     Running `target/debug/datafusion-cli`
> CREATE EXTERNAL TABLE t(a varchar, b TIMESTAMP)
STORED AS CSV
LOCATION '/tmp/foo.csv';

0 rows in set. Query took 0 seconds.
> select * from t;
+--------+------------+
| a      | b          |
+--------+------------+
| Andrew | 2018-11-13 |
| Jorge  | 2018-12-13 |
+--------+------------+
{code}

(note how it is only a date, not a timestamp)",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-04-07 18:26:13,11
13370178,[Rust][DataFusion] Min/Max are not supported for timestamp types,"If you try and aggregate (via MIN, for example) a column of a timestamp type, it generates an error:
```
Coercion from [Timestamp(Nanosecond, None)] to the signature Uniform(1, [Int8, Int16, Int32, Int64, UInt8, UInt16, UInt32, UInt64, Float32, Float64]) failed.
```

For example:

{code}
> show columns from t;
+---------------+--------------+------------+-------------+-----------------------------+-------------+
| table_catalog | table_schema | table_name | column_name | data_type                   | is_nullable |
+---------------+--------------+------------+-------------+-----------------------------+-------------+
| datafusion    | public       | t          | a           | Utf8                        | NO          |
| datafusion    | public       | t          | b           | Timestamp(Nanosecond, None) | NO          |
+---------------+--------------+------------+-------------+-----------------------------+-------------+
2 row in set. Query took 0 seconds.
> select sum(b) from t;
Plan(""Coercion from [Timestamp(Nanosecond, None)] to the signature Uniform(1, [Int8, Int16, Int32, Int64, UInt8, UInt16, UInt32, UInt64, Float32, Float64]) failed."")
{code}
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-04-07 18:23:28,11
13370164,[Rust] JSON writer does not support timestamp types,"Looks like the json writer.rs code in arrow doesn't support writing out timestamps. When I tried to write out a `TimestampNanosecondArray` I got the following error:

```
thread 'influxdb_ioxd::http::tests::test_query_json' panicked at 'Unsupported datatype: Timestamp(
    Nanosecond,
    None,
)', /Users/alamb/.cargo/git/checkouts/arrow-3a9cfebb6b7b2bdc/3e825a7/rust/arrow/src/json/writer.rs:326:13
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
```
",pull-request-available,['Rust'],ARROW,Bug,Major,2021-04-07 17:47:57,11
13370150,[Dev][Packaging] Move Crossbow to Archery,Crossbow should be part of the archery toolset enabling tighter integration and easier installation.,pull-request-available,"['Developer Tools', 'Packaging']",ARROW,Improvement,Major,2021-04-07 16:15:14,3
13370121,[Doc][C++][Python] Docs built and pushed with S3 and Flight disabled,"The {{ubuntu-docs}} image doesn't enable S3 nor Flight when building the C++ and Python docs.

https://arrow.apache.org/docs/python/api/flight.html
https://arrow.apache.org/docs/python/api/filesystems.html
",pull-request-available,"['C++', 'Documentation', 'Python']",ARROW,Bug,Major,2021-04-07 14:07:36,2
13370102,[Rust][DataFusion] Limit keeps polling input after limit is reached,"Once the number of rows needed for a limit query has been produced, any further work done to read values from its input is wasted.

The current implementation of LimitStream will keep polling its input for the next value, and returning Poll::Ready(None) , even once the limit has been reached

This is wasteful
",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-04-07 13:23:43,11
13370098,[Rust] [Ballista] Add Ballista tests to CI,Ballista is a standalone project (not part of the Arrow Rust workspace) and therefore the tests will not run in CI without additional work.,pull-request-available,['Rust - Ballista'],ARROW,Improvement,Major,2021-04-07 13:06:25,10
13370097,[Rust] Failing test arrow::arrow_writer::tests::fixed_size_binary_single_column,"I just pulled latest from master (commit d95c72f7f8e61b90c935ecb4e64d3e77648ef6d5) and updated submodules, then ran `cargo clean` followed by `cargo test`.

One test fails (sometimes). It can fail in multiple ways:
{code:java}
---- arrow::arrow_writer::tests::fixed_size_binary_single_column stdout ----
thread 'arrow::arrow_writer::tests::fixed_size_binary_single_column' panicked at 'called `Result::unwrap()` on an `Err` value: General(""Could not parse metadata: protocol error"")', parquet/src/arrow/arrow_writer.rs:920:54
 {code}
{code:java}
---- arrow::arrow_writer::tests::fixed_size_binary_single_column stdout ----
thread 'arrow::arrow_writer::tests::fixed_size_binary_single_column' panicked at 'Unable to get batch: ParquetError(""Parquet error: underlying Thrift error: end of file"")', parquet/src/arrow/arrow_writer.rs:927:14
 {code}",pull-request-available,['Rust'],ARROW,Bug,Major,2021-04-07 13:01:39,12
13370093,[C++] Allow static builds to change memory allocators ,"When building libarrow with {{ARROW_BUILD_SHARED=OFF}} setting the environment variable {{ARROW_DEFAULT_MEMORY_POOL}} is not respected, we always get the default memory allocator. ",pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-07 12:45:11,2
13369985,[Python][Doc] Tweak nightly build instructions,"Sometimes not all binaries for the latest nightly have been uploaded, and pip will try to use the source tarball instead:
https://github.com/apache/arrow/pull/9285#issuecomment-814672932
",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Minor,2021-04-07 07:49:03,2
13369963,[Python] Parallel csv reader cancellation test kills pytest,"CI job is okay, but it failed from my side. Tested on x86 skylake server with 32 cores, and Apple M1 with 8 cores.

Maybe I missed something?

Test steps:
{code:bash}
$ cmake -GNinja -DCMAKE_BUILD_TYPE=Release -DARROW_COMPUTE=ON -DARROW_PARQUET=ON -DARROW_BUILD_TESTS=ON -DCMAKE_INSTALL_PREFIX=$(pwd)/_install -DCMAKE_INSTALL_LIBDIR=lib -DARROW_PYTHON=ON -DCMAKE_CXX_COMPILER=/usr/bin/clang++-9 -DCMAKE_C_COMPILER=/usr/bin/clang-9 ..
$ ninja install
$ cd ~/arrow/python
# set LD_LIBRARY_PATH, ARROW_HOME to newly built binaries
$ python setup.py build_ext --inplace
$ pytest pyarrow
{code}

Error log:
{code:bash}
======================================================================================= test session starts ========================================================================================
platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1
rootdir: /home/cyb/arrow/python, configfile: setup.cfg
plugins: lazy-fixture-0.6.3, hypothesis-5.41.2
collected 3802 items / 3 skipped / 3799 selected                                                                                                                                                   

pyarrow/tests/test_adhoc_memory_leak.py s                                                                                                                                                    [  0%]
pyarrow/tests/test_array.py ......................s...........................................................................................s............................................. [  4%]
............................ss.........                                                                                                                                                      [  5%]
pyarrow/tests/test_builder.py ....                                                                                                                                                           [  5%]
pyarrow/tests/test_cffi.py ......                                                                                                                                                            [  5%]
pyarrow/tests/test_compute.py ....................................................................................................................................................           [  9%]
pyarrow/tests/test_convert_builtin.py ...................................................................................................................................................... [ 13%]
................................................................................................................................x..................................................x........ [ 18%]
...............ssssss.....................................................................................................sssssss                                                            [ 21%]
pyarrow/tests/test_csv.py ....................................F.......................

============================================================================================= FAILURES =============================================================================================
______________________________________________________________________________ TestParallelCSVRead.test_cancellation _______________________________________________________________________________

self = <pyarrow.tests.test_csv.TestParallelCSVRead testMethod=test_cancellation>

    def test_cancellation(self):
        if (threading.current_thread().ident !=
                threading.main_thread().ident):
            pytest.skip(""test only works from main Python thread"")

        if sys.version_info >= (3, 8):
            raise_signal = signal.raise_signal
        elif os.name == 'nt':
            # On Windows, os.kill() doesn't actually send a signal,
            # it just terminates the process with the given exit code.
            pytest.skip(""test requires Python 3.8+ on Windows"")
        else:
            # On Unix, emulate raise_signal() with os.kill().
            def raise_signal(signum):
                os.kill(os.getpid(), signum)

        large_csv = b""a,b,c\n"" + b""1,2,3\n"" * 30000000

        def signal_from_thread():
            time.sleep(0.2)
            raise_signal(signal.SIGINT)

        t1 = time.time()
        with pytest.raises(KeyboardInterrupt) as exc_info:
            threading.Thread(target=signal_from_thread).start()
>           self.read_bytes(large_csv)
E           Failed: DID NOT RAISE <class 'KeyboardInterrupt'>

pyarrow/tests/test_csv.py:927: Failed
========================================================================================= warnings summary =========================================================================================
../../archery/lib/python3.6/distutils/__init__.py:4
  /home/cyb/archery/lib/python3.6/distutils/__init__.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
    import imp

-- Docs: https://docs.pytest.org/en/stable/warnings.html
===================================================================================== short test summary info ======================================================================================
FAILED pyarrow/tests/test_csv.py::TestParallelCSVRead::test_cancellation - Failed: DID NOT RAISE <class 'KeyboardInterrupt'>
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! KeyboardInterrupt !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
pyarrow/error.pxi:221: KeyboardInterrupt
(to show a full traceback on KeyboardInterrupt use --full-trace)
================================================================= 1 failed, 864 passed, 21 skipped, 2 xfailed, 1 warning in 4.19s ==================================================================
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2021-04-07 06:36:45,2
13369882,[Rust][DataFusion] LIMIT returns incorrect results when used with several small partitions,"I noticed when I was running some queries locally that `LIMIT` was not behaving correctly. For my case, a query with `LIMIT 10` was always returning zero rows. 

I spent some time and I have found a self contained reproducer. If you put the following test in `rust/src/datafusion/execution/context.rs` it will fail.


{code}


    /// Return a RecordBatch with a single Int32 array with values (0..sz)
    fn make_partition(sz: i32) -> RecordBatch {
        let seq_start = 0;
        let seq_end =  sz;
        let values = (seq_start..seq_end).collect::<Vec<_>>();
        let schema = Arc::new(Schema::new(vec![Field::new(""i"", DataType::Int32, true)]));
        let arr = Arc::new(Int32Array::from(values));
        let arr = arr as ArrayRef;

        RecordBatch::try_new(schema.clone(),vec![arr]).unwrap()
    }

    #[tokio::test]
    async fn limit_multi_partitions() -> Result<()> {
        let tmp_dir = TempDir::new()?;
        let mut ctx = create_ctx(&tmp_dir, 1)?;

        let partitions = vec![
            vec![make_partition(0)],
            vec![make_partition(1)],
            vec![make_partition(2)],
            vec![make_partition(3)],
            vec![make_partition(4)],
            vec![make_partition(5)],
        ];
        let schema = partitions[0][0].schema();
        let provider = Arc::new(MemTable::try_new(schema, partitions).unwrap());

        ctx.register_table(""t"", provider)
            .unwrap();

        // select all rows
        let results = plan_and_collect(&mut ctx, ""SELECT i FROM t"")
            .await
            .unwrap();

        let num_rows: usize = results.into_iter().map(|b| b.num_rows()).sum();
        assert_eq!(num_rows, 15);

        for limit in 1..10 {
            let query = format!(""SELECT i FROM t limit {}"", limit);
            let results = plan_and_collect(&mut ctx, &query)
                .await
                .unwrap();

            let num_rows: usize = results.into_iter().map(|b| b.num_rows()).sum();
            assert_eq!(num_rows, limit, ""mismatch with query {}"", query);
        }

        Ok(())
    }

{code}",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-04-06 21:20:37,11
13369827,[C++][Dataset] Separate datasets backed by readers from InMemoryDataset,"From ARROW-10882/[https://github.com/apache/arrow/pull/9802]
 * Backing an InMemoryDataset with a reader is misleading. Let's split that out into a separate class.
 * Dataset scanning can then use an I/O thread for the new class. (Note that for Python, we'll need to be careful to release the GIL before any operations so that the I/O thread can acquire the GIL to call into the underlying Python reader/file object.)
 * Longer-term, we should interface with Python's async.",dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-04-06 15:31:20,0
13369803,[CI] Create base image for conda environments ,Currently all conda based image contains heavy dependencies like llvm which is required for gandiva. We can reduce the image sizes by introducing a base image containing a basic conda installation.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2021-04-06 13:06:47,3
13369796,[C++] ASAN error in s3fs_test.cc,"I get this locally:

{code}
[ RUN      ] TestS3FS.OpenOutputStreamBackgroundWrites
Attempting encryption of all config, IAM users and policies on MinIO backend
=================================================================
==145714==ERROR: AddressSanitizer: heap-use-after-free on address 0x6190001df758 at pc 0x7f4fd4035e5b bp 0x7ffcf2e8cbc0 sp 0x7ffcf2e8cbb8
READ of size 1 at 0x6190001df758 thread T0
    #0 0x7f4fd4035e5a in UploadPart ../src/arrow/filesystem/s3fs.cc:970
    #1 0x7f4fd4034a40 in UploadPart ../src/arrow/filesystem/s3fs.cc:958
    #2 0x7f4fd403474a in CommitCurrentPart ../src/arrow/filesystem/s3fs.cc:954
    #3 0x7f4fd4030899 in Close ../src/arrow/filesystem/s3fs.cc:851
    #4 0x563844f07905 in arrow::fs::TestS3FS::TestOpenOutputStream() ../src/arrow/filesystem/s3fs_test.cc:468
    #5 0x563844ec69e7 in arrow::fs::TestS3FS_OpenOutputStreamBackgroundWrites_Test::TestBody() ../src/arrow/filesystem/s3fs_test.cc:853
    #6 0x7f4fca6eb98d in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4c98d)
    #7 0x7f4fca6ebbe0 in testing::Test::Run() (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4cbe0)
    #8 0x7f4fca6ebf0e in testing::TestInfo::Run() (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4cf0e)
    #9 0x7f4fca6ec035 in testing::TestSuite::Run() (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4d035)
    #10 0x7f4fca6ec5eb in testing::internal::UnitTestImpl::RunAllTests() (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4d5eb)
    #11 0x7f4fca6ec858 in testing::UnitTest::Run() (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4d858)
    #12 0x7f4fca72807e in main (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest_main.so+0x107e)
    #13 0x7f4fc999f0b2 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x270b2)
    #14 0x563844e7c57d in _start (/home/antoine/arrow/dev/cpp/build-test/debug/arrow-s3fs-test+0x22757d)

0x6190001df758 is located 216 bytes inside of 936-byte region [0x6190001df680,0x6190001dfa28)
freed by thread T0 here:
    #0 0x7f4fdeaea78c in operator delete(void*) /home/conda/feedstock_root/build_artifacts/ctng-compilers_1601682258120/work/.build/x86_64-conda-linux-gnu/src/gcc/libsanitizer/asan/asan_new_delete.cc:165
    #1 0x7f4fd40c6ba7 in std::default_delete<arrow::fs::S3FileSystem::Impl>::operator()(arrow::fs::S3FileSystem::Impl*) const /usr/include/c++/9/bits/unique_ptr.h:81
    #2 0x7f4fd40aeeb8 in std::unique_ptr<arrow::fs::S3FileSystem::Impl, std::default_delete<arrow::fs::S3FileSystem::Impl> >::~unique_ptr() /usr/include/c++/9/bits/unique_ptr.h:292
    #3 0x7f4fd40418f0 in arrow::fs::S3FileSystem::~S3FileSystem() ../src/arrow/filesystem/s3fs.cc:1665
    #4 0x7f4fd4041961 in arrow::fs::S3FileSystem::~S3FileSystem() ../src/arrow/filesystem/s3fs.cc:1665
    #5 0x7f4fd4119874 in std::_Sp_counted_ptr<arrow::fs::S3FileSystem*, (__gnu_cxx::_Lock_policy)2>::_M_dispose() /usr/include/c++/9/bits/shared_ptr_base.h:377
    #6 0x563844f34ea9 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() /usr/include/c++/9/bits/shared_ptr_base.h:155
    #7 0x563844f1a19b in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count() /usr/include/c++/9/bits/shared_ptr_base.h:730
    #8 0x563844ef8202 in std::__shared_ptr<arrow::fs::S3FileSystem, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr() /usr/include/c++/9/bits/shared_ptr_base.h:1169
    #9 0x563844f2c02b in std::__shared_ptr<arrow::fs::S3FileSystem, (__gnu_cxx::_Lock_policy)2>::reset() /usr/include/c++/9/bits/shared_ptr_base.h:1287
    #10 0x563844f0725e in arrow::fs::TestS3FS::TestOpenOutputStream() ../src/arrow/filesystem/s3fs_test.cc:466
    #11 0x563844ec69e7 in arrow::fs::TestS3FS_OpenOutputStreamBackgroundWrites_Test::TestBody() ../src/arrow/filesystem/s3fs_test.cc:853
    #12 0x7f4fca6eb98d in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4c98d)

previously allocated by thread T0 here:
    #0 0x7f4fdeae9b5e in operator new(unsigned long) /home/conda/feedstock_root/build_artifacts/ctng-compilers_1601682258120/work/.build/x86_64-conda-linux-gnu/src/gcc/libsanitizer/asan/asan_new_delete.cc:104
    #1 0x7f4fd40414e6 in arrow::fs::S3FileSystem::S3FileSystem(arrow::fs::S3Options const&, arrow::io::IOContext const&) ../src/arrow/filesystem/s3fs.cc:1661
    #2 0x7f4fd4041cf0 in arrow::fs::S3FileSystem::Make(arrow::fs::S3Options const&, arrow::io::IOContext const&) ../src/arrow/filesystem/s3fs.cc:1671
    #3 0x563844efdaf3 in arrow::fs::TestS3FS::MakeFileSystem() ../src/arrow/filesystem/s3fs_test.cc:405
    #4 0x563844efa8a4 in arrow::fs::TestS3FS::SetUp() ../src/arrow/filesystem/s3fs_test.cc:376
    #5 0x7f4fca6eb98d in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so+0x4c98d)

SUMMARY: AddressSanitizer: heap-use-after-free ../src/arrow/filesystem/s3fs.cc:970 in UploadPart
Shadow bytes around the buggy address:
  0x0c3280033e90: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c3280033ea0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c3280033eb0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c3280033ec0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c3280033ed0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
=>0x0c3280033ee0: fd fd fd fd fd fd fd fd fd fd fd[fd]fd fd fd fd
  0x0c3280033ef0: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c3280033f00: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c3280033f10: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c3280033f20: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
  0x0c3280033f30: fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd fd
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2021-04-06 12:46:50,2
13369760," [Rust] Use stable rust for no default test, clean up CI tests","
# Rationale

1. As @jorgecarleitao noted on https://github.com/apache/arrow/pull/9889#discussion_r607720790, we should be running the check if arrow compiles with stable rust as that is what we target for the arrow crate
2. I noticed that there were several redundant settings of `RUSTFLAGS`
3. The titles of many of the tests are confusing (to me) as they have a lot of detailed architecture / rust version information rather than the test title
",pull-request-available,['Rust'],ARROW,Improvement,Minor,2021-04-06 10:52:36,11
13369751,[Dev][Packaging] Include build url in the crossbow console report,For easier navigation to the build log.,pull-request-available,"['Developer Tools', 'Packaging']",ARROW,Improvement,Major,2021-04-06 10:22:00,3
13369572,[Rust][CI] Reduce size of rust build artifacts in integration test,"# Rationale

The [integration test](https://github.com/apache/arrow/pull/9884/checks?check_run_id=2263730460) has a fixed size builder docker image and has builds from several Arrow implementations.

The Rust build artifacts (compiled binaries) in the integration tests still consume ~ 1GB of space even after https://github.com/apache/arrow/pull/9879 (see [~apitrou]'s comment on  https://github.com/apache/arrow/pull/9884#issuecomment-813037756).

It would be nice to reduce this even more (and speed up integration test while we are at it)",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-04-05 11:11:08,11
13369526,[R] Export and document list_compute_functions,"Since they're available to call in dplyr now, we should make it available. Note that not all compute functions are suitable to work in filter/mutate, and some will require custom C++ wiring for the FunctionOptions. But many/most just work now.",pull-request-available,['R'],ARROW,New Feature,Minor,2021-04-04 20:45:58,4
13369464,[Rust] [Parquet] Update zstd version,"updates zstd version used by parquet crate to zstd = ""0.7.0+zstd.1.4.9"".

",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-04-04 09:42:12,11
13369447,[Dev][Release] Use downloadable URL for archive download,"https://www.apache.org/dyn/closer.lua/arrow/arrow-3.0.0/apache-arrow-3.0.0.tar.gz shows a page that shows mirror URL. Users need to click download link in the page. It's inconvenient. We can't use {{wget}} or {{curl}} with the URL in our install page.

We can use https://www.apache.org/dyn/closer.lua?action=download&filename=arrow/arrow-3.0.0/apache-arrow-3.0.0.tar.gz with {{wget}} or {{curl}}:

{noformat}
wget --content-disposition https://www.apache.org/dyn/closer.lua?action=download&filename=arrow/arrow-3.0.0/apache-arrow-3.0.0.tar.gz
curl -L -O https://www.apache.org/dyn/closer.lua?action=download&filename=arrow/arrow-3.0.0/apache-arrow-3.0.0.tar.gz
{noformat}",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-04-03 21:38:25,1
13369305,[Docs] Switch to pydata-sphinx-theme for the main sphinx docs,"I have been working on using a new theme (the [pydata-sphinx-theme|https://pydata-sphinx-theme.readthedocs.io/en/latest/] with some custom styling for Arrow) for the sphinx documentation.

Initially, I tried out a few variants (see below), but unless more people give their preference, I will keep the layout the same as it is now (a single sidebar, option 3 below).



----

A few options for the layout, with a preview that I pushed to github pages:

1. Navbar + sidebar: the navbar contains the main sections (format, libraries, development), and then the sidebar further up to each page. 
 Two example links: [https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-1/format/CDataInterface.html] and [https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-1/python/parquet.html]

2. Navbar with dropdown + sidebar: similar as above, but with an additional dropdown for the different libraries (languages). As a consequence, the sidebar only contains the items for one language (eg python) at a time (giving a better view if you're only looking at the python docs IMO, but needs more clicks to switch to another language). 
 The same two example links: [https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-2-dropdown/format/CDataInterface.html], [https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-2-dropdown/python/parquet.html] (note that this option definitely still needs a better clue that you are eg in the python sub-part of the docs)

3. Only sidebar: this is similar in structure to what we have today with the RTD theme. 
 The same two example links: [https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-3-single-sidebar/format/CDataInterface.html], [https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-3-single-sidebar/python/parquet.html]

Lastly, I also made a option 4 that uses a single sidebar as option 3, but with a dropdown to choose the topic: https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-4-sidebar-with-dropdown/format/CDataInterface.html, https://jorisvandenbossche.github.io/arrow-docs-preview/html-option-4-sidebar-with-dropdown/python/parquet.html

Probably those previews can be further improved with some additional (CSS) tweaks. I ""just"" took the orange-like color that is also used on the home page as the main color for the navigation, to give it a bit a custom touch.
But so feedback on both which general structure you like most as details you would like to see improved is certainly welcome.

An additional question is if we want to integrate this somehow with the navbar we already have from the home page.
",pull-request-available,['Documentation'],ARROW,Improvement,Major,2021-04-02 13:29:34,5
13369111,[CI] Update setuptools in the ubuntu images,Our docs build is failing due to outdated setuptools version.,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2021-04-01 12:52:42,3
13369018,[GLib] Remove #include <config.h>,It's for GNU Autotools.,pull-request-available,['GLib'],ARROW,Improvement,Major,2021-04-01 05:19:31,1
13368962,[Python][Packaging] Pass python version as setuptools pretend version in the macOS wheel builds,Forgot to define SETUPTOOLS_SCM_PRETEND_VERSION in order to have the right version numbers in the wheels.,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2021-03-31 21:46:04,3
13368930,[C++] Fix compressed file reading with an empty stream at end of file,"Initially reported on the arrow-user mailing-list:
https://mail-archives.apache.org/mod_mbox/arrow-user/202103.mbox/%3Cb0a27ab4f8387e41b313fbfaa7ffb4e5764155c2.camel%40cnrgh.fr%3E

(with private access to the actual file)

",pull-request-available,['C++'],ARROW,Bug,Major,2021-03-31 17:54:58,2
13368686,[Rust] [Parquet] Return file metadata after writing Parquet file,"Parquet writers like delta-rs rely on the Parquet metadata to write file-level statistics for file pruning purposes.

We currently do not expose these stats, requiring the writer to read the file that has just been written, to get the stats. This is more problematic for in-memory sinks, as there is currently no way of getting the metadata from the sink before it's persisted.

Explore if we can expose these stats to the writer, to make the above easier.",pull-request-available,['Rust'],ARROW,New Feature,Major,2021-03-30 16:34:45,12
13368652,[Python] Bad type inference of mixed-precision Decimals,"Exporting _pyarrow.table_ that contains mixed-precision _Decimals_ using _parquet.write_table_ creates a parquet that contains invalid data/values.

In the example below the first value of _data_decimal_ is turned from Decimal('579.11999511718795474735088646411895751953125000000000') in the pyarrow table to Decimal('-378.68971792399258172661600550482428224218070136475136') in the parquet.


{code:java}
import pyarrow
from decimal import Decimal

values_floats = [579.119995117188, 6.40999984741211, 2.0] # floats
decs_from_values = [Decimal(v) for v in values_floats] # Decimal
decs_from_float = [Decimal.from_float(v) for v in values_floats]
decs_str = [Decimal(str(v)) for v in values_floats] # Decimal

data_dict = {""data_decimal"": decs_from_values, # python Decimal
 ""data_decimal_from_float"": decs_from_float,
 ""data_float"":values_floats, # python floats
 ""data_dec_str"": decs_str}

table = pyarrow.table(data=data_dict)
print(table.to_pydict()) # before saving
pyarrow.parquet.write_table(table, ""./pyarrow_decimal.parquet"") # saving
print(pyarrow.parquet.read_table(""./pyarrow_decimal.parquet"").to_pydict()) # after saving
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2021-03-30 13:26:21,5
13368457,[Python] undefined symbol: _ZN5arrow6StatusC1ENS_10StatusCodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE,"Using Ubuntu 20.04 in Github Actions CI to test a python extension that integrates with MongoDB and pyarrow, I get this error when attempting to import the Cython+pyarrow extension module:
{code:python}
ImportError: Failed to import test module: test_arrow
Traceback (most recent call last):
  File ""/usr/lib/python3.8/unittest/loader.py"", line 436, in _find_test_path
    module = self._get_module_from_name(name)
  File ""/usr/lib/python3.8/unittest/loader.py"", line 377, in _get_module_from_name
    __import__(name)
  File ""/home/runner/work/mongo-arrow/mongo-arrow/bindings/python/test/test_arrow.py"", line 21, in <module>
    from pymongoarrow.api import aggregate_arrow_all, find_arrow_all, Schema
  File ""/home/runner/work/mongo-arrow/mongo-arrow/bindings/python/pymongoarrow/__init__.py"", line 18, in <module>
    from pymongoarrow.lib import libbson_version
ImportError: /home/runner/work/mongo-arrow/mongo-arrow/bindings/python/pymongoarrow/lib.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN5arrow6StatusC1ENS_10StatusCodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
{code}

The task installs pyarrow 3.0 from the manylinux2014 wheel:
{code}
Collecting pyarrow>=3
  Downloading pyarrow-3.0.0-cp38-cp38-manylinux2014_x86_64.whl (20.7 MB)
{code}

The same project works fine locally on macOS (10.15) also using pyarrow 3.0 installed via pip. Upon googling I found this blog: https://uwekorn.com/2019/09/15/how-we-build-apache-arrows-manylinux-wheels.html

The article explains that the fix for {{""undefined symbol: _ZN5arrow6StatusC1ENS_10StatusCodeERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE""}} is to add {{-D_GLIBCXX_USE_CXX11_ABI=0}} to CFLAGS which did work for me. However, the article says this is only needed for manylinux1 wheels because they build on an old platform. Is it expected that users still need to define this flag when using manylinux2014 wheels?",pull-request-available,"['Documentation', 'Python']",ARROW,Bug,Minor,2021-03-29 19:05:54,2
13368423,[C++][CI] Valgrind failure on Grouper tests,See https://github.com/ursacomputing/crossbow/runs/2215907054,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2021-03-29 16:28:52,2
13368420,[Python][Packaging] Use vcpkg to build macOS wheels,"Manylinux and windows wheels use vcpkg as the dependency source already, port the macos wheel builds to align with the setup.",pull-request-available,"['Packaging', 'Python']",ARROW,New Feature,Major,2021-03-29 16:05:00,3
13368402,[C++] Add regex string match kernel,We have a basic {{match_substring}} kernel already but not a regular expression one.,pull-request-available,['C++'],ARROW,New Feature,Major,2021-03-29 15:20:39,0
13368301,[CI][GLib] Ensure upgrading MSYS2,"We need to refresh package database after system upgrade for ucrt64
repository.

See also: https://github.com/msys2/setup-msys2/issues/119


https://github.com/apache/arrow/runs/2215664184

{noformat}
+ pacman --needed --noconfirm --sync mingw-w64-x86_64-aws-sdk-cpp mingw-w64-x86_64-boost mingw-w64-x86_64-brotli mingw-w64-x86_64-ccache mingw-w64-x86_64-clang mingw-w64-x86_64-cmake mingw-w64-x86_64-gcc mingw-w64-x86_64-gflags mingw-w64-x86_64-grpc mingw-w64-x86_64-gtest mingw-w64-x86_64-libutf8proc mingw-w64-x86_64-llvm mingw-w64-x86_64-lz4 mingw-w64-x86_64-ninja mingw-w64-x86_64-polly mingw-w64-x86_64-protobuf mingw-w64-x86_64-python3-numpy mingw-w64-x86_64-rapidjson mingw-w64-x86_64-re2 mingw-w64-x86_64-snappy mingw-w64-x86_64-thrift mingw-w64-x86_64-zlib mingw-w64-x86_64-zstd mingw-w64-x86_64-gobject-introspection mingw-w64-x86_64-gtk-doc mingw-w64-x86_64-meson
warning: database file for 'ucrt64' does not exist (use '-Sy' to download)
warning: mingw-w64-x86_64-brotli-1.0.9-2 is up to date -- skipping
warning: mingw-w64-x86_64-clang-11.0.0-8 is up to date -- skipping
warning: mingw-w64-x86_64-gcc-10.2.0-9 is up to date -- skipping
warning: mingw-w64-x86_64-llvm-11.0.0-8 is up to date -- skipping
warning: mingw-w64-x86_64-lz4-1.9.3-1 is up to date -- skipping
warning: mingw-w64-x86_64-zlib-1.2.11-9 is up to date -- skipping
warning: mingw-w64-x86_64-zstd-1.4.8-2 is up to date -- skipping
error: failed to prepare transaction (could not find database)
Error: Process completed with exit code 1.
{noformat}",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Improvement,Major,2021-03-29 07:36:14,1
13368247,[CI][Crossbow] Remove (or fix) test-ubuntu-16.04-cpp job,"ARROW-8049 increased the minimum cmake version required for bundled thrift to 3.10, which is not what 16.04 ships. We removed packaging jobs in ARROW-11910 because it is EOL in April 2021, but we still have a nightly job that is failing and other related materials (Dockerfile etc.) for 16.04.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,New Feature,Major,2021-03-29 00:29:23,1
13368190,[Python] Cannot install via pip M1 mac,"when doing {{pip install pyarrow --no-use-pep517}}

{noformat}
Collecting pyarrow
 Using cached pyarrow-3.0.0.tar.gz (682 kB)
Requirement already satisfied: numpy>=1.16.6 in /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/lib/python3.8/site-packages (from pyarrow) (1.20.2)
Building wheels for collected packages: pyarrow
 Building wheel for pyarrow (setup.py) ... error
 ERROR: Command errored out with exit status 1:
 command: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""'; __file__='""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-wheel-vpkwqzyi
 cwd: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/
 Complete output (238 lines):
 running bdist_wheel
 running build
 running build_py
 creating build
 creating build/lib.macosx-11.2-arm64-3.8
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_generated_version.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/parquet.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas_compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_misc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_table.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_array.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization_deprecated.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_io.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_schema.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma_tf_op.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cython.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_builder.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_memory.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/deserialize_buffer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_scalars.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 running egg_info
 writing pyarrow.egg-info/PKG-INFO
 writing dependency_links to pyarrow.egg-info/dependency_links.txt
 writing entry points to pyarrow.egg-info/entry_points.txt
 writing requirements to pyarrow.egg-info/requires.txt
 writing top-level names to pyarrow.egg-info/top_level.txt
 warning: Failed to find the configured license file '../LICENSE.txt'
 reading manifest file 'pyarrow.egg-info/SOURCES.txt'
 reading manifest template 'MANIFEST.in'
 warning: no files found matching '../LICENSE.txt'
 warning: no files found matching '../NOTICE.txt'
 warning: no previously-included files matching '*.so' found anywhere in distribution
 warning: no previously-included files matching '*.pyc' found anywhere in distribution
 warning: no previously-included files matching '*~' found anywhere in distribution
 warning: no previously-included files matching '#*' found anywhere in distribution
 warning: no previously-included files matching '.git*' found anywhere in distribution
 warning: no previously-included files matching '.DS_Store' found anywhere in distribution
 no previously-included directories found matching '.asv'
 writing manifest file 'pyarrow.egg-info/SOURCES.txt'
 copying pyarrow/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_dataset.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_flight.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_hdfs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_json.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_plasma.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_s3fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/array.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/builder.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/config.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/error.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/gandiva.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io-hdfs.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/memory.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas-shim.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/public-api.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/scalar.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/table.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/tensor.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/common.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libplasma.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tensorflow/plasma_op.cc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 copying pyarrow/tests/data/feather/v0.17.0.version=2-compression=lz4.feather -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/common.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 running build_ext
 creating /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8
 -- Running cmake for pyarrow
 cmake -DPYTHON_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPython3_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PLASMA=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_USE_TENSORFLOW=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_BOOST=off -DPYARROW_GENERATE_COVERAGE=off -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_PARQUET_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e
 -- The C compiler identification is AppleClang 12.0.0.12000032
 -- The CXX compiler identification is AppleClang 12.0.0.12000032
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- System processor: x86_64
 -- Performing Test CXX_SUPPORTS_SSE4_2
 -- Performing Test CXX_SUPPORTS_SSE4_2 - Success
 -- Performing Test CXX_SUPPORTS_AVX2
 -- Performing Test CXX_SUPPORTS_AVX2 - Success
 -- Performing Test CXX_SUPPORTS_AVX512
 -- Performing Test CXX_SUPPORTS_AVX512 - Success
 -- Arrow build warning level: PRODUCTION
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
 -- Build Type: RELEASE
 -- Generator: Unix Makefiles
 -- Build output directory: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/release
 -- Found Python3: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python (found version ""3.8.7"") found components: Interpreter Development.Module NumPy
 -- Found Python3Alt: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python
 CMake Warning (dev) at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
 The package name passed to `find_package_handle_standard_args` (PkgConfig)
 does not match the name of the calling package (Arrow). This can lead to
 problems in calling code that expects `find_package` result variables
 (e.g., `_FOUND`) to follow a certain pattern.
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPkgConfig.cmake:70 (find_package_handle_standard_args)
 cmake_modules/FindArrow.cmake:39 (include)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)
 This warning is for project developers. Use -Wno-dev to suppress it.

-- Found PkgConfig: /opt/homebrew/bin/pkg-config (found version ""0.29.2"")
 -- Could NOT find Arrow (missing: Arrow_DIR)
 -- Checking for module 'arrow'
 -- No package 'arrow' found
 CMake Error at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
 Could NOT find Arrow (missing: ARROW_INCLUDE_DIR ARROW_LIB_DIR
 ARROW_FULL_SO_VERSION ARROW_SO_VERSION)
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
 cmake_modules/FindArrow.cmake:419 (find_package_handle_standard_args)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)


 -- Configuring incomplete, errors occurred!
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeOutput.log"".
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeError.log"".
 error: command 'cmake' failed with exit status 1
 ----------------------------------------
 ERROR: Failed building wheel for pyarrow
 Running setup.py clean for pyarrow
Failed to build pyarrow
Installing collected packages: pyarrow
 Running setup.py install for pyarrow ... error
 ERROR: Command errored out with exit status 1:
 command: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""'; __file__='""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-record-3v1lx8h6/install-record.txt --single-version-externally-managed --compile --install-headers /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/include/site/python3.8/pyarrow
 cwd: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/
 Complete output (238 lines):
 running install
 running build
 running build_py
 creating build
 creating build/lib.macosx-11.2-arm64-3.8
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_generated_version.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/parquet.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas_compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_misc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_table.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_array.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization_deprecated.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_io.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_schema.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma_tf_op.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cython.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_builder.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_memory.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/deserialize_buffer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_scalars.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 running egg_info
 writing pyarrow.egg-info/PKG-INFO
 writing dependency_links to pyarrow.egg-info/dependency_links.txt
 writing entry points to pyarrow.egg-info/entry_points.txt
 writing requirements to pyarrow.egg-info/requires.txt
 writing top-level names to pyarrow.egg-info/top_level.txt
 warning: Failed to find the configured license file '../LICENSE.txt'
 reading manifest file 'pyarrow.egg-info/SOURCES.txt'
 reading manifest template 'MANIFEST.in'
 warning: no files found matching '../LICENSE.txt'
 warning: no files found matching '../NOTICE.txt'
 warning: no previously-included files matching '*.so' found anywhere in distribution
 warning: no previously-included files matching '*.pyc' found anywhere in distribution
 warning: no previously-included files matching '*~' found anywhere in distribution
 warning: no previously-included files matching '#*' found anywhere in distribution
 warning: no previously-included files matching '.git*' found anywhere in distribution
 warning: no previously-included files matching '.DS_Store' found anywhere in distribution
 no previously-included directories found matching '.asv'
 writing manifest file 'pyarrow.egg-info/SOURCES.txt'
 copying pyarrow/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_dataset.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_flight.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_hdfs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_json.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_plasma.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_s3fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/array.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/builder.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/config.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/error.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/gandiva.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io-hdfs.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/memory.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas-shim.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/public-api.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/scalar.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/table.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/tensor.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/common.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libplasma.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tensorflow/plasma_op.cc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 copying pyarrow/tests/data/feather/v0.17.0.version=2-compression=lz4.feather -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/common.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 running build_ext
 creating /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8
 -- Running cmake for pyarrow
 cmake -DPYTHON_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPython3_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PLASMA=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_USE_TENSORFLOW=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_BOOST=off -DPYARROW_GENERATE_COVERAGE=off -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_PARQUET_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e
 -- The C compiler identification is AppleClang 12.0.0.12000032
 -- The CXX compiler identification is AppleClang 12.0.0.12000032
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- System processor: x86_64
 -- Performing Test CXX_SUPPORTS_SSE4_2
 -- Performing Test CXX_SUPPORTS_SSE4_2 - Success
 -- Performing Test CXX_SUPPORTS_AVX2
 -- Performing Test CXX_SUPPORTS_AVX2 - Success
 -- Performing Test CXX_SUPPORTS_AVX512
 -- Performing Test CXX_SUPPORTS_AVX512 - Success
 -- Arrow build warning level: PRODUCTION
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
 -- Build Type: RELEASE
 -- Generator: Unix Makefiles
 -- Build output directory: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/release
 -- Found Python3: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python (found version ""3.8.7"") found components: Interpreter Development.Module NumPy
 -- Found Python3Alt: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python
 CMake Warning (dev) at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
 The package name passed to `find_package_handle_standard_args` (PkgConfig)
 does not match the name of the calling package (Arrow). This can lead to
 problems in calling code that expects `find_package` result variables
 (e.g., `_FOUND`) to follow a certain pattern.
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPkgConfig.cmake:70 (find_package_handle_standard_args)
 cmake_modules/FindArrow.cmake:39 (include)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)
 This warning is for project developers. Use -Wno-dev to suppress it.

-- Found PkgConfig: /opt/homebrew/bin/pkg-config (found version ""0.29.2"")
 -- Could NOT find Arrow (missing: Arrow_DIR)
 -- Checking for module 'arrow'
 -- No package 'arrow' found
 CMake Error at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
 Could NOT find Arrow (missing: ARROW_INCLUDE_DIR ARROW_LIB_DIR
 ARROW_FULL_SO_VERSION ARROW_SO_VERSION)
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
 cmake_modules/FindArrow.cmake:419 (find_package_handle_standard_args)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)


 -- Configuring incomplete, errors occurred!
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeOutput.log"".
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeError.log"".
 error: command 'cmake' failed with exit status 1
 ----------------------------------------
ERROR: Command errored out with exit status 1: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""'; __file__='""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-record-3v1lx8h6/install-record.txt --single-version-externally-managed --compile --install-headers /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/include/site/python3.8/pyarrow Check the logs for full command output.

```

Collecting pyarrow
 Using cached pyarrow-3.0.0.tar.gz (682 kB)
 Installing build dependencies ... done
 Getting requirements to build wheel ... done
 Preparing wheel metadata ... done
Requirement already satisfied: numpy>=1.16.6 in /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/lib/python3.8/site-packages (from pyarrow) (1.20.2)
Building wheels for collected packages: pyarrow
 Building wheel for pyarrow (PEP 517) ... error
 ERROR: Command errored out with exit status 1:
 command: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/tmp4ivpzkwx
 cwd: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-z3ryfnqh/pyarrow_d395c1dcc85c426193f3e23fcde85666
 Complete output (221 lines):
 running bdist_wheel
 running build
 running build_py
 creating build
 creating build/lib.macosx-11.2-arm64-3.8
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_generated_version.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/parquet.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas_compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_misc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_table.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_array.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization_deprecated.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_io.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_schema.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma_tf_op.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cython.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_builder.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_memory.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/deserialize_buffer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_scalars.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 running egg_info
 writing pyarrow.egg-info/PKG-INFO
 writing dependency_links to pyarrow.egg-info/dependency_links.txt
 writing entry points to pyarrow.egg-info/entry_points.txt
 writing requirements to pyarrow.egg-info/requires.txt
 writing top-level names to pyarrow.egg-info/top_level.txt
 warning: Failed to find the configured license file '../LICENSE.txt'
 reading manifest file 'pyarrow.egg-info/SOURCES.txt'
 reading manifest template 'MANIFEST.in'
 warning: no files found matching '../LICENSE.txt'
 warning: no files found matching '../NOTICE.txt'
 warning: no previously-included files matching '*.so' found anywhere in distribution
 warning: no previously-included files matching '*.pyc' found anywhere in distribution
 warning: no previously-included files matching '*~' found anywhere in distribution
 warning: no previously-included files matching '#*' found anywhere in distribution
 warning: no previously-included files matching '.git*' found anywhere in distribution
 warning: no previously-included files matching '.DS_Store' found anywhere in distribution
 no previously-included directories found matching '.asv'
 writing manifest file 'pyarrow.egg-info/SOURCES.txt'
 copying pyarrow/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_dataset.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_flight.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_hdfs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_json.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_plasma.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_s3fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/array.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/builder.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/config.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/error.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/gandiva.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io-hdfs.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/memory.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas-shim.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/public-api.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/scalar.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/table.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/tensor.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/common.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libplasma.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tensorflow/plasma_op.cc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 copying pyarrow/tests/data/feather/v0.17.0.version=2-compression=lz4.feather -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/common.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 running build_ext
 creating /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-z3ryfnqh/pyarrow_d395c1dcc85c426193f3e23fcde85666/build/temp.macosx-11.2-arm64-3.8
 -- Running cmake for pyarrow
 cmake -DPYTHON_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPython3_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PLASMA=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_USE_TENSORFLOW=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_BOOST=off -DPYARROW_GENERATE_COVERAGE=off -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_PARQUET_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-z3ryfnqh/pyarrow_d395c1dcc85c426193f3e23fcde85666
 -- The C compiler identification is AppleClang 12.0.0.12000032
 -- The CXX compiler identification is AppleClang 12.0.0.12000032
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- System processor: x86_64
 -- Performing Test CXX_SUPPORTS_SSE4_2
 -- Performing Test CXX_SUPPORTS_SSE4_2 - Success
 -- Performing Test CXX_SUPPORTS_AVX2
 -- Performing Test CXX_SUPPORTS_AVX2 - Success
 -- Performing Test CXX_SUPPORTS_AVX512
 -- Performing Test CXX_SUPPORTS_AVX512 - Success
 -- Arrow build warning level: PRODUCTION
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
 -- Build Type: RELEASE
 -- Generator: Unix Makefiles
 -- Build output directory: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-z3ryfnqh/pyarrow_d395c1dcc85c426193f3e23fcde85666/build/temp.macosx-11.2-arm64-3.8/release
 CMake Error at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
 Could NOT find Python3 (missing: Python3_NumPy_INCLUDE_DIRS NumPy) (found
 version ""3.8.7"")
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPython/Support.cmake:3165 (find_package_handle_standard_args)
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPython3.cmake:485 (include)
 cmake_modules/FindPython3Alt.cmake:55 (find_package)
 CMakeLists.txt:200 (find_package)


 -- Configuring incomplete, errors occurred!
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-z3ryfnqh/pyarrow_d395c1dcc85c426193f3e23fcde85666/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeOutput.log"".
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-z3ryfnqh/pyarrow_d395c1dcc85c426193f3e23fcde85666/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeError.log"".
 error: command 'cmake' failed with exit status 1
 ----------------------------------------
 ERROR: Failed building wheel for pyarrow
Failed to build pyarrow
ERROR: Could not build wheels for pyarrow which use PEP 517 and cannot be installed directly
(dbt-sugar-lJO0x__U-py3.8)  dbt-sugar git:(main)  pip install pyarrow --no-use-pep517
Collecting pyarrow
 Using cached pyarrow-3.0.0.tar.gz (682 kB)
Requirement already satisfied: numpy>=1.16.6 in /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/lib/python3.8/site-packages (from pyarrow) (1.20.2)
Building wheels for collected packages: pyarrow
 Building wheel for pyarrow (setup.py) ... error
 ERROR: Command errored out with exit status 1:
 command: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""'; __file__='""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-wheel-vpkwqzyi
 cwd: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/
 Complete output (238 lines):
 running bdist_wheel
 running build
 running build_py
 creating build
 creating build/lib.macosx-11.2-arm64-3.8
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_generated_version.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/parquet.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas_compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_misc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_table.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_array.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization_deprecated.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_io.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_schema.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma_tf_op.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cython.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_builder.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_memory.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/deserialize_buffer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_scalars.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 running egg_info
 writing pyarrow.egg-info/PKG-INFO
 writing dependency_links to pyarrow.egg-info/dependency_links.txt
 writing entry points to pyarrow.egg-info/entry_points.txt
 writing requirements to pyarrow.egg-info/requires.txt
 writing top-level names to pyarrow.egg-info/top_level.txt
 warning: Failed to find the configured license file '../LICENSE.txt'
 reading manifest file 'pyarrow.egg-info/SOURCES.txt'
 reading manifest template 'MANIFEST.in'
 warning: no files found matching '../LICENSE.txt'
 warning: no files found matching '../NOTICE.txt'
 warning: no previously-included files matching '*.so' found anywhere in distribution
 warning: no previously-included files matching '*.pyc' found anywhere in distribution
 warning: no previously-included files matching '*~' found anywhere in distribution
 warning: no previously-included files matching '#*' found anywhere in distribution
 warning: no previously-included files matching '.git*' found anywhere in distribution
 warning: no previously-included files matching '.DS_Store' found anywhere in distribution
 no previously-included directories found matching '.asv'
 writing manifest file 'pyarrow.egg-info/SOURCES.txt'
 copying pyarrow/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_dataset.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_flight.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_hdfs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_json.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_plasma.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_s3fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/array.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/builder.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/config.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/error.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/gandiva.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io-hdfs.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/memory.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas-shim.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/public-api.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/scalar.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/table.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/tensor.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/common.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libplasma.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tensorflow/plasma_op.cc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 copying pyarrow/tests/data/feather/v0.17.0.version=2-compression=lz4.feather -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/common.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 running build_ext
 creating /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8
 -- Running cmake for pyarrow
 cmake -DPYTHON_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPython3_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PLASMA=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_USE_TENSORFLOW=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_BOOST=off -DPYARROW_GENERATE_COVERAGE=off -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_PARQUET_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e
 -- The C compiler identification is AppleClang 12.0.0.12000032
 -- The CXX compiler identification is AppleClang 12.0.0.12000032
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- System processor: x86_64
 -- Performing Test CXX_SUPPORTS_SSE4_2
 -- Performing Test CXX_SUPPORTS_SSE4_2 - Success
 -- Performing Test CXX_SUPPORTS_AVX2
 -- Performing Test CXX_SUPPORTS_AVX2 - Success
 -- Performing Test CXX_SUPPORTS_AVX512
 -- Performing Test CXX_SUPPORTS_AVX512 - Success
 -- Arrow build warning level: PRODUCTION
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
 -- Build Type: RELEASE
 -- Generator: Unix Makefiles
 -- Build output directory: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/release
 -- Found Python3: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python (found version ""3.8.7"") found components: Interpreter Development.Module NumPy
 -- Found Python3Alt: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python
 CMake Warning (dev) at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
 The package name passed to `find_package_handle_standard_args` (PkgConfig)
 does not match the name of the calling package (Arrow). This can lead to
 problems in calling code that expects `find_package` result variables
 (e.g., `_FOUND`) to follow a certain pattern.
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPkgConfig.cmake:70 (find_package_handle_standard_args)
 cmake_modules/FindArrow.cmake:39 (include)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)
 This warning is for project developers. Use -Wno-dev to suppress it.

-- Found PkgConfig: /opt/homebrew/bin/pkg-config (found version ""0.29.2"")
 -- Could NOT find Arrow (missing: Arrow_DIR)
 -- Checking for module 'arrow'
 -- No package 'arrow' found
 CMake Error at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
 Could NOT find Arrow (missing: ARROW_INCLUDE_DIR ARROW_LIB_DIR
 ARROW_FULL_SO_VERSION ARROW_SO_VERSION)
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
 cmake_modules/FindArrow.cmake:419 (find_package_handle_standard_args)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)


 -- Configuring incomplete, errors occurred!
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeOutput.log"".
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeError.log"".
 error: command 'cmake' failed with exit status 1
 ----------------------------------------
 ERROR: Failed building wheel for pyarrow
 Running setup.py clean for pyarrow
Failed to build pyarrow
Installing collected packages: pyarrow
 Running setup.py install for pyarrow ... error
 ERROR: Command errored out with exit status 1:
 command: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""'; __file__='""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-record-3v1lx8h6/install-record.txt --single-version-externally-managed --compile --install-headers /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/include/site/python3.8/pyarrow
 cwd: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/
 Complete output (238 lines):
 running install
 running build
 running build_py
 creating build
 creating build/lib.macosx-11.2-arm64-3.8
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_generated_version.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/parquet.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas_compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_misc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_table.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_array.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization_deprecated.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_io.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_schema.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma_tf_op.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cython.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_builder.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_memory.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/deserialize_buffer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_scalars.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 running egg_info
 writing pyarrow.egg-info/PKG-INFO
 writing dependency_links to pyarrow.egg-info/dependency_links.txt
 writing entry points to pyarrow.egg-info/entry_points.txt
 writing requirements to pyarrow.egg-info/requires.txt
 writing top-level names to pyarrow.egg-info/top_level.txt
 warning: Failed to find the configured license file '../LICENSE.txt'
 reading manifest file 'pyarrow.egg-info/SOURCES.txt'
 reading manifest template 'MANIFEST.in'
 warning: no files found matching '../LICENSE.txt'
 warning: no files found matching '../NOTICE.txt'
 warning: no previously-included files matching '*.so' found anywhere in distribution
 warning: no previously-included files matching '*.pyc' found anywhere in distribution
 warning: no previously-included files matching '*~' found anywhere in distribution
 warning: no previously-included files matching '#*' found anywhere in distribution
 warning: no previously-included files matching '.git*' found anywhere in distribution
 warning: no previously-included files matching '.DS_Store' found anywhere in distribution
 no previously-included directories found matching '.asv'
 writing manifest file 'pyarrow.egg-info/SOURCES.txt'
 copying pyarrow/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_dataset.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_flight.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_hdfs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_json.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_plasma.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_s3fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/array.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/builder.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/config.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/error.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/gandiva.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io-hdfs.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/memory.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas-shim.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/public-api.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/scalar.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/table.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/tensor.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/common.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libplasma.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tensorflow/plasma_op.cc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 copying pyarrow/tests/data/feather/v0.17.0.version=2-compression=lz4.feather -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/common.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 running build_ext
 creating /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8
 -- Running cmake for pyarrow
 cmake -DPYTHON_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPython3_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PLASMA=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_USE_TENSORFLOW=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_BOOST=off -DPYARROW_GENERATE_COVERAGE=off -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_PARQUET_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e
 -- The C compiler identification is AppleClang 12.0.0.12000032
 -- The CXX compiler identification is AppleClang 12.0.0.12000032
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- System processor: x86_64
 -- Performing Test CXX_SUPPORTS_SSE4_2
 -- Performing Test CXX_SUPPORTS_SSE4_2 - Success
 -- Performing Test CXX_SUPPORTS_AVX2
 -- Performing Test CXX_SUPPORTS_AVX2 - Success
 -- Performing Test CXX_SUPPORTS_AVX512
 -- Performing Test CXX_SUPPORTS_AVX512 - Success
 -- Arrow build warning level: PRODUCTION
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
 -- Build Type: RELEASE
 -- Generator: Unix Makefiles
 -- Build output directory: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/release
 -- Found Python3: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python (found version ""3.8.7"") found components: Interpreter Development.Module NumPy
 -- Found Python3Alt: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python
 CMake Warning (dev) at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:438 (message):
 The package name passed to `find_package_handle_standard_args` (PkgConfig)
 does not match the name of the calling package (Arrow). This can lead to
 problems in calling code that expects `find_package` result variables
 (e.g., `_FOUND`) to follow a certain pattern.
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPkgConfig.cmake:70 (find_package_handle_standard_args)
 cmake_modules/FindArrow.cmake:39 (include)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)
 This warning is for project developers. Use -Wno-dev to suppress it.

-- Found PkgConfig: /opt/homebrew/bin/pkg-config (found version ""0.29.2"")
 -- Could NOT find Arrow (missing: Arrow_DIR)
 -- Checking for module 'arrow'
 -- No package 'arrow' found
 CMake Error at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
 Could NOT find Arrow (missing: ARROW_INCLUDE_DIR ARROW_LIB_DIR
 ARROW_FULL_SO_VERSION ARROW_SO_VERSION)
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
 cmake_modules/FindArrow.cmake:419 (find_package_handle_standard_args)
 cmake_modules/FindArrowPython.cmake:46 (find_package)
 CMakeLists.txt:214 (find_package)


 -- Configuring incomplete, errors occurred!
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeOutput.log"".
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeError.log"".
 error: command 'cmake' failed with exit status 1
 ----------------------------------------
ERROR: Command errored out with exit status 1: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""'; __file__='""'""'/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-ri2w315u/pyarrow_8d01252c437341798da24cfec11f603e/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-record-3v1lx8h6/install-record.txt --single-version-externally-managed --compile --install-headers /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/include/site/python3.8/pyarrow Check the logs for full command output.
(dbt-sugar-lJO0x__U-py3.8)  dbt-sugar git:(main)  pip install pyarrow
Collecting pyarrow
 Using cached pyarrow-3.0.0.tar.gz (682 kB)
 Installing build dependencies ... done
 Getting requirements to build wheel ... done
 Preparing wheel metadata ... done
Requirement already satisfied: numpy>=1.16.6 in /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/lib/python3.8/site-packages (from pyarrow) (1.20.2)
Building wheels for collected packages: pyarrow
 Building wheel for pyarrow (PEP 517) ... error
 ERROR: Command errored out with exit status 1:
 command: /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python /Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/lib/python3.8/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/tmpxo27vpzk
 cwd: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-plkkl5f5/pyarrow_b2ed3ad78f6c4bbf987e34238d2e63c4
 Complete output (221 lines):
 running bdist_wheel
 running build
 running build_py
 creating build
 creating build/lib.macosx-11.2-arm64-3.8
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_generated_version.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/parquet.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas_compat.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_ipc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_convert_builtin.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_misc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_gandiva.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_adhoc_memory_leak.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/arrow_7980.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/util.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_orc.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_table.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_array.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_deprecations.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization_deprecated.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/__init__.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_io.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda_numba_interop.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cffi.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_schema.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_jvm.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma_tf_op.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_fs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_filesystem.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_threaded_import.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/pandas_examples.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cython.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_sparse_tensor.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_builder.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_cuda.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_extension_type.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_feather.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_memory.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_flight.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_json.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_serialization.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_compute.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_hdfs.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/deserialize_buffer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_strategies.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_csv.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_plasma.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_scalars.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 copying pyarrow/tests/test_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 running egg_info
 writing pyarrow.egg-info/PKG-INFO
 writing dependency_links to pyarrow.egg-info/dependency_links.txt
 writing entry points to pyarrow.egg-info/entry_points.txt
 writing requirements to pyarrow.egg-info/requires.txt
 writing top-level names to pyarrow.egg-info/top_level.txt
 warning: Failed to find the configured license file '../LICENSE.txt'
 reading manifest file 'pyarrow.egg-info/SOURCES.txt'
 reading manifest template 'MANIFEST.in'
 warning: no files found matching '../LICENSE.txt'
 warning: no files found matching '../NOTICE.txt'
 warning: no previously-included files matching '*.so' found anywhere in distribution
 warning: no previously-included files matching '*.pyc' found anywhere in distribution
 warning: no previously-included files matching '*~' found anywhere in distribution
 warning: no previously-included files matching '#*' found anywhere in distribution
 warning: no previously-included files matching '.git*' found anywhere in distribution
 warning: no previously-included files matching '.DS_Store' found anywhere in distribution
 no previously-included directories found matching '.asv'
 writing manifest file 'pyarrow.egg-info/SOURCES.txt'
 copying pyarrow/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_compute.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_csv.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_cuda.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_dataset.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_flight.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_hdfs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_json.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_orc.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_parquet.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_plasma.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/_s3fs.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/array.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/benchmark.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/builder.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/compat.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/config.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/error.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/feather.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/gandiva.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io-hdfs.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/io.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/ipc.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/lib.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/memory.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/pandas-shim.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/public-api.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/scalar.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/serialization.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/table.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/tensor.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 copying pyarrow/types.pxi -> build/lib.macosx-11.2-arm64-3.8/pyarrow
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/__init__.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/common.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_cuda.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_dataset.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_flight.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libarrow_fs.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libgandiva.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 copying pyarrow/includes/libplasma.pxd -> build/lib.macosx-11.2-arm64-3.8/pyarrow/includes
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tensorflow/plasma_op.cc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tensorflow
 copying pyarrow/tests/pyarrow_cython_example.pyx -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 copying pyarrow/tests/data/feather/v0.17.0.version=2-compression=lz4.feather -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/feather
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/README.md -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.emptyFile.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.test1.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/TestOrcFile.testDate1900.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.jsn.gz -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 copying pyarrow/tests/data/orc/decimal.orc -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/orc
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.all-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.column-metadata-handling.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 copying pyarrow/tests/data/parquet/v0.7.1.some-named-index.parquet -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/data/parquet
 creating build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/common.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/conftest.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_basic.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_data_types.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_dataset.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_datetime.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_metadata.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_pandas.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_file.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 copying pyarrow/tests/parquet/test_parquet_writer.py -> build/lib.macosx-11.2-arm64-3.8/pyarrow/tests/parquet
 running build_ext
 creating /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-plkkl5f5/pyarrow_b2ed3ad78f6c4bbf987e34238d2e63c4/build/temp.macosx-11.2-arm64-3.8
 -- Running cmake for pyarrow
 cmake -DPYTHON_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPython3_EXECUTABLE=/Users/bastienboutonnet/Library/Caches/pypoetry/virtualenvs/dbt-sugar-lJO0x__U-py3.8/bin/python -DPYARROW_BUILD_CUDA=off -DPYARROW_BUILD_FLIGHT=off -DPYARROW_BUILD_GANDIVA=off -DPYARROW_BUILD_DATASET=off -DPYARROW_BUILD_ORC=off -DPYARROW_BUILD_PARQUET=off -DPYARROW_BUILD_PLASMA=off -DPYARROW_BUILD_S3=off -DPYARROW_BUILD_HDFS=off -DPYARROW_USE_TENSORFLOW=off -DPYARROW_BUNDLE_ARROW_CPP=off -DPYARROW_BUNDLE_BOOST=off -DPYARROW_GENERATE_COVERAGE=off -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_PARQUET_USE_SHARED=on -DCMAKE_BUILD_TYPE=release /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-plkkl5f5/pyarrow_b2ed3ad78f6c4bbf987e34238d2e63c4
 -- The C compiler identification is AppleClang 12.0.0.12000032
 -- The CXX compiler identification is AppleClang 12.0.0.12000032
 -- Detecting C compiler ABI info
 -- Detecting C compiler ABI info - done
 -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
 -- Detecting C compile features
 -- Detecting C compile features - done
 -- Detecting CXX compiler ABI info
 -- Detecting CXX compiler ABI info - done
 -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
 -- Detecting CXX compile features
 -- Detecting CXX compile features - done
 -- System processor: x86_64
 -- Performing Test CXX_SUPPORTS_SSE4_2
 -- Performing Test CXX_SUPPORTS_SSE4_2 - Success
 -- Performing Test CXX_SUPPORTS_AVX2
 -- Performing Test CXX_SUPPORTS_AVX2 - Success
 -- Performing Test CXX_SUPPORTS_AVX512
 -- Performing Test CXX_SUPPORTS_AVX512 - Success
 -- Arrow build warning level: PRODUCTION
 Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})
 -- Build Type: RELEASE
 -- Generator: Unix Makefiles
 -- Build output directory: /private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-plkkl5f5/pyarrow_b2ed3ad78f6c4bbf987e34238d2e63c4/build/temp.macosx-11.2-arm64-3.8/release
 CMake Error at /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
 Could NOT find Python3 (missing: Python3_NumPy_INCLUDE_DIRS NumPy) (found
 version ""3.8.7"")
 Call Stack (most recent call first):
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPython/Support.cmake:3165 (find_package_handle_standard_args)
 /opt/homebrew/Cellar/cmake/3.20.0/share/cmake/Modules/FindPython3.cmake:485 (include)
 cmake_modules/FindPython3Alt.cmake:55 (find_package)
 CMakeLists.txt:200 (find_package)


 -- Configuring incomplete, errors occurred!
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-plkkl5f5/pyarrow_b2ed3ad78f6c4bbf987e34238d2e63c4/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeOutput.log"".
 See also ""/private/var/folders/v2/lfkghkc147j06_jd13v1f0yr0000gn/T/pip-install-plkkl5f5/pyarrow_b2ed3ad78f6c4bbf987e34238d2e63c4/build/temp.macosx-11.2-arm64-3.8/CMakeFiles/CMakeError.log"".
 error: command 'cmake' failed with exit status 1
 ----------------------------------------
 ERROR: Failed building wheel for pyarrow
Failed to build pyarrow
ERROR: Could not build wheels for pyarrow which use PEP 517 and cannot be installed directly

{noformat}





when doing `",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2021-03-28 12:37:06,3
13368155,[Rust] [Parquet] Arrow writer benchmarks,"The common concern with Parquet's Arrow readers and writers is that they're slow.
My diagnosis is that we rely on a chain of processes, which introduces overhead.
For example, writing an Arrow RecordBatch involves the following:

1. Iterate through arrays to create def/rep levels
2. Extract Parquet primitive values from arrays using these levels
3. Write primitive values, validating them in the process (when they already should be validated)
4. Split the already materialised values into small batches for Parquet chunks (consider where we have 1e6 values in a batch)
5. Write these batches, computing the stats of each batch, and encoding values

The above is as a side-effect of convenience, as it would likely require a lot more effort to bypass some of the steps.

I have ideas around going from step 1 to 5 directly, but won't know if it's better if there aren't performance benchmarks. I also struggle to see if I'm making improvements while I clean up the writer code, especially removing the allocations that I created to reduce the complexity of the level calculations.

With ARROW-12120 (random array & batch generator), it becomes more convenient to benchmark (and test many combinations of) the Arrow writer.

I would thus like to start adding benchmarks for the Arrow writer.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-03-28 07:53:49,12
13368134,[Rust] Generate random arrays and batches,"I need a random data generator for the Parquet <> Arrow integration. It takes me a while to craft a test case, so being able to create random data would make it a bit easier to improve test coverage and catch edge-cases in the code.",pull-request-available,['Rust'],ARROW,New Feature,Major,2021-03-27 22:16:17,12
13368051,[Rust] Fix or ignore 1.51 clippy lints,"Rust 1.51 introduces some lints that have broken CI. We can either fix or ignore them, depending on the amount of time it'll take to fix them.",pull-request-available,['Rust'],ARROW,Task,Minor,2021-03-27 04:06:38,12
13368027,[CI] No space left on device - AMD64 Conda Integration test,"One example:  https://github.com/apache/arrow/pull/9814/checks?check_run_id=2205470543#step:8:4716

{code}
+ npm install
npm WARN tar ENOSPC: no space left on device, write
npm WARN tar ENOSPC: no space left on device, write
npm ERR! code ENOSPC
npm ERR! syscall write
npm ERR! errno -28
npm ERR! nospc ENOSPC: no space left on device, write
npm ERR! nospc There appears to be insufficient space on your system to finish.
npm ERR! nospc Clear up some disk space and try again.

npm ERR! A complete log of this run can be found in:
npm ERR!     /root/.npm/_logs/2021-03-26T22_10_59_913Z-debug.log
228
Error: `docker-compose --file /home/runner/work/arrow/arrow/docker-compose.yml run --rm conda-integration` exited with a non-zero exit code 228, see the process log above.
{code}",pull-request-available,"['CI', 'Continuous Integration', 'Integration']",ARROW,Bug,Blocker,2021-03-26 22:31:03,11
13367991,"[C++] ""load of misaligned address"" in Parquet reader","When running the R sanitizer job, errors like these are reported:
{noformat}
/arrow/cpp/src/arrow/util/bpacking_avx2_generated.h:278:41: runtime error: load of misaligned address 0x7fb0a9c0e11d for type 'const uint32_t', which requires 4 byte alignment
0x7fb0a9c0e11d: note: pointer points here
 40 01 05 09 00 04 31  48 31 e1 a0 a4 16 63 cd  3d 18 e5 9a 87 d6 67 2f  16 00 00 00 00 00 00 00  00
             ^
    #0 0x7fb1786fa536 in arrow::internal::unpack32_avx2(unsigned int const*, unsigned int*, int, int) (/usr/local/RDsan/lib/R/site-library/arrow/libs/arrow.so+0x17fa2536)
    #1 0x7fb1768bfc03 in arrow::internal::unpack32(unsigned int const*, unsigned int*, int, int) (/usr/local/RDsan/lib/R/site-library/arrow/libs/arrow.so+0x16167c03)
    #2 0x7fb1721b37ae in int arrow::BitUtil::BitReader::GetBatch<int>(int, int*, int) (/usr/local/RDsan/lib/R/site-library/arrow/libs/arrow.so+0x11a5b7ae)
    #3 0x7fb1722d270f in int arrow::util::RleDecoder::GetBatchWithDict<double>(double const*, int, double*, int) (/usr/local/RDsan/lib/R/site-library/arrow/libs/arrow.so+0x11b7a70f)
    #4 0x7fb17202a35d in virtual thunk to parquet::(anonymous namespace)::DictDecoderImpl<parquet::PhysicalType<(parquet::Type::type)5> >::DecodeSpaced(double*, int, int, unsigned char const*, long) (/usr/local/RDsan/lib/R/site-library/arrow/libs/arrow.so+0x118d235d) {noformat}
The full log:

[^testthat.out]

cpuinfo:
{noformat}
processor	: 15
vendor_id	: GenuineIntel
cpu family	: 6
model		: 165
model name	: Intel(R) Core(TM) i9-10885H CPU @ 2.40GHz
stepping	: 2
microcode	: 0xe2
cpu MHz		: 852.969
cache size	: 16384 KB
physical id	: 0
siblings	: 16
core id		: 7
cpu cores	: 8
apicid		: 15
initial apicid	: 15
fpu		: yes
fpu_exception	: yes
cpuid level	: 22
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp pku ospke md_clear flush_l1d arch_capabilities
bugs		: spectre_v1 spectre_v2 spec_store_bypass swapgs itlb_multihit
bogomips	: 4800.00
clflush size	: 64
cache_alignment	: 64
address sizes	: 39 bits physical, 48 bits virtual
power management:
{noformat}",pull-request-available,['C++'],ARROW,Bug,Major,2021-03-26 19:41:16,2
13367979,[C#] Cannot round-trip record batch with PyArrow,"Has anyone ever tried to round-trip a record batch between Arrow C# and PyArrow? I can't get PyArrow to read the data correctly.

For context, I'm trying to do Arrow data-frames inter-process communication between C# and Python using shared memory (local TCP/IP is also an alternative). Ideally, I wouldn't even have to serialise the data and could just share the Arrow in-memory representation directly, but I'm not sure this is even possible with Apache Arrow. Full source code as attachment.

*C#*
{code:c#}
using (var stream = sharedMemory.CreateStream(0, 0, MemoryMappedFileAccess.ReadWrite))
{
    var recordBatch = /* ... */

    using (var writer = new ArrowFileWriter(stream, recordBatch.Schema, leaveOpen: true))
    {
        writer.WriteRecordBatch(recordBatch);
        writer.WriteEnd();
    }
}
{code}

*Python*
{code:python}
shmem = open_shared_memory(args)
address = get_shared_memory_address(shmem)
buf = pa.foreign_buffer(address, args.sharedMemorySize)
stream = pa.input_stream(buf)
reader = pa.ipc.open_stream(stream)
{code}

Unfortunately, it fails with the following error: {{pyarrow.lib.ArrowInvalid: Expected to read 1330795073 metadata bytes, but only read 1230}}.

I can see that the memory content starts with {{ARROW1\x00\x00\xff\xff\xff\xff\x08\x01\x00\x00\x10\x00\x00\x00}}. It seems that using the API calls above, PyArrow reads ""ARRO"" as the length of the metadata.

I assume I'm using the API incorrectly. Has anyone got a working example?",pull-request-available,"['C#', 'C++', 'Python']",ARROW,Bug,Blocker,2021-03-26 16:56:29,2
13367961,[R] Catch cpp build failures on linux,"See https://issues.apache.org/jira/browse/ARROW-11963?focusedCommentId=17303664&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17303664 and the comment below it.

There are some circumstances where the build does not succeed, but the catch for that (to print a useful error) is not being triggered.
",pull-request-available,['R'],ARROW,Improvement,Major,2021-03-26 15:33:40,4
13367748,[C++][R] Fix/workaround re2 building on clang/libc++,"See https://github.com/apache/arrow/pull/8468#issuecomment-807807284. We either need to fix the build (maybe there's something not getting passed through to build_re2 correctly in cmake) or figure out the conditions under which the C++ build should turn off re2. 

See also ARROW-11736 to make regex compute functions optional in R tests.",pull-request-available,"['C++', 'R']",ARROW,New Feature,Blocker,2021-03-26 02:16:25,4
13367657,[Doc] Fix warnings when building Sphinx docs,Some warnings are due to invalid markup or ambiguous cross-references.,pull-request-available,['Documentation'],ARROW,Bug,Major,2021-03-25 16:43:17,2
13367627,[C++] offline builds does not use ARROW_$LIBRARY_URL to search for packages,"I am following the instructions in order to build the parquet libraries, and there are some firewall restrictions in our dvpmt environment.

I have followed the instructions mentioned in [https://github.com/apache/arrow/blob/master/docs/source/developers/cpp/building.rst]

regardingOffline Builds:
{code:java}
./thirdparty/download_dependencies.sh $HOME/arrow-thirdparty >env.sh
. ./env.sh
cdcpp
mkdir debug
cd debug
cmake -DCMAKE_BUILD_TYPE=Debug -DARROW_PARQUET=ON -DARROW_DATASET=ON ..
make parquet arrow_dataset{code}
The result is that the process simply ignores the environment variables and tries to download the packages from the cloud - facing the firewall restrictions, and then it fails after a number of attempts at trying to download the packages.
h3. ",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2021-03-25 15:29:25,2
13367403,[R] schema use in open_dataset,"I have a directory of split .csvs that I'm importing with open_dataset(). Between files, a column is imported as either int64 (e.g. -2) and the other string (1986CD), and this throws an error when {{unify_schemas = T}}

{{ arrow::open_dataset('./split-csvs/nswcr/', format = 'csv', unify_schemas = T)}}

{{Error: Invalid: Unable to merge: Field SEIFACalcMethod has incompatible types: int64 vs string}}

If I use the schema parameter, and only want to specify this column, I only am able to import this column

{{arrow::open_dataset('./split-csvs/nswcr/',}}{{format = 'csv',}}{{schema = schema(SEIFACalcMethod = string()))}}

{{ }}
{{FileSystemDataset with 45 csv files}}
{{SEIFACalcMethod: string}}

I was expecting that could set the class of a select few columns, while the rest would be imported as-is. Similar to readr::read_csv(col_types = cols()) approach.

Not sure if this is expected behaviour, a bug, or a possible avenue for improvement. I've tagged this as the latter.(y)

",pull-request-available,['R'],ARROW,Improvement,Minor,2021-03-25 01:20:02,0
13367373,[R] Bindings for utf8_length,Following ARROW-11693,pull-request-available,['R'],ARROW,New Feature,Major,2021-03-24 22:24:19,4
13367344,[C++] Out-of-bounds write in ListArray::FromArrays,"This crashes when the batch is dropped with the error {{mimalloc: error: buffer overflow in heap block 0000040000001A00 of size 512: write after 512 bytes}}
{code:java}
autobatch_schema=schema({field(""list"",list(float64()),true),field(""f64"",float64())});
autobatch=random::GenerateBatch(batch_schema->fields(),/*batch_size=*/4096,/*seed=*/0);
{code}
Notably all the following are required:
 * The list column must be nullable
 * The float column must be present
 * The batch size must be at least 4096
 * Must be release mode (RelWithDebInfo does not crash)
 * Must be built in a similar way as AppVeyor (i.e. following [http://arrow.apache.org/docs/developers/cpp/windows.html#replicating-appveyor-builds])",pull-request-available,['C++'],ARROW,Bug,Major,2021-03-24 20:14:13,0
13367313,[Rust] Fix build,"

There was a logical conflict between https://github.com/apache/arrow/commit/eebf64b00e3a26f61c4bebec7241a0b24d27ec67 which removed the Arc in `ArrayData` and  https://github.com/apache/arrow/commit/8dd6abbb72b6b8958f3b2f35512bdadcaf43066f which optimized the compute kernels.



",pull-request-available,['Rust'],ARROW,Bug,Critical,2021-03-24 17:20:30,11
13367133,[GLib] Drop support for GNU Autotools,"If we drop support for GNU Autotools, we can simplify our source archive release process.",pull-request-available,['GLib'],ARROW,Improvement,Major,2021-03-24 03:34:24,1
13367079,[Python] Stop using distutils,"According to [PEP 632|https://www.python.org/dev/peps/pep-0632/], distutils will be deprecated in Python 3.10 and removed in 3.12.
",pull-request-available,['Python'],ARROW,Task,Critical,2021-03-23 18:45:59,2
13367039,[C++][Python] Segfault reading JSON file,"I noticed this when doing some analysis on a not very complex, but reasonably large json file and I've simplified it to a fairly minimal reproduction:

```

import pyarrow.json
 pyarrow.json.read_json('test.json')

```

and `test.json` is

```

{""A"":""<0 repeated 1.6 million times>""}

{""B"":[]}

```

this seems like it shouldn't be too large to load into memory all-at-once, so I'm surprised there is a segfault

running via gdb and getting a backtrace gives

```

(gdb) bt
 #0 0x00007ffff5c1965d in std::__shared_ptr<arrow::Buffer, (__gnu_cxx::_Lock_policy)2>::__shared_ptr(std::__shared_ptr<arrow::Buffer, (__gnu_cxx::_Lock_policy)2> const&) () from /home/patrick/.local/lib/python3.9/site-packages/pyarrow/libarrow.so.300
 #1 0x00007ffff5ca8d9e in arrow::json::ChunkedListArrayBuilder::Insert(long, std::shared_ptr<arrow::Field> const&, std::shared_ptr<arrow::Array> const&) () from /home/patrick/.local/lib/python3.9/site-packages/pyarrow/libarrow.so.300
 #2 0x00007ffff5cabcc8 in arrow::json::ChunkedStructArrayBuilder::Finish(std::shared_ptr<arrow::ChunkedArray>*) () from /home/patrick/.local/lib/python3.9/site-packages/pyarrow/libarrow.so.300
 #3 0x00007ffff5c1fc16 in arrow::json::TableReaderImpl::Read() () from /home/patrick/.local/lib/python3.9/site-packages/pyarrow/libarrow.so.300
 #4 0x00007fffcf73da69 in __pyx_pw_7pyarrow_5_json_1read_json(_object*, _object*, _object*) () from /home/patrick/.local/lib/python3.9/site-packages/pyarrow/_json.cpython-39-x86_64-linux-gnu.so
 #5 0x00007ffff7d35a43 in ?? () from /usr/lib/libpython3.9.so.1.0
 #6 0x00007ffff7d1be6d in _PyObject_MakeTpCall () from /usr/lib/libpython3.9.so.1.0
 #7 0x00007ffff7d17b3a in _PyEval_EvalFrameDefault () from /usr/lib/libpython3.9.so.1.0
 #8 0x00007ffff7d119ad in ?? () from /usr/lib/libpython3.9.so.1.0
 #9 0x00007ffff7d11371 in _PyEval_EvalCodeWithName () from /usr/lib/libpython3.9.so.1.0
 #10 0x00007ffff7dd3f83 in PyEval_EvalCode () from /usr/lib/libpython3.9.so.1.0
 #11 0x00007ffff7de43dd in ?? () from /usr/lib/libpython3.9.so.1.0
 #12 0x00007ffff7ddfc7b in ?? () from /usr/lib/libpython3.9.so.1.0
 #13 0x00007ffff7cf38ab in ?? () from /usr/lib/libpython3.9.so.1.0
 #14 0x00007ffff7cf3a63 in PyRun_InteractiveLoopFlags () from /usr/lib/libpython3.9.so.1.0
 #15 0x00007ffff7c81f6b in PyRun_AnyFileExFlags () from /usr/lib/libpython3.9.so.1.0
 #16 0x00007ffff7c7665c in ?? () from /usr/lib/libpython3.9.so.1.0
 #17 0x00007ffff7dc6fa9 in Py_BytesMain () from /usr/lib/libpython3.9.so.1.0
 #18 0x00007ffff7a43b25 in __libc_start_main () from /usr/lib/libc.so.6
 #19 0x000055555555504e in _start ()
 (gdb)

```

",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-03-23 15:38:21,2
13367003,[Python] Enable arithmetic operations on Expressions,"To make the dataset (projection) expressions more usable, we can add some more dunder methods to the class (like we already can do {{expr == 1}} for comparison operations, we can also enable arithmetic python operators. ",dataset pull-request-available,['Python'],ARROW,Sub-task,Major,2021-03-23 14:03:50,5
13366990,[Python] Remove direct usage of pandas' Block subclasses,"The {{CategoricalBlock}} was removed in pandas (https://github.com/pandas-dev/pandas/pull/40527), which breaks the nightly tests. 

I will argue in pandas to add it back on the short term (https://github.com/pandas-dev/pandas/issues/40226#issuecomment-804903010), but we should still remove the direct use of those Block subclasses in pyarrow.",pull-request-available,['Python'],ARROW,Bug,Major,2021-03-23 13:27:34,5
13366875,[GLib] Intermittent CI failure in test_add_column_type(TestCSVReader::#read::options),"See [https://github.com/apache/arrow/runs/2153825500?check_suite_focus=true] and [https://github.com/apache/arrow/runs/2170232403?check_suite_focus=true]



I don't know Ruby enough to know offhand what is going on but if no one is available to look at it I will try and dig a little deeper when I'm done with current work.",pull-request-available,['GLib'],ARROW,Bug,Minor,2021-03-23 04:11:41,1
13366826,[C++][Python][FlightRPC] Use StopToken to enable interrupting long Flight operations,"For example, calling do_get().read_all() can't be interrupted with Ctrl+C in Python.",pull-request-available,"['C++', 'FlightRPC', 'Python']",ARROW,Task,Major,2021-03-22 21:52:36,0
13366783,[Rust] [Parquet] Write fixed size binary arrays,"We already write FSB when writing binary arrays, so this extends the support by removing unimplemented code paths",pull-request-available,['Rust'],ARROW,Sub-task,Major,2021-03-22 16:59:08,12
13366155,[Rust] [Parquet] Update README for 2.6.0 support,"The Parquet README still talks about supporting 2.4.0, with a TODO for 2.5.0.
When the 2.6.0 support is completed, we can update the README.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2021-03-18 18:04:40,12
13365870,[C++] Move csv::ReadOptions::skip_rows to csv::ParseOptions::skip_rows,"This properly affects how the CSV is parsed and in turn the schema and data, so it belongs in ParseOptions. This means it can also be configured in Datasets.

For now, we'll duplicate the field and deprecate the original spot.",dataset datasets,['C++'],ARROW,Task,Major,2021-03-17 15:07:15,0
13365724,[Java] Support parallel vector element search with user-specified comparator,"This is in response to the discussion in https://github.com/apache/arrow/pull/5631#discussion_r339110228

Currently, we only support parallel search with {{RangeEqualsVisitor}}, which does not support user-specified comparators.
We want to provide the functionality in this issue to support wider range of use cases. ",pull-request-available,['Java'],ARROW,New Feature,Major,2021-03-17 06:51:56,7
13365666,[R] Make r/configure run successfully on Solaris,Replace some {{$()}} with backticks and use {{sed}} in a safe way,pull-request-available,['R'],ARROW,Sub-task,Major,2021-03-17 00:50:45,4
13365540,[Python] ImportError calling pyarrow from_pandas within ThreadPool,"From https://github.com/dask/dask/issues/7334

The referenced issue report is about an ImportError they get using Python 3.9 (and I can reproduce it). As far as I know how dask works, it's basically calling `pa.Table.from_pandas` within a ThreadPool, and inside `from_pandas` we do a `with futures.ThreadPoolExecutor`, which then fails with this error:

{code}
>>> df2.to_parquet('test99.parquet', engine='pyarrow-dataset')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/dataframe/core.py"", line 4127, in to_parquet
    return to_parquet(self, path, *args, **kwargs)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py"", line 671, in to_parquet
    out = out.compute(**compute_kwargs)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/base.py"", line 283, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/base.py"", line 565, in compute
    results = schedule(dsk, keys, **kwargs)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/threaded.py"", line 76, in get
    results = get_async(
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/local.py"", line 487, in get_async
    raise_exception(exc, tb)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/local.py"", line 317, in reraise
    raise exc
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/local.py"", line 222, in execute_task
    result = _execute_task(task, data)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/core.py"", line 121, in _execute_task
    return func(*(_execute_task(a, cache) for a in args))
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/utils.py"", line 35, in apply
    return func(*args, **kwargs)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py"", line 841, in write_partition
    t = cls._pandas_to_arrow_table(df, preserve_index=preserve_index, schema=schema)
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py"", line 814, in _pandas_to_arrow_table
    table = pa.Table.from_pandas(df, preserve_index=preserve_index, schema=schema)
  File ""pyarrow/table.pxi"", line 1479, in pyarrow.lib.Table.from_pandas
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/site-packages/pyarrow/pandas_compat.py"", line 596, in dataframe_to_arrays
    with futures.ThreadPoolExecutor(nthreads) as executor:
  File ""/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/concurrent/futures/__init__.py"", line 49, in __getattr__
    from .thread import ThreadPoolExecutor as te
ImportError: cannot import name 'ThreadPoolExecutor' from partially initialized module 'concurrent.futures.thread' (most likely due to a circular import) (/home/joris/miniconda3/envs/test-dask-pyarrow-bug/lib/python3.9/concurrent/futures/thread.py)
{code}

We can probably avoid that by moving the import top-level (not inline inside dataframe_to_arrays)",pull-request-available,['Python'],ARROW,Bug,Major,2021-03-16 14:40:41,5
13365519,[Rust] Donate Ballista Distributed Compute Platform,"See PR for details.

https://github.com/apache/arrow/pull/9723",pull-request-available,['Rust'],ARROW,New Feature,Major,2021-03-16 14:01:41,10
13365512,"[Python] Remove ""experimental"" status from Table.replace_schema_metadata","Table and RecordBatch {{replace_schema_metadata}} method's docstring has ""EXPERIMENTAL, but I don't think there is a need to label it like that any longer (the method has been there for multiple years)",pull-request-available,['Python'],ARROW,Improvement,Major,2021-03-16 13:16:56,5
13365484,[Rust] Combine limit into SortOptions,"The `sort_limit` kernel was added by @sundy-li in https://github.com/apache/arrow/pull/9602

While writing some doc examples in https://github.com/apache/arrow/pull/9721, it occured to me we could potentially simplify the API so I figured I would offer a proposed PR for comment

# Rationale

Since we already have a `SortOptions` structure that controls sorting options, we could also add the `limit` to that structure rather than adding a new `sort_limit` function and still avoid changing the API

# Changes

Move the `limit` option to `SortOptions`
",pull-request-available,[],ARROW,Improvement,Major,2021-03-16 11:12:15,11
13365468,[C++] Sporadic TSAN error in TestThreadPool.SetCapacity,"See https://github.com/ursacomputing/crossbow/runs/2114240297?check_suite_focus=true#step:6:3529

{code}
WARNING: ThreadSanitizer: data race (pid=5075)
  Write of size 8 at 0x7b3800000188 by main thread:
    #0 pthread_cond_destroy <null> (arrow-threading-utility-test+0x65035)
    #1 arrow::GatingTask::Impl::~Impl() /arrow/cpp/src/arrow/testing/gtest_util.cc:737:3 (libarrow_testing.so.400+0x2a974f)
[....]

  Previous read of size 8 at 0x7b3800000188 by thread T13:
    #0 pthread_cond_broadcast <null> (arrow-threading-utility-test+0x64ea2)
    #1 std::condition_variable::notify_all() <null> (libstdc++.so.6+0xd0e8c)
    #2 std::_Function_handler<void (), arrow::GatingTask::Impl::Task()::'lambda'()>::_M_invoke(std::_Any_data const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/bits/std_function.h:300:2 (libarrow_testing.so.400+0x2927aa)
    #3 std::function<void ()>::operator()() const /usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/bits/std_function.h:688:14 (arrow-threading-utility-test+0x23b296)
    #4 arrow::internal::FnOnce<void ()>::FnImpl<std::function<void ()> >::invoke() /arrow/cpp/src/arrow/util/functional.h:122:42 (arrow-threading-utility-test+0x23b207)
    #5 arrow::internal::FnOnce<void ()>::operator()() && /arrow/cpp/src/arrow/util/functional.h:110:17 (libarrow.so.400+0xf5f140)
    #6 arrow::internal::WorkerLoop(std::shared_ptr<arrow::internal::ThreadPool::State>, std::_List_iterator<std::thread>) /arrow/cpp/src/arrow/util/thread_pool.cc:108:11 (libarrow.so.400+0xfe7fcb)
[...]
{code}",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Minor,2021-03-16 09:56:35,2
13365452,[CI][GLib] Failed to update gcc,"https://github.com/apache/arrow/runs/2111869686?check_suite_focus=true#step:8:70

{noformat}
:: installing mingw-w64-x86_64-gcc (10.2.0-9) breaks dependency 'mingw-w64-x86_64-gcc=10.2.0-8' required by mingw-w64-x86_64-libgccjit
{noformat}",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Test,Major,2021-03-16 09:06:38,1
13365315,"[C++][Dataset] Extract IpcFragmentScanOptions, ParquetFragmentScanOptions",Follow-up to ARROW-9749.,dataset datasets pull-request-available,['C++'],ARROW,Task,Major,2021-03-15 18:18:17,0
13365270,[C++][CI] Fix Valgrind failures,"See errors in https://github.com/ursacomputing/crossbow/runs/2110197336 .
There are two sources of errors:
* GTest trying to print uninitialized memory when converting the CSV writer test params to a string
* The fast_float library sometimes reads uninitialized memory
",pull-request-available,['C++'],ARROW,Bug,Major,2021-03-15 15:43:10,2
13365265,[Rust][DataFusion] Improve Examples in documentation,"It would be cool to have an example on the main README.md of datafusion (that appears on the crates.io homepage) that shows a prospective user what DataFusion offers. 

e.g look at how tokio does it https://crates.io/crates/tokio)

I plan to lift the nice example from https://docs.rs/datafusion/3.0.0/datafusion/ ?",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-03-15 15:29:52,11
13365014,[GLib] GArrowChunkedArray: combine is missing,please add combine_chunks to Arrow::ChunkedArray classes.  This has been added separately to Python but would be good to have for all languages.,pull-request-available,['GLib'],ARROW,Improvement,Major,2021-03-13 21:51:33,1
13365011,[C++] Fix system re2 dependency detection for static library,{{re2Alt}} isn't listed in {{ARROW_SYSTEM_DEPENDENCIES}} in {{ArrowConfig.cmake}} even when we use system re2.,pull-request-available,['C++'],ARROW,Bug,Minor,2021-03-13 20:31:32,1
13364996,[C++] arrow/util/io_util.cc does not compile on Solaris,"Looks similar to ARROW-11740

{code}
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc: In function arrow::Status arrow::internal::MemoryMapRemap(void*, std::size_t, std::size_t, int, void**):
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1089:48: error: MREMAP_MAYMOVE was not declared in this scope
*new_addr = mremap(addr, old_size, new_size, MREMAP_MAYMOVE);
 ^
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1089:62: error: mremap was not declared in this scope
*new_addr = mremap(addr, old_size, new_size, MREMAP_MAYMOVE);
 ^
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc: In function arrow::Status arrow::internal::MemoryAdviseWillNeed(const std::vector&):
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1144:59: error: POSIX_MADV_WILLNEED was not declared in this scope
int err = posix_madvise(aligned.addr, aligned.size, POSIX_MADV_WILLNEED);
 ^
/export/home/XI4sjNd/Rtemp/RtmpvN4Lx2/fileef105d2909/cpp/src/arrow/util/io_util.cc:1144:78: error: posix_madvise was not declared in this scope
int err = posix_madvise(aligned.addr, aligned.size, POSIX_MADV_WILLNEED);
 ^
{code}",pull-request-available,['C++'],ARROW,Sub-task,Major,2021-03-13 17:49:35,2
13364957,[Rust] Make ArrayData --> GenericListArray fallable instead of `panic!`,"# Background:
Left over cleanups suggested by from @sunchao on  https://github.com/apache/arrow/pull/9425

Broken out from https://github.com/apache/arrow/pull/9508

# Rationale:

Rationale: don't use panic! directly (though the caller of this function still calls `unwrap()` so I am not sure how much of an improvement this change really is. However it sets us up for a more `safe` future eventually
",pull-request-available,['Rust'],ARROW,Improvement,Minor,2021-03-13 11:37:22,11
13364955,[Rust] Remove OffsetSize::prefix,"
Background:
Left over cleanups suggested by from @sunchao on  https://github.com/apache/arrow/pull/9425

Broken out from https://github.com/apache/arrow/pull/9508

Rationale:
This function is redundant with `OffsetSize::is_large`
",pull-request-available,['Rust'],ARROW,Improvement,Minor,2021-03-13 11:27:23,11
13364715,[R] filter doesn't accept negative numbers as valid,"Reprex with nyc-taxi data


{code:java}
R> library(arrow)
R> library(dplyr)
R> packageVersion(""arrow"")
[1] '3.0.0.9000'
R> 
 R> 
 R> open_dataset(""nyc-taxi"", partitioning = ""year"") %>% 
 filter(trip_distance >= -1) %>% 
 head(1) %>% 
 collect()
Error: Invalid: Function subtract_checked accepts 2 arguments but attempted to look up kernel(s) with 1
Backtrace:
 x
1. +-[ `%>%`(...) ]
2. +-[ dplyr::collect(...) ]
3. +-[ utils::head(...) ]
4. \-arrow:::head.arrow_dplyr_query(., 1)
5. \-arrow:::head.Dataset(x, n, ...)
6. \-Scanner$create(ensure_group_vars(x))
7. \-Scanner$create(...)
8. \-scanner_builder$Filter(filter)
9. \-arrow:::dataset___ScannerBuilder__Filter(self, expr){code}




",pull-request-available,['R'],ARROW,Bug,Major,2021-03-12 20:11:04,4
13364695,[Developer] Achery benchmark diff regression: cannot compare jsons,"see also ARROW-11189

{{archery benchmark diff}} should accept json files containing cached results from {{archery benchmark run}}.

A test should be added exercising this case and doc added to benchmarks.rst",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-03-12 17:00:17,6
13363938,[C++] GZip codec hangs if flushed twice,"{code:java}
    // ""If deflate returns with avail_out == 0, this function must be called
    //  again with the same value of the flush parameter and more output space
    //  (updated avail_out), until the flush is complete (deflate returns
    //  with non-zero avail_out).""
    return FlushResult{bytes_written, (bytes_written == 0)}; {code}
But contrary to the comment, we're checking bytes_written. So if we flush twice, the second time, we won't write any bytes, but we'll erroneously interpret that as zlib asking for a larger buffer, rather than zlib telling us there's no data to decompress. Then we'll enter a loop where we keep doubling the buffer size forever, hanging the program.",pull-request-available,['C++'],ARROW,Bug,Major,2021-03-11 21:03:35,0
13363924,[C++] Add push generator,"Sometimes a producer of values just wants to queue futures and let a consumer pop them iteratively.
",pull-request-available,['C++'],ARROW,Task,Major,2021-03-11 18:28:24,2
13363881,[C++] Provide ArrayBuilder::AppendScalar,"It would be useful to be able to append a Scalar (and/or ScalarVector) to an ArrayBuilder. For example, in https://github.com/apache/arrow/pull/9621#discussion_r587461083 (ARROW-11591) this could be used to accumulate an array of expected grouped aggregation results using existing scalar aggregate kernels",pull-request-available,['C++'],ARROW,Improvement,Minor,2021-03-11 15:19:50,0
13363668,[C++][Dataset][Compute] Refactor Dataset scans to use an ExecNode graph,"Per discussion on https://docs.google.com/document/d/1AyTdLU-RxA-Gsb9EsYnrQrmqPMOYMfPlWwxRi1Is1tQ

Once Datasets can be streamed from efficiently (ARROW-7001), they can be wrapped in ScanNodes which push scanned batches into an ExecNode graph.

Filtering and Projection should then be handled by subsequent nodes in that graph.",compute dataset pull-request-available,['C++'],ARROW,Improvement,Major,2021-03-10 22:53:56,6
13363666,[C++][Compute] Promote Expression to the compute namespace,"See discussion in https://docs.google.com/document/d/1AyTdLU-RxA-Gsb9EsYnrQrmqPMOYMfPlWwxRi1Is1tQ

This will be necessary for ARROW-11928 since some ExecNodes will be constructed with one or more Expressions. For example FilterNodes will evaluate an Expression against their input batches to produce selection vectors.

This should probably include factoring out dataset-specific considerations such as allowing input batches diverge slightly in schema. Bound Expressions should refer to their parameters by index so that by-name lookup can be avoided.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-03-10 22:40:06,6
13363665,[C++][Compute] Add ExecNode hierarchy,"Per discussion on https://docs.google.com/document/d/1AyTdLU-RxA-Gsb9EsYnrQrmqPMOYMfPlWwxRi1Is1tQ

Add an ExecNode interface with which a streaming execution graph can be constructed. Initial concrete classes will include:
- ScanNode, which wraps a dataset and is a pure emitter of batches (initially, this will only wrap memory sized datasets such as tables. See ARROW-11930)
- FilterNode, which evaluates an expression on inputs and based on the result removes rows from batches (eventually, this may defer materialization of the selection to other kernels. See ARROW-5005 ARROW-10474)
- ProjectNode, which evaluates expressions on inputs producing new columns.
- GroupedAggregateNode, which computes aggregations grouped on one or more keys.
",pull-request-available,['C++'],ARROW,Improvement,Major,2021-03-10 22:33:41,6
13363597,[C++] Provide streaming output from GetFileInfo,"For situations where a monolithic call to GetFileInfo will be slow, it would be useful to immediately receive any results which *are* ready through an {{AsyncGenerator<std::vector<FileInfo>>}} or so. This is probably a prerequisite of ARROW-8163, where the goal is to begin scanning known fragments while other fragments are still being discovered.

IIUC, one concrete example would be paging through a long output from S3's ListObjectsV2.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-03-10 16:06:03,2
13363580,[CI] Update branch name for dask dev integration tests,Nightly build is currently failing: https://github.com/ursacomputing/crossbow/runs/2074271628,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2021-03-10 14:59:01,5
13363119,[Packaging][Ubuntu] Drop support for 16.04,"https://help.ubuntu.com/community/EOL#Ubuntu_16.04_Xenial_Xerus

2021-04 is the EOL of Ubuntu 16.04.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2021-03-08 23:58:32,1
13363110,[C++] Get rid of MakeIteratorGenerator,MakeIteratorGenerator is a stop-gap measure while we convert readers to be asynchronous. It is not generally safe in async pipelines code (could use to nested deadlock) and should be removed once we stop using it as a stop-gap as it may encourage someone to use it in the future. This JIRA is a reminder to clean it up.,async-util pull-request-available,['C++'],ARROW,Sub-task,Major,2021-03-08 22:22:06,0
13363104,[Rust] Intermittent Flight integration test failures,"This is similar to the symptoms seen in ARROW-11717 but it is still happening  intermittently

On two separate PR I see similar failures:
https://github.com/apache/arrow/pull/9645/checks?check_run_id=2052183132
https://github.com/apache/arrow/pull/9647/checks?check_run_id=2051946608

Example failure:

{code}

subprocess.CalledProcessError: Command '['/build/cpp/debug/flight-test-integration-client', '-host', 'localhost', '-port=41743', '-scenario', 'auth:basic_proto']' died with <Signals.SIGABRT: 6>.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/arrow/dev/archery/archery/integration/runner.py"", line 308, in _run_flight_test_case
    consumer.flight_request(port, **client_args)
  File ""/arrow/dev/archery/archery/integration/tester_cpp.py"", line 116, in flight_request
    run_cmd(cmd)
  File ""/arrow/dev/archery/archery/integration/util.py"", line 148, in run_cmd
    raise RuntimeError(sio.getvalue())
RuntimeError: Command failed: /build/cpp/debug/flight-test-integration-client -host localhost -port=41743 -scenario auth:basic_proto
With output:
--------------
-- Arrow Fatal Error --
Invalid: Expected UNAUTHENTICATED but got Unavailable

--------------

################# FAILURES #################
FAILED TEST: auth:basic_proto Rust producing,  C++ consuming

{code}",pull-request-available,"['Integration', 'Rust']",ARROW,Bug,Major,2021-03-08 21:51:39,11
13363081,[C++] Use our own executor in S3FileSystem,"We use the AWS SDK async APIs in some places in S3FileSystem, but all they do is spawn a separate thread and run the sync API in it.

Instead we should use our IO executor, which would:
1) put an upper bound on the number of threads started
2) (presumably) reduce latency by reusing threads instead of spawning a throwaway thread for each async call
3) allow for cancellation
",pull-request-available,['C++'],ARROW,Task,Major,2021-03-08 19:22:51,2
13363028,[C++] SIMD info always returning none on MacOS,"I'm helping with ARROW-11507 and in testing it I noticed something odd, I'm getting none and none for both simd_level and detected_simd_level which seems odd. I also tried it in pyarrow (below) and I'm getting the same thing (this was built off of HEAD of apache/arrow) so I suspect that this is something lower than either of them (and in both {{runtime_info}} is a think wrapper around the c++).

{code:python}
>>> import pyarrow as pa
>>> pa.runtime_info()
RuntimeInfo(simd_level='none', detected_simd_level='none')
{code}

I do see the following when building:

{code}
--   ARROW_SIMD_LEVEL=SSE4_2 [default=NONE|SSE4_2|AVX2|AVX512]
--       Compile-time SIMD optimization level
--   ARROW_RUNTIME_SIMD_LEVEL=MAX [default=NONE|SSE4_2|AVX2|AVX512|MAX]
--       Max runtime SIMD optimization level
and sse/avx etc are built:

-- Performing Test CXX_SUPPORTS_SSE4_2
-- Performing Test CXX_SUPPORTS_SSE4_2 - Success
-- Performing Test CXX_SUPPORTS_AVX2
-- Performing Test CXX_SUPPORTS_AVX2 - Success
-- Performing Test CXX_SUPPORTS_AVX512
-- Performing Test CXX_SUPPORTS_AVX512 - Success
{code}

I've also tried confirming that ARROW_USER_SIMD_LEVEL is unset, and tried setting it explicitly:

{code}
(pyarrow-source) jkeane@het python % ARROW_USER_SIMD_LEVEL=sse4_2 python
Python 3.9.2 (default, Feb 24 2021, 13:26:09)
[Clang 12.0.0 (clang-1200.0.32.29)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyarrow as pa
>>> pa.runtime_info()
RuntimeInfo(simd_level='none', detected_simd_level='none')
{code}


I've tested it on two macOS machines (both running on intel chips) and both had the same behavior above. This appears to work on linux machines just fine.",pull-request-available,['C++'],ARROW,Bug,Major,2021-03-08 14:33:55,2
13362913,[Java] Refactor the compression codec implementation into core/Arrow specific parts,"This issue is in response to the discussion in https://github.com/apache/arrow/pull/8949/files#r588049088

We want to refactor the compression codec related code into two parts: one for the core compression logic, and the other for Arrow specific logic.

This will make it easier to support other compression types. ",pull-request-available,['Java'],ARROW,Improvement,Major,2021-03-08 04:53:49,7
13362839,[Rust] Pretty print columns,"We can pretty print a slice of record batches, but it's also useful to pretty print a slice of columns.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-03-07 09:35:31,12
13362772,[Rust] Hang / failure in CI on AMD64 Debian 10 Rust stable test workspace,"As observed first by [~nevi_me] on https://github.com/apache/arrow/pull/9592#issuecomment-791901636

The Rust CI tests seem to be failing due to a timeout, due to a timeout . For example: https://github.com/apache/arrow/runs/2045186826 

Data from [~nevi_me]


{code}
running 17 tests
test src/arrow/array_reader.rs - arrow::array_reader::StructArrayReader::next_batch (line 958) ... ignored
test src/arrow/array_reader.rs - arrow::array_reader::StructArrayReader::next_batch (line 964) ... ignored
test src/arrow/array_reader.rs - arrow::array_reader::StructArrayReader::next_batch (line 969) ... ignored
test src/arrow/mod.rs - arrow (line 26) ... ok
test src/column/mod.rs - column (line 38) ... ok
test src/compression.rs - compression (line 25) ... ok
test src/file/mod.rs - file (line 29) ... ok
test src/file/mod.rs - file (line 64) ... ok
test src/file/mod.rs - file (line 81) ... ok
test src/file/statistics.rs - file::statistics (line 23) ... ok
test src/file/properties.rs - file::properties (line 22) ... ok
test src/record/api.rs - record::api::Row::get_column_iter (line 62) ... ok
test src/schema/mod.rs - schema (line 22) ... ok
test src/schema/parser.rs - schema::parser (line 24) ... ok
{code}

On my machine, this is the last set of tests that run

{code}
test src/schema/types.rs - schema::types::ColumnPath::string (line 540) ... ok
test src/file/statistics.rs - file::statistics (line 23) ... ok
test src/schema/parser.rs - schema::parser (line 24) ... ok
test src/schema/mod.rs - schema (line 22) ... ok
test src/schema/printer.rs - schema::printer (line 23) ... ok
{code}

",pull-request-available,['Rust'],ARROW,Bug,Major,2021-03-06 12:32:19,11
13362709,[Rust][DataFusion] Fix Clippy Lint,"A linter error has appeared on master somehow:

```
error: unnecessary parentheses around `for` iterator expression
   --> datafusion/src/physical_plan/merge.rs:124:31
    |
124 |                 for part_i in (0..input_partitions) {
    |                               ^^^^^^^^^^^^^^^^^^^^^ help: remove these parentheses
    |
    = note: `-D unused-parens` implied by `-D warnings`

error: aborting due to previous error

Seen on these PRs:

https://github.com/apache/arrow/pull/9612/checks?check_run_id=2042047472

https://github.com/apache/arrow/pull/9639/checks?check_run_id=2042649120",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-03-05 21:37:54,11
13362676,[C++] Add initial microbenchmarks for Dataset internals,"A quick investigation of ARROW-11781 showed much of the overhead lies in evaluating partition expressions against the filter. While much of this is just kernel evaluation, we should have benchmarks of key Datasets internals like SimplifyWithGuarantee.",benchmark benchmarks dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-03-05 18:46:39,0
13362578,[Dev] Automatically run merge script in venv,"Running the merge script needs some additional boilerplate for people who don't already use a Conda environment or similar.

We could provide a wrapper script {{dev/merge_arrow_pr.sh}} that:
1) checks if a virtual env {{dev/.venv}} exists
2) if not, creates it and installs the required dependencies
3) runs the Python merge script in the virtual env
",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-03-05 11:14:09,2
13362389,[Rust] [DataFusion] Add DataFusion logos,"Issue automatically created from Pull Request [9630|https://github.com/apache/arrow/pull/9630]
I don't think this needs a JIRA?

These are the DataFusion logos that I had created before the project was donated to Apache Arrow. They weren't part of the source code repo so didn't get donated at the time.

<img width=""330"" alt=""DataFusion-Logo-Light"" src=""https://user-images.githubusercontent.com/934084/109990656-d55ddf80-7cc6-11eb-8bbc-f21946fd1dfc.png"">

<img width=""330"" alt=""DataFusion-Logo-Dark"" src=""https://user-images.githubusercontent.com/934084/109990665-d68f0c80-7cc6-11eb-891c-bf367cb5f447.png"">
",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-03-04 18:52:36,10
13362368,[GLib] GArrowArray: concatenate is missing,please add concat_arrays to Arrow::Array classes. This has been added separately to Python but would be good to have for all languages.,pull-request-available,['GLib'],ARROW,Improvement,Major,2021-03-04 18:21:28,1
13362331,[GLib] Gandiva Filter in GLib,I was trying to use Gandiva under ruby.  I was looking able to get a Projection working because that is annotated.  I was trying to do a Gandiva filter but this doesn't seem to be available with Gandiva. It is not listed in the Glib documentation.  Thanks,pull-request-available,['GLib'],ARROW,Improvement,Minor,2021-03-04 15:31:25,1
13361955,[C++] Add asynchronous read to parquet::arrow::FileReader,Allow reading,pull-request-available,['C++'],ARROW,Sub-task,Major,2021-03-02 20:35:44,0
13361908,[C++] Rewrite bit-unpacking optimizations using xsimd,"Currently, unpacking of fixed-size bit bundles uses custom generated code with SIMD intrinsics (the {{bpacking}} files in {{src/arrow/util}}).

By leveraging xsimd, we can make the SIMD generated code more generic (e.g. the 256-bit generated code would be able to compile with any 256-bit SIMD ISA supported by xsimd).",pull-request-available,['C++'],ARROW,Improvement,Minor,2021-03-02 16:26:19,2
13361902,[C++][Dataset] Expose originating fragment as a property of ScanTask,"This is generally useful for debugging and tracing ScanTasks, and can be used to remove the [explicit mapping in FileSystemDataset::Write|https://github.com/michalursa/arrow/commit/ae396b9d4c26621cba2cce955f1d55f43e8faab9#diff-2caf4e9bd3f139e05e55dca80725d8a9c436f5ccf65c76a37cebfa6ee9b36a6aR278]",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2021-03-02 16:07:14,0
13361653,[C++] gRPC compilation tests occur every time,"The gRPC compilation test results should probably be cached instead of being rerun every time {{cmake}} is invoked:
{code}
-- Checking support for TlsCredentialsOptions (gRPC >= 1.36)...
-- TlsCredentialsOptions (for gRPC 1.36) not found in grpc::experimental.
-- Checking support for TlsCredentialsOptions (gRPC >= 1.34)...
-- Found approximate gRPC version: 1.34 (ARROW_FLIGHT_REQUIRE_TLSCREDENTIALSOPTIONS=)
{code}
",pull-request-available,['C++'],ARROW,Bug,Trivial,2021-03-01 17:10:35,0
13361422,[Rust] [Parquet] Use logical types in Arrow writer,"Start using the logical type for Arrow <> Parquet schema conversion, so that we can support more Arrow types, like nanosecond temporal types.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2021-02-28 09:01:18,12
13361409,Support case sensitive for function,SELECT database() and SELECT DATABASE () are the same and can be queried normally,datafusion pull-request-available rust,['Rust'],ARROW,Improvement,Major,2021-02-28 07:30:38,11
13361369,[Rust] Add link to the doc,"Issue automatically created from Pull Request [9594|https://github.com/apache/arrow/pull/9594]
This is a test PR with a minor fix, that has no JIRA issue, to automatically create the issue",pull-request-available,['Rust'],ARROW,Bug,Major,2021-02-27 14:03:55,11
13361346,[Developer] Add option to auto-create JIRA issue for PRs which don't have it,"Improve dev workflow by automatically creating JIRA tickets if requested, as discussed here;
https://lists.apache.org/thread.html/rd4533c7f882adbfc51061aceafebe8d84ea194fa5108d6cebc3621e1%40%3Cdev.arrow.apache.org%3E",pull-request-available,[],ARROW,Improvement,Major,2021-02-27 10:40:39,11
13361325,[Rust] [Parquet] Support v2 LogicalType,"We currently do not read nor write the version 2 logical types. This is mainly because we do not have a mapping for it from parquet-format-rs.

To implement this, we can:
- convert ""parquet::basic::LogicalType"" to ""parquet::basic::ConvertedType""
- implement ""parquet::basic::LogicalType"" which mirrors ""parquet_format::LogicalType""
- create a mapping between ConvertedType and LogicalType
- write LogicalType to ""parquet_format::SchemaElement"" if v2 of the writer is used

This would be a good starting point for implementing 2.6 types (UUID, NANOS precision time & timestamp).
Follow-up work would be:
- parsing v2 of the schema [ARROW-11365]
- Using v2 in the Arrow writer (mostly schema conversion)
- Supporting nanosecond precision time & timestamp",pull-request-available,['Rust'],ARROW,Sub-task,Major,2021-02-27 08:41:36,12
13361280,[Rust][DataFusion] Mixing of crossbeam channel and async tasks can lead to deadlock,"[~edrevo] noticed, on https://github.com/apache/arrow/pull/9523#issuecomment-786237494, that the use of crossbeam channels can potentially deadlock datafusion

The use of crossbeam channel is left over from earlier, non `async` implementations and get been fingered in some hangs that [~MikeSeddonAU] has observed in DataFusion ). Specifically the crossbeam channel can block a thread when the channel is full or empty, which can result in blocking all the tokio executor threads and deadlocking the system

The proposal is is to use tokio's mpsc channels instead of crossbeam which can properly yield back to tokio to run another task when the channel is either full or empty. .
",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-02-26 21:39:45,11
13361247,[Integration] Update testing submodule,"Updates submodule after ARROW-11666, and removes references to files that no longer exist (generated_large_batch)",pull-request-available,['Integration'],ARROW,Task,Major,2021-02-26 18:56:01,12
13361234,[C++][Dataset] Provide Scanner methods to yield/visit scanned batches,"From discussion in https://issues.apache.org/jira/browse/ARROW-11782

It'd be useful to consumers of Scanner to receive an iterator of scanned record batches or apply a visitor to batches as they are scanned without handling ScanTasks. For example, this could enable aggregations or other computations which don't require the entire table to be materialized.",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2021-02-26 17:48:33,6
13361206,[Go] Add concurrent-safe ipc.FileReader.RecordAt(i),Arrow IPC files are safe to load concurrently. The implementation of `ipc.FileReader.Record(i)` is not safe due to stashing the current record internally. This adds a backward-compatible function `RecordAt` that behaves like ReadAt.,pull-request-available,['Go'],ARROW,Improvement,Minor,2021-02-26 14:59:14,13
13361057,[Rust][DataFusion] Change plan builder signature to take Vec<Expr> rather than &[Expr],"Another thing to do is to change the signagure of LogicalPlanBuilder
from taking slices of owned things &[Expr] to just taking Vec<Expr> entirely 

The rationale is that at all callsites you need to have an owned vec and Datafusion is going to copy anyways, so it would better to allow the caller to give up ownership",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2021-02-25 22:17:43,11
13361035,[C++] CMake output noisy,"This shouldn't appear in the default CMake configuration output:
{code}

Run Build Command(s):/usr/bin/ninja cmTC_d6806 && [1/2] Building CXX object CMakeFiles/cmTC_d6806.dir/check_tls_opts_136.cc.o
FAILED: CMakeFiles/cmTC_d6806.dir/check_tls_opts_136.cc.o 
/usr/bin/g++-9  -I/home/antoine/arrow/dev/cpp/thirdparty/flatbuffers/include -I/home/antoine/arrow/dev/cpp/build-test/jemalloc_ep-prefix/src -I/home/antoine/arrow/dev/cpp/build-test/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/include -I/home/antoine/arrow/dev/cpp/thirdparty/hadoop/include -I/home/antoine/arrow/dev/cpp/build-test/src -I/home/antoine/arrow/dev/cpp/src -I/home/antoine/arrow/dev/cpp/src/generated -isystem /home/antoine/miniconda3/envs/pyarrow/include -Wno-noexcept-type  -fdiagnostics-color=always -fuse-ld=gold -ggdb -O0  -Wall -Wno-conversion -Wno-deprecated-declarations -Wno-sign-conversion -Wno-unused-variable  -fno-semantic-interposition -msse4.2  -D_GLIBCXX_USE_CXX11_ABI=1 -D_GLIBCXX_USE_CXX11_ABI=1 -fno-omit-frame-pointer  -fPIE -pthread -std=c++11 -o CMakeFiles/cmTC_d6806.dir/check_tls_opts_136.cc.o -c /home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_136.cc
In file included from /usr/include/x86_64-linux-gnu/c++/9/bits/c++allocator.h:33,
                 from /usr/include/c++/9/bits/allocator.h:46,
                 from /usr/include/c++/9/memory:63,
                 from /home/antoine/miniconda3/envs/pyarrow/include/grpcpp/channel.h:22,
                 from /home/antoine/miniconda3/envs/pyarrow/include/grpcpp/grpcpp.h:53,
                 from /home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_136.cc:24:
/usr/include/c++/9/ext/new_allocator.h: In instantiation of 'void __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = grpc::experimental::TlsChannelCredentialsOptions; _Args = {}; _Tp = grpc::experimental::TlsChannelCredentialsOptions]':
/usr/include/c++/9/bits/alloc_traits.h:482:2:   required from 'static void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = grpc::experimental::TlsChannelCredentialsOptions; _Args = {}; _Tp = grpc::experimental::TlsChannelCredentialsOptions; std::allocator_traits<std::allocator<_Tp1> >::allocator_type = std::allocator<grpc::experimental::TlsChannelCredentialsOptions>]'
/usr/include/c++/9/bits/shared_ptr_base.h:548:39:   required from 'std::_Sp_counted_ptr_inplace<_Tp, _Alloc, _Lp>::_Sp_counted_ptr_inplace(_Alloc, _Args&& ...) [with _Args = {}; _Tp = grpc::experimental::TlsChannelCredentialsOptions; _Alloc = std::allocator<grpc::experimental::TlsChannelCredentialsOptions>; __gnu_cxx::_Lock_policy _Lp = __gnu_cxx::_S_atomic]'
/usr/include/c++/9/bits/shared_ptr_base.h:679:16:   required from 'std::__shared_count<_Lp>::__shared_count(_Tp*&, std::_Sp_alloc_shared_tag<_Alloc>, _Args&& ...) [with _Tp = grpc::experimental::TlsChannelCredentialsOptions; _Alloc = std::allocator<grpc::experimental::TlsChannelCredentialsOptions>; _Args = {}; __gnu_cxx::_Lock_policy _Lp = __gnu_cxx::_S_atomic]'
/usr/include/c++/9/bits/shared_ptr_base.h:1344:71:   required from 'std::__shared_ptr<_Tp, _Lp>::__shared_ptr(std::_Sp_alloc_shared_tag<_Tp>, _Args&& ...) [with _Alloc = std::allocator<grpc::experimental::TlsChannelCredentialsOptions>; _Args = {}; _Tp = grpc::experimental::TlsChannelCredentialsOptions; __gnu_cxx::_Lock_policy _Lp = __gnu_cxx::_S_atomic]'
/usr/include/c++/9/bits/shared_ptr.h:359:59:   required from 'std::shared_ptr<_Tp>::shared_ptr(std::_Sp_alloc_shared_tag<_Tp>, _Args&& ...) [with _Alloc = std::allocator<grpc::experimental::TlsChannelCredentialsOptions>; _Args = {}; _Tp = grpc::experimental::TlsChannelCredentialsOptions]'
/usr/include/c++/9/bits/shared_ptr.h:701:14:   required from 'std::shared_ptr<_Tp> std::allocate_shared(const _Alloc&, _Args&& ...) [with _Tp = grpc::experimental::TlsChannelCredentialsOptions; _Alloc = std::allocator<grpc::experimental::TlsChannelCredentialsOptions>; _Args = {}]'
/usr/include/c++/9/bits/shared_ptr.h:717:39:   required from 'std::shared_ptr<_Tp> std::make_shared(_Args&& ...) [with _Tp = grpc::experimental::TlsChannelCredentialsOptions; _Args = {}]'
/home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_136.cc:30:85:   required from here
/usr/include/c++/9/ext/new_allocator.h:145:20: error: no matching function for call to 'grpc::experimental::TlsChannelCredentialsOptions::TlsChannelCredentialsOptions()'
  145 |  noexcept(noexcept(::new((void *)__p)
      |                    ^~~~~~~~~~~~~~~~~~
  146 |        _Up(std::forward<_Args>(__args)...)))
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/credentials.h:31,
                 from /home/antoine/miniconda3/envs/pyarrow/include/grpcpp/create_channel.h:26,
                 from /home/antoine/miniconda3/envs/pyarrow/include/grpcpp/grpcpp.h:56,
                 from /home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_136.cc:24:
/home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/tls_credentials_options.h:197:12: note: candidate: 'grpc::experimental::TlsChannelCredentialsOptions::TlsChannelCredentialsOptions(std::shared_ptr<grpc::experimental::CertificateProviderInterface>)'
  197 |   explicit TlsChannelCredentialsOptions(
      |            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/tls_credentials_options.h:197:12: note:   candidate expects 1 argument, 0 provided
/home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/tls_credentials_options.h:195:7: note: candidate: 'grpc::experimental::TlsChannelCredentialsOptions::TlsChannelCredentialsOptions(const grpc::experimental::TlsChannelCredentialsOptions&)'
  195 | class TlsChannelCredentialsOptions final : public TlsCredentialsOptions {
      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/tls_credentials_options.h:195:7: note:   candidate expects 1 argument, 0 provided
/home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/tls_credentials_options.h:195:7: note: candidate: 'grpc::experimental::TlsChannelCredentialsOptions::TlsChannelCredentialsOptions(grpc::experimental::TlsChannelCredentialsOptions&&)'
/home/antoine/miniconda3/envs/pyarrow/include/grpcpp/security/tls_credentials_options.h:195:7: note:   candidate expects 1 argument, 0 provided
ninja: build stopped: subcommand failed.


-- TlsCredentialsOptions (for gRPC 1.36) not found in grpc::experimental.
-- Checking support for TlsCredentialsOptions (gRPC >= 1.34)...
-- Change Dir: /home/antoine/arrow/dev/cpp/build-test/src/arrow/flight/try_compile/CMakeFiles/CMakeTmp

Run Build Command(s):/usr/bin/ninja cmTC_a16f5 && [1/2] Building CXX object CMakeFiles/cmTC_a16f5.dir/check_tls_opts_134.cc.o
[2/2] Linking CXX executable cmTC_a16f5


{code}",pull-request-available,['C++'],ARROW,Bug,Minor,2021-02-25 19:19:56,0
13360963,[GLib][Ruby][Dataset] Remove bindings for internal classes,"GLib and ruby include bindings for internal classes such as ScanOptions, ScanContext, InMemoryScanTask, ScanTask, ... These are probably unnecessary and should be removed to present a simpler interface less prone to breakage under refactoring of the wrapped classes https://github.com/apache/arrow/pull/9532/checks?check_run_id=1974229719#step:8:2071",pull-request-available,"['GLib', 'Ruby']",ARROW,Improvement,Major,2021-02-25 14:32:31,1
13360833,[Rust] Allow json writer to write out JSON arrays as well as newline formatted objects,"
Currently the arrow json writer makes JSON that looks like this (one record per line):
```
{""foo"":1}
{""bar"":1}
```

Whereas a JSON array looks like this
```
[
  {""foo"":1},
  {""bar"":1}
]
```
It would be nice to write out json in a streaming fashion (we added such a feature in IOx via https://github.com/influxdata/influxdb_iox/pull/870/files)

/// Writes out well formed JSON arays in a streaming fashion
///
/// [{""foo"": ""bar""}, {""foo"": ""baz""}]
///
/// This is based on the arrow JSON writer (json::writer::Writer)
",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-02-24 22:30:48,11
13360727,[C++][CI] Make s390x build non-optional,"Now that big-endian support seems to stable (apart from Parquet), we should make the Travis-CI s390x job required, so that failures don't pass unnoticed.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2021-02-24 19:02:55,2
13360709,[C++] Scalar::hash may segfault for null scalars,"Scalar::Hash does not check Scalar::is_valid, which can result in inconsistent hashes when applied to uninitialized {{/.*Scalar::value/}} or may segfault (since null binary scalars may have a null buffer).",pull-request-available,['C++'],ARROW,Bug,Major,2021-02-24 18:10:33,6
13360638,[C++] Increase public API testing,"Currently, in {{public_api_test.cc}} we check various {{api.h}} files, but not all of them (since some of them depend on build options). We should ideally test all public APIs for private symbol leaks.",pull-request-available,['C++'],ARROW,Improvement,Minor,2021-02-24 14:38:59,2
13360434,[Python][Dataset] Add support for project expressions,ARROW-11174 adds support for arbitrary expressions in projected columns. This should be supported in the python binding as well,pull-request-available,['Python'],ARROW,Improvement,Major,2021-02-23 20:26:22,5
13360433,[C++][Dataset] Support projections between children of UnionDatasets,"ARROW-11174 adds support for arbitrary expressions in projection. It'd be useful to have that power when unifying disparate child datasets. For example, even the simple case where a column is renamed in a newer data file currently cannot be accommodated.",dataset,['C++'],ARROW,Improvement,Major,2021-02-23 20:12:57,6
13360364,[C++] Improve configurability of random data generation,"{{arrow::random::RandomArrayGenerator}} is useful for stress testing and benchmarking. Arrays of primitives can be generated with little boilerplate, however it is cumbersome to specify creation of nested arrays or record batches which are necessary for testing $n column operations such as group_by.

My ideal API for random generation takes only a FieldVector, a number of rows, and a seed as arguments. Other options (such as minimum, maximum, unique count, null probability, etc) are specified using field metadata so that they can be provided uniformly or granularly as necessary for a given test case:
{code:c++}
auto random_batch = Generate({
  field(""i32"", int32()), // i32 may take any value between INT_MAX and INT_MIN
                         // and will be null with default probability 0.01
  field(""f32"", float32(), false), // f32 will be entirely valid
  field(""probability"", float64(), true, key_value_metadata({
    // custom random generation properties:
    {""min"", ""0.0""},
    {""max"", ""1.0""},
    {""null_probability"", ""0.0001""},
  }),
  field(""list_i32"", list(
    field(""item"", int32(), true, key_value_metadata({
      // custom random generation properties can also be specified for nested fields:
      {""min"", ""0""},
      {""max"", ""1""},
    })
  )),
}, num_rows, 0xdeadbeef);
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2021-02-23 15:46:55,6
13360353,[C++] Add xsimd dependency,"See mailing-list discussion:
https://mail-archives.apache.org/mod_mbox/arrow-dev/202102.mbox/%3c909ed931-633c-1804-22fa-7fc480af6d15@arm.com%3e
",pull-request-available,['C++'],ARROW,Task,Major,2021-02-23 15:02:10,2
13360305,[C++] Decimal cast failure on big-endian,"This seems to be a regression, as shown on the Travis-CI s390x builds:
https://travis-ci.com/github/apache/arrow/jobs/485481635#L3100",pull-request-available,['C++'],ARROW,Bug,Major,2021-02-23 10:21:19,2
13360206,[C++] posix_memalign not declared in scope on Solaris,"{code}
[ 27%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/memory_pool.cc.o
/export/home/X4HzInm/Rtemp/Rtmp1Zx7Xc/file1f6372fd66ce/cpp/src/arrow/memory_pool.cc:In static member function static arrow::Status arrow::{anonymous}::SystemAllocator::AllocateAligned(int64_t, uint8_t**):
/export/home/X4HzInm/Rtemp/Rtmp1Zx7Xc/file1f6372fd66ce/cpp/src/arrow/memory_pool.cc:187:64:error: posix_memalignwas not declared in this scope
                                       static_cast<size_t>(size));
{code}",pull-request-available,['C++'],ARROW,Sub-task,Major,2021-02-23 00:00:47,4
13360196,[C++] Patch vendored xxhash for Solaris ,"It fails to compile, but interestingly just as I was looking into the error, I see that the issue has been fixed _today_ in xxhash: https://github.com/Cyan4973/xxHash/pull/498

So I think we just need to apply this patch.",pull-request-available,['R'],ARROW,Sub-task,Major,2021-02-22 22:47:53,4
13360192,[R] Allow string compute functions to be optional,"The Solaris build fails to build {{libarrow_bundled_dependencies.a}} because of some mismatch of arguments to the {{ar}} command: 

{code}
[ 19%] Bundling /export/home/XnknpBn/Rtemp/RtmpBOhxfH/file66df7a592ae4/release/libarrow_bundled_dependencies.a
gmake[2]: Entering directory '/export/home/XnknpBn/Rtemp/RtmpBOhxfH/file66df7a592ae4'
usage: ar -d[-SvV] archive file ...
       ar -m[-abiSvV] [posname] archive file ...
       ar -p[-vV][-sS] archive [file ...]
       ar -q[-cuvSV] [-abi] [posname] [file ...]
       ar -r[-cuvSV] [-abi] [posname] [file ...]
       ar -t[-vV][-sS] archive [file ...]
       ar -x[-vV][-sSCT] archive [file ...]
gmake[2]: *** [src/arrow/CMakeFiles/arrow_bundled_dependencies.dir/build.make:61: release/libarrow_bundled_dependencies.a] Error 1
{code}

If ARROW_PARQUET=OFF (ARROW-11735), the only dependencies to bundle are re2 and utf8proc. So we could either fix the {{ar}} invocation, or we could make re2 and utf8proc optional. Build-wise, they are optional, but we have some tests that call the string kernels, and we'd need to know that they should be skipped (i.e. another option in {{skip_if_not_available()}}.",pull-request-available,['R'],ARROW,Sub-task,Major,2021-02-22 21:47:34,4
13360157,[C++] Add implicit Future(Status) constructor for convenience,"When working with functions that return Future<T>, this would let us use macros like ARROW_RETURN_NOT_OK and ARROW_ASSIGN_OR_RAISE without having to manually construct and set a future.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-02-22 18:07:36,0
13359929,[C++] Namespace collisions with protobuf 3.15,We define {{pb}}as a namespace alias in the flight sources. This conflicts with {{protobuf}} starting to introduce it as its global namespace alias.,pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2021-02-21 21:04:44,8
13359788,[Integration] Intermittent (but frequent) flight integration failures with auth:basic_proto,"Link to discussion on list: 
https://lists.apache.org/thread.html/r0dcdc2b6334e7f067a828634cf7584406ed859ff4d3fb622fef1bdd7%40%3Cdev.arrow.apache.org%3E

I noticed that the Rust/CPP integration tests are failing seemingly
intermittently on master (and on Rust PRs). The tests pass if they are re-run (enough)

There are several commits that  the little red `X` meaning that CI didn't
pass on master https://github.com/apache/arrow/commits/master

Here are some Some example CI runs that are failing
https://github.com/apache/arrow/runs/1935673508
https://github.com/apache/arrow/runs/1926705212

Here is another example:
https://github.com/apache/arrow/pull/9359/checks?check_run_id=1941967422

Example failure:
{code}

==========================================================
Testing file auth:basic_proto
==========================================================
Traceback (most recent call last):
  File ""/arrow/dev/archery/archery/integration/util.py"", line 139, in run_cmd
    output = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
  File ""/opt/conda/envs/arrow/lib/python3.8/subprocess.py"", line 411, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File ""/opt/conda/envs/arrow/lib/python3.8/subprocess.py"", line 512, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/build/cpp/debug/flight-test-integration-client', '-host', 'localhost', '-port=33569', '-scenario', 'auth:basic_proto']' died with <Signals.SIGABRT: 6>.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/arrow/dev/archery/archery/integration/runner.py"", line 308, in _run_flight_test_case
    consumer.flight_request(port, **client_args)
  File ""/arrow/dev/archery/archery/integration/tester_cpp.py"", line 116, in flight_request
    run_cmd(cmd)
  File ""/arrow/dev/archery/archery/integration/util.py"", line 148, in run_cmd
    raise RuntimeError(sio.getvalue())
RuntimeError: Command failed: /build/cpp/debug/flight-test-integration-client -host localhost -port=33569 -scenario auth:basic_proto
With output:
--------------
-- Arrow Fatal Error --
Invalid: Expected UNAUTHENTICATED but got Unavailable
{code}
",pull-request-available,['Integration'],ARROW,Bug,Major,2021-02-20 13:51:57,11
13359772,[Rust][DataFusion] Implement ExprRewriter to avoid tree traversal redundancy,"
The idea is to
1. Reduce the amount repetitions in optimizer rules to make them easier to implement

2. Reduce the amount of repetition to make it easier to see the actual logic (rather than having it intertwined with the code needed to do recursion)

2. Set the stage for a more general `PlanRewriter` that doesn't have  to clone its input, and  can modify take their input by value and consume them

Plan is to make an ExprRewriter, the mutable counterpart to `ExpressionVisitor` and demonstrates its usefulness by rewriting several expression transformation rewrite passes using it.",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2021-02-20 10:46:49,11
13359771,[Rust][DataFusion] Move `expressions` and `inputs` into LogicalPlan rather than helpers in util,"
 move `expressions` and `inputs` into LogicalPlan rather than helpers in util, and use Visitor rather than hard coded list

Goal is to consolidate the expression walking in one place",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2021-02-20 10:44:59,11
13359708,[R] Wire up dplyr::mutate() for datasets,"https://github.com/apache/arrow/pull/9532/files#diff-621536af77a522e4dd62ede27eba1da5ab533dc6565140c498f86824b25b8cb3R209: 

{code}
  /// \brief Set expressions which will be evaluated to produce the materialized columns.
  ///
  /// Columns which are not referenced may not be loaded from disk.
  ///
  /// \param[in] exprs expressions to evaluate to produce columns.
  /// \param[in] names list of names for the resulting columns.
  ///
  /// \return Failure if any referenced column does not exists in the dataset's
  ///         Schema.
  Status Project(std::vector<Expression> exprs, std::vector<std::string> names);
{code}",pull-request-available,['R'],ARROW,New Feature,Major,2021-02-19 22:12:23,4
13359639,[R] Internationalize error handling in tidy eval,"We have 

{code}
  tryCatch(eval_tidy(expr, mask), error = function(e) {
    # Look for the cases where bad input was given, i.e. this would fail
    # in regular dplyr anyway, and let those raise those as errors;
    # else, for things not supported by Arrow return a ""try-error"",
    # which we'll handle differently
    msg <- conditionMessage(e)
    # TODO: internationalization?
    if (grepl(""object '.*'.not.found"", msg)) {
      stop(e)
    }
    if (grepl('could not find function "".*""', msg)) {
      stop(e)
    }
    invisible(structure(msg, class = ""try-error"", condition = e))
  })
{code}

and tests for this behavior, but the tests are skipped because they only match correctly in an English locale because these base R messages are translated.

We can generate these regular expressions dynamically by triggering the R errors on a known nonexistent object:

{code}
> tryCatch(X_____X, error = function(e) conditionMessage(e))
[1] ""object 'X_____X' not found""
> tryCatch(X_____X(), error = function(e) conditionMessage(e))
[1] ""could not find function \""X_____X\""""
> sub(""X_____X"", "".*"", tryCatch(X_____X, error = function(e) conditionMessage(e)))
[1] ""object '.*' not found""
{code}

And this will respect i18n:

{code}
> Sys.setenv(LANGUAGE=""FR_fr"")
> sub(""X_____X"", "".*"", tryCatch(X_____X, error = function(e) conditionMessage(e)))
[1] ""objet '.*' introuvable""
{code}

",pull-request-available,['R'],ARROW,New Feature,Minor,2021-02-19 17:57:14,4
13359555,[C++][FlightRPC][Packaging] Update support for disabling TLS server verification for recent gRPC versions,"It is regarding issue from github: [https://github.com/apache/arrow/issues/9525]

Output of `conda list`:
{code:java}
Name Version Build Channel
 abseil-cpp 20200923.3 h046ec9c_0 conda-forge
 aif360 0.3.0 pypi_0 pypi
 appdirs 1.4.4 pypi_0 pypi
 appnope 0.1.2 pypi_0 pypi
 arrow-cpp 3.0.0 py36h25f3d33_3_cpu conda-forge
 astunparse 1.6.3 pypi_0 pypi
 attrs 20.3.0 pypi_0 pypi
 aws-c-cal 0.4.5 hf7813a8_6 conda-forge
 aws-c-common 0.4.67 hbcf498f_0 conda-forge
 aws-c-event-stream 0.2.6 h8218164_4 conda-forge
 aws-c-io 0.8.3 h339dee7_1 conda-forge
 aws-checksums 0.1.11 h339dee7_1 conda-forge
 aws-sdk-cpp 1.8.138 h5307d9a_1 conda-forge
 backcall 0.2.0 pypi_0 pypi
 bayesian-optimization 1.2.0 pypi_0 pypi
 black 19.10b0 pypi_0 pypi
 boto3 1.17.9 pypi_0 pypi
 botocore 1.20.9 pypi_0 pypi
 brotli 1.0.9 h046ec9c_4 conda-forge
 bzip2 1.0.8 hc929b4f_4 conda-forge
 c-ares 1.17.1 hc929b4f_0 conda-forge
 ca-certificates 2020.12.5 h033912b_0 conda-forge
 cached-property 1.5.2 pypi_0 pypi
 category-encoders 2.1.0 pypi_0 pypi
 certifi 2020.12.5 py36h79c6626_1 conda-forge
 chardet 3.0.4 pypi_0 pypi
 click 7.1.2 pypi_0 pypi
 cycler 0.10.0 pypi_0 pypi
 cython 0.29.21 pypi_0 pypi
 decorator 4.4.2 pypi_0 pypi
 docutils 0.15.2 pypi_0 pypi
 flask 1.1.2 pypi_0 pypi
 future 0.18.2 pypi_0 pypi
 gflags 2.2.2 hb1e8313_1004 conda-forge
 glog 0.4.0 hb7f4fc5_3 conda-forge
 greenery 3.1 pypi_0 pypi
 grpc-cpp 1.35.0 h330f241_0 conda-forge
 grpcio 1.35.0 pypi_0 pypi
 h5py 2.10.0 pypi_0 pypi
 hpsklearn 0.1.0 pypi_0 pypi
 hyperopt 0.1.2 pypi_0 pypi
 ibm-cos-sdk 2.7.0 pypi_0 pypi
 ibm-cos-sdk-core 2.7.0 pypi_0 pypi
 ibm-cos-sdk-s3transfer 2.7.0 pypi_0 pypi
 ibm-watson-machine-learning 1.0.45 pypi_0 pypi
 idna 2.9 pypi_0 pypi
 imageio 2.9.0 pypi_0 pypi
 ipython 7.16.1 pypi_0 pypi
 ipython-genutils 0.2.0 pypi_0 pypi
 itsdangerous 1.1.0 pypi_0 pypi
 jedi 0.18.0 pypi_0 pypi
 jinja2 2.11.3 pypi_0 pypi
 jmespath 0.9.5 pypi_0 pypi
 joblib 1.0.1 pyhd8ed1ab_0 conda-forge
 jsonschema 2.6.0 pypi_0 pypi
 jsonsubschema 0.0.1 pypi_0 pypi
 keras 2.3.1 pypi_0 pypi
 keras-applications 1.0.8 pypi_0 pypi
 keras-preprocessing 1.1.2 pypi_0 pypi
 kiwisolver 1.3.1 pypi_0 pypi
 krb5 1.17.1 hddcf347_0 
 lale 0.4.13 pypi_0 pypi
 liac-arff 2.5.0 pypi_0 pypi
 libblas 3.9.0 8_openblas conda-forge
 libcblas 3.9.0 8_openblas conda-forge
 libcurl 7.71.1 h9bf37e3_8 conda-forge
 libcxx 11.0.1 habf9029_0 conda-forge
 libcxxabi 4.0.1 hcfea43d_1 
 libedit 3.1.20181209 hb402a30_0 
 libev 4.33 haf1e3a3_1 conda-forge
 libevent 2.1.10 hddc9c9b_3 conda-forge
 libffi 3.2.1 h475c297_4 
 libgfortran 5.0.0 9_3_0_h6c81a4c_18 conda-forge
 libgfortran5 9.3.0 h6c81a4c_18 conda-forge
 liblapack 3.9.0 8_openblas conda-forge
 libnghttp2 1.43.0 h07e645a_0 conda-forge
 libopenblas 0.3.12 openmp_h54245bb_1 conda-forge
 libprotobuf 3.14.0 hfd3ada9_0 conda-forge
 libssh2 1.9.0 h8a08a2b_5 conda-forge
 libthrift 0.13.0 h990abc0_6 conda-forge
 libutf8proc 2.6.1 h35c211d_0 conda-forge
 lightgbm 2.2.3 pypi_0 pypi
 llvm-openmp 11.0.1 h7c73e74_0 conda-forge
 lomond 0.3.3 pypi_0 pypi
 lxml 4.5.1 pypi_0 pypi
 lz4-c 1.9.3 h046ec9c_0 conda-forge
 markupsafe 1.1.1 pypi_0 pypi
 matplotlib 3.3.4 pypi_0 pypi
 minepy 1.2.5 pypi_0 pypi
 mock 4.0.3 pypi_0 pypi
 ncurses 6.2 h0a44026_0 
 networkx 2.4 pypi_0 pypi
 nose 1.3.7 pypi_0 pypi
 numexpr 2.7.2 pypi_0 pypi
 numpy 1.18.1 pypi_0 pypi
 opencv-python 4.5.1.48 pypi_0 pypi
 openml 0.9.0 pypi_0 pypi
 openssl 1.1.1j hbcf498f_0 conda-forge
 orc 1.6.7 h675e114_0 conda-forge
 pandas 0.25.3 pypi_0 pypi
 parquet-cpp 1.5.1 2 conda-forge
 parso 0.8.1 pypi_0 pypi
 pathspec 0.8.1 pypi_0 pypi
 patsy 0.5.1 pypi_0 pypi
 pexpect 4.8.0 pypi_0 pypi
 pickleshare 0.7.5 pypi_0 pypi
 pillow 8.1.0 pypi_0 pypi
 pip 20.0.2 py36_1 
 ply 3.11 pypi_0 pypi
 prompt-toolkit 3.0.16 pypi_0 pypi
 psutil 5.8.0 pypi_0 pypi
 ptyprocess 0.7.0 pypi_0 pypi
 pyarrow 3.0.0 py36h02a7306_3_cpu conda-forge
 pygments 2.8.0 pypi_0 pypi
 pymongo 3.10.1 pypi_0 pypi
 pyomo 5.7.3 pypi_0 pypi
 pyparsing 2.4.7 pypi_0 pypi
 python 3.6.10 h359304d_0 
 python-dateutil 2.8.1 pypi_0 pypi
 python-graphviz 0.13.2 pypi_0 pypi
 python-intervals 1.10.0 pypi_0 pypi
 python_abi 3.6 1_cp36m conda-forge
 pytz 2019.3 pypi_0 pypi
 pyutilib 6.0.0 pypi_0 pypi
 pywavelets 1.1.1 pypi_0 pypi
 pyyaml 5.4.1 pypi_0 pypi
 rbfopt 4.2.2 pypi_0 pypi
 re2 2020.11.01 h2e338ed_0 conda-forge
 readline 7.0 h1de35cc_5 
 regex 2020.11.13 pypi_0 pypi
 requests 2.23.0 pypi_0 pypi
 s3transfer 0.3.4 pypi_0 pypi
 scikit-image 0.17.2 pypi_0 pypi
 scikit-learn 0.20.3 pypi_0 pypi
 scipy 1.4.1 pypi_0 pypi
 seaborn 0.11.1 pypi_0 pypi
 setuptools 45.2.0 py36_0 
 six 1.14.0 pypi_0 pypi
 snappy 1.1.8 hb1e8313_3 conda-forge
 sqlite 3.31.1 ha441bb4_0 
 statsmodels 0.11.1 pypi_0 pypi
 tables 3.5.1 pypi_0 pypi
 tabulate 0.8.6 pypi_0 pypi
 threadpoolctl 2.1.0 pypi_0 pypi
 tifffile 2020.9.3 pypi_0 pypi
 tk 8.6.8 ha441bb4_0 
 toml 0.10.2 pypi_0 pypi
 tqdm 4.36.1 pypi_0 pypi
 traitlets 4.3.3 pypi_0 pypi
 typed-ast 1.4.2 pypi_0 pypi
 urllib3 1.25.9 pypi_0 pypi
 watson-machine-learning-client-v4 1.0.159 pypi_0 pypi
 wcwidth 0.2.5 pypi_0 pypi
 werkzeug 1.0.1 pypi_0 pypi
 wheel 0.34.2 py36_0 
 wrapt 1.12.1 pypi_0 pypi
 xgboost 0.90 pypi_0 pypi
 xlrd 2.0.1 pypi_0 pypi
 xmltodict 0.12.0 pypi_0 pypi
 xz 5.2.5 haf1e3a3_1 conda-forge
 zlib 1.2.11 h1de35cc_3 
 zstd 1.4.8 hf387650_1 conda-forge
{code}
",pull-request-available,"['C++', 'FlightRPC', 'Packaging']",ARROW,Bug,Major,2021-02-19 09:32:40,0
13359500,[C++] Array Take may dereference absent null bitmap,"Sorting a non-chunked array has some sort of reference problem. Some operations on the resulting indices array crash.
{code:python}
import pyarrow as pa, pyarrow.compute as pc

array = pa.array(list(""abcba""))
assert pc.sort_indices(pa.chunked_array([array])).take([0])
assert pc.array_sort_indices(pa.chunked_array([array])).take([0])
pc.sort_indices(array).take([0])  # crash
pc.array_sort_indices(array).take([0])  # crash
{code}
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Critical,2021-02-19 04:01:42,2
13359118,[C++][Dataset] Write documentation,"The dataset component is currently undocumented. Documentation should be added in two parts:
* a page in the User Guide
* a page in the API reference
",pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Major,2021-02-17 17:42:30,0
13359115,[CI][C++] Resolve ctest failures on VS 2019 builds,"Running {{ctest}} on Windows after building the Arrow library with Visual Studio 2019, I regularly see these two tests fail:
 * {{TestStatisticsSortOrder/0.MinMax}}
 * {{TestStatistic.Int32Extremums}}

Logs look like this:
{code:java}
[ RUN      ] TestStatisticsSortOrder/0.MinMax
D:/a/crossbow/crossbow/arrow/cpp/src/parquet/statistics_test.cc(670): error: Expected equality of these values:
  stats_[i].min()
    Which is: ""\0\0\0\0""
  cc_metadata->statistics()->EncodeMin()
    Which is: ""\x3\0\0\0""
D:/a/crossbow/crossbow/arrow/cpp/src/parquet/statistics_test.cc(838): error: Expected: this->VerifyParquetStats() doesn't generate new fatal failures in the current thread.
  Actual: it does.
[  FAILED  ] TestStatisticsSortOrder/0.MinMax, where TypeParam = struct parquet::PhysicalType<1> (1 ms)
{code}
{code:java}
[ RUN      ] TestStatistic.Int32Extremums
D:/a/crossbow/crossbow/arrow/cpp/src/parquet/statistics_test.cc(900): error: Expected equality of these values:
  stats->min()
    Which is: -2147483648
  expected_min
    Which is: 0
[  FAILED  ] TestStatistic.Int32Extremums (0 ms)
{code}
This happened in the crossbow task*test-build-vcpkg-win*until I removed{{ctest}}from{{dev/tasks/vcpkg-tests/cpp-build-vcpkg.bat}}to suppress it:[https://github.com/ursacomputing/crossbow/actions?query=branch:actions-99-github-test-build-vcpkg-win]

This also happened when I ran the release verification script {{dev/release/verify-release-candidate.bat}}on 4.0.0 RC4 using conda for dependencies.

Diagnose and resolve these failures.",pull-request-available,"['C++', 'CI', 'Continuous Integration']",ARROW,Task,Major,2021-02-17 17:30:01,2
13359082,[Rust] [DataFusion] Remove concurrency field from GlobalLimitExec,GlobalLimitExec has an unused concurrency field that can now be removed.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2021-02-17 14:43:40,10
13359061,[Python] Document precision and scale parameters of decimal128(),"In pyarrow, the {{decimal128()}} factory takes two parameters, {{precision}} and {{scale}}.

It is unclear to me, what these parameters are and how to use them. Unfortunately, the wikipedia entry for Decimal128 does not help either.",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Minor,2021-02-17 13:07:48,2
13358980,[C++] Support sorting for decimal data type.,"Seems sorting on decimal datum is not implemented yet til 3.0.0.

Is there special reason not support it?

Is the implementation on roadmap?

Any workaround to sort on decimal datum so far?",pull-request-available,['C++'],ARROW,Wish,Major,2021-02-17 02:35:55,2
13358958,[R] Handle mutate/rename inside group_by,Followup to ARROW-11657,pull-request-available,['R'],ARROW,Bug,Major,2021-02-16 22:59:40,4
13358956,[R] group_by with .drop specified errors,cf. https://github.com/tidyverse/dplyr/issues/5763,pull-request-available,['R'],ARROW,Bug,Major,2021-02-16 22:39:14,4
13358812,[CI] Use docker buildkit's inline cache to reuse build cache across different hosts,"Docker buildkit supports cache inlining, which means that the cache manifest gets bundled in the produced docker image. This helps us to avoid cache misses on different hosts, greatly improving the image reusability (regarding the build speed).",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2021-02-16 08:18:12,3
13358705,[CI] [Documentation] Maven default skin not found,"The docs nightly build has been failing for a few days with:

{code}
2021-02-15T07:26:05.8699084Z [INFO] Arrow Algorithms ................................... SUCCESS [ 15.498 s]
2021-02-15T07:26:05.8700080Z [INFO] Arrow Performance Benchmarks 4.0.0-SNAPSHOT ........ FAILURE [15:45 min]
2021-02-15T07:26:05.8700992Z [INFO] ------------------------------------------------------------------------
2021-02-15T07:26:05.8701554Z [INFO] BUILD FAILURE
2021-02-15T07:26:05.8702879Z [INFO] ------------------------------------------------------------------------
2021-02-15T07:26:05.8703563Z [INFO] Total time: 29:46 min (Wall Clock)
2021-02-15T07:26:05.8704366Z [INFO] Finished at: 2021-02-15T07:26:05Z
2021-02-15T07:26:05.8705209Z [INFO] ------------------------------------------------------------------------
2021-02-15T07:26:05.8707032Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.3:site (default-site) on project arrow-performance: SiteToolException: ArtifactResolutionException: Unable to find skin: Could not transfer artifact org.apache.maven.skins:maven-default-skin:jar:1.0 from/to central (https://repo.maven.apache.org/maven2): Connection timed out (Read failed)
2021-02-15T07:26:05.8708593Z [ERROR]   org.apache.maven.skins:maven-default-skin:jar:1.0
2021-02-15T07:26:05.8709136Z [ERROR] 
2021-02-15T07:26:05.8709618Z [ERROR] from the specified remote repositories:
2021-02-15T07:26:05.8710313Z [ERROR]   apache.snapshots (https://repository.apache.org/snapshots, releases=false, snapshots=true),
2021-02-15T07:26:05.8711035Z [ERROR]   central (https://repo.maven.apache.org/maven2, releases=true, snapshots=false)
2021-02-15T07:26:05.8711775Z [ERROR] -> [Help 1]
2021-02-15T07:26:05.8712234Z [ERROR] 
2021-02-15T07:26:05.8712989Z [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
2021-02-15T07:26:05.8714323Z [ERROR] Re-run Maven using the -X switch to enable full debug logging.
2021-02-15T07:26:05.8714873Z [ERROR] 
2021-02-15T07:26:05.8715478Z [ERROR] For more information about the errors and possible solutions, please read the following articles:
2021-02-15T07:26:05.8716188Z [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
2021-02-15T07:26:05.8716717Z [ERROR] 
2021-02-15T07:26:05.8717187Z [ERROR] After correcting the problems, you can resume the build with the command
2021-02-15T07:26:05.8717946Z [ERROR]   mvn <goals> -rf :arrow-performance
2021-02-15T07:26:07.0376588Z 1
2021-02-15T07:26:07.1031165Z Error: `docker-compose --file /home/vsts/work/1/s/arrow/docker-compose.yml run --rm -e SETUPTOOLS_SCM_PRETEND_VERSION=3.1.0.dev183 ubuntu-docs` exited with a non-zero exit code 1, see the process log above.
{code}

And, indeed the 1.0 version of maven-default-skin is not at https://repository.apache.org/content/groups/snapshots/org/apache/maven/skins/maven-default-skin/ (though it does appear to be https://repo.maven.apache.org/maven2/org/apache/maven/skins/maven-default-skin/)",pull-request-available,"['CI', 'Continuous Integration', 'Documentation']",ARROW,Bug,Major,2021-02-15 17:37:53,3
13358506,[CI][Gandiva][Linux] Fix Crossbow setup failure,"https://github.com/ursacomputing/crossbow/runs/1892434027

{noformat}
Collecting cryptography>=2.3 (from jwcrypto>=0.5.0->github3.py)
  Downloading https://files.pythonhosted.org/packages/27/5a/007acee0243186123a55423d49cbb5c15cb02d76dd1b6a27659a894b13a2/cryptography-3.4.4.tar.gz (545kB)
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
    ModuleNotFoundError: No module named 'setuptools'
{noformat}",pull-request-available,"['C++ - Gandiva', 'Continuous Integration']",ARROW,Improvement,Major,2021-02-13 22:55:53,1
13358482,[Rust] [DataFusion] Inconsistent use of Box and Arc for TableProvider,The API inconsistently uses Box<TableProvider> and Arc<TableProvider> -- we should standardize on Arc,pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-02-13 15:09:10,11
13358421,[C++] Rebuild trimmed boost bundle for 1.75.0,"And host somewhere other than bintray. We can prune it further now that we've dropped boost::regex, too.

Look to see if we can drop more of thrift's requirements too since we've upgraded it to 0.13.",pull-request-available,['C++'],ARROW,Sub-task,Major,2021-02-12 21:16:17,4
13358419,[C++] Download boost from sourceforge instead of bintray,e.g. https://sourceforge.net/projects/boost/files/boost/1.67.0/boost_1_67_0.tar.gz,pull-request-available,['C++'],ARROW,Sub-task,Major,2021-02-12 21:13:11,4
13358350,[CI] turbodbc integration tests are failing (build isue),"Both turbodbc builds are failing, see eg https://github.com/ursacomputing/crossbow/runs/1885201762

It seems a failure to build turbodbc: 

{code}
/build/turbodbc /
-- The CXX compiler identification is GNU 9.3.0
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /opt/conda/envs/arrow/bin/x86_64-conda-linux-gnu-c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Build type: Debug
CMake Error at CMakeLists.txt:14 (add_subdirectory):
  add_subdirectory given source ""pybind11"" which is not an existing
  directory.


-- Found GTest: /opt/conda/envs/arrow/lib/libgtest.so  
-- Found Boost: /opt/conda/envs/arrow/include (found version ""1.74.0"") found components: locale 
-- Detecting unixODBC library
--   Found header files at: /opt/conda/envs/arrow/include
--   Found library at: /opt/conda/envs/arrow/lib/libodbc.so
-- Found Boost: /opt/conda/envs/arrow/include (found version ""1.74.0"") found components: system date_time locale 
-- Detecting unixODBC library
--   Found header files at: /opt/conda/envs/arrow/include
--   Found library at: /opt/conda/envs/arrow/lib/libodbc.so
-- Found Boost: /opt/conda/envs/arrow/include (found version ""1.74.0"") found components: system 
-- Detecting unixODBC library
--   Found header files at: /opt/conda/envs/arrow/include
--   Found library at: /opt/conda/envs/arrow/lib/libodbc.so
CMake Error at cpp/turbodbc_python/Library/CMakeLists.txt:3 (pybind11_add_module):
  Unknown CMake command ""pybind11_add_module"".


-- Configuring incomplete, errors occurred!
See also ""/build/turbodbc/CMakeFiles/CMakeOutput.log"".
See also ""/build/turbodbc/CMakeFiles/CMakeError.log"".
1
Error: `docker-compose --file /home/runner/work/crossbow/crossbow/arrow/docker-compose.yml run --rm -e SETUPTOOLS_SCM_PRETEND_VERSION=3.1.0.dev174 conda-python-turbodbc` exited with a non-zero exit code 1, see the process log above.
{code}

cc [~uwe]",pull-request-available,"['CI', 'Continuous Integration']",ARROW,Improvement,Major,2021-02-12 13:02:45,8
13358256,[Rust] [DataFusion] Need guidance on HashAggregateExec reconstruction,"We have run into an issue in the Ballista project where we are reconstructing the Final and Partial HashAggregateExec operators [1] for distributed execution and we need some guidance.

The Partial HashAggregateExec gets created OK and executes correctly.

However, when we create the Final HashAggregateExec, it is not finding the expected schema in the input operator. The partial exec outputs field names ending with ""[sum]"" and ""[count]"" and so on but the final aggregate doesn't seem to be looking for those names.

It is also worth noting that the Final and Partial executors are not connected directly in this usage.

The Partial exec is executed and output streamed to disk.

The Final exec then runs against the output from the Partial exec.

We may need to make changes in DataFusion to allow other crates to support this kind of use case?

[1] https://github.com/ballista-compute/ballista/pull/491

",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-02-12 01:37:52,10
13358228,[Rust] Clippy CI is failing,"CI uses ""stable"" rust
1.50 stable was updated today: https://blog.rust-lang.org/2021/02/11/Rust-1.50.0.html

The new clippy is pickier resulting in many clippy warnings such as https://github.com/apache/arrow/pull/9469/checks?check_run_id=1881854256

We need to get CI back green",pull-request-available,[],ARROW,Bug,Major,2021-02-11 20:04:22,11
13358223,[C++][Dataset] Expose pre-buffering in ParquetFileFormatReaderOptions,This can help performance on high-latency filesystems.,dataset datasets pull-request-available,['C++'],ARROW,Improvement,Major,2021-02-11 19:51:27,0
13358194,[Python][Dataset] SIGSEGV when executing scan tasks with Python executors,"This crashes for me with a segfault:
{code:python}
import concurrent.futures
import queue

import numpy as np
import pyarrow as pa
import pyarrow.dataset as ds
import pyarrow.fs as fs
import pyarrow.parquet as pq


schema = pa.schema([(""foo"", pa.float64())])
table = pa.table([np.random.uniform(size=1024)], schema=schema)
path = ""/tmp/foo.parquet""
pq.write_table(table, path)
dataset = pa.dataset.FileSystemDataset.from_paths(
    [path],
    schema=schema,
    format=ds.ParquetFileFormat(),
    filesystem=fs.LocalFileSystem(),
)

with concurrent.futures.ThreadPoolExecutor(2) as executor:
    tasks = dataset.scan()
    q = queue.Queue()

    def _prebuffer():
        for task in tasks:
            iterator = task.execute()
            next(iterator)
            q.put(iterator)

    executor.submit(_prebuffer).result()
    next(q.get())
{code}

{noformat}
$ uname -a
Linux chaconne 5.10.4-arch2-1 #1 SMP PREEMPT Fri, 01 Jan 2021 05:29:53 +0000 x86_64 GNU/Linux
$ pip freeze
numpy==1.20.1
pyarrow==3.0.0
{noformat}
",dataset datasets pull-request-available,['Python'],ARROW,Bug,Major,2021-02-11 16:28:23,0
13358172,[C++][NIGHTLY:test-conda-cpp-valgrind] GenerateBitsUnrolled triggers valgrind on uninit inputs,"https://github.com/ursacomputing/crossbow/runs/1877315066#step:6:2818

Comparison kernels generate an output bitmap for all array values, including those masked by a null bit. This should be fine since the indeterminate bits are also masked in the output but valgrind still triggers on the branching in GenerateBitsUnrolled.",pull-request-available,['C++'],ARROW,Bug,Minor,2021-02-11 14:47:22,6
13358153,[Rust] Support pretty printing with NullArrays,"

The whole point of `NullArray::new_with_type` is to to be able to cheaply construct entirely null columns, with a smaller memory footprint.

Currently trying to print them out causes a painic:

{code}
    #[test]
    fn test_pretty_format_null() -> Result<()> {
        // define a schema.
        let schema = Arc::new(Schema::new(vec![
            Field::new(""a"", DataType::Utf8, true),
            Field::new(""b"", DataType::Int32, true),
        ]));

        let num_rows = 4;

        // define data (null)
        let batch = RecordBatch::try_new(
            schema,
            vec![
                Arc::new(NullArray::new_with_type(num_rows, DataType::Utf8)),
                Arc::new(NullArray::new_with_type(num_rows, DataType::Int32)),
            ],
        )?;

        let table = pretty_format_batches(&[batch])?;
}

{code}

Panics:

{code}

failures:

---- util::pretty::tests::test_pretty_format_null stdout ----
thread 'util::pretty::tests::test_pretty_format_null' panicked at 'called `Option::unwrap()` on a `None` value', arrow/src/util/display.rs:201:27

{code}",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-02-11 11:55:43,11
13358005,[R] Add methods for modifying Schemas,"$<-, [[<-, and (probably) [<- methods. We have the extracting versions implemented but not the updating ones, and that would be useful.

Motivating use case: schema detection for a dataset misreads a column, so take the autodetected schema, modify one field, and then re-create the dataset with the correct schema.",pull-request-available,['R'],ARROW,New Feature,Major,2021-02-10 19:10:05,4
13357840,[Rust] Remove unused variable in example,"As shown in https://github.com/apache/arrow/commit/3a380a4c4193c6683a71ba72dc31f8456bc661d5
",pull-request-available,['Rust'],ARROW,Improvement,Trivial,2021-02-09 22:38:28,11
13357725,[CI] Cancel stale Github Actions workflow runs,"When a new changeset is pushed in a PR, we should probably cancel previous GHA builds in that PR. This is not done automatically by Github, but can be enabled using a third-party action:

https://github.com/potiuk/cancel-workflow-runs

Here is an example of use:

[https://github.com/apache/pulsar/blob/master/.github/workflows/ci-cancel-duplicate-workflows.yaml]

",pull-request-available,['Continuous Integration'],ARROW,Wish,Minor,2021-02-09 09:58:52,2
13357628,[FlightRPC][C++][Python] Interrupting a Flight server results in abort ,"SIGINT results in this error from gRPC/Abseil:
{noformat}
[mutex.cc : 2046] RAW: Check waitp == nullptr || waitp->thread->waitp == nullptr || waitp->thread->suppress_fatal_errors failed: detected illegal recursion into Mutex code {noformat}
The reason is this upstream bug report: [https://github.com/grpc/grpc/issues/24884]

gRPC functions are not async-signal-safe, but we directly call them from a signal handler. This happened to work before but is no longer working with our version of gRPC.",pull-request-available,"['C++', 'FlightRPC', 'Python']",ARROW,Bug,Major,2021-02-08 21:27:03,0
13357608,[C++] Improve flatbuffers verification limits,"See discussion in [https://github.com/apache/arrow/pull/9349#issuecomment-775295349] :

Flatbuffers is able to encode a virtually unbounded of schema fields in a small buffer size. Verifying that many fields with the Flatbuffers verifier seems to result in potentially unlimited verification times, which is a denial of service risk.

The way to mitigate this risk is to pass an appropriate max_tables and/or max_depth limit to the Flatbuffers verifier.",pull-request-available,['C++'],ARROW,Improvement,Major,2021-02-08 18:46:25,2
13357420,[C++] RandomArrayGenerator::List size mismatch ,"RandomArrayGenerator::List consistently produces ListArrays with their length 1 below what they should be according to their documentation. Moreover the bitmaps we have are weird.

Here is some simple test:

{color:#dcdcaa}TEST{color}(TestAdapterWriteNested, ListTest) {
{color:#569cd6}int64_t{color} num_rows = {color:#b5cea8}2{color};
{color:#569cd6}static{color} {color:#569cd6}constexpr{color} {color:#4ec9b0}random{color}::SeedType kRandomSeed2 = {color:#b5cea8}0x0ff1ce{color};
{color:#4ec9b0}arrow{color}::{color:#4ec9b0}random{color}::RandomArrayGenerator {color:#dcdcaa}rand{color}(kRandomSeed2);
{color:#4ec9b0}std{color}::shared_ptr<Array> value_array = {color:#9cdcfe}rand{color}.{color:#dcdcaa}ArrayOf{color}({color:#dcdcaa}int32{color}(), {color:#b5cea8}2{color} * num_rows, {color:#b5cea8}0.2{color});
{color:#4ec9b0}std{color}::shared_ptr<Array> array = {color:#9cdcfe}rand{color}.{color:#dcdcaa}List{color}(*value_array, num_rows, {color:#b5cea8}1{color});
{color:#dcdcaa}RecordProperty{color}({color:#ce9178}""bitmap""{color},*({color:#9cdcfe}array{color}->{color:#dcdcaa}null_bitmap_data{color}()));
{color:#dcdcaa}RecordProperty{color}({color:#ce9178}""length""{color},{color:#9cdcfe}array{color}->{color:#dcdcaa}length{color}());
{color:#dcdcaa}RecordProperty{color}({color:#ce9178}""array""{color},{color:#9cdcfe}array{color}->{color:#dcdcaa}ToString{color}());
}

Here are the results:

{color:#808080}<{color}{color:#569cd6}testcase{color} {color:#9cdcfe}name{color}={color:#ce9178}""ListTest""{color} {color:#9cdcfe}status{color}={color:#ce9178}""run""{color} {color:#9cdcfe}result{color}={color:#ce9178}""completed""{color} {color:#9cdcfe}time{color}={color:#ce9178}""0""{color} {color:#9cdcfe}timestamp{color}={color:#ce9178}""2021-02-07T15:23:16""{color} {color:#9cdcfe}classname{color}={color:#ce9178}""TestAdapterWriteNested""{color}{color:#808080}>{color}
{color:#808080}<{color}{color:#569cd6}properties{color}{color:#808080}>{color}
{color:#808080}<{color}{color:#569cd6}property{color} {color:#9cdcfe}name{color}={color:#ce9178}""bitmap""{color} {color:#9cdcfe}value{color}={color:#ce9178}""3""{color}{color:#808080}/>{color}
{color:#808080}<{color}{color:#569cd6}property{color} {color:#9cdcfe}name{color}={color:#ce9178}""length""{color} {color:#9cdcfe}value{color}={color:#ce9178}""1""{color}{color:#808080}/>{color}
{color:#808080}<{color}{color:#569cd6}property{color} {color:#9cdcfe}name{color}={color:#ce9178}""array""{color} {color:#9cdcfe}value{color}={color:#ce9178}""[{color}{color:#569cd6}&#x0A;{color}{color:#ce9178} [{color}{color:#569cd6}&#x0A;{color}{color:#ce9178} null,{color}{color:#569cd6}&#x0A;{color}{color:#ce9178} 1074834796,{color}{color:#569cd6}&#x0A;{color}{color:#ce9178} 551076274,{color}{color:#569cd6}&#x0A;{color}{color:#ce9178} 1184187771{color}{color:#569cd6}&#x0A;{color}{color:#ce9178} ]{color}{color:#569cd6}&#x0A;{color}{color:#ce9178}]""{color}{color:#808080}/>{color}
{color:#808080}</{color}{color:#569cd6}properties{color}{color:#808080}>{color}
{color:#808080}</{color}{color:#569cd6}testcase{color}{color:#808080}>{color}

Here is what RandomArrayGenerator::List should do:

{color:#6a9955} /// {color}{color:#569cd6}\brief{color}{color:#6a9955} Generate a random ListArray{color}
{color:#6a9955} ///{color}
{color:#6a9955} /// {color}{color:#569cd6}\param{color}{color:#6a9955}[{color}{color:#569cd6}in{color}{color:#6a9955}] {color}{color:#9cdcfe}values{color}{color:#6a9955} The underlying values array{color}
{color:#6a9955} /// {color}{color:#569cd6}\param{color}{color:#6a9955}[{color}{color:#569cd6}in{color}{color:#6a9955}] {color}{color:#9cdcfe}size{color}{color:#6a9955} The size of the generated list array{color}
{color:#6a9955} /// {color}{color:#569cd6}\param{color}{color:#6a9955}[{color}{color:#569cd6}in{color}{color:#6a9955}] {color}{color:#9cdcfe}null_probability{color}{color:#6a9955} the probability of a list value being null{color}
{color:#6a9955} /// {color}{color:#569cd6}\param{color}{color:#6a9955}[{color}{color:#569cd6}in{color}{color:#6a9955}] {color}{color:#9cdcfe}force_empty_nulls{color}{color:#6a9955} if true, null list entries must have 0 length{color}
{color:#6a9955} ///{color}
{color:#6a9955} /// {color}{color:#569cd6}\return{color}{color:#6a9955} a generated Array{color}
{color:#4ec9b0}std{color}::{color:#4ec9b0}shared_ptr{color}<{color:#4ec9b0}Array{color}> {color:#dcdcaa}List{color}({color:#569cd6}const{color} {color:#4ec9b0}Array{color}{color:#569cd6}&{color} {color:#9cdcfe}values{color}, {color:#4ec9b0}int64_t{color} {color:#9cdcfe}size{color}, {color:#569cd6}double{color} {color:#9cdcfe}null_probability{color},
{color:#569cd6}bool{color} {color:#9cdcfe}force_empty_nulls{color} = {color:#569cd6}false{color});

Note that the generator failed in at least two aspects:
1. The length of the generated array is too low.
2. Even when null_probability is set to 1 there are still 1s in the bitmap.
3. The size of the bitmap is larger than the size of the Array.",pull-request-available,['C++'],ARROW,Bug,Major,2021-02-07 22:41:27,2
13357415,[Packaging][Conda][Drone] Nightly builds are failed by undefined variable error,"https://cloud.drone.io/ursacomputing/crossbow/125/1/2

{noformat}
/drone/src/arrow/dev/tasks/conda-recipes/build_steps.sh: line 43: R_CONFIG: unbound variable
{noformat}",pull-request-available,['Packaging'],ARROW,Bug,Major,2021-02-07 20:21:21,1
13357394,[Rust] [DataFusion] Implement as_any for AggregateExpr,Implement as_any for AggregateExpr so it can be downcast to a known implementation. Ballista needs this for serde.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-02-07 14:33:59,10
13357189,[R] Allow all C++ compute functions to be called by name in dplyr,"Followup to ARROW-9856. Use list_compute_functions (added here) to make all Arrow C++ compute functions available directly by name (in case you want to use the non-checked arithmetic, or an ascii specific kernel, or something without a natural R analogue). Will require a bit more refactoring to handle variable numbers of args, as well as some additional options handling.",pull-request-available,['R'],ARROW,New Feature,Major,2021-02-05 23:17:50,4
13356865,[C++] endianness check does not work on Solaris,"{code}
In file included from /export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/type_traits.h:26:0,
from /export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/scalar.h:36,
from /export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/datum.h:28,
from /export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/dataset/expression.h:32,
from /export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/dataset/dataset.h:28,
from /export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/dataset/dataset.cc:18:
/export/home/XXVfZhv/Rtemp/RtmpoK9Cps/file3f4a341e5d8f/cpp/src/arrow/util/bit_util.h:26:42: 
fatal error: endian.h: No such file or directory
{code}

Googling the error message shows some known issues and workarounds for this on Solaris, e.g.:

* https://github.com/Sereal/Sereal/issues/139
* https://gitlab.torproject.org/legacy/trac/-/issues/11426

cc [~kiszk]
",pull-request-available,['C++'],ARROW,Sub-task,Major,2021-02-04 22:18:31,4
13356864,[R] Allow bundled build script to run on Solaris,Minor changes that allow us to at least attempt a build on Solaris. Does not resolve C++ build issues,pull-request-available,['R'],ARROW,Sub-task,Major,2021-02-04 22:11:20,4
13356797,[Packaging] Remove all use of bintray,"Bintray is being shut down on May 1. 

https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter/

I've made subtasks for the bintray usage other than the dl.bintray.com/apache/arrow repository we use for hosting release artifacts.",pull-request-available,['Packaging'],ARROW,New Feature,Blocker,2021-02-04 15:54:30,1
13356596,[Rust][DataFusion] Make DataFrame Send+Sync,"Inspired by a question on the mailing list
https://lists.apache.org/thread.html/r8f81fae08346817fa283804037ed79a4309bb54aa8ed77c354d7baf0%40%3Cuser.arrow.apache.org%3E

Things need to be `Send + Sync` in order to be sent between threads (or async tasks). Thus we should make DataFrame require Send + Sync as well so as to be usable in async applications.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-02-03 22:28:37,11
13356526,[Java][C++][Integration] C++ integration test creates JSON files incompatible with Java,"The C++ {{arrow-json-integration-test}} runner always emits an empty {{children}} array for non-nested fields, but the Java JSON reader for some reason uses an inflexible tokenize-based parsing technique and refuses such files:
https://github.com/apache/arrow/blob/master/java/vector/src/main/java/org/apache/arrow/vector/ipc/JsonFileReader.java#L722-L740

Typically the error message is as follows:
{code}
Incompatible files
Expected END_OBJECT but got FIELD_NAME
16:30:12.557 [main] ERROR org.apache.arrow.tools.Integration - Incompatible files
java.lang.IllegalStateException: Expected END_OBJECT but got FIELD_NAME
	at org.apache.arrow.vector.ipc.JsonFileReader.readToken(JsonFileReader.java:779)
	at org.apache.arrow.vector.ipc.JsonFileReader.readFromJsonIntoVector(JsonFileReader.java:740)
	at org.apache.arrow.vector.ipc.JsonFileReader.read(JsonFileReader.java:219)
	at org.apache.arrow.tools.Integration$Command$3.execute(Integration.java:203)
	at org.apache.arrow.tools.Integration.run(Integration.java:118)
	at org.apache.arrow.tools.Integration.main(Integration.java:69)
{code}",pull-request-available,"['C++', 'Integration', 'Java']",ARROW,Bug,Major,2021-02-03 16:04:52,2
13356459,[Python] Segmentation fault reading parquet with date filter with INT96 column,"If I read a parquet file (see attachment) with timestamps generated in Spark and apply a filter on a date column I get segmentation fault

{code:java}
import pyarrow.parquet as pq 
now = datetime.datetime.now()
table = pq.read_table(""timestamp.parquet"", filters=[(""date"", ""<="", now)])
{code}


The attached parquet file is generated with this code in spark:
{code:java}
now = datetime.datetime.now() 
data = {""date"": [ now - datetime.timedelta(days=i) for i in range(100)]} 
schema = { ""type"": ""struct"", ""fields"": [{""name"": ""date"", ""type"": ""timestamp"", ""nullable"": True, ""metadata"": {}}, ], } 
spf = spark.createDataFrame(pd.DataFrame(data), schema=StructType.fromJson(schema)) 
spf.write.format(""parquet"").mode(""overwrite"").save(""timestamp.parquet"") 
{code}
If I downgrade pyarrow to 2.0.0 it works fine.

Python version3.7.7

pyarrow version 3.0.0",dataset pull-request-available,['C++'],ARROW,Bug,Major,2021-02-03 10:36:19,6
13356341,[R] Consider ways to make arrow.skip_nul option more user-friendly,"In Arrow 3.0.0, the {{arrow.skip_nul}} option effectively defaults to {{FALSE}}for consistency with {{base::readLines}}and {{base::scan}}.

If the user keeps this default option value, then conversion of string data containing embedded nuls causes an error with a message like:
{code:java}
embedded nul in string: '\0' {code}
If the user sets the option to {{TRUE}}, then no error occurs, but this warning is issued:
{code:java}
Stripping '\0' (nul) from character vector {code}
Consider whether we should:
 # Keep this all as it is
 # Change the default option value to {{TRUE}}
 # Keep the default option value as it is, but catch the error and re-throw it with a more actionable message that tells the user how to set the option",pull-request-available,['R'],ARROW,Improvement,Minor,2021-02-03 01:48:51,4
13356279,[C++] Upgrade mimalloc,"I tried this in ARROW-11350 but ran into an issue (https://github.com/microsoft/mimalloc/issues/353). That has since been resolved and we could apply a patch to bring it in. Or we can wait for it to get into a proper release.

There is also now a 1.7 release, which claims to work on the Apple M1, as well as a 2.0 version, which claims better performance. ",pull-request-available,['C++'],ARROW,New Feature,Major,2021-02-02 19:11:43,2
13356196,[Python][CI] Kartothek integrations build is failing with numpy 1.20,"See eg https://github.com/ursacomputing/crossbow/runs/1804464537, failure looks like:

{code}
 ____________ ERROR collecting tests/io/dask/dataframe/test_read.py _____________
tests/io/dask/dataframe/test_read.py:185: in <module>
    @pytest.mark.parametrize(""col"", get_dataframe_not_nested().columns)
kartothek/core/testing.py:65: in get_dataframe_not_nested
    ""unicode"": pd.Series([""""], dtype=np.unicode),
/opt/conda/envs/arrow/lib/python3.7/site-packages/pandas/core/series.py:335: in __init__
    data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True)
/opt/conda/envs/arrow/lib/python3.7/site-packages/pandas/core/construction.py:480: in sanitize_array
    subarr = _try_cast(data, dtype, copy, raise_cast_failure)
/opt/conda/envs/arrow/lib/python3.7/site-packages/pandas/core/construction.py:587: in _try_cast
    maybe_cast_to_integer_array(arr, dtype)
/opt/conda/envs/arrow/lib/python3.7/site-packages/pandas/core/dtypes/cast.py:1723: in maybe_cast_to_integer_array
    casted = np.array(arr, dtype=dtype, copy=copy)
E   ValueError: invalid literal for int() with base 10: ''
{code}

So it seems that {{pd.Series([""""], dtype=np.unicode)}} stopped working with numpy 1.20.0",pull-request-available,['Python'],ARROW,Bug,Major,2021-02-02 12:44:39,5
13356063,[Python] pyarrow.parquet.read_pandas doesn't conform to its docs,"The {{*pyarrow.parquet.read_pandas*}}[implementation|https://github.com/apache/arrow/blob/816c23af4478fe28f31d474e90b433baeadb7a78/python/pyarrow/parquet.py#L1740-L1754] doesn't conform to its [docs|https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_pandas.html#pyarrow.parquet.read_pandas] in at least these ways:
 # The docs state that a *{{filesystem}}* option can be provided, as it should be. Without this option I cannot read from S3, etc. The implementation, however, doesn't have this option! As such I currently cannot use it to read from S3!
 # The docs (_not in the type definition!_) state that the default value for *{{use_legacy_dataset}}* is False, whereas the implementation has a value of True. Actually, however, if a value of True is used, then I can't read a partitioned dataset at all, so in this case maybe its the doc that needs to change to True.

It looks to have been implemented and reviewed pretty carelessly.

",pull-request-available,['Python'],ARROW,Bug,Major,2021-02-01 22:15:02,5
13355676,[Rust] [DataFusion] Add method to CsvExec to get CSV schema,It is not possible to inspect an already created CsvExec and determine what the underlying schema is,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-30 21:12:52,10
13355437,[C++] Arrow uses AVX512 instructions even when not supported by the OS,"*Update*: Azure (D2_v2) VM no longer spins-up withXeon Platinum 8171m, so I'm unable to test it with other OS's. Azure VM's are assigned different type of CPU's of same ""class"" depending on availability. I will try my ""luck"" later.

VM's w/ Xeon Platinum 8171m running on Azure (D2_v2) start crashing after upgrading from pyarrow 2.0 to pyarrow 3.0. However, this only happens when reading parquet files larger than 4096 bits!?

Windows closes Python with exit code 255 and produces this:


{code:java}
Faulting application name: python.exe, version: 3.8.3150.1013, time stamp: 0x5ebc7702 Faulting module name: arrow.dll, version: 0.0.0.0, time stamp: 0x60060ce3 Exception code: 0xc000001d Fault offset: 0x000000000047aadc Faulting process id: 0x1b10 Faulting application start time: 0x01d6f4a43dca3c14 Faulting application path: D:\SvcFab\_App\SomeApp.FabricType_App32\SomeApp.Fabric.Executor.ProcessActorPkg.Code.1.0.218-prod\Python38\python.exe Faulting module path: D:\SvcFab\_App\SomeApp.FabricType_App32\temp\Executions\50cfffe8-9250-4ac7-8ba8-08d8c2bb3edf\.venv\lib\site-packages\pyarrow\arrow.dll{code}


Tested on:
||OS||Xeon Platinum 8171m or 8272CL||Other CPUs||
|Windows Server 2012 Data Center|Fail|OK|
|Windows Server 2016 Data Center|OK|OK|
|Windows Server 2019 Data Center|||
|Windows 10||OK|



Example code (Python):
{code:java}
importnumpyasnp
importpandasaspd

data_len = 2**5
data=pd.DataFrame(
    {""values"":np.arange(0.,float(data_len),dtype=float)},
    index=np.arange(0,data_len,dtype=int)
)

data.to_parquet(""test.parquet"")
data=pd.read_parquet(""test.parquet"",engine=""pyarrow"")  # fails here!
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-01-29 13:26:42,2
13355297,[R] value_counts and some StructArray methods,"Exposing value_counts() is useful for exploration, even if it is limited to counting over a single (non-struct) array. And since it returns a StructArray, I found it useful to implement some more methods on that object.",pull-request-available,['R'],ARROW,New Feature,Major,2021-01-28 23:26:43,4
13355249,[Doc] Add IPC buffer compression to support matrix,In [https://arrow.apache.org/docs/status.html#ipc-format] there should be an additional line for buffer compression support.,pull-request-available,['Documentation'],ARROW,Improvement,Major,2021-01-28 18:04:45,2
13355248,[Integration] Add integration test for buffer compression,Buffer compression is an official feature of the IPC format but interoperability is currently untested between implementations. We should add such a test case to the integration suite.,pull-request-available,"['C++', 'Integration']",ARROW,Task,Critical,2021-01-28 18:03:41,2
13355164,[Rust] Reduce copies in Schema::try_merge,"https://github.com/apache/arrow/blob/ab5fc979c69ccc5dde07e1bc1467b02951b4b7e9/rust/arrow/src/datatypes.rs#L1832-L1860

I was looking at this code yesterday while using it in IOx -- https://github.com/influxdata/influxdb_iox/pull/703

Even though Schema::try_merge requires a slice of Schemas (not schema refs), it copies all of its fields. This is not ideal in the common case where most of the fields in the Schema will be the same",pull-request-available,['Rust'],ARROW,Improvement,Minor,2021-01-28 11:45:31,11
13355092,"[Python] Expressions not working with logical boolean operators  (and, or, not)","There's a problem with boolean ""and"", ""or"" and ""not"" expressions when creating them in python (or I'm doing something completely stupid).


{code:java}
>>> import pyarrow
>>> pyarrow.__version__
'3.0.0'
>>> import pyarrow.dataset as ds
>>> ds. scalar(False) and ds.scalar(True) # <--- I expect false
<pyarrow.dataset.Expression true>
>>> ds.scalar(True) and ds.scalar(False) # this works
<pyarrow.dataset.Expression false>

>>> ds.scalar(True) or ds.scalar(False) # this works
<pyarrow.dataset.Expression true>
>>> ds.scalar(False) or ds.scalar(True) # <--- I expect true
<pyarrow.dataset.Expression false>

>>> not ds.scalar(True)   # this works                                                                                                                                                                                                  
False                                                                                                                                                                                                                       
>>> not ds.scalar(False)      <--- I expect true                                                                                                                                                                                              
False  
{code}
I tried to figure out what goes wrong here, but there are no obvious problems in the python code, same for C++ (but I didn't quite understand everything of it yet).



This happens with pyarrow3 and pyarrow2



",pull-request-available,['Python'],ARROW,Bug,Major,2021-01-28 06:25:05,5
13355047,[Packaging][Linux] Disable arm64 nightly builds,We'll re-enable them with new CI configuration by the next release.,pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Improvement,Major,2021-01-27 23:37:31,1
13355019,[CI][C++] Fix caching on Travis-CI builds,"The ccache-based caching on Travis-CI currently doesn't work: the restored cache is always empty.

Fixing the caching setup should allow shortening wait queues.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Trivial,2021-01-27 19:45:06,2
13354985,[C++][Dataset] Allow more aggresive implicit casts for literals,"After ARROW-8919, a literal in an Expression may cause unnecessary implicit casting of a column. For example {{equal(field_ref(""i8""), literal(1))}} will cause column i8 to be promoted to the type of the literal (int32) for comparison. Since we have access to the literal value at bind time we could examine {{1}} and determine that it can safely be *de*moted to int8, which produces a semantically equivalent and more performant filter.

",dataset,['C++'],ARROW,Improvement,Major,2021-01-27 17:20:32,6
13354959,[Python] Pickled ParquetFileFragment has invalid partition_expresion with dictionary type in pyarrow 2.0,"From https://github.com/dask/dask/pull/7066#issuecomment-767156623

Simplified reproducer:

{code:python}
import pyarrow.parquet as pq
import pyarrow.dataset as ds

table = pa.table({'part': ['A', 'B']*5, 'col': range(10)})
pq.write_to_dataset(table, ""test_partitioned_parquet"", partition_cols=[""part""])

# with partitioning_kwargs = {} there is no error
partitioning_kwargs = {""max_partition_dictionary_size"": -1}
dataset = ds.dataset(
    ""test_partitioned_parquet/"", format=""parquet"", 
    partitioning=ds.HivePartitioning.discover( **partitioning_kwargs)
)

frag = list(dataset.get_fragments())[0]
{code}

Querying this fragment works fine, but after serialization/deserialization with pickle, it gives errors (and with the original data example I actually got a segfault as well):

{code}
In [16]: import pickle

In [17]: frag2 = pickle.loads(pickle.dumps(frag))

In [19]: frag2.partition_expression
...
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 16: invalid continuation byte

In [20]: frag2.to_table(schema=schema, columns=columns)
Out[20]: 
pyarrow.Table
col: int64
part: dictionary<values=string, indices=int32, ordered=0>

In [21]: frag2.to_table(schema=schema, columns=columns).to_pandas()
...
~/miniconda3/envs/arrow-20/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.table_to_blocks()

ArrowException: Unknown error: Wrapping  failed
{code}

It seems the issue was specifically with a partition expression with dictionary type. 
Also when using an integer columns as the partition column, you get wrong values (but silently in this case):

{code:python}
In [42]: frag.partition_expression
Out[42]: 
<pyarrow.dataset.Expression (part == [
  1,
  2
][0]:dictionary<values=int32, indices=int32, ordered=0>)>

In [43]: frag2.partition_expression
Out[43]: 
<pyarrow.dataset.Expression (part == [
  170145232,
  32754
][0]:dictionary<values=int32, indices=int32, ordered=0>)>
{code}

Now, it seems this is fixed in master. But since I don't remember it was fixed intentionally ([~bkietz]?), it would be good to add some tests for it.",dataset pull-request-available,['Python'],ARROW,Bug,Minor,2021-01-27 14:52:32,5
13354726,[C++] HdfsOutputStream::Write unsafely truncates integers exceeding INT32_MAX,"Originally reported on user@, see

https://github.com/apache/arrow/blob/master/cpp/src/arrow/io/hdfs.cc#L277

{{tSize} is a 32-bit integer",pull-request-available,['C++'],ARROW,Bug,Major,2021-01-26 17:33:55,2
13354484,[C++][Dataset] Reading dataset with filtering on timestamp partition field crashes,"{code}
In [1]: df = pd.DataFrame({""dates"": list(pd.date_range(""2012-01-01"", periods=2, freq=""D"")) * 5, ""col"": range(10)})

In [2]: df.to_parquet(""test_partition_timestamps"", partition_cols=[""dates""])

In [3]: !ls test_partition_timestamps/
'dates=2012-01-01 00:00:00'  'dates=2012-01-02 00:00:00'

In [4]: import pyarrow.dataset as ds

In [6]: part = ds.partitioning(pa.schema([(""dates"", pa.timestamp(""s""))]), flavor=""hive"")

In [7]: dataset = ds.dataset(""test_partition_timestamps/"", format=""parquet"", partitioning=part)
{code}

Reading the dataset is fine and fives the correct types:

{code}
In [10]: dataset.to_table()
Out[10]: 
pyarrow.Table
col: int64
dates: timestamp[s]
{code}

but filtering on the timestamp column segfaults:

{code}
In [11]: dataset.to_table(filter=ds.field(""dates"") > pd.Timestamp(""2012-01-01""))
../src/arrow/compute/kernels/scalar_cast_temporal.cc:129:  Check failed: (batch[0].kind()) == (Datum::ARRAY) 
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xc2224a)[0x7f68d2ccf24a]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xc221c8)[0x7f68d2ccf1c8]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xc221ea)[0x7f68d2ccf1ea]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZN5arrow4util8ArrowLogD1Ev+0x47)[0x7f68d2ccf549]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xf0252a)[0x7f68d2faf52a]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZNSt17_Function_handlerIFvPN5arrow7compute13KernelContextERKNS1_9ExecBatchEPNS0_5DatumEEPS9_E9_M_invokeERKSt9_Any_dataOS3_S6_OS8_+0x69)[0x7f68d2e8ab86]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZNKSt8functionIFvPN5arrow7compute13KernelContextERKNS1_9ExecBatchEPNS0_5DatumEEEclES3_S6_S8_+0x7a)[0x7f68d2deec04]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xd3d6f9)[0x7f68d2dea6f9]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xd3cd5b)[0x7f68d2de9d5b]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZNK5arrow7compute8Function7ExecuteERKSt6vectorINS_5DatumESaIS3_EEPKNS0_15FunctionOptionsEPNS0_11ExecContextE+0x8c7)[0x7f68d2df9963]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(+0xd2eed2)[0x7f68d2ddbed2]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZNK5arrow7compute12MetaFunction7ExecuteERKSt6vectorINS_5DatumESaIS3_EEPKNS0_15FunctionOptionsEPNS0_11ExecContextE+0x15d)[0x7f68d2dfac8f]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZN5arrow7compute12CallFunctionERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorINS_5DatumESaISA_EEPKNS0_15FunctionOptionsEPNS0_11ExecContextE+0x26c)[0x7f68d2dedc6f]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZN5arrow7compute12CallFunctionERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKSt6vectorINS_5DatumESaISA_EEPKNS0_15FunctionOptionsEPNS0_11ExecContextE+0x93)[0x7f68d2deda96]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZN5arrow7compute4CastERKNS_5DatumERKNS0_11CastOptionsEPNS0_11ExecContextE+0xf7)[0x7f68d2ddd493]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.300(_ZN5arrow7compute4CastERKNS_5DatumESt10shared_ptrINS_8DataTypeEERKNS0_11CastOptionsEPNS0_11ExecContextE+0x77)[0x7f68d2ddd6e2]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.300(+0x1c5c21)[0x7f68b30cfc21]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.300(+0x1c6789)[0x7f68b30d0789]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.300(+0x1c5097)[0x7f68b30cf097]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.300(_ZNK5arrow7dataset10Expression4BindENS_10ValueDescrEPNS_7compute11ExecContextE+0x732)[0x7f68b30d22e8]
...
{code}

",dataset pull-request-available,['C++'],ARROW,Bug,Major,2021-01-25 16:20:28,6
13354455,[Rust] CI fails due to deprecation warning in clippy,"Rust clippy lint test on CI started failing with this error:

{code}
   Compiling arrow-flight v3.0.0-SNAPSHOT (/__w/arrow/arrow/rust/arrow-flight)
error: use of deprecated struct `criterion::Benchmark`: Please use BenchmarkGroups instead.
  --> arrow/benches/builder.rs:39:9
   |
39 |         Benchmark::new(""bench_primitive"", move |b| {
   |         ^^^^^^^^^^^^^^
   |
   = note: `-D deprecated` implied by `-D warnings`

error: use of deprecated struct `criterion::Benchmark`: Please use BenchmarkGroups instead.
  --> arrow/benches/builder.rs:62:9
   |
62 |         Benchmark::new(""bench_bool"", move |b| {
   |         ^^^^^^^^^^^^^^

error: use of deprecated associated function `criterion::Criterion::<M>::bench`: Please use BenchmarkGroups instead.
  --> arrow/benches/builder.rs:37:7
   |
37 |     c.bench(
   |       ^^^^^

error: use of deprecated associated function `criterion::Criterion::<M>::bench`: Please use BenchmarkGroups instead.
  --> arrow/benches/builder.rs:60:7
   |
60 |     c.bench(
   |       ^^^^^
{code}

It appears related to the latest release of criterion: https://crates.io/crates/criterion/0.3.4 (On Jan 24 2021)

",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-25 14:32:48,9
13354428,[Python] Make legacy pyarrow.filesystem / pyarrow.serialize warnings more visisble,"Both the legacy filesystems ({{pyarrow.filesytem}} as the serialization functionality was deprecated in pyarrow 2.0.0, and uses DeprecationWarnings. In a next step (before actually removing), we can turn them into FutureWarnings, to make them more visisble (always shown by default in contrast to DeprecationWarnings)",pull-request-available,['Python'],ARROW,Improvement,Major,2021-01-25 13:01:12,5
13354393,Support RC verification on macOS-ARM64,There are some assumptions in the verification scripts that assume an x86 system.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2021-01-25 11:48:29,8
13354253,[DataFusion] Split expressions.rs,It is now 3k LOC that are largely independent.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-25 04:21:36,9
13354160,[Rust] [Parquet] Implement parsers for v2 of the text schema,"V2 of the writer produces schema like:

  required INT32 fieldname INTEGER(32, true);

We should support parsing this format, as it maps to logical types.
I'm unsure of what the implications are for fields that don't have a logical type representation, but have a converted type (e.g. INTERVAL). We can try write a V2 file with parquet-cpp and observe the behaviour.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2021-01-24 07:13:08,12
13354111,[Rust] take primitive implementation is unsound,There is an out of bound read.,pull-request-available,['Rust'],ARROW,Bug,Major,2021-01-23 14:12:01,9
13353694,[Python][CI] Nightly pandas builds failing because of internal pandas change,"The nightly pandas builds are failing because we were relying on a private pandas construct to test something, and this has changed in pandas.

See eg https://github.com/ursacomputing/crossbow/runs/1733034838",pull-request-available,['Python'],ARROW,Test,Major,2021-01-21 09:17:35,5
13353541,[Rust] [DataFusion] Add DictionaryArray support for create_batch_empty,the create_batch_empty function is used for creating output during aggregation. As part of my plan for better dictionary support it also needs to support DictionaryArray as well.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-20 14:28:07,11
13353378,"[Rust][DataFusion] ComputeError(""concat requires input of at least one array"")) with queries with ORDER BY or GROUP BY that return no ","If you run a SQL query in datafusion which has predicates that produces no rows that also includes a GROUP BY or ORDER BY clause, you get the following error:

Error of ""ArrowError(ComputeError(""concat requires input of at least one array""))""

Here are two test cases that show the problem: https://github.com/apache/arrow/blob/master/rust/datafusion/src/execution/context.rs#L889

{code}
    #[tokio::test]
    async fn sort_empty() -> Result<()> {
        // The predicate on this query purposely generates no results
        let results =
            execute(""SELECT c1, c2 FROM test WHERE c1 > 100000 ORDER BY c1 DESC, c2 ASC"", 4).await?;
        assert_eq!(results.len(), 0);
        Ok(())
    }


    #[tokio::test]
    async fn aggregate_empty() -> Result<()> {
        // The predicate on this query purposely generates no results
        let results = execute(""SELECT SUM(c1), SUM(c2) FROM test where c1 > 100000"", 4).await?;
        assert_eq!(results.len(), 0);
        Ok(())
    }

{code}",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2021-01-19 21:52:22,11
13353367,[Rust] Arrow `memory` made private is a breaking API change,"We depend on functionality in the Arrow memory module for buffer building and this was recently made private.



Please make this module public again.",pull-request-available,['Rust'],ARROW,Bug,Critical,2021-01-19 21:17:02,9
13353283,[C++] Spurious test failure when creating temporary dir,"When running the release verification script, I sometimes get this error:
{code}
[----------] 5 tests from TestInt8/TestSparseTensorRoundTrip/0, where TypeParam = arrow::Int8Type
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexRowMajor
/tmp/arrow-3.0.0.4SRpe/apache-arrow-3.0.0/cpp/src/arrow/ipc/tensor_test.cc:53: Failure
Failed
'_error_or_value8.status()' failed with IOError: Path already exists: '/tmp/ipc-test-qj6ng827/'
[  FAILED  ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexRowMajor, where TypeParam = arrow::Int8Type (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexColumnMajor
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCOOIndexColumnMajor (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSRIndex
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSRIndex (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSCIndex
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSCIndex (0 ms)
[ RUN      ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSFIndex
[       OK ] TestInt8/TestSparseTensorRoundTrip/0.WithSparseCSFIndex (1 ms)
[----------] 5 tests from TestInt8/TestSparseTensorRoundTrip/0 (1 ms total)
{code}

It seems that in some fringe cases, the random generation of temporary directory names produces duplicates. Most likely this means the random generator is getting the same seed from different processes.",pull-request-available,['C++'],ARROW,Bug,Minor,2021-01-19 13:53:21,2
13353279,[Rust] [DataFusion] Improve test comparisons to record batch,"The test::format_batch function does not have wide range of type support (e.g. it doesn't support dictionaries) and its output makes tests hard to read / update, in my opinion. We should consolidate the tests to use `arrow::util::pretty::pretty_format_batches` both to reduce code duplication as well as increase type support
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-19 13:26:34,11
13353275,"[Rust] Support pretty printing timestamp, date, and time types","
I found this while removing `test::format_batches` (PR to come shortly),

pretty printing was printing numbers rather than dates.
",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-19 12:57:37,11
13353176,"[Rust] unset_bit is toggling bits, not unsetting them","The functions {{bit_util::unset_bit[_raw]}} are currently toggling bits, not setting them.",pull-request-available,['Rust'],ARROW,Bug,Major,2021-01-19 05:05:02,9
13353072,[C++] Fix reading LZ4-compressed Parquet files produced by Java Parquet implementation,"We slightly misunderstood the Hadoop LZ4 format. A compressed buffer can actually contain several ""frames"", each prefixed with (de)compressed size.

See https://issues.apache.org/jira/browse/ARROW-9177?focusedCommentId=17267058&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17267058",pull-request-available,['C++'],ARROW,Bug,Critical,2021-01-18 12:53:23,2
13352870,[Rust] [DataFusion] Support GROUP BY for Dictionary columns,"If you try and run a query that groups on a column encoded as a Dictionary you get an error about ""unsupported GROUP BY type"". You should get the same result as if your data was encoded using a primitive array type. ",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-17 13:49:15,11
13352727,[C++] Fix compilation error in dataset expressions on macOS 10.11,"See https://github.com/autobrew/homebrew-core/pull/61#issuecomment-761605455

R binary packages for macOS are built with an old SDK, so this is needed. ",pull-request-available,['C++'],ARROW,Bug,Major,2021-01-16 21:39:52,6
13352714,[Rust] [Parquet] List schema to Arrow parser misinterpreting child nullability,"We currently do not propagate child nullability correctly when reading parquet files from Spark 3.0.1 (parquet-mr 1.10.1).

For example, the below taken from [https://github.com/apache/parquet-format/blob/master/LogicalTypes.md]is currently interpreted incorrectly:


{code:java}
// List<String> (list nullable, elements non-null) 
optional group my_list (LIST) {
    repeated group list { 
        required binary element (UTF8); 
    } 
}{code}
The Arrow type should be:
{code:java}
Field::new(
    ""my_list"",
    DataType::List(
        box Field::new(""element"", DataType::Utf8, nullable: false),
    ),
    nullable: true
){code}
but we currently end up with
{code:java}
Field::new(
   ""my_list"",
   DataType::List(
       box Field::new(""list"", DataType::Utf8, nullable: true),
   ),
   nullable: true
)
{code}
This doesn't seem to be an issue with the master branch as of opening this issue, so it might not be severe enough to try force into the 3.0.0 release.

I tested null and non-null Spark files, and was able to read them correctly. This becomes an issue with nested lists, which I'm working on.

",pull-request-available,['Rust'],ARROW,Bug,Major,2021-01-16 19:53:48,12
13352687,[Rust] Unable to read Parquet file because of mismatch in column-derived and embedded schemas,"The issue seems to stem from the new(-ish) behavior of the Arrow Parquet reader where the embedded arrow schema is used instead of deriving the schema from the Parquet columns.



However it seems like some cases still derive the schema type from the column types, leading to the Arrow record batch reader erroring out that the column types must match the schema types.



In our case, the column type is an int96 datetime (ns) type, and the Arrow type in the embedded schema is DataType::Timestamp(TimeUnit::Nanoseconds, Some(""UTC"")). However, the code that constructs the Arrays seems to re-derive this column type as DataType::Timestamp(TimeUnit::Nanoseconds, None) (because the Parquet schema has no timezone information). And so, Parquet files that we were able to read successfully with our branch of Arrow circa October are now unreadable.



I've attached an example of a Parquet file that demonstrates the problem. This file was created in Python (as most of our Parquet files are).



I've also attached a sample Rust program that will demonstrate the error.",pull-request-available,['Rust'],ARROW,Bug,Blocker,2021-01-16 16:29:23,12
13352469,[C++][Dataset] Don't require dictionaries for reading dataset with schema-based Partitioning,"As a follow-up on ARROW-10247 (see also https://github.com/apache/arrow/pull/9130#issuecomment-760801124). We currently require the user to pass manually specified dictionary values when reading a dataset with a Partitioning based on a schema with dictionary typed fields. 

In practice that means that the user for example needs to parse the file paths to get all the possible values the partition field can take, while Arrow will then afterwards again do the same to construct the dataset object. 
_Naively_, it seems that it should be possible to let Arrow infer the dictionary _values_, even when providing an explicit schema with a dictionary field for the Partitioning (i.e. so when not letting the partitioning schema itself be inferred from the file paths).

An example use case is when you have a Partitioning schema with both dictionary and non-dictionary fields. When discovering the schema, you can only have all or nothing (all dictionary fields or no dictionary fields).

cc [~bkietz]",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2021-01-15 11:24:45,0
13352374,[Packaging][Conda][macOS] Fix Python version,This was introduced by ARROW-10224 unexpectedly.,pull-request-available,['Packaging'],ARROW,Bug,Major,2021-01-14 23:56:12,1
13352116,[C++] Infer date32 columns in CSV,See ARROW-11243 for the original report,pull-request-available,['C++'],ARROW,New Feature,Major,2021-01-13 23:02:38,4
13352107,[C++] Parse time32 from string and infer in CSV reader,"When reading a CSV with read_csv_arrow() with date types and time types, the dates are read as datetimes rather than dates and times are read as characters rather than time.

The first problem can be fixed by supplying date32() to schema(), though better inference would be nice. However, supplying time32() to schema() causes an error.

Here is a sample dataset, also attached.

date,time,reading
 2021-01-01,00:00:00,67.8
 2021-01-01,00:00:00,72.4
 2021-01-01,00:00:00,63.1
 2021-01-01,00:05:00,67.8

Reading with readr::read_csv() results in a tibble with three columns: date, time, dbl, as expected.

{code:r}
samp_readr <- readr::read_csv('sampledata.csv')
samp_readr
{code}

{code:r}
# A tibble: 4 x 3
  date       time   reading
  <date>     <time>   <dbl>
1 2021-01-01 00'00""    67.8
2 2021-01-01 00'00""    72.4
3 2021-01-01 00'00""    63.1
4 2021-01-01 05'00""    67.8
{code}

Reading with arrow::read_csv_arrow() without providing schema() results in a tibble with three columns: dttm,chr, dbl.

{code:r}
samp_arrow_plain <- arrow::read_csv_arrow('sampledata.csv')
samp_arrow_plain
{code}
{code:r}
# A tibble: 4 x 3
  date                time     reading
  <dttm>              <chr>      <dbl>
1 2020-12-31 19:00:00 00:00:00    67.8
2 2020-12-31 19:00:00 00:00:00    72.4
3 2020-12-31 19:00:00 00:00:00    63.1
4 2020-12-31 19:00:00 00:05:00    67.8
{code}

Reading with arrow::read_csv_arrow() and providing date=date32() via schema() to col_types results in a tibble with three columns: date,chr, dbl.

{code:r}
samp_arrow_date <- arrow::read_csv_arrow('sampledata.csv', col_types=schema(date=date32()))
samp_arrow_date
{code}
{code:r}
# A tibble: 4 x 3
  date       time     reading
  <date>     <chr>      <dbl>
1 2021-01-01 00:00:00    67.8
2 2021-01-01 00:00:00    72.4
3 2021-01-01 00:00:00    63.1
4 2021-01-01 00:05:00    67.8
{code}

Reading with arrow::read_csv_arrow() and providing time=time32() via schema() to col_types generates an error.

{code:r}
samp_arrow_time <- arrow::read_csv_arrow('sampledata.csv', col_types=schema(time=time32()))
{code}
{code:r}
Error in csv___TableReader__Read(self) : 
  NotImplemented: CSV conversion to time32[ms] is not supported
{code}

The same error occurs when using compact string notation.

{code:r}
samp_arrow_string <- arrow::read_csv_arrow('sampledata.csv', col_types='DTc', col_names=c('date', 'time', 'reading'), skip=1)
{code}
{code:r}
Error in csv___TableReader__Read(self) : 
  NotImplemented: CSV conversion to time32[ms] is not supported
{code}

This is something in the internals, so far beyond me to figure out a fix, but I saw it in action and wanted to report it.",pull-request-available,['C++'],ARROW,Improvement,Minor,2021-01-13 22:01:07,2
13352027,[Rust] array::transform::tests::test_struct failed,"Test *array::transform::tests::test_struct*in *arrow/src/array/transform/mod.rs*failed when swap the first two elements:

change from
{code:java}
// code placeholder
let strings: ArrayRef = Arc::new(StringArray::from(vec![
    Some(""joe""),
    None,{code}
to
{code:java}
// code placeholder
let strings: ArrayRef = Arc::new(StringArray::from(vec![
    None,
    Some(""joe""),{code}
The failure was first found when I reporthttps://issues.apache.org/jira/browse/ARROW-11160



",pull-request-available,['Rust'],ARROW,Bug,Major,2021-01-13 14:41:57,9
13351971,[C++] Compiler error with GLog and unity build enabled,"{code}
In file included from src/arrow/CMakeFiles/arrow_objlib.dir/Unity/unity_8_cxx.cxx:7:
In file included from /home/antoine/arrow/dev/cpp/src/arrow/util/logging.cc:29:
glog_ep-prefix/src/glog_ep/include/glog/logging.h:640:9: error: use of overloaded operator '<<' is ambiguous (with operand types 'std::ostream' (aka 'basic_ostream<char>') and 'const nullptr_t')
  (*os) << v;
  ~~~~~ ^  ~
glog_ep-prefix/src/glog_ep/include/glog/logging.h:696:3: note: in instantiation of function template specialization 'google::MakeCheckOpValueString<nullptr_t>' requested here
  MakeCheckOpValueString(comb.ForVar2(), v2);
  ^
glog_ep-prefix/src/glog_ep/include/glog/logging.h:720:1: note: in instantiation of function template specialization 'google::MakeCheckOpString<std::unordered_map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char>, std::hash<std::string>, std::equal_to<std::__cxx11::basic_string<char> >, std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> > > > *, nullptr_t>' requested here
DEFINE_CHECK_OP_IMPL(Check_NE, !=)  // Use CHECK(x == NULL) instead.
^
glog_ep-prefix/src/glog_ep/include/glog/logging.h:709:17: note: expanded from macro 'DEFINE_CHECK_OP_IMPL'
    else return MakeCheckOpString(v1, v2, exprtext); \
                ^
/home/antoine/arrow/dev/cpp/src/arrow/util/key_value_metadata.cc:75:3: note: in instantiation of function template specialization 'google::Check_NEImpl<std::unordered_map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char>, std::hash<std::string>, std::equal_to<std::__cxx11::basic_string<char> >, std::allocator<std::pair<const std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> > > > *, nullptr_t>' requested here
  DCHECK_NE(out, nullptr);
  ^
glog_ep-prefix/src/glog_ep/include/glog/logging.h:996:31: note: expanded from macro 'DCHECK_NE'
#define DCHECK_NE(val1, val2) CHECK_NE(val1, val2)
                              ^
glog_ep-prefix/src/glog_ep/include/glog/logging.h:791:30: note: expanded from macro 'CHECK_NE'
#define CHECK_NE(val1, val2) CHECK_OP(_NE, !=, val1, val2)
                             ^
glog_ep-prefix/src/glog_ep/include/glog/logging.h:766:3: note: expanded from macro 'CHECK_OP'
  CHECK_OP_LOG(name, op, val1, val2, google::LogMessageFatal)
  ^
glog_ep-prefix/src/glog_ep/include/glog/logging.h:746:18: note: expanded from macro 'CHECK_OP_LOG'
         google::Check##name##Impl(                      \
                 ^
<scratch space>:101:1: note: expanded from here
Check_NEImpl
^
/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/ostream:108:7: note: candidate function
      operator<<(__ostream_type& (*__pf)(__ostream_type&))
      ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/ostream:117:7: note: candidate function
      operator<<(__ios_type& (*__pf)(__ios_type&))
      ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/ostream:127:7: note: candidate function
      operator<<(ios_base& (*__pf) (ios_base&))
      ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/ostream:245:7: note: candidate function
      operator<<(const void* __p)
      ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/ostream:276:7: note: candidate function
      operator<<(__streambuf_type* __sb);
      ^
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2021-01-13 10:18:07,2
13351965,[Python] S3 test failures inside non-default regions,"The following two cases are failing when they are executed outside of us-east-1:

test_fs.py::test_s3_real_aws
test_fs.py::test_s3_real_aws_region_selection

cc [~apitrou]",pull-request-available,['Python'],ARROW,Bug,Major,2021-01-13 10:05:33,2
13351904,[C++][Flight] Fail to link with bundled gRPC and Abseil,"https://app.circleci.com/pipelines/github/ursa-labs/crossbow/69113/workflows/fb13620e-7211-4eb8-8fcf-3d053fbf3d40/jobs/14532

{noformat}
[507/724] Linking CXX executable debug/flight-test-serverting.so.300 debug/libarrow_flight_testing.soK[K[K[Kc.o[KoK
FAILED: debug/flight-test-server 
: && /usr/bin/ccache /usr/lib/ccache/g++  -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-deprecated-declarations -Wno-sign-conversion -Wno-unused-variable -Werror -fno-semantic-interposition -msse4.2  -g   src/arrow/flight/CMakeFiles/flight-test-server.dir/test_server.cc.o  -o debug/flight-test-server  -Wl,-rpath,/build/cpp/debug:/build/cpp/googletest_ep-prefix/lib debug/libarrow_flight_testing.so.300.0.0 debug/libarrow_testing.so.300.0.0 /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so orc_ep-install/lib/liborc.a protobuf_ep-install/lib/libprotobuf.a awssdk_ep-install/lib/libaws-cpp-sdk-identity-management.a awssdk_ep-install/lib/libaws-cpp-sdk-sts.a awssdk_ep-install/lib/libaws-cpp-sdk-cognito-identity.a awssdk_ep-install/lib/libaws-cpp-sdk-s3.a awssdk_ep-install/lib/libaws-cpp-sdk-core.a awssdk_ep-install/lib/libaws-c-event-stream.a awssdk_ep-install/lib/libaws-checksums.a awssdk_ep-install/lib/libaws-c-common.a /usr/lib/x86_64-linux-gnu/libutf8proc.so /usr/lib/x86_64-linux-gnu/libre2.so -ldl googletest_ep-prefix/lib/libgtest_maind.so googletest_ep-prefix/lib/libgmockd.so /usr/lib/x86_64-linux-gnu/libboost_filesystem.so /usr/lib/x86_64-linux-gnu/libboost_system.so -ldl /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.2 googletest_ep-prefix/lib/libgtestd.so debug/libarrow_flight.so.300.0.0 grpc_ep-install/lib/libgrpc++.a absl_ep-install/lib/libabsl_bad_optional_access.a absl_ep-install/lib/libabsl_base.a absl_ep-install/lib/libabsl_cord.a absl_ep-install/lib/libabsl_graphcycles_internal.a absl_ep-install/lib/libabsl_int128.a absl_ep-install/lib/libabsl_malloc_internal.a absl_ep-install/lib/libabsl_raw_logging_internal.a absl_ep-install/lib/libabsl_spinlock_wait.a absl_ep-install/lib/libabsl_stacktrace.a absl_ep-install/lib/libabsl_status.a absl_ep-install/lib/libabsl_str_format_internal.a absl_ep-install/lib/libabsl_strings.a absl_ep-install/lib/libabsl_strings_internal.a absl_ep-install/lib/libabsl_symbolize.a absl_ep-install/lib/libabsl_synchronization.a absl_ep-install/lib/libabsl_throw_delegate.a absl_ep-install/lib/libabsl_time.a absl_ep-install/lib/libabsl_time_zone.a grpc_ep-install/lib/libgrpc.a grpc_ep-install/lib/libgpr.a grpc_ep-install/lib/libupb.a grpc_ep-install/lib/libaddress_sorting.a cares_ep-install/lib/libcares.a /usr/lib/x86_64-linux-gnu/libz.so debug/libarrow.so.300.0.0 /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so orc_ep-install/lib/liborc.a protobuf_ep-install/lib/libprotobuf.a awssdk_ep-install/lib/libaws-cpp-sdk-identity-management.a awssdk_ep-install/lib/libaws-cpp-sdk-sts.a awssdk_ep-install/lib/libaws-cpp-sdk-cognito-identity.a awssdk_ep-install/lib/libaws-cpp-sdk-s3.a awssdk_ep-install/lib/libaws-cpp-sdk-core.a /usr/lib/x86_64-linux-gnu/libcurl.so /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libcrypto.so awssdk_ep-install/lib/libaws-c-event-stream.a awssdk_ep-install/lib/libaws-checksums.a awssdk_ep-install/lib/libaws-c-common.a /usr/lib/x86_64-linux-gnu/libutf8proc.so /usr/lib/x86_64-linux-gnu/libre2.so -ldl jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/libmimalloc-debug.a -pthread -lrt -lpthread && :
/usr/bin/ld: absl_ep-install/lib/libabsl_stacktrace.a(stacktrace.cc.o): in function `void** NextStackFrame<false, false>(void**, void const*)':
/build/cpp/absl_ep-prefix/src/absl_ep/absl/debugging/internal/stacktrace_x86-inl.inc:283: undefined reference to `absl::lts_2020_09_23::debugging_internal::AddressIsReadable(void const*)'
/usr/bin/ld: absl_ep-install/lib/libabsl_stacktrace.a(stacktrace.cc.o): in function `void** NextStackFrame<false, true>(void**, void const*)':
/build/cpp/absl_ep-prefix/src/absl_ep/absl/debugging/internal/stacktrace_x86-inl.inc:283: undefined reference to `absl::lts_2020_09_23::debugging_internal::AddressIsReadable(void const*)'
/usr/bin/ld: absl_ep-install/lib/libabsl_symbolize.a(symbolize.cc.o): in function `absl::lts_2020_09_23::InitializeSymbolizer(char const*)':
/build/cpp/absl_ep-prefix/src/absl_ep/absl/debugging/symbolize_elf.inc:91: undefined reference to `absl::lts_2020_09_23::debugging_internal::VDSOSupport::Init()'
/usr/bin/ld: absl_ep-install/lib/libabsl_symbolize.a(symbolize.cc.o): in function `absl::lts_2020_09_23::debugging_internal::DemangleInplace(char*, int, char*, int)':
/build/cpp/absl_ep-prefix/src/absl_ep/absl/debugging/symbolize_elf.inc:1153: undefined reference to `absl::lts_2020_09_23::debugging_internal::Demangle(char const*, char*, int)'
/usr/bin/ld: absl_ep-install/lib/libabsl_symbolize.a(symbolize.cc.o): in function `absl::lts_2020_09_23::debugging_internal::(anonymous namespace)::Symbolizer::GetSymbol(void const*)':
/build/cpp/absl_ep-prefix/src/absl_ep/absl/debugging/symbolize_elf.inc:1383: undefined reference to `absl::lts_2020_09_23::debugging_internal::VDSOSupport::VDSOSupport()'
/usr/bin/ld: /build/cpp/absl_ep-prefix/src/absl_ep/absl/debugging/symbolize_elf.inc:1386: undefined reference to `absl::lts_2020_09_23::debugging_internal::VDSOSupport::LookupSymbolByAddress(void const*, absl::lts_2020_09_23::debugging_internal::ElfMemImage::SymbolInfo*) const'
collect2: error: ld returned 1 exit status
{noformat}",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Blocker,2021-01-13 01:53:42,1
13351894,[Packaging] Add mimalloc to Linux builds,As a result of ARROW-11009/ARROW-11049/etc. it is now possible to switch allocators at runtime. Previously only one allocator was compiled into the released builds (e.g. the PYPI version of pyarrow only included jemalloc on Linux and mimalloc on Windows). It might now make sense to compile both mimalloc and jemalloc on Linux builds to allow the user to switch out to the appropriate allocator at runtime if they need to.,pull-request-available,"['C++', 'Packaging', 'Python']",ARROW,Improvement,Major,2021-01-13 00:41:45,1
13351873,[C++][Dataset] Static build is failed,"This is caused by ARROW-10322.

https://app.circleci.com/pipelines/github/ursa-labs/crossbow/68346/workflows/fad0ef52-c3be-4193-806d-c1d39a58391b/jobs/14491

{noformat}
[469/638] Linking CXX executable debug/arrow-dataset-expression-testt.dir/level_conversion_test.cc.oKo.o.cc.oKKK
FAILED: debug/arrow-dataset-expression-test 
: && /usr/bin/ccache /usr/bin/c++  -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-deprecated-declarations -Wno-sign-conversion -Wno-unused-variable -Werror -fno-semantic-interposition -msse4.2  -g   src/arrow/dataset/CMakeFiles/arrow-dataset-expression-test.dir/expression_test.cc.o  -o debug/arrow-dataset-expression-test  -Wl,-rpath,/build/cpp/googletest_ep-prefix/lib debug/libarrow_testing.a debug/libarrow.a /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so orc_ep-install/lib/liborc.a /usr/lib/x86_64-linux-gnu/libprotobuf.so /usr/lib/x86_64-linux-gnu/libglog.so /usr/lib/x86_64-linux-gnu/libutf8proc.so /usr/lib/x86_64-linux-gnu/libre2.so -ldl /usr/lib/x86_64-linux-gnu/libboost_filesystem.so /usr/lib/x86_64-linux-gnu/libboost_system.so debug/libarrow_dataset.a debug/libarrow_testing.a debug/libarrow.a /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so orc_ep-install/lib/liborc.a /usr/lib/x86_64-linux-gnu/libprotobuf.so /usr/lib/x86_64-linux-gnu/libglog.so /usr/lib/x86_64-linux-gnu/libutf8proc.so /usr/lib/x86_64-linux-gnu/libre2.so -ldl googletest_ep-prefix/lib/libgtest_maind.so googletest_ep-prefix/lib/libgtestd.so googletest_ep-prefix/lib/libgmockd.so /usr/lib/x86_64-linux-gnu/libboost_filesystem.so /usr/lib/x86_64-linux-gnu/libboost_system.so debug/libparquet.a debug/libarrow.a /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so orc_ep-install/lib/liborc.a /usr/lib/x86_64-linux-gnu/libprotobuf.so /usr/lib/x86_64-linux-gnu/libglog.so /usr/lib/x86_64-linux-gnu/libutf8proc.so /usr/lib/x86_64-linux-gnu/libre2.so -ldl /usr/lib/x86_64-linux-gnu/libbz2.so /usr/lib/x86_64-linux-gnu/liblz4.so /usr/lib/x86_64-linux-gnu/libsnappy.so /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libzstd.so jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt thrift_ep-install/lib/libthriftd.a && :
debug/libarrow_dataset.a(expression.cc.o): In function `arrow::dataset::CallNotNull(arrow::dataset::Expression const&)':
/arrow/cpp/src/arrow/dataset/expression_internal.h:37: multiple definition of `arrow::dataset::CallNotNull(arrow::dataset::Expression const&)'
src/arrow/dataset/CMakeFiles/arrow-dataset-expression-test.dir/expression_test.cc.o:/arrow/cpp/src/arrow/dataset/expression_internal.h:37: first defined here
collect2: error: ld returned 1 exit status
{noformat}",pull-request-available,['C++'],ARROW,Bug,Blocker,2021-01-12 22:25:21,1
13351844,[Python][CI]  AMD64 Conda Python 3.7 Pandas 0.24  cron job failing in to_pandas extension dtype test,"See https://github.com/apache/arrow/pull/9096/checks?check_run_id=1689113764#step:8:5354

I suppose an error in the test specifically for pandas 0.24 (since we have quite some pandas version checks in that test)

This is happening only in the cron job, other (nightly) builds only have pandas 0.23, latest or master",pull-request-available,['Python'],ARROW,Bug,Major,2021-01-12 19:32:30,5
13351838,[Python][CI] Filesystem tests failing with s3fs 0.5.2,"See eg https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/37228135/job/mj3stkb1y0y4knmp?fullLog=true. 

There is no recent s3fs or fsspec release, but since a few hours conda started to install the latest s3fs 0.5.2, instead of the older 0.4.2, as it did before (without that s3fs is being pinned exactly)",pull-request-available,['Python'],ARROW,Bug,Major,2021-01-12 19:27:58,5
13351601,[Rust] Improve documentation for StringDictionaryBuilder,I find myself trying to remember the exact incantation to create a `StringDictionaryBuilder` so it should be a doc example,pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-11 22:57:44,11
13351590,[Packaging][Python] Use vcpkg as dependency source for manylinux and windows wheels,"So we can pin an explicit version of the environment, preventing any issues arising from dependency sources (like conda) which drag over time.

We can also enforce static linking by installing the static libraries only.",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2021-01-11 21:36:18,3
13351576,[CI] Restore workflows that had been blocked by INFRA,"See INFRA-21239, ARROW-11092, ARROW-11132",pull-request-available,['Continuous Integration'],ARROW,New Feature,Major,2021-01-11 19:55:28,4
13351497,"[C++][Compute][Python] Rename ""project"" kernel to ""make_struct""","The ""project"" compute Function is necessary for ARROW-11174. However it is not intended for direct use outside an Expression ([where the correspondence to projection is not immediately obvious|https://github.com/apache/arrow/pull/9131#issuecomment-757764173]) so it may be preferable to do one/more of:
 * rename the function to ""wrap_struct"" or similar so it does make sense outside Expressions
 * ensure the function is not exposed to python/R bindings except through Expressions
 * remove the function from the default registry",compute dataset pull-request-available,['C++'],ARROW,Improvement,Major,2021-01-11 15:02:17,6
13351334,[C++] Fix build failure with bundled gRPC and Protobuf,This is caused by ARROW-9400.,pull-request-available,['C++'],ARROW,Improvement,Blocker,2021-01-10 22:11:34,1
13351309,[Rust] [DateFusion] Physical operators and expressions should have public accessor methods,"Physical operators and expressions should have public accessor methods that expose the values used to construct them.

This allows projects that use DataFusion to be able to serialize a physical plan.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-10 15:23:24,10
13351307,[Packaging][Python] Ensure setuptools version during build supports markdown,We use a {{text/markdown}} long description and thus should always build/upload with at least setuptools 38.6.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2021-01-10 15:14:55,8
13351256,[Rust] [DataFusion] Built-in table providers should expose relevant fields,"For projects that depend on the DataFusion logical plan, there is a breaking change currently compared to 2.0.0 where it is not possible to introspect the TableScan node for CSV and Parquet files to get the file path and other meta-data (such as delimiter for CSV).

I propose adding public methods to the specific providers so that other crates can downcast to specific implementations in order to access this data.

My specific use case is the ability to serialize a DataFusion logical plan.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-09 20:39:29,10
13351252,[Rust] Enable SIMD for aarch64,"Enable SIMD for aarch64, which includes the Apple ARM CPUs.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-09 19:25:50,12
13351126,[C++][Dataset] Clean up compiler warnings,ARROW-10322 introduced some compiler warnings such as [https://github.com/apache/arrow/runs/1669466544#step:8:1434],pull-request-available,['C++'],ARROW,Bug,Major,2021-01-08 16:51:14,6
13351110,[Rust] [Parquet] Pin specific parquet-format-rs version,"We released paquet-format-rs v2.7.0, which has some incomatibilities with v2.6.x, so we should pin to the latter version.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-08 15:41:25,12
13350973,[R] Expose memory pool name and document setting it,"Followup to ARROW-11009, which did this in C++ and added the binding in Python. This could be useful not only for debugging but also for benchmarking.",pull-request-available,['R'],ARROW,New Feature,Major,2021-01-07 22:21:42,4
13350968,[C++][Dataset] Make Expressions available for projection,"RecordBatchProjector should be replaced by an expression calling the ""project"" compute function.

Projection currently supports only reordering and subselection of fields, materializing virtual columns where necessary. Replacement with an Expression will enable specifying arbitrary expressions for projected columns:
{code:java}
// project an explicit selection:
// SELECT a as ""a"", b as ""b"" ...
project({field_ref(""a""), field_ref(""b"")}, {""a"", ""b""});

// project an arithmetic expression:
// SELECT a + b as ""a + b"" ...
project({add(field_ref(""a""), field_ref(""b""))}, {""a + b""}){code}
This will also allow the same expression optimization machinery used for filters to be directly applied to projections. Virtual columns become a consequence of constant folding:
{code:java}
// project in a partition where a == 3:
assert(
  SimplifyWithGuarantee(
    project({field_ref(""a""), field_ref(""b"")}, {""a"", ""b""}),
    equal(field_ref(""a""), literal(3))
  )
  == project({literal(3), field_ref(""b"")}, {""a"", ""b""})
){code}
",pull-request-available,['C++'],ARROW,New Feature,Major,2021-01-07 21:56:26,6
13350934,[Rust] Add a comment explaining where float total_order algorithm came from,https://github.com/apache/arrow/pull/8882#discussion_r552202489,pull-request-available,['Rust'],ARROW,Improvement,Trivial,2021-01-07 17:01:03,11
13350920,[Python][Compute] Add bindings for ProjectOptions,"Similarly as ARROW-10725, need to expose {{ProjectOptions}}:

{code}
In [4]: from pyarrow.compute import project
/home/joris/scipy/repos/arrow/python/pyarrow/compute.py:137: RuntimeWarning: Python binding for ProjectOptions not exposed
  warnings.warn(""Python binding for {} not exposed""
{code}",pull-request-available,['Python'],ARROW,Bug,Blocker,2021-01-07 15:57:02,6
13350917,[Rust] [DataFusion] Document the desired SQL dialect for DataFusion,"As discussed https://github.com/apache/arrow/pull/9108#issuecomment-755791235

We would like to pick a single SQL dialect in DataFusion to avoid what happened with Spark SQL where functions, https://spark.apache.org/docs/latest/api/sql/index.html, were added seemingly ad-hoc making their usage very difficult and no clear feature matrix available.

Using an existing dialect will also allow us to re-use the documentation (and other tools) from that dialect. 

",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2021-01-07 15:42:11,11
13350913,[C++][Python] Compressed Feather file written with pyarrow 0.17 not readable in pyarrow 2.0.0+,"Originally from https://stackoverflow.com/questions/65413407/reading-in-feather-file-in-pyarrow-error-arrowinvalid-unrecognized-compressio


Writing with pyarrow 0.17:

{code:python}
In [1]: pa.__version__
Out[1]: '0.17.0'

In [2]: table = pa.table({'a': range(100)})

In [3]: from pyarrow import feather

In [4]: feather.write_feather(table, ""test_pa017_explicit.feather"", compression=""lz4"", version=2)

# according to docstring, this should do the same, but apparently not
In [5]: feather.write_feather(table, ""test_pa017_default.feather"")
{code}

Reading with pyarrow 1.0.0 works for both files, but reading it with master (pyarrow 2.0.0 gives the same error):

{code:python}
In [121]: pa.__version__
Out[121]: '3.0.0.dev552+g634f993f4'

In [123]: feather.read_table(""test_pa017_default.feather"")
Out[123]:
pyarrow.Table
a: int64

In [124]: feather.read_table(""test_pa017_explicit.feather"")
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-124-700e4b059ed5> in <module>
----> 1 feather.read_table(""test_py017_explicit.feather"")

~/scipy/repos/arrow/python/pyarrow/feather.py in read_table(source, columns, memory_map)
    238
    239     if columns is None:
--> 240         return reader.read()
    241
    242     column_types = [type(column) for column in columns]

~/scipy/repos/arrow/python/pyarrow/feather.pxi in pyarrow.lib.FeatherReader.read()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Unrecognized compression type: LZ4
In ../src/arrow/ipc/reader.cc, line 538, code: (_error_or_value8).status()
In ../src/arrow/ipc/reader.cc, line 594, code: GetCompressionExperimental(message, &compression)
In ../src/arrow/ipc/reader.cc, line 942, code: (_error_or_value23).status()
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2021-01-07 14:57:24,5
13350906,[Python][C++] S3Filesystem: file Content-Type not set correctly?,"I am using the Fileystem abstraction to write out html / text files to the local filesystem as well as s3.

I noticed that when using s3_fs.open_output_stream in combination with file.write(bytes), the object that gets created has a Content-Type of 'application/xml' even tough it's plain text, which is problematic for me.

Here is a minimal example:
{code:java}
import boto3
BUCKET = ""my-bucket""
path = f""s3://{BUCKET}/pyarrow_encoding.txt""
s3_fs, output_path = FileSystem.from_uri(path)
with s3_fs.open_output_stream(path=output_path, compression=None) as f:
    f.write('hello'.encode('UTF-8'))

s3 = boto3.client('s3')
response = s3.get_object(Bucket=BUCKET, Key='pyarrow_encoding.txt')
print(response['ContentType']) # Output: application/xml
print(response['Body'].read().decode('UTF-8')) # Output: hello

s3.put_object(Bucket=BUCKET,
              Key='boto3_encoding.txt',
              Body='hello'.encode('UTF-8'))
response = s3.get_object(Bucket=BUCKET, Key='boto3_encoding.txt')
print(response['ContentType']) # Output: binary/octet-stream
print(response['Body'].read().decode('UTF-8')) # Output: hello
{code}
I know, that the S3Filesystem implementation of pyarrow might no have mime type inference implemented, but I am wondering, why always 'application/xml' is the resulting Content-Type? Maybe this is hardcoded somewhere?

Originally, I tried this with '.html' files and also there, the objects on s3 always got the'application/xml' Content-Type. (Please also see attachment from the s3 console)



Any help or pointer is appreciated.

Thank you,

Nicolas",filesystem pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2021-01-07 14:25:28,2
13350735,[CI][C++] Fix Homebrew numpy installation on macOS builds,Numpy fails to install with homebrew because it tries to upgrade gcc and hits a {{brew link}} error. Running {{brew unlink gcc@8 gcc@9}} before {{brew install}} could work around this.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2021-01-06 19:33:41,4
13350728,[Rust] Set up bi-weekly Rust sync call and update website,"Given the momentum on the Rust implementation, I am going to set up a bi-weekly sync call on Google Meet most likely. The call will be at the same time as the current sync call but on alternate weeks.

I will update the web site to list both calls.",pull-request-available,['Rust'],ARROW,Task,Major,2021-01-06 18:35:39,10
13350668,[C++][Python][CI] Fix HDFS nightly build,"Hadoop version 2.9.2 is not available in downloads anymore, we need to move to a newer version.",pull-request-available,"['C++', 'Continuous Integration', 'Python']",ARROW,Bug,Major,2021-01-06 12:57:56,2
13349253,[C++][CI] ARM64 job on Travis-CI doesn't run tests,"When the ARM64 job is run on my Arrow fork, the tests are not run. It just compiles Arrow and installs it:
https://travis-ci.com/github/pitrou/arrow/jobs/468840776
",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2021-01-05 18:14:47,2
13349197,[CI] Use pip to install crossbow's dependencies for the comment bot,"setup-conda action is no longer enabled on github actions, so use an alternative way to install archery bot.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2021-01-05 13:06:43,3
13348984,[Rust] Document and test ARROW-10656,"Looks like I rebased against the PR branch, but didn't push my changes before the PR was merged.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-04 18:17:22,12
13348980,[Rust] Implement logical equality for list arrays,"We implemented logical equality for struct arrays, but not list arrays.

This work is now required for the Parquet nested list writer.",pull-request-available,['Rust'],ARROW,Improvement,Major,2021-01-04 18:04:49,12
13348613,[CI] (Temporarily) move offending workflows to separate files,"Without warning, INFRA broke several of our GitHub Actions workflows, and have been unresponsive all week. See https://issues.apache.org/jira/browse/INFRA-21239. Since then, the Rust developers have removed their offending actions, so those are no longer blocked. This PR does harm reduction for C++ and R workflows, moving the workflows that INFRA doesn't like to their own files (temporarily, I hope, while this business gets sorted out). This enables the other workflows in each file to run, so we at least get some C++ and R tests running, and we can still verify on our personal forks the workflows that have been blocked on apache/arrow.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-12-31 17:04:24,4
13348576,[Rust] Rust CI no longer works b/c it uses action-rs: Migrate CI away from action-rs/*,"INFRA team deactivated github actions for action-rs, which caused all our CI to stop working.

Since our dependency on it is really small, I propose that we just migrate our builds to not use it.",pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Bug,Blocker,2020-12-31 10:02:50,9
13348562,"[CI] Build ""Source Release and Merge Script"" is broken","The build ""Source Release and Merge Script"" is currently broken with the following error:


{code:java}
The following packages have unmet dependencies:
 dotnet-sdk-2.2 : Depends: aspnetcore-runtime-2.2 (>= 2.2.7) but it is not going to be installed
          Depends: dotnet-runtime-2.2 (>= 2.2.7) but it is not going to be installed
 E: Unable to correct problems, you have held broken packages.
{code}
 when running
{code:java}
  wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb
  sudo dpkg -i packages-microsoft-prod.deb
  sudo apt-get install apt-transport-https
  sudo apt-get update
  sudo apt-get install dotnet-sdk-2.2
{code}

I suspect that the reason this is not showing up in master is that this execution is behind a docker layer and therefore not triggered.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-12-31 07:42:52,1
13348534,[Java] Make IPC option immutable,"By making it immutable, the following benefits can be obtained:
1. It makes the code easier to reason about.
2. It allows JIT to make more optimizations.
3. Immutable objects can be shared, so many object allocations can be avoided.",pull-request-available,['Java'],ARROW,Improvement,Major,2020-12-31 02:05:12,7
13348465,[Rust] Lint Error on CI Tests in /arrow/rust/arrow/src/ipc/reader.rs,"Rustfmt error was introduced in this PR: https://github.com/apache/arrow/commit/30ce2eb5d4dc6136594f005f6b7ec7315afc9a88

Which leads to errors such as 

https://github.com/apache/arrow/pull/8648/checks?check_run_id=1625423680 

{code}
Diff in /arrow/rust/arrow/src/ipc/reader.rs at line 160:
             let null_count = struct_node.null_count() as usize;
             let struct_array = if null_count > 0 {
                 // create struct array from fields, arrays and null data
-                StructArray::from((
-                    struct_arrays,
-                    null_buffer,
-                ))
+                StructArray::from((struct_arrays, null_buffer))
             } else {
                 StructArray::from(struct_arrays)
             };
{code}

On all PRs in the repo ",pull-request-available,['Rust'],ARROW,Bug,Major,2020-12-30 12:04:56,11
13348321,[Java] Is there a bug in flight AddWritableBuffer,"[https://github.com/apache/arrow/blob/9bab12f03ac486bb8270f031b83f0a0411766b3e/java/flight/flight-core/src/main/java/org/apache/arrow/flight/grpc/AddWritableBuffer.java#L94]

buf.readBytes(stream, buf.readableBytes());

is this line redundant
In my perf.svg, this will copy the data from buf to OutputStream, which can not realize zero-copy.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2020-12-29 15:27:05,0
13348275,[Rust] Validate null counts when building arrays,"ArrayDataBuilder allows the user to specify a null count, alternatively calculating it if it is not set.

The problem is that the user-specified null count is never validated against the actual count of the buffer.

I suggest removing the ability to specify a null-count, and instead always calculating it.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-12-29 10:27:27,12
13348208,"[Rust] [DataFusion] Implement ""coalesce batches"" operator","When we have a FilterExec in the plan, it can produce lots of small batches and we therefore lose efficiency of vectorized operations.

We should implement a new CoalesceBatchExec and wrap every FilterExec with one of these so that small batches can be recombined into larger batches to improve the efficiency of upstream operators.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-12-29 02:46:34,10
13348190,[Rust] [DataFusion] Optimize joins with dynamic capacity for output batches,Rather than using the size of the left or right batches to determine the capacity of the output batches we can use the average size of previous output batches.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-12-28 22:13:23,10
13348179,[Rust] [DataFusion] Implement metrics in join operator,Implement metrics in join operator to make it easier to debug performance issues.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-12-28 20:40:09,10
13348165,[R] Handle RecordBatch in write_parquet,"write_parquet() fatally crashes the R environment when writing a 'record_batch' object

#Repro
{code:java}
// 

working_dir <- getwd()
dir.create(paste0(working_dir, '/test'))
out_file <- '/test.snappy.parquet'data(mtcars)
batch <- record_batch(mtcars)
write_parquet(batch, paste0(working_dir,out_file)){code}
",pull-request-available,['R'],ARROW,Bug,Major,2020-12-28 18:01:45,4
13348161,[Python] Expose alternate memory pools,"Currently, the default memory pool is exposed in Python but not the explicit memory pool singletons (jemalloc, mimalloc, system).

",pull-request-available,['Python'],ARROW,Improvement,Minor,2020-12-28 17:43:04,2
13347965,[Rust] Simplify builders with generics,There is some code duplication on the builders that can be removed via generics.,pull-request-available,['Rust'],ARROW,Task,Major,2020-12-26 19:17:53,9
13347915,[Rust] Remove `BufferBuilderTrait` and associated Result requirement.,"The trait is no longer used, as we completely split the build of boolean and non-boolean buffers, and the purpose of that trait was to support multiple implementations of the buffer building.



This trait also has `Result` for infallible methods.",pull-request-available,['Rust'],ARROW,Task,Minor,2020-12-26 06:13:08,9
13347914,[Rust] Improve performance of string fromIter,"Avoid copying from Vec to Buffer, writing directly to a buffer instead.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-12-26 06:03:09,9
13347773,[Rust] [DataFusion] Document why join order optimization does not work with filter pushdown,"When I run TPC-H query 12, I see that the join order is not optimized to put the smaller table on the left. I added some debug logging and see that the optimization sees row count of None on the left side because there is a filter wrapping the table scan.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-12-24 16:13:30,10
13347750,[Rust]: Run tests without requiring environment variables,"The desured outcome is that developers can now simply run cargo test in a typical checkout without having to mess with environment variables. I think this will lower the barrier to entry for people to contribute.

Building on the work on ARROW-10967 in https://github.com/apache/arrow/pull/8967 , 

",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-12-24 12:31:25,11
13347696,[C++][Parquet] Writing List<Struct> to parquet sometimes writes wrong data,"Sometimes when writing tables that contain List<Struct> columns, the data is written incorrectly. Here is a code sample that produces the error. There are no exceptions raised here, but a simple equality check via equals() yields False for the second test case...


{code:java}
import pyarrow as pa
import pyarrow.parquet as pq

# Write small amount of data to parquet file, and read it back. In this case, both tables are equal.
data1 = [[{'x':'abc','y':'abc'}]]*100 + [[{'x':'abc','y':'gcb'}]]*100
array1 = pa.array(data1)
table1 = pa.table([array1],names=['column'])
pq.write_table(table1,'temp1.parquet')
table1_1 = pq.read_table('temp1.parquet')
print(table1_1.equals(table1))

# Write larger amount of data to parquet file, and read it back. In this case, the tables are not equal.
data2 = data1*100
array2 = pa.array(data2)
table2 = pa.table([array2],names=['column'])
pq.write_table(table2,'temp2.parquet')
table2_1 = pq.read_table('temp2.parquet')
print(table2_1.equals(table2))

{code}


",pull-request-available,['Python'],ARROW,Bug,Major,2020-12-24 05:57:05,5
13347454,[Rust] [DataFusion] ParquetExec reports incorrect statistics,ParquetExec represents one or more Parquet files and currently only returns statistics based on the first file.,pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2020-12-22 23:14:07,10
13347446,[Rust] [DataFusion] Make write_csv and write_parquet concurrent,"ExecutionContext.write_csv and write_parquet currently iterate over the output partitions and execute one at a time and write the results out. We should run these as tokio tasks so they can run concurrently. This should, in theory, help with memory usage when the plan contains repartition operators.

We may want to add a configuration option so we can choose between serial and parallel writes?",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-12-22 21:48:18,10
13347422,[Python] Add environment variable to elect default usage of system memory allocator instead of jemalloc/mimalloc,We routinely get reports like ARROW-11007 where there is suspicion of a memory leak (which may or may not be valid)  having an easy way (requiring no changes to application code) to toggle usage of the non-system memory allocator would help with determining whether the memory usage patterns are the result of the allocator being used. ,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-12-22 19:45:34,2
13347166,[Rust] TPC-H parquet files cannot be read by Apache Spark,"The TPC-H parquet files generated by the benchmark crate cannot be read by Apache Spark because they use unsigned ints, which cannot be read in Spark (I am guessing because Java only has signed ints).

I would like to use the same data sets for benchmarking DataFusion, Apache Spark, and other tools.",pull-request-available,['Rust'],ARROW,Bug,Major,2020-12-21 16:05:00,10
13347054,[Rust] [DataFusion] Improve parallelism when reading Parquet files,"Currently the unit of parallelism is the number of parquet files being read.

For example, if we run a query against a Parquet table that consists of 8 partitions then we will attempt to run 8 async tasks in parallel and if there is a single Parquet file then we will only try and run 1 async task so this does not scale well. Also, if there are hundreds or thousands of Parquet files then we will try and process them all concurrently which also doesn't scale well.

These are the options for improving this situation:


 # Use Parquet row groups as the unit of partitioning and divide the number of row groups by the desired level of concurrency (defaulting to number of cores)
 # Keep file as the unit of partitions and add a RepartitionExec into the plan if there are fewer partitions (files) than cores and in the case where there are more files than cores, split the files up into lists so that each partition is a list of files rather than a single file. Each partition task will process one file at a time.



",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-12-20 23:07:10,10
13347049,[Rust] Fix bugs in TPC-H file conversion,"The utility in tpch for converting tbl files to CSV/Parquet no longer works now that we have multiple tables.

The UX is also terrible and the tool only supports generating uncompressed parquet files.

I'm going to create one PR to fix these things to make this more usable.",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2020-12-20 21:26:23,10
13347047,[CI][macOS] Fix Python 3.9 installation by Homebrew,"https://github.com/apache/arrow/runs/1579780011#step:4:520

{noformat}
==> Pouring python@3.9-3.9.1_1.catalina.bottle.tar.gz
Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink bin/2to3
Target /usr/local/bin/2to3
already exists. You may want to remove it:
  rm '/usr/local/bin/2to3'

To force the link and overwrite all conflicting files:
  brew link --overwrite python@3.9

To list all files that would be deleted:
  brew link --overwrite --dry-run python@3.9

Possible conflicting files are:
/usr/local/bin/2to3 -> /Library/Frameworks/Python.framework/Versions/2.7/bin/2to3
{noformat}",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-12-20 19:17:49,1
13346964,[C++] Require CMake 3.5 or later,"https://lists.apache.org/thread.html/r3bcee82bfbfaa77f5180e064b6bd9d2fe2fb5b36fb0f571b08ad6fac%40%3Cdev.arrow.apache.org%3E

{noformat}
We require CMake 3.2 or later. Can we require more newer
CMake? I don't to want to add workaround for CMake 3.2 like
this:

https://github.com/apache/arrow/blob/master/cpp/cmake_modules/Findre2Alt.cmake#L78-L79

  # CMake 3.2 does uppercase the FOUND variable
  if(re2Alt_FOUND OR RE2ALT_FOUND)


Ubuntu 16.04 ships CMake 3.5.1:

  https://packages.ubuntu.com/search?keywords=cmake

CentOS 7 doesn't ship CMake 3 but EPEL provides CMake 3.17:

  https://src.fedoraproject.org/rpms/cmake3/blob/epel7/f/cmake3.spec


Can we require CMake 3.5 or later?
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-12-19 22:39:00,1
13346941,[Rust] Update unsafe guidelines for adding JIRA references,We should document known soundness issues in Jira issues and reference them from the source code.,pull-request-available,['Rust'],ARROW,Sub-task,Minor,2020-12-19 15:50:34,10
13346827,[Java][FlightRPC] FlightData deserializer should accept missing fields,"To be compatible with Protobuf implementations, missing fields should not be treated as an error. The C++ implementation has the same issue, and this is causing issues with the C# and Rust implementations (and presumably the Go implementation)",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2020-12-18 16:48:39,0
13346799,[C++] Add scalar string join kernel,"Similar to Python's str.join

For example, if the separator were {{""-""}}, this takes input:
{code}
ListArray<list<item: string>>
[
  [""foo"",""bar""],
  [""push"",""pop""]
]
{code}
and returns output:
{code}
Array<string>
[ 
  ""foo-bar"",
  ""push-pop""
] 
{code}",pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2020-12-18 14:47:12,2
13346706,[C++] Reading empty json lists results in invalid non-nullable null type,"We're using Arrow to convert from JSON to Parquet and occasionally have empty lists in our json. Reading such JSON into an Arrow table and writing it to Parquet currently fails. We noticed this issue in our C++ Arrow code, but it also happens from Python.

Minimal repro:

input.json:

{""foo"": []}



convert.py:
 import pyarrow.json
 import pyarrow.parquet

t = pyarrow.json.read_json(""input.json"")
 pyarrow.parquet.write_table(t, ""out.parquet"")
 

Produces:

Traceback (most recent call last):
 File ""repro.py"", line 5, in <module>
 pyarrow.parquet.write_table(t, ""out.parquet"")
env/lib/python3.8/site-packages/pyarrow/parquet.py"", line 1717, in write_table
 with ParquetWriter(
 File ""env/lib/python3.8/site-packages/pyarrow/parquet.py"", line 554, in __init__
 self.writer = _parquet.ParquetWriter(
 File ""pyarrow/_parquet.pyx"", line 1409, in pyarrow._parquet.ParquetWriter.__cinit__
 File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
 pyarrow.lib.ArrowInvalid: NullType Arrow field must be nullable

",json pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-12-18 03:41:26,2
13346573,[Python][CI] Nightly pandas builds failing because of pytest monkeypatch issue,See https://github.com/apache/arrow/pull/8934#issuecomment-746975918,pull-request-available,['Python'],ARROW,Improvement,Major,2020-12-17 11:29:13,5
13346406,[Rust] Implement min/max kernels for BooleanArray,"While this operation is of very limited utility, for completness and uniformity I would like to have a min/max aggregation kernel that works for BooleanArrays. Currently we have ones for primitive value (e.g. numeric) arrays as well as strings, etc.
",pull-request-available,['Rust'],ARROW,New Feature,Minor,2020-12-16 18:23:15,11
13346387,[C++] S3FileSystem::Impl::IsEmptyDirectory fails on Amazon S3,"Running S3FileSystem::GetFileInfo() where the path is in the form ""bucket-name/dir-name"" and this is a bucket on AWS S3, it throws the following error:

""When reading information for key 'dir-name' in bucket 'bucket-name': AWS Error [code 15]: No response body.

I tracked down the issue to the IsEmptyDirectory method, and noticed that removing kSep from this line:
 req.SetKey(ToAwsString(key) + kSep);

fixes the issue.

However, I don't know why kSep is needed in the first place so I'm not sure what a good solution would be.
 Also, the key variable on entering IsEmptyDirectory is just the name of the directory (doesn't have separators).",pull-request-available,['C++'],ARROW,Bug,Major,2020-12-16 16:41:00,2
13346236,[Python] Tests are failed with fsspec-0.8.5,"https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/36851219/job/lwywl76d82coawpd?fullLog=true#L2284

{noformat}
================================== FAILURES ===================================
_ test_get_file_info_with_selector[PyFileSystem(FSSpecHandler(fsspec.filesystem(""memory"")))] _
fs = <pyarrow._fs.PyFileSystem object at 0x00000140F4C0A3A0>
pathfn = <function py_fsspec_memoryfs.<locals>.<lambda> at 0x00000140F4BFBB58>
    def test_get_file_info_with_selector(fs, pathfn):
        base_dir = pathfn('selector-dir/')
        file_a = pathfn('selector-dir/test_file_a')
        file_b = pathfn('selector-dir/test_file_b')
        dir_a = pathfn('selector-dir/test_dir_a')
        file_c = pathfn('selector-dir/test_dir_a/test_file_c')
        dir_b = pathfn('selector-dir/test_dir_b')
    
        try:
            fs.create_dir(base_dir)
            with fs.open_output_stream(file_a):
                pass
            with fs.open_output_stream(file_b):
                pass
            fs.create_dir(dir_a)
            with fs.open_output_stream(file_c):
                pass
            fs.create_dir(dir_b)
    
            # recursive selector
            selector = FileSelector(base_dir, allow_not_found=False,
                                    recursive=True)
            assert selector.base_dir == base_dir
    
            infos = fs.get_file_info(selector)
            if fs.type_name == ""py::fsspec+s3"":
                # s3fs only lists directories if they are not empty
                assert len(infos) == 4
            else:
                assert len(infos) == 5
    
            for info in infos:
                if (info.path.endswith(file_a) or info.path.endswith(file_b) or
                        info.path.endswith(file_c)):
                    assert info.type == FileType.File
                elif (info.path.rstrip(""/"").endswith(dir_a) or
                      info.path.rstrip(""/"").endswith(dir_b)):
                    assert info.type == FileType.Directory
                else:
                    raise ValueError('unexpected path {}'.format(info.path))
                check_mtime_or_absent(info)
    
            # non-recursive selector -> not selecting the nested file_c
            selector = FileSelector(base_dir, recursive=False)
    
            infos = fs.get_file_info(selector)
            if fs.type_name == ""py::fsspec+s3"":
                # s3fs only lists directories if they are not empty
                assert len(infos) == 3
            else:
                assert len(infos) == 4
    
        finally:
>           fs.delete_dir(base_dir)
pyarrow\tests\test_fs.py:716: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow\_fs.pyx:472: in pyarrow._fs.FileSystem.delete_dir
    check_status(self.fs.DeleteDir(directory))
pyarrow\_fs.pyx:1035: in pyarrow._fs._cb_delete_dir
    handler.delete_dir(frombytes(path))
pyarrow\fs.py:262: in delete_dir
    self.fs.rm(path, recursive=True)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:176: in rm
    self.rm_file(p)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\spec.py:840: in rm_file
    self._rm(path)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:163: in _rm
    self.rmdir(path)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = <fsspec.implementations.memory.MemoryFileSystem object at 0x00000140F4ABBC58>
path = 'selector-dir'
    def rmdir(self, path):
        path = path.rstrip(""/"")
        if path in self.pseudo_dirs:
            if not self.ls(path):
                self.pseudo_dirs.remove(path)
            else:
>               raise OSError(ENOTEMPTY, ""Directory not empty"", path)
E               OSError: [Errno 41] Directory not empty: 'selector-dir'
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:110: OSError
__ test_delete_dir[PyFileSystem(FSSpecHandler(fsspec.filesystem(""memory"")))] __
fs = <pyarrow._fs.PyFileSystem object at 0x00000140F4C1CDC0>
pathfn = <function py_fsspec_memoryfs.<locals>.<lambda> at 0x00000140F50BC738>
    def test_delete_dir(fs, pathfn):
        skip_fsspec_s3fs(fs)
    
        d = pathfn('directory/')
        nd = pathfn('directory/nested/')
    
        fs.create_dir(nd)
>       fs.delete_dir(d)
pyarrow\tests\test_fs.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow\_fs.pyx:472: in pyarrow._fs.FileSystem.delete_dir
    check_status(self.fs.DeleteDir(directory))
pyarrow\_fs.pyx:1035: in pyarrow._fs._cb_delete_dir
    handler.delete_dir(frombytes(path))
pyarrow\fs.py:262: in delete_dir
    self.fs.rm(path, recursive=True)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:176: in rm
    self.rm_file(p)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\spec.py:840: in rm_file
    self._rm(path)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:163: in _rm
    self.rmdir(path)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = <fsspec.implementations.memory.MemoryFileSystem object at 0x00000140F4ABBC58>
path = 'directory'
    def rmdir(self, path):
        path = path.rstrip(""/"")
        if path in self.pseudo_dirs:
            if not self.ls(path):
                self.pseudo_dirs.remove(path)
            else:
>               raise OSError(ENOTEMPTY, ""Directory not empty"", path)
E               OSError: [Errno 41] Directory not empty: 'directory'
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:110: OSError
{noformat}",pull-request-available,['Python'],ARROW,Improvement,Blocker,2020-12-16 03:23:30,5
13346203,[Rust] Update docs in regard to stable rust,Update the docs to include changes after https://github.com/apache/arrow/pull/8698,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-12-15 21:27:19,11
13346194,[Python] LargeListType doesn't have a value_field,"This one is easy: it looks like the LargeListType is just missing this field. Here it is for a 32-bit list (the reason I want this is to get at the ""nullable"" field, although the ""metadata"" would be nice, too):
{code:java}
>>> import pyarrow as pa
>>> small_array = pa.ListArray.from_arrays(pa.array([0, 3, 3, 5]), pa.array([1.1, 2.2, 3.3, 4.4, 5.5]))
>>> small_array.type.value_field
pyarrow.Field<item: double>
>>> small_array.type.value_field.nullable
True{code}
Now with a large list:
{code:java}
>>> large_array = pa.LargeListArray.from_arrays(pa.array([0, 3, 3, 5]), pa.array([1.1, 2.2, 3.3, 4.4, 5.5]))
>>> large_array.type.value_field
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
AttributeError: 'pyarrow.lib.LargeListType' object has no attribute 'value_field'{code}
Verifying version:
{code:java}
>>> pa.__version__
'2.0.0'{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-12-15 20:49:10,5
13346193,[Rust] Migrate CI tests to stable rust,"With the merging of https://github.com/apache/arrow/pull/8698 the parquet writer now supports stable rust and we should be able to run most of our CI checks with stable rust rather than nightly to ensure no more unstable features are added. 

[~jorgecarleitao] has started on this -- in particular this patch: https://github.com/jorgecarleitao/arrow/commit/ca66d6d945e265dd2c83464bd80ff1dd7d231f7c",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-12-15 20:48:16,11
13346100,[C++][Doc] Document supported Parquet features,We should document the Parquet features supported by our C++ implementation.,pull-request-available,"['C++', 'Documentation']",ARROW,Task,Major,2020-12-15 12:35:11,2
13346097,[Rust][Doc] Update feature matrix,The [status matrix](https://github.com/apache/arrow/blob/master/docs/source/status.rst) should be updated with the latest Rust additions (for example the C data interface support).,pull-request-available,"['Documentation', 'Rust']",ARROW,Task,Blocker,2020-12-15 12:28:35,12
13345723,[Rust] [DataFusion] Easier clippy fixes,Address some of the clippy lints that clippy can fix automatically,pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-12-13 10:31:33,12
13345700,[Rust] Document our approach to unsafe code in README,It would be helpful to document the project's stance on the use of unsafe code in a prominent location such as in the top-level README so that we can refer people to this when questions are asked about this.,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-12-12 19:06:00,9
13345696,[C++][Doc] Document IPC API,"I just noticed that we forgot to document the C++ Arrow IPC APIs, both in the ""user guide"" and ""API reference"" chapters.
",pull-request-available,"['C++', 'Documentation']",ARROW,Task,Major,2020-12-12 18:50:33,2
13345490,[Python][Dataset] Writing dataset from python iterator of record batches,"At the moment, from python you can write a dataset with {{ds.write_dataset}} for example starting from a *list* of record batches. 

But this currently needs to be an actual list (or gets converted to a list), so an iterator or generated gets fully consumed (potentially bringing the record batches in memory), before starting to write. 

We should also be able to use the python iterator itself to back a {{RecordBatchIterator}}-like object, that can be consumed while writing the batches.

We already have a {{arrow::py::PyRecordBatchReader}} that might be useful here.",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2020-12-11 10:58:26,0
13345475,[C++]EXC_BAD_ACCESS in BaseSetBitRunReader<false>::NextRun,"{{./release/parquet-encoding-benchmark}} fails with

{code}
BM_PlainDecodingFloat/65536                              4206 ns         4206 ns       167354 bytes_per_second=58.0474G/s
error: libparquet.300.dylib debug map object file '/Users/uwe/Development/arrow/cpp/build/src/parquet/CMakeFiles/parquet_objlib.dir/encoding.cc.o' has changed (actual time is 2020-12-10 22:57:29.000000000, debug map time is 2020-12-10 21:02:52.000000000) since this executable was linked, file will be ignored
Process 11120 stopped
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)
    frame #0: 0x000000010047fe04 libparquet.300.dylib`arrow::internal::BaseSetBitRunReader<false>::NextRun() + 192
libparquet.300.dylib`arrow::internal::BaseSetBitRunReader<false>::NextRun:
->  0x10047fe04 <+192>: ldur   x11, [x9, #-0x8]
    0x10047fe08 <+196>: str    x9, [x19]
    0x10047fe0c <+200>: str    x11, [x19, #0x18]
    0x10047fe10 <+204>: rbit   x10, x11
Target 0: (parquet-encoding-benchmark) stopped.
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)
  * frame #0: 0x000000010047fe04 libparquet.300.dylib`arrow::internal::BaseSetBitRunReader<false>::NextRun() + 192
    frame #1: 0x000000010047f808 libparquet.300.dylib`parquet::(anonymous namespace)::PlainEncoder<parquet::PhysicalType<(parquet::Type::type)0> >::PutSpaced(bool const*, int, unsigned char const*, long long) + 336
    frame #2: 0x0000000100008970 parquet-encoding-benchmark`parquet::BM_PlainEncodingSpacedBoolean(benchmark::State&) at encoding_benchmark.cc:249:14 [opt]
    frame #3: 0x000000010000881c parquet-encoding-benchmark`parquet::BM_PlainEncodingSpacedBoolean(state=0x000000016fdfd4b8) at encoding_benchmark.cc:257 [opt]
    frame #4: 0x00000001001614f4 libbenchmark.0.dylib`benchmark::internal::BenchmarkInstance::Run(unsigned long long, int, benchmark::internal::ThreadTimer*, benchmark::internal::ThreadManager*) const + 68
    frame #5: 0x0000000100173ae8 libbenchmark.0.dylib`benchmark::internal::(anonymous namespace)::RunInThread(benchmark::internal::BenchmarkInstance const*, unsigned long long, int, benchmark::internal::ThreadManager*) + 80
    frame #6: 0x00000001001723c8 libbenchmark.0.dylib`benchmark::internal::RunBenchmark(benchmark::internal::BenchmarkInstance const&, std::__1::vector<benchmark::BenchmarkReporter::Run, std::__1::allocator<benchmark::BenchmarkReporter::Run> >*) + 1284
    frame #7: 0x000000010015ee7c libbenchmark.0.dylib`benchmark::RunSpecifiedBenchmarks(benchmark::BenchmarkReporter*, benchmark::BenchmarkReporter*) + 1824
    frame #8: 0x000000010014beec libbenchmark_main.0.dylib`main + 76
    frame #9: 0x000000019e270f54 libdyld.dylib`start + 4
{code}",osx-arm64 pull-request-available,['C++'],ARROW,Task,Major,2020-12-11 09:29:40,8
13345474,[Java] Support compressing RecordBatch IPC buffers by LZ4,Support compressing/decompressing RecordBatch IPC buffers by LZ4.,pull-request-available,['Java'],ARROW,New Feature,Major,2020-12-11 09:25:11,7
13345338,[C++] Apple Silicon is reported as arm64 in CMake,Currently we try to build with AVX2 on this platform which raises a lot of errors.,osx-arm64 pull-request-available,['C++'],ARROW,Task,Major,2020-12-10 16:41:59,8
13345156,build failure on aarch64 with -DARROW_PYTHON=ON and gcc,"Arrow will trigger compiler errors in (at least) gcc7, gcc8 and gcc9 on aarch64 on a https://aws.amazon.com/ec2/instance-types/c6/ instance.
Compiling with clang-11 works fine.

```
../src/arrow/compute/kernels/scalar_cast_nested.cc: In function void arrow::compute::internal::CastListExec(arrow::compute::KernelContext*, const arrow
::compute::ExecBatch&, arrow::Datum*) [with Type = arrow::LargeListType]:                                                                              ../src/arrow/compute/kernels/scalar_cast_nested.cc:33:6: internal compiler error: Segmentation fault
 void CastListExec(KernelContext* ctx, const ExecBatch& batch, Datum* out) {
      ^~~~~~~~~~~~
Please submit a full bug report,
with preprocessed source if appropriate.                                                                                                                See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
```

Full build log attached.",pull-request-available,['C++'],ARROW,Task,Blocker,2020-12-09 23:08:36,2
13345142,"[Rust][DataFusion] More ergonomic conversion between Schema, SchemaRef, DFSchema, and DFSchemaRef","https://github.com/apache/arrow/pull/8839 added a DFSchema wrapper object into DataFusion so we can represent multiple relations (e.g. tables) easily

However, it is somewhat annyoing to use now (as I discovered while trying to update IOx to use the latest version of Arrow).

Specifically, there are things like this:
```
Arc::new(DFSchema::from(&parquet.schema())?),
```
Instead I would like to be able to write

```
parquet.schema().try_into()?
```


There are several other conversions I would like to be able to use the standard Rust into  or `try_into` for:

Schema -> DFSchema
Schema -> Arc<DFSchema>
Arc<Schema> -> DFSchema
Arc<Schema> -> Arc<DFSchema>

DFSchema -> Schema
DFSchema -> Arc<Schema>
Arc<DFSchema> -> Schema
Arc<DFSchema> -> Arc<Schema>
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-12-09 20:47:45,11
13345074,[Python] ExtensionArray.to_pandas not working,"Hi!

When I create a pandas interval series and then convert it into a pyarrow array and then try to convert that pyarrow array back to pandas I'm getting an attribute error. The error says that Series object has no attribute `to_pandas`.

I've added the code that produces the error below.


{code:java}
// code placeholder

In [14]: x = pd.Series([pd.Interval(0, 1), pd.Interval(2, 3), pd.Interval(3, 4)])

In [15]: y = pa.Array.from_pandas(x)

In [16]: y.type
Out[17]: ArrowIntervalType(extension<pandas.interval>)

In [17]: y.to_pandas()
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-17-48c5b189df56> in <module>
----> 1 y.to_pandas()~/compose/etc/conda/cuda_10.2/envs/rapids/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()~/compose/etc/conda/cuda_10.2/envs/rapids/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.ExtensionArray._to_pandas()~/compose/etc/conda/cuda_10.2/envs/rapids/lib/python3.7/site-packages/pandas/core/generic.py in __getattr__(self, name)
   5128             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5129                 return self[name]
-> 5130             return object.__getattribute__(self, name)
   5131 
   5132     def __setattr__(self, name: str, value) -> None:AttributeError: 'Series' object has no attribute 'to_pandas'


{code}
It would be great tp have a method to convert from a pyarrow interval array directly into a pandas series.",pandas pull-request-available python,['Python'],ARROW,Bug,Major,2020-12-09 12:47:46,5
13345029,[Python] Update minimal NumPy version to 1.16.6,As part of the mitigation of https://github.com/numpy/numpy/issues/17913,pull-request-available,['Python'],ARROW,Task,Major,2020-12-09 09:47:48,8
13344880,[C++] Reduce code size of vector_sort.cc,"Code is generated for many types with a same physical representation, such as temporals.",pull-request-available,['C++'],ARROW,Task,Minor,2020-12-08 16:48:20,2
13344856,[Python] Handle numpy deprecation warnings for builtin type aliases,See https://numpy.org/devdocs/release/1.20.0-notes.html#using-the-aliases-of-builtin-types-like-np-int-is-deprecated,pull-request-available,['Python'],ARROW,Improvement,Major,2020-12-08 14:57:08,5
13344823,[C++] Add async filesystem operations,It would probably be useful to have Future-returning variants of some filesystem operations (at least {{GetFileInfo}} and {{OpenInput(File|Stream)}}).,pull-request-available,['C++'],ARROW,Improvement,Major,2020-12-08 12:20:55,2
13344734,[Rust] [DataFusion] join of two DataFrames is not possible,"The complete failing test:


{code:java}
use std::sync::Arc;

use arrow::{array::{Int32Array, StringArray}, record_batch::RecordBatch};
use arrow::datatypes::{DataType, Field, Schema};

use datafusion::{datasource::MemTable, prelude::JoinType};
use datafusion::error::Result;

use datafusion::execution::context::ExecutionContext;

#[tokio::test]
async fn join() -> Result<()> {
    let schema1 = Arc::new(Schema::new(vec![
        Field::new(""a"", DataType::Utf8, false),
        Field::new(""b"", DataType::Int32, false),
    ]));
    let schema2 = Arc::new(Schema::new(vec![
        Field::new(""a"", DataType::Utf8, false),
        Field::new(""c"", DataType::Int32, false),
    ]));

    // define data.
    let batch1 = RecordBatch::try_new(
        schema1.clone(),
        vec![
            Arc::new(StringArray::from(vec![""a"", ""b"", ""c"", ""d""])),
            Arc::new(Int32Array::from(vec![1, 10, 10, 100])),
        ],
    )?;
    // define data.
    let batch2 = RecordBatch::try_new(
        schema2.clone(),
        vec![
            Arc::new(StringArray::from(vec![""a"", ""b"", ""c"", ""d""])),
            Arc::new(Int32Array::from(vec![1, 10, 10, 100])),
        ],
    )?;

    let mut ctx = ExecutionContext::new();

    let table1 = MemTable::new(schema1, vec![vec![batch1]])?;
    let table2 = MemTable::new(schema2, vec![vec![batch2]])?;

    ctx.register_table(""aa"", Box::new(table1));

    let df1 = ctx.table(""aa"")?;

    ctx.register_table(""aaa"", Box::new(table2));

    let df2 = ctx.table(""aaa"")?;

    let a = df1.join(df2, JoinType::Inner, &[""a""], &[""a""])?;

    let batches = a.collect().await?;
    assert_eq!(batches.len(), 1);

    Ok(())
}
{code}


When the create dataframes via `ctx.table`, they receive a clone of the \{{ExecutionContextState}} If at a later stage the context receives a new table, that table will not be part of the state on the first DataFrame. On a Join op, the left DataFrame's state is passed to the newly created DataFrame, which is then used in collect(). Because the right side has a table not in the state of the left, the execution fails.



We may need an Arc<Mutex<{{ExecutionContextState}}>> to share a common mutable state across multiple DataFrames. Alternatively, not require tables to be registered in the context to be used by DataFrames.

Note that the current example in `DataFrame::join` docs works because the table is registered for both DataFrames.",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Blocker,2020-12-08 07:36:18,9
13344447,[Rust] [DataFusion] Implement Into<Schema> for DFSchema,Implement Into<Schema> for DFSchema,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-12-06 18:43:55,10
13344383,[Rust] [DataFusion] Add optional qualifier to Expr::Column,"This follows on from https://issues.apache.org/jira/browse/ARROW-10732 and does the integration to complete the feature.



Expr::Column will be updated to


{code:java}
Column {
    /// Optional qualifier
    qualifier: Option<String>,
    /// Column name
    name: String,
}, {code}",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-12-05 23:31:48,10
13344381,[Packaging][deb] Drop support for Debian GNU/Linux Stretch,"It reached EOL at 2020-07-06.

See also: https://wiki.debian.org/DebianReleases",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-12-05 21:15:11,1
13344379,[Rust] [DataFusion] Implement DFSchema,Implement DFSchema,pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-12-05 20:55:40,10
13344359,[R][CI] Remove nightly centos6 build,It has stopped working since CentOS 6 went EOL.,pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2020-12-05 16:08:28,4
13344157,[Rust] Remove UB on parquet crate,"While trying to remove the unsafe `FatPtr`, I stumbled upon the borrow checker not allowing the code to compile, which suggests that the `FatPtr` is being used to bypass the borrow checked, which I do not think is intentional.",pull-request-available,['Rust'],ARROW,Bug,Major,2020-12-04 06:18:08,9
13344117,[R] Support R >= 3.3 and add CI,"R install on clean AWS EMR fails. I've tried multiple methods:



From within R

{{> source(""[https://raw.githubusercontent.com/apache/arrow/master/r/R/install-arrow.R]"")}}

{{> install_arrow()}}

{{}}

{{And also via:}}
sudo R -e ""install.packages('arrow', repos ='https://cloud.r-project.org',dependencies = TRUE)""

All dependencies seem to install ok but the arrow R package itself fails:

++ -m64 -std=gnu++11 -I/usr/include/R -DNDEBUG -I/mnt/tmp/RtmpVCRQK4/R.INSTALL591557cd8b23/arrow/libarrow/arrow-2.0.0/include -DARROW_R_WITH_ARROW -I""/usr/lib64/R/library/cpp11/include"" -I/usr/local/include -fpic -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -c array_from_vector.cpp -o array_from_vector.o
array_from_vector.cpp: In instantiation of 'std::shared_ptr<arrow::Array> arrow::r::MakeSimpleArray(SEXP) [with int RTYPE = 13; RVector = cpp11::r_vector<int>; Type = arrow::Int32Type; SEXP = SEXPREC*]':
array_from_vector.cpp:1406:65: required from here
*array_from_vector.cpp:1361:59: error: 'DATAPTR' was not declared in this scope*
 *auto p_vec_start = reinterpret_cast<value_type*>(DATAPTR(vec));*
 *~~~~~~~^~~~~*
*array_from_vector.cpp:1380:10: error: unable to deduce 'auto' from 'first_na'*
 *auto p_vec = first_na;*
 ^~~~~
array_from_vector.cpp: In instantiation of 'std::shared_ptr<arrow::Array> arrow::r::MakeSimpleArray(SEXP) [with int RTYPE = 14; RVector = cpp11::r_vector<double>; Type = arrow::Int64Type; SEXP = SEXPREC*]':
*array_from_vector.cpp:1408:65: required from here*
*array_from_vector.cpp:1361:59: error: 'DATAPTR' was not declared in this scope*
 *auto p_vec_start = reinterpret_cast<value_type*>(DATAPTR(vec));*
 ~~~~~~~^~~~~
array_from_vector.cpp:1380:10: error: unable to deduce 'auto' from 'first_na'
 auto p_vec = first_na;
 ^~~~~
array_from_vector.cpp: In instantiation of 'std::shared_ptr<arrow::Array> arrow::r::MakeSimpleArray(SEXP) [with int RTYPE = 14; RVector = cpp11::r_vector<double>; Type = arrow::DoubleType; SEXP = SEXPREC*]':
*array_from_vector.cpp:1410:66: required from here*
*array_from_vector.cpp:1361:59: error: 'DATAPTR' was not declared in this scope*
 auto p_vec_start = reinterpret_cast<value_type*>(DATAPTR(vec));
 ~~~~~~~^~~~~
*array_from_vector.cpp:1380:10: error: unable to deduce 'auto' from 'first_na'*
 *auto p_vec = first_na;*
 ^~~~~
array_from_vector.cpp: In instantiation of 'std::shared_ptr<arrow::Array> arrow::r::MakeSimpleArray(SEXP) [with int RTYPE = 24; RVector = cpp11::r_vector<unsigned char>; Type = arrow::UInt8Type; SEXP = SEXPREC*]':
array_from_vector.cpp:1412:61: required from here
*array_from_vector.cpp:1361:59: error: 'DATAPTR' was not declared in this scope*
 *auto p_vec_start = reinterpret_cast<value_type*>(DATAPTR(vec));*
 ~~~~~~~^~~~~
*array_from_vector.cpp:1380:10: error: unable to deduce 'auto' from 'first_na'*
 *auto p_vec = first_na;*
 ^~~~~
make: *** [array_from_vector.o] Error 1
ERROR: compilation failed for package 'arrow'
* removing '/usr/lib64/R/library/arrow'

The downloaded source packages are in
 '/mnt/tmp/RtmpyLdG80/downloaded_packages'
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Warning message:
In install.packages(""arrow"", repos = arrow_repos(repos, nightly), :
 installation of package 'arrow' had non-zero exit status

{{}}",pull-request-available,['R'],ARROW,Bug,Major,2020-12-04 00:08:52,4
13344058,[C++] Investigate faster random generation for tests and benchmarks,Test and benchmark harness sometimes has a non-trivial cost when generating large random data (e.g. {{vector_sort_benchmark.cc}}). We may try a non-standard PRNG such as [PCG|https://www.pcg-random.org/using-pcg-cpp.html].,pull-request-available,['C++'],ARROW,Wish,Trivial,2020-12-03 16:32:15,2
13344011,[C++] Investigate RecordBatch sort performance,"A RecordBatch is currently sorted as a one-chunk Table. It is suboptimal as it's paying the chunk resolution cost even though it's not necessary.

A dedicated RecordBatch sort may also form the basis for a faster Table sort (first sort individual batches, then use merge sort along the sorted batches).
",pull-request-available,['C++'],ARROW,Wish,Major,2020-12-03 12:41:27,2
13343828,[C++][Compute] Investigate ChunkedArray sort performance,"From our micro-benchmarks, it seems that sorting a ChunkedArray can be significantly slower than sorting a linear Array of the same size. Perhaps this can be improved.",pull-request-available,['C++'],ARROW,Wish,Major,2020-12-02 17:39:01,2
13343758,[C++] Make S3 recursive walks parallel,"Doing a recursive S3 directory walk using GetFileInfo(Selector) currently lists all encountered directories serially, waiting for the results of one directory listing (or portion thereof) before launching the next one. Instead, we should use the Async APIs provided by the AWS SDK to parallelize HTTP requests as much as possible.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-12-02 12:41:48,2
13343682,[Packaging][RPM] Drop support for CentOS 6,Because CentOS 6 reached EOL at 2020-11-30.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-12-02 03:08:49,1
13343412,[Packaging][Python] PyPI pyarrow source dist (sdist) contains architecture dependent binaries ,"Downloading the most recent pyarrow ""sdist"" *source* tarball from Pypi and upon extraction, the package contains multiple *binary*libraries compiled for x86-64. (libarrow, libparquet, etc.)



The ultimate result is that this isn't a source package at all - it would be fine to include binaries in a python wheel but including arch/platform specific binaries in a sdist breaks pip and the install. (In my case, trying to install on aarch64.)

As a general observation, this will become a larger issue as, for example, the ARM-based Macs come to market.

That said, one commonly implemented option is to make the python source package download and build any dependent libraries.



At the very least, the source package should not contain binaries. I suppose it's not much different from a *source* Debian package containing compiled binary code.

",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-11-30 21:42:30,1
13343368,[C++] Provide iterator access to primitive elements inside an Array,"STL-style iteration can be convenient in non performance critical scenarios, for example testing.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-11-30 18:13:41,2
13343229,[Rust] Support reading nested JSON lists,"The JSON reader now supports reading nested structs, but we are still left with nested lists, which can be lists of lists, or lists of structs.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-11-30 05:32:16,12
13343194,[CI] Integration tests are failing in master,"Caused by this PR being merged: [https://github.com/apache/arrow/pull/8715]

We could revert this PR to unblock people.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-11-29 22:36:00,12
13343163,[Rust] Compute nested definition and repetition for list arrays,This extends on ARROW-9728 by only focusing on list array repetition and definition levels,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-11-29 11:30:26,12
13343114,[Rust] [DataFusion] Predicate push down does not support joins correctly,"See this ignored test in tests/sql.rs for an example (once [https://github.com/apache/arrow/pull/8785] is merged)
{code:java}
---- equijoin_implicit_syntax_with_filter stdout ----
thread 'equijoin_implicit_syntax_with_filter' panicked at 'Creating physical plan for 'SELECT t1_id, t1_name, t2_name FROM t1, t2 WHERE t1_id > 0 AND t1_id = t2_id AND t2_id < 99 ORDER BY t1_id': Sort: #t1_id ASC NULLS FIRST
  Projection: #t1_id, #t1_name, #t2_name
    Join: t1_id = t2_id
      Filter: #t1_id Gt Int64(0) And #t2_id Lt Int64(99)
        TableScan: t1 projection=Some([0, 1])
      Filter: #t1_id Gt Int64(0) And #t2_id Lt Int64(99)
        TableScan: t2 projection=Some([0, 1]): ArrowError(InvalidArgumentError(""Unable to get field named \""t2_id\"". Valid fields: [\""t1_id\"", \""t1_name\""]""))', datafusion/tests/sql.rs:1262:48
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
 {code}",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2020-11-28 16:05:36,9
13343097,[Rust] [CI] Sporadic failures due to disk filling up,"CI is failing due to disk size filling up, affecting almost all Rust PRs",pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Bug,Blocker,2020-11-28 12:55:14,12
13343039,[C++] Add RE2 to minimal build example,"We require RE2 with ARROW_COMPUTE by default.

See also: ARROW-10541",pull-request-available,['C++'],ARROW,Bug,Minor,2020-11-27 20:02:51,1
13342972,[C++] Incorrect string format for Datum with the collection type,"The current format looks like this: {{Collection(...}}, the right embrace is omitted. ",pull-request-available,['C++'],ARROW,Bug,Trivial,2020-11-27 10:31:08,7
13342900,[Java] TimeStampMilliVector cannot be cast to TimeStampMilliTZVector,"I tried to leverage `org.apache.arrow.adapter.jdbc.JdbcToArrow.sqlToArrow` to query a Hive table but got the following error message on Timestamp columns. Notice that Date columns works well.

{{java.lang.ClassCastException: java.lang.ClassCastException: org.apache.arrow.vector.TimeStampMilliVector cannot be cast to org.apache.arrow.vector.TimeStampMilliTZVector}}",pull-request-available,['Java'],ARROW,Bug,Major,2020-11-27 05:03:41,7
13342686,[R] Remove arrow-without-arrow wrapping,"Depends on ARROW-10734. If we can successfully build Arrow with the C++ library on all of CRAN's platforms, we can remove the conditional build wrapping and codegen. This will simplify the R package's C++ code, remove some bloat, and make for a simpler user experience.

We do still though need the conditional build of S3 support.",pull-request-available,['R'],ARROW,New Feature,Major,2020-11-25 18:58:44,4
13342675,[R] Improvements to Linux installation troubleshooting,"* Some people think that they need to install the arrow package and then call install_arrow... which just installs the package again. A number of packages with external dependencies (python, etc.) have this pattern, so it's understandable why people are confused.
* If installation fails, you get the arrow-without-arrow package, which tells you to run install_arrow. This was reasonable advice when we first got on CRAN and the CRAN version did not build the C++ library but install_arrow would. But now that installation from CRAN will try to build the C++ library, if installation failed the first time, it will probably fail the second time too. Some possible improvements: (1) don't recommend install_arrow in that error message; (2) set ARROW_R_DEV=true in install_arrow for verbosity, detect if installation failed, and direct the user to report the full logs if it did; (3) if the user is on centos-7, print a special message pointing to the installation troubleshooting.
* The linux installation vignette's troubleshooting section should prominently recommend retrying with ARROW_R_DEV=true and reporting the logs in your bug report. It's usually the first thing I ask in response to a bug report.",pull-request-available,['R'],ARROW,New Feature,Major,2020-11-25 18:30:02,4
13342673,[Rust] [DataFusion] Add SQL support for table/relation aliases and compound identifiers,"We need to support referencing columns in queries using table name and/or alias prefixes so that we can support use cases such as joins between tables that have duplicate column names.

For example:
{code:java}
SELECT t1.id, t1.name, t2.name FROM t1 JOIN t2 ON t1.id = t2.id {code}",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-11-25 18:26:29,10
13342662,[Rust] [DataFusion] Add SQL support for JOIN using implicit syntax,"Add SQL support for JOIN using implicit syntax where there are multiple tables in the FROM clause and the join conditions are part of the WHERE clause.

Example:
{code:java}
SELECT * 
FROM t1, t2, t3 
WHERE t1.foo = t2.foo AND t2.bar = t3.bar{code}",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-11-25 17:09:59,10
13342559,[Python][Compute] Exposing bindings for sort options,"We need to expose the missing option classes in Python:
{code}
pyarrow/compute.py:136
  /home/antoine/arrow/dev/python/pyarrow/compute.py:136: RuntimeWarning: Python binding for ArraySortOptions not exposed
    .format(class_name), RuntimeWarning)

pyarrow/compute.py:136
  /home/antoine/arrow/dev/python/pyarrow/compute.py:136: RuntimeWarning: Python binding for SortOptions not exposed
    .format(class_name), RuntimeWarning)
{code}

Those are used by the ""sort_indices"" and ""array_sort_indices"" compute functions.",pull-request-available,['Python'],ARROW,Task,Blocker,2020-11-25 08:47:25,5
13342517,[Developer Tools] Add labeler to when PRs need rebase,"This is something that we are already doing in rust manually, but that helps if it is automatic.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-11-25 04:34:53,9
13342419,[Rust] [DataFusion] Add tests to TPC-H benchmarks,"To avoid regressions, we should add tests in the benchmark crate to at least parse and plan the queries.

We could also run the benchmarks if an environment variable is defined that points to the SF=1 data set and verify that the correct results are returned.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-11-24 16:51:52,10
13342400,[CI]Remove set-env from auto-tune to work with new GHA settings,See https://github.blog/changelog/2020-10-01-github-actions-deprecating-set-env-and-add-path-commands/,pull-request-available,"['Continuous Integration', 'Developer Tools']",ARROW,Bug,Major,2020-11-24 15:30:34,8
13342358,[Rust] Example flight server is broken after tokio upgrade (among other things),"As pointed out by [~rdettai] on https://github.com/apache/arrow/pull/8697#issuecomment-732936572

Doing this:
{code}
cd rust/arrow-flight; cargo run --example server
{code}

results in the following compile error
{code}
error: The #[tokio::main] macro requires rt or rt-multi-thread.
   --> arrow-flight/examples/server.rs:121:1
    |
121 | #[tokio::main]
    | ^^^^^^^^^^^^^^
    |
    = note: this error originates in an attribute macro (in Nightly builds, run with -Z macro-backtrace for more info)

warning: unused import: `tonic::transport::Server`
  --> arrow-flight/examples/server.rs:21:5
   |
21 | use tonic::transport::Server;
   |     ^^^^^^^^^^^^^^^^^^^^^^^^
   |
   = note: `#[warn(unused_imports)]` on by default

warning: unused import: `flight_service_server::FlightServiceServer`
  --> arrow-flight/examples/server.rs:25:43
   |
25 |     flight_service_server::FlightService, flight_service_server::FlightServiceServer,
   |                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

error[E0601]: `main` function not found in crate `server`
   --> arrow-flight/examples/server.rs:18:1
    |
18  | / use std::pin::Pin;
19  | |
20  | | use futures::Stream;
21  | | use tonic::transport::Server;
...   |
130 | |     Ok(())
131 | | }
    | |_^ consider adding a `main` function to `arrow-flight/examples/server.rs`

error: aborting due to 2 previous errors; 2 warnings emitted

{code}",pull-request-available,['Rust'],ARROW,Bug,Major,2020-11-24 12:32:26,11
13342134,[C++] BitmapUInt64Reader doesn't work on big-endian,"I didn't notice this when merging ARROW-10655 (the s390x CI is allowed to fail).
https://travis-ci.com/github/apache/arrow/jobs/445803711#L3534

",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2020-11-23 13:30:58,2
13342128,[C++] Consolidate bitmap word readers,"We currently have {{BitmapWordReader}}, {{BitmapUInt64Reader}} and {{Bitmap::VisitWords}}.

We should try to consolidate those, assuming benchmarks don't regress.",pull-request-available,['C++'],ARROW,Task,Minor,2020-11-23 13:18:21,2
13342127,[C++] Investigate a bit run reader that would only return runs of set bits,Followup to PR discussion: https://github.com/apache/arrow/pull/8703#discussion_r526263665,pull-request-available,['C++'],ARROW,Task,Minor,2020-11-23 13:17:01,2
13342045,[Rust] Segfault while array buffer append,"{quote}// src/buffer.rs:657
  
 ///Ensuresthatthisbufferhasatleast`capacity`slotsinthisbuffer.Thiswill
 ///alsoensurethenewcapacitywillbeamultipleof64bytes.
 ///
 ///Returnsthenewcapacityforthisbuffer.
 pubfnreserve(&mutself,capacity:usize)->Result<usize>\{
 ifcapacity>self.capacity\{
      unsafe \{memory :: reallocate(self.data,self.capacity,new_capacity)};
 self.data=new_dataas*mutu8;
 self.capacity=new_capacity;
 }
 Ok(self.capacity)
 }{quote}

 Above code is not checking if new_data is null, which is causing segfault on following memcpy when reallocate failed.",pull-request-available,['Rust'],ARROW,Bug,Blocker,2020-11-23 04:52:18,9
13342009,[Rust] [DataFusion] Support CASE WHEN from SQL,Support CASE WHEN from SQL,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-11-22 23:54:27,10
13342008,[Rust] [DataFusion] Support CASE WHEN from DataFrame API,Support CASE WHEN from DataFrame API,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-11-22 23:54:11,10
13341974,[Rust] Logical equality should consider parent array nullability,"When creating a struct array with a primitive child array, it is possible for the child to be non-nullable, while its parent struct array is nullable.

In this scenario, the child array's slots where the parent is null, become invalidated, such that an array with [1, 2, 3] having slot 2 being null, should be interpreted as [1, 0, 3].

This issue becomes evident in Parquet roundtrip tests, as we end up not able to correctly compare nested structures that have non-null children.

The specification caters for the above behaviour, see [http://arrow.apache.org/docs/format/Columnar.html#struct-layout].

When a struct has nulls, its child array(s) nullability is subject to the parent struct.",pull-request-available,['Rust'],ARROW,Bug,Major,2020-11-22 10:15:50,12
13341935,[Rust] [DataFusion] Implement TPC-H Query 12,Implement TPC-H Query 12 so that we can test JOIN support more fully. We will need to fake some parts for now because we don't support all the expressions in this query.,pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-11-21 19:34:48,10
13341934,[Rust] [DataFusion] Implement SQL CASE WHEN physical expression,"Implement SQL CASE WHEN expression so that we can support TPC-H query 12 fully.



Postgres: [https://www.postgresqltutorial.com/postgresql-case/]

Spark: [http://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-case.html]

",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-11-21 19:28:05,10
13341918,[Rust] Fix Bug and Add tests as documentation showing supported csv parsing,"ARROW-10654 adds some specialized parsing for the csv reader but there was no unit test coverage and it introduced a bug

https://github.com/apache/arrow/pull/8714/files# / ARROW-10654 added some specialized parsing for the csv reader and among other things added additional boolean parsing support. 

We should add some tests as documentation of what boolean types are supported",pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-11-21 14:05:41,11
13341864,[C++][Compute] Support Scalar inputs to boolean kernels,Currently only Invert supports scalar arguments,pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-20 22:02:43,6
13341765,[C++/Doc] The IsIn kernel ignores the skip_nulls option of SetLookupOptions,"The C++ docs of {{SetLookupOptions}} has this explanation of the {{skip_nulls}} option:

{code}
  /// Whether nulls in `value_set` count for lookup.
  ///
  /// If true, any null in `value_set` is ignored and nulls in the input
  /// produce null (IndexIn) or false (IsIn) values in the output.
  /// If false, any null in `value_set` is successfully matched in
  /// the input.
  bool skip_nulls;
{code}

(from https://github.com/apache/arrow/blob/8b9f6b9d28b4524724e60fac589fb1a3552a32b4/cpp/src/arrow/compute/api_scalar.h#L78-L84)

However, for {{IsIn}} this explanation doesn't seem to hold in practice:

{code}
In [16]: arr = pa.array([1, 2, None])

In [17]: pc.is_in(arr, value_set=pa.array([1, None]), skip_null=True)
Out[17]: 
<pyarrow.lib.BooleanArray object at 0x7fcf666f9408>
[
  true,
  false,
  true
]

In [18]: pc.is_in(arr, value_set=pa.array([1, None]), skip_null=False)
Out[18]: 
<pyarrow.lib.BooleanArray object at 0x7fcf666b13a8>
[
  true,
  false,
  true
]
{code}

This documentation was added in https://github.com/apache/arrow/pull/7695 (ARROW-8989)/
.

BTW, for ""index_in"", it works as documented:

{code}
In [19]: pc.index_in(arr, value_set=pa.array([1, None]), skip_null=True)
Out[19]: 
<pyarrow.lib.Int32Array object at 0x7fcf666f04c8>
[
  0,
  null,
  null
]

In [20]: pc.index_in(arr, value_set=pa.array([1, None]), skip_null=False)
Out[20]: 
<pyarrow.lib.Int32Array object at 0x7fcf666f0ee8>
[
  0,
  null,
  1
]
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-11-20 10:44:21,2
13341698,[Java] Avoid integer overflow for Json file reader,"For the current implementation, it uses {{int}} to represent the buffer size. However, the buffer can be larger than Integer.MAX_VALUE, which will lead to integer overflow and unexpected behaviors. ",pull-request-available,['Java'],ARROW,Bug,Major,2020-11-20 02:16:28,7
13341641,[Python][Packaging] Wheel builds for Apple Silicon,We are only able to create Intel builds at the moment,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-11-19 18:01:33,3
13341622,[Rust] New RecordBatch requires exact match of Data Types,"When instanciating a new RecordBatch with {{try_new()}}, the data types of columns are checked to match their corresponding fields in the schema with {{==}}.The {{==}} operator will consider all attribues of the two data types and compare strictly if all values are equal.However, a code comment above this comparison indicates _[1]_:

??list types can have different names, but we only need the data types to be the same??

SinceARROW-10261([PR|https://github.com/apache/arrow/pull/8608]) was merged lists contain a {{Field}} instead of just a {{DataType}}. Therefore, the values of this field are striclty compared. This behavior contradicts the comment.

*Solution*



The data type comparison in {{try_new()}} should be changed into only matching data types, including the nested ones, but leaving out other values.



_[1] src/record_batch.rs:103_",pull-request-available,['Rust'],ARROW,Bug,Major,2020-11-19 16:31:03,12
13341608,[C++] Add LRU cache facility,We will need a simple LRU cache to memoize results of costly functions.,pull-request-available,['C++'],ARROW,Improvement,Minor,2020-11-19 15:34:08,2
13341391,[Rust] [Parquet] Port parquet benchmarks to new repo,"https://issues.apache.org/jira/browse/ARROW-10645

The idea is to port the original benchmarks from https://github.com/sunchao/parquet-rs/tree/master/benches in  ervice of helping to get https://github.com/apache/arrow/pull/8698 merged.

",pull-request-available,[],ARROW,Improvement,Major,2020-11-18 16:46:35,11
13341388,[C++][FlightRPC] Disable flaky test,"One of the Flight tests is flaky on Windows, as it appears gRPC doesn't always return us the address of the connected client. We can just disable this part of the test since it's really testing gRPC.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2020-11-18 16:30:31,0
13341371,[Python] Consolidate path/filesystem handling in pyarrow.dataset and pyarrow.fs,"The {{pyarrow.dataset}} module grew some custom code to deal with paths and filesystems, but also the{{pyarrow.fs}} package has some general utilities for this. ",pull-request-available,['Python'],ARROW,Improvement,Major,2020-11-18 15:22:15,5
13341336,[R] Can't get Table from RecordBatchReader with 0 batches,"Objective is to build a 0 rows data.frame using an arrow schema field definition






{code:java}
#IPC stream containing only a schema
stream<-as.raw(c(255,255,255,255,16,1,0,0,16,0,0,0,0,0,10,0,12,0,6,0,5,0,8,0,10,0,0,0,0,1,3,0,12,0,0,0,8,0,8,0,0,0,4,0,8,0,0,0,4,0,0,0,4,0,0,0,160,0,0,0,92,0,0,0,48,0,0,0,4,0,0,0,128,255,255,255,0,0,1,5,20,0,0,0,12,0,0,0,4,0,0,0,0,0,0,0,176,255,255,255,7,0,0,0,82,69,80,79,78,83,69,0,168,255,255,255,0,0,1,5,20,0,0,0,12,0,0,0,4,0,0,0,0,0,0,0,216,255,255,255,6,0,0,0,68,69,84,65,73,76,0,0,208,255,255,255,0,0,1,5,24,0,0,0,16,0,0,0,4,0,0,0,0,0,0,0,4,0,4,0,4,0,0,0,8,0,0,0,68,65,84,65,84,89,80,69,0,0,0,0,16,0,20,0,8,0,6,0,7,0,12,0,0,0,16,0,16,0,0,0,0,0,1,7,36,0,0,0,20,0,0,0,4,0,0,0,0,0,0,0,8,0,12,0,4,0,8,0,8,0,0,0,38,0,0,0,9,0,0,0,8,0,0,0,77,65,67,84,65,95,73,68,0,0,0,0,0,0,0,0))
readr <- RecordBatchStreamReader$create(stream)
readr$read_table()
# Error in Table__from_RecordBatchStreamReader(self) : 
# Invalid: Must pass at least one record batch or an explicit Schema
# Now trying to be too clever
tb <-Table$create(data.frame(), schema = readr$schema)
dtf <- as.data.frame(tb)
# This will crash you R session
{code}




Tested on nightly, same behavior. It's borderline a bug / feature request, but to be a drop in replacement for some DBI methods, it needs to be able to build 0 rows data.frame with the correct class for each column.



Thank you and have a nice day.",pull-request-available,['R'],ARROW,Bug,Minor,2020-11-18 13:19:16,4
13341032,[CI] MinGW builds broken on Github Actions,"See https://github.com/apache/arrow/pull/8542/checks?check_run_id=1411805276

{code}
+ echo '::set-env name=ARROW_USE_CCACHE::ON'
Error: Unable to process command '::set-env name=ARROW_USE_CCACHE::ON' successfully.
Error: The `set-env` command is disabled. Please upgrade to using Environment Files or opt into unsecure command execution by setting the `ACTIONS_ALLOW_UNSECURE_COMMANDS` environment variable to `true`. For more information see: https://github.blog/changelog/2020-10-01-github-actions-deprecating-set-env-and-add-path-commands/
{code}
",pull-request-available,"['C', 'Continuous Integration']",ARROW,Bug,Major,2020-11-17 11:30:40,2
13340941,[Rust] Github master does not compile for WASM target,"The current version of Apache Arrow on Github's master branch (as of [https://github.com/apache/arrow/commit/e5fce7f6a70c370c758b133f025444c98cdd305d]) does not compile for wasm32-unknown-unknown, whereas the latest disted version (2.0.0) compiles successfully.

I've tested this on a new cargo project by adding:
{code:java}
[dependencies]
 arrow = {git = ""https://github.com/apache/arrow""}{code}
and I see the following error:

!Screen Shot 2020-11-16 at 7.13.47 PM.png!",pull-request-available,['Rust'],ARROW,Bug,Minor,2020-11-17 00:20:20,12
13340912,[R] Nameof<>() is incorrect in r-arrow build environment,"{{r-arrow}} builds in an environment which combines MSVC and clang, invalidating assumptions made in {{nameof<>()}} which is used to look up R6 class names. Specifically:
- MSVC prefixes ""struct "" or ""class "" to some type names, but this doesn't happen in clang+msvc
- clang+msvc renders the signature of a function taking no arguments as ""(void)"" instead of ""()""",pull-request-available,['R'],ARROW,Bug,Major,2020-11-16 20:32:59,6
13340844,[C++] Fix crash on unsupported IPC stream (OSS-Fuzz),"A number of regressions in input validation happened following ARROW-10566, detected by OSS-Fuzz.",pull-request-available,['C++'],ARROW,Bug,Major,2020-11-16 14:45:57,2
13340825,[Developer] Expand PR labeler to all supported languages,This would help me to browse through past PRs.,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-11-16 12:07:38,8
13340784,[C++] arrow-utility-test and arrow-csv-test causes failures on a big-endian platform,"After [https://github.com/apache/arrow/pull/8494]has been merged, the following tests, which use {{1eN}} format, cause failures.

{code}
[ RUN      ] FloatingPointConversion.Basics
/arrow/cpp/src/arrow/testing/gtest_util.cc:128: Failure
Failed
@@ -1, +1 @@
--1e+30
+0
[  FAILED  ] FloatingPointConversion.Basics (3 ms)

...

[ RUN      ] StringConversion.ToFloat
/arrow/cpp/src/arrow/util/value_parsing_test.cc:35: Failure
Expected equality of these values:
  out
    Which is: 0
  expected
    Which is: -1e+20
Conversion failed for '-1e20'
[  FAILED  ] StringConversion.ToFloat (0 ms)
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2020-11-16 08:55:29,2
13340747,[Ruby] Support Decimal256 type,The C++ implementation now support it. We need to ensure Ruby/Gobject bindings do as well.,pull-request-available,['Ruby'],ARROW,Improvement,Major,2020-11-16 04:14:14,1
13340715,[C++] Improve performance of GenerateBitsUnrolled ,"internal::GenerateBitsUnrolled doesn't vectorize too well, there are some improvements we can make to get better code generation",pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-15 16:03:58,14
13340702,[Rust] Improve take benchmark,"The take benchmark has three issues:

1. it is always taking the same element (indices is a constant vector), which is very easy to speculatively predict what the element is going to be.
2. It also does not compare with vs without nulls
3. all elements of the array are equal, which is again easy to speculatively predict",pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-11-15 12:12:29,9
13340686,[Rust] Remove Date32(Millisecond) from test,This is not supported by the arrow specification and `make_array` does not support it.,pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-11-15 08:32:22,9
13340647,[Rust] [DataFusion] Add join support to DataFrame and LogicalPlan,"Add join support to DataFrame and LogicalPlan.
h2. Logical Plan

My initial thoughts on the design of the LogicalPlan struct would be:
{code:java}
struct InnerJoin {
  left: Box<LogicalPlan>,
  right: Box<LogicalPlan>,
  left_keys: Vec<Expr>,
  right_keys: Vec<Expr>
} {code}
The left_keys and right_keys vectors must have the same length. Example pseudo-code:
{code:java}
let join = InnerJoin {
 left: read_parquet(""customers""),
  right: read_parquer(""orders""),
  left_keys: vec![col(""id"")],
  right_keys: vec![col(""customer_id"")]
};   {code}
h2. DataFrame
{code:java}
let customer = ctx.read_parquet(""customers"").alias(""c"");
let orders = ctx.read_parquet(""orders"").alias(""o"");

// generic join method that can support all types of join
let join = customer.join(orders, col(""c.id"").eq(""o.customer_id""))

// or we could start with a more specific equijoin method
let join = customer.inner_join(orders, vec![col(""id"")], vec![col(""customer_id"")]);{code}






",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-11-14 16:14:15,10
13340646,[Rust] [DataFusion] Implement SQL join support using explicit JOIN ON syntax,Update the SQL to DataFrame / LogicalPlan logic to support inner equijoin. Suitable error should be returned for any unsupported join.,pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-11-14 16:13:04,10
13340643,"[Rust] [DataFusion] Implement ""repartition"" operator","The repartition operator should read batches from its input partitions and then map that data to its output partitions using a specific partitioning scheme.

The simplest and most efficient partition schema would be a ""round robin batch partitioner"". For each input batch, it would pick the next output partition to write to. This is a convenient way to change the number of partitions up or down with minimal overhead.

Another example of a partitioning scheme would be a hash partitioner, which computes the hash of the partition keys on each incoming row and then applies a modulus to determine which output partition to write to.





",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-11-14 15:33:01,10
13340530,"[C++] When Validating, ensure DenseUnionArray offsets are increasing","https://github.com/apache/arrow/pull/8652#discussion_r522332386

The format documentation states that ""The respective offsets for each child value array must be in order / increasing.""
https://arrow.apache.org/docs/format/Columnar.html#dense-union

However this is not currently checked in {{Array::ValidateFull}}
",pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-13 14:29:02,2
13340288,[C++][Python] Poor Table filtering performance,"From the mailing list


{code:java}
import pandas as pd
import pyarrow as pa
import pyarrow.compute as pc
import numpy as np

num_rows = 10_000_000
data = np.random.randn(num_rows)

df = pd.DataFrame({'data{}'.format(i): data
         for i in range(100)})

df['key'] = np.random.randint(0, 100, size=num_rows)

rb = pa.record_batch(df)
t = pa.table(df)

I found that the performance of filtering a record batch is very similar:

In [22]: timeit df[df.key == 5]
71.3 ms  148 s per loop (mean  std. dev. of 7 runs, 10 loops each)

In [24]: %timeit rb.filter(pc.equal(rb[-1], 5))
75.8 ms  2.47 ms per loop (mean  std. dev. of 7 runs, 10 loops each)

Whereas the performance of filtering a table is absolutely abysmal (no
idea what's going on here)

In [23]: %timeit t.filter(pc.equal(t[-1], 5))
961 ms  3.5 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
 {code}


[https://lists.apache.org/thread.html/r4d4ffa7935efb2902600b9024859211e53aa6552d43ba0ad83517af5%40%3Cuser.arrow.apache.org%3Ehttps://lists.apache.org/thread.html/r4d4ffa7935efb2902600b9024859211e53aa6552d43ba0ad83517af5%40%3Cuser.arrow.apache.org%3E]",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-11-12 15:58:34,2
13340273,[C++] Array validation should work on ArrayData,"Going from ArrayData to Array involves a MakeArray() call, which may trigger debug assertions. It should be possible to call validation directly on ArrayData.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-11-12 14:44:13,2
13340200,[Packaging][C++] CMake find_package(Arrow 2.0 CONFIG REQUIRED) broken,"Adding Arrow targets to a CMake project using the installed config files no longer works with version 2.0.0.


{code:java}
find_package(Arrow 2.0 CONFIG REQUIRED)
{code}
I get the following error:


{code:java}
CMake Error at /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:146 (message):
  Could NOT find Snappy (missing: Snappy_LIB Snappy_INCLUDE_DIR)
Call Stack (most recent call first):
  /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:393 (_FPHSA_FAILURE_MESSAGE)
  /usr/lib/x86_64-linux-gnu/cmake/arrow/FindSnappy.cmake:50 (find_package_handle_standard_args)
  /usr/share/cmake-3.16/Modules/CMakeFindDependencyMacro.cmake:47 (find_package)
  /usr/lib/x86_64-linux-gnu/cmake/arrow/ArrowConfig.cmake:95 (find_dependency)
  CMakeLists.txt:5 (find_package)
{code}
The snappy lib is installed:
{code:java}
find /usr -name ""libsnappy*""
/usr/share/doc/libsnappy1v5
/usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.8
/usr/lib/x86_64-linux-gnu/libsnappy.so.1
{code}
It's possible to reproduce the error using [https://github.com/mbrobbel/arrow-cmake]. Build the image to get the error.
{code:java}
git clone https://github.com/mbrobbel/arrow-cmake
cd arrow-cmake
docker build -t arrow-cmake .
{code}
This used to work with 1.x.",pull-request-available,"['C++', 'Packaging']",ARROW,Bug,Major,2020-11-12 11:44:01,1
13340035,[Python] Crash when creating array with string over 2GB,"{code:python}
>>> import pyarrow as pa
>>> data = [b""x"" * (1<<32)]
>>> arr = pa.array(data)
Erreur de segmentation
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-11 18:52:59,2
13340004,[Rust] [DataFusion] Break up logical_plan/mod.rs into smaller modules,"The module has gotten fairly large and so refactoring it into smaller chunks will improve readability  -- as suggested by Jorge https://github.com/apache/arrow/pull/8619#pullrequestreview-527391221
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-11-11 14:47:08,11
13339990,[Python] Filesystem S3 tests not independent (native s3 influences s3fs),"The filesystem tests in {{test_fs.py}} that are parametrized with all the tested filesystems have some ""state"" shared between them, at least in the case of S3. 

When first a test is run with our own S3FileSystem, which eg creates a directory, this directory is still present when we test the s3fs wrapped filesystem, which causes some tests to pass that would otherwise fail if run in isolation.",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-11 13:50:20,5
13339790,[Rust][DataFusion] Filter pushdown loses filters if below a user defined node,"I have a LogicalPlan like this (where ""Extension"" is some extension node type):

{code}
    Extension [non_null_column:Utf8]
      Filter: #state Eq Utf8(""MA"") [city:Utf8;N, state:Utf8;N, borough:Utf8;N]
        InMemoryScan: projection=None [city:Utf8;N, state:Utf8;N, borough:Utf8;N]
{code}

When I run this plan through {{ExecutionContext::optimize}} the plan that results is as follows (note the filter has been lost):

{code}
    Extension [non_null_column:Utf8]
      InMemoryScan: projection=Some([0, 1, 2]) [city:Utf8;N, state:Utf8;N, borough:Utf8;N]
{code}

I have debugged the problem and the root cause of the issue is that the FilterPushdown logic is not recursing into the inputs of user defined nodes. I will get a PR up showing and fixing the problem.

More generally, I am not sure how / if the filter pushdown logic would work if there are multiple child inputs as no existing LogicalPlan variant his multiple inputs at this time. We will have to handle this when / if we want to support joins (typically filters get  pushed down each join input if possible)
",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2020-11-10 16:22:37,11
13339787,[Python] Deprecate the S3FSWrapper class,"Follow-up on ARROW-10433 / discussion at https://github.com/apache/arrow/pull/8557#issuecomment-724225124

The {{S3FSWrapper}} class has been used in the past to wrap s3fs filesystems, before fsspec subclassed {{pyarrow.filesystem}} filesystems. This is however already more than 2 years ago, and AFAIK nobody should still be using {{S3FSWrapper}}.",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-10 16:00:35,5
13339748,[Developer] Update dev instructions to note there may be a timelag ,"When following the developer instructions, I was hung up for a while on the fact that there seemed to be a timelag between when I completed the gitbox setup and when permissions were actually granted to the repo. Update the docs to try and help the next person.",pull-request-available,['Developer Tools'],ARROW,Improvement,Minor,2020-11-10 13:15:31,11
13339691,[C++] Add re2 library to core arrow / ARROW_WITH_RE2,"For https://issues.apache.org/jira/browse/ARROW-10195we need the re2 linked into the core arrow library, as discussed:
[https://github.com/apache/arrow/pull/8459#pullrequestreview-508337720]

This might be good to put under anARROW_WITH_RE2 CMake option, maybe default on whenARROW_COMPUTE=ON?",pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-10 09:11:13,8
13339668,[Rust] Allow unary kernels of arbitrary array types,"Currently,{{filter}} support is limited to a set of types, and these types cannot be arbitrarily nested. This hints at a fundamental limitation of the design, that is currently typed (in rust's sense).

However, this operation is fundamentally un-typed: we do not require knowing type information to filter an array.

This issue tracks the development of an abstraction to support these operations for arbitrary types.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-11-10 07:16:16,9
13339654,[Packaging][Python] Use GitHub Actions to build wheels for Windows,"Because we are banned by AppVeyor.

https://lists.apache.org/thread.html/rb56a964fc59bded278cf3cbc753a879c70b2078e9c0758057c982a2f%40%3Cdev.arrow.apache.org%3E",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-11-10 04:34:46,1
13339615,[Python] Mangled pandas_metadata when specified schema has different order as DataFrame columns,"When calling pyarrow.Table.from_pandas() with an explicit schema, the ordering of the columns in the dataframe and the schema have to be identical, because the pandas_metadata fields are associated with columns on the basis of the ordering, rather than the name of their column. If the ordering of the dataframe columns and schema fields isn't identical, then you end up associating metadata with the wrong fields, which leads to all kinds of errors.


{code:java}
import pyarrow as pa
import pandas as pd
import numpy as np

data_col = np.random.random_sample(2)
datetime_col = pd.date_range(""2020-01-01T00:00:00Z"", freq=""H"", periods=2)

data_field = pa.field(""data_col"", pa.float32(), nullable=True)
datetime_field = pa.field(""datetime_utc"", pa.timestamp(""s"", tz=""UTC""), nullable=False)

df = pd.DataFrame({""datetime_utc"": datetime_col, ""data_col"": data_col})

good_schema = pa.schema([datetime_field, data_field])
bad_schema = pa.schema([data_field, datetime_field])

pa.Table.from_pandas(df, preserve_index=False, schema=good_schema).schema.pandas_metadata
#{'index_columns': [],
# 'column_indexes': [],
# 'columns': [{'name': 'datetime_utc',
#   'field_name': 'datetime_utc',
#   'pandas_type': 'datetimetz',
#   'numpy_type': 'datetime64[ns]',
#   'metadata': {'timezone': 'UTC'}},
#  {'name': 'data_col',
#   'field_name': 'data_col',
#   'pandas_type': 'float32',
#   'numpy_type': 'float64',
#   'metadata': None}],
# 'creator': {'library': 'pyarrow', 'version': '2.0.0'},
# 'pandas_version': '1.1.4'}

pa.Table.from_pandas(df, preserve_index=False, schema=bad_schema).schema.pandas_metadata
#{'index_columns': [],
# 'column_indexes': [],
# 'columns': [{'name': 'data_col',
#   'field_name': 'data_col',
#   'pandas_type': 'float32',
#   'numpy_type': 'datetime64[ns]',
#   'metadata': {'timezone': 'UTC'}},
#  {'name': 'datetime_utc',
#   'field_name': 'datetime_utc',
#   'pandas_type': 'datetimetz',
#   'numpy_type': 'float64',
#   'metadata': None}],
# 'creator': {'library': 'pyarrow', 'version': '2.0.0'},
# 'pandas_version': '1.1.4'}
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-09 22:51:59,5
13339614,[Rust] [DataFusion] Better display for logical plans: Graphviz and Schema information,"  I have been tracking down issues in our use of DataFusion for my work project, and I have found myself wanting to print out the state of the logical_plan several times. The existing debug formatting is ok, but it is missing a few key items:

# schema information (as in when did columns appear / disappear in the plan)
# A visual representation (graphviz)
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-11-09 22:50:32,11
13339598,[R] Optionally use distro package in linuxlibs.R,"{{distro}} is now on CRAN. The script should prefer the package version if present, which may be updated/patched apart from {{arrow}}.",pull-request-available,['R'],ARROW,New Feature,Minor,2020-11-09 20:33:49,4
13339569,[C++][Dataset] Add FlightFragment,Allow wrapping a flight service as a dataset/fragment,dataset,['C++'],ARROW,Improvement,Major,2020-11-09 17:39:27,6
13339535,[C++][R] Implement add/remove/replace for RecordBatch,"With https://github.com/apache/arrow/pull/8579 the R client can remove, add and replace columns in an arrow table. But it can't currently do the same for RecordBatch. From [~npr]: As it turns out, that will require a little more C++ work because there is AddColumn and RemoveColumn but no SetColumn in record_batch.h.",pull-request-available,['R'],ARROW,Improvement,Major,2020-11-09 14:23:11,4
13339231,[Python] Table.to_pandas() failing when timezone-awareness mismatch in metadata,"We're having an issue with timezones in the Table {{to_pandas}} methods. See example below.

{code:python}
import pyarrow as pa
import pandas as pd

print(pa.__version__)
# 2.0.0

df = pd.DataFrame({""time"": pd.to_datetime([0, 0])})

time_field = pa.field(""time"",type=pa.timestamp(""ms"", tz=""utc""), nullable=False)
schema = pa.schema([time_field])

tab = pa.Table.from_pandas(df, schema)

tab.to_pandas() 

# File "".../pandas_compat.py"", line 777, in table_to_blockmanager
#   table = _add_any_metadata(table, pandas_metadata)
# File "".../pandas_compat.py"", line 1184, in _add_any_metadata
#   tz = col_meta['metadata']['timezone']
# TypeError: 'NoneType' object is not subscriptable

{code}


Related issues:
https://github.com/catalyst-cooperative/pudl/issues/705",pull-request-available,['Python'],ARROW,Bug,Minor,2020-11-06 19:13:23,5
13339073,[C++] Suppress UBSAN pointer-overflow warning in RapidJSON,"RapidJSON triggers ubsan's {{pointer-overflow}} checker https://github.com/Tencent/rapidjson/issues/1724 , which is problematic since many of our tests depend on ArrayFromJSON to generate test data. This can be resolved by adding an entry to {{sanitizer-disallowed-entries.txt}}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-05 18:06:34,6
13338898,[R][CI] Fix conda-r job,Broken since cpp11 was added because conda dependencies aren't pulled from r/DESCRIPTION.,pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Minor,2020-11-04 21:46:46,4
13338833,[C++] find_package(Arrow) is broken on Ubuntu 18,"On Ubuntu 18, after updating Arrow version to 2.0.0, ROOT ([https://github.com/root-project/root)] builds are broken:
{code:java}
CMake Warning at cmake/modules/SearchInstalledSoftware.cmake:16 (_find_package):
 By not providing ""FindRE2.cmake"" in CMAKE_MODULE_PATH this project has
 asked CMake to find a package configuration file provided by ""RE2"", but
 CMake did not find one.
Could not find a package configuration file provided by ""RE2"" with any of
 the following names:
RE2Config.cmake
 re2-config.cmake
Add the installation prefix of ""RE2"" to CMAKE_PREFIX_PATH or set ""RE2_DIR""
 to a directory containing one of the above files. If ""RE2"" provides a
 separate development package or SDK, be sure it has been installed.
 Call Stack (most recent call first):
 /usr/share/cmake-3.10/Modules/CMakeFindDependencyMacro.cmake:48 (find_package)
 /usr/lib/x86_64-linux-gnu/cmake/arrow/ArrowConfig.cmake:95 (find_dependency)
 cmake/modules/SearchInstalledSoftware.cmake:16 (_find_package)
{code}

 Looks like a problem is that for a Debian/Ubuntu packaging is missing_FindRE2.cmake_ in a list [https://github.com/apache/arrow/blob/master/dev/tasks/linux-packages/apache-arrow/debian/libarrow-dev.install]

CMake files I got after installation:
{code:java}
~$ ls -la /usr/lib/x86_64-linux-gnu/cmake/arrow
t
-rw-r--r-- 1 root root 1175 Okt 13 01:47 arrow-config.cmake
-rw-r--r-- 1 root root 4677 Okt 13 01:47 ArrowConfig.cmake
-rw-r--r-- 1 root root 1269 Okt 13 01:47 ArrowConfigVersion.cmake
-rw-r--r-- 1 root root 7345 Okt 13 01:47 ArrowOptions.cmake
-rw-r--r-- 1 root root 4298 Okt 13 01:47 ArrowTargets.cmake
-rw-r--r-- 1 root root 1342 Okt 13 01:47 ArrowTargets-release.cmake
-rw-r--r-- 1 root root 16770 Okt 13 01:47 FindArrow.cmake
-rw-r--r-- 1 root root 5755 Okt 13 01:47 FindBrotli.cmake
-rw-r--r-- 1 root root 3043 Okt 13 01:47 FindLz4.cmake
-rw-r--r-- 1 root root 5258 Okt 13 01:47 FindParquet.cmake
-rw-r--r-- 1 root root 2384 Okt 13 01:47 FindSnappy.cmake
-rw-r--r-- 1 root root 2703 Okt 13 01:47 Findutf8proc.cmake
-rw-r--r-- 1 root root 3243 Okt 13 01:47 Findzstd.cmake
-rw-r--r-- 1 root root 2988 Okt 13 01:47 ParquetConfig.cmake
-rw-r--r-- 1 root root 1269 Okt 13 01:47 ParquetConfigVersion.cmake
-rw-r--r-- 1 root root 4094 Okt 13 01:47 ParquetTargets.cmake
-rw-r--r-- 1 root root 1370 Okt 13 01:47 ParquetTargets-release.cmake{code}


Manually adding _FindRE2.cmake_ in_/usr/lib/x86_64-linux-gnu/cmake/arrow_ is fixing CMake configuration issue.",pull-request-available,['Packaging'],ARROW,Bug,Major,2020-11-04 13:39:43,1
13338778,[Java][JDBC] Allow users to config the mapping between SQL types and Arrow types,"According to the current implementation of JDBC adapter, the conversion between SQL types and Arrow types is hard-coded. This will cause some problems in practice:
 # The appropriate conversion may vary for different databases. For example, for SQL Server, type {{real}}corresponds to 4 byte floating point values ([https://docs.microsoft.com/en-us/sql/t-sql/data-types/float-and-real-transact-sql?view=sql-server-ver15),]whereas for SQLite, \{{real}} corresponds to 8 byte floating point values ([https://www.sqlitetutorial.net/sqlite-data-types/).]If the maping is not right, some extra conversion would happen, which can impact performance severely.
 # Our current implementation determines the type conversion solely based on the type ID. However, the appropriate conversion may also depend some other information, like precision and scale. For example, for {{FLOAT( n )}}, it should correspond to 4 byte floating point values, if n <= 24, otherwise, it should correspond to 8 byte floating point values.

To address the problems, we should allow users to customize the conversion between SQL and Arrow types.",pull-request-available,['Java'],ARROW,Improvement,Major,2020-11-04 06:38:46,7
13338718,"[C++] Future<{void,Status}> could be more generic","The members of {{Future<{void,Status}>}} differ from other instantiations of {{Future<>}} since they contain only a Status and not a value. This is reasonable, however it complicates generic usage of {{Future<>}} since special cases must be added for the different interfaces. IMHO it'd be acceptable to provide an empty ""ValueType"" (or maybe {{std::nullptr_t}} to follow the precedent of Datum's default state) for those specializations to keep the interface generic.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-03 20:21:05,6
13338687,[C++] Move Executor into a separate header,{{Executor}} is a general purpose interface but it currently resides in {{thread_pool.h}} so it can't be used (for example) by {{future.h}} without introducing a circular dependency. It can probably just reside in {{future.h}},pull-request-available,['C++'],ARROW,Improvement,Trivial,2020-11-03 16:46:45,6
13338685,[Python] Specifying compression type on a column basis when writing Parquet not working,"From https://stackoverflow.com/questions/64666270/using-per-column-compression-codec-in-parquet-write-table

According to the docs, you can specify the compression type on a column-by-column basis, but that doesn't seem to be working:

{code}
In [5]: table = pa.table([[1, 2], [3, 4], [5, 6]], names=[""foo"", ""bar"", ""baz""])

In [6]: pq.write_table(table, 'test1.parquet', compression=dict(foo='zstd',bar='snappy',baz='brotli'))
...
~/scipy/repos/arrow/python/pyarrow/_parquet.cpython-37m-x86_64-linux-gnu.so in string.from_py.__pyx_convert_string_from_py_std__in_string()

TypeError: expected bytes, str found
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-03 16:30:28,5
13338646,"[Python] Parquet write_table creates gzipped Parquet file, not Parquet with gzip compression","Writing ""foo.parquet.gz"" in Arrow 2.0.0 creates a gzipped Parquet file, which Arrow can't read back, while in 1.0.1 it created a Parquet file with gzip compression. Hence I think this is a regression.

In Arrow 2.0.0:
{noformat}
> pip freeze
numpy==1.19.4
pyarrow==2.0.0
> python write.py
Arrow: 2.0.0
Read/write with PyArrow:
test.pyarrow.gz: gzip compressed data, from Unix, original size modulo 2^32 630
Traceback (most recent call last):
  File ""write.py"", line 12, in <module>
    print(pq.read_table(""test.pyarrow.gz""))
  File ""/home/lidavidm/Code/twosigma/arrow-regression/venv/lib/python3.8/site-packages/pyarrow/parquet.py"", line 1607, in read_table
    dataset = _ParquetDatasetV2(
  File ""/home/lidavidm/Code/twosigma/arrow-regression/venv/lib/python3.8/site-packages/pyarrow/parquet.py"", line 1452, in __init__
    [fragment], schema=fragment.physical_schema,
  File ""pyarrow/_dataset.pyx"", line 761, in pyarrow._dataset.Fragment.physical_schema.__get__
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
OSError: Could not open parquet input source 'test.pyarrow.gz': Invalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file. {noformat}
But in Arrow 1.0.1:
{noformat}
> pip freeze
numpy==1.19.4
pyarrow==1.0.1
> python write.py
Arrow: 1.0.1
Read/write with PyArrow:
test.pyarrow.gz: Apache Parquet
pyarrow.Table
ints: int64 {noformat}
Reproduction:
{code:python}
import pyarrow as pa
import pyarrow.parquet as pq
import subprocess

print(""Arrow:"", pa.__version__)
print()

print(""Read/write with PyArrow:"")
table = pa.table([pa.array(range(4))], names=[""ints""])
pq.write_table(table, ""test.pyarrow.gz"", compression=""GZIP"")
subprocess.check_call([""file"", ""test.pyarrow.gz""])
print(pq.read_table(""test.pyarrow.gz""))
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-03 13:48:15,0
13338552,[Rust] Add support for iterators over binary arrays,"Like we do for primitive arrays and string arrays, this issue concerns adding iterator support for binary arrays.",pull-request-available,['Rust'],ARROW,New Feature,Major,2020-11-03 05:36:24,9
13338544,[Rust] Allow string array to be built from iterator of &str,Currently we support string arrays to be built from an iterator of {{String}}. This issue aims to support building a string array from an iterator of {{&str}}.,pull-request-available,['Rust'],ARROW,New Feature,Major,2020-11-03 04:36:01,9
13338527,[++][FlightRPC] Arrow Flight Server / Client cannot be initialized with Ipv6 host,"We want to support Arrow Flight compatibility protocol in ClickHouse ([https://github.com/ClickHouse/ClickHouse]). Our code needs Ipv6 support.

In our code I wrote:
{code:java}
std::string host = ""[::]"";
int port = 9993;
arrow::flight::Location location;
auto status = arrow::flight::Location::ForGrpcTcp(host, port, &location);
std::cerr << ""Status="" << status.ToString() << std::endl;
std::cerr << ""Location="" << location.ToString() << std::endl; {code}
The output seems to be ok:
{code:java}
Status=OK
Location=grpc+tcp://[::]:9993{code}
After that I initialized FlightServerBase using method Init(options).

In flight library I wrote next 3 lines before [code|#L825]
{code:java}
std::cerr << ""Location="" << location.ToString() << std::endl;
std::cerr << ""Host="" << location.uri_->host() << "", Port="" << location.uri_->port_text() << std::endl;
std::cerr << ""Host:Port="" << location.uri_->host() << "":"" << location.uri_->port_text() << std::endl;{code}


The output is:
{code:java}
Location=grpc+tcp://[::]:9993
Host=::, Port=9993
Host:Port=:::9993
E1103 03:18:01.978794160 612780 server_chttp2.cc:40]
{""created"":""@1604362681.978626229"",""description"":""Name or service not known"",""errno"":-2,""file"":""../contrib/grpc/src/core/lib/iomgr/resolve_address_posix.cc"",""file_line"":108,""os_error"":""Name or service not known"",""syscall"":""getaddrinfo"",""target_address"":"":::9993""}{code}
Location returns host without square brackets that must be used in addresses with port. The problem is here:
{code:java}
std::stringstream address;
address << location.uri_->host() << ':' << location.uri_->port_text();{code}
The same issueis also observed in the client [code|#L845]",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-11-03 01:08:49,0
13338498,[C++][Python] casting a scalar timestamp to date32 results in Aborted (core dump),"Consider the following example: I have an array of timestamp[s]
{code:java}
>>> import pyarrow.compute as pc
>>> import pyarrow as pa
>>> arr = pc.strptime(['2020-11-01', '2020-11-02', '2020-11-03'], format='%Y-%m-%d', unit='s')
>>> arr
<pyarrow.lib.TimestampArray object at 0x7f1fc2ef4228>
[
 2020-11-01 00:00:00,
 2020-11-02 00:00:00,
 2020-11-03 00:00:00
]{code}
I am able to cast the array to date32:
{code:java}
>>> pc.cast(arr, pa.date32())
<pyarrow.lib.Date32Array object at 0x7f1fc2fd3588>
[
 2020-11-01,
 2020-11-02,
 2020-11-03
]{code}
but when I try to do the same with a scalar I get core dumped failure
{code:java}
>>> arr[0]
<pyarrow.TimestampScalar: datetime.datetime(2020, 11, 1, 0, 0)>
>>> pc.cast(arr[0], pa.date32())
terminate called after throwing an instance of 'mpark::bad_variant_access'
 what(): bad_variant_access
Aborted (core dumped)

{code}
Below is a stack trace from gdb
{code:java}
$ gdb /usr/bin/python3
GNU gdb (Ubuntu 8.1-0ubuntu3.2) 8.1.0.20180409-git
Copyright (C) 2018 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law. Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
<http://www.gnu.org/software/gdb/documentation/>.
For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from /usr/bin/python3...(no debugging symbols found)...done.
(gdb) run sample.py
Starting program: /usr/bin/python3 sample.py
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff39ff700 (LWP 4314)]
[New Thread 0x7fffebfff700 (LWP 4315)]
[New Thread 0x7fffeb7fe700 (LWP 4316)]
[New Thread 0x7fffe8ffd700 (LWP 4317)]
[New Thread 0x7fffe47fc700 (LWP 4318)]
[New Thread 0x7fffe1ffb700 (LWP 4319)]
[New Thread 0x7fffdf7fa700 (LWP 4320)]
[New Thread 0x7fffdcff9700 (LWP 4321)]
[New Thread 0x7fffda7f8700 (LWP 4322)]
[New Thread 0x7fffd7ff7700 (LWP 4323)]
[New Thread 0x7fffd57f6700 (LWP 4324)]
[New Thread 0x7fffd2ff5700 (LWP 4325)]
[Thread 0x7fffd2ff5700 (LWP 4325) exited]
[Thread 0x7fffd57f6700 (LWP 4324) exited]
[Thread 0x7fffd7ff7700 (LWP 4323) exited]
[Thread 0x7fffda7f8700 (LWP 4322) exited]
[Thread 0x7fffdcff9700 (LWP 4321) exited]
[Thread 0x7fffdf7fa700 (LWP 4320) exited]
[Thread 0x7fffe1ffb700 (LWP 4319) exited]
[Thread 0x7fffe47fc700 (LWP 4318) exited]
[Thread 0x7fffe8ffd700 (LWP 4317) exited]
[Thread 0x7fffeb7fe700 (LWP 4316) exited]
[Thread 0x7fffebfff700 (LWP 4315) exited]
terminate called after throwing an instance of 'mpark::bad_variant_access'
 what(): bad_variant_access

Thread 1 ""python3"" received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
51 ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) backtrace
#0 __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1 0x00007ffff7a248b1 in __GI_abort () at abort.c:79
#2 0x00007ffff477d957 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#3 0x00007ffff4783ae6 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#4 0x00007ffff4783b21 in std::terminate() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#5 0x00007ffff4783d54 in __cxa_throw () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#6 0x00007ffff5104012 in mpark::throw_bad_variant_access() ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#7 0x00007ffff51e1f51 in arrow::compute::internal::CastFunctor<arrow::Date32Type, arrow::TimestampType, void>::Exec(arrow::compute::KernelContext*, arrow::compute::ExecBatch const&, arrow::Datum*) ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#8 0x00007ffff52ab5ab in arrow::compute::detail::ScalarExecutor::ExecuteBatch(arrow::compute::ExecBatch const&, arrow::compute::detail::ExecListener*) () from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#9 0x00007ffff52abba1 in arrow::compute::detail::ScalarExecutor::Execute(std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::detail::ExecListener*) ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#10 0x00007ffff529cfe6 in arrow::compute::Function::Execute(std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const [clone .localalias.47] ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#11 0x00007ffff52a5878 in arrow::compute::internal::CastMetaFunction::ExecuteImpl(std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#12 0x00007ffff5299f8f in arrow::compute::MetaFunction::Execute(std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const [clone .localalias.48] ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.200
#13 0x00007fffebb685a0 in __pyx_pf_7pyarrow_8_compute_8Function_6call(__pyx_obj_7pyarrow_8_compute_Function*, _object*, __pyx_obj_7pyarrow_8_compute_FunctionOptions*, __pyx_obj_7pyarrow_3lib_MemoryPool*) [clone .isra.455] ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/_compute.cpython-36m-x86_64-linux-gnu.so
#14 0x00007fffebb69309 in __pyx_pw_7pyarrow_8_compute_8Function_7call(_object*, _object*, _object*) ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/_compute.cpython-36m-x86_64-linux-gnu.so
#15 0x0000000000566f73 in PyCFunction_Call ()
#16 0x00007fffebb5a855 in __Pyx_PyObject_Call(_object*, _object*, _object*) ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/_compute.cpython-36m-x86_64-linux-gnu.so
#17 0x00007fffebb6565a in __pyx_pw_7pyarrow_8_compute_7call_function(_object*, _object*, _object*) ()
 from /usr/local/lib/python3.6/dist-packages/pyarrow/_compute.cpython-36m-x86_64-linux-gnu.so
#18 0x000000000050a4a5 in ?? ()
#19 0x000000000050beb4 in _PyEval_EvalFrameDefault ()
#20 0x0000000000507be4 in ?? ()
#21 0x0000000000509900 in ?? ()
---Type <return> to continue, or q <return> to quit---
#22 0x000000000050a2fd in ?? ()
#23 0x000000000050beb4 in _PyEval_EvalFrameDefault ()
#24 0x0000000000507be4 in ?? ()
#25 0x000000000050ad03 in PyEval_EvalCode ()
#26 0x0000000000634e72 in ?? ()
#27 0x0000000000634f27 in PyRun_FileExFlags ()
#28 0x00000000006386df in PyRun_SimpleFileExFlags ()
#29 0x0000000000639281 in Py_Main ()
#30 0x00000000004b0dc0 in main ()
(gdb)
{code}
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2020-11-02 20:41:14,5
13338482,[C++][Compute] Refactor FunctionExecutor -> KernelExecutor,"FunctionExecutor currently handles dispatch to a kernel as well as allocation and invocation of the kernel. In some contexts (such as a filter expression bound to a specific schema) we only need to perform dispatch once, so repeatedly dispatching to kernel is redundant. Refactor to a KernelExecutor which is constructed from a pre-selected kernel.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-11-02 19:11:53,6
13338435,[Rust] Implement utility to convert TPC-H tbl files to CSV and Parquet,"It would be convenient to have a utility as part of the benchmark crate for converting generated TPC-H tbl files into CSV and Parquet files so that we can easily run benchmarks on these different file formats without relying on third party software to perform the conversions.

Ideally, the tool would also perform re-partitioning of the data so that the user can specify how many partitions to generate.",pull-request-available,['Rust'],ARROW,New Feature,Major,2020-11-02 14:32:29,10
13338433,[Python] ParquetDatasetPiece's path broken when using fsspec fs on Windows,"Dask reported some failures starting with the pyarrow 2.0 release, and specifically on Windows: https://github.com/dask/dask/issues/6754

After some investigation, it seems that this is due to the {{ParquetDatasetPiece}} its {{path}} attribute now returning a path with a mixture of &#92;&#92;  and / in it. 

It specifically happens when dask is passing a posix-style base path pointing to the dataset base directory (so using all {{/}}), and passing an fsspec-based (local) filesystem.  
From a debugging output during one of the dask tests:

{code}
(Pdb) dataset
<pyarrow.parquet.ParquetDataset object at 0x00000290D7506308>
(Pdb) dataset.paths
'C:/Users/joris/AppData/Local/Temp/pytest-of-joris/pytest-25/test_partition_on_pyarrow_0'
(Pdb) dataset.pieces[0].path
'C:/Users/joris/AppData/Local/Temp/pytest-of-joris/pytest-25/test_partition_on_pyarrow_0\\a1=A\\a2=X\\part.0.parquet'
{code}

So you can see that the result here has a mix of &#92;&#92; and {{/}}. Using pyarrow 1.0, this was consistently using {{/}}.

The reason for the change is that in pyarrow 2.0 we started to replace fsspec LocalFileSystem with our own LocalFileSystem (assuming for a local filesystem that should be equivalent). But it seems that our own LocalFileSystem has a {{pathsep}}} property that equals to {{os.path.sep}}, which is &#92;&#92; on Windows (https://github.com/apache/arrow/blob/9231976609d352b7050f5c706b86c15e8c604927/python/pyarrow/filesystem.py#L304-L306.

So note that while this started being broken in pyarrow 2.0 when using fsspec filesystem, this was already ""broken"" before when using our own local filesystem (or when not passing any filesystem). But, 1) dask always passes an fsspec filesystem, and 2) dask uses the piece's path as dictionary key and is thus especially sensitive to the change (using it as a file path to read something in, it will probably still work even with the mixture of path separators).",pull-request-available,['Python'],ARROW,Bug,Major,2020-11-02 14:31:01,5
13338240,[Rust] Fix CI cache misses on windows,"Currently the CI for windows is not correctly caching. Specifically, the key used to cache is always the same, irrespectively of the files that were changed. Consequently, the build is not leveraging cache.



Example: https://github.com/apache/arrow/runs/1336703057
{code:java}
Run actions/cache@v1
with:
    path: rust/target
    key: windows-rust-
    restore-keys: windows-rust-
{code}
note how the {{key}} does not contain the files hash.",pull-request-available,['Rust'],ARROW,Task,Major,2020-11-01 05:34:03,9
13338209,[Rust] Add support for merge-sort,Add kernel that merges two arrays according to a third vector of whether the item should come from the left array or the right array,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-31 12:44:05,9
13338186,[Rust] Make dictionary keys be a PrimitiveArray,"Currently, dictionary keys use a NullIterator to iterate them. However, physically, they are fully allocated in memory and are interpretable as a PrimitiveArray.

This issue's main purpose is to have `keys` return a `PrimitiveArray`, so that users can use it as any other array.

This will also allow to remove `NullIter`.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-31 06:56:55,9
13338181,[Rust] PrimitiveArray::new can create arrays not in spec,"Currently, PrimitiveArray::new passed with `null_count != 0` creates an array that does not follow the specification, as it is initialized with a{{None}} null bitmap but a non-zero null_count.

This method also makes no attempt to check for the buffer's aligment, which leads to UB.

Since a change in this method's signature requires a backward incompatible change, and it is only used in tests, I propose that we just remove it: we have good offers to create primitive arrays:
 * from an {{ArrayData}},
 * from a vector or vector of optionals
 * from an iterator

which covers all major cases.",pull-request-available,['Rust'],ARROW,Bug,Major,2020-10-31 05:11:09,9
13338137,[C++][Dataset][Python] Add a callback to visit file writers just before Finish(),This will fill the role of (for example) {{metadata_collector}} or allow stats to be embedded in IPC file footer metadata.,pull-request-available,['C++'],ARROW,Improvement,Major,2020-10-30 20:31:29,6
13337954,[C++] Arrow type large_string cannot be written to Parquet type column descriptor,"When trying to write a dataset in parquet format, arrow errors with the message: ""Arrow type large_string cannot be written to Parquet type column descriptor""
{code:java}
arrow::write_dataset(
 dataframe,
 ""/directory/"",
 ""parquet"",
 ""partitioning"" = c(""col1"", ""col2"")
)
{code}
The dataframe in question is very large with one column containing the text of message board posts encoded in HTML.",parquet pull-request-available,"['C++', 'R']",ARROW,Bug,Minor,2020-10-29 21:49:30,2
13337938,[Rust] Simplify code for impl PrimitiveArray,"With the end of specialization, there is now a simpler way of writing the generics for PrimitiveArrays.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-29 20:26:40,9
13337934,[Rust] Removed unused BinaryArrayBuilder,"The trait BinaryArrayBuilder is not used anywhere, and I think that it is an artifact from old code. Consider removing it.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-29 20:09:40,9
13337931,[R] Feather reader/writer should accept a MemoryPool,"https://github.com/apache/arrow/pull/8533#issuecomment-717516187

Currently feather readers and writers will allocate all buffers from the default memory pool. This is contrary to the Arrow convention of allowing control over memory allocation.
",pull-request-available,['C++'],ARROW,Improvement,Major,2020-10-29 19:46:42,0
13337930,"[C++] FileSystem::OpenInput{File,Stream} should accept a MemoryPool","https://github.com/apache/arrow/pull/8533#issuecomment-717516187

Currently buffers read from files opened by a FileSystem will always be allocated from the default memory pool. This is contrary to the Arrow convention of allowing control over memory allocation.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-10-29 19:44:48,2
13337743,[R] Support Tables in Flight,"When I wrote the flight POC bindings in the arrow package, the Python/reticulate interface didn't support Tables/ChunkedArrays yet, so it forced everything to work with RecordBatches only. ",pull-request-available,['R'],ARROW,New Feature,Major,2020-10-28 20:35:39,4
13337668,"[C++] CMake Build Fails with grpc 1.33.1, ""GRPC_CPP_PLUGIN-NOTFOUND: program not found or is not executable""","A cmake build of Apache Arrow 2.0.0 fails when using library grpc version 1.33.1 with the error:
{code:java}
GRPC_CPP_PLUGIN-NOTFOUND: program not found or is not executable
{code}
This is for a Macports port of Apache Arrow; seehttps://github.com/macports/macports-ports/pull/7791.

-The build previously worked on grpc version1.30.2.- (Correction: this was using the Makefile, not the cmake system.)

I am following the build instructions at

[https://github.com/apache/arrow/blob/master/docs/source/developers/python.rst#build-and-test]

I verify that the executablegrpc_cpp_plugin is installed with all grpc shared libraries.

I've readcpp/cmake_modules/FindgRPCAlt.cmake and believe that I've set all the correct flags for cmake (below) and believe that this is an issue with the arrow cmake configuration.

The error occurs at this line:


{code:java}
/opt/local/bin/protoc -I/opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/arrow-2.0.0/cpp/../format --cpp_out=/opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/build/src/arrow/flight /opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/arrow-2.0.0/cpp/../format/Flight.proto
cd /opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/build/src/arrow/flight && /opt/local/bin/protoc -I/opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/arrow-2.0.0/cpp/../format --grpc_out=/opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/build/src/arrow/flight --plugin=protoc-gen-grpc=GRPC_CPP_PLUGIN-NOTFOUND /opt/local/var/macports/build/_Users_runner_work_1_s_devel_apache-arrow/apache-arrow/work/arrow-2.0.0/cpp/../format/Flight.proto
GRPC_CPP_PLUGIN-NOTFOUND: program not found or is not executable
Please specify a program using absolute path or make sure the program is available in your PATH system variable
--grpc_out: protoc-gen-grpc: Plugin failed with status code 1.{code}
Example build log: [https://paste.z0k.xyz/ad7d47f2f9e6.txt]

Cmake flags:
{code:java}
          -DARROW_FLIGHT=ON \
          -DARROW_GRPC_USE_SHARED=ON \
          -DARROW_JEMALLOC=OFF \
          -DARROW_ORC=ON \
          -DARROW_PARQUET=ON \
          -DARROW_PLASMA=ON \
          -DARROW_PROTOBUF_USE_SHARED=ON \
          -DARROW_PYTHON=ON \
          -DARROW_USE_CCACHE=OFF \
          -DARROW_WITH_BZ2=ON \
          -DARROW_WITH_ZLIB=ON \
          -DARROW_WITH_ZSTD=ON \
          -DARROW_WITH_LZ4=ON \
          -DARROW_WITH_SNAPPY=ON \
          -DARROW_WITH_BROTLI=ON \
          -DARROW_INSTALL_NAME_RPATH=OFF \
          -DCARES_PREFIX=${prefix} \
          -DgRPC_INSTALL=OFF \
          -DgRPC_ROOT=${prefix} \
          -DgRPC_BUILD_TESTS=OFF \
          -DgRPC_CARES_PROVIDER=package \
          -DgRPC_ABSL_PROVIDER=package \
          -DgRPC_PROTOBUF_PROVIDER=package \
          -DgRPC_RE2_PROVIDER=package \
          -DgRPC_SSL_PROVIDER=package \
          -DgRPC_ZLIB_PROVIDER=package \
          -DLLVM_ROOT=${llvm_prefix}
{code}",cmake grpc pull-request-available,['C++'],ARROW,Bug,Major,2020-10-28 13:32:21,1
13337499,[C++] Unify dictionaries when writing IPC file in a single shot,"I read a big (taxi) csv file and specified that I wanted to dictionary-encode some columns. The resulting Table has ChunkedArrays with 1604 chunks. When I go to write this Table to the IPC file format (write_feather), I get an error: 

{code}
  Invalid: Dictionary replacement detected when writing IPC file format. Arrow IPC files only support a single dictionary for a given field accross all batches.
{code}

I can write this to Parquet and read it back in, and the roundtrip of the data is correct. We should be able to do this in IPC too.",pull-request-available,['C++'],ARROW,Wish,Major,2020-10-27 19:10:07,2
13337468,[Rust] Improve array equality,"Currently, equality between two arrays requires downcasting the array to the correct type before comparing them. This is not very ergonomic.

The root cause of this is that array equality currently depends on the concrete types. I.e. the actual calculations depend on the concrete type.

The goal of this task is to simplify array equality by making it independent of the concrete type that the array depends on.

",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-27 16:08:28,9
13337295,[R] Fix performance regression from cpp11::r_string,"On a taxi CSV read demo, reading a data.frame went from ~7 seconds to ~30 seconds. One of the cpp11 changes was identified to be a cause, and reverting it recovered the lost performance.",pull-request-available,['R'],ARROW,Bug,Critical,2020-10-26 22:10:12,4
13337280,[Rust] [Parquet] Expose SliceableCursor and FileSource,"https://github.com/apache/arrow/commit/7155cd5488310c15d864428252ca71dd9ebd3b48 Reworked how the parquet reader traits were implemented to be interms of a `ChunkReader` trait (for the better, in my opinion). 

That commit includes two helper classes,  `SliceableCursor` and `FileSource`, which implement `ChunkReader` for a `Cursor` like thing and `File`s, respectively

My project uses the parquet SerializedFileWriter with things that look like `File` and `Cursor` and thus I would like to re-use the logic in  `SliceableCursor` and `FileSource` without having to copy/paste them. 
",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-26 19:56:37,11
13337266,[Rust] [Large]BinaryArray can be created from non-binary datatypes,"In the same way StringArray has a dedicated offsetsize with the corresponding datatype, we can do that for BinaryArray to validate the type.",pull-request-available,['Rust'],ARROW,Bug,Major,2020-10-26 18:08:28,9
13337227,[Rust] [Parquet] Regression Can not implement custom ParquetWriter because `TryClone` is not publically exported,"As of this commit

https://github.com/apache/arrow/commit/7155cd5488310c15d864428252ca71dd9ebd3b48

I don't think it is possible for a user of the arrow trait to implement a custom Parquet writer anymore. Specifically, theParquetWriter trait requires `TryClone` implemented,  https://github.com/apache/arrow/blob/master/rust/parquet/src/file/writer.rs#L117-L118

{code}

pub trait ParquetWriter: Write + Seek + TryClone {}
impl<T: Write + Seek + TryClone> ParquetWriter for T {}

/// A serialized implementation for Parquet [`FileWriter`].
/// See documentation on file writer for more information.
pub struct SerializedFileWriter<W: ParquetWriter> {

{code}


but `TryClone` is can not be used. It is a `pub` trait:

https://github.com/apache/arrow/blob/master/rust/parquet/src/util/io.rs#L28-L32

{code}
/// TryClone tries to clone the type and should maintain the `Seek` position of the given
/// instance.
pub trait TryClone: Sized {
    /// Clones the type returning a new instance or an error if it's not possible
    /// to clone it.
    fn try_clone(&self) -> Result<Self>;
}
{code}

But the module it is (util.io) in is not marked as `pub`: https://github.com/apache/arrow/blob/master/rust/parquet/src/lib.rs#L39

```
 #[macro_use]
mod util;
#[cfg(any(feature = ""arrow"", test))]
pub mod arrow;
pub mod column;
pub mod compression;
mod encodings;
pub mod file;
pub mod record;
pub mod schema;
{code}


",pull-request-available,['Rust'],ARROW,Bug,Major,2020-10-26 14:31:01,11
13337159,[Java] Fix Spark integration build failure,"As discussed in [https://github.com/apache/arrow/pull/8475#issuecomment-716377181,]the integration build is failing because we have changed the constructor API.

We need to restore the original constructor and make it deprecated.",pull-request-available,['Java'],ARROW,Bug,Major,2020-10-26 08:25:56,7
13337037,[Rust] Generalize Arrow to support MergeSort,"Currently, the code to sort is centered around creating an array that can be sorted. This is useful for intra-array comparison, but does not allow things like `merge-sort`, where a comparison between two arrays (of the same data type) is needed.

The goal of this issue is to generalize the current code to support both.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-24 16:19:17,9
13336853,[Rust] Remove PrimitiveArrayOps,"It is no longer needed, now that we no longer have specialization.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-23 06:05:08,9
13336771,[C++][Dataset] Read compressed CSVs ,It would be nice if arrow can read compressed csv files,dataset pull-request-available,"['C++', 'R']",ARROW,New Feature,Major,2020-10-22 16:38:52,0
13336754,[R] Linux system requirements check needs to support older cmake versions,"I do not necessarily think this is a bug but no other category seemed to fit correctly.



When I attempt to install the newest package on Ubuntu Bionic, the code seems to be converting my environmental variable{{ARROW_S3=ON}}to{{ARROW_S3=OFF}}.

!image-2020-10-22-08-10-10-757.png!

The issue seems to be with the{{libcurl4-openssl-dev}}package, but I already have this package installed.

!image-2020-10-22-08-10-19-711.png!

I am guessing that I am making a simple error, but curious if anyone else has come upon a similar issue with the install?

Thanks for the great package,
Mike



",pull-request-available,['R'],ARROW,Bug,Minor,2020-10-22 15:10:48,4
13336735,[Python] Spurious s3fs-related test failures,"I frequently get this error when running the Python test suite:
{code}
_____________________________________________________________ test_write_to_dataset_pathlib_nonlocal[False] _____________________________________________________________
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 984, in _initiate_upload
    Bucket=self.bucket, Key=self.key, ACL=self.acl)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 971, in _call_s3
    **kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 189, in _call_s3
    return method(**additional_kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/botocore/client.py"", line 357, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/botocore/client.py"", line 661, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.NoSuchBucket: An error occurred (NoSuchBucket) when calling the CreateMultipartUpload operation: The specified bucket does not exist

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_parquet.py"", line 2978, in test_write_to_dataset_pathlib_nonlocal
    tempdir / ""test1"", use_legacy_dataset, filesystem=fs)
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_parquet.py"", line 2853, in _test_write_to_dataset_with_partitions
    pq.write_metadata(output_table.schema, f)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1457, in __exit__
    self.close()
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close
    self.flush(force=True)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush
    self._initiate_upload()
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload
    raise translate_boto_error(e)
FileNotFoundError: The specified bucket does not exist
------------------------------------------------------------------------- Captured stderr call --------------------------------------------------------------------------
Exception ignored in: Exception ignored in: Exception ignored in: <function AbstractBufferedFile.__del__ at 0x7f1b119097a0>
Traceback (most recent call last):
<function AbstractBufferedFile.__del__ at 0x7f1b119097a0>
Exception ignored in:   File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1446, in __del__
<function AbstractBufferedFile.__del__ at 0x7f1b119097a0>
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1446, in __del__
<function AbstractBufferedFile.__del__ at 0x7f1b119097a0>
Exception ignored in: Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1446, in __del__
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1446, in __del__
<function AbstractBufferedFile.__del__ at 0x7f1b119097a0>    Exception ignored in: <function AbstractBufferedFile.__del__ at 0x7f1b119097a0>    self.close()
self.close()
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1446, in __del__
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close

  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close
        self.flush(force=True)    
self.flush(force=True)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush

self.close()
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close
            self._initiate_upload()
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload
Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1446, in __del__
self.close()self.flush(force=True)    self._initiate_upload()
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload


  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close
      File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush
            raise translate_boto_error(e)self.close()self.close()    
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close

  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1425, in close
raise translate_boto_error(e)
self._initiate_upload()
FileNotFoundError: 
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload
The specified bucket does not existFileNotFoundError: 
The specified bucket does not exist
        self.flush(force=True)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush
self.flush(force=True)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush
    self.flush(force=True)
      File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 1297, in flush
    self._initiate_upload()raise translate_boto_error(e)
FileNotFoundError: The specified bucket does not exist

  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload
        self._initiate_upload()
self._initiate_upload()
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/s3fs/core.py"", line 986, in _initiate_upload
    raise translate_boto_error(e)
FileNotFoundError: The specified bucket does not exist
        raise translate_boto_error(e)raise translate_boto_error(e)

FileNotFoundErrorFileNotFoundError: : The specified bucket does not existThe specified bucket does not exist

{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-10-22 13:58:28,5
13336711,[Dev] Fix archery release utility test cases,"New releases of python-semver don't support comparing semver objects with strings.
See build errors: https://github.com/apache/arrow/pull/8504/checks?check_run_id=1292157257",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-10-22 12:10:05,3
13336633,[Rust] [DataFusion] Remove collect from merge,"The merge currently uses a collect within the thread, which causes the accumulation to happen before merging all batches together, see [https://github.com/apache/arrow/pull/8473/files#r506304982]

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-10-22 05:09:46,11
13336587,[R] Remove duplicate setting of S3 flag on macOS,"Observed here: https://github.com/autobrew/scripts/pull/3#issuecomment-712494384

It doesn't hurt anything but might as well clean it up.",pull-request-available,['R'],ARROW,Bug,Trivial,2020-10-21 21:48:09,4
13336574,[Dev][Archery] Test is failed with semver 2.13.0,"https://github.com/apache/arrow/runs/1276765550?check_suite_focus=true

{noformat}
=================================== FAILURES ===================================
_____________________________ test_release_basics ______________________________

fake_jira = <test_release.FakeJira object at 0x7fac73641630>

    def test_release_basics(fake_jira):
>       r = Release.from_jira(""1.0.0"", jira=fake_jira)

archery/tests/test_release.py:202: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '1.0.0'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
________________________ test_previous_and_next_release ________________________

fake_jira = <test_release.FakeJira object at 0x7fac7372d668>

    def test_previous_and_next_release(fake_jira):
>       r = Release.from_jira(""3.0.0"", jira=fake_jira)

archery/tests/test_release.py:229: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '3.0.0'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
_____________________________ test_release_issues ______________________________

fake_jira = <test_release.FakeJira object at 0x7fac73896588>

    def test_release_issues(fake_jira):
        # major release issues
>       r = Release.from_jira(""1.0.0"", jira=fake_jira)

archery/tests/test_release.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '1.0.0'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
_______________________ test_release_commits[1.0.0-771] ________________________

fake_jira = <test_release.FakeJira object at 0x7fac73856e10>, version = '1.0.0'
ncommits = 771

    @pytest.mark.parametrize(('version', 'ncommits'), [
        (""1.0.0"", 771),
        (""0.17.1"", 27),
        (""0.17.0"", 569),
        (""0.15.1"", 41)
    ])
    def test_release_commits(fake_jira, version, ncommits):
>       r = Release.from_jira(version, jira=fake_jira)

archery/tests/test_release.py:313: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '1.0.0'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
_______________________ test_release_commits[0.17.1-27] ________________________

fake_jira = <test_release.FakeJira object at 0x7fac739019e8>, version = '0.17.1'
ncommits = 27

    @pytest.mark.parametrize(('version', 'ncommits'), [
        (""1.0.0"", 771),
        (""0.17.1"", 27),
        (""0.17.0"", 569),
        (""0.15.1"", 41)
    ])
    def test_release_commits(fake_jira, version, ncommits):
>       r = Release.from_jira(version, jira=fake_jira)

archery/tests/test_release.py:313: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '0.17.1'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
_______________________ test_release_commits[0.17.0-569] _______________________

fake_jira = <test_release.FakeJira object at 0x7fac73823198>, version = '0.17.0'
ncommits = 569

    @pytest.mark.parametrize(('version', 'ncommits'), [
        (""1.0.0"", 771),
        (""0.17.1"", 27),
        (""0.17.0"", 569),
        (""0.15.1"", 41)
    ])
    def test_release_commits(fake_jira, version, ncommits):
>       r = Release.from_jira(version, jira=fake_jira)

archery/tests/test_release.py:313: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '0.17.0'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
_______________________ test_release_commits[0.15.1-41] ________________________

fake_jira = <test_release.FakeJira object at 0x7fac73942470>, version = '0.15.1'
ncommits = 41

    @pytest.mark.parametrize(('version', 'ncommits'), [
        (""1.0.0"", 771),
        (""0.17.1"", 27),
        (""0.17.0"", 569),
        (""0.15.1"", 41)
    ])
    def test_release_commits(fake_jira, version, ncommits):
>       r = Release.from_jira(version, jira=fake_jira)

archery/tests/test_release.py:313: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '0.15.1'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
_______________________ test_maintenance_patch_selection _______________________

fake_jira = <test_release.FakeJira object at 0x7fac73620128>

    def test_maintenance_patch_selection(fake_jira):
>       r = Release.from_jira(""0.17.1"", jira=fake_jira)

archery/tests/test_release.py:322: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
archery/release.py:281: in from_jira
    version = jira.project_version(version, project='ARROW')
archery/release.py:93: in project_version
    return versions[versions.index(version_string)]
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:203: in wrapper
    return operator(self, other)
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:573: in __eq__
    return self.compare(other) == 0
/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:493: in compare
    other = cls.parse(other)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'archery.release.Version'>, version = '0.17.1'

        @classmethod
        def parse(cls, version):
            """"""
            Parse version string to a VersionInfo instance.
    
            :param version: version string
            :return: a :class:`VersionInfo` instance
            :raises: :class:`ValueError`
            :rtype: :class:`VersionInfo`
    
            .. versionchanged:: 2.11.0
               Changed method from static to classmethod to
               allow subclasses.
    
            >>> semver.VersionInfo.parse('3.4.5-pre.2+build.4')
            VersionInfo(major=3, minor=4, patch=5, \
    prerelease='pre.2', build='build.4')
            """"""
            match = cls._REGEX.match(ensure_str(version))
            if match is None:
                raise ValueError(""%s is not valid SemVer string"" % version)
    
            version_parts = match.groupdict()
    
            version_parts[""major""] = int(version_parts[""major""])
            version_parts[""minor""] = int(version_parts[""minor""])
            version_parts[""patch""] = int(version_parts[""patch""])
    
>           return cls(**version_parts)
E           TypeError: __init__() got an unexpected keyword argument 'major'

/opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/semver.py:734: TypeError
=============================== warnings summary ===============================
../../../../../../../opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/jwt/utils.py:8
  /opt/hostedtoolcache/Python/3.5.10/x64/lib/python3.5/site-packages/jwt/utils.py:8: CryptographyDeprecationWarning: Python 3.5 support will be dropped in the next release ofcryptography. Please upgrade your Python.
    from cryptography.hazmat.primitives.asymmetric.utils import (

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED archery/tests/test_release.py::test_release_basics - TypeError: __init...
FAILED archery/tests/test_release.py::test_previous_and_next_release - TypeEr...
FAILED archery/tests/test_release.py::test_release_issues - TypeError: __init...
FAILED archery/tests/test_release.py::test_release_commits[1.0.0-771] - TypeE...
FAILED archery/tests/test_release.py::test_release_commits[0.17.1-27] - TypeE...
FAILED archery/tests/test_release.py::test_release_commits[0.17.0-569] - Type...
FAILED archery/tests/test_release.py::test_release_commits[0.15.1-41] - TypeE...
FAILED archery/tests/test_release.py::test_maintenance_patch_selection - Type...
=================== 8 failed, 34 passed, 1 warning in 23.52s ===================
Error: Process completed with exit code 1.
{noformat}",pull-request-available,"['Archery', 'Developer Tools']",ARROW,Test,Major,2020-10-21 20:29:46,1
13336513,[Python] Remove workaround for CMake bug in manylinux,"The following issue should be fixed in CMake 3.18:
https://github.com/pypa/manylinux/issues/484",pull-request-available,"['Packaging', 'Python']",ARROW,Task,Minor,2020-10-21 16:18:58,2
13336435,[CI] Bump github actions cache version,"Currently we're using actions/cache@v1 but there is a newer version available, see [what's new|https://github.com/actions/cache/tree/v2.1.1#whats-new].",pull-request-available,['Continuous Integration'],ARROW,Improvement,Trivial,2020-10-21 09:19:07,2
13336351,[R] Followups to 2.0.0 release,"* List optional SystemRequirements
* Restore (and deprecate) properties and arrow_properties arguments to write_parquet because a package depending on arrow happened to use them",pull-request-available,['R'],ARROW,Bug,Major,2020-10-20 18:09:49,4
13336278,[C++] Parquet decompresses DataPageV2 pages even if is_compressed==0,"According to the parquet-format specification, DataPageV2 pages have an is_compressed flag. Even if the column chunk has a decompression codec set, the page is only compressed if this flag is true (this likely enables not compressing some pages where the compression wouldn't save memory).

Here is the relevant excerpt from parquet.thrift describing the semantics of the is_compressed flag in a DataPageV2:

_/** whether the values are compressed._
 _Which means the section of the page between_
 _definition_levels_byte_length + repetition_levels_byte_length + 1 and compressed_page_size (included)_
 _is compressed with the compression_codec._
 _If missing it is considered compressed */_
 _7: optional bool is_compressed = 1;_



It seems that the apache parquet cpp library (haven't checked other languages but might have the bug as well) totally disregards this flag and decompresses the page in all cases if a decompressor is set for the column chunk.

The erroneous code is in column_reader.cc:

std::shared_ptr<Page> SerializedPageReader::NextPage()

This method first decompresses the page if there is a decompressor set and only then does a case distinction on whether this page is a DataPageV2 and has the is_compressed flag. Thus, even if the page would have this flag set to 0, the page would be decompressed anyway.

The method that should use the is_compressed flag but doesn't is:

std::shared_ptr<Buffer> SerializedPageReader::DecompressPage

This method doesn't look at the is_compressed flag at all.



The reason why this bug probably doesn't show in any unit test is that the write implementation seems to do the same mistake: It always compresses the page, even if the page has its is_compressed flag set to false.

",pull-request-available,['C++'],ARROW,Bug,Major,2020-10-20 11:33:24,2
13336126,[Python] Build and publish aarch64 wheels,"The currently released source distribution for Arrow on pypi.org doesn't build on Ubuntu 20.04. It may be possible install additional build dependencies to make it work, but it would be better to publish aarch64 (arm64) wheels to pypi.org in addition to the currently published x86_64 wheels for Linux.

{{$ pip install pyarrow}}

should just work on Linux/aarch64.",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-10-19 18:30:10,3
13336069,[Python] Default S3 region is eu-central-1 even with LANG=C,"Verifying the macOS wheels using {{LANG=C dev/release/verify-release-candidate.sh wheels 2.0.0 2}} fails for me with

{code}
    @pytest.mark.s3
    def test_s3_real_aws():
        # Exercise connection code with an AWS-backed S3 bucket.
        # This is a minimal integration check for ARROW-9261 and similar issues.
        from pyarrow.fs import S3FileSystem
        fs = S3FileSystem(anonymous=True)
>       assert fs.region == 'us-east-1'  # default region
E       AssertionError: assert 'eu-central-1' == 'us-east-1'
E         - us-east-1
E         + eu-central-1
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2020-10-19 13:02:59,8
13335855,[Rust] Allow CSV reader to start from a line,"Currently, the CSV reader always start from 0 and go until the end, which makes it impossible for DataFusion to split a single file in parts, e.g. via multiple readers.

The goal of this issue is to generalize the reader to support starting from an arbitrary location (via iterating), and also have an optional upper limit.

",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-17 08:18:53,9
13335842,[Rust] [DataFusion] Re-organize errors,"DataFusion's errors do not have much love these days, and I think that they need a lift. For example,
 * we use ""General"" very often
 * the error is called ""ExecutionError"", even though sometimes it happens during planning
 * the error ""InvalidColumn"" is not being used
 * There is not much documentation about the errors

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-10-17 04:21:24,9
13335793,[C++] Consider using fast-double-parser,"We use Google's double-conversion library for parsing strings to doubles. We should consider using this library, which is more than 2x faster.
https://github.com/lemire/fast_double_parser

Parsing doubles is important for CSV performance.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-10-16 14:41:51,2
13335691,[C++][Dataset] Minimize Expression to a wrapper around compute::Function,"The Expression class hierarchy was originally intended to provide generic, structured representations of compute functionality. On the former point they have been superseded by compute::

{Function, Kernel, ...}

which encapsulates validation and execution. In light of this Expression can be drastically simplified and improved by composition with these classes. Each responsibility which can be deferred implies less boilerplate when exposing a new compute function for use in datasets. Ideally any compute function will be immediately available to use in a filter or projection.
{code:java}
struct Expression {
  using Literal = std::shared_ptr<Scalar>;

  struct Call {
    std::shared_ptr<ScalarFunction> function;
    std::shared_ptr<FunctionOptions> options;
    std::vector<Expression> arguments;
  };

  util::variant<Literal, FieldRef, Call> value;
};
{code}
A simple discriminated union as above should be sufficient to represent arbitrary filters and projections: any expression which results in type {{bool}} is a valid filter, and any expression which is a {{Projection}} may be used to map one record batch to another.

Expression simplification (currently implemented in {{Expression::Assume}}) is an optimization used for example in predicate pushdown, and therefore need not exhaustively cover the full space of available compute functions.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-10-16 00:59:45,6
13335657,[Rust] Convert RecordBatchIterator to a Stream,So that we the unit of work is a single record batch instead of a part of a partition.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-10-15 18:43:31,9
13335636,[Python] Consider using __wrapped__ for compute function introspection,"As suggested by [~bkietz] here:
https://github.com/apache/arrow/pull/8457#discussion_r504966207
",pull-request-available,['Python'],ARROW,Task,Major,2020-10-15 16:45:22,2
13335611,[C++] Improve UTF8 validation speed and CSV string conversion,"Based on profiling from ARROW-10308, UTF8 validation is a bottleneck of CSV string conversion.

This is because we must validate many small UTF8 strings individually.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-10-15 13:05:04,2
13335581,[Release] Update crossbow verification process,The automatized crossbow RC verification tasks needs to be updated since multiple builds are failing.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-10-15 10:12:36,3
13335281,[Python]Don't double-package plasma-store-server,This is part of the {{arrow-cpp}} and {{pyarrow}} conda packages. We shouldn't ship the version in {{pyarrow}} as this is just a copy to a different location.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-10-13 19:57:21,8
13335262,[Rust] Improve benchmark documentation for generating/converting TPC-H data,"The TPC-H benchmark for datafusion works with Parquet/CSV data but the data generation routine described in the README generates `.tbl` data.

Could we describe how the TPC-H Parquet/CSV data can be generated to make the benchmark easier to setup and more reproducible ?",pull-request-available,['Rust'],ARROW,Wish,Minor,2020-10-13 16:58:09,10
13335249,[Rust] Support reading and writing V5 of IPC metadata,"This is mostly alignment issues and tracking when we encounter the v4 legacy padding.

I had done this work in another branch, but discarded it without noticing.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-10-13 15:44:27,12
13335130,[Rust] [DataFusion] Simplify accumulators,Replace Rc<RefCell<>> by Box<>.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-10-13 05:57:48,9
13335127,[Java] Resolve problems of DecimalVector APIs on ArrowBufs,"Unlike other fixed width vectors, DecimalVectors have some APIs that directly manipulate an ArrowBuf (e.g. \{{void set(int index, int isSet, int start, ArrowBuf buffer)}}).

After supporting 64-bit ArrowBufs, we need to adjust such APIs so that they work properly.",pull-request-available,['Java'],ARROW,Bug,Major,2020-10-13 04:52:19,7
13335124,[Rust] [DataFusion] Fix benchmarks,"They are only benchmarking planning, not execution.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-10-13 04:25:45,9
13335061,[Rust] Support reading dictionary streams,"We support reading dictionaries in the IPC file reader.

We should do the same with the stream reader.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-10-12 17:51:55,12
13335042,[C++] Compilation fails on i386,"{code}
[446/645] Building CXX object src/parquet/CMakeFiles/parquet_objlib.dir/level_conversion_bmi2.cc.o
FAILED: src/parquet/CMakeFiles/parquet_objlib.dir/level_conversion_bmi2.cc.o 
/usr/bin/ccache /usr/bin/c++  -DARROW_HAVE_RUNTIME_AVX2 -DARROW_HAVE_RUNTIME_AVX512 -DARROW_HAVE_RUNTIME_BMI2 -DARROW_HAVE_RUNTIME_SSE4_2 -DARROW_HAVE_SSE4_2 -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR="""" -DARROW_NO_DEPRECATED_API -DARROW_USE_GLOG -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_TIMING_TESTS -DARROW_WITH_UTF8PROC -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DGTEST_LINKED_AS_SHARED_LIBRARY=1 -DHAVE_INTTYPES_H -DHAVE_NETDB_H -DHAVE_NETINET_IN_H -DPARQUET_EXPORTING -isystem /arrow/cpp/thirdparty/flatbuffers/include -isystem thrift_ep-install/include -isystem jemalloc_ep-prefix/src -isystem googletest_ep-prefix/include -isystem /arrow/cpp/thirdparty/hadoop/include -isystem orc_ep-install/include -Isrc -I/arrow/cpp/src -I/arrow/cpp/src/generated -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-deprecated-declarations -Wno-sign-conversion -Wno-unused-variable -Werror -fno-semantic-interposition -msse4.2  -g -fPIC   -std=c++11 -march=haswell -mavx2 -DARROW_HAVE_BMI2 -mbmi2 -MD -MT src/parquet/CMakeFiles/parquet_objlib.dir/level_conversion_bmi2.cc.o -MF src/parquet/CMakeFiles/parquet_objlib.dir/level_conversion_bmi2.cc.o.d -o src/parquet/CMakeFiles/parquet_objlib.dir/level_conversion_bmi2.cc.o -c /arrow/cpp/src/parquet/level_conversion_bmi2.cc
In file included from /arrow/cpp/src/parquet/level_conversion_bmi2.cc:20:0:
/arrow/cpp/src/parquet/level_conversion_inc.h: In function 'uint64_t parquet::internal::bmi2::ExtractBits(uint64_t, uint64_t)':
/arrow/cpp/src/parquet/level_conversion_inc.h:263:10: error: '_pext_u64' was not declared in this scope
   return _pext_u64(bitmap, select_bitmap);
          ^~~~~~~~~
/arrow/cpp/src/parquet/level_conversion_inc.h:263:10: note: suggested alternative: '_pext_u32'
   return _pext_u64(bitmap, select_bitmap);
          ^~~~~~~~~
          _pext_u32
{code}",pull-request-available,['C++'],ARROW,Bug,Critical,2020-10-12 15:18:00,2
13335024,[C++] Avoid std::random_device whenever possible,"I just tried running the tests on a default build on Ubuntu 20.04 with an AMD Ryzen CPU (where presumably RDRAND can't be relied on, but I don't know if that's the underlying reason), and the tests appear to hand blocking on {{std::random_device}} at some point. I suppose some versions of libstdc++ will use {{/dev/random}} instead of {{/dev/urandom}} (which is a bad idea in itself).

We should probably try to minimize our usage of {{std::random_device}}. After all, we're not generating cryptographic keys or anything.",pull-request-available,['C++'],ARROW,Wish,Minor,2020-10-12 14:16:38,2
13335016,[C++][Flight] Misleading CMake errors,"When building Arrow, one can get errors such as the following

{code}
-- Build output: Change Dir: /home/antoine/arrow/dev/cpp/build-test/src/arrow/flight/try_compile/CMakeFiles/CMakeTmp

Run Build Command(s):/usr/bin/ninja cmTC_cfc86 && [1/2] Building CXX object CMakeFiles/cmTC_cfc86.dir/check_tls_opts_132.cc.o
FAILED: CMakeFiles/cmTC_cfc86.dir/check_tls_opts_132.cc.o 
/usr/bin/g++-9  -I/home/antoine/arrow/dev/cpp/thirdparty/flatbuffers/include -I/home/antoine/arrow/dev/cpp/build-test/jemalloc_ep-prefix/src -I/home/antoine/arrow/dev/cpp/build-test/mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.6/include -I/home/antoine/arrow/dev/cpp/thirdparty/hadoop/include -I/home/antoine/arrow/dev/cpp/build-test/src -I/home/antoine/arrow/dev/cpp/src -I/home/antoine/arrow/dev/cpp/src/generated -isystem /home/antoine/miniconda3/envs/pyarrow/include -Wno-noexcept-type  -fdiagnostics-color=always -fuse-ld=gold -ggdb -O0  -Wall -Wno-conversion -Wno-deprecated-declarations -Wno-sign-conversion -Wno-unused-variable  -fno-semantic-interposition -march=haswell -mavx2  -D_GLIBCXX_USE_CXX11_ABI=1 -D_GLIBCXX_USE_CXX11_ABI=1 -fno-omit-frame-pointer  -fPIE -std=c++11 -o CMakeFiles/cmTC_cfc86.dir/check_tls_opts_132.cc.o -c /home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_132.cc
/home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_132.cc:28:31: error: 'TlsCredentialsOptions' in namespace 'grpc::experimental' does not name a type; did you mean 'AltsCredentialsOptions'?
   28 |     const grpc::experimental::TlsCredentialsOptions* options) {
      |                               ^~~~~~~~~~~~~~~~~~~~~
      |                               AltsCredentialsOptions
/home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_132.cc: In function 'grpc_tls_server_verification_option check(const int*)':
/home/antoine/arrow/dev/cpp/src/arrow/flight/try_compile/check_tls_opts_132.cc:29:61: error: request for member 'server_verification_option' in '* options', which is of non-class type 'const int'
   29 |   grpc_tls_server_verification_option server_opt = options->server_verification_option();
      |                                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~
ninja: build stopped: subcommand failed.


{code}

There are two problems:
1) this is displayed as an error, but it's not an actual error from the user's point of view (Arrow builds fine nevertheless)
2) this is displayed each time when building, even if nothing needs rebuilding",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-10-12 13:43:51,0
13335005,[Python] Pyarrow is raising deprecation warning about filesystems on import,"This happens on import (when setting the warning to be visisble), so even when the user doesn't use the deprecated filesystems:

{code}
In [1]: import warnings

In [2]: warnings.simplefilter(""always"")

In [3]: import pyarrow
/home/joris/scipy/repos/arrow/python/pyarrow/filesystem.py:255: DeprecationWarning: pyarrow.filesystem.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.
  cls._instance = LocalFileSystem()
{code}

I forgot to add a filter in the construction of the LocalFileSystem ..",pull-request-available,['Python'],ARROW,Bug,Major,2020-10-12 12:53:47,5
13335003,"[Python] Python deprecation warning for ""PY_SSIZE_T_CLEAN will be required for '#' formats""","We have a few cases that run into this python deprecation warning:

{code}
pyarrow/tests/test_pandas.py: 9 warnings
pyarrow/tests/test_parquet.py: 7790 warnings
  sys:1: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats
pyarrow/tests/test_pandas.py::TestConvertDecimalTypes::test_decimal_with_None_explicit_type
pyarrow/tests/test_pandas.py::TestConvertDecimalTypes::test_decimal_with_None_infer_type
  /buildbot/AMD64_Conda_Python_3_8/python/pyarrow/tests/test_pandas.py:114: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats
    result = pd.Series(arr.to_pandas(), name=s.name)
pyarrow/tests/test_pandas.py::TestConvertDecimalTypes::test_strided_objects
  /buildbot/AMD64_Conda_Python_3_8/python/pyarrow/pandas_compat.py:1127: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats
    result = pa.lib.table_to_blocks(options, block_table, categories,
{code}

Related to https://bugs.python.org/issue36381

I think one such usage example is at https://github.com/apache/arrow/blob/0b481523b7502a984788d93b822a335894ffe648/cpp/src/arrow/python/decimal.cc#L106",pull-request-available,['Python'],ARROW,Bug,Major,2020-10-12 12:34:41,5
13334985,[Python] Fix warnings when running tests,We have accumulated quite some warnings,pull-request-available,['Python'],ARROW,Bug,Major,2020-10-12 11:08:55,5
13334962,[Packaging][Python] Fix macOS wheel artifact patterns,The platform tags have changed for certain macos wheel artifacts.,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-10-12 08:59:58,3
13334961,[Release][Python] Fix verification script to align with the new macos wheel platform tags,The verification script hardcodes platform tag based on interpreter flags.,pull-request-available,['Python'],ARROW,Bug,Major,2020-10-12 08:58:52,3
13334914,[C++] Support comparing scalars approximately,"As discussed in [https://github.com/apache/arrow/pull/7748#discussion_r469997286,]we need to compare scalars approximately in some scenarios.",pull-request-available,['C++'],ARROW,New Feature,Major,2020-10-12 02:44:52,2
13334828,[Packaging][Python] Pin newer multibuild version to avoid updating homebrew,Build failure: https://travis-ci.org/github/ursa-labs/crossbow/builds/734324594,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-10-10 22:15:03,3
13334823,[Rust] packed_simd is broken and continued under a new project,"The dependency doesn't compile on newer versions of nightly. This is also known by the (new) project maintainers. Due to complications they continued the project under a new name: `packed_simd_2`.


packed_simd = { version = ""0.3.4"", package = ""packed_simd_2"" }


See:

https://github.com/rust-lang/packed_simd",pull-request-available,['Rust'],ARROW,Bug,Blocker,2020-10-10 19:25:27,12
13334821,[R] Fix CSV timestamp_parsers test on R-devel,"Apparently there is a change in the development version of R with respect to timezone handling. I suspect it is this: https://github.com/wch/r-source/blob/trunk/doc/NEWS.Rd#L296-L300

It causes this failure:

{code}
 1. Failure: read_csv_arrow() can read timestamps (@test-csv.R#216)  
`tbl` not equal to `df`.
Component ""time"": 'tzone' attributes are inconsistent ('UTC' and '')

 2. Failure: read_csv_arrow() can read timestamps (@test-csv.R#219)  
`tbl` not equal to `df`.
Component ""time"": 'tzone' attributes are inconsistent ('UTC' and '')
{code}

This needs to be fixed for the CRAN release because they check on the devel version. But it doesn't need to block the 2.0 release candidate because I can (at minimum) skip these tests before submitting to CRAN (FYI [~kszucs])

I'll also add a CI job to test on R-devel. I just removed 2 R jobs so we can afford to add one back.

cc [~romainfrancois]",pull-request-available,['R'],ARROW,Bug,Major,2020-10-10 18:33:00,4
13334813,[Rust] Update nightly: Oct 2020 Edition,"We should update to a more recent nighly after the 2.0.0 release. It carries some clippy annoyances, which will mean that I have to revert much of what I did around float comparisons.

Might also be preferable to do this sooner, so that we can complete the clippy integration and throw away the carrot in favour of the stick.",pull-request-available,['Rust'],ARROW,Task,Major,2020-10-10 14:26:28,12
13334775,[C++][Python] Parquet test failing with HadoopFileSystem URI,"Follow-up on ARROW-10175. In the HDFS integration tests, there is a test using a URI failing if we use the new filesystem / dataset implementation:

{code}
FAILED opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/tests/test_hdfs.py::TestLibHdfs::test_read_multiple_parquet_files_with_uri
{code}

fails with

{code}
pyarrow.lib.ArrowInvalid: Path '/tmp/pyarrow-test-838/multi-parquet-uri-48569714efc74397816722c9c6723191/0.parquet' is not relative to '/user/root'
{code}

while it is passing a URI (and not a filesystem object) to {{parquet.read_table}}, and the new filesystems/dataset implementation should be able to handle URIs.

cc [~apitrou]",filesystem hdfs pull-request-available,['Python'],ARROW,Bug,Major,2020-10-10 08:16:26,2
13334751,[Rust] [BREAKING] Lists should take Field instead of DataType,"There is currently no way of tracking nested field metadata on lists. For example, if a list's children are nullable, there's no way of telling just by looking at the Field.

This causes problems with integration testing, and also affects Parquet roundtrips.

I propose the breaking change of [Large|FixedSize]List taking a Field instead of Box<DataType>, as this will overcome this issue, and ensure that the Rust implementation passes integration tests.

CC [~andygrove] [~jorgecarleitao] [~alamb] [~jhorstmann] ([~carols10cents]as this addresses some of the roundtrip failures).

I'm leaning towards this landing in 3.0.0, as I'd love for us to have completed or made significant traction on the Arrow Parquet writer (and reader), and integration testing, by then.",pull-request-available,"['Integration', 'Rust']",ARROW,Sub-task,Major,2020-10-10 01:31:06,12
13334700,[Python] Add option to skip inclusion of Arrow headers in Python installation,We don't want to have them as part of the conda package as the single source should be {{arrow-cpp}}.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-10-09 18:49:06,8
13334699,[Rust] [DataFusion] MemTable::load() should load partitions in parallel,"MemTable::load() should load partitions in parallel using async tasks, rather than loading one partition at a time.

Also, we should make batch size configurable. It is currently hard-coded to 1024*1024 which can be quite inefficient.",beginner pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-10-09 18:35:49,10
13334689,[FlightRPC][C++] Remove default constructor for FlightClientOptions,We should delete the default constructor for FlightClientOptions and require the struct to always be initialized with Defaults().,pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2020-10-09 17:24:29,0
13334641,[C++][Dataset] Dataset writing does not write schema metadata,"Not sure if this is related to the writing refactor that landed yesterday, but `write_dataset` does not preserve the schema metadata (eg used for pandas metadata):

{code}
In [20]: df = pd.DataFrame({'a': [1, 2, 3]})

In [21]: table = pa.Table.from_pandas(df)

In [22]: table.schema
Out[22]: 
a: int64
-- schema metadata --
pandas: '{""index_columns"": [{""kind"": ""range"", ""name"": null, ""start"": 0, ""' + 396

In [23]: ds.write_dataset(table, ""test_write_dataset_pandas"", format=""parquet"")

In [24]: pq.read_table(""test_write_dataset_pandas/part-0.parquet"").schema
Out[24]: 
a: int64
  -- field metadata --
  PARQUET:field_id: '1'
{code}

I tagged it for 2.0.0 for a moment in case it's possible today, but I didn't yet look into how easy it would be to fix.

cc [~bkietz]",pull-request-available,['C++'],ARROW,Bug,Major,2020-10-09 12:13:44,6
13334637,[C++][Dataset] Cannot write dataset with dictionary column as partition field,"When the column to use for partitioning is dictionary encoded, we get this error:

{code}
In [9]: import pyarrow.dataset as ds

In [10]: part = [""xxx""] * 3 + [""yyy""] * 3
    ...: table = pa.table([
    ...:     pa.array(range(len(part))),
    ...:     pa.array(part).dictionary_encode(),
    ...: ], names=['col', 'part'])

In [11]: part = ds.partitioning(table.select([""part""]).schema)

In [12]: ds.write_dataset(table, ""test_dataset_dict_part"", format=""parquet"", partitioning=part)
---------------------------------------------------------------------------
ArrowTypeError                            Traceback (most recent call last)
<ipython-input-12-c7b81c9b0bda> in <module>
----> 1 ds.write_dataset(table, ""test_dataset_dict_part"", format=""parquet"", partitioning=part)

~/scipy/repos/arrow/python/pyarrow/dataset.py in write_dataset(data, base_dir, basename_template, format, partitioning, schema, filesystem, file_options, use_threads)
    773     _filesystemdataset_write(
    774         data, base_dir, basename_template, schema,
--> 775         filesystem, partitioning, file_options, use_threads,
    776     )

~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset._filesystemdataset_write()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowTypeError: scalar xxx (of type string) is invalid for part: dictionary<values=string, indices=int32, ordered=0>
In ../src/arrow/dataset/filter.cc, line 1082, code: VisitConjunctionMembers(*and_.left_operand(), visitor)
In ../src/arrow/dataset/partition.cc, line 257, code: VisitKeys(expr, [&](const std::string& name, const std::shared_ptr<Scalar>& value) { auto&& _error_or_value28 = (FieldRef(name).FindOneOrNone(*schema_)); do { ::arrow::Status __s = ::arrow::internal::GenericToStatus((_error_or_value28).status()); do { if ((__builtin_expect(!!(!__s.ok()), 0))) { ::arrow::Status _st = (__s); _st.AddContextLine(""../src/arrow/dataset/partition.cc"", 257, ""(_error_or_value28).status()""); return _st; } } while (0); } while (false); auto match = std::move(_error_or_value28).ValueUnsafe();;; if (match) { const auto& field = schema_->field(match[0]); if (!value->type->Equals(field->type())) { return Status::TypeError(""scalar "", value->ToString(), "" (of type "", *value->type, "") is invalid for "", field->ToString()); } values[match[0]] = value.get(); } return Status::OK(); })
In ../src/arrow/dataset/file_base.cc, line 321, code: (_error_or_value24).status()
In ../src/arrow/dataset/file_base.cc, line 354, code: task_group->Finish()
{code}

While this seems a quit normal use case, as this column will typically be repeated many times (and we also support reading it as such with dictionary type, so a roundtrip is currently not possible in that case)

I tagged it for 2.0.0 for a moment in case it's possible today, but I didn't yet look into how easy it would be to fix.",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-10-09 12:00:06,6
13334498,[C++] aws-sdk-cpp apparently requires zlib too,"https://github.com/aws/aws-sdk-cpp/blob/master/cmake/external_dependencies.cmake#L9

If you happen to be building on a bare system without zlib, the bundled aws-sdk-cpp build fails: https://github.com/ursa-labs/arrow-r-nightly/runs/1227805500?check_suite_focus=true#step:4:930

",pull-request-available,"['C++', 'Packaging']",ARROW,Bug,Major,2020-10-08 19:57:43,1
13334489,[C++] Duplicate values in a dictionary result in corrupted parquet,Initial discussion: https://lists.apache.org/thread.html/r8afb5aed3855e35fe03bd3a27f2c7e2177ed2825c5ad5f455b2c9078%40%3Cdev.arrow.apache.org%3E,pull-request-available,['C++'],ARROW,Bug,Major,2020-10-08 19:06:49,6
13334438,[Rust] [DataFusion] Make DataFusion casting rules consistent with cast kernel,"There are plan time checks for valid type casts in DataFusion that are designed to catch errors early before plan execution

Sadly the cast types that DataFusion thinks are valid is a significant subset of what the arrow cast kernel supports.  The goal of this ticket is to bring DataFusion to parity with the type casting supported by arrow and  allow DataFusion to plan all casts that are supported by the arrow cast kernel

(I want this implicitly so when I add support for DictionaryArray casts in Arrow they also are part of DataFusion)

Previously the notions of coercion and casting were somewhat conflated. I have tried to clarify them in https://github.com/apache/arrow/pull/8399 as well

For more detail, see https://github.com/apache/arrow/pull/8340#discussion_r501257096 from [~jorgecarleitao]
",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-08 14:55:25,11
13334411,[Rust][DataFusion] Improve documentation for type coercion,The code / comments for type coercion are a little confusing and don't make the distinction between coercion and casting clear -- we could improve the documentation to clarify this.,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-10-08 12:22:31,11
13334404,[Rust] Make array_value_to_string available in all Arrow builds,"Make array_value_to_string available in all Arrow builds

Currently the array_value_to_string function it is only available if the `feature = ""prettyprint""` is enabled. 

The rationale for making this change is that I want to be able to use `array_value_to_string` to write tests (such as on https://github.com/apache/arrow/pull/8346) but currently it is only available when `feature = ""prettyprint""` is enabled.

It appears that [~nevi_me] made prettyprint compilation optional so that arrow could be compiled for wasm in https://github.com/apache/arrow/pull/7400. https://issues.apache.org/jira/browse/ARROW-9088 explains that this is due to some dependency of pretty-table;   `array_value_to_string` has no needed dependencies.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-08 10:48:07,11
13334390,[CI] Unable to download minio in arm32v7 docker image,See build log https://github.com/apache/arrow/runs/1224947766#step:5:2021,pull-request-available,['CI'],ARROW,Improvement,Major,2020-10-08 09:50:57,3
13334387,[JS][Doc] JavaScript documentation fails to build,Probably because of typedoc updates.,pull-request-available,"['Documentation', 'JavaScript']",ARROW,Bug,Major,2020-10-08 09:40:26,3
13334302,[Rust] [Parquet] Fix null bitmap comparisons in roundtrip tests,"The Arrow spec allows makes the null bitmap optional if an array has no nulls [~carols10cents], so the tests that were failing were because we're comparing `None` with a 100% populated bitmap.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-10-07 22:32:41,12
13334194,"[Rust] [DataFusion] Rename ""Source"" typedef","The name ""Source"" for this type doesn't make sense to me. I would like to discuss alternate names for it.
{code:java}
type Source = Box<dyn RecordBatchReader + Send>; {code}
My first thoughts are:
 * RecordBatchIterator
 * RecordBatchStream
 * SendableRecordBatchReader",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-10-07 13:51:56,9
13334188,[Python] UnicodeDecodeError when printing schema with binary metadata,"The following small example raises a `UnicodeDecodeError` error, since PyArrow version 0.17.0:
{code:java}
import pyarrow as pa

bdata = b""\xff\xff\xff\xff8\x02\x00\x00\x10\x00\x00\x00\x00\x00\n\x00\x0c\x00\x06\x00\x05\x00\x08\x00\n\x00\x00\x00\x00\x01\x04\x00\x0c\x00\x00\x00\x08\x00\x08\x00\x00\x00\x04\x00\x08\x00\x00\x00\x04\x00\x00\x00\x02\x00\x00\x00\x00\x01\x00\x00\x04\x00\x00\x00\x1a\xff\xff\xff\x00\x00\x00\x0c\xd0\x00\x00\x00\x9c\x00\x00\x00\x90\x00\x00\x00\x04\x00\x00\x00\x02\x00\x00\x00P\x00\x00\x00\x04\x00\x00\x00\xc0\xfe\xff\xff\x08\x00\x00\x00 \x00\x00\x00\x14\x00\x00\x00ARROW:extension:name\x00\x00\x00\x00\x1b""

t = pa.table({""data"": pa.array([1, 2])}, metadata={b""k"": bdata})
print(t.schema){code}
In our case, the binary data is coming from the serialization of another PyArrow schema. But I guess the error can appear with any binary metadata in the schema.

The print used to work fine with PyArrow 0.16, getting this output:
{code:java}
data: int64
metadata
--------
OrderedDict([(b'k',
              b'\xff\xff\xff\xff8\x02\x00\x00\x10\x00\x00\x00\x00\x00\n\x00'
              b'\x0c\x00\x06\x00\x05\x00\x08\x00\n\x00\x00\x00\x00\x01\x04\x00'
              b'\x0c\x00\x00\x00\x08\x00\x08\x00\x00\x00\x04\x00'
              b'\x08\x00\x00\x00\x04\x00\x00\x00\x02\x00\x00\x00'
              b'\x00\x01\x00\x00\x04\x00\x00\x00\x1a\xff\xff\xff'
              b'\x00\x00\x00\x0c\xd0\x00\x00\x00\x9c\x00\x00\x00'
              b'\x90\x00\x00\x00\x04\x00\x00\x00\x02\x00\x00\x00P\x00\x00\x00'
              b'\x04\x00\x00\x00\xc0\xfe\xff\xff\x08\x00\x00\x00 \x00\x00\x00'
              b'\x14\x00\x00\x00ARROW:extension:name\x00\x00\x00\x00\x1b')])
{code}
I can work on a patch to reverse the behaviour back to PyArrow 0.16?",pull-request-available,['Python'],ARROW,Bug,Minor,2020-10-07 13:30:45,2
13334145,[C++] String split kernels do not propagate nulls correctly on sliced input,"I am not sure if this is a specific test issue or valid behavior, but when writing a test in [https://github.com/apache/arrow/pull/8271]

The following test fails:
{code:java}
this->CheckUnary(""split_pattern"", R""([""foo bar"", ""foo"", null])"", list(this->type()),  //                  R""([[""foo"", ""bar""], [""foo""], null])"", &options);
{code}
with the following output
{code:java}
Failed:
Got: 
  [
    [
      [
        ""foo"",
        ""bar""
      ]
    ],
    [
      [
        ""foo""
      ],
      null
    ]
  ]
Expected: 
  [
    [
      [
        ""foo"",
        ""bar""
      ]
    ],
    [
      [
        ""foo""
      ],
      null
    ]
  ]
{code}
while the outputs are the same, the arrays are seen as unequal.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-10-07 09:23:37,2
13334140,[C++] Unary kernels that results in a list have no preallocated offset buffer,"I noticed in

[https://github.com/apache/arrow/pull/8271]

That a string->list[string] kernel does not have the offsets preallocated in the output. I believe there is a preference for not doing allocations in kernels, so this can be optimized at a higher level. I think it can also be done in this case.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-10-07 09:13:09,2
13334108,[Doc] Capture guidance for endianness support in contributors guide.,https://mail-archives.apache.org/mod_mbox/arrow-dev/202009.mbox/%3cCAK7Z5T--HHhr9Dy43PYhD6m-XoU4qoGwQVLwZsG-kOxXjPTyZA@mail.gmail.com%3e,pull-request-available,['Documentation'],ARROW,Improvement,Major,2020-10-07 05:07:43,15
13333943,[C++] Add Future::DeferNotOk(),"Provide a static method mapping Result<Future<T>> -> Future<T>. If the Result is an error, a finished future containing its Status will be constructed.",pull-request-available,['C++'],ARROW,Improvement,Trivial,2020-10-06 14:25:38,6
13333928,[Python] Segfault when converting to fixed size binary array,"Reproducer:
{code:python}
data = [b'\x19h\r\x9e\x00\x00\x00\x00\x01\x9b\x9fA']
assert len(data[0]) == 12
ty = pa.binary(12)
arr = pa.array(data, type=ty)
{code}

Trace:
{code}
pyarrow/tests/test_convert_builtin.py::test_fixed_size_binary_length_check ../src/arrow/array/builder_binary.cc:53:  Check failed: (size) == (byte_width_) Appending wrong size to FixedSizeBinaryBuilder
0   libarrow.200.0.0.dylib              0x000000010e7f9704 _ZN5arrow4util7CerrLog14PrintBackTraceEv + 52
1   libarrow.200.0.0.dylib              0x000000010e7f9622 _ZN5arrow4util7CerrLogD2Ev + 98
2   libarrow.200.0.0.dylib              0x000000010e7f9585 _ZN5arrow4util7CerrLogD1Ev + 21
3   libarrow.200.0.0.dylib              0x000000010e7f95ac _ZN5arrow4util7CerrLogD0Ev + 28
4   libarrow.200.0.0.dylib              0x000000010e7f9492 _ZN5arrow4util8ArrowLogD2Ev + 82
5   libarrow.200.0.0.dylib              0x000000010e7f94c5 _ZN5arrow4util8ArrowLogD1Ev + 21
6   libarrow.200.0.0.dylib              0x000000010e303ec1 _ZN5arrow22FixedSizeBinaryBuilder14CheckValueSizeEx + 209
7   libarrow.200.0.0.dylib              0x000000010e30c361 _ZN5arrow22FixedSizeBinaryBuilder12UnsafeAppendEN6nonstd7sv_lite17basic_string_viewIcNSt3__111char_traitsIcEEEE + 49
8   libarrow_python.200.0.0.dylib       0x000000010b4efa7d _ZN5arrow2py20PyPrimitiveConverterINS_19FixedSizeBinaryTypeEvE6AppendEP7_object + 813
{code}

The input {{const char*}} value gets implicitly casted to string_view which makes the length check fail in debug builds.",pull-request-available,['Python'],ARROW,Bug,Major,2020-10-06 12:56:22,3
13333908,[C++][Python] Segfault when converting nested struct array with dictionary field to pandas series,"Reproducer:

{code:python}
def test_struct_array_with_dictionary_field_to_pandas():
    ty = pa.struct([
        pa.field('dict', pa.dictionary(pa.int64(), pa.int32())),
    ])
    data = [
        {'dict': -1859762450}
    ]
    arr = pa.array(data, type=ty)
    arr.to_pandas()
{code}

Raises SIGSTOP:
{code}
* thread #1, stop reason = signal SIGSTOP
  * frame #0: 0x00007fff6e2b733a libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x00007fff6e373e60 libsystem_pthread.dylib`pthread_kill + 430
    frame #2: 0x00007fff6e1ce93e libsystem_c.dylib`raise + 26
    frame #3: 0x00007fff6e3685fd libsystem_platform.dylib`_sigtramp + 29
    frame #4: 0x000000011517adfd libarrow_python.200.0.0.dylib`arrow::py::ConvertStruct(options=0x00007f84fc5a0230, data=0x00007f84fc59ef18, out_values=0x00007f84fc53d140) at arrow_to_pandas.cc:685:54
    frame #5: 0x000000011514c642 libarrow_python.200.0.0.dylib`arrow::py::ObjectWriterVisitor::Visit(this=0x00007ffee06a1a88, type=0x00007f84fc5a00e8) at arrow_to_pandas.cc:1031:12
    frame #6: 0x00000001151499c4 libarrow_python.200.0.0.dylib`arrow::Status arrow::VisitTypeInline<arrow::py::ObjectWriterVisitor>(type=0x00007f84fc5a00e8, visitor=0x00007ffee06a1a88) at visitor_inline.h:88:5
    frame #7: 0x0000000115149305 libarrow_python.200.0.0.dylib`arrow::py::ObjectWriter::CopyInto(this=0x00007f84fc5a0228, data=std::__1::shared_ptr<arrow::ChunkedArray>::element_type @ 0x00007f84fc59ef18 strong=2 weak=1, rel_placement=0) at arrow_to_pand
as.cc:1055:12
{code}

{code:cpp}
frame #4: 0x000000011517adfd libarrow_python.200.0.0.dylib`arrow::py::ConvertStruct(options=0x00007f84fc5a0230, data=0x00007f84fc59ef18, out_values=0x00007f84fc53d140) at arrow_to_pandas.cc:685:54
   682            if (!arr->field(static_cast<int>(field_idx))->IsNull(i)) {
   683              // Value exists in child array, obtain it
   684              auto array = reinterpret_cast<PyArrayObject*>(fields_data[field_idx].obj());
-> 685              auto ptr = reinterpret_cast<const char*>(PyArray_GETPTR1(array, i));
   686              field_value.reset(PyArray_GETITEM(array, ptr));
   687              RETURN_IF_PYERROR();
   688            } else {
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-10-06 11:29:43,2
13333840,"[Doc] C data interface example for i32 uses `l`, not `i`, in the format","The [specification|https://arrow.apache.org/docs/format/CDataInterface.html#data-type-description-format-strings] uses ""i"" to represent i32, but the [example|https://arrow.apache.org/docs/format/CDataInterface.html#exporting-a-simple-int32-array] uses ""l"" (i64).",pull-request-available,['Documentation'],ARROW,Bug,Major,2020-10-06 05:11:42,9
13333827,[Rust] [DataFusion] Some examples are broken,"The flight server example fails with ""No such file or directory"".

The dataframe example produces an empty result set.

The simple_udaf example produces no output at all.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-10-06 00:22:15,9
13333804,[Rust] Arrow tests fail to compile on Raspberry Pi (32 bit),"Raspberry Pi still tends to use 32-bit operating systems although there is a beta 64 bit version of Raspbian. It would be nice to be able to at least disable these tests when runnign on 32-bit. 
{code:java}
error: literal out of range for `usize`
   --> arrow/src/util/bit_util.rs:421:25
    |
421 |         assert_eq!(ceil(10000000000, 10), 1000000000);
    |                         ^^^^^^^^^^^
    |
    = note: `#[deny(overflowing_literals)]` on by default
    = note: the literal `10000000000` does not fit into the type `usize` whose range is `0..=4294967295`error: literal out of range for `usize`
   --> arrow/src/util/bit_util.rs:422:29
    |
422 |         assert_eq!(ceil(10, 10000000000), 1);
    |                             ^^^^^^^^^^^
    |
    = note: the literal `10000000000` does not fit into the type `usize` whose range is `0..=4294967295`error: literal out of range for `usize`
   --> arrow/src/util/bit_util.rs:423:25
    |
423 |         assert_eq!(ceil(10000000000, 1000000000), 10);
    |                         ^^^^^^^^^^^
    |
    = note: the literal `10000000000` does not fit into the type `usize` whose range is `0..=4294967295`
 {code}",pull-request-available,['Rust'],ARROW,Bug,Major,2020-10-05 21:47:37,10
13333803,[C++][Doc] Update dependency management docs following aws-sdk-cpp addition,"https://arrow.apache.org/docs/developers/cpp/building.html#build-dependency-management needs updating after (esp.) ARROW-10068. aws-sdk-cpp can be ""bundled"" but still has system dependencies that cannot be, for example.",pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Major,2020-10-05 21:44:19,1
13333790,[Rust] Labeler is not labeling,The labeler is not doing its job and erroring. There is a bug on its declaration.,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-10-05 20:08:10,9
13333762,[CI] Nightly valgrind job fails,https://github.com/ursa-labs/crossbow/runs/1204693039,pull-request-available,"['C++', 'CI']",ARROW,Bug,Major,2020-10-05 16:32:59,6
13333761,[CI] Nightly hdfs integration test job fails,"Two tests fail:
https://github.com/ursa-labs/crossbow/runs/1204680589

[removed bogus investigation]",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2020-10-05 16:24:59,5
13330934,[Rust] [DataFusion] Add `ExecutionContext::from<ExecutionContextState>`,"An {{ExecutionContext}} can be safely created from an {{ExecutionContextState}} . Therefore, we can make it public (instead of the current {{pub(crate)}}).",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-10-04 05:44:46,9
13330673,"[Rust] Nulls should be rendered as """" rather than default value when pretty printing arrays","
Null values should be printed as """" when pretty printing. However, as of now, null values in primative arrays  are rendered as the type's default value .

For example:
{code}
    fn test_pretty_format_batches() -> Result<()> {
        // define a schema.
        let schema = Arc::new(Schema::new(vec![
            Field::new(""a"", DataType::Utf8, true),
            Field::new(""b"", DataType::Int32, true),
        ]));

        // define data.
        let batch = RecordBatch::try_new(
            schema,
            vec![
                Arc::new(array::StringArray::from(vec![Some(""a""), Some(""b""), None, Some(""d"")])),
                Arc::new(array::Int32Array::from(vec![Some(1), None, Some(10), Some(100)])),
            ],
        )?;

        println!(pretty_format_batches(&[batch])?);

        Ok(())
    }
{code}

Outputs:

{code}
+---+-----+
| a | b   |
+---+-----+
| a | 1   |
| b | 0   |
|   | 10  |
| d | 100 |
+---+-----+
{code}

The second row of b should be '', not 0. The third row of a should also be '', which I think t is by accident


Thanks to [~jhorstmann] horstmann for pointing this out on https://github.com/apache/arrow/pull/8331#issuecomment-702964608
",pull-request-available,['Rust'],ARROW,Bug,Minor,2020-10-03 10:33:48,11
13330628,[Rust] Support display of DictionaryArrays in sql.rs,"When I try to display a DictionaryArray values, I get either a ??? in sql.rs

This ticket tracks adding proper support for printing these types
",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-10-02 20:12:06,11
13330616,[Rust] [DataFusion] Allow DataFusion to cast all type combinations supported by Arrow cast kernel,"When the  DataFusion planner inserts casts, today it relies on special logic to determine the valid coded casts. 

The actual arrow cast kernels support a much wider range of data types, and thus DataFusion is artificially limiting the casts it supports for no particularly good reason I can see.

This ticket tracks the work to remove the extra casting checking in the datafusion planner and instead simply rely on runtime check of arrow cast compute kernel

The potential  downside of this approach is that the error may be generated later in the execution process (rather than the planner), and possibly have a less specific error message, the upside is there is less code and we get several conversions immediately (like timestamp predicate casting)

I also plan to add DictionaryArray support to the casting kernels and I would like to avoid having to replicate some part of that logic in DataFusion

",pull-request-available,[],ARROW,Sub-task,Minor,2020-10-02 18:10:09,11
13330614,[Rust] Add support for DictionaryArray types to cast kernels,"This ticket tracks the work to support casting to/from DictionaryArray's, (my usecase is DictionaryArray's with a Utf8 dictionary). 

There is prototype work on https://github.com/alamb/arrow/tree/alamb/datafusion-string-dictionary",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-10-02 18:05:17,11
13330613,[Rust] [DataFusion] Add DictionaryArray coercion support,"

--- 


There is code in the datafusion physical planner that coerces arguments to compatible types for some expressions (e.g. for equals: https://github.com/apache/arrow/blob/master/rust/datafusion/src/physical_plan/expressions.rs#L1153)

This code needs to be modified to understand dictionary types (so, for example we can express a predicate like col1 = ""foo"", where col1 is a DictionaryArray.

",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-10-02 18:02:49,11
13330611,[Rust] Support display of DictionaryArrays in pretty printing,"When I try to display a DictionaryArray values, I get 'Unsupported {:?} type for repl."" error in rust/arrow/src/util/pretty.rs

This ticket tracks adding proper support for printing these types
",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-10-02 17:54:26,11
13330578,[Rust] [DataFusion] Simplify expression tests,"There is some code duplication in the tests of datafusion expressions.rs.

Lets try to DRY them.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-10-02 15:10:03,9
13330547,[Rust] Improve documentation of DictionaryType,"Improve documentation about DictionaryType (clarify which type is which)
",pull-request-available,['Rust'],ARROW,Sub-task,Trivial,2020-10-02 11:52:43,11
13330541,[Rust][DataFusion] Add support for Dictionary types in data fusion,"We have a system that need to process low cardinality string data (aka there are only a few distinct values, but there are many millions of values).

Using a `StringArray` is very expensive as the same string value is copied over and over again. The `DictionaryArray` was exactly designed to handle this situatio:  rather than repeating each string, it uses indexes into a dictionary and thus repeats integer values. 

Sadly, DataFusion does not support processing on `DictionaryArray` types for several reasons.

This test (to be added to `arrow/rust/datafusion/tests/sql.rs`) shows what I would like to be possible:

{code}

#[tokio::test]
async fn query_on_string_dictionary() -> Result<()> {
    // ensure that data fusion can operate on dictionary types
    // Use StringDictionary (32 bit indexes = keys)
    let field_type = DataType::Dictionary(
        Box::new(DataType::Int32),
        Box::new(DataType::Utf8),
    );
    let schema = Arc::new(Schema::new(vec![Field::new(""d1"", field_type, true)]));


    let keys_builder = PrimitiveBuilder::<Int32Type>::new(10);
    let values_builder = StringBuilder::new(10);
    let mut builder = StringDictionaryBuilder::new(
        keys_builder, values_builder
    );

    builder.append(""one"")?;
    builder.append_null()?;
    builder.append(""three"")?;
    let array = Arc::new(builder.finish());

    let data = RecordBatch::try_new(
        schema.clone(),
        vec![array],
    )?;

    let table = MemTable::new(schema, vec![vec![data]])?;
    let mut ctx = ExecutionContext::new();
    ctx.register_table(""test"", Box::new(table));


    // Basic SELECT
    let sql = ""SELECT * FROM test"";
    let actual = execute(&mut ctx, sql).await.join(""\n"");
    let expected = ""\""one\""\nNULL\n\""three\"""".to_string();
    assert_eq!(expected, actual);

    // basic filtering
    let sql = ""SELECT * FROM test WHERE d1 IS NOT NULL"";
    let actual = execute(&mut ctx, sql).await.join(""\n"");
    let expected = ""\""one\""\n\""three\"""".to_string();
    assert_eq!(expected, actual);

    // filtering with constant
    let sql = ""SELECT * FROM test WHERE d1 = 'three'"";
    let actual = execute(&mut ctx, sql).await.join(""\n"");
    let expected = ""\""three\"""".to_string();
    assert_eq!(expected, actual);

    // Expression evaluation
    let sql = ""SELECT concat(d1, '-foo') FROM test"";
    let actual = execute(&mut ctx, sql).await.join(""\n"");
    let expected = ""\""one-foo\""\nNULL\n\""three-foo\"""".to_string();
    assert_eq!(expected, actual);

    // aggregation
    let sql = ""SELECT COUNT(d1) FROM test"";
    let actual = execute(&mut ctx, sql).await.join(""\n"");
    let expected = ""2"".to_string();
    assert_eq!(expected, actual);


    Ok(())
}
{code}

However, it errors immediately:

{code}

---- query_on_string_dictionary stdout ----
thread 'query_on_string_dictionary' panicked at 'assertion failed: `(left == right)`
  left: `""\""one\""\nNULL\n\""three\""""`,
 right: `""???\nNULL\n???""`', datafusion/tests/sql.rs:989:5
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace

{code{

This ticket tracks adding proper support Dictionary types to DataFusion. I will break the work down into several smaller subtasks",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-10-02 11:43:53,11
13330534,[Rust] Add more documentation about take,It would be nice to have a little more documentation of what the take kernel did.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Trivial,2020-10-02 11:04:43,11
13330501,[Rust] Auto-label PRs,"Auto-label PRs on github so that we do not have to label them ourselves.

The decision should be based on the code that changed, and not only on the title.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-10-02 07:05:02,9
13330324,[Rust] Add support to external release of un-owned buffers,"Currently we can't release externally owned resources (for FFI).

The goal of this issue is to support such a release.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-10-01 10:10:18,9
13330232,[Rust] Add documentation to lib.rs,"Currently, the crate page looks rather empty.

This issue aims to move the documentation from the README to the crate, so that it has a broader audience and follows Rust best practices.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-30 18:29:53,9
13330205,[Python] Parquet metadata to_dict raises attribute error,"h2. Description

When accessing rowgroup metadata and trying to convert it to a dict using the method {{to_dict}} I sometimes receive an Attribute error.

This can be consistently produced with an empty dataframe (see example below) but I have also seen it already for non empty dataframes. I couldn't track down what makes the non-empty Attribute errors special, therefore the example below.
h2. Expected behaviour

I would expect the to_dict to always, consistently return a dictionary with the appropriate metadata and statistics irregardless of the file content.
h2. Minimal Example


{code:python}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

df = pd.DataFrame({""col"": [1]}).head(0)
table = pa.Table.from_pandas(df)
buf = pa.BufferOutputStream()
pq.write_table(table, buf)
reader = pa.BufferReader(buf.getvalue())
parquet_file = pq.ParquetFile(reader)
# Raises Attribute Error
parquet_file.metadata.to_dict()
{code}
h3. Traceback
{code:java}
~/miniconda3/envs/kartothek-dev/lib/python3.7/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.FileMetaData.to_dict()

~/miniconda3/envs/kartothek-dev/lib/python3.7/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.RowGroupMetaData.to_dict()

~/miniconda3/envs/kartothek-dev/lib/python3.7/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.ColumnChunkMetaData.to_dict()

AttributeError: 'NoneType' object has no attribute 'to_dict'
{code}
h3. Versions
{code:java}
In [28]: pa.__version__
Out[28]: '1.0.1'

In [29]: pd.__version__
Out[29]: '1.0.5'
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2020-09-30 16:13:40,5
13330201,[C++][Dataset] Assert integer overflow in partitioning falls back to string,"From https://stackoverflow.com/questions/64137664/how-to-override-type-inference-for-partition-columns-in-hive-partitioned-dataset

Small reproducer:

{code}
import pyarrow as pa
import pyarrow.parquet as pq

table = pa.table({'part': [3760212050]*10, 'col': range(10)})
pq.write_to_dataset(table, ""test_int64_partition"", partition_cols=['part'])

In [35]: pq.read_table(""test_int64_partition/"")
...
ArrowInvalid: error parsing '3760212050' as scalar of type int32
In ../src/arrow/scalar.cc, line 333, code: VisitTypeInline(*type_, this)
In ../src/arrow/dataset/partition.cc, line 218, code: (_error_or_value26).status()
In ../src/arrow/dataset/partition.cc, line 229, code: (_error_or_value27).status()
In ../src/arrow/dataset/discovery.cc, line 256, code: (_error_or_value17).status()

In [36]: pq.read_table(""test_int64_partition/"", use_legacy_dataset=True)
Out[36]: 
pyarrow.Table
col: int64
part: dictionary<values=int64, indices=int32, ordered=0>
{code}",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-09-30 15:58:22,6
13330181,[C++] ArrayRangeEquals should accept EqualOptions,"Besides, the underlying implementations of ArrayEquals and ArrayRangeEquals should be shared (right now they are duplicated).",pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-30 14:10:36,2
13330049,[C++] Add support for building arrow_testing without building tests,"{{ARROW_BUILD_TESTS}} installs the following arrow_testing related files implicitly:

{noformat}
lib/cmake/arrow/ArrowTestingConfig.cmake
lib/cmake/arrow/ArrowTestingConfigVersion.cmake
lib/cmake/arrow/ArrowTestingTargets-%%CMAKE_BUILD_TYPE%%.cmake
lib/cmake/arrow/ArrowTestingTargets.cmake
lib/cmake/arrow/FindArrowTesting.cmake
lib/libarrow_testing.so
lib/libarrow_testing.so.100
lib/libarrow_testing.so.100.1.0
libdata/pkgconfig/arrow-testing.pc
{noformat}

If we have {{ARROW_TESTING}} or something, users can do it explicitly.

The original GitHub bug report: [https://github.com/apache/arrow/issues/8306]",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-09-29 22:22:55,1
13330035,[R] Fix cpp helper that breaks if libarrow is not present,https://github.com/apache/arrow/pull/8246/commits/87c26f7a7e5a393681057923a790f6448ea07817 breaks if libarrow is not found/not successfully built. Anything that requires an arrow header or an {{arrow::}} class needs to be wrapped in {{#if defined(ARROW_R_WITH_ARROW)}},pull-request-available,['R'],ARROW,Bug,Major,2020-09-29 20:23:37,6
13329966,"[Rust][Arrow] Nulls are transformed into """" after filtering for StringArray","
When I use the filter kernel with Null strings, any input column that was Null turns into an empty string after filtering.

For example, this test (in filter.rs) should pass:

{code}

    #[test]
    fn test_filter_string_array_with_null() {
        let a = StringArray::from(vec![Some(""hello""), None, Some(""world""), None]);
        let b = BooleanArray::from(vec![true, false, false, true]);
        let c = filter(&a, &b).unwrap();
        let d = c.as_ref().as_any().downcast_ref::<StringArray>().unwrap();
        assert_eq!(2, d.len());
        assert_eq!(""hello"", d.value(0));
        assert_eq!(true, d.is_null(1));
    }
{code}

But instead it fails (the second element in the output array should be null )

{code}
---- compute::kernels::filter::tests::test_filter_string_array_with_null stdout ----
thread 'compute::kernels::filter::tests::test_filter_string_array_with_null' panicked at 'assertion failed: `(left == right)`
  left: `true`,
 right: `false`', arrow/src/compute/kernels/filter.rs:686:9
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
{code}
",pull-request-available,['Rust'],ARROW,Bug,Major,2020-09-29 15:50:15,11
13329948,[C++][Dataset] Add ParquetFileFragment::num_row_groups property,"From https://github.com/dask/dask/pull/6534#issuecomment-699512602, comment by [~rjzamora]:

bq.  it would be great to have access the total row-group count for the fragment from a {{num_row_groups}} attribute (which pyarrow should be able to get without parsing all row-group metadata/statistics - I think?).

One question is: does this attribute correspond to the row groups in the parquet file, or the (subset of) row groups represented by the fragment? 
I expect the second (so if you do SplitByRowGroup, you would get a fragment with num_row_groups==1), but this might be a potential confusing aspect of the attribute.",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-29 13:49:22,6
13329930,[C++][Dataset] Lazily parse parquet metadata / statistics in ParquetDatasetFactory and ParquetFileFragment,"Related to ARROW-9730, parsing of the statistics in parquet metadata is expensive, and therefore should be avoided when possible.

For example, the {{ParquetDatasetFactory}} ({{ds.parquet_dataset()}} in python) parses all statistics of all files and all columns. While when doing a filtered read, you might only need the statistics of certain files (eg if a filter on a partition field already excluded many files) and certain columns (eg only the columns on which you are actually filtering).

The current API is a bit all-or-nothing (both ParquetDatasetFactory, or a later EnsureCompleteMetadata parse all statistics, and don't allow parsing a subset, or only parsing the other (non-statistics) metadata, ...), so I think we should try to think of better abstractions.

cc [~rjzamora] [~bkietz]",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-29 12:32:47,6
13329922,"[C++][Dataset] ParquetFileFragment::SplitByRowGroup does not preserve ""complete_metadata"" status","Splitting a ParquetFileFragment in  multiple fragments per row group ({{SplitByRowGroup}}) calls {{EnsureCompleteMetadata}} initially, but then the created fragments afterwards don't have the {{has_complete_metadata_}} property set. This means that when calling {{EnsureCompleteMetadata}} on the splitted fragments, it will read/parse the metadata again, instead of using the cached ones (which are already present).

Small example to illustrate:

{code:python}
In [1]: import pyarrow.dataset as ds

In [2]: dataset = ds.parquet_dataset(""nyc-taxi-data/dask-partitioned/_metadata"", partitioning=""hive"")

In [3]: rg_fragments = [rg for frag in dataset.get_fragments() for rg in frag.split_by_row_group()]

In [4]: len(rg_fragments)
Out[4]: 4520

# row group fragments actually have statistics
In [7]: rg_fragments[0].row_groups[0].statistics
Out[7]: 
{'vendor_id': {'min': '1', 'max': '4'},
 'pickup_at': {'min': datetime.datetime(2009, 1, 1, 0, 5, 51),
  'max': datetime.datetime(2018, 12, 26, 14, 48, 54)},
...

# but calling ensure_complete_metadata still takes a lot of time the first call
In [8]: %time _ = [fr.ensure_complete_metadata() for fr in rg_fragments]
CPU times: user 1.72 s, sys: 203 ms, total: 1.92 s
Wall time: 1.9 s

In [9]: %time _ = [fr.ensure_complete_metadata() for fr in rg_fragments]
CPU times: user 1.34 ms, sys: 0 ns, total: 1.34 ms
Wall time: 1.35 ms
{code}",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Bug,Major,2020-09-29 11:49:08,5
13329835,[Format] Update specification to support 256-bit Decimal types,This will require a vote to approve merging.,pull-request-available,['Format'],ARROW,Improvement,Major,2020-09-29 04:19:39,15
13329812,[R] Int64 downcast check doesn't consider all chunks,"I've got a proprietary dataset where one of the columns is an integer64 but all of the values would fit within 32bits. As I understand it, arrow/feather will downcast that column when the data is read back into R (not ideal IMO, but not an issue generally). However, I'm having some trouble with a specific dataset.

When I read in the data, the column is set to the class ""integer64"", however the column type (typeof) is 'integer' and not 'double', which is the underlying type used by bit64. This mismatch causes R data.table to error out ([https://github.com/Rdatatable/data.table/blob/master/src/rbindlist.c#L325)]

I do not have any issue with integer64 columns which have values > 2^32, and suspiciously I am also unable to recreate the issue by manually creating a data.table with an int64 column with small values (e.g data.table(col=as.integer64(c(1,2,3))) )

I did look thru the arrow::r cpp source and couldnt find an obvious case where the underlying storage array would be an integer but also have the 'integer64' class attr assigned... A fix would either be to remove the integer64 class attr, or ensure that the underlying data store is a REALSXP instead of INTEGERSXP

My company's network policies wont let me upload the sample dataset, hoping to see if this triggers an immediate thoughts. If not, I can try to figure our how to upload the dataset or otherwise provide details from it as requested.


{code:java}
> arrow::write_feather(df[,list(testCol)][1], '~/test.feather')
> test = arrow::read_feather('~/test.feather')
> class(test$testCol)
[1] ""integer64"" ""np.ulong""
> typeof(test$testCol)
[1] ""integer""

> str(test)
Classes tbl_df, tbl and 'data.frame':       1 obs. of  1 variable: $ testCol:Error in as.character.integer64(object) :  REAL() can only be applied to a 'numeric', not a 'integer'


#In the larger original dataset, it handles most columns properly, only the 'testCol' breaks things.  Note the difference:
> typeof(df$goodCol)
[1] ""double""
> class(df$goodCol)
[1] ""integer64"" ""np.ulong""

> typeof(df$testCol)
[1] ""integer""
> class(df$testCol)
[1] ""integer64"" ""np.ulong""

> str(df)
Classes data.table and 'data.frame':  214781 obs. of  17 variables: 
$ goodCol        :integer64 1599777000000604025 ... 
$ testCol        :Error in as.character.integer64(object) :

> sessionInfo()
R version 3.6.1 (2019-07-05)Platform: x86_64-pc-linux-gnu (64-bit)Running under: Red Hat Enterprise Linux Server 7.7 (Maipo)
Matrix products: defaultBLAS:   /usr/lib64/libblas.so.3.4.2LAPACK: /usr/lib64/liblapack.so.3.4.2locale: 

[1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C [9] LC_ADDRESS=C               LC_TELEPHONE=C[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:[1] stats     graphics  grDevices utils     datasets  methods   baseother attached packages:[1] data.table_1.13.0 bit64_4.0.5       bit_4.0.4loaded via a namespace (and not attached): [1] Rcpp_1.0.5           lattice_0.20-41      arrow_1.0.1 [4] assertthat_0.2.1     rappdirs_0.3.1       grid_3.6.1 [7] R6_2.4.1             jsonlite_1.7.1       magrittr_1.5[10] rlang_0.4.7          Matrix_1.2-18        vctrs_0.3.4[13] reticulate_1.14-9001 tools_3.6.1          glue_1.4.2[16] purrr_0.3.4          compiler_3.6.1       tidyselect_1.1.0{code}",pull-request-available,['R'],ARROW,Bug,Critical,2020-09-28 23:04:46,4
13329799,[R] Write functions don't follow umask setting,"



The write_feather and write_parquet functions don't seem to follow the R umask settings. Please let me know if you need any other information from me to replicate or document this issue. Thanks!

Code to replicate bug:


{code:java}
library(arrow)
Sys.umask(mode = ""2"")
Sys.umask()

arrow::write_feather(x = cars, sink = ""~/test.feather"")
arrow::write_parquet(x = cars, sink = ""~/test.parquet"")
write.csv(x = cars, file = ""~/test.csv"")

system(""ls -l ~/test.feather"")
system(""ls -l ~/test.parquet"")
system(""ls -l ~/test.csv"")

sessionInfo()
{code}
Full output:




{code:java}
> library(arrow)

Attaching package: 'arrow'

The following object is masked from 'package:utils':

    timestamp

Warning message:
package 'arrow' was built under R version 4.0.2 
> 
> Sys.umask(mode = ""2"")
> Sys.umask()
[1] ""2""
> 
> arrow::write_feather(x = cars, sink = ""~/test.feather"")
> arrow::write_parquet(x = cars, sink = ""~/test.parquet"")
> write.csv(x = cars, file = ""~/test.csv"")
> 
> system(""ls -l ~/test.feather"")
-rw-r--r--  1 chacalle  staff  994 Sep 28 12:14 /Users/chacalle/test.feather
> system(""ls -l ~/test.parquet"")
-rw-r--r--  1 chacalle  staff  1243 Sep 28 12:14 /Users/chacalle/test.parquet
> system(""ls -l ~/test.csv"")
-rw-rw-r--  1 chacalle  staff  552 Sep 28 12:14 /Users/chacalle/test.csv
> 
> sessionInfo()
R version 4.0.0 (2020-04-24)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Catalina 10.15.6

Matrix products: default
BLAS:   /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] arrow_1.0.1

loaded via a namespace (and not attached):
 [1] tidyselect_1.1.0 bit_4.0.4        compiler_4.0.0   magrittr_1.5     assertthat_0.2.1
 [6] R6_2.4.1         tools_4.0.0      glue_1.4.2       Rcpp_1.0.5       bit64_4.0.5     
[11] vctrs_0.3.4      rlang_0.4.7      purrr_0.3.4     
{code}",pull-request-available,"['C++', 'R']",ARROW,Bug,Major,2020-09-28 19:30:37,2
13329792,[Python] Selecting one column of multi-index results in a duplicated value column.,"When I read one column of a multi-index, that column is duplicated as a value column in the resulting Pandas data frame.
{code:python}
>> tbl = pa.table({""first"": list(range(5)), ""second"": list(range(5)), ""value"": np.arange(5)}) 
>>> df = table.to_pandas().set_index([""first"", ""second""])
>>> print(df)
              value
first second
0     0           0
1     1           1
2     2           2
3     3           3
4     4           4
>>> pq.write_table(pa.Table.from_pandas(df), ""/tmp/test.parquet"")
>>> data = ds.dataset(""/tmp/test.parquet"")
{code}
This works as expected, as does selecting all or no columns.
{code:python}
>>> print(data.to_table(columns=[""first"", ""second"", ""value""]).to_pandas())
              value
first second
0     0           0
1     1           1
2     2           2
3     3           3
4     4           4
{code}
This does not work as expected, as the {{first}} column is both an index and a value.
{code:python}
>>> print(data.to_table(columns=[""first"", ""value""]).to_pandas())
       first  value
first
0          0      0
1          1      1
2          2      2
3          3      3
4          4      4{code}
This is easy to workaround by specifying the full multi-index in {{to_table}}, but does this behavior make sense?",pull-request-available,['Python'],ARROW,Bug,Minor,2020-09-28 18:33:40,5
13329785,[C++][Python] Variable dictionaries do not survive roundtrip to IPC stream,"Failing test case (from dev@ https://lists.apache.org/thread.html/r338942b4e9f9316b48e87aab41ac49c7ffedd45733d4a6349523b7eb%40%3Cdev.arrow.apache.org%3E)

{code}
import pyarrow as pa
from io import BytesIO

pa.__version__

schema = pa.schema([pa.field('foo', pa.int32()), pa.field('bar', pa.dictionary(pa.int32(), pa.string()))] )
r1 = pa.record_batch(
    [
        [1, 2, 3, 4, 5],
        pa.array([""a"", ""b"", ""c"", ""d"", ""e""]).dictionary_encode()
    ],
    schema
)

r1.validate()
r2 = pa.record_batch(
    [
        [1, 2, 3, 4, 5],
        pa.array([""c"", ""c"", ""e"", ""f"", ""g""]).dictionary_encode()
    ],
    schema
)

r2.validate()

assert r1.column(1).dictionary != r2.column(1).dictionary


sink =  pa.BufferOutputStream()
writer = pa.RecordBatchStreamWriter(sink, schema)

writer.write(r1)
writer.write(r2)

serialized = BytesIO(sink.getvalue().to_pybytes())
stream = pa.ipc.open_stream(serialized)

deserialized = []

while True:
    try:
        deserialized.append(stream.read_next_batch())
    except StopIteration:
        break

assert deserialized[1][1].to_pylist() == r2[1].to_pylist()
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Blocker,2020-09-28 17:26:29,2
13329778,[C++][Parquet] Create reading benchmarks for 2-level nested data,"We already have benchmarks for reading one-level list and one-level struct. It would be nice to add list-of-list, list-of-struct, struct-of-struct.",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-09-28 16:14:30,2
13329748,[Python][Packaging] Fix gRPC linking error in macOS wheels builds,"The nightly builds are consistently failing for about two weeks now:
https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-09-28-0-travis-wheel-osx-cp36m

Fall back to bundled grpc build.",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-09-28 13:18:51,3
13329739,[C++] CSV empty quoted string is treated as NULL,"When parsing my CSV I have set
{color:#267f99}ConvertOptions{color}::s{color:#001080}trings_can_be_null{color} to true.

Now as I have values:
{code:java}
1234,"""",345
{code}
the string value which is an empty string is treated as NULL.
I've checked default valeus of {color:#267f99}ConvertOptions{color}::{color:#001080}null_values{color} and there is empty string considered null, but here we have empty quoted string which shouldn't be treated as NULL in my opinion. Similar behavior we have in Postgresql, empty quoted string is not treated as a NULL: https://www.postgresql.org/docs/12/sql-copy.html",pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-28 12:18:40,2
13329630,[Rust] Add support to consume C Data Interface,"The goal of this issue is to support consuming C Data arrays from Rust using FFI.

The use-case that motivated this issue was the possibility of running DataFusion from Python and support moving arrays from DataFusion to Python/Pyarray and vice-versa.

In particular, so that users can write Python UDFs that expect arrow arrays and return arrow arrays, in the same spirit as pandas-udfs in Spark work for Pandas.

The brute-force way of writing these arrays is by converting element by element from and to native types. The efficient way of doing it to pass the memory address from and to each implementation, which is zero-copy.

To support the latter, we need an FFI implementation in Rust that produces and consumes [C's Data interface|https://arrow.apache.org/docs/format/CDataInterface.html]",pull-request-available,['Rust'],ARROW,New Feature,Major,2020-09-27 14:26:58,9
13329629,[Rust] Add support to produce a C Data interface,"The goal of this issue is to support producing C Data arrays of Rust.

The use-case that motivated this issue was the possibility of running DataFusion from Python and support moving arrays from DataFusion to Python/Pyarray and vice-versa.

In particular, so that users can write Python UDFs that expect arrow arrays and return arrow arrays, in the same spirit as pandas-udfs in Spark work for Pandas.

The brute-force way of writing these arrays is by converting element by element from and to native types. The efficient way of doing it to pass the memory address from and to each implementation, which is zero-copy.

To support the latter, we need an FFI implementation in Rust that produces and consumes [C's Data interface|https://arrow.apache.org/docs/format/CDataInterface.html]",pull-request-available,['Rust'],ARROW,New Feature,Major,2020-09-27 14:26:06,9
13329515,[Python]Separate tests into its own conda package,"We currently ship the tests with the source code. This is nice to test theintegrity of the installation, it is not needed for runtime though. In the case of conda, the overhead to turn them into a separate package is small.",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-09-26 06:39:32,8
13329509,[Rust] Add a Contains kernel,Add a `contains` function that checks whether a list array contains a primitive value. The result of the function is a boolean array,pull-request-available,['Rust'],ARROW,New Feature,Major,2020-09-26 01:10:05,12
13329482,[C++][Dataset] Ability to read/subset a ParquetFileFragment with given set of row group ids,"From discussion at https://github.com/dask/dask/pull/6534#issuecomment-698723009 (dask using the dataset API in their parquet reader), it might be useful to somehow ""subset"" or read a subset of a ParquetFileFragment for a specific set of row group ids.

Use cases:

* Read only a set of row groups ids (this is similar as {{ParquetFile.read_row_groups}}), eg because you want to control the size of the resulting table by reading subsets of row groups
* Get a ParquetFileFragment with a subset of row groups (eg based on a filter) to then eg get the statistics of only those row groups

The first case could for example be solved by adding a {{row_groups}} keyword to {{ParquetFileFragment.to_table}} (but, this is then a keyword specific to the parquet format, and we should then probably also add it to {{scan}} et al).

The second case is something you can in principle do yourself manually by recreating a fragment with {{fragment.format.make_fragment(fragment.path, ..., row_groups=[...])}}. However, this is a) a bit cumbersome and b) statistics might need to be parsed again?  
The statistics of a set of filtered row groups could also be obtained by using {{split_by_row_group(filter)}} (and then get the statistics of each of the fragments), but if you then want a single fragment, you need to recreate a fragment with the obtained row group ids.

So one idea I have now (but mostly brainstorming here). Would it be useful to have a method to create a ""subsetted"" ParquetFileFragment, either based on a list of row group ids ({{fragment.subset(row_groups=[...])}} or either based on a filter ({{fragment.subset(filter=...)}}, which would be equivalent as split_by_row_group+recombining into a single fragment) ?

cc [~bkietz] [~rjzamora]

",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-25 19:51:53,5
13329480,[C++][Dataset] Also allow integer partition fields to be dictionary encoded,"In ARROW-8647, we added the option to indicate that you partition field columns should be dictionary encoded, but it currently does only do this for string type, and not for integer type (wiht the reasoning that for integers, it is not giving any memory efficiency gains to use dictionary encoding). 

In dask, they have been using categorical dtypes for _all_ partition fields, also if they are integers. They would like to keep doing this (apart from memory efficiency, using categorical/dictionary type also gives information about all uniques values of the column, without having to calculate this), so it would be nice to enable this use case. 

So I think we could either simply always dictionary encode also integers when {{max_partition_dictionary_size}} indicates partition fields should be dictionary encoded, or either have an additional option to indicate also integer partition fields should be encoded (if the other option indicates dictionary encoding should be used).

Based on feedback from the dask PR using the dataset API at https://github.com/dask/dask/pull/6534#issuecomment-698723009

cc [~rjzamora] [~bkietz]",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-25 19:40:45,6
13329479,[R][Doc] Fix copy_files doc mismatch,Followup to ARROW-10003,pull-request-available,['R'],ARROW,Bug,Major,2020-09-25 19:38:06,4
13329477,[C++] Persist SetLookupState in between usages of IsIn when filtering dataset batches,"Building a large hash table has a non-trivial cost. 

See mailing list discussion

https://lists.apache.org/thread.html/rb85519cc21ffb09a836a9107919e07b076165ff81c22fb88b59a8296%40%3Cuser.arrow.apache.org%3E",dataset,['C++'],ARROW,Improvement,Major,2020-09-25 19:34:50,6
13329438,[Rust] [DataFusion] Remove unused code,"The CsvBatchIterator on datafusion::datasources::csv is unused, and seems to be duplicated code from physical_plan::csv.",pull-request-available,['Rust - DataFusion'],ARROW,Task,Minor,2020-09-25 16:14:11,9
13329408,[Dev][Go] Add grpc generated go files to rat exclusion list,The RAT check is failing https://github.com/apache/arrow/runs/1161033600,pull-request-available,"['Developer Tools', 'Go']",ARROW,Task,Major,2020-09-25 13:31:24,3
13329343,"[R] inject base class for Array, ChunkedArray and Scalar","Related to comment [https://github.com/apache/arrow/pull/8256#discussion_r494402171]

There's a bunch of these :

#' @export
sum.Array <- function(..., na.rm = FALSE) scalar_aggregate(""sum"", ..., na.rm = na.rm)

#' @export
sum.ChunkedArray <- sum.Array

#' @export
sum.Scalar <- sum.Array

and having a empty base class between these and ArrowObject would help, we would only have one function and let S3 do its thing",pull-request-available,['R'],ARROW,Improvement,Minor,2020-09-25 06:51:43,4
13329299,[CI] Fix nightly docs job,"At least one issue is that R's pkgdown package now requires the systemfonts package, which has system dependencies that are not met: https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=17388&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181&l=7431",pull-request-available,"['Continuous Integration', 'Documentation']",ARROW,Bug,Major,2020-09-24 23:04:10,4
13329275,[Rust] Migrate min_large_string -> min_string kernels,"Currently, the kernels are named 

`min_string`, `max_string`, `max_large_string`, `min_large_string`, but this is no longer needed, as strings are now the same generic struct",pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-09-24 19:44:20,9
13329258,[C++] S3 tests fail on AppVeyor,"See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/35387750

{code}
[----------] 3 tests from S3RegionResolutionTest
[ RUN      ] S3RegionResolutionTest.PublicBucket
C:/projects/arrow/cpp/src/arrow/filesystem/s3fs_test.cc(273): error: Failed
'_error_or_value55.status()' failed with IOError: When resolving region for bucket 'ursa-labs-taxi-data': AWS Error [code 99]: Encountered network error when sending http request
[  FAILED  ] S3RegionResolutionTest.PublicBucket (6946 ms)
[ RUN      ] S3RegionResolutionTest.RestrictedBucket
C:/projects/arrow/cpp/src/arrow/filesystem/s3fs_test.cc(283): error: Failed
'_error_or_value58.status()' failed with IOError: When resolving region for bucket 'ursa-labs-r-test': AWS Error [code 99]: Encountered network error when sending http request
[  FAILED  ] S3RegionResolutionTest.RestrictedBucket (7037 ms)
[ RUN      ] S3RegionResolutionTest.NonExistentBucket
[       OK ] S3RegionResolutionTest.NonExistentBucket (178 ms)
[----------] 3 tests from S3RegionResolutionTest (14161 ms total)
{code}

Traces with S3 logging enabled can be found here: https://ci.appveyor.com/project/pitrou/arrow/builds/35387461?fullLog=true",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2020-09-24 16:34:52,2
13329221,[C++] Improve Parquet fuzz seed corpus,It would be nice to add more nested data to our Parquet seed corpus.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Wish,Minor,2020-09-24 14:16:10,2
13329177,[R] Arrow does not release unused memory,"Im having problems when {{collect()}}-ing Arrow data sources into data frames that are close in size to the available memory on the machine. Consider the following workflow. I have a dataset which I want to query so that at some point in needs to be {{collect()}}-ed but at the same Im also reducing the result. During the intermediate step the entire data frame fits into memory, and the following code runs without any problems.
{code:r}
test_ds <- ""memory_test""

ds1 <- open_dataset(test_ds) %>%
  collect() %>%
  dim()
{code}
However, running the same code in the same R session again fails with R running out of memory.
{code:r}
ds1 <- open_dataset(test_ds) %>%
  collect() %>%
  dim()
{code}
The example might be a but contrived but you can easily imagine a workflow where different queries are ran on a dataset and the reduced results are stored.

As far as I understand, R is a garbage collected language, and in this case there arent any references left to large objects in memory. And indeed, the second query succeeds when manually forcing a garbage collection.

Is this the expected behaviour from Arrow?

I know, this is quite hard to reproduce, as the exact dataset size required to trigger this behaviour depends on the particular machine but I prepared a reproducible example in [this gist|https://gist.github.com/svraka/c63fca51c6cc50020551e2319ff652b7], that should give the same result on Ubuntu 20.04 with 1GB RAM and no swap. See attachment for {{sessionInfo()}} output. I ran it on a Digitalocean {{s-1vcpu-1gb}} droplet.

First, lets create a a partitioned Arrow dataset:
{code:java}
$ Rscript ds_prep.R 1000000 5
{code}
The first command line argument gives the number of rows in each partition, and second gives the number of partitions. The parameters are set so that the entire dataset should fit into memory.

Then running the two queries fails:
{code:java}
$ Rscript ds_read.R
Running query, 1st try...
ds size, 1st run: 56
Running query, 2nd try...
[1]    11151 killed     Rscript ds_read.R
{code}
However, when forcing a {{gc()}} (which Im controlling here with a command line argument), it succeeds:
{code:java}
$ Rscript ds_read.R 1
Running query, 1st try...
ds size, 1st run: 56
running gc() ...
          used (Mb) gc trigger  (Mb) max used  (Mb)
Ncells  703052 37.6    1571691  84.0  1038494  55.5
Vcells 1179578  9.0   36405636 277.8 41188956 314.3
Running query, 2nd try...
ds size, 2nd run: 56
{code}
In general, [one shouldnt have to use {{gc()}} manually|https://adv-r.hadley.nz/names-values.html#gc]. Interestingly, setting Rs garbage collection more aggressive (see {{?Memory}}) doesnt help either:
{code:java}
$ R_GC_MEM_GROW=0 Rscript ds_read.R
Running query, 1st try...
ds size, 1st run: 56
Running query, 2nd try...
[1]    11422 killed     Rscript ds_read.R
{code}
I didnt try to reproduce this problem on macOS, as my Mac would probably start swapping furiously but I managed to reproduce it on a Windows 7 machine with practically no swap. Of course the parameters are different, and the error messages are presumably system specific.
{code:java}
$ Rscript ds_prep.R 1000000 40
$ Rscript ds_read.R
Running query, 1st try...
ds size, 1st run: 56
Running query, 2nd try...
Error in dataset___Scanner__ToTable(self) :
  IOError: Out of memory: malloc of size 524288 failed
Calls: collect ... shared_ptr -> shared_ptr_is_null -> dataset___Scanner__ToTable
Execution halted
$ Rscript ds_read.R 1
Running query, 1st try...
ds size, 1st run: 56
running gc() ...
          used (Mb) gc trigger   (Mb)  max used (Mb)
Ncells  688789 36.8    1198030   64.0   1198030   64
Vcells 1109451  8.5  271538343 2071.7 321118845 2450
Running query, 2nd try...
ds size, 2nd run: 56
$ R_GC_MEM_GROW=0 Rscript ds_read.R
Running query, 1st try...
ds size, 1st run: 56
Running query, 2nd try...
Error in dataset___Scanner__ToTable(self) :
  IOError: Out of memory: malloc of size 524288 failed
Calls: collect ... shared_ptr -> shared_ptr_is_null -> dataset___Scanner__ToTable
Execution halted
{code}",pull-request-available,['R'],ARROW,Bug,Major,2020-09-24 10:11:07,6
13329116,[C++] Potential overflow in bit_stream_utils.h multiplication.,"We use a literal ""8"" for BitsPerByte which is interpretted as int32_t which can overflow.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-24 02:53:15,15
13329115,[C++] Use TemporaryDir for all tests that don't already use it.,This ensures all files are cleaned up and for some build system it avoid the issue of requiring the ability to write to source/build path when running the test,pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-24 02:50:36,15
13329113,[C++] Don't use string_view.to_string(),This makes our non standard string_view incompatible with std::string_view when we eventually upgrade to C++17,pull-request-available,['C++'],ARROW,Improvement,Minor,2020-09-24 02:46:09,15
13329003,[Python] Test test_parquet_nested_storage relies on dict item ordering,"This causes sporadic test failures on python 3.5, where that ordering is not guaranteed:

https://github.com/apache/arrow/pull/8240/checks?check_run_id=1154644456#step:8:2885",pull-request-available,['Python'],ARROW,Bug,Major,2020-09-23 14:09:42,6
13328883,[C++] Add bundled external project for aws-sdk-cpp,"Currently {{build_awssdk}} errors with a FIXME message. We should fix it. aws-sdk-cpp is not widely available on package managers, and in some cases (like Homebrew) its cmake config is broken. ",pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2020-09-22 22:42:12,4
13328838,[C++] Make sure that default AWS region is respected,See https://aws.amazon.com/blogs/developer/aws-sdk-for-c-version-1-8-developer-preview/. The sdk will detect the region from EC2 metadata if present; it will also (as of 1.8 at least) let you specify a default region via environment variable. So we should make sure that we don't override it with us-east-1 if the SDK pulls a region from one of those. (Sounds like how access_key/secret_key are currently handled.),pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-22 17:37:10,2
13328806,[Rust] DRY downcasted Arrays,"Currently, we have a significant amount of repeated code to cover slight variations of downcasted arrays:
 * BinaryArray
 * LargeBinaryArray
 * ListArray
 * StringArray
 * LargeStringArray

The goal of this issue is to clean up repeated code around.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-22 15:26:15,9
13328771,[C++] Resolve compile warnings on Apple Clang 12,"{code}
../src/arrow/util/uri.cc:172:19: error: loop variable 'seg' of type 'const nonstd::sv_lite::basic_string_view<char, std::__1::char_traits<char> >' creates a copy from type 'const nonstd::sv_lite::basic_string_view<char, std::__1::char_traits<char> >' [-Werror,-Wrange-loop-analysis]
  for (const auto seg : segments) {
                  ^
../src/arrow/util/uri.cc:172:8: note: use reference type 'const nonstd::sv_lite::basic_string_view<char, std::__1::char_traits<char> > &' to prevent copying
  for (const auto seg : segments) {
       ^~~~~~~~~~~~~~~~
                  & {code}

{code}
../src/arrow/ipc/metadata_internal.cc:1157:20: error: loop variable 'pair' is always a copy because the range of type 'const arrow::ipc::internal::KVVector' (aka 'const Vector<Offset<flatbuf::KeyValue> >') does not return a reference [-Werror,-Wrange-loo
p-analysis]
  for (const auto& pair : *fb_metadata) {
                   ^
../src/arrow/ipc/metadata_internal.cc:1157:8: note: use non-reference type 'const org::apache::arrow::flatbuf::KeyValue *'
  for (const auto& pair : *fb_metadata) {
       ^~~~~~~~~~~~~~~~~~
{code}

",pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-22 12:42:36,3
13328770,[Archery][CI] Fetch main branch in archery build only when it is a pull request,"Arrow's git data for the main branch is required to test the release curation scripts.

While the build properly works from pull requests it is failing on the main branch since the requested branch already exists:https://github.com/apache/arrow/runs/1148584775",pull-request-available,"['Archery', 'Continuous Integration']",ARROW,Task,Major,2020-09-22 12:27:36,3
13328671,[Rust] [DataFusion] MergeExec currently discards partitions with errors,IMO they should be accounted for.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-09-22 02:46:12,9
13328654,[R][Doc] Give more advice on how to set up C++ build,"I'd like to add a function to help do some of this in a way that's tailored to the user's platform, but improving the README is a start. Stuff like which cmake flags need to be on for the R package, etc.",pull-request-available,"['Documentation', 'R']",ARROW,Improvement,Major,2020-09-21 23:09:11,4
13328623,[C++] Investigate performance of LevelsToBitmap without BMI2,"Currently, when some Parquet nested data involves some repetition levels, converting the levels to bitmap goes through a slow scalar path unless the BMI2 instruction set is available and efficient (the latter using the PEXT instruction to process 16 levels at once).

It may be possible to emulate PEXT for 5- or 6-bit masks by using a lookup table, allowing to process 5-6 levels at once.

(also, it would be good to add nested reading benchmarks for non-trivial nesting; currently we only benchmark one-level struct and one-level list)",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-09-21 18:52:28,2
13328621,[C++] Add Parquet-Arrow roundtrip tests for nested data,We need hand-written non-trivial roundtrip tests for nested data.,pull-request-available,['C++'],ARROW,Sub-task,Major,2020-09-21 18:48:11,2
13328552,[Python] Slice methods should return empty arrays instead of crashing,"This crashes because of a CHECK assert. However, it's surprising to Python users who would expect an empty slice, or at least an exception.
{code:java}
>>> import pyarrow as pa
>>> table = pa.table([[1,2,3], [4,5,6]], names=[""a"", ""b""])
>>> table.slice(4, 5)
/arrow/cpp/src/arrow/chunked_array.cc:122:  Check failed: (offset) <= (length_) Slice offset greater than array length
/home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.8/site-packages/pyarrow/libarrow.so.200(+0x550e68)[0x7fe2dead1e68]
/home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.8/site-packages/pyarrow/libarrow.so.200(_ZN5arrow4util8ArrowLogD1Ev+0xdd)[0x7fe2dead27dd]
/home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.8/site-packages/pyarrow/libarrow.so.200(_ZNK5arrow12ChunkedArray5SliceEll+0x634)[0x7fe2debdfdd4]
/home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.8/site-packages/pyarrow/libarrow.so.200(_ZNK5arrow11SimpleTable5SliceEll+0x64)[0x7fe2deb798e4]
/home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.8/site-packages/pyarrow/lib.cpython-38-x86_64-linux-gnu.so(+0x168425)[0x7fe2dfb5a425]
/usr/lib/libpython3.8.so.1.0(+0x130ed5)[0x7fe2e08feed5]
/usr/lib/libpython3.8.so.1.0(_PyEval_EvalFrameDefault+0x761)[0x7fe2e08ecac1]
/usr/lib/libpython3.8.so.1.0(_PyEval_EvalCodeWithName+0x304)[0x7fe2e08eb044]
/usr/lib/libpython3.8.so.1.0(PyEval_EvalCode+0x23)[0x7fe2e099c3a3]
/usr/lib/libpython3.8.so.1.0(+0x1d9c18)[0x7fe2e09a7c18]
/usr/lib/libpython3.8.so.1.0(+0x1d3e33)[0x7fe2e09a1e33]
/usr/lib/libpython3.8.so.1.0(+0x10151a)[0x7fe2e08cf51a]
/usr/lib/libpython3.8.so.1.0(PyRun_InteractiveLoopFlags+0xee)[0x7fe2e08d07ff]
/usr/lib/libpython3.8.so.1.0(PyRun_AnyFileExFlags+0x3e)[0x7fe2e086b08e]
/usr/lib/libpython3.8.so.1.0(+0x89ca3)[0x7fe2e0857ca3]
/usr/lib/libpython3.8.so.1.0(Py_BytesMain+0x39)[0x7fe2e0990c59]
/usr/lib/libc.so.6(__libc_start_main+0xf2)[0x7fe2e062d152]
python(_start+0x2e)[0x5611f54b104e]
 {code}",pull-request-available,['Python'],ARROW,Improvement,Major,2020-09-21 12:38:48,0
13328423,[Rust] Error in aggregate of min/max for strings,There is an error in computing the min/max of strings with nulls.,pull-request-available,['Rust'],ARROW,Bug,Major,2020-09-20 17:25:41,9
13328400,[Rust] [DataFusion] Made `*Iterator` implement Iterator,"Currently, we use{{next_batch -> Result<Option<RecordBatch>>}} to iterate over batches. However, this is very similar to the iterator pattern that Rust offers.

This issue concerns migrating our code from{{next() -> Option<Result<RecordBatch>>}}

on the trait Iterator, so that we can leverage the rich Rust iterator API.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-09-20 10:23:29,9
13328311,[Rust] Possible to create LargeStringArray with DataType::Utf8,"We don't perform enough checks on ArrayData when creating StringArray and LargeStringArray. As they use different integer sizes for offsets, this can create a problem where Offset<Vec<i32>> could be correctly reinterpreted as Offset<Vec<i64>> and vice versa.

We should add checks that pervent this. The same might apply for List and LargeList",pull-request-available,['Rust'],ARROW,Bug,Major,2020-09-18 20:38:15,9
13328260,[C++] SetCpuThreadPoolCapacity(1) spins up nCPUs threads,"When I call {{arrow::SetCpuThreadPoolCapacity(1);}}, Arrow does this:

1. Spins up a singleton {{ThreadPool}} with _the default thread count_;
2. Sets the number of threads on that {{ThreadPool}} to 1 -- killing the extra threads.

On my Intel system, I'm forced to spin up four threads to set the CPU thread-pool capacity to 1. This goes against the spirit of the API method -- or at least, my understanding of it (and my experience with other thread pools).

My workaround, for calling code: instead of calling {{arrow::SetCpuThreadPoolCapacity(1)}}, call {{setenv(""OMP_NUM_THREADS"", ""1"", 1)}}.

Brainstorming, here are some ideas for Arrow's global thread pool that would stop launching {{>limit}} threads to set the limit:

* {{cpu_thread_pool_capacity}} could be a global variable, not an attribute on the global {{ThreadPool}}. API users would be expected to set the thread-pool capacity _before_ creating the thread pool. (They're probably doing this anyway.)
* {{SetCpuThreadPoolCapacity()}} could call {{setenv(""OMP_NUM_THREADS"", ...)}}
* {{ThreadPool}} could create threads on-demand instead of in the ctor. An unused {{ThreadPool}} would launch zero threads -- resolving ARROW-10033 as a side-effect",pull-request-available,['C++'],ARROW,Bug,Minor,2020-09-18 14:04:16,2
13328207,[C++] Workaround to force find AWS SDK to look for shared libraries ,Since the recent conda forge feedstock update find aws sdk fails to locate the proper aws sdk cmake files:[https://github.com/apache/arrow/runs/1131824712#step:8:3700],pull-request-available,['C++'],ARROW,Task,Major,2020-09-18 09:02:30,3
13328149,[Rust] Master build broken,I merged quite a few PRs today. There was a conflict and I need to revert one of them. I am working on it.,pull-request-available,['Rust'],ARROW,Bug,Major,2020-09-17 23:43:24,10
13328082,[Documentation] C++ Windows docs are out of date,"""Replicating AppVeyor Builds"" needs the following changes: [https://arrow.apache.org/docs/developers/cpp/windows.html#replicating-appveyor-builds]
 * The recommended VM does not include the C++ compiler - we should link to the build tools and describe which of them needs installation
 * Boost: the b2 script now requires --with not -with flags
 * The batch script were renamed (appveyor-cpp-build/appveyor-cpp-setup)
 * Prefer JOB=Build_Debug as otherwise it forces clcache
 * BOOST_INCLUDEDIR must be set to C:\Boost\include\boost_VERSION
 * Use conda manually to install gtest gflags ninja rapidjson grpc-cpp protobuf

Even with this:
 * The developer prompt can't find cl.exe (the compiler). (You must restart the VM!)
 * The PowerShell prompt can't use conda (it complains a config file isn't signed)
 Solution: run a PowerShell instance as administrator and run ""Set-ExecutionPolicy -ExecutionPolicy Unrestricted""",pull-request-available,['Documentation'],ARROW,Improvement,Major,2020-09-17 16:11:15,0
13327976,[Rust] Support fromIter and toIter,"Proposal for comments: [https://docs.google.com/document/d/1d6rV1WmvIH6uW-bcHKrYBSyPddrpXH8Q4CtVfFHtI04/edit?usp=sharing]

(dump of the document above)

Rust Arrow supports two main computational models:
 # Batch Operations, that leverage some form of vectorization
 # Element-by-element operations, that emerge in more complex operations

This document concerns element-by-element operations, that are common outside of the library (and sometimes in the library).
h2. Element-by-element operations

These operations are programmatically written as:
 # Downcast the array to its specific type
 # Initialize buffers
 # Iterate over indices and perform the operation, appending to the buffers accordingly
 # Create ArrayData with the required null bitmap, buffers, childs, etc.
 # return ArrayRef from ArrayData



We can split this process in 3 parts:
 # Initialization (1 and 2)
 # Iteration (3)
 # Finalization (4 and 5)

Currently, the API that we offer to our users is:
 # as_any() to downcast the array based on its DataType
 # Builders for all types, that users can initialize, matching the downcasted array
 # Iterate
 ## Use for i in (0..array.len())
 ## Use {{Array::value(i)}} and {{Array::is_valid(i)/is_null(i)}}
 ## use builder.append_value(new_value) or builder.append_null()
 # Finish the builder and wrap the result in an Arc

This API has some issues:
 # value(i) +is unsafe+, even though it is not marked as such
 # builders are usually slow due to the checks that they need to perform
 # The API is not intuitive

h2. Proposal

This proposal aims at improving this API in 2 specific ways:
 * Implement IntoIterator Iterator<Item=T> and Iterator<Item=Option<T>>
 * Implement FromIterator<Item=T> and Item=Option<T>

so that users can write:
{code:java}
// incoming array
let array = Int32Array::from(vec![Some(0), None, Some(2), None, Some(4)]);
let array = Arc::new(array) as ArrayRef;
let array = array.as_any().downcast_ref::<Int32Array>().unwrap();

// to and from iter, with a +1
let result: Int32Array = array
 .iter()
 .map(|e| if let Some(r) = e { Some(r + 1) } else { None })
 .collect();

let expected = Int32Array::from(vec![Some(1), None, Some(3), None, Some(5)]); 

assert_eq!(result, expected);
{code}


This results in an API that is:
 # efficient, as it is our responsibility to create `FromIterator` that are efficient in populating the buffers/child etc from an iterator
 # Safe, as it does not allow segfaults
 # Simple, as users do not need to worry about Builders, buffers, etc, only native Rust.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-17 07:07:17,9
13327959,[Rust] Simplify macro def_numeric_from_vec,"Currently we need to pass too many parameters to it, when they can be inferred.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-17 04:44:01,9
13327920,[Python] Incorrect null column returned when using a dataset filter expression.,"When using dataset filter expressions (which I <3) with Parquet files, entire {{null}} columns are returned, rather than rows that matched other columns in the filter. 

Here's an example.

{code:python}
In [7]: import pyarrow as pa
In [8]: import pyarrow.dataset as ds
In [9]: import pyarrow.parquet as pq

In [10]: table = pa.Table.from_arrays(
 ...:     arrays=[
 ...:         pa.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),
 ...:         pa.array([""zero"", ""one"", ""two"", ""three"", ""four"", ""five"", ""six"", ""seven"", ""eight"", ""nine""]),
 ...:         pa.array([None, None, None, None, None, None, None, None, None, None]),
 ...:     ],
 ...:     names=[""id"", ""name"", ""other""],
 ...: )

In [11]: table
Out[11]:
pyarrow.Table
id: int64
name: string
other: null

In [12]: table.to_pandas()
Out[12]:
   id   name other
0   0   zero  None
1   1    one  None
2   2    two  None
3   3  three  None
4   4   four  None
5   5   five  None
6   6    six  None
7   7  seven  None
8   8  eight  None
9   9   nine  None

In [13]: pq.write_table(table, ""/tmp/test.parquet"", data_page_version=""2.0"")
In [14]: data = ds.dataset(""/tmp/test.parquet"")
In [15]: table = data.to_table(filter=ds.field(""id"").isin([1, 4, 7]))
In [16]: table
Out[16]:
pyarrow.Table
id: int64
name: string
other: null

In [17]: table.to_pydict()
Out[17]:
{'id': [1, 4, 7],
 'name': ['one', 'four', 'seven'],
 'other': [None, None, None, None, None, None, None, None, None, None]}
{code}
The {{to_pydict}} method highlights the strange behavior: the {{id}} and {{name}} columns have 3 elements, but the {{other}} column has all 10. When I call {{to_pandas}} on the filtered table, the program crashes.

This could be a C++ issue, but, since my examples are in Python, I categorized it as a Python issue. Let me know if that's wrong and I'll note that for the future.",pull-request-available,['Python'],ARROW,Bug,Major,2020-09-16 21:00:00,5
13327733,[Rust] Add substring kernel,"substring returns a substring of a StringArray starting at a given index, and with a given optional length.

{{fn substring(array: &Array, start: i32, length: &Option<u32>) -> Result<ArrayRef>}}

This operation is common in strings, and it is useful for string-based transformations",pull-request-available,['Rust'],ARROW,New Feature,Major,2020-09-15 20:59:56,9
13327731,[CI] Disable Sphinx and API documentation build since it takes 6 hours on master,"The reason is unclear from the build logs, but I suggest to turn it off until it gets resolved.",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2020-09-15 20:57:56,3
13327668,[C++][CI] Flight test failure in TestFlightClient.GenericOptions,"This failure seems non-deterministic?
On AppVeyor (Windows): https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/35214577/job/7aolb47v3s80pdgb
{code}
[ RUN      ] TestFlightClient.GenericOptions
C:/projects/arrow/cpp/src/arrow/flight/flight_test.cc(1429): error: Failed
Expected 'status' to fail with Invalid, but got OK
[  FAILED  ] TestFlightClient.GenericOptions (5 ms)
{code}

On GitHub Actions (macOS): https://github.com/apache/arrow/pull/8193/checks?check_run_id=1117619662#step:6:217
{code}
[ RUN      ] TestFlightClient.GenericOptions
/Users/runner/work/arrow/arrow/cpp/src/arrow/flight/flight_test.cc:1429: Failure
Failed
Expected 'status' to fail with Invalid, but got OK
[  FAILED  ] TestFlightClient.GenericOptions (6 ms)
{code}
",pull-request-available,"['C++', 'Continuous Integration', 'FlightRPC']",ARROW,Bug,Major,2020-09-15 12:52:15,0
13327643,[C++] Sporadic failures in CopyFiles test,"Here is an example failure on AppVeyor (Windows):
https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/35210915/job/yhdxkfu7hta3kugp#L1913
{code}
[ RUN      ] TestSubTreeFileSystem.CopyFiles
C:/projects/arrow/cpp/src/arrow/filesystem/filesystem_test.cc(608): error: Failed
'CopyFiles({{subfs_, ""ab""}, {subfs_, ""cd""}, {subfs_, ""ef""}}, {{dest_fs, ""AB/ab""}, {dest_fs, ""CD/CD/cd""}, {dest_fs, ""EF/EF/EF/ef""}})' failed with IOError: Path does not exist 'sub/copy/AB/ab'
[  FAILED  ] TestSubTreeFileSystem.CopyFiles (9 ms)
{code}

Similar failure on Linux, showing the line number:
https://ci.ursalabs.org/#/builders/101/builds/4412/steps/8/logs/stdio
{code}
[ RUN      ] TestSubTreeFileSystem.CopyFiles
../src/arrow/filesystem/filesystem_test.cc:608: Failure
Failed
'CopyFiles({{subfs_, ""ab""}, {subfs_, ""cd""}, {subfs_, ""ef""}}, {{dest_fs, ""AB/ab""}, {dest_fs, ""CD/CD/cd""}, {dest_fs, ""EF/EF/EF/ef""}})' failed with IOError: Path does not exist 'sub/copy/CD/CD/cd'
In ../src/arrow/filesystem/filesystem.cc, line 467, code: (_error_or_value13).status()
[  FAILED  ] TestSubTreeFileSystem.CopyFiles (17 ms)
{code}

The erroring line number (467) corresponds to the {{OpenOutputStream}} call below:
{code:c++}

        auto dest_dir = internal::GetAbstractPathParent(destinations[i].path).first;
        if (!dest_dir.empty()) {
          RETURN_NOT_OK(destinations[i].filesystem->CreateDir(dest_dir));
        }

-->     ARROW_ASSIGN_OR_RAISE(
            auto destination,
            destinations[i].filesystem->OpenOutputStream(destinations[i].path));
{code}

It's not obvious why the error occurs, since the directory is created just above (race condition in the CreateDir implementation?).",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Minor,2020-09-15 10:15:04,2
13327607,[C++] Make FindRE2.cmake re-entrant,"Repeated calls to FindRE2.cmake try to recreate the exisiting target {{RE2::re2}} which is prohibited by CMake and fails with the following error:

{code}
CMake Warning (dev) at C:/Miniconda37-x64/envs/arrow/Library/share/cmake-3.17/Modules/FindPackageHandleStandardArgs.cmake:272 (message):
  The package name passed to `find_package_handle_standard_args` (RE2) does
  not match the name of the calling package (re2).  This can lead to problems
  in calling code that expects `find_package` result variables (e.g.,
  `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  cmake_modules/FindRE2.cmake:63 (find_package_handle_standard_args)
  C:/Miniconda37-x64/envs/arrow/Library/lib/cmake/grpc/gRPCConfig.cmake:21 (find_package)
  cmake_modules/ThirdpartyToolchain.cmake:2472 (find_package)
  CMakeLists.txt:495 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.
CMake Error at cmake_modules/FindRE2.cmake:66 (add_library):
  add_library cannot create imported target ""RE2::re2"" because another target
  with the same name already exists.
Call Stack (most recent call first):
  C:/Miniconda37-x64/envs/arrow/Library/lib/cmake/grpc/gRPCConfig.cmake:21 (find_package)
  cmake_modules/ThirdpartyToolchain.cmake:2472 (find_package)
  CMakeLists.txt:495 (include)
{code}

Note that this issue only occurs currently on case-insensitive file systems when ARROW_FLIGHT=ON is set.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-09-15 07:48:45,8
13327588,[Rust] Speedup arithmetic,"There are some optimizations possible in arithmetics kernels.



PR to follow",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-15 05:42:17,9
13327548,[Python] pyarrow.parquet.read_table fails with predicate pushdown on categorical data with use_legacy_dataset=False,"I apologise if this is a known issue; I looked both in this issue tracker and on github and I didn't find it.

There seems to be a problem reading a dataset with predicate pushdown (filters) on columns with categorical data. The problem only occurs with `use_legacy_dataset=False` (but if that's True it has no effect if the column isn't a partition key.

Reproducer:
{code:python}
import shutil
import sys, platform
from pathlib import Path
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd

# Settings
CATEGORICAL_DTYPE = True
USE_LEGACY_DATASET = False

print('Platform:', platform.platform())
print('Python version:', sys.version)
print('Pandas version:', pd.__version__)
print('pyarrow version:', pa.__version__)
print('categorical enabled:', CATEGORICAL_DTYPE)
print('use_legacy_dataset:', USE_LEGACY_DATASET)
print()

# Clean up test dataset if present
path = Path('blah.parquet')
if path.exists():
    shutil.rmtree(str(path))

# Simple data
d = dict(col1=['a', 'b'], col2=[1, 2])

# Either categorical or not
if CATEGORICAL_DTYPE:
    df = pd.DataFrame(data=d, dtype='category')
else:
    df = pd.DataFrame(data=d)

# Write dataset
table = pa.Table.from_pandas(df)
pq.write_to_dataset(table, str(path))

# Load dataset
table = pq.read_table(
    str(path),
    filters=[('col1', '=', 'a')],
    use_legacy_dataset=USE_LEGACY_DATASET,
)
df = table.to_pandas()
print(df.dtypes)
print(repr(df))

{code}
Output:
{code:java}
$ python categorical_predicate_pushdown.py 
Platform: Linux-5.8.9-050809-generic-x86_64-with-glibc2.10
Python version: 3.8.5 (default, Aug  5 2020, 08:36:46) 
[GCC 7.3.0]
Pandas version: 1.1.2
pyarrow version: 1.0.1
categorical enabled: True
use_legacy_dataset: False

/arrow/cpp/src/arrow/result.cc:28: ValueOrDie called on an error: Type error: Cannot compare scalars of differing type: dictionary<values=string, indices=int32, ordered=0> vs string
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow.so.100(+0x4fc128)[0x7f50568c6128]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow4util8ArrowLogD1Ev+0xdd)[0x7f50568c693d]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow8internal14DieWithMessageERKSs+0x51)[0x7f50569757c1]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow8internal17InvalidValueOrDieERKNS_6StatusE+0x4c)[0x7f505697716c]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZNK5arrow7dataset20ComparisonExpression21AssumeGivenComparisonERKS1_+0x438)[0x7f5043334f18]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZNK5arrow7dataset20ComparisonExpression6AssumeERKNS0_10ExpressionE+0x34)[0x7f5043334fa4]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZNK5arrow7dataset20ComparisonExpression6AssumeERKNS0_10ExpressionE+0xce)[0x7f504333503e]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZNK5arrow7dataset20ComparisonExpression6AssumeERKNS0_10ExpressionE+0xce)[0x7f504333503e]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZNK5arrow7dataset12RowGroupInfo7SatisfyERKNS0_10ExpressionE+0x1c)[0x7f50433116ac]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZN5arrow7dataset19ParquetFileFragment15FilterRowGroupsERKNS0_10ExpressionE+0x563)[0x7f5043311cb3]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZNK5arrow7dataset17ParquetFileFormat8ScanFileESt10shared_ptrINS0_11ScanOptionsEES2_INS0_11ScanContextEEPNS0_12FileFragmentE+0x203)[0x7f50433168a3]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZN5arrow7dataset12FileFragment4ScanESt10shared_ptrINS0_11ScanOptionsEES2_INS0_11ScanContextEE+0x55)[0x7f5043329785]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZZN5arrow7dataset19GetScanTaskIteratorENS_8IteratorISt10shared_ptrINS0_8FragmentEEEES2_INS0_11ScanOptionsEES2_INS0_11ScanContextEEENKUlS4_E_clES4_+0x91)[0x7f50433485a1]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZN5arrow8IteratorINS0_ISt10shared_ptrINS_7dataset8ScanTaskEEEEE4NextINS_11MapIteratorIZNS2_19GetScanTaskIteratorENS0_IS1_INS2_8FragmentEEEES1_INS2_11ScanOptionsEES1_INS2_11ScanContextEEEUlSA_E_SA_S5_EEEENS_6ResultIS5_EEPv+0xde)[0x7f504334b55e]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZN5arrow15FlattenIteratorISt10shared_ptrINS_7dataset8ScanTaskEEE4NextEv+0x127)[0x7f50433616b7]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZN5arrow8IteratorISt10shared_ptrINS_7dataset8ScanTaskEEE4NextINS_15FlattenIteratorIS4_EEEENS_6ResultIS4_EEPv+0x14)[0x7f5043361874]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/libarrow_dataset.so.100(_ZN5arrow7dataset7Scanner7ToTableEv+0x611)[0x7f5043336691]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/_dataset.cpython-38-x86_64-linux-gnu.so(+0x3b150)[0x7f50435c9150]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/_dataset.cpython-38-x86_64-linux-gnu.so(+0x2c0eb)[0x7f50435ba0eb]
/home/caleb/Documents/kapiche/chrysalis/venv38/lib/python3.8/site-packages/pyarrow/_dataset.cpython-38-x86_64-linux-gnu.so(+0x2d9ab)[0x7f50435bb9ab]
python(PyCFunction_Call+0x56)[0x562843a6dce6]
python(_PyObject_MakeTpCall+0x22f)[0x562843a2b5cf]
python(_PyEval_EvalFrameDefault+0x11d7)[0x562843aaf727]
python(_PyEval_EvalCodeWithName+0x2d2)[0x562843a78802]
python(+0x18bb80)[0x562843a79b80]
python(+0x1001e3)[0x5628439ee1e3]
python(_PyEval_EvalCodeWithName+0x2d2)[0x562843a78802]
python(_PyFunction_Vectorcall+0x1e3)[0x562843a797a3]
python(+0x1001e3)[0x5628439ee1e3]
python(_PyEval_EvalCodeWithName+0x2d2)[0x562843a78802]
python(PyEval_EvalCodeEx+0x44)[0x562843a795b4]
python(PyEval_EvalCode+0x1c)[0x562843b07bdc]
python(+0x219c84)[0x562843b07c84]
python(+0x24be94)[0x562843b39e94]
python(PyRun_FileExFlags+0xa1)[0x562843a0279a]
python(PyRun_SimpleFileExFlags+0x3b4)[0x562843a02b7f]
python(+0x115a44)[0x562843a03a44]
python(Py_BytesMain+0x39)[0x562843b3c9b9]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)[0x7f5058f2a0b3]
python(+0x1dea83)[0x562843acca83]
Aborted (core dumped)
{code}
With `CATEGORICAL_DTYPE = False`, it works as expected:
{code:java}
$ python categorical_predicate_pushdown.py 
Platform: Linux-5.8.9-050809-generic-x86_64-with-glibc2.10
Python version: 3.8.5 (default, Aug  5 2020, 08:36:46) 
[GCC 7.3.0]
Pandas version: 1.1.2
pyarrow version: 1.0.1
categorical enabled: False
use_legacy_dataset: Falsecol1    object
col2     int64
dtype: object
  col1  col2
0    a     1

{code}


",categorical category dataset filters parquet predicate pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-09-15 00:40:58,6
13327498,[C++] Create directories in CopyFiles when copying within the same filesystem,"CopyFiles creates parent directories for destination files, but only when copying between different filesystems. This behavior should be made consistent",pull-request-available,['C++'],ARROW,Bug,Major,2020-09-14 17:43:41,6
13327492,[Rust] Trait-specialization requires nightly,"Trait specialization is widely used in the Rust Arrow implementation. Uses can be identified by searching for instances of {{default fn}} in the codebase:


{code:java}
$> rg -c 'default fn' ../arrow/rust/
 ../arrow/rust/parquet/src/util/test_common/rand_gen.rs:1
 ../arrow/rust/parquet/src/column/writer.rs:2
 ../arrow/rust/parquet/src/encodings/encoding.rs:16
 ../arrow/rust/parquet/src/arrow/record_reader.rs:1
 ../arrow/rust/parquet/src/encodings/decoding.rs:13
 ../arrow/rust/parquet/src/file/statistics.rs:1
 ../arrow/rust/arrow/src/array/builder.rs:7
 ../arrow/rust/arrow/src/array/array.rs:3
 ../arrow/rust/arrow/src/array/equal.rs:3{code}


This feature requires Nightly Rust. Additionally, there is [no schedule for stabilization|https://github.com/rust-lang/rust/issues/31844#issue-135807289], primarily due to an [unresolved soundness hole|http://aturon.github.io/blog/2017/07/08/lifetime-dispatch]. (Note: there has been further discussion and ideas for resolving the soundness issue, but to my knowledge no definitive action.)

If we can remove specialization from the Rust codebase, we will not be blocked on the Rust team's stabilization of that feature in order to move to stable Rust.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-09-14 17:13:53,9
13327462,[R] Snappy Codec Support not built,"I am reading my file on a Linux based server which has no Snappy compression. Even though I call the function to do uncompressed compression. I still get an error Snappy codec support not built. How do I overcome this error and read a parquet file without snappy codec on linux?
read_parquet(file,as_data_frame=TRUE,compression='UNCOMPRESSED')

Error in parquet___arrow___FileReader__ReadTable1(self) : IOError: NotImplemented: Snappy codec support not built",Snappy,['R'],ARROW,Bug,Major,2020-09-14 15:29:11,4
13327391,[C++][Python] Refactor python to arrow conversions based on a reusable conversion API ,We have a lot of technical debt accumulated in the python to arrow conversion code paths including hidden bugs. We need to simplify the implementation. ,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-09-14 09:22:14,3
13327336,[Rust] [DataFusion] NOT is not plannable,"We have the physical operator, but it is not usable in the logical planning.",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Major,2020-09-14 03:32:31,9
13327326,[Rust] [DataFusion] Added std::ops to logical expressions,"So that we can write {{col(""a"") + col(""b"")}}",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-09-14 02:26:12,9
13327285,[Rust][DataFusion] TO_TIMESTAMP function erroneously requires fractional seconds when no timezone is present,"Reported by [~jhorstmann] here: https://github.com/apache/arrow/pull/8161#issuecomment-691468844


>One (not directly related) issue I noticed while trying this out, is that the local patterns seem to require the millisecond part, while for utc timestamps with ""Z"" they are optional:

Both of the following timestamps should be supported, but only the one with an explicit timestamp is:

{code}
> select to_timestamp('2020-09-12T10:30:00') from test limit 1;
ArrowError(ExternalError(General(""Error parsing \'2020-09-12T10:30:00\' as timestamp"")))

> select to_timestamp('2020-09-12T10:30:00Z') from test limit 1;
+-------------------------------------------+
| totimestamp(Utf8(""2020-09-12T10:30:00Z"")) |
+-------------------------------------------+
| 1599906600000000000                       |
+-------------------------------------------+
{code}",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Minor,2020-09-13 11:19:00,11
13327244,[C++][Dataset][Python] Use larger default batch size than 32K for Datasets API,"Dremio uses 64K batch sizes. We could probably get away with even larger batch sizes (e.g. 256K or 1M) and allow memory-constrained users to elect a smaller batch size. 

See example of some performance issues related to this in ARROW-9924",dataset,['C++'],ARROW,Improvement,Major,2020-09-12 20:01:58,6
13327229,[Rust] Allow configuring flight IPC with IpcWriteOptions,"We have introduced an IPC write option, but we use the default for the arrow-flight crate, which is not ideal. Change this to allow configuring writer options.",pull-request-available,['Rust'],ARROW,Sub-task,Minor,2020-09-12 16:58:48,12
13327225,[Rust] Fix parquet crate clippy lints,This addresses most clippy lints on the parquet crate. Other remaining lints can be addressed as part of future PRs,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-09-12 15:21:34,12
13327223,[Rust] Fix arrow crate clippy lints,"This fixes many clippy lints, but not all. It takes hours to address lints, ansd we can work on remaining ones in future PRs",pull-request-available,['Rust'],ARROW,Sub-task,Minor,2020-09-12 15:16:15,12
13327126,[Rust] Add min/max for [Large]String,Strings are ordered and thus we can apply min/max as other types.,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-11 17:05:09,9
13326967,[CI] Work around grpc-re2 clash on Homebrew,See https://github.com/grpc/grpc/issues/24077 for context,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2020-09-10 22:05:51,4
13326915,[C++] UBSAN link failure with __int8_t,"See https://github.com/apache/arrow/runs/1093237142 :
{code}
src/arrow/util/CMakeFiles/arrow-utility-test.dir/decimal_test.cc.o: In function `arrow::Decimal128Test_Multiply_Test::TestBody()':
/arrow/cpp/src/arrow/util/decimal_test.cc:935: undefined reference to `__muloti4'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
{code}",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2020-09-10 17:32:55,2
13326893,[C++] CSV date support,"There is no support for reading date type from CSV file. I'd like to read such a value:
{code:java}
1991-02-03
{code}
",pull-request-available,['C++'],ARROW,Improvement,Major,2020-09-10 15:11:44,2
13326879,[Python] Recognize datetime.timezone.utc as UTC on conversion python->pyarrow,"Related to ARROW-5248, but specifically for the stdlib {{datetime.timezone.utc}}, I think it would be nice to ""recognize"" this as UTC. Currently it is converted to ""+00:00"", while for pytz this is not the case:

{code}
from datetime import datetime, timezone
import pytz

print(pa.array([datetime.now(timezone.utc)]).type)
print(pa.array([datetime.now(pytz.utc)]).type)
{code}

gives

{code}
timestamp[us, tz=+00:00]
timestamp[us, tz=UTC]
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-09-10 13:55:28,5
13326872,[Python] Conversion to pandas with index column using fixed timezone fails,"From https://github.com/pandas-dev/pandas/issues/35997: it seems we are handling a normal column and index column differently in the conversion to pandas.

{code}
In [5]: import pandas as pd
   ...: from datetime import datetime, timezone
   ...: 
   ...: df = pd.DataFrame([[datetime.now(timezone.utc), datetime.now(timezone.utc)]], columns=['date_index', 'date_column'])
   ...: table = pa.Table.from_pandas(df.set_index('date_index'))
   ...: 

In [6]: table
Out[6]: 
pyarrow.Table
date_column: timestamp[ns, tz=+00:00]
date_index: timestamp[ns, tz=+00:00]

In [7]: table.to_pandas()
...
UnknownTimeZoneError: '+00:00'
{code}

So this happens specifically for ""fixed offset"" timezones, and only for index columns (eg {{table.select([""date_column""]).to_pandas()}} works fine).

It seems this is because for columns we use our helper {{make_tz_aware}} to convert the string ""+01:00"" to a python timezone, which is then understood by pandas (the string is not handled by pandas). But for the index column we fail to do this.",pull-request-available,['Python'],ARROW,Bug,Major,2020-09-10 13:29:45,5
13326849,[Rust][DataFusion] to_timestamp function parses timestamp without timezone offset as UTC rather than local,"ARROW-9944 added a TO_TIMESTAMP function that supports parsing timestamps without a specified timezone, such as {{2020-09-08T13:42:29.190855}}

Such timestamps are supposed to be interpreted as in the local timezone, but instead are interpreted as UTC. ",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-09-10 11:18:11,11
13326789,[Rust] Remove unmaintained tempdir dependency,"Replace tempdir with tempfile, also removing older versions of some dependencies like rand.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Trivial,2020-09-10 06:35:41,12
13326676,[Python] Use pyarrow.dataset writing for pq.write_to_dataset,"Now ARROW-9658 and ARROW-9893 are in, we can explore using the {{pyarrow.dataset}} writing capabilities in {{parquet.write_to_dataset}}.

Similarly as was done in {{pq.read_table}}, we could initially have a keyword to switch between both implementations, eventually defaulting to the new datasets one, and to deprecated the old (inefficient) python implementation.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-09-09 13:05:42,5
13326582,[Rust] [DataFusion] Allow UDF usage without registry,"This is a functionality relevant only for the DataFrame API.

Sometimes a UDF declaration happens during planning, and it makes it very expressive when the user does not have to access the registry at all to plan the UDF.
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-09-09 03:21:23,9
13326519,[C++][Dataset] Refactor Expression::Assume to return a Result,"Expression::Assume can abort if the two expressions are not valid against a single schema. This is not ideal since a schema is not always easily available. The method should be able to fail gracefully in the case of a best-effort simplification where validation against a schema is not desired.

https://github.com/apache/arrow/pull/8037#discussion_r475594117",dataset,['C++'],ARROW,Improvement,Major,2020-09-08 16:00:40,6
13326510,[Rust] Implement TO_TIMESTAMP function,"Implement the TO_TIMESTAMP function, as described in https://docs.google.com/document/d/18O9YPRyJ3u7-58J02NtNVYb6TDWBzi3mIQC58VhwxUk/edit",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-09-08 15:00:37,11
13326503,[C++] Arrow metadata not applied recursively when reading Parquet file,"Currently, {{ApplyOriginalMetadata}} in {{src/parquet/arrow/schema.cc}} is only applied for the top-level node of each schema field. Nested metadata (such as dicts-inside-lists, etc.) will not be applied.",pull-request-available,['C++'],ARROW,Bug,Major,2020-09-08 14:31:59,2
13326389,[Rust] [DataFusion] Average is not correct,"The current design of aggregates makes the calculation of the average incorrect.

Namely, if there are multiple input partitions, the result is average of the averages. For example if the input it in two batches {{[1,2]}}, and {{[3,4,5]}}, datafusion will say ""average=3.25"" rather than ""average=3"".

 It also makes it impossible to compute the [geometric mean|https://en.wikipedia.org/wiki/Geometric_mean], distinct sum, and other operations.

The central issue is that Accumulator returns a `ScalarValue` during partial aggregations via {{get_value}}, but very often a `ScalarValue` is not sufficient information to perform the full aggregation.

A simple example is the average of 5 numbers, x1, x2, x3, x4, x5, that are distributed in batches of 2,

{[x1, x2], [x3, x4],[x5]}

. Our current calculation performs partial means,

{(x1+x2)/2, (x3+x4)/2, x5}

, and then reduces them using another average, i.e.

{{((x1+x2)/2 + (x3+x4)/2 + x5)/3}}

which is not equal to {{(x1 + x2 + x3 + x4 + x5)/5}}.

I believe that our Accumulators need to pass more information from the partial aggregations to the final aggregation.

We could consider taking an API equivalent to [spark]([https://docs.databricks.com/spark/latest/spark-sql/udaf-scala.html]), i.e. have an `update`, a `merge` and an `evaluate`.

Code with a failing test ({{src/execution/context.rs}})
{code:java}
    #[test]
    fn simple_avg() -> Result<()> {
        let schema = Schema::new(vec![
            Field::new(""a"", DataType::Int32, false),
        ]);

        let batch1 = RecordBatch::try_new(
            Arc::new(schema.clone()),
            vec![
                Arc::new(Int32Array::from(vec![1, 2, 3])),
            ],
        )?;
        let batch2 = RecordBatch::try_new(
            Arc::new(schema.clone()),
            vec![
                Arc::new(Int32Array::from(vec![4, 5])),
            ],
        )?;

        let mut ctx = ExecutionContext::new();

        let provider = MemTable::new(Arc::new(schema), vec![vec![batch1], vec![batch2]])?;
        ctx.register_table(""t"", Box::new(provider));

        let result = collect(&mut ctx, ""SELECT AVG(a) FROM t"")?;

        let batch = &result[0];
        assert_eq!(1, batch.num_columns());
        assert_eq!(1, batch.num_rows());

        let values = batch
            .column(0)
            .as_any()
            .downcast_ref::<Float64Array>()
            .expect(""failed to cast version"");
        assert_eq!(values.len(), 1);
        // avg(1,2,3,4,5) = 3.0
        assert_eq!(values.value(0), 3.0_f64);
        Ok(())
    }
{code}",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-09-08 07:08:26,9
13326387,[Python] Fix / test relative file paths in pyarrow.parquet,"It seems that I broke writing parquet to relative file paths in ARROW-9718 (again, something similar happened in the pyarrow.dataset reading), so should fix that and properly test this.

{code}
In [3]: pq.write_table(table, ""test_relative.parquet"")
...
~/scipy/repos/arrow/python/pyarrow/_fs.pyx in pyarrow._fs.FileSystem.from_uri()

ArrowInvalid: URI has empty scheme: 'test_relative.parquet'
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-09-08 07:04:45,5
13326287,[C++] Speed up integer parsing slightly,"By exiting early out of the parsing routine when the input is exhausted, we can save a little bit a processing time.",pull-request-available,['C++'],ARROW,Improvement,Trivial,2020-09-07 10:31:26,2
13326213,[Python] Performance regression reading individual Parquet files using Dataset interface,"I haven't investigated very deeply but this seems symptomatic of a problem:

{code}
In [27]: df = pd.DataFrame({'A': np.random.randn(10000000)})                                                                                                                              

In [28]: pq.write_table(pa.table(df), 'test.parquet')                                                                                                                                     

In [29]: timeit pq.read_table('test.parquet')                                                                                                                                             
79.8 ms  1.25 ms per loop (mean  std. dev. of 7 runs, 10 loops each)

In [30]: timeit pq.read_table('test.parquet', use_legacy_dataset=True)                                                                                                                    
66.4 ms  1.33 ms per loop (mean  std. dev. of 7 runs, 10 loops each)
{code}",pull-request-available,['Python'],ARROW,Bug,Critical,2020-09-06 22:05:12,6
13326159,[Rust] Add `from(Vec<Option<&str>>)` to [Large]StringArray,"and deprecate TryFrom, that currently uses a builder.

This should have some performance improvement, and simplifies the interface.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-09-05 17:28:16,9
13326153,[Python] pyarrow.concat_arrays segfaults when passing it a chunked array,"One can concat the chunks of a ChunkedArray with {{concat_arrays}} passing it the list of chunks:

{code}
In [1]: arr = pa.chunked_array([[0, 1], [3, 4]])

In [2]: pa.concat_arrays(arr.chunks)
Out[2]: 
<pyarrow.lib.Int64Array object at 0x7ff88fc76408>
[
  0,
  1,
  3,
  4
]
{code}

but if passing the chunked array itself, you get a segfault:

{code}
In [4]: pa.concat_arrays(arr)
Segmentation fault (core dumped)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-09-05 14:14:30,5
13326141,[Rust] [DataFusion] Math functions,See main issue.,pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-09-05 07:21:22,9
13326090,[Rust][DataFusion] Document the SQL -> Arrow type mapping,"DataFusion uses SQL data types for its query language, but Arrow types for its plans and execution. We should document the mapping of SQL types to Arrow types. ",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-09-04 14:49:18,11
13325966,[Rust] [DataFusion] Type coercion of Variadic is wrong,"Specifically, the case


{code:java}
            // common type is u64
            case(
                vec![DataType::UInt32, DataType::UInt64],
                Signature::Variadic(vec![DataType::UInt32, DataType::UInt64]),
                vec![DataType::UInt64, DataType::UInt64],
            )?,
{code}
fails.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-09-03 17:56:51,9
13325930,[Python] Crash in test_parquet.py::test_parquet_writer_filesystem_s3_uri (closing NativeFile from S3FileSystem),"See https://github.com/apache/arrow/pull/7991#discussion_r481247263 and the commented out test added in that PR.

It doesn't give any clarifying traceback or crash message, but it segfaults when closing the {{NativeFile}} returned from {{S3FileSystem.open_output_stream}} in {{ParquetWriter.close()}}.

With {{gdb}} I get a bit more context:

{code}
Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
0x00007fa1a39df8f2 in arrow::fs::(anonymous namespace)::ObjectOutputStream::UploadPart (this=0x5619a95ce820, data=0x7fa197641ec0, nbytes=15671, owned_buffer=...) at ../src/arrow/filesystem/s3fs.cc:806
806       client_->UploadPartAsync(req, handler);
{code}

Another S3 crash in the parquet tests: ARROW-9814 (although it doesn't seem fully related)",filesystem pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-09-03 13:43:28,2
13325811,[Rust] [DataFusion] Add support for array(),"`array` is a function that takes an arbitrary number of columns and returns a fixed-size array with their values.

This function is notoriously difficult to implement because it receives an arbitrary number of arguments or arbitrary but common types, but it is also useful for grouping values together, e.g. to pass to a UDF.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-09-02 21:12:49,9
13325786,[C++] Add hand-crafted Parquet to Arrow reconstruction test for nested reading,"We should write tests where definition and repetition levels are explicitly written out for a particular Parquet schema, then read as a Arrow column.

Sketch here:
https://gist.github.com/pitrou/282dd790cac0eb2c1b59e8c9ab1941d8",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-09-02 17:54:32,2
13325755,[Rust][DataFusion] Use Arc<> instead of Box<> in LogicalPlan,The idea is to continue to simplify the code and improve performance: the inputs to nodes are often copied and using Box requires unnecessary deep copies,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Minor,2020-09-02 14:52:15,11
13325753,[Rust] [DataFusion] Switch from Box<Schema> --> SchemaRef (Arc<Schema>) to be consistent with the rest of Arrow,"The idea is to  use SchemaRef (which is an Arc<Schema>) instead of Box<Schema> inside Datafusion to be consistent with the rest of the arrow implementation, avoid so many copies, and make the code simpler.

",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Minor,2020-09-02 14:44:41,11
13325509,[Python] Bindings for writing datasets to Parquet,"Added to C++ in ARROW-9646, follow-up on Python bindings of ARROW-9658",pull-request-available,['Python'],ARROW,Improvement,Major,2020-09-01 11:45:35,5
13325463,[Rust] [DataFusion] Add support for concat,"So that we can concatenate strings together.

{{pub fn concat(args: Vec<Expr>) -> Expr}}

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-09-01 06:56:36,9
13325455,[Rust] [DataFusion] Make math functions support f32,"Given a math function `g`, we compute g(f32) using g(cast(f32 AS f64)).

The goal of this issue is to make the operation be cast(g(f32) AS f64) instead.

Since computations on f32 are faster than on f64, this is a simple optimization.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-09-01 06:00:18,9
13325375,[R] Add zstandard compression codec in macOS build,"I am using the default macOS build of R arrow 1.0.1 (R 4.0.2) and it doesn't support zstandard/zstd for compression:
{code:r}
> arrow::write_parquet(cars, '~/Downloads/cars.parquet', compression = 'zstd')
Error in parquet___arrow___FileWriter__WriteTable(self, table, chunk_size) : 
  NotImplemented: ZSTD codec support not built
> arrow::codec_is_available('zstd')
[1] FALSE
{code}
Like ARROW-6960 which adds the lz4/zstd support in Windows, It'd be a great to have the zstd support by default in macOS as well.

I don't know if I have the right knowledge to add such support, but let me know how I can help. Thank you for making this great package!",pull-request-available,['R'],ARROW,Improvement,Minor,2020-08-31 17:32:58,4
13325330,"[Rust][DataFusion] Datafusion CLI: CREATE EXTERNAL TABLE errors with ""Unsupported logical plan variant""","When I try to make an external table using the DataFusion CLI I get an error:

h3. Reproducer:

# Check out master
# Build via {{cd arrow/rust; cargo run --bin datafusion-cli}}
# run this query: {{create external table test(c1 boolean) stored as CSV location '/tmp/foo'}}

*Expected Result*: An external table is created successfully

*Actual Result*: An error is reported:

{code}
>    create external table test(c1 boolean) stored as CSV location '/tmp/foo';
General(""Unsupported logical plan variant"")
>
{code}
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-31 12:03:33,11
13325321,[Rust] [DataFusion] ExecutionContext can not be shared between threads,"As suggested by Jorge on  https://github.com/apache/arrow/pull/8079

The high level idea is to allow ExecutionContext on multi-threaded environments such as Python.

The two use-cases:

1. when a project is planning a complex number of plans that depend on a common set of sources and UDFs, it would be nice to be able to multi-thread the planning. This is particularly important when planning requires reading remote metadata to formulate themselves (e.g. when the source is in s3 with many partitions). Metadata reading is often slow and network bounded, which makes threads suitable for these workloads. If multi-threading is not possible, either each plan needs to read the metadata independently (one context per plan) or planning must be sequential (with lots of network waiting).

2. when creating bindings to programming languages that support multi-threading, it would be nice for the ExecutionContext to be thread safe, so that we can more easily integrate with those languages.

The code might look like:
{code}
alamb@MacBook-Pro rust % git diff
diff --git a/rust/datafusion/src/execution/context.rs b/rust/datafusion/src/execution/context.rs
index 5f8aa342e..7374b0a78 100644
--- a/rust/datafusion/src/execution/context.rs
+++ b/rust/datafusion/src/execution/context.rs
@@ -460,7 +460,7 @@ mod tests {
     use arrow::array::{ArrayRef, Int32Array};
     use arrow::compute::add;
     use std::fs::File;
-    use std::io::prelude::*;
+    use std::{sync::Mutex, io::prelude::*};
     use tempdir::TempDir;
     use test::*;
 
@@ -928,6 +928,28 @@ mod tests {
         Ok(())
     }
 
+    #[test]
+    fn send_context_to_threads() -> Result<()> {
+        // ensure that ExecutionContext's can be read by multiple threads concurrently
+        let tmp_dir = TempDir::new(""send_context_to_threads"")?;
+        let partition_count = 4;
+        let mut ctx = Arc::new(Mutex::new(create_ctx(&tmp_dir, partition_count)?));
+
+        let threads: Vec<JoinHandle<Result<_>>> = (0..2)
+            .map(|_| { ctx.clone() })
+            .map(|ctx_clone| thread::spawn(move || {
+                let ctx = ctx_clone.lock().expect(""Locked context"");
+                // Ensure we can create logical plan code on a separate thread.
+                ctx.create_logical_plan(""SELECT c1, c2 FROM test WHERE c1 > 0 AND c1 < 3"")
+            }))
+            .collect();
+
+        for thread in threads {
+            thread.join().expect(""Failed to join thread"")?;
+        }
+        Ok(())
+    }
+
     #[test]
     fn scalar_udf() -> Result<()> {
         let schema = Schema::new(vec![
{code}


At the moment, Rust refuses to compile this example (and also refuses to share ExecutionContexts between threads) due to the following (namely that there are several `dyn` objects that are also not marked as Send + Sync:

{code}
   Compiling datafusion v2.0.0-SNAPSHOT (/Users/alamb/Software/arrow/rust/datafusion)
error[E0277]: `(dyn execution::physical_plan::PhysicalPlanner + 'static)` cannot be sent between threads safely
   --> datafusion/src/execution/context.rs:940:30
    |
940 |             .map(|ctx_clone| thread::spawn(move || {
    |                              ^^^^^^^^^^^^^ `(dyn execution::physical_plan::PhysicalPlanner + 'static)` cannot be sent between threads safely
    | 
   ::: /Users/alamb/.rustup/toolchains/nightly-2020-04-22-x86_64-apple-darwin/lib/rustlib/src/rust/src/libstd/thread/mod.rs:616:8
    |
616 |     F: Send + 'static,
    |        ---- required by this bound in `std::thread::spawn`
    |
    = help: the trait `std::marker::Send` is not implemented for `(dyn execution::physical_plan::PhysicalPlanner + 'static)`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<(dyn execution::physical_plan::PhysicalPlanner + 'static)>`
    = note: required because it appears within the type `std::option::Option<std::sync::Arc<(dyn execution::physical_plan::PhysicalPlanner + 'static)>>`
    = note: required because it appears within the type `execution::context::ExecutionConfig`
    = note: required because it appears within the type `execution::context::ExecutionContextState`
    = note: required because it appears within the type `execution::context::ExecutionContext`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Mutex<execution::context::ExecutionContext>`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>`
    = note: required because it appears within the type `[closure@datafusion/src/execution/context.rs:940:44: 944:14 ctx_clone:std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>]`

error[E0277]: `(dyn execution::physical_plan::PhysicalPlanner + 'static)` cannot be shared between threads safely
   --> datafusion/src/execution/context.rs:940:30
    |
940 |             .map(|ctx_clone| thread::spawn(move || {
    |                              ^^^^^^^^^^^^^ `(dyn execution::physical_plan::PhysicalPlanner + 'static)` cannot be shared between threads safely
    | 
   ::: /Users/alamb/.rustup/toolchains/nightly-2020-04-22-x86_64-apple-darwin/lib/rustlib/src/rust/src/libstd/thread/mod.rs:616:8
    |
616 |     F: Send + 'static,
    |        ---- required by this bound in `std::thread::spawn`
    |
    = help: the trait `std::marker::Sync` is not implemented for `(dyn execution::physical_plan::PhysicalPlanner + 'static)`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<(dyn execution::physical_plan::PhysicalPlanner + 'static)>`
    = note: required because it appears within the type `std::option::Option<std::sync::Arc<(dyn execution::physical_plan::PhysicalPlanner + 'static)>>`
    = note: required because it appears within the type `execution::context::ExecutionConfig`
    = note: required because it appears within the type `execution::context::ExecutionContextState`
    = note: required because it appears within the type `execution::context::ExecutionContext`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Mutex<execution::context::ExecutionContext>`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>`
    = note: required because it appears within the type `[closure@datafusion/src/execution/context.rs:940:44: 944:14 ctx_clone:std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>]`

error[E0277]: `(dyn datasource::datasource::TableProvider + 'static)` cannot be sent between threads safely
   --> datafusion/src/execution/context.rs:940:30
    |
940 |             .map(|ctx_clone| thread::spawn(move || {
    |                              ^^^^^^^^^^^^^ `(dyn datasource::datasource::TableProvider + 'static)` cannot be sent between threads safely
    | 
   ::: /Users/alamb/.rustup/toolchains/nightly-2020-04-22-x86_64-apple-darwin/lib/rustlib/src/rust/src/libstd/thread/mod.rs:616:8
    |
616 |     F: Send + 'static,
    |        ---- required by this bound in `std::thread::spawn`
    |
    = help: the trait `std::marker::Send` is not implemented for `(dyn datasource::datasource::TableProvider + 'static)`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>`
    = note: required because it appears within the type `(std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>)`
    = note: required because of the requirements on the impl of `std::marker::Send` for `hashbrown::raw::RawTable<(std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>)>`
    = note: required because it appears within the type `hashbrown::map::HashMap<std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>, std::collections::hash_map::RandomState>`
    = note: required because it appears within the type `std::collections::HashMap<std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>>`
    = note: required because it appears within the type `execution::context::ExecutionContextState`
    = note: required because it appears within the type `execution::context::ExecutionContext`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Mutex<execution::context::ExecutionContext>`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>`
    = note: required because it appears within the type `[closure@datafusion/src/execution/context.rs:940:44: 944:14 ctx_clone:std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>]`

error[E0277]: `(dyn datasource::datasource::TableProvider + 'static)` cannot be shared between threads safely
   --> datafusion/src/execution/context.rs:940:30
    |
940 |             .map(|ctx_clone| thread::spawn(move || {
    |                              ^^^^^^^^^^^^^ `(dyn datasource::datasource::TableProvider + 'static)` cannot be shared between threads safely
    | 
   ::: /Users/alamb/.rustup/toolchains/nightly-2020-04-22-x86_64-apple-darwin/lib/rustlib/src/rust/src/libstd/thread/mod.rs:616:8
    |
616 |     F: Send + 'static,
    |        ---- required by this bound in `std::thread::spawn`
    |
    = help: the trait `std::marker::Sync` is not implemented for `(dyn datasource::datasource::TableProvider + 'static)`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>`
    = note: required because it appears within the type `(std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>)`
    = note: required because of the requirements on the impl of `std::marker::Send` for `hashbrown::raw::RawTable<(std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>)>`
    = note: required because it appears within the type `hashbrown::map::HashMap<std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>, std::collections::hash_map::RandomState>`
    = note: required because it appears within the type `std::collections::HashMap<std::string::String, std::sync::Arc<(dyn datasource::datasource::TableProvider + 'static)>>`
    = note: required because it appears within the type `execution::context::ExecutionContextState`
    = note: required because it appears within the type `execution::context::ExecutionContext`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Mutex<execution::context::ExecutionContext>`
    = note: required because of the requirements on the impl of `std::marker::Send` for `std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>`
    = note: required because it appears within the type `[closure@datafusion/src/execution/context.rs:940:44: 944:14 ctx_clone:std::sync::Arc<std::sync::Mutex<execution::context::ExecutionContext>>]`

   Compiling arrow-benchmarks v2.0.0-SNAPSHOT (/Users/alamb/Software/arrow/rust/benchmarks)
error: aborting due to 4 previous errors

For more information about this error, try `rustc --explain E0277`.
error: could not compile `datafusion`.

To learn more, run the command again with --verbose.
warning: build failed, waiting for other jobs to finish...
error: build failed
{code}
",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Minor,2020-08-31 10:39:41,11
13325116,[Rust] [DataFusion] Simplify code to test cast,"We have 3 tests with similar functionality, but that only vary on the types they test. Let's create a macro to apply to all of them, so that the tests are equivalent and DRY.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Trivial,2020-08-29 06:04:55,9
13325108,[Rust] [DataFusion] Simplify code of type coercion for binary types,"The function `numerical_coercion` only uses the operator `op` for its error formatting. But the function's intent can be simply generalized to ""coerce two types to numerically equivalent types"".",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Task,Trivial,2020-08-29 04:08:09,9
13325086,[R] Bindings for writing datasets to Parquet,Depends on ARROW-9646,dataset pull-request-available,['R'],ARROW,New Feature,Major,2020-08-28 22:17:56,4
13325084,[R] Fix linuxlibs.R install script for R < 3.6,See https://github.com/apache/arrow/issues/8062,pull-request-available,['R'],ARROW,Bug,Major,2020-08-28 21:42:11,4
13325005,[Python]ChunkedArray.__getitem__ doesn't work with numpy scalars,"

{{import pyarrow as pa
import numpy as np
pa.chunked_array(pa.array([1,2]))[np.int32(0)]}}

fails with error{{TypeError: key must either be a slice or integer}}.",pull-request-available,['Python'],ARROW,Bug,Major,2020-08-28 09:41:57,8
13324954,[Python] table to_pandas self_destruct=True + split_blocks=True cannot prevent doubling memory,"Test on: pyarrow 1.0.1, system: Ubuntu 16.04, python3.7



Reproduce code:

Generate about 800MB data first.
{code:java}
import pyarrow as pa

# generate about 800MB data
data = [pa.array([10]* 1000)]
batch = pa.record_batch(data, names=['f0'])
with open('/tmp/t1.pa', 'wb') as f1:
	writer = pa.ipc.new_stream(f1, batch.schema)
	for i in range(100000):
		writer.write_batch(batch)
	writer.close()
{code}
Test to_pandas with self_destruct=True, split_blocks=True, use_threads=False
{code:python}
import pyarrow as pa
import time
import sys

import os
pid = os.getpid()
print(f'run `psrecord {pid} --plot /tmp/t001.png` and then press ENTER.')
sys.stdin.readline()

with open('/tmp/t1.pa', 'rb') as f1:
	reader = pa.ipc.open_stream(f1)
	batches = [b for b in reader]

pa_table = pa.Table.from_batches(batches)
del batches
time.sleep(3)
pdf = pa_table.to_pandas(self_destruct=True, split_blocks=True, use_threads=False)
del pa_table
time.sleep(3)
{code}
The attached file is psrecord profiling result.",pull-request-available,"['Documentation', 'Python']",ARROW,Bug,Major,2020-08-28 03:41:00,0
13324874,[CI][C++] Travis ARM jobs timeout,"The ARM job on Travis-CI often gets close to the 50 minutes timeout, and sometimes it triggers the timeout. We should try to make the build lighter.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Critical,2020-08-27 16:19:36,2
13324864,[Python] Let FileSystem.get_file_info accept a single path,Currently you need to do {{fs.get_file_info([path])[0]}} to get the info of a single path. We can make the function also accept that directly (instead of a list): {{fs.get_file_info(path)}}.,pull-request-available,['Python'],ARROW,Improvement,Major,2020-08-27 15:10:29,5
13324849,[C++] NewStreamWriter / NewFileWriter don't own output stream,Also the naming doesn't follow usual conventions for factories (e.g. {{MakeStreamWriter}}).,pull-request-available,['C++'],ARROW,Bug,Minor,2020-08-27 13:54:55,2
13324723,[R] Friendly interface for filesystems (S3),"The Filesystem methods don't provide a human-friendly interface for basic operations like ls, mkdir, etc. Since we provide access to S3 and potentially other cloud storage, it would be nice to have simple methods for exploring it.

Additional ideas:

* S3Bucket class/constructor: it's basically a SubTreeFileSystem containing S3FS and a path, except that we can auto-detect a bucket's region.
* Add a class like the FileLocator C++ struct list(fs, path). _also_ kinda like a SubTreeFileSystem, but with different methods and intents. Aside from use in ls/mkdir/cp, it could be used in file reader/writers instead of having an extra {{filesystem}} argument added everywhere, e.g. {{fs$path(""path/to/file"")}}. See https://github.com/apache/arrow/pull/8197#discussion_r494325934",pull-request-available,['R'],ARROW,New Feature,Major,2020-08-26 22:04:22,4
13324722,[R] Implement full S3FileSystem/S3Options constructor,"Currently you can access data in S3 by URI, which can include auth and other query parameters (ARROW-9854), but you can't directly construct an S3Options object.",pull-request-available,['R'],ARROW,New Feature,Major,2020-08-26 22:01:09,4
13324716,[C++] Provide utility for copying files between filesystems,"{{CopyStream}} in arrow/filesystem/util_internal.h does this, but we should expose it, multithread it (can read in one thread while the other thread writes), and further see if there are filesystem-specific optimizations (e.g. S3 multipart uploading/downloading). We may also want a version that takes a FileSelector or vector of paths and parallelizes the operations on them.

",filesystem pull-request-available s3,['C++'],ARROW,New Feature,Major,2020-08-26 21:38:53,6
13324664,[Python] pathlib.Path not supported in write_to_dataset with partition columns,"Copying over from https://github.com/pandas-dev/pandas/issues/35902


{code:python}
import pathlib

df = pd.DataFrame({'A':[1,2,3,4], 'B':'C'})

df.to_parquet('tmp_path1.parquet')  # OK
df.to_parquet(pathlib.Path('tmp_path2.parquet'))  # OK

df.to_parquet('tmp_path3.parquet', partition_cols=['B'])  # OK
df.to_parquet(pathlib.Path('tmp_path4.parquet'), partition_cols=['B'])  # TypeError
{code}

{{to_parquet}} method raises TypeError when using {{pathlib.Path()}} as an argument in case when `partition_cols` argument is not None. If no partition cols are provided, then {{pathlib.Path()}} is properly accepted

{code}
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-53-cae5a944d982> in <module>
      3 
      4 df.to_parquet('tmp_path3.parquet', partition_cols=['B']) # OK
----> 5 df.to_parquet(pathlib.Path('tmp_path4.parquet'), partition_cols=['B'])  # TypeError
...

~/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py in write_to_dataset(table, root_path, partition_cols, partition_filename_cb, filesystem, **kwargs)
   1790             subtable = pa.Table.from_pandas(subgroup, schema=subschema,
   1791                                             safe=False)
-> 1792             _mkdir_if_not_exists(fs, '/'.join([root_path, subdir]))
   1793             if partition_filename_cb:
   1794                 outfile = partition_filename_cb(keys)

TypeError: sequence item 0: expected str instance, PosixPath found
{code}",parquet pull-request-available,['Python'],ARROW,Bug,Major,2020-08-26 15:04:34,5
13324654,[C++] [PARQUET] Optimize meta data recovery of ApplicationVersion,"The class contains two large regexes which are compiled in the ApplicationVersion::ApplicationVersion(const std:::string) constructor. This is the constructor that is used when reading files.

I stopped a server in gdb that had been processing several files at once and 4 threads out of 8 were building those regexes!",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-08-26 13:22:29,2
13324553,[C++] S3 FileSystemFromUri with special char in secret key fails,"S3 Secret access keys can contain special characters like {{/}}. When they do

1) FileSystemFromUri will fail to parse the URI unless you URL-encode them (e.g. replace / with %2F)
2) When you do escape the special characters, requests that require authorization fail with the message ""The request signature we calculated does not match the signature you provided. Check your key and signing method."" This may suggest that there's some extra URL encoding/decoding that needs to happen inside.

I was only able to work around this by generating a new access key that happened not to have special characters.",pull-request-available,"['C++', 'Documentation', 'Python']",ARROW,Bug,Major,2020-08-25 20:42:15,2
13324548,[C++][Python][Docs] Expand user guide for FileSystem,"https://arrow.apache.org/docs/python/filesystems.html is pretty thin

https://arrow.apache.org/docs/python/api/filesystems.html doesn't mention S3

and in general there are some tricks to getting FileSystemFromUri to work",pull-request-available,"['C++', 'Documentation', 'Python']",ARROW,New Feature,Major,2020-08-25 20:28:12,5
13324530,[R] Add bindings for string compute functions,"See https://arrow.apache.org/docs/cpp/compute.html#string-predicates and below. Since R's base string functions, as well as stringr/stringi, aren't generics that we can define methods for, this will probably make most sense within the context of a dplyr expression where we have more control over the evaluation.

This will require enabling utf8proc in the builds; there's already an rtools-package for it.",pull-request-available,['R'],ARROW,New Feature,Major,2020-08-25 18:19:59,4
13324528,[R] Fix bad merge/Rcpp conflict,"ARROW-8001 merged after the switch to cpp11 but was based on master before it, so that brought some generated code that still referenced Rcpp",pull-request-available,['R'],ARROW,Bug,Major,2020-08-25 18:11:57,4
13324526,[R] Support reading/writing data to/from S3,"Current S3 support is limited to (1) being able to instantiate an S3FileSystem object, primarily from a URI, and (2) ability to open_dataset from an S3 URI. Before widely declaring that we support S3 in R, we should be able to:

* download dataset (i.e. copy files/directory recursively)
* read_parquet/feather/etc. from S3 (use FileSystem->OpenInputFile(path))
* write_$FORMAT via FileSystem->OpenOutputStream(path)
* write_dataset
* for linux, an argument to install_arrow to help, assuming you've installed aws-sdk-cpp already (turn on ARROW_S3, AWSSDK_SOURCE=SYSTEM)
* testing with minio on CI
* set up a real test bucket and user for e2e testing
* update docs and vignettes",pull-request-available,['R'],ARROW,New Feature,Major,2020-08-25 18:05:32,4
13324472,[C++] Valgrind errors due to unrecognized instructions,"Valgrind seems to barf on AVX512 instructions:

https://github.com/ursa-labs/crossbow/runs/1025065792",pull-request-available,['C++'],ARROW,Bug,Major,2020-08-25 12:56:14,2
13324396,[Rust] [DataFusion] Make UDFs not need a Field,"[https://github.com/apache/arrow/pull/7967,] shows that it is possible to not require users to pass a `Field` to UDFs declarations and instead just pass a `DataType`.

Let's deprecate Field from them, and instead just use `DataType`.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-08-25 06:57:13,9
13324376,[Rust] Implement changes to ensure flatbuffer alignment,"See ARROW-6313, changes were made to all IPC implementations except for Rust",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-08-25 04:09:58,12
13324167,[Rust] [DataFusion] Add ability to downcast ExecutionPlan to specific operator,"Sometimes it is necessary to have operator-specific logic in a query optimizer, so we need the ability to get an Any reference to an ExecutionPlan so we can downcast it.

We do something very similar in PrimitiveArray already with an as_any method but the difference there is we can call get_type first to know exactly what type to cast to and we don't have anything like that in ExecutionPlan, but we could at least speculatively try casting to specific operators.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-23 22:42:25,10
13324153,[Rust] [DataFusion] Improve API for usage of UDFs,"TL;DR; currently, users call UDFs through
 
 {color:#000000}df.select(scalar_functions(sqrt, vec![col(a)], DataType::Float64)){color}
 
 Proposal:
 
 {color:#000000}let f = df.registry();{color}

{color:#000000}df.select(f.udf(sqrt, vec![col(a)])?){color}
 
 so that they do not have to remember the UDFs return type when using it.
 
 This API will in the future allow to declare the UDF as part of the planning, like spark, instead of having to register it in the registry before using it (we just need to check if the UDF is registered or not before doing so).
 See complete proposal here: [https://docs.google.com/document/d/1Kzz642ScizeKXmVE1bBlbLvR663BKQaGqVIyy9cAscY/edit?usp=sharing]

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-23 15:11:13,9
13324142,[Rust] [DataFusion] Remove FunctionMeta,"Currently, our code works as follows:

1. Declare a UDF via {{udf::ScalarFunction}}
 2. Register the UDF
 3. call {{scalar_function(""name"", vec![...], type)}} to use it during planning.

However, during planning, we:

1. Get the ScalarFunction by name
 2. convert it to FunctionMetadata
 3. get the ScalarFunction associated with FunctionMetadata's name

I.e. {{FunctionMetadata}} does not seem to be needed.

Goal: remove {{FunctionMetadata}} and just pass {{udf::ScalarFunction}} directly from the registry to the physical plan.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-23 11:38:18,9
13324110,[Rust] [DataFusion] Refactor TableProvider.scan to return ExecutionPlan,Refactor TableProvider.scan to return ExecutionPlan instead of Vec<Partition> in preparation for removing Partition trait.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-22 20:19:39,10
13324078,[Python] pandas.read_parquet fails for wide parquet files and pyarrow 1.0.X,"I recently tried to update my pyarrow from 0.17.1 to 1.0.0 and I'm encountering a serious bug where wide DataFrames fail during pandas.read_parquet. Small parquet files (m=10000) read correctly, medium files (m=40000) fail with a ""Bus Error: 10"", and large files (m=100000) completely hang. I've tried python 3.8.5, pandas 1.0.5, pyarrow 1.0.0, and OSX 10.14.

The driver code and output is below:
{code:python}
import pandas as pd
import numpy as np
import sys

filename = ""test.parquet""
n = 10
m = int(sys.argv[1])
print(m)
x = np.zeros((n, m))
x = pd.DataFrame(x, columns=[f""A_{i}"" for i in range(m)])
x.to_parquet(filename)
y = pd.read_parquet(filename, engine='pyarrow')
{code}
{code:java}
time python test_pyarrow.py 10000
real 0m4.018s user 0m5.286s sys 0m0.514s
time python test_pyarrow.py 40000
40000
Bus error: 10
{code}


On a pyarrow 0.17.1 environment, the 40,000 case completes in 8 seconds.

This was cross-posted on the pandas tracker as well:[https://github.com/pandas-dev/pandas/issues/35846]",pull-request-available,['Python'],ARROW,Bug,Major,2020-08-22 15:02:42,5
13324038,[CI][C++][MinGW] Enable S3,[,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2020-08-21 22:13:58,1
13324033,[Rust][DataFusion] User Defined PlanNode / Operator API,"The basic goal is to  allow users to implement their own PlanNodes. I will provide a google doc opened for comments shortly.

Proposal: https://docs.google.com/document/d/1IHCGkCuUvnE9BavkykPULn6Ugxgqc1JShT4nz1vMi7g/edit#

See also mailing list discussion here: https://lists.apache.org/thread.html/rf8ae7d1147e93e3f6172bc2e4fa50a38abcb35f046cc5830e09da6cc%40%3Cdev.arrow.apache.org%3E
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-08-21 21:01:58,11
13323980,[C++] Bump mimalloc to 1.6.4,"1.6.4 includes a fix for https://github.com/microsoft/mimalloc/issues/277, which we need to enable it in the R Windows CRAN packages.",pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2020-08-21 15:25:10,4
13323824,[Rust] [DataFusion] Deadlock in creation of physical plan with two udfs,"This one took me some time to understand, but I finally have a reproducible example: when two udfs are called, one after the other, we cause a deadlock when creating the physical plan.

Example test

{code}
#[test]
fn csv_query_sqrt_sqrt() -> Result<()> {
    let mut ctx = create_ctx()?;
    register_aggregate_csv(&mut ctx)?;
    let sql = ""SELECT sqrt(sqrt(c12)) FROM aggregate_test_100 LIMIT 1"";
    let actual = execute(&mut ctx, sql);
    // sqrt(sqrt(c12=0.9294097332465232)) = 0.9818650561397431
    let expected = ""0.9818650561397431"".to_string();
    assert_eq!(actual.join(""\n""), expected);
    Ok(())
}
{code}

I believe that this is due to the recursive nature of the physical planner, that locks scalar_functions within a match, which blocks the whole thing.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-20 19:46:40,9
13323797,[Python] Crash in test_parquet.py::test_read_partitioned_directory_s3fs,"This seems to happen with some Minio versions, but is definitely a problem in Arrow.
The crash message says:
{code}
pyarrow/tests/test_parquet.py::test_read_partitioned_directory_s3fs[False] ../src/arrow/dataset/discovery.cc:188:  Check failed: relative.has_value() GetFileInfo() yielded path outside selector.base_dir
{code}

The underlying problem is that we pass a full URI for the selector base_dir (such as ""s3://bucket/path."") and the S3 filesystem implementation then returns regular paths (such as ""bucket/path/foo/bar"").

I think we should do two things:
1) error out rather than crash (and include the path strings in the error message), which would be more user-friendly
2) fix the issue that full URIs are passed in base_dir",pull-request-available,"['C++', 'Python']",ARROW,Bug,Critical,2020-08-20 16:07:52,2
13323793,[C++] Disable semantic interposition,"On gcc, semantic interposition is enabled by default. It can be beneficial to disable it when building Arrow libraries (and it's most certainly harmless anyway).

See https://stackoverflow.com/questions/35745543/new-option-in-gcc-5-3-fno-semantic-interposition for more background on this.",pull-request-available,['C++'],ARROW,Wish,Trivial,2020-08-20 15:53:11,2
13323719,[C++] Unchecked floating point division by 0 should succeed,"Currently we return a ""division by zero"" error even for unchecked floating point division. Instead we should simply succeed and store an infinite (or NaN if 0/0 is requested).",pull-request-available,['C++'],ARROW,Wish,Major,2020-08-20 11:39:14,7
13323681,[C++][Parquet] Generalize existing null bitmap generation ,"Right now null bitmap generation assumes only list nesting. Generalize and refactor exisitn code without changing existing functionality to accept additional parameters to support arrow nested types:



1. Repeated ancestor def level

2. Null slot usage (for fixed size lists)



",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-08-20 06:38:25,15
13323668,[Rust] [DataFusion] logical schema = physical schema is not true,"In tests/sql.rs, we test that the physical and the optimized schema must match. However, this is not necessarily true for all our queries. An example:
{code:java}
#[test]
fn csv_query_sum_cast() {
    let mut ctx = ExecutionContext::new();
    register_aggregate_csv_by_sql(&mut ctx);
    // c8 = i32; c9 = i64
    let sql = ""SELECT c8 + c9 FROM aggregate_test_100"";
    // check that the physical and logical schemas are equal
    execute(&mut ctx, sql);
}
{code}
The physical expression (and schema) of this operation, after optimization, is {{CAST(c8 as Int64) Plus c9}} (this test fails).

AFAIK, the invariant of the optimizer is that the output types and nullability are the same.

Also, note that the reason the optimized logical schema equals the logical schema is that our type coercer does not change the output names of the schema, even though it re-writes logical expressions. I.e. after the optimization, `.to_field()` of an expression may no longer match the field name nor type in the Plan's schema. IMO this is currently by (implicit?) design, as we do not want our logical schema's column names to change during optimizations, or all column references may point to non-existent columns. This is something that brought up on the mailing list about polymorphism.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-20 06:20:20,9
13323564,"[Rust] [Parquet] ""min"" and ""max"" written to standard out when writing columns","For example the following is printed during local execution of a test:

{code}
min: [0] 1
max: [1] 1
min: [0] 1
max: [1] 1
min: [0] 1
max: [1] 1
min: [0] 1
max: [1] 1
min: [0] 1
max: [1] 1
min: [0] 1
max: [1] 1
{code}

It appears to have been introduced by this PR and was probably vestigial.

https://github.com/apache/arrow/commit/12b30dda1a23bad70e5b11b8cef845d0effd01d4
{code}
commit 12b30dda1a23bad70e5b11b8cef845d0effd01d4
Author: Ze'ev Maor <zeevm@microsoft.com>
Date:   Thu Jul 2 17:14:16 2020 -0700

    ARROW-9280: [Rust] [Parquet] Calculate page and column statistics

    Allow writer to provide pre-calculated stats
{code}",pull-request-available,['Rust'],ARROW,Bug,Minor,2020-08-19 13:37:05,11
13323457,[Rust] [DataFusion] Logical aggregate functions should not return Result,"I introduced a small regression in the DataFrame refactor. The methods min, max, avg, count, sum used in constructing logical plans do not need to return results. Any invalid use will be caught during the creation of the physical plan.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-19 01:13:54,10
13323442,[Rust] [Parquet] ParquetFileArrowReader fails to decode all pages if batches fall exactly on row group boundaries,"When I was reading a parquet file into RecordBatches using {{ParquetFileArrowReader}} that had row groups that were 100,000 rows in length with a batch size of 60,000, after reading 300,000 rows successfully, I started seeing this error

{code}
 ParquetError(""Parquet error: Not all children array length are the same!"")
{code}

Upon investigation, I found that when reading with {{ParquetFileArrowReader}}, if the parquet input file has multiple row groups, and if a batch happens to end at the end of a row group for Int or Float, no subsequent row groups are read

Visually:
{code}
+-----+
| RG1 |
|     |
+-----+  <-- If a batch ends exactly at the end of this row group (page), RG2 is never read
+-----+
| RG2 |
|     |
+-----+
{code}

A reproducer is attached. 20 values should be read by the {{ParquetFileArrowReader}} regardless of the batch size. However, when using batch sizes such as {{5}} or {{3}} (which fall on a boundary between row groups) not all the rows are read. 

To run the reproducer, decompress the attachment  [^parquet_file_arrow_reader.zip] and do `cargo run`

The output is as follows:

{code}
wrote 20 rows in 4 row groups to /tmp/repro.parquet
Size when reading with batch_size 100 : 20
Size when reading with batch_size 7 : 20
Size when reading with batch_size 5 : 5
{code}

The expected output is as follows (should always read 20 rows, regardless of the batch size):
{code}
wrote 20 rows in 4 row groups to /tmp/repro.parquet
Size when reading with batch_size 100 : 20
Size when reading with batch_size 7 : 20
Size when reading with batch_size 5 : 20
{code}

h2. Workaround
Use a different batch size that will not fall on record batch boundaries",pull-request-available,['Rust'],ARROW,Bug,Major,2020-08-18 22:24:04,11
13323436,[C++] Don't install jemalloc in parallel,"On ARROW-6437 (https://github.com/apache/arrow/pull/7928) we saw occasional ""File exists"" errors on jemalloc_ep on macOS. Googling the error message led back to ARROW-739 (https://github.com/apache/arrow/pull/456), which fixed this before by forcing install with {{-j1}}. ARROW-3492 later made it so jemalloc would build (but not install) in parallel. Then ARROW-3539 (https://github.com/apache/arrow/pull/2779) was addressing a problem with that change and, along with fixing the build parallelization issue, unfortunately reverted the original {{make -j1 install}} fix.",pull-request-available,['C++'],ARROW,Bug,Major,2020-08-18 20:59:13,4
13323418,"Handle naming inconsistencies between SQL, DataFrame API and struct names","Currently, we have naming inconsistencies between the different APIs that make it a bit confusing. The typical example atm is 

`df.where().to_plan?.explain()` shows a ""Selection"" in the plan when ""Selection"" in SQL and many other query languages is a projection, not a filter.

Other examples:

```
name: Selection
SQL: WHERE
DF: filter
```

```
name: Aggregation
SQL: GROUP BY
DF: aggregate
```

```
name: Projection
SQL: SELECT
DF: select,select_columns
```

I suggest that we align them with a common notation, preferably aligned with other more common query languages.

I am assigning this to you [~andygrove] as you are probably the only person that can take a decision on this.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-08-18 18:47:38,10
13323378,[Python] pyarrow/tests/test_fs.py::test_s3_options too slow,This single test seems to be taking ~30 seconds here.,pull-request-available,['Python'],ARROW,Bug,Trivial,2020-08-18 15:13:57,2
13323359,[Rust] [DataFusion] Improve instructions for running tpch benchmark,"While trying to ru the TPCH benchmark introduced in https://github.com/apache/arrow/pull/7946/files, I found that the referenced `tpch-dbgen` program did not produce files in the way that the benchmark wanted. 

It would be good to update the code/instructions to be reproducable",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-08-18 13:40:00,11
13323358,[Rust] [DataFusion] Logical aggregate expressions require explicit data type,"When constructing a logical plan either directly or via the DataFrame API, it is not possible to construct an aggregate expression without providing a data type. This makes no sense because the aggregate functions need to determine their own data type.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-18 13:27:28,10
13323345,"[C++][Dataset] Ability to write "".feather"" files with IpcFileFormat","With the new dataset writing bindings, one can do {{ds.write_dataset(data, format=""feather"")}} (Python) or {{write_dataset(data, format = ""feather"")}} (R) to write a dataset to feather files. 

However, because ""feather"" is just an alias for the IpcFileFormat, it will currently write all files with the {{.ipc}} extension.   
I think this can be a bit confusing, since many people will be more familiar with ""feather"" and expect such an extension. 

(more generally, "".ipc"" is maybe not the best default, since it's not very descriptive extension. Something like "".arrow"" might be better?)

cc [~npr] [~bkietz]
",dataset pull-request-available,"['C++', 'Python', 'R']",ARROW,Improvement,Major,2020-08-18 12:06:54,6
13323314,[C++] Fix uninitialized value warnings,The nightly valgrind build show warnings due to unitialized values: [https://github.com/ursa-labs/crossbow/runs/996955686],pull-request-available,['C++'],ARROW,Improvement,Major,2020-08-18 11:05:11,3
13323161,[Rust] [DataFusion] Increase stability of average accumulator,"Currently, our method to compute the average is based on:

1. compute sum of all terms
2. compute count of all terms
3. compute sum / count

however, the sum may overflow.

There is a typical solution to this based on an online formula described e.g. [here|http://www.heikohoffmann.de/htmlthesis/node134.html] to keep the numbers small.
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-18 06:12:17,9
13323152,[Rust] [DataFusion] Logical and physical schemas' nullability does not match in 8 out of 20 end-to-end tests,"In `tests/sql.rs`, if we re-write the ```execute``` function to test the end schemas, as

```
/// Execute query and return result set as tab delimited string
fn execute(ctx: &mut ExecutionContext, sql: &str) -> Vec<String> {
    let plan = ctx.create_logical_plan(&sql).unwrap();
    let plan = ctx.optimize(&plan).unwrap();
    let physical_plan = ctx.create_physical_plan(&plan).unwrap();
    let results = ctx.collect(physical_plan.as_ref()).unwrap();
    if results.len() > 0 {
        // results must match the logical schema
        assert_eq!(plan.schema().as_ref(), results[0].schema().as_ref());
    }
    result_str(&results)
}
```

we end up with 8 tests failing, which indicates that our physical and logical plans are not aligned. In all cases, the issue is nullability: our logical plan assumes nullability = true, while our physical plan may change the nullability field.

If we do not plan to track nullability on the logical level, we could consider replacing Schema by a type that does not track nullability.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-18 04:19:56,10
13323126,[Rust] Implement IPC changes to catch up to 1.0.0 format,"There are a number of IPC changes and features which the Rust implementation has fallen behind on. It's effectively using the legacy format that was released in 0.14.x.

Some that I encountered are:
 * change padding from 4 bytes to 8 bytes (along with the padding algorithm)
 * add an IPC writer option to support the legacy format and updated format
 * add error handling for the different metadata versions, we should support v4+ so it's an oversight to not explicitly return errors if unsupported versions are read

Some of the work already has Jiras open (e.g. body compression), I'll find them and mark them as related to this.

I'm tight for spare time, but I'll try work on this before the next release (along with the Parquet writer)",pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-08-17 23:53:44,12
13323104,[C++] Automatic S3 region selection,"Currently, PyArrow and ArrowCpp need to be provided the region of the S3 file/bucket, else it defaults to using 'us-east-1'. Ideally, PyArrow and ArrowCpp can automatically detect the region and get the files, etc. For instance, s3fs and boto3 can read and write files without having to specify the region explicitly. Similar functionality to auto-detect the region would be great to have in PyArrow and ArrowCpp.",filesystem pull-request-available,"['C++', 'Python']",ARROW,Wish,Major,2020-08-17 20:17:32,2
13323068,[Rust] [DataFusion] Predicate Pushdown Improvement: treat predicates separated by AND separately,"As discussed by [~jorgecarleitao] and [~houqp] here: https://github.com/apache/arrow/pull/7880#pullrequestreview-468057624

If a predicate is a conjunction (aka AND'd) together, each of the clauses can be treated separately (e.g. a single filter expression {{A > 5 And B < 4}} can be broken up and each of {{A > 5}} and {{B < 4}} can be potentially pushed down different levels

The filter pushdown logic works for the following case (when {{a}} and {{b}} are are separate selections, predicate for a is pushed below the {{Aggregate}} in the optimized plan):

{code}
********Original plan:
Selection: #b GtEq Int64(1)
  Selection: #a LtEq Int64(1)
    Aggregate: groupBy=[[#a]], aggr=[[MIN(#b)]]
      TableScan: test projection=None

********Optimized plan:
Selection: #b GtEq Int64(1)
  Aggregate: groupBy=[[#a]], aggr=[[MIN(#b)]]
    Selection: #a LtEq Int64(1)
      TableScan: test projection=None
{code}

But not for this when {{a}} and {{b}} are {{AND}}'d together

{code}
********Original plan:
Selection: #a LtEq Int64(1) And #b GtEq Int64(1)
  Aggregate: groupBy=[[#a]], aggr=[[MIN(#b)]]
    TableScan: test projection=None
********Optimized plan:
Selection: #a LtEq Int64(1) And #b GtEq Int64(1)
  Aggregate: groupBy=[[#a]], aggr=[[MIN(#b)]]
    TableScan: test projection=None
{code}
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-08-17 14:42:19,9
13323057,[Python] Remove skip for in-memory fsspec in test_move_file,Follow-up of https://issues.apache.org/jira/browse/ARROW-9621 which should be applied once a new version of fsspec is going to be available.,pull-request-available,['Python'],ARROW,Improvement,Major,2020-08-17 13:55:06,2
13322988,[Python] Pyarrow allows for unsafe conversions of datetime objects to timestamp nanoseconds,"Hi,

In parquet, I want to store date values as timestamp format with nanoseconds precision. This works fine with most dates except those past pandas.Timestamp.max:[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.max.html.|https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.max.html]

I was expecting some exception to be raised (like in Pandas), however this did not happen and the value was processed incorrectly. Note that this is with safe=True. Can this please be looked into? Thanks

{{Example Code:}}

{{pa.array([datetime(2262,4,12)], type=pa.timestamp(""ns""))}}
 \{{}}

{{Return:}}

{{[}}
 \{{ 1677-09-21 00:25:26.290448384}}
 {{]}}",pull-request-available,['Python'],ARROW,Bug,Minor,2020-08-17 05:08:45,5
13322986,[C++][Parquet] Add EngineVersion to properties to allow for toggling new vs old logic,This will provide an escape hatch in case the new logic some how has unuseable bugs in it.,pull-request-available,['C++'],ARROW,Sub-task,Major,2020-08-17 04:19:56,15
13322958,[Rust] [DataFusion] ExecutionContext::sql should return DataFrame,"ExecutionContext::sql currently returns Vec<RecordBatch> but should return DataFrame instead, then the caller can call collect on the DataFrame.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-16 19:47:57,10
13322956,[C++] Add experimental pull-based iterator structures to C interface implementation,This purpose of this would be to validate some initial use cases / workflows prior to potentially formalizing the interface in the C ABI,pull-request-available,['C++'],ARROW,Improvement,Major,2020-08-16 19:26:29,2
13322953,[Rust] [DataFusion] Implement DataFrame::explain,Implement DataFrame::explain - we already have explain implemented in the SQL API,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-16 19:07:15,9
13322952,[Rust] [DataFusion] Implement DataFrame::sort,Implement DataFrame::sort,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-16 19:06:42,10
13322942,[Rust] [DataFusion] Implement extension API for DataFusion,"I would like the ability to extend DataFusion by providing my own:
 * Logical plan optimization rules
 * Physical query planner for converting logical plan to physical plan

Later, I would also like to be able to provide physical plan optimization rules but we don't have a physical optimizer yet.

These changes would allow people to extend DataFusion easily without implementing their own competing DataFrame APIs.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-16 15:30:48,10
13322939,"[Rust] [DataFusion] Use ""pub use"" to expose a clean public API","We should use ""pub use"" to expose a clean public API for the structs and traits that users will typically interact with, such as ExecutionContext, DataFrame, and functions for creating logical expressions.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-16 14:55:24,10
13322923,[Rust] [DataFusion] Add support for generic return types of scalar UDFs,"Currently, we have math functions declared as UDFs that while they can be evaluated against float32 and float64, their return type is fixed (float64).

This issue is about extending the UDF interface to support generic return types (a function), so that developers (us included), can register UDFs or variable return types.

There is a small overhead in this, since evaluating types now requires a function call, however, IMO this is still very small when compared to anything, as we are talking about plans. We can always apply some caching if needed.
",pull-request-available,[],ARROW,New Feature,Major,2020-08-16 10:34:00,9
13322878,[Rust] [DataFusion] Implement async in DataFusion traits,Implement async in DataFusion traits,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-15 14:13:09,10
13322874,[Rust] [DataFusion] Remove the use of Mutex in ExecutionPlan trait,The ExecutionPlan trait should not return Arc<Mutex<RecordBatchIterator>> but just Arc<RecordBatchIterator> since most operators do not need to be mutable. Those that do can use interior mutability.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-15 13:33:14,9
13322872,[Rust] [DataFusion] Add support for Aggregate UDFs,"This will allow to more easily extend the existing offering of aggregate functions.

The existing functions shall be migrated to this interface.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-08-15 13:16:38,9
13322863,[Rust] [DataFusion] Extend UDFs to accept more than one type per argument,"Most math functions accept float32 and float64, `length` will accept Utf8 and lists soon, etc.

The goal of this story is to allow UDFs to accept more than one datatype.

Design: the accepted datatypes should be a vector ordered by ""faster/smaller"" to ""slower/larger"" (cpu/memory). When the plan reaches a UDF, we try to cast the input expression like before, from ""faster/smaller"" to ""slower/larger"".

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-08-15 10:52:29,9
13322820,[C++][Dataset] Extract format-specific scan options from FileFormat,"Currently format specific scan options are embedded as members of the corresponding subclass of FileFormat. Extracting these to an options struct would provide better separation of concerns; currently the only way to scan a parquet formatted dataset with different options is to reconstruct it in a differently optioned format from its component files.

CsvFileFormat could retain ParseOptions as a member, since (for example) tab-separated vs comma-separated values can justifiably be considered different formats.",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-08-14 21:46:54,0
13322791,[Python] Failed to install on aarch64,"My team is attempting to migrate some workloads from x86-64 to ARM64, a blocker for this is PyArrow failing to install. `pip install pyarrow` fails to build the wheel as -march isn't correctly resolved:

{noformat}
 -- System processor: aarch64
 -- Performing Test CXX_SUPPORTS_ARMV8_ARCH
 -- Performing Test CXX_SUPPORTS_ARMV8_ARCH - Failed
 -- Arrow build warning level: PRODUCTION
 CMake Error at cmake_modules/SetupCxxFlags.cmake:338 (message):
 Unsupported arch flag: -march=.
{noformat}

It's possible to get the build to work after editing `cmake_modules/SetupCxxFlags.cmake` to force ARROW_ARMV8_ARCH_FLAG to end up as an architecture such as 'armv8-a' - although some more elaborate logic is really needed to pick up the correct extensions.

I can see that there have been a number of items discussed in the past both on Jira and in GitHub issues ranging from simple fixes to the cmake script to more elaborate fixes cross-product for arch detection - but I wasn't able to discern how the project wishes to proceed.

With AWS pushing their ARM-based instances heavily at this point I would advocate for picking a direction before an influx of new issues.

",pull-request-available,['Python'],ARROW,Bug,Major,2020-08-14 17:58:20,1
13322785,[R] Sanitize paths in open_dataset,"{{open_dataset(""~/path"")}} currently fails. As everywhere else we pass paths to the filesystem module, we need to normalize the path.",pull-request-available,['R'],ARROW,Bug,Minor,2020-08-14 17:03:22,4
13322758,[Rust] Create one standard DataFrame API,"There was a discussion in last Arrow sync call about the fact that there are numerous Rust DataFrame projects and it would be good to have one standard, in the Arrow repo.

I do think it would be good to have a DataFrame trait in Arrow, with an implementation in DataFusion, and making it possible for other projects to extend/replace the implementation e.g. for distributed compute, or for GPU compute, as two examples.

[~jhorstmann] Does this capture what you were suggesting in the call?",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-08-14 14:02:45,10
13322622,[Rust] [DataFusion] TableProvider.scan executing partitions prematurely,"TableProvider.scan was immediately executing all partitions in parallel instead of just returning the partitions and letting the caller execute them.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-13 22:39:06,10
13322600,[Rust][DataFusion] Aggregates COUNT/MIN/MAX don't work on VARCHAR columns,"h2. Reproducer:

Create a table with a string column:

Repro:
{code}
CREATE EXTERNAL TABLE repro(a INT, b VARCHAR)
STORED AS CSV
WITH HEADER ROW
LOCATION 'repro.csv';
{code}


The contents of repro.csv are as follows (also attached):
{code}
a,b
1,One
1,Two
2,One
2,Two
2,Two
{code}


Now, run a query that tries to aggregate that column:
{code}
select a, count(b) from repro group by a;
{code}


*Actual behavior*:
{code}
> select a, count(b) from repro group by a;
ArrowError(ExternalError(ExecutionError(""Unsupported data type Utf8 for result of aggregate expression"")))
{code}

*Expected Behavior*:
The query runs and produces results
{code}
a, count(b)
1,2
2,3
{code}

h2. Discussion

Using Min/Max aggregates on varchar also doesn't work (but should):

{code}

> select a, min(b) from repro group by a;
ArrowError(ExternalError(ExecutionError(""Unsupported data type Utf8 for result of aggregate expression"")))
> select a, max(b) from repro group by a;
ArrowError(ExternalError(ExecutionError(""Unsupported data type Utf8 for result of aggregate expression"")))
{code}


Fascinatingly these formulations work fine:

{code}
> select a, count(a) from repro group by a;
+---+----------+
| a | count(a) |
+---+----------+
| 2 | 3        |
| 1 | 2        |
+---+----------+
2 row in set. Query took 0 seconds.
> select a, count(1) from repro group by a;
+---+-----------------+
| a | count(UInt8(1)) |
+---+-----------------+
| 2 | 3               |
| 1 | 2               |
+---+-----------------+
2 row in set. Query took 0 seconds.
{code}
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-13 19:32:41,9
13322594,"[C++][Dataset] Port ""head"" method from R to C++ Dataset Scanner","ARROW-9665 (https://github.com/apache/arrow/pull/7913) added amongst other things a {{head}} method for Dataset in R:

https://github.com/apache/arrow/blob/586c060c8b1851f1077911fae6d02a10ed83e7fb/r/src/dataset.cpp#L266-L282

It might be nice to move this to C++ and expose it on the python side as well (and since it's written already in C++ on the R side, it should be relatively straightforward to port I assume)",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-08-13 18:47:24,0
13322574,[Rust] [Parquet] Compute nested definition and repetition for structs,"When computing definition levels for deeply nested arrays that include lists, the definition levels are correctly calculated, but they are not translated into correct indexes for the eventual primitive arrays.

For example, an int32 array could have no null values, but be a child of a list that has null values. If say the first 5 values of the int32 array are members of the first list item (i.e. list_array[0] = [1,2,3,4,5], and that list is itself a child of a struct whose index is null, the whole 5 values of the int32 array *should* be skipped. Further, the list's definition and repetition levels will be represented by 1 slot instead of the 5.

The current logic cannot cater for this, and potentially results in slicing the int32 array incorrectly (sometimes including some of those first 5 values).

This Jira is for the work necessary to compute the index into the eventual leaf arrays correctly.

I started doing it as part of the initial writer PR, but it's complex and is blocking progress.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-08-13 16:43:43,12
13322559,[Rust] [DataFusion] ParquetScanExec launches threads too early,"ParquetScanExec launches threads in partitions() ahead of execute() being called on those partitions. This results on ""too many open files"" when there are many partitions.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-13 14:59:48,10
13322556,[Rust] [DataFusion] LimitExec and SortExec should use MergeExec,"Both LimitExec and SortExec need to collapse inputs down to a single partition and they both contain similar code for executing their inputs on threads and combining the results. This is exactly what the MergeExec operator does, so they should just use that instead.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-13 14:40:00,10
13322506,[Packaging][Python] Update wheel dependency files,Should not include pandas and keras peprocessing.,pull-request-available,"['Packaging', 'Python']",ARROW,New Feature,Major,2020-08-13 09:35:11,3
13322504,[Python] Long-term fate of pyarrow.parquet.ParquetDataset,"The business logic of the python implementation of reading partitioned parquet datasets in {{pyarrow.parquet.ParquetDataset}} has been ported to C++ (ARROW-3764), and has also been optionally enabled in ParquetDataset(..) by using {{use_legacy_dataset=False}} (ARROW-8039).

But the question still is: what do we do with this class long term? 

So for users who now do:

{code}
dataset = pq.ParquetDataset(...)
dataset.metadata
table = dataset.read()
{code}

what should they do in the future?  
Do we keep a class like this (but backed by the pyarrow.dataset implementation), or do we deprecate the class entirely, pointing users to `dataset = ds.dataset(..., format=""parquet"")` ?

In any case, we should strive to entirely delete the current custom python implementation, but we could keep a {{ParquetDataset}} class that wraps or inherits {{pyarrow.dataset.FileSystemDataset}} and adds some parquet specifics to it (eg access to the parquet schema, the common metadata, exposing the parquet-specific constructor keywords more easily, ..). 

Features the {{ParquetDataset}} currently has that are not exactly covered by pyarrow.dataset:

- Partitioning information (the {{.partitions}} attribute
- Access to common metadata ({{.metadata_path}}, {{.common_metadata_path}} and {{.metadata}} attributes)
- ParquetSchema of the dataset
",dataset-parquet-read,['Python'],ARROW,Improvement,Major,2020-08-13 09:33:56,5
13322493,[Python] Make pyarrow.parquet work with the new filesystem interfaces,"The place internally where the ""legacy"" `pyarrow.filesystem` filesystems are still used is in the {{pyarrow.parquet}} module.

It is used in:

- ParquetWriter
- ParquetManifest/ParquetDataset
- write_to_dataset

For {{ParquetWriter}}, we need to update this to work with the new filesystems (since ParquetWriter is not dataset related, and thus won't be deprecated).  
For {{ParquetManifest}}/{{ParquetDataset}}, it might not need to be updated, since those might get deprecated itself (to be discussed -> ARROW-9720), and when using the {{use_legacy_dataset=False}} option, it already uses the new datasets.  
For {{write_to_dataset}}, this might depend on how the writing capabilities of the dataset project evolve.

",filesystem pull-request-available,['Python'],ARROW,Sub-task,Major,2020-08-13 08:46:31,5
13322423,[Rust] [DataFusion] MergeExec  should have concurrency limit,"MergeExec currently spins up one thread per input partition which causes apps to effectively hang if there are substantially more partitions than available cores.

We can implement a configurable limit here pretty easily.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-13 00:19:32,10
13322407,[Rust] [DataFusion] TypeCoercionRule not implemented for Limit or Sort,"TypeCoercionRule not implemented for Limit or Sort, causing TPC-H query 1 to fail.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-12 23:09:13,10
13322403,[Rust][DataFusion] Remove explicit panics,There are two explicit panics in the datafusion codebase. We should remove them.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-08-12 22:58:27,10
13322393,[Rust] [DataFusion] ParquetScanExec panics on error,ParquetScanExec panics on error,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-12 22:33:58,10
13322375,[Rust] Add benchmark based on TPC-H,"We need better benchmarks for testing at scale, so TPC benchmarks are ideal since data can be generated at different scale factors. TPC-H seems like a good fit for Arrow so I would like to contribute a benchmark based on that.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-08-12 21:31:34,10
13322291,[Rust] [DataFusion] Re-implement threading model,"The current threading model is very simple and does not scale. We currently use 1-2 dedicated threads per partition and they all run simultaneously, which is a huge problem if you have more partitions than logical or physical cores.

This task is to re-implement the threading model so that query execution uses a fixed (configurable) number of threads. Work will be broken down into stages and tasks and each in-process executor (running on a dedicated thread) will process its queue of tasks.

This process will be driven by a scheduler.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2020-08-12 15:25:40,10
13322261,[Developer][Archery] Restartable cherry-picking process for creating maintenance branches,"Archery already had some features to generate the cherry-picking commands, but conflicting patches can make the manual application/reapplication procedure complicated.

1. Add an archery command to recreate the maintenance branch based on a jira release.
2. Extend the above command with an option to continout the cherry picking process after a conslifc resolution.",pull-request-available,"['Archery', 'Developer Tools']",ARROW,New Feature,Major,2020-08-12 12:55:01,3
13322125,[C++][Dataset] num_rows method for Dataset/Scanner,"Something like Scanner::ToTable except first Project to keep 0 columns, and for each record batch, grab the num_rows. Then sum the resulting vector.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2020-08-11 21:21:27,0
13322112,[Rust][DataFusion] Improve documentation on LogicalPlan variants,I think we could improve the documentation somewhat on LogicalPlan nodes. I will submit a PR with a proposal. ,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-08-11 20:19:25,11
13322035,[CI][Docs] Nightly docs build fails,"https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=15998&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181

{code}
...
reading sources... [ 13%] format/Integration
reading sources... [ 13%] format/Layout
reading sources... [ 13%] format/Metadata
reading sources... [ 13%] format/Other
reading sources... [ 14%] format/Versioning
reading sources... [ 14%] index
reading sources... [ 14%] java/index
reading sources... [ 14%] java/ipc
reading sources... [ 14%] java/vector
reading sources... [ 15%] java/vector_schema_root
reading sources... [ 15%] python/api
reading sources... [ 15%] python/api/arrays
/arrow/docs/source/cpp/api/flight.rst:204: WARNING: doxygenfunction: Unable to resolve multiple matches for function ""arrow::flight::MakeFlightError"" with arguments () in doxygen xml output for project ""arrow_cpp"" from directory: ../../cpp/apidoc/xml.
Potential matches:
    - Status MakeFlightError(FlightStatusCode code, const std::string &message)
    - Status MakeFlightError(FlightStatusCode code, const std::string &message, const std::string &extra_info)

Extension error:
Handler <function _process_docstring at 0x7ff3d2a3f378> for event 'autodoc-process-docstring' threw an exception (exception: <built-in function array> is not a module, class, method, function, traceback, frame, or code object)
Error: `docker-compose --file /home/vsts/work/1/s/arrow/docker-compose.yml run --rm -e SETUPTOOLS_SCM_PRETEND_VERSION=1.1.0.dev75 ubuntu-docs` exited with a non-zero exit code 2, see the process log above.

{code}

cc [~lidavidm] [~kszucs]",pull-request-available,"['Continuous Integration', 'Documentation', 'Python']",ARROW,Bug,Major,2020-08-11 14:39:59,3
13322025,[Python] distutils import warning,"Running {{setup.py}} displays this warning message:
{code}
/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/setuptools/distutils_patch.py:26: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2020-08-11 13:47:36,2
13322018,[Rust] [DataFusion] Make sql_statement_to_plan public,There have been some changes to SQL parsing from the upgrade to the latest version of the sql parser crate and we need to make one sql_statement_to_plan public so that other crates can continue to delegate to DataFusion to convert sql statements to logical plans.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-11 13:13:52,10
13321826,[Rust][DataFusion] Implement Debug for ExecutionPlan trait,"For ARROW-9653, I was trying to debug the execution plan and I would have found it easier if there had been a way to display the execution plan to better understand and isolate the issue. This would also be nice to have as part of EXPLAIN plan functionality in ARROW-9654

In general, for debugging purposes, we would like to be able to dump out an execution plan. To do so in the idiomatic rust way, we should require that `ExecutionPlan` also implement `std::fmt::Debug`

Here is an example plan for ""SELECT c1, c2, MIN(c3) FROM aggregate_test_100 GROUP BY c1, c2""

{code}
physical plan is HashAggregateExec {
    group_expr: [
        Column {
            name: ""c1"",
        },
        Column {
            name: ""c2"",
        },
    ],
    aggr_expr: [
        Min {
            expr: Column {
                name: ""c3"",
            },
        },
    ],
    input: DataSourceExec {
        schema: Schema {
            fields: [
                Field {
                    name: ""c1"",
                    data_type: Utf8,
                    nullable: false,
                    dict_id: 0,
                    dict_is_ordered: false,
                },
                Field {
                    name: ""c2"",
                    data_type: UInt32,
                    nullable: false,
                    dict_id: 0,
                    dict_is_ordered: false,
                },
                Field {
                    name: ""c3"",
                    data_type: Int8,
                    nullable: false,
                    dict_id: 0,
                    dict_is_ordered: false,
                },
            ],
            metadata: {},
        },
        partitions.len: 1,
    },
    schema: Schema {
        fields: [
            Field {
                name: ""c1"",
                data_type: Utf8,
                nullable: true,
                dict_id: 0,
                dict_is_ordered: false,
            },
            Field {
                name: ""c2"",
                data_type: UInt32,
                nullable: true,
                dict_id: 0,
                dict_is_ordered: false,
            },
            Field {
                name: ""MIN(c3)"",
                data_type: Int64,
                nullable: true,
                dict_id: 0,
                dict_is_ordered: false,
            },
        ],
        metadata: {},
    },
}
{code}",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2020-08-10 15:49:48,11
13321753,[Java] Support non-nullable vectors,"This issue was first discussed in the ML ([https://lists.apache.org/thread.html/r480387ec9ec822f3ed30e9131109e43874a1c4d18af74ede1a7e41c5%40%3Cdev.arrow.apache.org%3E]), from which we have received some feedback.

We briefly resate it here as below:


1. Non-nullable vectors are widely used in practice. For example, in a database engine, a column can be declared as not null, so it cannot contain null values.
2.Non-nullable vectors has significant performance advantages compared with their nullable conterparts, such as:
 1) the memory space of the validity buffer can be saved.
 2) manipulation of the validity buffer can be bypassed
 3) some if-else branches can be replaced by sequential instructions (by the JIT compiler), leading to high throughput for the CPU pipeline.

We open this Jira to facilitate further discussions, and we may provide a sample PR, which will help us make a clearer decision.",pull-request-available,['Java'],ARROW,New Feature,Major,2020-08-10 09:06:04,7
13321701,[Rust] [DataFusion] HashAggregate walks map many times building final batch,The current HashAggregate implementation iterates over the final hash map once for each grouping expression and once for each aggregate expression. This is inefficient and possibly dangerous depending on the ordering gaurantees made by the hash map implementation.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-08-09 16:50:15,10
13321699,[Rust] [DataFusion] Improve projection push down to remove unused columns,"Currently, the projection push down only removes columns that are never referenced in the plan. However, sometimes a projection declares columns that themselves are never used.

This issue is about improving the projection push-down to remove any column that is not logically required by the plan.

Failing unit-test with the idea:

{code:java}
    #[test]
    fn table_unused_column() -> Result<()> {
        let table_scan = test_table_scan()?;
        assert_eq!(3, table_scan.schema().fields().len());
        assert_fields_eq(&table_scan, vec![""a"", ""b"", ""c""]);

        // we never use ""b"" in the first projection => remove it
        let plan = LogicalPlanBuilder::from(&table_scan)
            .project(vec![col(""c""), col(""a""), col(""b"")])?
            .filter(col(""c"").gt(&lit(1)))?
            .project(vec![col(""c""), col(""a"")])?
            .build()?;

        assert_fields_eq(&plan, vec![""c"", ""a""]);

        let expected = ""\
        Projection: #c, #a\
        \n  Selection: #c Gt Int32(1)\
        \n    Projection: #c, #a\
        \n      TableScan: test projection=Some([0, 2])"";

        assert_optimized_plan_eq(&plan, expected);

        Ok(())
    }
{code}

This issue was firstly identified by [~andygrove] [here|https://github.com/ballista-compute/ballista/issues/320].
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-08-09 16:45:33,9
13321659,the table does not load if the file path has a dot at start ,"Please check the below code. If I uncomment line six then the table written to disk is loaded as an empty table.

Similar behavior is observed when the directory name starts with `_`.

This was not the case in earlier versions.



```python

import pyarrow.parquet as pq
 import pyarrow as pa
 import pathlib
 _path = pathlib.Path(""abc"").resolve()
 _path /= ""xyz""
 _path /= "".lmn"" # uncomment this has a 'dot' at start of the folder
 _path /= ""file_name""
 pq.write_to_dataset(
   table=pa.table({""a"": [1], ""b"": [2]}), 
   root_path=_path.as_posix()
 )
 t = pq.read_table(_path)
 print(t.to_pandas())

```",read table,['Python'],ARROW,Bug,Major,2020-08-09 00:47:07,6
13321585,[C++][FlightRPC] Close()ing a DoPut with an ongoing read locks up the client,This section accidentally recurses and ends up trying to re-acquire a lock: https://github.com/apache/arrow/blob/9c04867930eae5454dbb1ea4c7bd869b12fc6e9d/cpp/src/arrow/flight/client.cc#L215,pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-08-07 20:42:47,0
13321386,[Python][wheel][Windows] library missing failure by ARROW-9412,"https://ci.appveyor.com/project/Ursa-Labs/crossbow/builds/34509432?fullLog=true#L3147

{noformat}
C:\projects\crossbow>C:\Python38-x64\python.exe -c ""import pyarrow""   || exit /B 
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Python38-x64\lib\site-packages\pyarrow\__init__.py"", line 62, in <module>
    import pyarrow.lib as _lib
ImportError: DLL load failed while importing lib: The specified module could not be found.
{noformat}",pull-request-available,['Python'],ARROW,Bug,Major,2020-08-06 21:23:50,1
13321194,[C++] IPC - dictionaries in maps,"I created the following record batch which has a single column with a type of map<dict, string> where dict is defined as: dict<int8,string>:


{code:java}
arrow::MapBuilder map_builder(arrow::default_memory_pool(),
    std::make_shared<arrow::StringDictionaryBuilder>(),
    std::make_shared<arrow::StringBuilder>());
auto key_builder = 
    dynamic_cast<arrow::StringDictionaryBuilder *>(map_builder.key_builder());
auto item_builder = 
    dynamic_cast<arrow::StringBuilder *>(map_builder.item_builder());

// Add a first row with k<i>=v<i> for i 0..14;
ASSERT_OK(map_builder.Append());
for (int i = 0; i < 15; ++i) {
  ASSERT_OK(key_builder->Append(""k"" + std::to_string(i)));
  ASSERT_OK(item_builder->Append(""v"" + std::to_string(i)));
}
// Add a second row with k<i>=w<i> for i 0..14;
ASSERT_OK(map_builder.Append());
for (int i = 0; i < 15; ++i) {
  ASSERT_OK(key_builder->Append(""k"" + std::to_string(i)));
  ASSERT_OK(item_builder->Append(""w"" + std::to_string(i)));
}
std::shared_ptr<arrow::Array> array;
ASSERT_OK(map_builder.Finish(&array));
std::shared_ptr<arrow::Schema> schema = 
    arrow::schema({arrow::field(""s"", array->type())});
std::shared_ptr<arrow::RecordBatch> batch = 
    arrow::RecordBatch::Make(schema, array->length(), {array});
{code}
When one attempts to send this in a round trip IPC:
 # On IpcFormatWriter::Start(): The memo records one entry for field_to_id and id_to_type_ where the dict id = 0.
 # On IpcFormatWriter::CollectDictionaries: The memo records a new entry for field_to_id and id_to_type with id=1 and also records in id_to_dictionary_. At this point we have 2 entries with the entry id=0 having no associated dict.
 # On IpcFormatWriter;:WriteDictionaries: It writes the dict with entry = 1

When reading:
 # GetSchema eventually gets to the nested dictionary in FieldFromFlatBuffer
 # The recovered dict id is 0.
 # This adds to the memo the field_to_id and id_to_type with id = 0
 # My round trip code calls ""ReadAll"".
 # RecordBatchStreamReaderImpl::ReadNext attempts to load the initial dicts
 # It recovers id = 1
 # The process aborts because id = 1 is not in the memo: dictionary_memo->GetDictionaryType(id, &value_type)

A similar example with a dict inside a ""struct"" worked fine and only used dict id = 0. So it looks like something wrong when gathering the schema for the map. Unless I did not construct the map correctly?

",pull-request-available,['C++'],ARROW,Bug,Major,2020-08-05 23:53:13,2
13321115,[Rust][DataFusion] Slightly confusing error message when unsupported type is provided to CREATE EXTERNAL TABLE,"
When I make a typo in the CREATE EXTERNAL TABLE command, I get an error message that doesn't doesn't tell me the valid values of STORED AS:

As a user, I would like the software to guide me to the correct resolution and tell me what the supported formats are.


For example:
{code}
CREATE EXTERNAL TABLE repro
STORED AS ARROW
LOCATION 'repro.arrow';
{code}

results in:

{code}
> CREATE EXTERNAL TABLE repro
STORED AS ARROW
LOCATION 'repro.arrow';

ParserError(ParserError(""Expected fileformat, found: ARROW""))
{code}

Ideally the error should say that the valid choices are PARQUET, CSV, or DSJSON
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-08-05 14:40:57,11
13321096,[Rust][DataFusion] Add an EXPLAIN command to the datafusion CLI,"In order to understand what DataFusion's planner is doing, it would be helpful to have an ""EXPLAIN PLAN"" feature. All other database systems I have worked with have such a feature (e.g. see [MySql|https://dev.mysql.com/doc/refman/8.0/en/explain-output.html]). 

I have found EXPLAIN helpful for both users but also very much developers.

The feature would look something like
{code}
EXPLAIN <sql>
{code}

Which would then print a string with the logical plan. One way to implement this would simply be to use the debugging printout of the logical and physical plans
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2020-08-05 12:51:52,11
13321093,[Rust][DataFusion] Multi-column Group by: Invalid Argument Error ,"Repro:
{code}
CREATE EXTERNAL TABLE repro(a INT, b INT)
STORED AS CSV
WITH HEADER ROW
LOCATION 'repro.csv';
{code}

The contents of repro.csv are as follows (also attached):
{code}
a,b
1,100
1,200
2,100
2,200
2,300
{code}


Then try to run the following query (to sum the values of a, grouped by b).
{code}
select sum(a), a, b from repro group by a, b;
{code}


*Expected result*: a table with three output columns: sum(a), a, and b

*Actual result*: An arrow error

{code}

> select sum(a), a, b from repro group by a, b;
ArrowError(InvalidArgumentError(""number of columns(4) must match number of fields(3) in schema""))

{code}
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-08-05 12:45:17,11
13321085,[Rust][DataFusion] Panic trying to select * from a CSV (panicked at 'index out of bounds: the len is 0 but the index is 0),"I was trying to write a reproducer for another bug, and I hit this one:

Repro:

{code}
CREATE EXTERNAL TABLE repro
STORED AS CSV
LOCATION 'repro.csv';

select * from repro;
{code}

The contents of repro.csv are as follows (also attached):
{code}
a,b
1,100
1,200
2,100
2,200
2,300
{code}

Expected behavior: a table of 2 columns, 5 rows is returned

Actual behavior: A panic occurs:

{code}
> select * from repro;
thread 'main' panicked at 'index out of bounds: the len is 0 but the index is 0', datafusion/src/optimizer/projection_push_down.rs:238:31
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
{code}

I'll try and look into it myself, but I wanted to get the repro up first.
",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Minor,2020-08-05 12:20:22,11
13320968,[C++][Dataset] Debug segfault in dataset writing on 32-bit mingw (RTools 35),"Followup to ARROW-8002, which skipped the dataset writing tests on windows.",dataset,['C++'],ARROW,Bug,Major,2020-08-04 21:38:16,6
13320890,[C++][Dataset] Add support for writing parquet datasets ,IpcFileFormat is currently the only format which supports writing.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-08-04 15:16:19,6
13320854,[Python] Deprecate the legacy pyarrow.filesystem interface,"The {{pyarrow.filesystem}} interfaces are dubbed ""legacy"" (in favor of {{pyarrow.fs}}), but at some point we should actually deprecate (and eventually remove) them. 

There is probably still some work to do before that: ensure the new filesystems can be used instead in all places (eg in pyarrow.parquet), improve the docs about the new filesystems, ..",pull-request-available,['Python'],ARROW,Improvement,Major,2020-08-04 12:44:49,5
13320802,[C++][Dataset] Do not check for ignore_prefixes in the base path,"Somewhat related to ARROW-8427, and from https://github.com/apache/arrow/issues/7857

I am not sure we should check the {{ignore_prefixes}} for the base path provided by the user. Because if that contains eg an underscore, it simply skips the full dataset resulting in an empty dataset.

{code:python}
import tempfile
import pathlib

path = tempfile.mkdtemp()
tmpdir =  pathlib.Path(path)                                                                                                                                                              

# base path with a directory with an underscore 
datadir = tmpdir / ""_data"" / ""dataset""                                                                                                                                                                    
datadir.mkdir(parents=True, exist_ok=True)                                                                                                                                                                

# create a parquet file at that location
import pyarrow as pa
import pyarrow.parquet as pq

table = pa.table({'a': [1, 2, 3]})                                                                                                                                                                        
pq.write_table(table, datadir / ""data.parquet"")                                                                                                                                                           

# reading dataset skips everything
import pyarrow.dataset as ds                                                                                                                                                                              

In [26]: ds.dataset(datadir)                                                                                                                                                                                       
Out[26]: <pyarrow._dataset.FileSystemDataset at 0x7fbfd8779bb0>

In [27]: ds.dataset(datadir).files                                                                                                                                                                                 
Out[27]: []
{code}

cc [~bkietz] [~npr]",dataset pull-request-available,"['C++', 'Python', 'R']",ARROW,Bug,Major,2020-08-04 08:04:52,6
13320707,[C++][Python] Restore non-UTC time zones when reading Parquet file that was previously Arrow,"This was reported on the mailing list

{code}
In [20]: df = pd.DataFrame({'a': pd.Series(np.arange(0, 10000, 1000)).astype(pd.DatetimeTZDtype('ns', 'America/Los_Angeles'
    ...: ))})                                                                                                              

In [21]: t = pa.table(df)                                                                                                  

In [22]: t                                                                                                                 
Out[22]: 
pyarrow.Table
a: timestamp[ns, tz=America/Los_Angeles]

In [23]: pq.write_table(t, 'test.parquet')                                                                                 

In [24]: pq.read_table('test.parquet')                                                                                     
Out[24]: 
pyarrow.Table
a: timestamp[us, tz=UTC]

In [25]: pq.read_table('test.parquet')[0]                                                                                  
Out[25]: 
<pyarrow.lib.ChunkedArray object at 0x7f72eb4b68f0>
[
  [
    1970-01-01 00:00:00.000000,
    1970-01-01 00:00:00.000001,
    1970-01-01 00:00:00.000002,
    1970-01-01 00:00:00.000003,
    1970-01-01 00:00:00.000004,
    1970-01-01 00:00:00.000005,
    1970-01-01 00:00:00.000006,
    1970-01-01 00:00:00.000007,
    1970-01-01 00:00:00.000008,
    1970-01-01 00:00:00.000009
  ]
]
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-08-03 16:37:33,5
13320669,[Python] Kartothek integration tests failing due to missing freezegun module,"See eg https://github.com/ursa-labs/crossbow/runs/939266052

{code}
==================================== ERRORS ====================================
________________________ ERROR collecting test session _________________________
/opt/conda/envs/arrow/lib/python3.7/importlib/__init__.py:127: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1006: in _gcd_import
    ???
<frozen importlib._bootstrap>:983: in _find_and_load
    ???
<frozen importlib._bootstrap>:967: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:677: in _load_unlocked
    ???
/opt/conda/envs/arrow/lib/python3.7/site-packages/_pytest/assertion/rewrite.py:170: in exec_module
    exec(co, module.__dict__)
tests/cli/conftest.py:11: in <module>
    from freezegun import freeze_time
E   ModuleNotFoundError: No module named 'freezegun'
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-08-03 13:02:17,5
13320652,[Rust] Clippy PR test failing intermittently on Rust / AMD64 MacOS ,"As reported by [~jorgecarleitao], on https://github.com/apache/arrow/commit/aa6889a74c57d6faea0d27ea8013d9b0c7ef809a#commitcomment-41124305

"" I believe that this is somehow interacting with the caching system and sometimes failing the build of clippy. E.g. this build is failing for Mac OS, and it hits the cache: https://github.com/apache/arrow/runs/937976656

{code}
  Downloaded heck v0.3.1
  Downloaded aho-corasick v0.7.13
  Downloaded fnv v1.0.7
  Downloaded futures-io v0.3.5
  Downloaded base64 v0.11.0
  Downloaded dirs v1.0.5
  Downloaded async-stream-impl v0.2.1
  Downloaded async-stream v0.2.1
  Downloaded anyhow v1.0.32
  Downloaded atty v0.2.14
  Downloaded num-integer v0.1.43
   Compiling arrow-flight v2.0.0-SNAPSHOT (/Users/runner/work/arrow/arrow/rust/arrow-flight)
error[E0463]: can't find crate for `prost_derive` which `tonic_build` depends on
  --> arrow-flight/build.rs:36:9
   |
36 |         tonic_build::compile_protos(""../../format/Flight.proto"")?;
   |         ^^^^^^^^^^^ can't find crate

error: aborting due to previous error
{code}
",pull-request-available,['Rust'],ARROW,Bug,Major,2020-08-03 11:16:05,11
13320581,[Python] test_move_file() is failed with fsspec 0.8.0,"It works with fsspec 0.7.4: https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/34414340/job/os9t8kj9t4afgym9

Failed with fsspec 0.8.0: https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/34422556/job/abedu9it26qvfxkm

{noformat}
================================== FAILURES ===================================
__ test_move_file[PyFileSystem(FSSpecHandler(fsspec.filesystem(""memory"")))] ___
fs = <pyarrow._fs.PyFileSystem object at 0x0000003D057AA520>
pathfn = <function py_fsspec_memoryfs.<locals>.<lambda> at 0x0000003D04F70B58>
    def test_move_file(fs, pathfn):
        s = pathfn('test-move-source-file')
        t = pathfn('test-move-target-file')
    
        with fs.open_output_stream(s):
            pass
    
>       fs.move(s, t)
pyarrow\tests\test_fs.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow\_fs.pyx:519: in pyarrow._fs.FileSystem.move
    check_status(self.fs.Move(source, destination))
pyarrow\_fs.pyx:1024: in pyarrow._fs._cb_move
    handler.move(frombytes(src), frombytes(dest))
pyarrow\fs.py:199: in move
    self.fs.mv(src, dest, recursive=True)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\spec.py:744: in mv
    self.copy(path1, path2, recursive=recursive, maxdepth=maxdepth)
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\spec.py:719: in copy
    self.cp_file(p1, p2, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
self = <fsspec.implementations.memory.MemoryFileSystem object at 0x0000003D01096A78>
path1 = 'test-move-source-file/', path2 = 'test-move-target-file/'
kwargs = {'maxdepth': None}
    def cp_file(self, path1, path2, **kwargs):
        if self.isfile(path1):
>           self.store[path2] = MemoryFile(self, path2, self.store[path1].getbuffer())
E           KeyError: 'test-move-source-file/'
C:\Miniconda37-x64\envs\arrow\lib\site-packages\fsspec\implementations\memory.py:134: KeyError
{noformat}",pull-request-available,['Python'],ARROW,Bug,Major,2020-08-03 03:33:29,5
13320540,[Rust] [DataFusion] Add predicate push-down,"Like the title says, add an optimizer to push filters down the plan as farther as logically possible.",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-08-02 12:08:27,9
13320534,[Rust] [DataFusion] Make it easier to write optimizers,"Currently, it is a bit painful to write optimizers as we need to cover all branches of a match to of logical plans.

There could be some utility functions to remove boilerplate code around these.
",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-08-02 07:53:04,9
13320526,[Rust] [DataFusion] Add length of string array,"With ARROW-9615, we can port the operator to DataFusion.",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-08-02 04:57:28,9
13320428,[C++] CSV datasets don't materialize virtual columns,"They're included but the values are all null. 

Minimal reproducer in R:

{code:r}
library(arrow)
dir.create(""testds/c=5"", recursive = TRUE)
cat(""a,b\n1,2\n"", file = ""testds/c=5/test.csv"")
ds <- open_dataset(""testds"", format = ""csv"")
ds
## FileSystemDataset with 1 csv file
## a: int64
## b: int64
## c: int32
dplyr::collect(ds)
## # A tibble: 1 x 3
##       a     b     c
##   <int> <int> <int>
## 1     1     2    NA
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2020-07-31 15:22:26,6
13320349,[C++][Dataset] in expressions don't work with >1 partition levels,"When filtering nested partitions using %in%, no rows are returned, both for Hive and non-Hive partitioning. == and other comparison operators do work, and the problem also goes away when only one partition level is declared in the schema.

This is not caused by the dplyr wrappers, the lower-level functions have the same problem.

{code}
library(arrow)
#> 
#> Attaching package: 'arrow'
#> The following object is masked from 'package:utils':
#> 
#>     timestamp
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union

## Write files
pqdir <- file.path(tempdir(), paste(sample(letters, 6), collapse = """"))

for (foo in 0:1) {
  for (faa in 0:1) {
    fdir <- file.path(pqdir, letters[foo + 1], letters[faa + 1])
    dir.create(fdir, recursive = TRUE)
    rng <- (foo * 5 + faa + 1):(foo * 5 + faa + 5)
    write_parquet(data.frame(col = letters[rng]),
                         file.path(fdir, ""file.parquet""))
  }
}

## What doesn't work: using %in% with both partitions defined
ds <- open_dataset(pqdir,
                   partitioning = schema(foo = string(), faa = string()))

collect(filter(ds, foo %in% ""a""))
#> # A tibble: 0 x 3
#> # ... with 3 variables: col <chr>, foo <chr>, faa <chr>

## == does work
collect(filter(ds, foo == ""a""))
#> # A tibble: 10 x 3
#>    col   foo   faa  
#>    <chr> <chr> <chr>
#>  1 a     a     a    
#>  2 b     a     a    
#>  3 c     a     a    
#>  4 d     a     a    
#>  5 e     a     a    
#>  6 b     a     b    
#>  7 c     a     b    
#>  8 d     a     b    
#>  9 e     a     b    
#> 10 f     a     b

## Declaring only one partition does work
ds <- open_dataset(pqdir, partitioning = schema(foo = string()))
collect(filter(ds, foo %in% ""a""))
#> # A tibble: 10 x 2
#>    col   foo  
#>    <chr> <chr>
#>  1 a     a    
#>  2 b     a    
#>  3 c     a    
#>  4 d     a    
#>  5 e     a    
#>  6 b     a    
#>  7 c     a    
#>  8 d     a    
#>  9 e     a    
#> 10 f     a

## The lower-level API has the same problem
ds <- open_dataset(pqdir,
                   partitioning = schema(foo = string(), faa = string()))

flt <- Expression$in_(Expression$field_ref(""foo""), Array$create(""a""))

sc <- Scanner$create(ds, filter = flt)
sc$ToTable()
#> Table
#> 0 rows x 3 columns
#> $col <string>
#> $foo <string>
#> $faa <string>
{code}",dataset pull-request-available,"['C++', 'R']",ARROW,Bug,Major,2020-07-31 08:46:23,6
13320283,[C++][Parquet] Write Arrow relies on unspecified behavior for nested types,"parquet/column_writer.cc WriteArrow implementations at certain points checks null counts/required data and passes through the null bitmap for encoding. This only works for nested data types if the if the null slot on a parent implies a null slot on the leaf. This relationship is not required by the specifications.



Most paths for creating arrays follow this pattern so it would be esoteric to hit this bug, but we should still fix it.



All branches that rely on reading nullness should generate a new null bitmap based on definition levels if the column is nested, and decisions should be based off of that.",pull-request-available,['C++'],ARROW,Bug,Major,2020-07-30 22:29:46,15
13320260,[R] Improve cmake detection in Linux build,"
{code:java}
> arrow::write_parquet(iris, ""~/iris"") 
*** caught segfault ***
address (nil), cause 'memory not mapped' Traceback: 1: Table__from_dots(dots, schema) 2: shared_ptr_is_null(xp) 3: shared_ptr(Table, Table__from_dots(dots, schema)) 4: Table$create(x) 5: arrow::write_parquet(iris, ""~/iris"")

{code}
The segfault is easy to generate trying to write iris data to parquet. I have tried R 4.0.0 and R 4.0.2, I've installed the arrow (R) package from CRAN, source, nightly build, both with and without using the system arrow C++ installation. When using system arrow the installed version is:
{noformat}
Installed Packages 
Name    : arrow-devel 
Arch    : x86_64 
Version   : 1.0.0 
Release   : 1.el7 
Size    : 32 M 
Repo    : installed 
From repo  : apache-arrow 
Summary   : Libraries and header files for Apache Arrow C++ 
URL     : https://arrow.apache.org/ 
License   : Apache-2.0 
Description : Libraries and header files for Apache Arrow C++.

{noformat}
I realize that this is so basic that it seems improbable that your CI didn't catch something (i.e., that the issue has to do with my local environment) but would appreciate verification that version 1.0 works for others on centOS7",pull-request-available,['R'],ARROW,Bug,Major,2020-07-30 18:51:57,4
13320206,"[Rust] When used as a crate dependency, arrow-flight is rebuilt on every invocation of cargo build","When used as a crate dependency, arrow-flight is rebuilt on every invocation of cargo build
h1. *Repro*:

Create a new repo, add `arrow=1.0.0` as a dependency, and then run `cargo build`

*Expected behavior:* After the first successful invocation of `cargo build`, arrow-flight will not recompile if no other changes are made.

*Actual behavior*: After every invocation of `cargo build`, arrow-flight is recompiled, even when nothing has changed
h1. Example


 Create a new crate
{code:java}
 alamb@ip-192-168-0-129 arrow_rebuilds % cargo new too_many_rebuilds --bin
 cargo new too_many_rebuilds --bin
 Created binary (application) `too_many_rebuilds` package
{code}
Add arrow as a dependency in Cargo.toml:
{code:java}
 diff --git a/Cargo.toml b/Cargo.toml
 index a239680..44ed358 100644
  a/Cargo.toml
 +++ b/Cargo.toml
 @@ -5,3 +5,6 @@ authors = [""alamb <andrew@nerdnetworks.org>""]
 edition = ""2018""
 # See more keys and their definitions at [https://doc.rust-lang.org/cargo/reference/manifest.html]
 +
 +[dependencies]
 +arrow = ""1.0.0""
{code}
Now, all invocations of `cargo build` will rebuild arrow, even though nothing in the code has changed:
{code:java}
 alamb@ip-192-168-0-129 too_many_rebuilds % cargo build
 cargo build
 Compiling arrow-flight v1.0.0
 Compiling arrow v1.0.0
 Compiling too_many_rebuilds v0.1.0 (/Users/alamb/Software/bugs/arrow_rebuilds/too_many_rebuilds)
 Finished dev [unoptimized + debuginfo] target(s) in 8.70s
 alamb@ip-192-168-0-129 too_many_rebuilds % cargo build
 cargo build
 Compiling arrow-flight v1.0.0
 Compiling arrow v1.0.0
 Compiling too_many_rebuilds v0.1.0 (/Users/alamb/Software/bugs/arrow_rebuilds/too_many_rebuilds)
 Finished dev [unoptimized + debuginfo] target(s) in 8.65s
{code}
You can see what is happening by checking out a fresh copy of arrow/master (no Cargo.log) and running `cargo build`  you'll see your local checkout has changes in rust/arrow-flight/src/arrow.flight.protocol.rs:
{code:java}
 alamb@ip-192-168-0-129 arrow % cd rust/arrow
 cd rust/arrow
 alamb@ip-192-168-0-129 arrow % git status
 git status
 On branch master
 Your branch is up to date with 'origin/master'.

nothing to commit, working tree clean
 alamb@ip-192-168-0-129 arrow % cargo build
 cargo build
 Compiling futures-task v0.3.5
 ...
 Compiling arrow v2.0.0-SNAPSHOT (/Users/alamb/Software/arrow/rust/arrow)
 Finished dev [unoptimized + debuginfo] target(s) in 21.76s
 alamb@ip-192-168-0-129 arrow %

alamb@ip-192-168-0-129 arrow % git status
 git status
 On branch master
 Your branch is up to date with 'origin/master'.

Changes not staged for commit:
 (use ""git add <file>..."" to update what will be committed)
 (use ""git restore <file>..."" to discard changes in working directory)
 modified: ../arrow-flight/src/arrow.flight.protocol.rs

no changes added to commit (use ""git add"" and/or ""git commit -a"")
{code}
 # Root Cause Analysis

The issue is that the build.rs of arrow-flight calls `tonic_build` to auto generate `rust/arrow-flight/src/arrow.flight.protocol.rs`, which is also checked in (first done in [https://github.com/apache/arrow/commit/ec84b7b8102f227295f865c420496830c66a6281]).

This file and the version of tonic were updated on [https://github.com/apache/arrow/commit/7b49cbc23f22ed99eebf85cc0b9acb1f0d3f832f] on July 11, 2020

It turns out that the output of ""tonic_build"" depends on not only on the version of tonic, but also on the version of proc-macro2, and the version of proc-macro2 is not specifically pinned. 

`proc-macro2 = ""1.0.19""` was released on July 19, 2020 ([https://crates.io/crates/proc-macro2/1.0.19]) and it appears to subtlety changes the resulting output of arrow.flight.protocol.rs; Thus the output no longer matches what is checked in. This means that anyone without a Cargo.lock file that pins proc-macro2 to 1.0.18 would get 1.0.19 and thus also a local modification during build.

h1. Workaround
If we pin Cargo.toml to use proc-macro2 1.0.18 the local modification stops.
 {code}
 proc-macro2 = ""1.0.18""
 {code}

",pull-request-available,['Rust'],ARROW,Bug,Major,2020-07-30 14:21:53,10
13320205,[CI] Appveyor toolchain build fails because CMake detects different C and C++ compilers,"Build log: https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/34377790/job/f955ccj8irpgh565#L440

Caused by a recent CMake release 3.18.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-07-30 14:18:18,3
13320106,[C++][Parquet]  Spaced definition levels is not assigned correctly.,The existing code assumes that there is only a single repeated parent. Code needs to backtrack until null or or a repeated parent. Unfortunately without ability to read path that can read mixed struct/repeated values we can't fully test the fix.,pull-request-available,['C++'],ARROW,Bug,Major,2020-07-30 04:30:53,15
13320053,"[CI][Crossbow] Fix homebrew-cpp again, again","{code}
Error: 1 problem in 1 formula detected
apache-arrow:
  * C: 7: col 47: Use the new Ruby 1.9 hash syntax.
The command ""brew audit $ARROW_FORMULA"" failed and exited with 1 during .
{code}",pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Major,2020-07-29 20:08:49,4
13319835,[C++/R] arrow_exports.h contains structs declared as class,This is an issue in an MSVC-based toolchain.,pull-request-available,['R'],ARROW,Bug,Major,2020-07-28 19:47:31,8
13319832,[C++] clang/win: Copy constructor of ParquetInvalidOrCorruptedFileException not correctly triggered,"The copy constructor of ParquetInvalidOrCorruptedFileException doesn't seem to be taken correctly when building with clang 9.0.1 on Windows in a MSVC toolchain.

Adding {{ParquetInvalidOrCorruptedFileException(const ParquetInvalidOrCorruptedFileException&) = default;}} as an explicit copy constructor didn't help.

Happy to any ideas here, probably a long shot as there are other clang-msvc problems.

{code}
[49/62] Building CXX object src/parquet/CMakeFiles/parquet_shared.dir/Unity/unity_1_cxx.cxx.obj
FAILED: src/parquet/CMakeFiles/parquet_shared.dir/Unity/unity_1_cxx.cxx.obj
C:\Users\Administrator\miniconda3\conda-bld\arrow-cpp-ext_1595962790058\_build_env\Library\bin\clang++.exe  -DARROW_HAVE_RUNTIME_AVX2 -DARROW_HAVE_RUNTIME_AVX512 -DARROW_HAVE_RUNTIME_SSE4_2 -DARROW_HAVE_S
SE4_2 -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_TIMING_TESTS -DARROW_WITH_UTF8PROC -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DAWS_COMMON_USE_IMPORT_EXPORT -DAWS_EVE
NT_STREAM_USE_IMPORT_EXPORT -DAWS_SDK_VERSION_MAJOR=1 -DAWS_SDK_VERSION_MINOR=7 -DAWS_SDK_VERSION_PATCH=164 -DHAVE_INTTYPES_H -DHAVE_NETDB_H -DNOMINMAX -DPARQUET_EXPORTING -DUSE_IMPORT_EXPORT -DUSE_IMPORT
_EXPORT=1 -DUSE_WINDOWS_DLL_SEMANTICS -D_CRT_SECURE_NO_WARNINGS -Dparquet_shared_EXPORTS -Isrc -I../src -I../src/generated -isystem ../thirdparty/flatbuffers/include -isystem C:/Users/Administrator/minico
nda3/conda-bld/arrow-cpp-ext_1595962790058/_h_env/Library/include -isystem ../thirdparty/hadoop/include -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -march=k8 -mtune=haswell -ftree-vectorize
-fstack-protector-strong -O2 -ffunction-sections -pipe -D_CRT_SECURE_NO_WARNINGS -D_MT -D_DLL -nostdlib -Xclang --dependent-lib=msvcrt -fuse-ld=lld -fno-aligned-allocation -Qunused-arguments -fcolor-diagn
ostics -O3 -DNDEBUG  -Wa,-mbig-obj -Wall -Wno-unknown-warning-option -Wno-pass-failed -msse4.2  -O3 -DNDEBUG -D_DLL -D_MT -Xclang --dependent-lib=msvcrt   -std=c++14 -MD -MT src/parquet/CMakeFiles/parquet
_shared.dir/Unity/unity_1_cxx.cxx.obj -MF src\parquet\CMakeFiles\parquet_shared.dir\Unity\unity_1_cxx.cxx.obj.d -o src/parquet/CMakeFiles/parquet_shared.dir/Unity/unity_1_cxx.cxx.obj -c src/parquet/CMakeF
iles/parquet_shared.dir/Unity/unity_1_cxx.cxx
In file included from src/parquet/CMakeFiles/parquet_shared.dir/Unity/unity_1_cxx.cxx:3:
In file included from C:/Users/Administrator/miniconda3/conda-bld/arrow-cpp-ext_1595962790058/work/cpp/src/parquet/column_scanner.cc:18:
In file included from ../src\parquet/column_scanner.h:29:
In file included from ../src\parquet/column_reader.h:25:
In file included from ../src\parquet/exception.h:26:
In file included from ../src\parquet/platform.h:23:
In file included from ../src\arrow/buffer.h:28:
In file included from ../src\arrow/status.h:25:
../src\arrow/util/string_builder.h:49:10: error: invalid operands to binary expression ('std::ostream' (aka 'basic_ostream<char, char_traits<char> >') and 'parquet::ParquetInvalidOrCorruptedFileException'
)
  stream << head;
  ~~~~~~ ^  ~~~~
../src\arrow/util/string_builder.h:61:3: note: in instantiation of function template specialization 'arrow::util::StringBuilderRecursive<parquet::ParquetInvalidOrCorruptedFileException &>' requested here
  StringBuilderRecursive(ss.stream(), std::forward<Args>(args)...);
  ^
../src\arrow/status.h:160:31: note: in instantiation of function template specialization 'arrow::util::StringBuilder<parquet::ParquetInvalidOrCorruptedFileException &>' requested here
    return Status(code, util::StringBuilder(std::forward<Args>(args)...));
                              ^
../src\arrow/status.h:204:20: note: in instantiation of function template specialization 'arrow::Status::FromArgs<parquet::ParquetInvalidOrCorruptedFileException &>' requested here
    return Status::FromArgs(StatusCode::Invalid, std::forward<Args>(args)...);
                   ^
../src\parquet/exception.h:129:49: note: in instantiation of function template specialization 'arrow::Status::Invalid<parquet::ParquetInvalidOrCorruptedFileException &>' requested here
      : ParquetStatusException(::arrow::Status::Invalid(std::forward<Args>(args)...)) {}
                                                ^
C:/Users/Administrator/miniconda3/conda-bld/arrow-cpp-ext_1595962790058/work/cpp/src/parquet/file_reader.cc:270:13: note: in instantiation of function template specialization 'parquet::ParquetInvalidOrCor
ruptedFileException::ParquetInvalidOrCorruptedFileException<parquet::ParquetInvalidOrCorruptedFileException &>' requested here
      throw ParquetInvalidOrCorruptedFileException(""Parquet file size is 0 bytes"");
            ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:480:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const void *' for 1st ar
gument; take the address of the argument with &
        basic_ostream& __CLR_OR_THIS_CALL operator<<(const void *_Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:204:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'std::basic_ostream<char,
 char_traits<char> > &(*)(std::basic_ostream<char, char_traits<char> > &) __attribute__((cdecl))' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(basic_ostream& (__cdecl *_Pfn)(basic_ostream&))
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:209:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'std::basic_ostream<char,
 std::char_traits<char> >::_Myios &(*)(std::basic_ostream<char, std::char_traits<char> >::_Myios &) __attribute__((cdecl))' (aka 'basic_ios<char, std::char_traits<char> > &(*)(basic_ios<char, std::char_tr
aits<char> > &)') for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(_Myios& (__cdecl *_Pfn)(_Myios&))
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:215:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'std::ios_base &(*)(std::
ios_base &) __attribute__((cdecl))' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(ios_base& (__cdecl *_Pfn)(ios_base&))
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:221:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'bool' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(bool _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:241:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'short' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(short _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:275:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'unsigned short' for 1st
argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(unsigned short _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:295:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'int' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(int _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:320:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'unsigned int' for 1st ar
gument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(unsigned int _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:340:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'long' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(long _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:360:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'unsigned long' for 1st a
rgument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(unsigned long _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:380:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'long long' for 1st argum
ent
        basic_ostream& __CLR_OR_THIS_CALL operator<<(long long _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:400:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'unsigned long long' for
1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(unsigned long long _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:420:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'float' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(float _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:440:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'double' for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(double _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:460:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'long double' for 1st arg
ument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(long double _Val)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:508:36: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'std::basic_ostream<char,
 std::char_traits<char> >::_Mysb *' (aka 'basic_streambuf<char, std::char_traits<char> > *') for 1st argument
        basic_ostream& __CLR_OR_THIS_CALL operator<<(_Mysb *_Strbuf)
                                          ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:702:33: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const char *' f
or 2nd argument
        basic_ostream<_Elem, _Traits>& operator<<(
                                       ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:749:33: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'char' for 2nd a
rgument
        basic_ostream<_Elem, _Traits>& operator<<(
                                       ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:787:32: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const char *' f
or 2nd argument
        basic_ostream<char, _Traits>& operator<<(
                                      ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:834:32: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'char' for 2nd a
rgument
        basic_ostream<char, _Traits>& operator<<(
                                      ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:960:32: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const signed ch
ar *' for 2nd argument
        basic_ostream<char, _Traits>& operator<<(
                                      ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:967:32: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'signed char' fo
r 2nd argument
        basic_ostream<char, _Traits>& operator<<(
                                      ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:974:32: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const unsigned
char *' for 2nd argument
        basic_ostream<char, _Traits>& operator<<(
                                      ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:981:32: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'unsigned char'
for 2nd argument
        basic_ostream<char, _Traits>& operator<<(
                                      ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:1047:3: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const std::erro
r_code' for 2nd argument
                operator<<(basic_ostream<_Elem, _Traits>& _Ostr,
                ^
../src\arrow/type.h:182:15: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const arrow::DataType' for 2nd argument
std::ostream& operator<<(std::ostream& os, const DataType& type);
              ^
../src\arrow/type.h:1046:15: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'TimeUnit::type' for 2nd argument
std::ostream& operator<<(std::ostream& os, TimeUnit::type unit);
              ^
../src\arrow/array/array_base.h:203:29: note: candidate function not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'const arrow::Array' for 2nd argument
static inline std::ostream& operator<<(std::ostream& os, const Array& x) {
                            ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\thread:240:27: note: candidate function template not viable: no known conversion from 'parquet::ParquetInvalidOrCorruptedFileException' to 'thread::id' for
2nd argument
        basic_ostream<_Ch, _Tr>& operator<<(
                                 ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:921:33: note: candidate template ignored: deduced conflicting types for parameter '_Elem' ('char' vs. 'parquet::ParquetInvalidOrCorruptedFileExcepti
on')
        basic_ostream<_Elem, _Traits>& operator<<(
                                       ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\memory:1718:33: note: candidate template ignored: could not match 'shared_ptr<type-parameter-0-2>' against 'parquet::ParquetInvalidOrCorruptedFileException'
        basic_ostream<_Elem, _Traits>& operator<<(basic_ostream<_Elem, _Traits>& _Out, const shared_ptr<_Ty>& _Px)
                                       ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\memory:2757:33: note: candidate template ignored: could not match 'unique_ptr<type-parameter-0-2, type-parameter-0-3>' against 'parquet::ParquetInvalidOrCor
ruptedFileException'
        basic_ostream<_Elem, _Traits>& operator<<(basic_ostream<_Elem, _Traits>& _Out, const unique_ptr<_Yty, _Dx>& _Px)
                                       ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:874:33: note: candidate template ignored: could not match 'const _Elem *' against 'parquet::ParquetInvalidOrCorruptedFileException'
        basic_ostream<_Elem, _Traits>& operator<<(
                                       ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\ostream:1008:10: note: candidate template ignored: requirement 'conjunction_v<std::negation<std::is_lvalue_reference<std::basic_ostream<char, std::char_trai
ts<char> > &> >, std::is_base_of<std::ios_base, std::basic_ostream<char, std::char_traits<char> > &>, std::_Can_stream_out<std::basic_ostream<char, std::char_traits<char> > &, parquet::ParquetInvalidOrCor
ruptedFileException, void> >' was not satisfied [with _Ostr = std::basic_ostream<char, char_traits<char> > &, _Ty = parquet::ParquetInvalidOrCorruptedFileException]
        _Ostr&& operator<<(_Ostr&& _Os, const _Ty& _Val)
                ^
C:\BuildTools\VC\Tools\MSVC\14.16.27023\include\string:170:33: note: candidate template ignored: could not match 'basic_string<type-parameter-0-0, type-parameter-0-1, type-parameter-0-2>' against 'parquet
::ParquetInvalidOrCorruptedFileException'
        basic_ostream<_Elem, _Traits>& operator<<(
                                       ^
1 error generated.
[54/62] Building CXX object src/parquet/CMakeFiles/parquet_shared.dir/Unity/unity_0_cxx.cxx.obj
ninja: build stopped: subcommand failed.
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-07-28 19:42:18,8
13319814,[FlightRPC][Java] Clean up DoPut/FlightStream memory handling,"We've been running into issues with DoPut in Java. In particular:
 * Closing a FlightStream without draining it should not send a cancellation to the other side (or should send the cancellation, but also drain the queue). A server will have sent an explicit error message, or will simply just not want to read the entire stream. A client should explicitly cancel/gRPC will cancel for you anyways when you end the call. Also, the gRPC call may already have ended and cancelling the call may result in a runtime exception.
 * Cancelling a FlightStream explicitly should not immediately mark the stream as completed - it should wait for gRPC to acknowledge the cancellation as there may be undelivered messages.
 * Make sure there is no race between the gRPC observer in the FlightStream and the consumer. (Ideally the only way for a FlightStream to end is for the observer to end the stream; that does open us up to the possibility of a FlightStream being stuck forever for servers that do not respect cancellation.)
 * The server should close/clean up things properly in DoPut (it should act like DoExchange and tie closing of the stream to the onCompleted/onError callbacks). Otherwise trying to use it with ARROW-9586 becomes impossible (you need to close the FlightStream before ending the call, or you'll close the per-call allocator before you close the FlightStream)

I think this also ties into flakiness in unit tests.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Improvement,Major,2020-07-28 17:48:48,0
13319813,[FlightRPC][Java] Allow using a per-call Arrow allocator,"We've been running into issues with Flight and gRPC leaking direct memory at scale. One thing we'd like to do is have a (child) allocator per DoGet/DoPut call, so we can more accurately track memory usage. We have a candidate implementation that is rather messy, but can be upstreamed as part of flight-grpc.

This also requires changes to _ensure_ all Arrow resources are cleaned up before we notify gRPC that the call has finished.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Improvement,Major,2020-07-28 17:45:40,0
13319750,[Rust] Implement Array::memory_size(),I would like to be able to determine how much memory is being used by Arrow Arrays so that I can better monitor and report on memory usage when profiling and tuning code.,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-07-28 13:14:58,10
13319729,[Dev][Release] Bump next snapshot versions to 2.0.0,"The upcoming major release will have version 2.0.0, update the hardcoded version numbers.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-07-28 11:22:35,3
13319637,[Python][C++] posix_madvise error on Debian in pyarrow 1.0.0,"The following writes and reads back from a Parquet file in both pyarrow 0.17.0 and 1.0.0 on Ubuntu 18.04:

{code:java}
>>> import pyarrow.parquet
>>> a = pyarrow.array([[1.1, 2.2, 3.3], [], [4.4, 5.5]])
>>> t = pyarrow.Table.from_batches([pyarrow.RecordBatch.from_arrays([a], [""stuff""])])
>>> pyarrow.parquet.write_table(t, ""stuff.parquet"")
>>> t2 = pyarrow.parquet.read_table(""stuff.parquet"") {code}

However, the same thing raises the following exception on Debian 9 (stretch) in pyarrow 1.0.0 but not in pyarrow 0.17.0:
{code:java}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jpivarski/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py"", line 1564, in read_table
    filters=filters,
  File ""/home/jpivarski/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py"", line 1433, in __init__
    partitioning=partitioning)
  File ""/home/jpivarski/miniconda3/lib/python3.7/site-packages/pyarrow/dataset.py"", line 667, in dataset
    return _filesystem_dataset(source, **kwargs)
  File ""/home/jpivarski/miniconda3/lib/python3.7/site-packages/pyarrow/dataset.py"", line 434, in _filesystem_dataset
    return factory.finish(schema)
  File ""pyarrow/_dataset.pyx"", line 1451, in pyarrow._dataset.DatasetFactory.finish
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
OSError: posix_madvise failed. Detail: [errno 0] Success{code}
It's a little odd that the error says that it failed with ""detail: success"". That suggests to me that an ""if"" predicate is backward (missing ""not""), which might only be triggered on some OS/distributions.",pull-request-available,['Python'],ARROW,Bug,Major,2020-07-27 23:24:26,2
13319616,[R] gcc-UBSAN failure on CRAN,"https://www.stats.ox.ac.uk/pub/bdr/memtests/gcc-UBSAN/arrow/tests/testthat.Rout shows 

{code}

R Under development (unstable) (2020-07-24 r78910) -- ""Unsuffered Consequences""
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # Licensed to the Apache Software Foundation (ASF) under one
> # or more contributor license agreements.  See the NOTICE file
> # distributed with this work for additional information
> # regarding copyright ownership.  The ASF licenses this file
> # to you under the Apache License, Version 2.0 (the
> # ""License""); you may not use this file except in compliance
> # with the License.  You may obtain a copy of the License at
> #
> #   http://www.apache.org/licenses/LICENSE-2.0
> #
> # Unless required by applicable law or agreed to in writing,
> # software distributed under the License is distributed on an
> # ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
> # KIND, either express or implied.  See the License for the
> # specific language governing permissions and limitations
> # under the License.
> 
> library(testthat)
> library(arrow)

Attaching package: 'arrow'

The following object is masked from 'package:testthat':

    matches

The following object is masked from 'package:utils':

    timestamp

> library(tibble)
> 
> test_check(""arrow"")
/tmp/Rtmp1j26sr/working_dir/Rtmpv45o92/file767be1f86b2af/apache-arrow-1.0.0/cpp/src/parquet/arrow/path_internal.cc:517:8: runtime error: load of value 190, which is not a valid value for type 'bool'
    #0 0x7f06849027ae in parquet::arrow::(anonymous namespace)::PathInfo::PathInfo(parquet::arrow::(anonymous namespace)::PathInfo const&) [clone .cold] (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xc1fc7ae)
    #1 0x7f06867cd87c in void parquet::arrow::(anonymous namespace)::PathBuilder::AddTerminalInfo<arrow::NumericArray<arrow::Int32Type> >(arrow::NumericArray<arrow::Int32Type> const&) (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xe0c787c)
    #2 0x7f0686874010 in arrow::Status arrow::VisitArrayInline<parquet::arrow::(anonymous namespace)::PathBuilder>(arrow::Array const&, parquet::arrow::(anonymous namespace)::PathBuilder*) (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xe16e010)
    #3 0x7f068687bdb4 in parquet::arrow::MultipathLevelBuilder::Make(arrow::Array const&, bool) (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xe175db4)
    #4 0x7f0686c19207 in parquet::arrow::FileWriterImpl::WriteColumnChunk(std::shared_ptr<arrow::ChunkedArray> const&, long, long) (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xe513207)
    #5 0x7f0686a3158a in parquet::arrow::FileWriterImpl::WriteTable(arrow::Table const&, long)::{lambda(long, long)#1}::operator()(long, long) const (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xe32b58a)
    #6 0x7f0686a4fac5 in parquet::arrow::FileWriterImpl::WriteTable(arrow::Table const&, long) (/data/gannet/ripley/R/packages/tests-gcc-SAN/arrow.Rcheck/arrow/libs/arrow.so+0xe349ac5)
    #7 0x7f0685023f0f in parquet___arrow___FileWriter__WriteTable(std::shared_ptr<parquet::arrow::FileWriter> const&, std::shared_ptr<arrow::Table> const&, long) /data/gannet/ripley/R/packages/tests-gcc-SAN/arrow/src/parquet.cpp:246
    #8 0x7f0684df514f in _arrow_parquet___arrow___FileWriter__WriteTable /data/gannet/ripley/R/packages/tests-gcc-SAN/arrow/src/arrowExports.cpp:4527
    #9 0x57c023 in R_doDotCall /data/gannet/ripley/R/svn/R-devel/src/main/dotcode.c:604
    #10 0x6361b1 in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7670
    #11 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #12 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #13 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #14 0x66de2f in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:846
    #15 0x6769e6 in do_begin /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:2508
    #16 0x66e258 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:798
    #17 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #18 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #19 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #20 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #21 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #22 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #23 0x66de2f in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:846
    #24 0x6769e6 in do_begin /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:2508
    #25 0x66e258 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:798
    #26 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #27 0x66db73 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:746
    #28 0x6769e6 in do_begin /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:2508
    #29 0x66e258 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:798
    #30 0x67d517 in do_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:3335
    #31 0x62584b in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7114
    #32 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #33 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #34 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #35 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #36 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #37 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #38 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #39 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #40 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #41 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #42 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #43 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #44 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #45 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #46 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #47 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #48 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #49 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #50 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #51 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #52 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #53 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #54 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #55 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #56 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #57 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #58 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #59 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #60 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #61 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #62 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #63 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #64 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #65 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #66 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #67 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #68 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #69 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #70 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #71 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #72 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #73 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #74 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #75 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #76 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #77 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #78 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #79 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #80 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #81 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #82 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #83 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #84 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #85 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #86 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #87 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #88 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #89 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #90 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #91 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #92 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #93 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #94 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #95 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #96 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #97 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #98 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #99 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #100 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #101 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #102 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #103 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #104 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #105 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #106 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #107 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #108 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #109 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #110 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #111 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #112 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #113 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #114 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #115 0x66de2f in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:846
    #116 0x6769e6 in do_begin /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:2508
    #117 0x66e258 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:798
    #118 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #119 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #120 0x66de2f in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:846
    #121 0x67df57 in do_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:3358
    #122 0x62584b in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7114
    #123 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #124 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #125 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #126 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #127 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #128 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #129 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #130 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #131 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #132 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #133 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #134 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #135 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #136 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #137 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #138 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #139 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #140 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #141 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #142 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #143 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #144 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #145 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #146 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #147 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #148 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #149 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #150 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #151 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #152 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #153 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #154 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #155 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #156 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #157 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #158 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #159 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #160 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #161 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #162 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #163 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #164 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #165 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #166 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #167 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #168 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #169 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #170 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #171 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #172 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #173 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #174 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #175 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #176 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #177 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #178 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #179 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #180 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #181 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #182 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #183 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #184 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #185 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #186 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #187 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #188 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #189 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #190 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #191 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #192 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #193 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #194 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #195 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #196 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #197 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #198 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #199 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #200 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #201 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #202 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #203 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #204 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #205 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #206 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #207 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #208 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #209 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #210 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #211 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #212 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #213 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #214 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #215 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #216 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #217 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #218 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #219 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #220 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #221 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #222 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #223 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #224 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #225 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #226 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #227 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #228 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #229 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #230 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #231 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #232 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #233 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #234 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #235 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #236 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #237 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #238 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #239 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #240 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #241 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #242 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #243 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #244 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #245 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #246 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #247 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #248 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #249 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #250 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #251 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #252 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #253 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #254 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #255 0x67b5bd in R_forceAndCall /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1955
    #256 0x44c07a in do_lapply /data/gannet/ripley/R/svn/R-devel/src/main/apply.c:70
    #257 0x7179d9 in do_internal /data/gannet/ripley/R/svn/R-devel/src/main/names.c:1385
    #258 0x620afc in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7134
    #259 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #260 0x672834 in R_execClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1888
    #261 0x674ce4 in Rf_applyClosure /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:1814
    #262 0x643fae in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:7082
    #263 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #264 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #265 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #266 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #267 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866
    #268 0x66d567 in Rf_eval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:723
    #269 0x66f5a2 in forcePromise /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:551
    #270 0x66fd37 in FORCE_PROMISE /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5135
    #271 0x66fd37 in getvar /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:5176
    #272 0x6341da in bcEval /data/gannet/ripley/R/svn/R-devel/src/main/eval.c:6866

 testthat results  
[ OK: 2382 | SKIPPED: 34 | WARNINGS: 0 | FAILED: 0 ]
> 
> proc.time()
   user  system elapsed 
 67.988   5.209  71.229 
{code}

I know we have an R sanitizer job that runs but it didn't catch this.",pull-request-available,['R'],ARROW,Bug,Critical,2020-07-27 20:42:42,4
13319614,[R] Cleanups for CRAN 1.0.0 release,"Fixes a bad URL and removes arrow.so stripping, which unfortunately caused a failed incoming check.",pull-request-available,"['Packaging', 'R']",ARROW,Bug,Major,2020-07-27 20:37:48,4
13319600,[Python] Parquet doesn't load when partitioned column starts with '_',"When the loading parquet with partitioned column that starts with an underscore '_', nothing is loaded. No exceptions are raised either. Loading this parquet have worked for me in pyarrow 0.17.1, but not working anymore in pyarrow 1.0.0.

On the other hand, loading parquet with a partitioned column starting with '_' is possible by using the `use_legacy_dataset` option. Also, when the column that starts with an underscore is not a partitioned column, loading parquet seems to work as expected.

{code:python}
>>> import pyarrow as pa
>>> import pyarrow.parquet as pq
>>> import pandas as pd
>>> df1 = pd.DataFrame(data={'_COL_1': [1, 2], 'COL_2': [3, 4], 'COL_3': [5, 6]})
>>> table1 = pa.Table.from_pandas(df1)
>>> pq.write_to_dataset(table1, partition_cols=['_COL_1', 'COL_2'], root_path='test_parquet1')
>>> df_pq1 = pq.read_table('test_parquet1')
>>> df_pq1
pyarrow.Table
>>> len(df_pq1)
0
>>> df_pq1_legacy = pq.read_table('test_parquet1', use_legacy_dataset=True)
pyarrow.Table
COL_3: int64
_COL_1: dictionary<values=int64, indices=int32, ordered=0>
COL_2: dictionary<values=int64, indices=int32, ordered=0>
>>> len(df_pq1_legacy)
2
>>> df2 = pd.DataFrame(data={'COL_1': [1, 2], 'COL_2': [3, 4], '_COL_3': [5, 6]})
>>> table2 = pa.Table.from_pandas(df2)
>>> pq.write_to_dataset(table2, partition_cols=['COL_1', 'COL_2'], root_path='test_parquet2')
>>> df_pq2 = pq.read_table('test_parquet2')
>>> df_pq2
pyarrow.Table
_COL_3: int64
COL_1: int32
COL_2: int32
>>> len(df_pq2)
2
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-07-27 19:00:20,6
13319593,[Doc] Clean up sphinx sidebar,"See comment on https://github.com/apache/arrow-site/pull/67

The sidebar text is too wide to fit (at least on my screen).",pull-request-available,['Documentation'],ARROW,Bug,Major,2020-07-27 18:43:55,4
13319583,[CI] Use official msys action on GHA,"There is now an official msys2 action: https://github.com/msys2/setup-msys2

We could probably delete some of our custom msys setup scripts.

cc [~kou]",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-07-27 17:46:18,1
13319515,[Dev][Release] Use archery's changelog generator when creating release notes for the website ,The previous changelog generation script has been removed.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-07-27 09:57:32,3
13319326,[Rust] [DataFusion] Revert privatization of exprlist_to_fields,"This function was privatized by mistake by cd503c3f583dab.
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-07-26 04:10:54,9
13319232,[Python][C++] Segfaults in UnionArray with null values,"Extracting null values from a UnionArray containing nulls and constructing a UnionArray with a bitmask in pyarrow.Array.from_buffers causes segfaults in pyarrow 1.0.0. I have an environment with pyarrow 0.17.0 and all of the following run correctly without segfaults in the older version.

Here's a UnionArray that works (because there are no nulls):


{code:java}
# GOOD
a = pyarrow.UnionArray.from_sparse(
 pyarrow.array([0, 1, 0, 0, 1], type=pyarrow.int8()),
 [
 pyarrow.array([0.0, 1.1, 2.2, 3.3, 4.4]),
 pyarrow.array([True, True, False, True, False]),
 ],
)
a.to_pylist(){code}


Here's one the fails when you try a.to_pylist() or even just a[2], because one of the children has a null at 2:


{code:java}
# SEGFAULT
a = pyarrow.UnionArray.from_sparse(
 pyarrow.array([0, 1, 0, 0, 1], type=pyarrow.int8()),
 [
 pyarrow.array([0.0, 1.1, None, 3.3, 4.4]),
 pyarrow.array([True, True, False, True, False]),
 ],
)
a.to_pylist() # also just a[2] causes a segfault{code}


Here's another that fails because both children have nulls; the segfault occurs at both positions with nulls:


{code:java}
# SEGFAULT
a = pyarrow.UnionArray.from_sparse(
 pyarrow.array([0, 1, 0, 0, 1], type=pyarrow.int8()),
 [
 pyarrow.array([0.0, 1.1, None, 3.3, 4.4]),
 pyarrow.array([True, None, False, True, False]),
 ],
)
a.to_pylist() # also a[1] and a[2] cause segfaults{code}


Here's one that succeeds, but it's dense, rather than sparse:


{code:java}
# GOOD
a = pyarrow.UnionArray.from_dense(
 pyarrow.array([0, 1, 0, 0, 0, 1, 1], type=pyarrow.int8()),
 pyarrow.array([0, 0, 1, 2, 3, 1, 2], type=pyarrow.int32()),
 [pyarrow.array([0.0, 1.1, 2.2, 3.3]), pyarrow.array([True, True, False])],
)
a.to_pylist(){code}


Here's a dense that fails because one child has a null:


{code:java}
# SEGFAULT
a = pyarrow.UnionArray.from_dense(
 pyarrow.array([0, 1, 0, 0, 0, 1, 1], type=pyarrow.int8()),
 pyarrow.array([0, 0, 1, 2, 3, 1, 2], type=pyarrow.int32()),
 [pyarrow.array([0.0, 1.1, None, 3.3]), pyarrow.array([True, True, False])],
)
a.to_pylist() # also just a[3] causes a segfault{code}


Here's a dense that fails in two positions because both children have a null:


{code:java}
# SEGFAULT
a = pyarrow.UnionArray.from_dense(
 pyarrow.array([0, 1, 0, 0, 0, 1, 1], type=pyarrow.int8()),
 pyarrow.array([0, 0, 1, 2, 3, 1, 2], type=pyarrow.int32()),
 [pyarrow.array([0.0, 1.1, None, 3.3]), pyarrow.array([True, None, False])],
)
a.to_pylist() # also a[3] and a[5] cause segfaults{code}


In all of the above, we created the UnionArray using its from_dense method. We could instead create it with pyarrow.Array.from_buffers. If created with content0 and content1 that have no nulls, it's fine, but if created with nulls in the content, it segfaults as soon as you view the null value.


{code:java}
# GOOD
content0 = pyarrow.array([0.0, 1.1, 2.2, 3.3, 4.4])
content1 = pyarrow.array([True, True, False, True, False])
# SEGFAULT
content0 = pyarrow.array([0.0, 1.1, 2.2, None, 4.4])
content1 = pyarrow.array([True, True, False, True, False])
types = pyarrow.union(
 [pyarrow.field(""0"", content0.type), pyarrow.field(""1"", content1.type)],
 ""sparse"",
 [0, 1],
)
a = pyarrow.Array.from_buffers(
 types,
 5,
 [
 None,
 pyarrow.py_buffer(numpy.array([0, 1, 0, 0, 1], numpy.int8)),
 ],
 children=[content0, content1],
)
a.to_pylist() # also just a[3] causes a segfault{code}


Similarly for a dense union.


{code:java}
# GOOD
content0 = pyarrow.array([0.0, 1.1, 2.2, 3.3])
content1 = pyarrow.array([True, True, False])
# SEGFAULT
content0 = pyarrow.array([0.0, 1.1, None, 3.3])
content1 = pyarrow.array([True, True, False])
types = pyarrow.union(
 [pyarrow.field(""0"", content0.type), pyarrow.field(""1"", content1.type)],
 ""dense"",
 [0, 1],
)
a = pyarrow.Array.from_buffers(
 types,
 7,
 [
 None,
 pyarrow.py_buffer(numpy.array([0, 1, 0, 0, 0, 1, 1], numpy.int8)),
 pyarrow.py_buffer(numpy.array([0, 0, 1, 2, 3, 1, 2], numpy.int32)),
 ],
 children=[content0, content1],
)
a.to_pylist() # also just a[3] causes a segfault{code}


The next segfaults are different: instead of putting the null values in the content, we put the null value in the UnionArray itself. This time, it segfaults when it is being created. It also prints some output (all of the above were silent segfaults).


{code:java}
# SEGFAULT (even to create)
content0 = pyarrow.array([0.0, 1.1, 2.2, 3.3, 4.4])
content1 = pyarrow.array([True, True, False, True, False])
types = pyarrow.union(
 [pyarrow.field(""0"", content0.type), pyarrow.field(""1"", content1.type)],
 ""sparse"",
 [0, 1],
)
a = pyarrow.Array.from_buffers(
 types,
 5,
 [
 pyarrow.py_buffer(numpy.array([251], numpy.uint8)), # (11111011)
 pyarrow.py_buffer(numpy.array([0, 1, 0, 0, 1], numpy.int8)),
 # exepct null here -----^
# None <--- placeholder required in pyarrow 0.17.0, not 1.0.0
 ],
 children=[content0, content1],
)
# /arrow/cpp/src/arrow/array/array_nested.cc:617: Check failed: (data_->buffers[0]) == (nullptr) 
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(+0x4e9938)[0x7feea9937938]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow4util8ArrowLogD1Ev+0xdd)[0x7feea993814d]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow16SparseUnionArray7SetDataESt10shared_ptrINS_9ArrayDataEE+0x144)[0x7feea9a869a4]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow16SparseUnionArrayC1ESt10shared_ptrINS_9ArrayDataEE+0x5a)[0x7feea9a86a2a]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow15VisitTypeInlineINS_8internal16ArrayDataWrapperEEENS_6StatusERKNS_8DataTypeEPT_+0x9fc)[0x7feea9a5145c]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow9MakeArrayERKSt10shared_ptrINS_9ArrayDataEE+0x3f)[0x7feea9a2698f]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/lib.cpython-38-x86_64-linux-gnu.so(+0x1c7853)[0x7feeaa998853]
# python(+0x13af9e)[0x56146ee77f9e]
# python(_PyObject_MakeTpCall+0x3bf)[0x56146ee6d30f]
# python(_PyEval_EvalFrameDefault+0x5452)[0x56146ef20602]
# python(_PyEval_EvalCodeWithName+0x260)[0x56146ef06190]
# python(PyEval_EvalCode+0x23)[0x56146ef07a03]
# python(+0x23e2f2)[0x56146ef7b2f2]
# python(+0x251082)[0x56146ef8e082]
# python(+0x1063b9)[0x56146ee433b9]
# python(PyRun_InteractiveLoopFlags+0xea)[0x56146ee43559]
# python(+0x1065f3)[0x56146ee435f3]
# python(+0x106817)[0x56146ee43817]
# python(Py_BytesMain+0x39)[0x56146ef91a19]
# /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7)[0x7feeac198b97]
# python(+0x1f8807)[0x56146ef35807]
# Aborted (core dumped)
{code}


And similarly for dense.


{code:java}
# SEGFAULT (even to create)
content0 = pyarrow.array([0.0, 1.1, 2.2, 3.3])
content1 = pyarrow.array([True, True, False])
types = pyarrow.union(
 [pyarrow.field(""0"", content0.type), pyarrow.field(""1"", content1.type)],
 ""dense"",
 [0, 1],
)
a = pyarrow.Array.from_buffers(
 types,
 7,
 [
 pyarrow.py_buffer(numpy.array([251], numpy.uint8)), # (11111011)
 pyarrow.py_buffer(numpy.array([0, 1, 0, 0, 0, 1, 1], numpy.int8)),
 pyarrow.py_buffer(numpy.array([0, 0, 1, 2, 3, 1, 2], numpy.int32)),
 # exepct null here -----^
 ],
 children=[content0, content1],
)
# /arrow/cpp/src/arrow/array/array_nested.cc:627: Check failed: (data_->buffers[0]) == (nullptr) 
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(+0x4e9938)[0x7f2fb6ad7938]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow4util8ArrowLogD1Ev+0xdd)[0x7f2fb6ad814d]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow15DenseUnionArray7SetDataERKSt10shared_ptrINS_9ArrayDataEE+0x174)[0x7f2fb6c274a4]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow15DenseUnionArrayC2ERKSt10shared_ptrINS_9ArrayDataEE+0x44)[0x7f2fb6c27524]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow15VisitTypeInlineINS_8internal16ArrayDataWrapperEEENS_6StatusERKNS_8DataTypeEPT_+0xb14)[0x7f2fb6bf1574]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/libarrow.so.100(_ZN5arrow9MakeArrayERKSt10shared_ptrINS_9ArrayDataEE+0x3f)[0x7f2fb6bc698f]
# /home/pivarski/miniconda3/envs/test-arrow/lib/python3.8/site-packages/pyarrow/lib.cpython-38-x86_64-linux-gnu.so(+0x1c7853)[0x7f2fb7b38853]
# python(+0x13af9e)[0x558cf09edf9e]
# python(_PyObject_MakeTpCall+0x3bf)[0x558cf09e330f]
# python(_PyEval_EvalFrameDefault+0x5452)[0x558cf0a96602]
# python(_PyEval_EvalCodeWithName+0x260)[0x558cf0a7c190]
# python(PyEval_EvalCode+0x23)[0x558cf0a7da03]
# python(+0x23e2f2)[0x558cf0af12f2]
# python(+0x251082)[0x558cf0b04082]
# python(+0x1063b9)[0x558cf09b93b9]
# python(PyRun_InteractiveLoopFlags+0xea)[0x558cf09b9559]
# python(+0x1065f3)[0x558cf09b95f3]
# python(+0x106817)[0x558cf09b9817]
# python(Py_BytesMain+0x39)[0x558cf0b07a19]
# /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7)[0x7f2fb9338b97]
# python(+0x1f8807)[0x558cf0aab807]
# Aborted (core dumped){code}


It might be two distinct bugs, but they're both related to UnionArrays and nulls, and they're both newer than 0.17.0.",pull-request-available,['Python'],ARROW,Bug,Major,2020-07-24 22:24:44,3
13319220,[Rust] [DataFusion] Add inner (hash) equijoin physical plan,"Here is an overview of how I think we should implement support for equijoins, at least for the initial implementation.
 * Read all batches from the left-side of the join into a single Vec<RecordBatch>
 * Create a map something like HashMap<Vec<ScalarValue>, Vec<(usize,usize)>> to map keys to batch/row indices
 * Iterate over this Vec<RecordBatch> and create an entry in a hash map, mapping the join keys to the index of the batch and row in the Vec<RecordBatch>
 * For each input partition on the right-side of the join, return an output partition that is an iterator/stream that:
 ** For each input row, evaluate the join keys
 ** Look up those join keys in the hash map
 ** If a match is found:
 *** For each (batch, row) index create an output row which has the values from both the left and right row and emit it
 ** If no match is found:
 *** Do not emit a row",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2020-07-24 19:24:23,9
13319180,[Rust] Release script doesn't bump parquet crate's arrow dependency version,"After rebasing the master the rust builds have started to fail.

The solution is to bump a version number gere https://github.com/apache/arrow/pull/7829",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-07-24 14:07:31,3
13319168,[Rust] Parquet no longer builds,"I believe that there is a typo in {{rust/parquet/Cargo.toml}}:

it reads {{arrow = { path = ""../arrow"", version = ""1.0.0-SNAPSHOT"", optional = true }}}, but it should read {{arrow = { path = ""../arrow"", version = ""1.1.0-SNAPSHOT"", optional = true }}}, or the project does not compile.

",pull-request-available,['Rust'],ARROW,Bug,Major,2020-07-24 13:21:41,10
13318757,[C++] Simplify parsing/conversion utilities,"Various improvement ideas extracted from https://github.com/apache/arrow/pull/7793 , see https://github.com/apache/arrow/commit/740d8132d3220b0bcddde138bd1ab70030e227fd

- provide a convenience function FormatValue() to complement ParseValue()
- parameterize both parsing and formatting with the corresponding DataType subclass (for example a formatting or parsing a timestamp requires a TimeUnit and this can be derived from a TimestampType)
- rename StringConverter and StringFormatter to ParseValueTraits and FormatValueTraits, to emphasize the convenience functions as the preferred interface
- ParseValue should accept a string_view for simplicity, and an overload should be provided which returns an optional<>",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-07-22 18:20:19,6
13318290,[Packaging][Release] Update conda forge dependency pins,All of the nightly conda package builds are failing with dependency resolution issues: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-07-20-0-azure-conda,pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-07-20 16:04:40,3
13318223,[Dev][Release] Improvements to release verification scripts,"- Install all test dependencies for running Python unit tests on Windows
- Run unit tests when verifying wheels
- Add option to run the C++/Python RC verification without Gandiva (didn't have LLVM configured properly on one machine)",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-07-20 10:33:07,14
13318179,[Rust] Remove un-needed dev-dependencies,We currently have some dev-dependencies that are not needed. Let's remove them,pull-request-available,['Rust'],ARROW,Bug,Major,2020-07-20 07:03:50,9
13317523,[Rust] Improve error message when getting a field by name from schema,"Currently, when a name that does not exist on the Schema is passed to{{Schema::index_of}}, the error message is just the name itself. This makes it a bit difficult to debug.



I propose that we change the that error message to something like

{{Unable to get field named ""nickname"". Valid fields: [""first_name"", ""last_name"", ""address""]}}

to make it easier to understand.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-07-18 06:20:34,9
13317508,[Python] Deprecate pyarrow serialization,Per mailing list discussion,pull-request-available pyarrow-serialization,['Python'],ARROW,Improvement,Major,2020-07-18 00:21:09,5
13317413,[C++] Variadic template unpack inside lambda doesn't compile with gcc,Gandiva nightlies have been failing for a past couple of days,pull-request-available,['C++'],ARROW,Bug,Major,2020-07-17 13:23:12,14
13317380,[Packaging][Release] Set conda packages' build number to 0,The artifacts patterns in the crossbow tasks definitions expect conda packages with build number 0. We should keep the build number always zero in the vendored conda-forge recipes.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-07-17 10:42:07,3
13317378,[Release] Don't test Gandiva in the windows wheel verification script,Since Gandiva is no longer included.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-07-17 10:38:28,3
13317233,[Packaging][Python] Fix macOS wheel build failures,"- Python 3.7 tests were failing because of too many open files.
- There is an ssl issues during the artifact uploading in the 3.5 build. ",pull-request-available,['Packaging'],ARROW,Bug,Major,2020-07-16 18:53:52,3
13317111,[Python]Segmentation fault on ChunkedArray.take,"This leads to a segementation fault with the latest conda nigthlies on Python 3.8 / macOS

{code}
import pyarrow as pa
import numpy as np

arr = pa.chunked_array([
  [
    ""m"",
    ""J"",
    ""q"",
    ""k"",
    ""t""
  ],
  [
    ""m"",
    ""J"",
    ""q"",
    ""k"",
    ""t""
  ]
])

indices = np.array([0, 5, 1, 6, 2, 7, 3, 8, 4, 9])
arr.take(indices)
{code}",pull-request-available,['Python'],ARROW,Bug,Critical,2020-07-16 08:00:18,3
13317065,[C++] Fix segfault with std::to_string in -O3 builds on gcc 7.5.0,"There seems to be a gcc bug related to {{std::to_string}} that only appears in {{-O3}} builds. It can be seen in something innocuous like

{code}
    return Status::Invalid(""Float value "", std::to_string(val), "" was truncated converting to"",
                           *output.type());
{code}

where val is NaN. I haven't found a canonical reference but using something other than to_string for the formatting (here just letting {{std::ostringstream}} take care of it) makes the problem go away

I wasn't able to reproduce the issue with gcc-8

EDIT: it's a known bug in gcc https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86274",pull-request-available,['C++'],ARROW,Bug,Major,2020-07-16 03:04:14,14
13317047,[C++][Parquet] Fix failure caused by malformed repetition/definition levels,Fix a case discovered by OSS-Fuzz,pull-request-available,['C++'],ARROW,Bug,Major,2020-07-16 00:10:37,14
13317025,[C++] Equality assertions don't handle Inf /  -Inf properly,"I got this error when working on a PR which added unit tests:
{code}
../src/arrow/testing/gtest_util.cc:101: Failure
Failed
Expected:
  [
    2.5,
    inf,
    -inf
  ]
Actual:
  [
    2.5,
    inf,
    -inf
  ]
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-07-15 20:55:45,7
13317005,[Python][Dataset] Dictionary encode string partition columns by default,"ARROW-9139 switched the default of use_legacy_dataset from True to False, but left dictionary encoding of string partition columns off by default.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-07-15 18:52:21,6
13316983,[C++][Dataset] Support implicit casting InExpression::set_ to dict,{{test_filters_inclusive_set}} is still failing due to lack of support for cast to dictionary. Add fallbacks to DictionaryEncode if conversion to a dictionary array is required,pull-request-available,['C++'],ARROW,Bug,Major,2020-07-15 17:12:14,6
13316961,[Docs] Update is* functions to be is_* in the compute docs,Followup to the followup ARROW-9390,pull-request-available,['Documentation'],ARROW,Improvement,Major,2020-07-15 15:32:56,4
13316936,[C++] Improve error message on unsupported cast types,"Currently, the error message when trying an unsupported cast looks like this:
{code}
No cast function available to cast to dictionary<values=string, indices=int32, ordered=0>
{code}

It would be more informative if the source type was also mentioned.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-07-15 13:53:52,2
13316904,[C++][Dataset] HivePartitioning discovery with dictionary types fails for multiple fields,"Apparently, ARROW-9288 was not fully / correctly fixing the issue. With a single string partition field, it now works fine. But once you have multiple string fields, you get parsing errors.

A reproducible example:

{code}
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds 

foo_keys = np.array(['a', 'b', 'c'], dtype=object)
bar_keys = np.array(['d', 'e', 'f'], dtype=object)
N = 30

table = pa.table({
    'foo': foo_keys.repeat(10),
    'bar': np.tile(np.tile(bar_keys, 5), 2),
    'values': np.random.randn(N)
})

base_path = ""test_partition_directories3""
pq.write_to_dataset(table, base_path, partition_cols=[""bar"", ""foo""])

# works
ds.dataset(base_path, partitioning=""hive"")
# fails
part = ds.HivePartitioning.discover(max_partition_dictionary_size=-1)
ds.dataset(base_path, partitioning=part)
{code}


cc [~bkietz]

",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-07-15 12:00:47,6
13316758,[R] Provide configurable MetadataVersion in IPC API and environment variable to set default to V4 when needed,See ARROW-9395 for the Python version of this.,pull-request-available,['R'],ARROW,New Feature,Blocker,2020-07-14 18:14:41,4
13316737,[CI][Java] Run Maven in parallel,"It looks like Maven nowadays supports multi-threaded builds, but we're not using them:
https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3
",pull-request-available,"['Continuous Integration', 'Developer Tools', 'Java']",ARROW,Improvement,Minor,2020-07-14 16:35:13,2
13316728,[Python] Make more objects weakrefable,"Currently, some PyArrow objects (like Array) are weakrefable, but others (like Buffer) are not. There's no reason not to allow that, it just needs the required (short) boilerplate.",pull-request-available,['Python'],ARROW,Wish,Major,2020-07-14 15:29:40,2
13316704,[Python] Improve ergonomics of compute functions,"Introspection of exported compute functions currently yield suboptimal output:
{code:python}
>>> from pyarrow import compute as pc                                                                                                                                 
>>> pc.list_flatten                                                                                                                                                   
<function pyarrow.compute._simple_unary_function.<locals>.func(arg)>
>>> ?pc.list_flatten                                                                                                                                                  
Signature: pc.list_flatten(arg)
Docstring: <no docstring>
File:      ~/arrow/dev/python/pyarrow/compute.py
Type:      function
>>> help(pc.list_flatten)                                                                                                                                             
Help on function func in module pyarrow.compute:

func(arg)
{code}

The function should ideally have:
* the right global name
* an appropriate signature
* a docstring
",pull-request-available,['Python'],ARROW,Wish,Major,2020-07-14 13:19:44,2
13316701,[Rust] [DataFusion] Physical plan refactor to support optimization rules and more efficient use of threads,"I would like to propose a refactor of the physical/execution planning based on the experience I have had in implementing distributed execution in Ballista.

This will likely need subtasks but here is an overview of the changes I am proposing.
h3. *Introduce physical plan optimization rule to insert ""shuffle"" operators*

We should extend the ExecutionPlan trait so that each operator can specify its input and output partitioning needs, and then have an optimization rule that can insert any repartitioning or reordering steps required.

For example, these are the methods to be added to ExecutionPlan. This design is based on Apache Spark.


{code:java}
/// Specifies how data is partitioned across different nodes in the cluster
fn output_partitioning(&self) -> Partitioning {
    Partitioning::UnknownPartitioning(0)
}

/// Specifies the data distribution requirements of all the children for this operator
fn required_child_distribution(&self) -> Distribution {
    Distribution::UnspecifiedDistribution
}

/// Specifies how data is ordered in each partition
fn output_ordering(&self) -> Option<Vec<SortOrder>> {
    None
}

/// Specifies the data distribution requirements of all the children for this operator
fn required_child_ordering(&self) -> Option<Vec<Vec<SortOrder>>> {
    None
}
 {code}
A good example of applying this rule would be in the case of hash aggregates where we perform a partial aggregate in parallel across partitions and then coalesce the results and apply a final hash aggregate.

Another example would be a SortMergeExec specifying the sort order required for its children.



",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-07-14 13:10:14,10
13316684,[Rust] Reading Date32 and Date64 errors - they are incorrectly converted to RecordBatch,"Steps to reproduce:

1. Create a file `a.parquet` using the following code:


{code:python}
import pyarrow.parquet
import numpy


def _data_datetime(f):
    data = numpy.array([
        numpy.datetime64('2018-08-18 23:25'),
        numpy.datetime64('2019-08-18 23:25'),
        numpy.datetime64(""NaT"")
    ])
    data = numpy.array(data, dtype=f'datetime64[{f}]')
    return data

def _write_parquet(path, data):
    table = pyarrow.Table.from_arrays([pyarrow.array(data)], names=['a'])
    pyarrow.parquet.write_table(table, path)
    return path


_write_parquet('a.parquet', _data_datetime('D'))
{code}

2. Write a small example to read it to RecordBatches

3. observe the error {{ArrowError(ParquetError(""InvalidArgumentError(\""column types must match schema types, expected Date32(Day) but found UInt32 at column index 0\"")""))}}



",pull-request-available,['Rust'],ARROW,Bug,Major,2020-07-14 11:56:11,9
13316583,[Rust] [DateFusion] Improve performance of parquet scan,I found a trivial optimization to significantly speed up the parquet scan by changing the channel communication between threads. PR coming up shortly.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-07-14 01:08:33,10
13316577,[Python] Unsigned integer types will accept string values in pyarrow.array,"See

{code}
In [12]: pa.array(['5'], type='uint32')                                                                                                                                                   
Out[12]: 
<pyarrow.lib.UInt32Array object at 0x7fabbdb1edc8>
[
  5
]
{code}

Also:

{code}
In [9]: pa.scalar('5', type='uint8')                                                                                                                                                      
Out[9]: <pyarrow.UInt8Scalar: 5>

In [10]: pa.scalar('5', type='uint16')                                                                                                                                                    
Out[10]: <pyarrow.UInt16Scalar: 5>

In [11]: pa.scalar('5', type='uint32')                                                                                                                                                    
Out[11]: <pyarrow.UInt32Scalar: 5>
{code}

But:

{code}
In [13]: pa.array(['5'], type='int32')                                                                                                                                                    
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-48092f69d948> in <module>
----> 1 pa.array(['5'], type='int32')

~/code/arrow/python/pyarrow/array.pxi in pyarrow.lib.array()
    267     else:
    268         # ConvertPySequence does strict conversion if type is explicitly passed
--> 269         return _sequence_to_array(obj, mask, size, type, pool, c_from_pandas)
    270 
    271 

~/code/arrow/python/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()
     36 
     37     with nogil:
---> 38         check_status(ConvertPySequence(sequence, mask, options, &out))
     39 
     40     if out.get().num_chunks() == 1:

TypeError: an integer is required (got type str)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-07-13 23:58:18,2
13316575,"[Python] ""pytest pyarrow"" takes over 10 seconds to collect tests and start executing",This is caused by the {{_find_new_unicode_codepoints}} function in test_compute.py. ,pull-request-available,['Python'],ARROW,Bug,Major,2020-07-13 23:45:04,2
13316559,[R] Strip arrow.so,"Recent changes ballooned the size of the shared library that gets built in the R package, especially with the bundled libs on linux. It turns out we can add something to the makefile to strip it: http://dirk.eddelbuettel.com/blog/2017/08/14/#009_compact_shared_libraries",pull-request-available,"['Packaging', 'R']",ARROW,Bug,Major,2020-07-13 21:33:34,4
13316557,[Java] Circular initialization between ArrowBuf and BaseAllocator leads to null HistoricalLog for empty buffer,"Still working on a reproduction (this is internal code), but what I see is:
 # Test initializes a RootAllocator in a JUnit @Before
 # BaseAllocator.<clinit> creates a Config
 # The constructor of ImmutableConfig gets the allocation manager
 # This eventually runs NettyAllocationManager.<clinit>
 # This creates a static ArrowBuf as the empty buffer
 # The ArrowBuf has a null HistoricalLog as BaseAllocator hasn't fully initialized yet",pull-request-available,['Java'],ARROW,Bug,Blocker,2020-07-13 21:18:37,0
13316551,[Rust][DataFusion] Allow closures as ScalarUDFs,"Currently the ScalarUDF requires to be a function. However, most applications would prefer to declare a UDF as a closure.

Let's add support for this.",pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-07-13 20:55:34,9
13316540,[C++] Export compiler information in BuildInfo,This may help improve debugging and reporting,pull-request-available,['C++'],ARROW,Improvement,Major,2020-07-13 19:43:12,14
13316532,[Python] Revert Array.equals changes + expose comparison ops in compute,"Revert part of ARROW-5854. Following a mailing list discussion when the Python Scalar classes were added (https://github.com/apache/arrow/pull/7519), to not implement rich comparisons, but keep {{__eq__}} to mean object equality, not element-wise equality.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-07-13 18:57:17,5
13316520,[C++] Bundled bz2 build should only build libbz2,"It appears that the nightly centos8 R build is failing to build bz2. I observed this locally while doing ARROW-9397 but assumed (wrongly, it seems) that there was something transient with bz2 and didn't circle back to debug it. 

For reasons that were not clear, bz2 was failing to build because {{cmp: command not found}}. It turns out that cmp is only called in make test, which we were calling via just make https://sourceware.org/git/?p=bzip2.git;a=blob;f=Makefile;h=b0fef950f361d84a5ec42749529fb34276e2de2d;hb=HEAD#l38. This was doing unnecessary work, aside from testing it was building command line utilities etc., so I made it only build libbz2. It's possible that we need to do something different on MSVC but I don't understand this well enough to be sure.",pull-request-available,"['C++', 'Packaging', 'R']",ARROW,Bug,Major,2020-07-13 18:26:31,4
13316519,[Python] Do not force Validate() to be called in pyarrow_wrap_table,"I have discovered that the forced validation check in pyarrow_wrap_table can add 20-30% time to a call to {{RecordBatchStreamReader.read_all}}, which should be expected to be already valid. ",pull-request-available,['Python'],ARROW,Improvement,Major,2020-07-13 18:25:44,14
13316470,[Python][Packaging] Homebrew fails to install build dependencies in the macOS wheel builds,See build log https://travis-ci.org/github/ursa-labs/crossbow/builds/707597009#L4528,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-07-13 14:13:41,3
13316466,[C++][CI] Valgrind errors in fill_null kernel tests,See log in https://github.com/ursa-labs/crossbow/runs/864988561#step:6:4679,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2020-07-13 13:53:08,2
13316442,"[C++/Python] Kernel for SetItem(BooleanArray, values)",We should have a kernel that allows overriding the values of an array by supplying a boolean mask and a scalar or an array of equal length.,pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2020-07-13 12:07:32,0
13316441,[Python] ChunkedArray.to_numpy,Currently one needs to construct a {{pandas.Series}}and call {{values}}to get a numpy array as a result of {{ChunkedArray}}. We should provide a simpler {{to_numpy}} function that doesn't construct the {{pandas.Series}} overhead.,pull-request-available,['Python'],ARROW,New Feature,Major,2020-07-13 12:05:27,3
13316420,[C++] Update documentation for buffer allocation functions,"The memory allocation of buffers as documented in

[https://arrow.apache.org/docs/cpp/memory.html]

has the unfortunate property that it is indicated as ""obsolete"" when compiling.

The proposed API using the ""Result"" and unique pointer construction is not obvious to use.

Could the documentation be updated for the new API? Complete example would be appreciated especially considering that the use of unique pointer seems to contradict the use of shared pointer in other parts of the API.

",pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Minor,2020-07-13 10:39:40,2
13316412,[Rust][DataFusion] Add pub fn ExecutionContext.tables(),This allows users to know what names can be passed to {{table(&str)}}.,pull-request-available,['Rust - DataFusion'],ARROW,New Feature,Major,2020-07-13 09:50:41,9
13316394,[Rust][DataFusion] Make ExecutionContext sharable between threads,"I have been playing with pyo3 to ship and call this library directly from Python, and the current ExecutionContext can't be used within Python with threads due to the usage of `Box<dyn TableProvider>` on `datasources`, i.e. no `Sync` nor `Send`.

This issue likely affects integration of DataFusion with other libraries.

I propose that we allow ExecutionContext to be sharable between threads, e.g. using {{Box<dyn TableProvider + Send + Sync>}}.
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-07-13 07:32:33,9
13316346,[Rust][DataFusion] Add repartition/shuffle plan,"Some operations (group by, join, over(window.partition_by)) greatly benefit from hash partitioning.

This is a proposal to add hash partitioning (based on a expression) to this library, so that optimizers can be written to optimize the plan based on the required hashing.
",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2020-07-12 17:10:25,9
13316345,"[C++] Test that ""fill_null"" function works with sliced inputs, expand tests",I observed some bugs in the implementation that I did yesterday so adding tests to cover them,pull-request-available,['C++'],ARROW,Bug,Major,2020-07-12 16:45:19,14
13316259,"[C++] apt package includes headers for S3 interface, but no support",I believe that the apt package is built without S3 support. But s3fs.h is exported in filesystem/api.h anyway. This creates undefined reference errors when trying to link to the package.,pull-request-available,['C++'],ARROW,Bug,Minor,2020-07-11 00:55:02,1
13316245,[Rust] Fix clippy lint on master,There was a clippy lint error with the float sort PR.,pull-request-available,['Rust'],ARROW,Bug,Major,2020-07-10 22:32:43,12
13316239,[Rust] Update dependencies,Update dependencies like tonic and rand (to reduce total dependencies),pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-07-10 21:04:06,12
13316234,[CI][Crossbow] Fix homebrew-cpp again,"brew audit is now failing because python is an alias: https://travis-ci.org/github/ursa-labs/crossbow/builds/706771359

This was changed upstream months ago.",pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Major,2020-07-10 20:38:19,4
13316230,[CI][Crossbow] Nightly conda-r fails,"Adding {{decor}} to Suggests means we need to add it to the conda env too (even though we don't use it in tests, etc. etc.)",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2020-07-10 20:19:02,4
13316228,[Integration] Tests do not run in Windows due to numpy 64-bit errors,"We found that the integer range check when generating integration data doesn't work in Windows because the default C integers that numpy uses are 32-bit by default in Windows.

This fixes that issue by forcing 64-bit integers.",pull-request-available,['Integration'],ARROW,Bug,Minor,2020-07-10 20:08:18,12
13316222,[Python] Accept pd.NA as missing value in array constructor,"Currently we don't support using {{pandas.NA}} at all:

{code}
In [1]: import pyarrow as pa

In [2]: import pandas as pd

In [3]: pa.array([pd.NA, ""A""])
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-3-77e774fa8521> in <module>
----> 1 pa.array([pd.NA, ""A""])

~/miniconda3/envs/fletcher/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib.array()

~/miniconda3/envs/fletcher/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()

~/miniconda3/envs/fletcher/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Could not convert <NA> with type NAType: did not recognize Python value type when inferring an Arrow data type
{code}",pull-request-available,['Python'],ARROW,New Feature,Major,2020-07-10 18:43:33,14
13316034,[Python] Do not depend on conda-forge static libraries in Windows wheel builds,Based on https://github.com/conda-forge/cfep/blob/e9bb3f58eca79107baede71cb9b05311705a10f2/cfep-18.md it appears that static libraries may not be included in the future in many packages that we use for building the Windows Python wheels. We should change the build to use BUNDLED builds so we don't have this issue,pull-request-available,['Python'],ARROW,Improvement,Blocker,2020-07-10 02:31:33,8
13316023,[C++] Add forward compatibility checks for unrecognized future MetadataVersion,"We should have no need of these checks in theory, but they present a safeguard should some years in the future it became necessary to increment the MetadataVersion. ",pull-request-available,['C++'],ARROW,Improvement,Critical,2020-07-10 00:50:43,14
13316007,[R] Pass CC/CXX to cmake when building libarrow in Linux build,"TL;DR one of CRAN's test machines uses a bespoke clang build that uses libc++ instead of libstdc++: https://www.stats.ox.ac.uk/pub/bdr/Rconfig/r-devel-linux-x86_64-fedora-clang. R may have various make conf set that it uses when compiling the R bindings in `r/src`, and we need to use those settings when we shell out to cmake to build Arrow C++. Package fails to load due to undefined symbols otherwise: https://www.r-project.org/nosvn/R.check/r-devel-linux-x86_64-fedora-clang/arrow-00install.html",pull-request-available,['R'],ARROW,Bug,Blocker,2020-07-09 23:19:14,4
13315987,[Python] Provide configurable MetadataVersion in IPC API and environment variable to set default to V4 when needed,This is a follow up to ARROW-9265 and must be implemented in order to release 1.0.0,pull-request-available,['Python'],ARROW,Improvement,Blocker,2020-07-09 20:49:42,0
13315984,[Python] Support pickling of Scalars,"Scalars don't currently support pickling.

Could this be as implemented with {{Scalar, (self.type, self.as_py())}}?",pull-request-available,['Python'],ARROW,Improvement,Major,2020-07-09 20:30:35,3
13315958,[C++] Review compute function names,"We should probably make compute function naming more consistent while it's not too late.

Examples:
* ""isin"", ""minmax"" but ""less_equal"", ""or_kleene"", ""binary_contains_exact""
* ""binary_contains_exact"" only works on string types
* ""ascii_length"" accepts non-ascii input and simply counts the number of bytes
* ""partition_to_indices"" vs. ""NthToIndices""",pull-request-available,['C++'],ARROW,Wish,Blocker,2020-07-09 17:43:33,2
13315948,[C++] Can't call isin/match through CallFunction,"From R:

{code:r}
library(arrow)
a <- Array$create(1:4)
b <- Array$create(c(2L, 4L, 3L))
arrow:::call_function(""isin"", a, b)
{code}

says that ""isin"" takes only 1 argument, not 2, which doesn't make sense. In C++ scalar_set_lookup.cc, I see {{auto isin = std::make_shared<ScalarFunction>(""isin"", Arity::Unary());}}, which is the source of that validation I guess, but the kernel in api_scalar.cc has signature {{Result<Datum> IsIn(const Datum& values, const Datum& value_set, ExecContext* ctx)}}.

If I actually call ""isin"" with one argument, i.e. {{arrow:::call_function(""isin"", a)}}, it segfaults.

Changing the definition to Arity::Binary(), it accepts 2 arguments, but it errors with {{NotImplemented: Function isin has no kernel matching input types (array[int32], array[int32])}}",compute pull-request-available,['C++'],ARROW,Bug,Major,2020-07-09 17:05:57,14
13315947,[C++] Division kernels,"We now have add, subtract, multiply, but no division",compute pull-request-available,['C++'],ARROW,Sub-task,Major,2020-07-09 16:58:11,7
13315858,[Python] [CI] jpype integration failure,"Following the Netty changes in Java, the Python jpype integration tests are failing:

https://github.com/ursa-labs/crossbow/runs/852764453",pull-request-available,"['Java', 'Python']",ARROW,Bug,Major,2020-07-09 10:23:03,14
13315703,[C++] Segfaults in compute::CallFunction,"I triggered these from R, so that's what the reproducers are in.

1. Calling ""filter"" with no args segfaults.

{code:r}
arrow:::compute__CallFunction(""filter"", list(), list(keep_na = FALSE))
{code}

Top of the backtrace from lldb:

{code}
  * frame #0: 0x0000000109e1c2c7 libarrow.100.dylib`arrow::Datum::type() const + 7
    frame #1: 0x000000010a14a232 libarrow.100.dylib`arrow::compute::internal::(anonymous namespace)::FilterMetaFunction::ExecuteImpl(std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const + 66
    frame #2: 0x0000000109fc32c9 libarrow.100.dylib`arrow::compute::MetaFunction::Execute(std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const + 41
    frame #3: 0x0000000109fb3d3c libarrow.100.dylib`arrow::compute::CallFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) + 844
    frame #4: 0x0000000109fb3c47 libarrow.100.dylib`arrow::compute::CallFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) + 599
{code}

This is not the case with at least some other functions. If I try to call ""sum"" with no args, I get {{Invalid: Function accepts 1 arguments but passed 0}} and no segfault.

2. Something is strange with is_null. It creates what appears to be a valid boolean array, but if I pass it to filter, it segfaults. I'm adding bindings for this in ARROW-9187 but this should run on current master:

{code:r}
library(arrow)
a <- Array$create(1:4)
b <- arrow:::shared_ptr(Array, arrow:::call_function(""is_null"", a))
a$Filter(b)
{code}

Backtrace:

{code}
 * frame #0: 0x000000010a120bb6 libarrow.100.dylib`arrow::compute::internal::GetFilterOutputSize(arrow::ArrayData const&, arrow::compute::FilterOptions::NullSelectionBehavior) + 38
    frame #1: 0x000000010a125659 libarrow.100.dylib`arrow::compute::internal::(anonymous namespace)::PrimitiveFilter(arrow::compute::KernelContext*, arrow::compute::ExecBatch const&, arrow::Datum*) + 121
    frame #2: 0x0000000109fbbea4 libarrow.100.dylib`arrow::compute::detail::VectorExecutor::ExecuteBatch(arrow::compute::ExecBatch const&, arrow::compute::detail::ExecListener*) + 996
    frame #3: 0x0000000109fba3e6 libarrow.100.dylib`arrow::compute::detail::VectorExecutor::Execute(std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::detail::ExecListener*) + 150
    frame #4: 0x0000000109fc0948 libarrow.100.dylib`arrow::compute::Function::Execute(std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const + 1016
    frame #5: 0x0000000109fb3d3c libarrow.100.dylib`arrow::compute::CallFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) + 844
    frame #6: 0x000000010a14a9b5 libarrow.100.dylib`arrow::compute::internal::(anonymous namespace)::FilterMetaFunction::ExecuteImpl(std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const + 1989
    frame #7: 0x0000000109fc32c9 libarrow.100.dylib`arrow::compute::MetaFunction::Execute(std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const + 41
    frame #8: 0x0000000109fb3d3c libarrow.100.dylib`arrow::compute::CallFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) + 844
    frame #9: 0x0000000109fb3c47 libarrow.100.dylib`arrow::compute::CallFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<arrow::Datum, std::__1::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) + 599
{code}

BUT: if I call {{as.vector}} on {{b}} before using it as a Filter, it works--even though I've discarded the as.vector result and am still using the Array to filter. 

{code:r}
library(arrow)
a <- Array$create(1:4)
b <- arrow:::shared_ptr(Array, arrow:::call_function(""is_null"", a))
as.vector(b)
a$Filter(b)
{code}

Just printing (calling {{ToString}}) on {{b}} doesn't prevent the segfault. And I have not observed this with other boolean kernels. E.g. this does not segfault:

{code:r}
library(arrow)
a <- Array$create(1:4)
b <- arrow:::shared_ptr(Array, arrow:::call_function(""greater"", a, Scalar$create(3L)))
a$Filter(b)
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-07-08 18:29:46,14
13315700,[Java] Support unsigned dictionary indices,child of ARROW-9259,pull-request-available,['Java'],ARROW,Improvement,Major,2020-07-08 18:24:45,7
13315656,[C++][Python] Expose MakeArrayFromScalar,Currently there is no efficient way to create a pyarrow array with identical values.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-07-08 15:16:46,3
13315565,[Python] Rename predicate argument to filter in split_by_row_group(),For consistency with to_table() and get_fragments(),pull-request-available,['Python'],ARROW,Bug,Major,2020-07-08 07:29:27,5
13315488,[C++][Dataset] ParquetDatasetFactory schema: pandas metadata is lost,"When using the standard factory, the pandas metadata is included in the schema metadata of the dataset, but when using the ParquetDatasetFactory, it is not included:

Using dask to write a small partitioned dataset with written {{_metadata}} file:

{code:python}
df = pd.DataFrame({""part"": [""A"", ""A"", ""B"", ""B""], ""col"": [1, 2, 3, 4]})                                                                                                                                     

import dask.dataframe as dd                                                                                                                                                                                
ddf = dd.from_pandas(df, npartitions=2)                                                                                                                                                                    
ddf.to_parquet(""test_parquet_pandas_metadata/"", engine=""pyarrow"")                                                                                                                                          
{code}

{code:python}
In [9]: import pyarrow.dataset as ds                                                                                                                                                                               

# with ds.dataset -> pandas metadata included
In [11]: ds.dataset(""test_parquet_pandas_metadata/"", format=""parquet"", partitioning=""hive"").schema                                                                                                                 
Out[11]: 
part: string
  -- field metadata --
  PARQUET:field_id: '1'
col: int64
  -- field metadata --
  PARQUET:field_id: '2'
index: int64
  -- field metadata --
  PARQUET:field_id: '3'
-- schema metadata --
pandas: '{""index_columns"": [""index""], ""column_indexes"": [{""name"": null, ""' + 558

# with parquet_dataset -> pandas metadata not included
In [14]: ds.parquet_dataset(""test_parquet_pandas_metadata/_metadata"",  partitioning=""hive"").schema                                                                                                                 
Out[14]: 
part: string
  -- field metadata --
  PARQUET:field_id: '1'
col: int64
  -- field metadata --
  PARQUET:field_id: '2'
index: int64
  -- field metadata --
  PARQUET:field_id: '3'

# to show that the pandas metadata are present in the actual Parquet FileMetadata
In [17]: pq.read_metadata(""test_parquet_pandas_metadata/_metadata"").metadata                                                                                                                                       
Out[17]: 
{b'ARROW:schema': b'/////4ADAAAQAAAAAAAKAA4AB...',
 b'pandas': b'{""index_columns"": [""index""], ...'}
{code}",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Bug,Major,2020-07-07 20:57:01,5
13315481,[Rust] Move other array types into their own modules,The array module is getting too big to be practical. We should leave the core types like the Array trait in `array.rs` and move each array type into its own sub-module as we did while implementing the Union array.,pull-request-available,['Rust'],ARROW,Improvement,Blocker,2020-07-07 20:21:22,9
13315466,[Integration] Reconsider generated_large_batch.json,"The dominant part of the time spent running the integration tests is one test case named ""generated_large_batch.json"". It is not obvious how useful this test case is. Ideally, integration test cases should be small enough to only take a fraction of a second.",pull-request-available,"['FlightRPC', 'Integration']",ARROW,Wish,Major,2020-07-07 18:47:50,0
13315447,[C++] Turbodbc latest fails to build in the integration tests,See build log https://github.com/ursa-labs/crossbow/runs/844367735,pull-request-available,['C++'],ARROW,Bug,Major,2020-07-07 16:29:44,3
13315446,[Python][CI] Nightly dask integration jobs fail,"On dask latest:

{code}
=================================== FAILURES ===================================
___________________________ test_dataframe_picklable ___________________________

    def test_dataframe_picklable():
        from pickle import loads, dumps
    
        cloudpickle = pytest.importorskip(""cloudpickle"")
        cp_dumps = cloudpickle.dumps
    
        d = _compat.makeTimeDataFrame()
        df = dd.from_pandas(d, npartitions=3)
        df = df + 2
    
        # dataframe
        df2 = loads(dumps(df))
        assert_eq(df, df2)
>       df2 = loads(cp_dumps(df))
E       ValueError: unsupported pickle protocol: 5

opt/conda/envs/arrow/lib/python3.7/site-packages/dask/dataframe/tests/test_dataframe.py:1623: ValueError
{code}

https://github.com/ursa-labs/crossbow/runs/844367735


Dask master has many more failures: https://github.com/ursa-labs/crossbow/runs/844351816",pull-request-available,['C++'],ARROW,Bug,Blocker,2020-07-07 16:27:38,5
13315443,[C++][CI] Nightly test-ubuntu-18.04-cpp-cmake32 fails,"https://app.circleci.com/pipelines/github/ursa-labs/crossbow/44681/workflows/982b49c0-a3de-434a-a77d-b16415f2c7ca/jobs/12731/steps

{code}
CMake Error at cmake_modules/DefineOptions.cmake:401 (if):
 
  if given arguments:
 

 
    ""NOT"" ""CMAKE_VERSION"" ""VERSION_LESS"" ""3.3"" ""AND"" ""NOT"" ""("" ""SSE4_2"" ""IN_LIST"" ""possible_values"" "")""
 

 
  Unknown arguments specified
 
Call Stack (most recent call first):
 
  CMakeLists.txt:881 (validate_config)
 

 

 
-- Configuring incomplete, errors occurred!
 {code}

It appears that ARROW-9013 introduced this: https://github.com/apache/arrow/blame/master/cpp/cmake_modules/DefineOptions.cmake#L401",pull-request-available,['C++'],ARROW,Bug,Blocker,2020-07-07 16:21:14,1
13315442,[C++][CI] Nightly valgrind job failures,https://github.com/ursa-labs/crossbow/runs/844367595,pull-request-available,['C++'],ARROW,Bug,Blocker,2020-07-07 16:17:40,2
13315374,[Python] Tests fail with latest fsspec,"Using the latest fsspec from upstream git repo (changeset 656be423ef36ac533fd3319850e107c2669cce3d), I get this error when running the tests:

{code}
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_fs.py"", line 778, in test_move_directory
    fs.move(s, t)
  File ""pyarrow/_fs.pyx"", line 519, in pyarrow._fs.FileSystem.move
    check_status(self.fs.Move(source, destination))
  File ""pyarrow/_fs.pyx"", line 1024, in pyarrow._fs._cb_move
    handler.move(frombytes(src), frombytes(dest))
  File ""/home/antoine/arrow/dev/python/pyarrow/fs.py"", line 173, in move
    self.fs.mv(src, dest)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/spec.py"", line 743, in mv
    self.rm(path1, recursive=recursive)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/fsspec/implementations/local.py"", line 108, in rm
    os.remove(path)
IsADirectoryError: [Errno 21] Is a directory: '/tmp/pytest-of-antoine/pytest-0/test_move_directory_PyFileSyst1/source-dir'
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-07-07 10:58:27,5
13315359,[C++][Python][Dataset] Add total_byte_size metadata to RowGroupInfo,"See https://github.com/apache/arrow/pull/7546#issuecomment-651177576. 

The row group information in Parquet's FileMetadata includes a ""total_byte_size"" field. It would be useful to expose this information on the RowGroupInfo object, similarly as we already expose ""num_rows"".

cc [~rjzamora]",dataset dataset-dask-integration pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-07-07 09:17:38,5
13315351,[C++][Dataset] Expression with dictionary type should work with operand of value type ,"Related to ARROW-8647, see comment at https://github.com/apache/arrow/pull/7536#issuecomment-653124260

When using dictionary type for the partition fields, this now creates partition expressions that also use a dictionary type. Which means that doing something like {{dataset.to_table(filter=ds.field(""part"") == ""A"")}} to filter on the partition field with a plain string expression doesn't work, limiting the usability of this option (and even with the new Python scalar stuff, it would not be easy to construct the correct expression):

{code}
In [9]: part = ds.HivePartitioning.discover(max_partition_dictionary_size=2)  

In [10]: dataset = ds.dataset(""test_partitioned_filter/"", format=""parquet"", partitioning=part)

In [11]: fragment = list(dataset.get_fragments())[0]   

In [12]: fragment.partition_expression  
Out[12]: 
<pyarrow.dataset.Expression (part == [
  ""A"",
  ""B""
][0]:dictionary<values=string, indices=int32, ordered=0>)>

In [13]: dataset.to_table(filter=ds.field(""part"") == ""A"") 
...
ArrowNotImplementedError: cast from string
{code}

It might be an option to keep the {{partition_expression}} use the dictionary _value type_ instead of dictionary type? Or alternatively, as [~fsaintjacques] proposed, ensure that any comparison involving the dict type should also work with the ""effective"" logical type (the value type of the dict).",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-07-07 08:53:50,6
13315229,[Rust] Add instructions for running clippy locally,"Similar to the ""Code Formatting"" section in the top level README it would be useful to add instructions for running clippy locally to avoid wasted CI time.",pull-request-available,['Rust'],ARROW,Sub-task,Minor,2020-07-06 20:23:21,12
13315207,"[R] On C++ library build failure, give an unambiguous message","See e.g. ARROW-9303, where the downstream error message is misleading.",pull-request-available,['R'],ARROW,Improvement,Minor,2020-07-06 17:56:49,4
13315201,[Ruby] Creating RecordBatch with structs missing keys results in a malformed table,"Using {{::Arrow::RecordBatch.new(schema, data)}} (which uses the {{RecordBatchBuilder}}) appears to handle when a record is missing an entry for a top level column, but it doesn't handle when a record is missing an entry within a struct column. For example, I'd expect the following code to print out {{true}} for each {{puts}}, but 2 of them are {{false}}:
{code:ruby}
require 'parquet'
require 'arrow'

schema = [
  {name: ""a"", type: ""string""},
  {name: ""b"", type: ""struct"", fields: [
     {name: ""c"", type: ""string""},
     {name: ""d"", type: ""string""},
   ]
  },
]

arrow_schema = ::Arrow::Schema.new(schema)

record_batch = ::Arrow::RecordBatch.new(
  arrow_schema,
  [
    {""a"" => ""a"", ""b"" => {""c"" => ""c"",           }},
    {            ""b"" => {""c"" => ""c"",           }},
    {            ""b"" => {            ""d"" => ""d""}},
  ]
)
table = record_batch.to_table

puts(table['a'][0] == 'a')
puts(table['a'][1].nil?)
puts(table['a'][2].nil?)

puts(table['b'][0].key?('c'))
puts(table['b'][0]['c'] == 'c')
puts(table['b'][0].key?('d'))
puts(table['b'][0]['d'].nil?) # False ?
puts(!table['b'][0].key?('e'))

puts(table['b'][1].key?('c'))
puts(table['b'][1]['c'] == 'c')
puts(table['b'][1].key?('d'))
puts(table['b'][1]['d'].nil?)
puts(!table['b'][1].key?('e'))

puts(table['b'][2].key?('c'))
puts(table['b'][2]['c'].nil?)
puts(table['b'][2].key?('d'))
puts(table['b'][2]['d'] == 'd') # False ?
puts(!table['b'][2].key?('e'))
{code}
I'd expect {{puts(table)}} to print this representation:
{noformat}
	a	b
0	a	{""c""=>""c"", ""d""=>nil}
1	 	{""c""=>""c"", ""d""=>nil}
2	 	{""c""=>nil, ""d""=>""d""}
{noformat}
But it prints this instead:
{noformat}
	a	b
0	a	{""c""=>""c"", ""d""=>""d""}
1	 	{""c""=>""c"", ""d""=>nil}
2	 	{""c""=>nil, ""d""=>nil}
{noformat}

Furthermore, trying to write that table out to a parquet file results in the following error:

{noformat}
Traceback (most recent call last):
	7: from arrow_parquet2.rb:53:in `<main>'
	6: from /usr/local/lib/ruby/gems/2.6.0/gems/red-arrow-0.17.1/lib/arrow/block-closable.rb:25:in `open'
	5: from arrow_parquet2.rb:54:in `block in <main>'
	4: from /usr/local/lib/ruby/gems/2.6.0/gems/red-arrow-0.17.1/lib/arrow/block-closable.rb:25:in `open'
	3: from arrow_parquet2.rb:56:in `block (2 levels) in <main>'
	2: from /usr/local/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:514:in `block in define_method'
	1: from /usr/local/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:600:in `invoke'
/usr/local/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.4.3/lib/gobject-introspection/loader.rb:600:in `invoke': [parquet][arrow][file-writer][write-table]: Invalid: Column 1: In chunk 0: Invalid: Struct child array #0 has length different from struct array (2 != 3) (Arrow::Error::Invalid)
 {noformat}
",pull-request-available,['Ruby'],ARROW,Bug,Major,2020-07-06 17:32:31,1
13315196,[Dev][Archery] Push ancestor docker images,"The ""debian-c-glib"" and ""ubuntu-c-glib"" docker-compose configurations fail with the following message:
{code:java}
CMake Error at /usr/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:137 (message):
  Could NOT find utf8proc (missing: UTF8PROC_LIB UTF8PROC_INCLUDE_DIR)
Call Stack (most recent call first):
  /usr/share/cmake-3.13/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE)
  cmake_modules/Findutf8proc.cmake:41 (find_package_handle_standard_args)
  cmake_modules/ThirdpartyToolchain.cmake:159 (find_package)
  cmake_modules/ThirdpartyToolchain.cmake:2096 (resolve_dependency)
  CMakeLists.txt:467 (include)

 {code}",pull-request-available,"['Archery', 'C', 'Developer Tools', 'GLib']",ARROW,Bug,Major,2020-07-06 16:54:40,3
13315193,[Python] Expose more IPC write options in Python,We want to allow Python users to use the latest metadata version and/or enable buffer compression.,pull-request-available,['Python'],ARROW,Improvement,Major,2020-07-06 16:26:29,2
13315006,[Python] Setuptools 49.1.0 appears to break our Python 3.6 builds,"Not sure who thought it was a good idea to release setuptools on July 3, a holiday in the United States, but it appears to be breaking some of our builds

https://github.com/apache/arrow/pull/7539/checks?check_run_id=835994558

{code}
  File ""/opt/conda/envs/arrow/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 297, in run
    self.find_sources()
  File ""/opt/conda/envs/arrow/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 304, in find_sources
    mm.run()
  File ""/opt/conda/envs/arrow/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 535, in run
    self.add_defaults()
  File ""/opt/conda/envs/arrow/lib/python3.6/site-packages/setuptools/command/egg_info.py"", line 571, in add_defaults
    sdist.add_defaults(self)
  File ""/opt/conda/envs/arrow/lib/python3.6/site-packages/setuptools/_distutils/command/sdist.py"", line 228, in add_defaults
    self._add_defaults_ext()
  File ""/opt/conda/envs/arrow/lib/python3.6/site-packages/setuptools/_distutils/command/sdist.py"", line 312, in _add_defaults_ext
    self.filelist.extend(build_ext.get_source_files())
  File ""/opt/conda/envs/arrow/lib/python3.6/distutils/command/build_ext.py"", line 420, in get_source_files
    self.check_extensions_list(self.extensions)
  File ""/opt/conda/envs/arrow/lib/python3.6/distutils/command/build_ext.py"", line 362, in check_extensions_list
    ""each element of 'ext_modules' option must be an ""
distutils.errors.DistutilsSetupError: each element of 'ext_modules' option must be an Extension instance or 2-tuple
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-07-04 20:35:37,14
13314911,"[C++][Dataset] Allow to ""collect"" statistics for ParquetFragment row groups if not constructed from _metadata","Right now, the statistics of the {{RowGroupInfo}} of ParquetFileFragments are only available when the dataset was constructed from a {{_metadata}} file:

{code:python}
import pandas as pd
df = pd.DataFrame({""part"": ['A', 'A', 'B', 'B'], ""col"": range(4)})                                                                                                                                        
# use dask to write partitioned dataset *with* _metadata file
import dask.dataframe as dd                                                                                                                                                                               
ddf = dd.from_pandas(df, npartitions=2) 
ddf.to_parquet(""test_dataset"", partition_on=[""part""], engine=""pyarrow"")                                                                                                                     

import pyarrow.dataset as ds
dataset_no_metadata = ds.dataset(""test_dataset/"", format=""parquet"", partitioning=""hive"")
dataset_from_metadata = ds.parquet_dataset(""test_dataset/_metadata"", partitioning=""hive"")                                                                                                                 
{code}

{code}

In [28]: list(dataset_no_metadata.get_fragments())[0].row_groups                                                                                                                                                   

In [30]: list(dataset_from_metadata.get_fragments())[0].row_groups                                                                                                                                                 
Out[30]: [<pyarrow._dataset.RowGroupInfo at 0x7fd7882c0030>]

In [32]: list(dataset_from_metadata.get_fragments())[0].row_groups[0].statistics                                                                                                                                   
Out[32]: {'col': {'min': 2, 'max': 3}, 'index': {'min': 2, 'max': 3}}
{code}

For some applications (eg dask), one could want access to those statistics, even if the original dataset / fragments were not created from a {{_metadata}} file. This should not happen automatically since it's costly, but a method to trigger collecting all metadata would be useful.

cc [~rjzamora] ",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-07-03 14:47:25,6
13314823,[Java] Fix the failure of testAllocationManagerType,"It appears sometimes in the CI build. 

[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.503 s <<< FAILURE! - in org.apache.arrow.memory.TestAllocationManager
[ERROR] testAllocationManagerType  Time elapsed: 0.483 s  <<< ERROR!
java.lang.ExceptionInInitializerError
	at org.apache.arrow.memory.TestAllocationManager.testAllocationManagerType(TestAllocationManager.java:35)
Caused by: java.lang.NullPointerException: allocationManagerFactory
	at org.apache.arrow.memory.TestAllocationManager.testAllocationManagerType(TestAllocationManager.java:35)
",pull-request-available,['Java'],ARROW,Bug,Major,2020-07-03 03:45:16,7
13314793,[Python] Dependency load failure in Windows wheel build,The Windows wheels are experiencing a DLL load failure probably due to one of the dependencies,pull-request-available,['Python'],ARROW,Bug,Blocker,2020-07-03 00:06:27,3
13314792,"[C++] Add ""AppendEmptyValue"" builder APIs for use inside StructBuilder::AppendNull","StructBuilder should probably also add ""UnsafeAppendNull"" so that there is the option of using the Unsafe* operations on the children",pull-request-available,['C++'],ARROW,New Feature,Major,2020-07-03 00:00:02,16
13314789,[R] Linux static build should always bundle dependencies,"I'm following the instructions here: https://arrow.apache.org/install/

arrow::install_arrow()

gives error:

{{./configure: line 132: cd: libarrow/arrow-0.17.1/lib: No such file or directory}}

{{This leaves me without a working arrow::read_feather.}}",pull-request-available,['R'],ARROW,Bug,Major,2020-07-02 23:52:56,4
13314667,[C++][Dataset] Dataset scanner cannot handle large binary column (> 2 GB),"Related to ARROW-3762 (the parquet issue which has been solved), and discovered in ARROW-9139.

When creating a Parquet file with a large binary column (larger than BinaryArray capacity):

{code}
# code from the test_parquet.py::test_binary_array_overflow_to_chunked test
values = [b'x'] + [ 
    b'x' * (1 << 20) 
] * 2 * (1 << 10)                                                                                                                                                                                     

table = pa.table({'byte_col': values})                                                                                                                                                                    
pq.write_table(table, ""test_large_binary.parquet"")                                                                                                                                                        
{code}

then reading this with the parquet API works (fixed by ARROW-3762):

{code}
In [3]: pq.read_table(""test_large_binary.parquet"")                                                                                                                                        
Out[3]: 
pyarrow.Table
byte_col: binary
{code}

but with the Datasets API this still fails:

{code}
In [1]: import pyarrow.dataset as ds                                                                                                                                                                               

In [2]: dataset = ds.dataset(""test_large_binary.parquet"", format=""parquet"")                                                                                                                                        

In [4]: dataset.to_table()                                                                                                                                                                                         
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-4-6fb0d79c4511> in <module>
----> 1 dataset.to_table()

~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Dataset.to_table()

~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Scanner.to_table()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: This class cannot yet iterate chunked arrays

{code}",dataset dataset-parquet-read pull-request-available,['C++'],ARROW,Bug,Major,2020-07-02 12:05:36,6
13314549,[Rust] Update feature matrix with passing tests,"When we created the feature matrix, I preemptively populated the Rust column with supported features. We've subsequently been having trouble with integration tests.

This blocker is so that I can update the feature matrix before 1.0.0 release based on which tests are passing by then.

CC [~wesm] [~apitrou]",pull-request-available,['Rust'],ARROW,Sub-task,Blocker,2020-07-01 19:46:40,12
13314445,[C++][Dataset] Discovery of partition field as dictionary type segfaulting with HivePartitioning,"Testing new feature from ARROW-8647, python test that reproduces it:

{code:python}
@pytest.mark.parquet
@pytest.mark.parametrize('partitioning', [""directory"", ""hive""])
def test_open_dataset_partitioned_dictionary_type(tempdir, partitioning):
    import pyarrow.parquet as pq
    table = pa.table({'a': range(9), 'b': [0.] * 4 + [1.] * 5})

    path = tempdir / ""dataset""
    path.mkdir()

    for part in [""A"", ""B"", ""C""]:
        fmt = ""{}"" if partitioning == ""directory"" else ""part={}""
        part = path / fmt.format(part)
        part.mkdir()
        pq.write_table(table, part / ""test.parquet"")

    if partitioning == ""directory"":
        part = ds.DirectoryPartitioning.discover([""part""], max_partition_dictionary_size=-1)
    else:
        part = ds.HivePartitioning.discover(max_partition_dictionary_size=-1)
    
    dataset = ds.dataset(str(path), partitioning=part)
    expected_schema = table.schema.append(
        pa.field(""part"", pa.dictionary(pa.int32(), pa.string()))
    )
    assert dataset.schema.equals(expected_schema)
{code}

This test fails (segfaults) for HivePartitioning, but works for DirectoryPartitioning",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-07-01 10:03:11,5
13314398,[C++] Implement support for unsigned dictionary indices,Follow on work from ARROW-9259,pull-request-available,['C++'],ARROW,New Feature,Major,2020-07-01 01:35:45,14
13314381,"[C++] Add function ""aliases"" to compute::FunctionRegistry",The purpose of aliases would be to avoid breaking APIs when/if functions are renamed in between releases,pull-request-available,['C++'],ARROW,New Feature,Major,2020-06-30 21:47:18,16
13314340,[Python] Expose C++ build info,Followup to ARROW-6521 on the C++ side,pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-30 16:23:14,2
13314337,[R] Remove usage of _EXTPTR_PTR,"See comments at the end ofARROW-9219. There was an ABI change in R, apparently an interface that was not officially supported.",pull-request-available,['R'],ARROW,Bug,Critical,2020-06-30 15:55:27,4
13314321,[R] Turn off utf8proc in R builds,ARROW-9220 unfortunately stopped short of doing this.,pull-request-available,"['Packaging', 'R']",ARROW,Improvement,Major,2020-06-30 15:16:28,4
13314307,[C++] Implement PrettyPrint for Scalars,"It would be useful, especially for nested scalar objects.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-30 14:25:07,16
13314288,[Dev] Enable ARROW_CUDA when generating API documentations,"-Update the post-09-docs.sh script to check that CUDA device is available and use the ubuntu-cuda-docs docker image to generate the apidocs.-

Enable CUDA in the documentation builds by installing nvidia-cuda-toolkit with dummy libraries.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-06-30 12:25:40,3
13314249,[Rust] [Integration Testing] Read i64 from json files as strings,"The integration files were recently changed to use strings for 64-bit numbers, and this caused failures for the relevant Rust Arrow IPC test cases. This Jira is for the work to fix that.",pull-request-available,['Rust'],ARROW,Bug,Critical,2020-06-30 09:07:54,12
13314166,[C++][Python] Reduce complexity in python to arrow conversion,"The original motivation for this patch was to reuse the same conversions path for both the scalars and arrays. 

In my recent patch the scalars are converted from a single element list to a single element array then copied out from it.

On the long term we should convert them directly, perhaps with a more generic converter API, until that this patch aims to reduce code complexity without introducing any regressions.",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-06-29 22:25:57,3
13314068,[C++] Add support for writing MetadataVersion::V4-compatible IPC messages for compatibility with library versions <= 0.17.1,"While we need to increment the MetadataVersion, we should not strand old library versions since V4 is backward compatible with V5. ",pull-request-available,['C++'],ARROW,Improvement,Blocker,2020-06-29 14:14:44,2
13313904,[C++] Cleanup Parquet Arrow Schema code,"PrecedingARROW-8493 there is some cleanup. Variable renaming, using Result instead of output parameter and moving code from reader_internal.cc to schema.cc",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-06-29 03:16:52,15
13313885,[Packaging][Linux][CI] Use Ubuntu 18.04 to build ARM64 packages on Travis CI,"We got the following error with Ubuntu 20.04:

{noformat}
gpg: Fatal: can't disable core dumps: Operation not permitted
{noformat}
",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-06-28 21:15:28,1
13313880,[Python][Packaging] S3FileSystem curl errors in manylinux wheels,"https://issues.apache.org/jira/browse/ARROW-9109introduced S3 support in manylinux wheels. However, when trying to use S3FileSystem it fails with


{code:java}
Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
 File ""pyarrow/_fs.pyx"", line 597, in pyarrow._fs.FileSystem.open_input_stream
 File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
 File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
OSError: When reading information for key 'xxxxx' in bucket 'xxxxx': AWS Error [code 99]: curlCode: 77, Problem with the SSL CA cert (path? access rights?) with address{code}
It seems like it can't find the SSL CA cert directory that is installed in the runtime machine (tested on Ubuntu 16.04 andUbuntu 18.04). It always searches in/etc/pki/tls/certs/ca-bundle.crt probably because the wheels are built on centos, whereas in Ubuntu the path is/etc/ssl/certs/ca-certificates.crt and is different on other distributions.

Reproduce with:
{code:java}
virtualenv -p python3.8 arrowenv
source arrowenv/bin/activate
pip install --extra-index-url https://repo.fury.io/arrow-nightlies/ --pre pyarrow
python -c ""from pyarrow.fs import S3FileSystem; fs = S3FileSystem(); fs.open_input_stream('mybucket/myfile')""{code}


",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-06-28 18:57:37,2
13313875,"[CI] ""ARM64v8 Ubuntu 20.04 C++"" fails","This GHA build should be disabled until it is passing reliably, e.g. https://github.com/apache/arrow/runs/816007838. This seems to be similar to the Travis CI failure",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-06-28 18:16:42,1
13313862,[Format] Add V5 MetadataVersion,Per mailing list discussion,pull-request-available,['Format'],ARROW,Improvement,Blocker,2020-06-28 15:13:49,14
13313799,[C++] Factor out some integer casting internals so it can be reused with temporal casts,"The ""CastNumberToNumberUnsafe"" function can be shared outside of scalar_cast_numeric.cc",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-27 23:07:08,14
13313792,"[Integration] GitHub Actions integration test job does not test against ""gold"" 0.14.1 files in apache/arrow-testing","I'm not sure when and why this was dropped but it is critical that these tests from 

https://github.com/apache/arrow/commit/26d72f328b82bcff4e074109a5f905ebf069a416#diff-776ea3bf11df5829827f7afb43c37174

are restored",pull-request-available,['Developer Tools'],ARROW,Bug,Blocker,2020-06-27 21:18:43,14
13313778,[C++] Move JSON testing code for integration tests to libarrow_testing,"This code contributes over 700KB to release builds and is never used

{code}
-rw------- 1 wesm wesm  34104 Jun 27 11:14 dictionary.cc.o
-rw------- 1 wesm wesm 199592 Jun 27 11:14 feather.cc.o
-rw------- 1 wesm wesm  63448 Jun 27 11:14 json_integration.cc.o
-rw------- 1 wesm wesm 727336 Jun 27 11:14 json_internal.cc.o
-rw------- 1 wesm wesm 828056 Jun 27 11:14 json_simple.cc.o
-rw------- 1 wesm wesm 185344 Jun 27 11:14 message.cc.o
-rw------- 1 wesm wesm 223592 Jun 27 11:14 metadata_internal.cc.o
-rw------- 1 wesm wesm   3416 Jun 27 11:14 options.cc.o
-rw------- 1 wesm wesm 557960 Jun 27 11:14 reader.cc.o
-rw------- 1 wesm wesm 285744 Jun 27 11:14 writer.cc.o
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-27 16:22:00,14
13313777,[C++] Compact generated code in compute/kernels/scalar_set_lookup.cc using same method as vector_hash.cc,This module can be made to compile smaller and faster by using common kernels for types having the same binary representation,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-27 16:19:15,14
13313767,[Java] Add forward compatibility checks for Decimal::bitWidth,[~rymurr] can you help with this?,pull-request-available,['Java'],ARROW,Sub-task,Major,2020-06-27 15:43:43,16
13313744,[R] 0.17 install on Arch Linux,"After reading the installation vignette I'm reporting this installation issue.

I couldn't install the R package by building the C++ libraries as I hit:
{code:r}
# arch arrow install unable to load shared object R/x86_64-pc-linux-gnu-library/4.0/00LOCK-arrow/00new/arrow/libs/arrow.so: undefined symbol: _ZTIN6apache6thrift8protocol9TProtocolE{code}
I tried to set R_LD_LIBRARY_PATH as suggested but it didn't help. Maybe I made a mistake.

Using a prebuild arrow binary was failing as the version on the Arch AUR was 0.17.0 while the CRAN was wanting 0.17.1, and the even the AUR 0.17.0 install was failing because there are apparently issues with grpc 1.28+ (I was on 1.29.1) according to [https://aur.archlinux.org/packages/arrow/]

I finally solved the issue by setting -DARROW_FLIGHT to OFF during installation from the AUR ( I also turned -DARROW_WITH_SNAPPY to ON):
{code:bash}
yay --editmenu -S arrow
{code}
Then to install the R package I had to set NOT_CRAN=true & install the 0.17.0 version:
{code:r}
Sys.setenv(""NOT_CRAN""=""true"")
packageurl <- ""http://cran.r-project.org/src/contrib/Archive/arrow/arrow_0.17.0.tar.gz""
install.packages(packageurl, repos=NULL, type=""source"")
{code}
This led to a working R install, but I never solved my issue compiling the C++ binaries during R install. Perhaps this relates to the same issue with grpc?

I'm sorry I didn't save all the error messages I hit, but I've included a working PKGBUILD for my system, edited from [https://aur.archlinux.org/packages/arrow/]. I hope it helps.",Linux R install,['R'],ARROW,Bug,Major,2020-06-27 10:42:58,4
13313691,[GLib][CUDA] Add support for dictionary memo on reading record batch from buffer,This is a follow up task for ARROW-8927.,pull-request-available,"['GLib', 'GPU']",ARROW,Improvement,Major,2020-06-26 21:20:37,1
13313683,[C++] is_null on NullArray should be true for all values,"While working on ARROW-9187, I found that it was reporting false for all NullArray values. ",pull-request-available,['C++'],ARROW,Bug,Major,2020-06-26 20:04:59,14
13313369,[Python] Fix to_pandas() export for timestamps within structs,Currently timestamps within structs unilaterally have their timezone discarded for backwards compatibility reasons. There is a TODO in the code to come up with a better solution. This Jira tracks the solution.,pull-request-available,['Python'],ARROW,Bug,Major,2020-06-25 02:51:47,15
13313340,[Format][Proposal] Remove validity bitmap from Union types,See mailing list discussion,pull-request-available,['Format'],ARROW,Improvement,Major,2020-06-24 22:33:08,14
13313339,"ArrowBuf#setBytes(int, ByteBuffer) doesn't check the byte buffer's endianness","[https://github.com/apache/arrow/blob/c9c5f5f2e8e8f5db4141d06b6e1841f0a55d905a/java/memory/src/main/java/org/apache/arrow/memory/ArrowBuf.java#L832]

This should check/set the buffer's endianness. This came up while debugging an issue in Flight; PutResult#fromProtocol asks Protobuf for the byte buffers that underlie a ByteString, but if the implementation is LiteralByteString, Protobuf has no buffers. So it wraps the underlying byte[], and of course the default endianness is BIG_ENDIAN.

[https://github.com/apache/arrow/blob/c9c5f5f2e8e8f5db4141d06b6e1841f0a55d905a/java/flight/flight-core/src/main/java/org/apache/arrow/flight/PutResult.java#L81]

[https://github.com/protocolbuffers/protobuf/blob/2b7b7f7f72e3617191972fbafb298cf7ec31e95e/java/core/src/main/java/com/google/protobuf/ByteString.java#L1371]",pull-request-available,['Java'],ARROW,Bug,Major,2020-06-24 22:31:04,0
13313337,[C++] Disable relevant compute kernels if ARROW_WITH_UTF8PROC=OFF,utf8proc should not be a hard dependency of ARROW_COMPUTE,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-24 22:07:21,2
13313308,[R] coerce_timestamps in Parquet write options does not work,"i am trying to truncate timestamps to milliseconds when writing a parquet file.

with:



{{tutu <- as.POSIXct(""2020/06/03 18:00:00"",tz = ""UTC"")}}

if i do:



{{write_parquet(data.frame(tutu),""~/Downloads/tutu.test.parquet"")}}

i get 1591207200000000

if i do:



{{write_parquet(data.frame(tutu),""~/Downloads/tutu.test.parquet"", coerce_timestamps = ""ms"", allow_truncated_timestamps = TRUE)}}

i get the error message:



{{Error in parquet___ArrowWriterProperties___Builder__coerce_timestamps(unit) : 
  argument ""unit"" is missing, with no default}}

what am i doing wrong? thanks in advance.",pull-request-available,['R'],ARROW,Bug,Major,2020-06-24 18:41:16,4
13313066,[C++] Avoid util::optional in favor of separate inlineable functions in arrow/visitor_inline.h,"Currently null is being checked for twice (once when deciding what kind of optional to send to the passed function, and then again inside that function) when it only need be checked once",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-23 14:27:20,14
13312875,[C++] Use OptionalBitBlockCounter in ArrayDataInlineVisitor,"This will speed up nearly every code path where it's used in the cases where data is mostly not-null or mostly-null, while costing nothing in the null_count == 0 case. ",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-22 17:17:03,14
13312820,[Python][Dataset] Clean-up internal FileSource class,"Follow-up on ARROW-8074, see PR comment https://github.com/apache/arrow/pull/7156#discussion_r441836943",pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-22 11:50:13,5
13312714,"[Archery] Render-human readable table when using ""archery benchmark diff""",The output of {{archery benchmark diff}} isn't that user friendly. It would be nice to get a human-readable table sorted by % change in the console similar to what's printed by the Ursabot bot,pull-request-available,"['Archery', 'Developer Tools']",ARROW,Improvement,Major,2020-06-21 20:49:25,14
13312704,[C++] Revamp numeric casts: faster performance and reduced binary size,I'm working on overhauling integer and floating point casts in scalar_cast_numeric.cc. Patch coming soon,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-21 17:31:50,14
13312703,[C++] Make temporal casts work on Scalar inputs,"I'm adding scalar tests to scalar_cast_test.cc in a different patch, so I will disable the scalar tests for these casts until they can be properly implemented",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-21 17:31:03,6
13312518,[Website] Improve contributor guide,"https://spark.apache.org/contributing.html is much richer and welcoming than ours, for example. We probably don't need everything on that page but we can do better.

Includes ARROW-4429",pull-request-available,"['Documentation', 'Website']",ARROW,Improvement,Major,2020-06-19 16:38:31,4
13312514,[C++] Do not always statically link Brotli libraries,"I discovered by symbol sleuthing that even if the Brotli shared libraries are available on the system, the static libraries will be used for linking. Here's a dump of the symbols on my machine

https://gist.github.com/wesm/b7e911424ce3ca3a175b652666fe6f89

The largest of these symbols {{kBrotliDictionaryData}} is 120KB

Since we statically link everything in our Python wheels, we might consider disabling Brotli in manylinux builds at least since it adds 800KB to libarrow.so",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-19 16:16:02,14
13312326,"[C++] Use ""applicator"" namespace for kernel operator-to-kernel functors, streamline argument unboxing",I have some changes to improve readability,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-19 00:30:11,14
13312321,[C++] Instantiate fewer templates in Cast kernel implementation,There are some places where we are instantiating some templates where we don't need to,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-18 23:54:17,14
13312317,[R] Replace usage of iris dataset in tests,It's not a particularly interesting dataset to test with anyway.,pull-request-available,['R'],ARROW,Improvement,Major,2020-06-18 22:38:22,4
13312316,[R] Improve documentation about CSV reader,Existing overview docs misleadingly state that our CSV reader isn't worth trying.,pull-request-available,"['Documentation', 'R']",ARROW,Improvement,Minor,2020-06-18 22:36:16,4
13312298,[C++][Parquet] Tracking issue for cross-implementation LZ4 Parquet compression compatibility,"per PARQUET-1878, it seems that there are still problems with our use of LZ4 compression in the Parquet format. While we should fix this (the Parquet specification and our implementation of it), we may need to disable use of LZ4 compression until the appropriate compatibility testing can bed one. ",pull-request-available,['C++'],ARROW,Bug,Critical,2020-06-18 20:12:45,2
13312262,[FlightRPC][C++][Python] Expose connected peer,gRPC exposes the address of the connected client - this should be exposed to Flight services as well as it can be useful for metrics/logging in internal deployments.,pull-request-available,"['C++', 'FlightRPC', 'Python']",ARROW,Improvement,Major,2020-06-18 15:31:10,0
13312048,"[C++] Add methods to StringArray, LargeStringArray, to validate whether its values are all UTF-8",This would be useful to check in instances where it matters,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-17 20:29:37,2
13312002,[Python] Expose Add/Subtract/Multiply arithmetic kernels,They are now available on the C++ side.,pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-17 16:44:19,3
13311916,[C++] Implement string/binary contains for exact matches,"Implement {{contains}} for exact matches of subportions of a string. Using the KnuthMorrisPratt algorithm, we should be able to do this in a linear runtime with a tiny bit of preprocessing at the invocation.",Analytics pull-request-available,['C++'],ARROW,Bug,Major,2020-06-17 09:12:03,8
13311885,[Python] Expose the isnull/isvalid kernels,"In ARROW-971 the IsValid/IsNull kernels got implemented with the new kernel machinery in C++, so now we can update the existing dummy methods in Python.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-17 07:09:02,5
13311836,[Developer] Use GitHub issue templates better,To encourage JIRA and the mailing lists more,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-06-16 23:09:04,4
13311826,[Python] Add bindings for StructScalar,follow up to ARROW-8769,pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-16 21:37:13,3
13311817,[C++] Create specialized filter implementation for varbinary types,Variable-binary types need special handling as follow up to ARROW-9075. ,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-16 20:51:23,14
13311805,[R][CI] Fix Rtools 4.0 build: pacman sync,Apparently the package index was out of sync and nothing was syncing it. The Rtools 3.5 build does sync because it sets a different package index.,pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2020-06-16 19:24:48,4
13311733,[C++][Dataset] Support null -> other type promotion in Dataset scanning,"With regarding schema evolution / normalization, we support inserting nulls for a missing column or changing nullability, or normalizing column order, but we do not yet seem to support promotion of null type to any other type.

Small python example:

{code}
In [11]: df = pd.DataFrame({""col"": np.array([None, None, None, None], dtype='object')})
    ...: df.to_parquet(""test_filter_schema.parquet"", engine=""pyarrow"")
    ...:
    ...: import pyarrow.dataset as ds
    ...: dataset = ds.dataset(""test_filter_schema.parquet"", format=""parquet"", schema=pa.schema([(""col"", pa.int64())]))
    ...: dataset.to_table()
...
~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Dataset.to_table()
~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Scanner.to_table()
~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()
~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()
ArrowTypeError: fields had matching names but differing types. From: col: null To: col: int64
{code}",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Sub-task,Major,2020-06-16 14:00:19,6
13311730,[C++][Dataset] Scanning a Fragment with a filter + mismatching schema shouldn't abort,"If you have filter on a column where the physical and dataset schema differs, scanning aborts (as right now the dataset schema, if specified, gets used for implicit casts, but then the expression might have a different type as the actual physical column):

Small parquet file with one int32 column:
{code:python}
df = pd.DataFrame({""col"": np.array([1, 2, 3, 4], dtype='int32')})
df.to_parquet(""test_filter_schema.parquet"", engine=""pyarrow"")

import pyarrow.dataset as ds
dataset = ds.dataset(""test_filter_schema.parquet"", format=""parquet"")
fragment = list(dataset.get_fragments())[0]
{code}

and then reading in a fragment with a filter on that column, without and with specifying a dataset/read schema:

{code}
In [48]: fragment.to_table(filter=ds.field(""col"") > 2).to_pandas() 
Out[48]: 
   col
0    3
1    4

In [49]: fragment.to_table(filter=ds.field(""col"") > 2, schema=pa.schema([(""col"", pa.int64())])).to_pandas()                                                                                                        
../src/arrow/result.cc:28: ValueOrDie called on an error: Type error: Cannot compare scalars of differing type: int64 vs int32
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.100(+0xee2f86)[0x7f6b56490f86]
...
Aborted (core dumped)
{code}

Now this int32->int64 type change is something we don't support yet (in the schema evolution/normalization, when scanning a dataset), but it also shouldn't abort but raise a normal error about type mismatch.",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-06-16 13:56:27,6
13311705,[C++] Add true_count / false_count methods to BooleanArray,These counts would exclude nulls. We can use the bit block counters to compute this efficient now,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-16 12:23:58,14
13311701,[CI] OSS-Fuzz build fails because recent changes in the google repository,The {{infra/travis}} directory was renamed to {{infra/ci}} so we need to update our builds.,pull-request-available,['Continuous Integration'],ARROW,Task,Major,2020-06-16 11:59:13,3
13311648,[C++] RecordBatch::Slice erroneously sets non-nullable field's internal null_count to unknown,"A segfault is triggered by calling dictionary_encode on a column after slicing a Recordbatch:
{code:java}
import pyarrow as pa
print(pa.__version__)

array = pa.array(['foo', 'bar', 'baz'])
batch = pa.RecordBatch.from_arrays([array], names=['a'])

batch.column(0).dictionary_encode() ### works fine

sub_batch = batch.slice(1)
sub_batch.column(0).dictionary_encode() ### segfault
{code}
Slicing the underlying array and dictionary_encoding works as expected:
{code:java}
array.slice(1).dictionary_encode()
{code}
For what it's worth, this can be worked around by converting the sub_batch to and from a table with:
{code:java}
happy_sub_batch = pa.Table.from_batches([sub_batch]).to_batches()[0]
happy_sub_batch.column(0).dictionary_encode() ### works fine
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-06-16 07:42:59,14
13311572,[R] Update cross-package documentation links,"R has added a new --as-cran check in the development version, which is causing our nightly as-cran builds to fail. Regenerating the man pages with https://github.com/r-lib/roxygen2/pull/1109 will fix it.",pull-request-available,['R'],ARROW,Bug,Blocker,2020-06-15 22:29:19,4
13311520,[GLib][Ruby] Allow to read Parquet files in chunks (by RowGroup),"At the moment only `gparquet_arrow_file_reader_read_table` is exported which requires the reader to load all row-groups. Additionally, it would be extremely helpful to have access to the `ReadRowGroup` method which would allow reading parts of the file without having to load everything into memory.",pull-request-available,"['GLib', 'Ruby']",ARROW,Improvement,Major,2020-06-15 16:15:43,1
13311474,[C++] Implement hash kernels for dictionary data with constant dictionaries,"Enabling [`strings_as_dictionary`](https://turbodbc.readthedocs.io/en/latest/pages/advanced_usage.html?highlight=strings_as_dictionary#obtaining-apache-arrow-result-sets) in `turbodbc` returns a `ChunkedArray` of `dictionary` type (IIUC).

I'd like to enable this for better performance however it seems not all functionality is implemented for `dictionary` types? In particular, `unique` seems not to be implemented:

{code}
In [40]: nmi.__class__.mro()
Out[40]: [pyarrow.lib.ChunkedArray, pyarrow.lib._PandasConvertible, object]

In [41]: nmi.type
Out[41]: DictionaryType(dictionary<values=string, indices=int32, ordered=0>)

In [42]: nmi.unique()
Traceback (most recent call last):

  File ""<ipython-input-42-0fcb7893d5c4>"", line 1, in <module>
    nmi.unique()

  File ""pyarrow\table.pxi"", line 307, in pyarrow.lib.ChunkedArray.unique

  File ""pyarrow\error.pxi"", line 106, in pyarrow.lib.check_status

ArrowNotImplementedError: unique not implemented for dictionary<values=string, indices=int32, ordered=0>
{code}

It would be very useful if the `dictionary` type supported all the usual operations.
",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-06-15 12:02:07,14
13311375,[Python] Add deprecated wrappers functions to a pyarrow/compat.py module for 1.0.0 that will be removed later,Some third party projects (e.g. Dask) unfortunately have imported symbols from pyarrow.compat. ,pull-request-available,['Python'],ARROW,Improvement,Blocker,2020-06-15 02:12:27,14
13311360,[Python][JPype] Test is failed with JPype 0.7.5,"https://github.com/ursa-labs/crossbow/runs/769433714#step:6:7995

{noformat}
>       if jpype.__version_info__ >= (0, 7):
E       TypeError: '>=' not supported between instances of 'list' and 'tuple'
{noformat}",pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-14 20:53:40,1
13311296,[Python][wheel] Use libzstd.a explicitly,{{ARROW_ZSTD_USE_SHARED}} is introduced by ARROW-9084. We need to set {{ARROW_ZSTD_USE_SHARED=OFF}} explicitly to use static zstd library.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-06-13 21:56:16,1
13311288,[C++] Adapt ascii_lower/ascii_upper bulk transforms to work on sliced arrays,"See comments at https://github.com/apache/arrow/pull/7418#discussion_r439754427

Also add unit tests to verify that only the referenced data slice has been transformed in the result",pull-request-available,['C++'],ARROW,Bug,Major,2020-06-13 20:08:19,14
13311286,[C++] Do not wipe the filesystem when path is empty,"The `DeleteDirContents` method in the filesystems api has a default behavior or *wiping* the whole filesystem if we give it an empty path.



It's documented as:
 > Like DeleteDir, but doesnt delete the directory itself. Passing an empty path () will wipe the entire filesystem tree.



And the corresponding code confirms that:
{code:java}
  auto parts = SplitAbstractPath(path);
  RETURN_NOT_OK(ValidateAbstractPathParts(parts));  
  
  if (parts.empty()) {
    // Wipe filesystem
    impl_->RootDir().entries.clear();
    return Status::OK();
  }
{code}


This is a weird default that does not make sense. If the user wanted really to wipe his filesystem, he'd pass a `/`.

",pull-request-available,['C++'],ARROW,Bug,Minor,2020-06-13 19:56:45,2
13311217,"[C++] Lint and Format C++ files with ""codegen"" in file name","Currently, headers named /*_internal.h/ are neither clang-formatted nor cpplinted. Since they're not exported, CLI lint (forbid <mutex>, nullptr, ...) need not be applied",pull-request-available,['C++'],ARROW,Bug,Critical,2020-06-13 01:29:10,14
13311142,[C++] Add BinaryArray::total_values_length(),"It's often useful to compute the total data size of a binary array.
Sample implementation:
{code:c++}
  int64_t total_values_length() const {
    return raw_value_offsets_[length() + data_->offset] - raw_value_offsets_[data_->offset];
  }
{code}
",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-06-12 13:33:22,14
13311131,[C++] Process data buffers in batch in ascii_lower / ascii_upper kernels rather than using string_view value iteration,Also add a benchmark,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-12 12:22:02,14
13311004,[R] Update autobrew script location,Jeroen is moving it to a different location.,pull-request-available,['R'],ARROW,Task,Major,2020-06-11 22:16:42,4
13310982,[C++] Fix CPU cache size detection on macOS,Running certain benchmarks on macOS never ends because CpuInfo detects the RAM size as  the size of L1 cache.,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-11 19:18:26,3
13310936,[C++][Dataset] Time-based types support,"We lack the support of date/timestamp partitions, and predicate pushdown rules. Timestamp columns are usually the most important predicate in OLAP style queries, we need to support this transparently.",dataset pull-request-available,['C++'],ARROW,Improvement,Critical,2020-06-11 15:48:14,6
13310935,[C++] Add C++ foundation to ease file transcoding,"In some situations (e.g. reading a Windows-produced CSV file), the user might transcode data before ingesting it into Arrow. Rather than build transcoding in C++ (which would require a library of encodings), we could delegate it to bindings as needed, by providing a generic InputStream facility.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-11 15:43:42,2
13310927,[C++] ParquetFileFragment scanning doesn't handle filter on partition field,"When splitting a fragment into row group fragments, filtering on the partition field raises an error.

Python reproducer:

{code:python}
df = pd.DataFrame({""dummy"": [1, 1, 1, 1], ""part"": [""A"", ""A"", ""B"", ""B""]})
df.to_parquet(""test_partitioned_filter"", partition_cols=[""part""], engine=""pyarrow"")

import pyarrow.dataset as ds
dataset = ds.dataset(""test_partitioned_filter"", format=""parquet"", partitioning=""hive"")
fragment = list(dataset.get_fragments())[0]
{code}

{code}
In [31]: dataset.to_table(filter=ds.field(""part"") == ""A"").to_pandas()                                                                                                                                              
Out[31]: 
   dummy part
0      1    A
1      1    A

In [32]: fragment.split_by_row_group(ds.field(""part"") == ""A"")                                                                                                                                                      
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-32-371cba80fd6f> in <module>
----> 1 fragment.split_by_row_group(ds.field(""part"") == ""A"")

~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.ParquetFileFragment.split_by_row_group()

~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset._insert_implicit_casts()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Field named 'part' not found or not unique in the schema.
{code}

This is probably a ""strange"" thing to do, since the fragment from a partitioned dataset is already coming only from a single partition (so will always only satisfy a single equality expression). But it's still nice that as a user you don't have to care about only passing part of the filter down to {{split_by_row_groups}}.
",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Bug,Major,2020-06-11 14:25:49,6
13310883,[Packaging] Upload built manylinux docker images,"However the secrets were set on azure pipelines the upload step is failing: 
https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=13104&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181

So the manylinux builds take more than two hours. This is due to azure's secret handling, we need to explicitly export the azure secret variables as environment variables.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-06-11 09:34:09,3
13310739,[Rust] Fix NullArray to comply with spec,"When I implemented the NullArray, I didn't comply with the spec under the premise that I'd handle reading and writing IPC in a spec-compliant way as that looked like the easier approach.

After some integration testing, I realised that I wasn't doing it correctly, so it's better to comply with the spec by not allocating any buffers for the array.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-06-10 19:47:10,12
13310682,[FlightRPC][C++][Python] Allow setting gRPC client options,"There's no way to set generic gRPC options which are useful for tuning behavior (e.g. round-robin load balancing). Rather than bind all of these one by one, gRPC allows setting arguments as generic string-string or string-integer pairs; we could expose this (and leave the interpretation implementation-dependent).",pull-request-available,"['C++', 'FlightRPC', 'Python']",ARROW,Improvement,Major,2020-06-10 14:44:55,0
13310671,[C++] Utilize function's default options when passing no options to CallFunction for a function that requires them,Otherwise benign usage of {{CallFunction}} can cause an unintuitive segfault in some cases,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-10 13:51:02,6
13310646,[C++] Bump versions of bundled libraries,"We should bump the versions of bundled dependencies, wherever possible, to ensure that users get bugfixes and improvements made in those third-party libraries.",pull-request-available,['C++'],ARROW,Task,Major,2020-06-10 12:52:48,2
13310593,[Python] A PyFileSystem handler for fsspec-based filesystems,Follow-up on ARROW-8766 to use this machinery to add an FSSpecHandler,pull-request-available,['Python'],ARROW,Sub-task,Major,2020-06-10 08:19:09,5
13310577,[Rust] Recent version of arrow crate does not compile into wasm target,"arrow 0.16 compiles successfully into wasm32-unknown-unknown, but recent git version does not. it would be nice to fix that.

compiler errors:


{noformat}
error[E0433]: failed to resolve: could not find `unix` in `os`
    --> /home/regl/.cargo/registry/src/github.com-1ecc6299db9ec823/dirs-1.0.5/src/lin.rs:41:18
     |
  41 |     use std::os::unix::ffi::OsStringExt;
     |                  ^^^^ could not find `unix` in `os`
  
  error[E0432]: unresolved import `unix`
   --> /home/regl/.cargo/registry/src/github.com-1ecc6299db9ec823/dirs-1.0.5/src/lin.rs:6:5
    |
  6 | use unix;
    |     ^^^^ no `unix` in the root{noformat}
the problem is thatprettytable-rs dependency depends on term->dirs which causes this error

consider makingprettytable-rs as dev dependency

",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-06-10 07:23:08,12
13310494,[C++][CI] Appveyor CI test failures,"See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/33417919

These seem to have been introduced by 

https://github.com/apache/arrow/commit/b058cf0d1c26ad7984c104bb84322cc7dcc66f00",pull-request-available,['C++'],ARROW,Bug,Blocker,2020-06-09 23:19:50,14
13310447,"[R] collect int64, uint32, uint64 as R integer type if not out of bounds","{{bit64::integer64}} can be awkward to work with in R (one example: https://github.com/apache/arrow/issues/7385). Often in Arrow we get {{int64}} types from [compute methods|https://github.com/apache/arrow/pull/7308] or other translation methods that auto-promote to the largest integer type, but they would fit fine in a 32-bit integer, which is R's native type. 

When calling {{Array__as_vector}} on an int64, we could first call the minmax function on the array, and if the extrema are within the range of a 32-bit int, return a regular R integer vector. This would add a little bit of ambiguity as to what R type you'll get from an Arrow type, but I wonder if the benefits are worth it since you can't do much with an integer64 in R. (We could also make this optional, similar to ARROW-7657, so you could specify a ""strict"" mode if you are in a use case where roundtrip fidelity is more important than R usability.)

Likewise, uint32 and uint64 could be kept as integers and prevent the conversion to double that is currently implemented.",pull-request-available,['R'],ARROW,Improvement,Major,2020-06-09 17:45:18,4
13310381,[C++] Write benchmark for arithmetic kernels,"The add kernel's implementation has changed in https://github.com/apache/arrow/pull/7341, in order to ensure that no performance regression was introduced write a benchmark for the kernels and compare the results with the previous implementation.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-09 12:15:15,3
13310330,[C++] Parquet writing of extension type with nested storage type fails,"A reproducer in Python:

{code:python}
import pyarrow as pa
import pyarrow.parquet as pq


class MyStructType(pa.PyExtensionType): 
 
    def __init__(self): 
        pa.PyExtensionType.__init__(self, pa.struct([('left', pa.int64()), ('right', pa.int64())])) 
 
    def __reduce__(self): 
        return MyStructType, () 


struct_array = pa.StructArray.from_arrays(
    [
        pa.array([0, 1], type=""int64"", from_pandas=True),
        pa.array([1, 2], type=""int64"", from_pandas=True),
    ],
    names=[""left"", ""right""],
)

# works
table = pa.table({'a': struct_array})
pq.write_table(table, ""test_struct.parquet"")

# doesn't work
mystruct_array = pa.ExtensionArray.from_storage(MyStructType(), struct_array)
table = pa.table({'a': mystruct_array})
pq.write_table(table, ""test_struct.parquet"")
{code}

Writing the simple StructArray nowadays works (and reading it back in as well). 

But when the struct array is the storage array of an ExtensionType, it fails with the following error:

{code}
ArrowException: Unknown error: data type leaf_count != builder_leaf_count1 2
{code}",parquet pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-09 08:18:14,2
13310273,[C++] Optimize Filter implementation,I split this off from ARROW-5760 ,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-09 03:07:49,14
13310162,[C++][Dataset] Simplify Partitioning interface,"The `int segment` of `Partitioning::Parse` should not be exposed to the user. KeyValuePartiioning should be a private Impl interface, not in public headers. 

The same apply to `Partitioning::Format`.",dataset pull-request-available,['C++'],ARROW,Improvement,Minor,2020-06-08 15:06:05,6
13310067,[C++] Support parsing date32 in dataset partition folders,"I have some data which is partitioned by year/month/date. It would be useful if the date could be automatically parsed:

{code:python}

In [17]: schema = pa.schema([(""year"", pa.int16()), (""month"", pa.int8()), (""day"", pa.date32())])

In [18]: partition = DirectoryPartitioning(schema)

In [19]: partition.parse(""/2020/06/2020-06-08"")
---------------------------------------------------------------------------
ArrowNotImplementedError Traceback (most recent call last)
<ipython-input-19-c227c808b401> in <module>
----> 1 partition.parse(""/2020/06/2020-06-08"")

~\envs\dev\lib\site-packages\pyarrow\_dataset.pyx in pyarrow._dataset.Partitioning.parse()

~\envs\dev\lib\site-packages\pyarrow\error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~\envs\dev\lib\site-packages\pyarrow\error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: parsing scalars of type date32[day]
{code}


Not a big issue since you can just use string and convert, but nevertheless it would be nice if it Just Worked
{code}

In [22]: schema = pa.schema([(""year"", pa.int16()), (""month"", pa.int8()), (""day"", pa.string())])

In [23]: partition = DirectoryPartitioning(schema)

In [24]: partition.parse(""/2020/06/2020-06-08"")
Out[24]: <pyarrow.dataset.AndExpression (((year == 2020:int16) and (month == 6:int8)) and (day == 2020-06-08:string))>
{code}",dataset,"['C++', 'Python']",ARROW,Sub-task,Minor,2020-06-08 09:23:38,6
13309976,[GLib] Add support for building Apache Arrow Datasets GLib with non-installed Apache Arrow Datasets,"It's required for packaging: https://travis-ci.org/github/ursa-labs/crossbow/builds/695595159

{noformat}
  CXX      libarrow_dataset_glib_la-scanner.lo
scanner.cpp:24:33: fatal error: arrow/util/iterator.h: No such file or directory
 #include <arrow/util/iterator.h>
                                 ^
{noformat}",pull-request-available,['GLib'],ARROW,Improvement,Major,2020-06-07 20:23:39,1
13309922,[Packaging][wheel] Boost download is failed,"https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=12893&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181

{noformat}
+ curl -sL https://dl.bintray.com/boostorg/release/1.68.0/source/boost_1_68_0.tar.gz -o /boost_1_68_0.tar.gz
+ tar xf boost_1_68_0.tar.gz
tar: This does not look like a tar archive
tar: Error exit delayed from previous errors
{noformat}",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-06-07 08:58:28,1
13309901,[C++] Support scalar aggregation over scalars,"See discussion on https://github.com/apache/arrow/pull/7308. Many/most would no-op (sum, mean, min, max), but maybe they should exist and not error? Maybe they're not needed, but I could see how you might invoke a function on the result of a previous aggregation or something.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-06 22:18:10,0
13309900,[C++] Add sum/mean kernels for Boolean type,See https://github.com/apache/arrow/pull/7308 (ARROW-6978),pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-06 22:08:09,3
13309798,[C++][R] Put more things in type_fwds,Hopefully to reduce compile time.,pull-request-available,"['C++', 'R']",ARROW,Improvement,Major,2020-06-05 21:22:28,6
13309775,[C++] Improve and expand Take/Filter benchmarks,I'm putting this up as a separate patch for review,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-05 17:15:33,14
13309724,[Go] Temporarily copy LICENSE.txt to go/,"{{go mod}} needs to find a license file in the root of the Go module. In the future ""go mod"" may be able to follow symlinks in which case this can be replaced by a symlink.",pull-request-available,['Go'],ARROW,Improvement,Major,2020-06-05 13:14:45,14
13309723,[C++] Add Subtract and Multiply arithmetic kernels with wrap-around behavior,Also avoid undefined behaviour caused by signed integer overflow.,pull-request-available,['C++'],ARROW,Sub-task,Major,2020-06-05 12:58:48,3
13309466,[C++] Implement binary (two bitmap) version of BitBlockCounter,The current BitBlockCounter from ARROW-9029 is useful for unary operations. Some operations involve multiple bitmaps and so it's useful to be able to determine the block popcounts of the AND of the respective words in the bitmaps. So each returned block would contain the number of bits that are set in both bitmaps at the same locations,pull-request-available,['C++'],ARROW,New Feature,Major,2020-06-04 14:14:18,14
13309302,[C++] Split arrow/util/bit_util.h into multiple header files,This header has grown quite large and any given compilation unit's use of it is likely limited to only a couple of functions or classes. I suspect it would improve compilation time to split up this header into a few headers organized by frequency of code use. ,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-04 01:04:26,14
13309245,[R] Implement conversion from Type::UINT64 to R vector,This case is not handled in array_to_vector.cpp,pull-request-available,['R'],ARROW,Improvement,Major,2020-06-03 17:57:09,4
13309236,"[Python] Clean up some usages of pyarrow.compat, move some common functions/symbols to lib.pyx",I started doing this while looking into ARROW-4633,pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-03 17:11:48,14
13309222,[C++] Implement BitBlockCounter interface for blockwise popcounts of validity bitmaps,"In analytics, it is common for data to be all not-null or mostly not-null. Data with > 50% nulls tends to be more exceptional. In this light, our {{BitmapReader}} class which allows iteration of each bit in a bitmap can be computationally suboptimal for mostly set validity bitmaps.

I propose instead a new interface for use in kernel implementations, for lack of a better term {{BitmapScanner}}. This works as follows:

* Uses hardware popcount to compute the number of set values in 256 bits at a time (or whatever is the right window size).
* Code can use the returned ""run"" (length + # of set bits) to switch between nullable/non-nullable code paths

For data with a lot of nulls, this may degrade performance somewhat but probably not that much empirically. However, data that is mostly-not-null should benefit from this. 

This BitmapScanner utility can probably also be used to accelerate the implementation of Filter for mostly-not-null data

I tried some other things that were slower (like trying to find the largest consecutive run of all-set words) before doing the simple 256-bit popcount solution.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-03 16:08:24,14
13309169,[C++] Add/Sub/Mul arithmetic kernels with overflow check,"-Currently the output type of the Add function is identical with the argument types which makes it unsafe to add numeric limit values, so instead of using {{(int8, int8) -> int8}} signature we should use {{((int8, int8) -> int16}}.-

The current kernels do wrap around for performance reasons, so implement different kernels with overflow checking and add arithmetics options to let the caller choose which variant to run",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-06-03 11:32:48,3
13309063,[Python] read_json won't respect explicit_schema in parse_options,"I am trying to read a json file using an explicit schema but it looks like the schema is ignored. Moreover, if the my schema contains a field not present in the json file, then the output table contains all the fields in the json file plus the fields of my schema not found in the file.

A minimal example:
{code:python}
import pyarrow as pa
from pyarrow import json

# allowing for type inference
print(json.read_json('tmp.json'))
# prints:
# pyarrow.Table
# foo: string
# baz: string

# using an explicit schema that would read only ""foo""
schema = pa.schema([('foo', pa.string())])
print(json.read_json('tmp.json', parse_options=json.ParseOptions(explicit_schema=schema)))
# prints:
# pyarrow.Table
# foo: string
# baz: string

# using an explicit schema that would read only ""not_a_field"",
# which is not present in the json file
schema = pa.schema([('not_a_field', pa.string())])
print(json.read_json('tmp.json', parse_options=json.ParseOptions(explicit_schema=schema)))
# prints:
# pyarrow.Table
# not_a_field: string
# foo: string
# baz: string
{code}

And the tmp.json file looks like:
{code:json}
{""foo"": ""bar"", ""baz"": ""1""}

{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-06-02 22:46:13,3
13308990,[Python] hdfs fails to connect to for HDFS 3.x cluster,"I'm trying to use the pyarrow hdfs connector with Hadoop 3.1.3 and I get an error that looks like a protobuf or jar mismatch problem with Hadoop. The same code works on a Hadoop 2.9 cluster.

I'm wondering if there is something special I need to do or if pyarrow doesn't support Hadoop 3.x yet?
Note I tried with pyarrow 0.15.1, 0.16.0, and 0.17.1.

  import pyarrow as pa
  hdfs_kwargs = dict(host=""namenodehost"",
           port=9000,
           user=""tgraves"",
           driver='libhdfs',
           kerb_ticket=None,
           extra_conf=None)
  fs = pa.hdfs.connect(**hdfs_kwargs)
  res = fs.exists(""/user/tgraves"")

Error that I get on Hadoop 3.x is:

dfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:
ClassCastException: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to org.apache.hadoop.shaded.com.google.protobuf.Messagejava.lang.ClassCastException: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to org.apache.hadoop.shaded.com.google.protobuf.Message
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
    at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:904)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
    at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1661)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1577)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1574)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1589)
    at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)",filesystem hdfs,['Python'],ARROW,Bug,Major,2020-06-02 16:45:33,3
13308983,[Python] Refactor the Scalar classes,"The situation regarding scalars in Python is currently not optimal.

We have two different ""types"" of scalars:

- {{ArrayValue(Scalar)}} (and subclasses of that for all types):  this is used when you access a single element of an array (eg {{arr[0]}})
- {{ScalarValue(Scalar)}} (and subclasses of that for _some_ types): this is used when wrapping a C++ scalar into a python scalar, eg when you get back a scalar from a reduction like {{arr.sum()}}.

And while we have two versions of scalars, neither of them can actually easily be used as scalar as they both can't be constructed from a python scalar (there is no {{scalar(1)}} function to use when calling a kernel, for example).

I think we should try to unify those scalar classes? (which probably means getting rid of the ArrayValue scalar)

In addition, there is an issue of trying to re-use python scalar <-> arrow conversion code, as this is also logic for this in the {{python_to_arrow.cc}} code. But this is probably a bigger change. cc [~kszucs] 

",pull-request-available,['Python'],ARROW,Improvement,Major,2020-06-02 15:44:26,3
13308963,[Packaging] Bump the minor part of the automatically generated version in crossbow,"Crossbow uses setuptools_scm to generate a development version number using git describe command. This means that it finds the latest {{reachable}} tag from the current commit on master. 

The minor releases are created from the master branch whereas the patch release tags point to commits on maintenance branches (like 0.17.x) which means that if we already have released a patch version, like 0.17.1 then crossbow generates a version number like 0.17.0.dev{number-of-commits-from-0.17.0} and bumps its patch tag, eventually creating binary packages with version 0.17.1.dev123.

The main problem with this is that the produced nightly python wheels are not picked up by pip, because on pypi we already have that patch release available and pip doesn't consider 0.17.1.dev123 newer than 0.17.1 (with --pre option passed). 

So to force pip to install the newer nightly packages we need to bump the minor version instead. ",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-06-02 14:06:57,3
13308952,[C++] Validate enum-style CMake options,"It seems that some CMake options silently allow invalid values, such as {{-DARROW_SIMD_LEVEL=foobar}}. We should validate inputs to avoid typos (such as ""SSE42"" instead of ""SSE4_2"").",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2020-06-02 13:07:36,2
13308924,[Java] Framework and interface changes for RecordBatch IPC buffer compression,"This is the first sub-work item of ARROW-8672 (
[Java] Implement RecordBatch IPC buffer compression from ARROW-300). However, it does not involve any concrete compression algorithms. The purpose of this PR is to establish basic interfaces for data compression, and make changes to the IPC framework so that different compression algorithms can be plug-in smoothly. ",pull-request-available,['Java'],ARROW,New Feature,Major,2020-06-02 09:49:08,7
13308917,[C++][Dataset] ARROW:schema should be removed from schema's metadata when reading Parquet files,"When reading a parquet file (which was written by Arrow) with the datasets API, it preserves the ""ARROW:schema"" field in the metadata:

{code:python}
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds

table = pa.table({'a': [1, 2, 3]})
pq.write_table(table, ""test.parquet"")

dataset = ds.dataset(""test.parquet"", format=""parquet"")
{code}

{code}
In [7]: dataset.schema                                                                                                                                                                        
Out[7]: 
a: int64
  -- field metadata --
  PARQUET:field_id: '1'
-- schema metadata --
ARROW:schema: '/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABAwAMAAAACAAIAAAABA' + 114

In [8]: dataset.to_table().schema                                                                                                                                                             
Out[8]: 
a: int64
  -- field metadata --
  PARQUET:field_id: '1'
-- schema metadata --
ARROW:schema: '/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABAwAMAAAACAAIAAAABA' + 114
{code}

while when reading with the `parquet` module reader, we do not preserve this metadata:

{code}
In [9]: pq.read_table(""test.parquet"").schema                                                                                                                                                  
Out[9]: 
a: int64
  -- field metadata --
  PARQUET:field_id: '1'
{code}

Since the ""ARROW:schema"" information is used to properly reconstruct the Arrow schema from the ParquetSchema, it is no longer needed once you already have the arrow schema, so it's probably not needed to keep it as metadata in the arrow schema.",dataset,['C++'],ARROW,Bug,Major,2020-06-02 09:26:38,14
13308904,[Rust] Support appending arrays by merging array data,"ARROW-9005 introduces a concat kernel which allows for concatenating multiple arrays of the same type into a single array. This is useful for sorting on multiple arrays, among other things.

The concat kernel is implemented for most array types, but not yet for nested arrays (lists, structs, etc).

This Jira is for creating a way of appending/merging all array types, so that concat (and functionality that depends on it) can support all array types.",pull-request-available,['Rust'],ARROW,New Feature,Major,2020-06-02 08:25:38,12
13308845,[C++] Use Cast kernels to implement Scalar::Parse and Scalar::CastTo,We should not maintain distinct (and possibly differently behaving) implementations of elementwise array casting and scalar casting. The new kernels framework provides for relatively easily generating kernels that can process arrays or scalars. ,pull-request-available,['C++'],ARROW,Improvement,Major,2020-06-02 02:05:21,3
13308748,"[Python][C++] Non-deterministic segfault in ""AMD64 MacOS 10.15 Python 3.7"" build","I've been seeing this segfault periodically the last week, does anyone have an idea what might be wrong?

https://github.com/apache/arrow/pull/7273/checks?check_run_id=717249862",dataset,['Python'],ARROW,Bug,Major,2020-06-01 15:10:39,3
13308744,[Archery] Benchmark formatter should have friendly units,The current output is not friendly to glance at. Usage of humanfriendly can help here.,pull-request-available,['Archery'],ARROW,Improvement,Minor,2020-06-01 15:01:14,13
13308560,[C++] Document available functions in compute::FunctionRegistry,Create a compute page in the C++ section of the Sphinx docs and make a list of the available functions and what they do,pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Major,2020-05-31 12:59:30,2
13308557,[Python] After upgrade pyarrow from 0.15 to 0.17.1 connect to hdfs don`t work with libdfs jni,"h2. Problem

After upgrade pyarrow from 0.15 to 0.17, I have a some troubles. I understand, that libhdfs3 no support now. However, in my case, libhdfs not work too. See below.

My experience in the Hadoop ecosystem is not big. Maybe, I took a some wrongs. I installed Hortonworks HDP from Ambari service on the virtual machine, installed on my PC.

I try that..

1. just connect..

%xmode Verbose
import pyarrow as pa

hdfs = pa.hdfs.connect(host='hdp.test.com', port=8020, user='hdfs')

---

FileNotFoundError: [Errno 2] No such file or directory: 'hadoop': 'hadoop' ([#1.txt])

2.to bypass if driver == 'libhdfs'..

%xmode Verbose

import pyarrow as pa

hdfs = pa.HadoopFileSystem(host='hdp.test.com', port=8020, user='hdfs', driver=None')

---

OSError: Unable to load libjvm: /usr/java/latest//lib/server/libjvm.so: cannot open shared object file: No such file or directory ([#2.txt])

3. With libhdfs3 it working:

import hdfs3

hdfs = hdfs3.HDFileSystem(host='hdp.test.com', port=8020, user='hdfs')

#ls remote folder
hdfs.ls('/data/', detail=False)

['/data/TimeSheet.2020-04-11', '/data/test', '/data/test.json']
h2. Environment.
h4. +Client PC:+

OS: Debian 10. Dev.: Anaconda3 (python 3.7.6), Jupyter Lab 2, pyarrow 0.17.1 (from conda-forge)

+Hadoop+ (on VM  Oracle VirtualBox):

OS: Oracle Linux 7.6. Distr.: Hortonworks HDP 3.1.4

libhdfs.so:

[root@hdp /]# find / -name libhdfs.so
 /usr/lib/ams-hbase/lib/hadoop-native/libhdfs.so
 /usr/hdp/3.1.4.0-315/usr/lib/libhdfs.so



Java path:

[root@hdp /]# sudo alternatives --config java
 
-----------------------------------------------
 *+ 1 java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64/jre/bin/java)



libjvm:       

[root@hdp /]# find / -name libjvm.*
 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64/jre/lib/amd64/server/libjvm.so
 /usr/jdk64/jdk1.8.0_112/jre/lib/amd64/server/libjvm.so



I tried many settings (. Below last :

# etc/profile.
 ...
export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac)))))
export JRE_HOME=$JAVA_HOME/jre
export JAVA_CLASSPATH=$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar
export HADOOP_HOME=/usr/hdp/3.1.4.0-315/hadoop
export HADOOP_CLASSPATH=$(find $HADOOP_HOME -name '*.jar' | xargs echo | tr ' ' ':')
export ARROW_LIBHDFS_DIR=/usr/lib/ams-hbase/lib/hadoop-native

export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export CLASSPATH==.:$CLASSPATH:$JAVA_CLASSPATH:$HADOOP_CLASSPATH

export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JRE_HOME/lib/amd64/server


",filesystem hdfs hortonworks libhdfs,['Python'],ARROW,Bug,Major,2020-05-31 12:41:53,3
13308532,"[Format] Add ""byte width"" field with default of 16 to Decimal Flatbuffers type for forward compatibility",This will permit larger or smaller decimals to be added to the format later without having to add a new Type union value,pull-request-available,['Format'],ARROW,Improvement,Major,2020-05-30 22:25:28,14
13308302,[Python] Metadata grows exponentially when using schema from disk,"When overwriting parquet files we first read the schema that is already on disk this is mainly to deal with some type harmonizing between pyarrow and pandas (that I wont go into).

Regardless here is a simple example (below) with no weirdness. If I continously re-write the same file by first fetching the schema from disk, creating a writer with that schema and then writing same dataframe the file size keeps growing even though the amount of rows has not changed.

Note: My solution was to remove `b'ARROW:schema'` data from the `schema.metadata.` this seems to stop the file size growing. So I wonder if the writer keeps appending to it or something? TBH I'm not entirely sure but I have a hunch that the ARROW:schema is just the metadata serialised or something.

I should also note that once the metadata gets to big this leads to a buffer overflow in another part of the code 'thrift' which was referenced here:https://issues.apache.org/jira/browse/PARQUET-1345
{code:java}
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow as pa
import pandas as pd
import pathlib
import sys
def main():
    print(f""python: {sys.version}"")
    print(f""pa version: {pa.__version__}"")
    print(f""pd version: {pd.__version__}"")    fname = ""test.pq""
    path = pathlib.Path(fname)    df = pd.DataFrame({""A"": [0] * 100000})
    df.to_parquet(fname)    print(f""Wrote test frame to {fname}"")
    print(f""Size of {fname}: {path.stat().st_size}"")    for _ in range(5):
        file = pq.ParquetFile(fname)
        tmp_df = file.read().to_pandas()
        print(f""Number of rows on disk: {tmp_df.shape}"")
        print(""Reading schema from disk"")
        schema = file.schema.to_arrow_schema()
        print(""Creating new writer"")
        writer = pq.ParquetWriter(fname, schema=schema)
        print(""Re-writing the dataframe"")
        writer.write_table(pa.Table.from_pandas(df))
        writer.close()
        print(f""Size of {fname}: {path.stat().st_size}"")
if __name__ == ""__main__"":
    main()
{code}
{code:java}
(sdm)  ~ python growing_metadata.py
python: 3.7.3 | packaged by conda-forge | (default, Dec 6 2019, 08:36:57)
[Clang 9.0.0 (tags/RELEASE_900/final)]
pa version: 0.16.0
pd version: 0.25.2
Wrote test frame to test.pq
Size of test.pq: 1643
Number of rows on disk: (100000, 1)
Reading schema from disk
Creating new writer
Re-writing the dataframe
Size of test.pq: 3637
Number of rows on disk: (100000, 1)
Reading schema from disk
Creating new writer
Re-writing the dataframe
Size of test.pq: 8327
Number of rows on disk: (100000, 1)
Reading schema from disk
Creating new writer
Re-writing the dataframe
Size of test.pq: 19301
Number of rows on disk: (100000, 1)
Reading schema from disk
Creating new writer
Re-writing the dataframe
Size of test.pq: 44944
Number of rows on disk: (100000, 1)
Reading schema from disk
Creating new writer
Re-writing the dataframe
Size of test.pq: 104815{code}",metadata parquet pull-request-available pyarrow python schema,['Python'],ARROW,Bug,Major,2020-05-29 11:19:55,14
13308171,"[C++][Compute] ""Conditional jump or move depends on uninitialised value(s)"" Valgrind warning","https://github.com/ursa-labs/crossbow/runs/715700830#step:6:4277

{noformat}
[ RUN      ] TestCallScalarFunction.PreallocationCases
==5357== Conditional jump or move depends on uninitialised value(s)
==5357==    at 0x51D69A6: void arrow::internal::TransferBitmap<false, true>(unsigned char const*, long, long, long, unsigned char*) (bit_util.cc:176)
==5357==    by 0x51CE866: arrow::internal::CopyBitmap(unsigned char const*, long, long, unsigned char*, long, bool) (bit_util.cc:208)
==5357==    by 0x52B6325: arrow::compute::detail::NullPropagator::PropagateSingle() (exec.cc:295)
==5357==    by 0x52B36D1: Execute (exec.cc:378)
==5357==    by 0x52B36D1: arrow::compute::detail::PropagateNulls(arrow::compute::KernelContext*, arrow::compute::ExecBatch const&, arrow::ArrayData*) (exec.cc:412)
==5357==    by 0x52BA7F3: ExecuteBatch (exec.cc:586)
==5357==    by 0x52BA7F3: arrow::compute::detail::ScalarExecutor::Execute(std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::detail::ExecListener*) (exec.cc:542)
==5357==    by 0x52BC21F: arrow::compute::Function::Execute(std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) const (function.cc:94)
==5357==    by 0x52B141C: arrow::compute::CallFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::FunctionOptions const*, arrow::compute::ExecContext*) (exec.cc:937)
==5357==    by 0x52B16F2: arrow::compute::CallFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<arrow::Datum, std::allocator<arrow::Datum> > const&, arrow::compute::ExecContext*) (exec.cc:942)
==5357==    by 0x155515: arrow::compute::detail::TestCallScalarFunction_PreallocationCases_Test::TestBody()::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)#1}::operator()(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) const (exec_test.cc:756)
==5357==    by 0x156AF2: arrow::compute::detail::TestCallScalarFunction_PreallocationCases_Test::TestBody() (exec_test.cc:786)
==5357==    by 0x5BE4862: void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /opt/conda/envs/arrow/lib/libgtest.so)
==5357==    by 0x5BDEDE2: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (in /opt/conda/envs/arrow/lib/libgtest.so)
==5357== 
{noformat}
",pull-request-available,['C++'],ARROW,Bug,Major,2020-05-28 21:47:13,14
13308159,[R] Table$create with schema crashes with some dictionary index types,"On the latest nightly build in R, using Table$create with a custom schema can crash R entirely (fatal error/bomb in RStudio) when the schema includes a different index_type for the dictionary than expected.

Example:
{code:r}
library(arrow)

native = data.frame(a = c(1, 2, 3), b = as.factor(c(""a"", ""b"", ""c"")))

#Works. 'a' is <float>, dictionary is string - int8
Table$create(native)

# Works, although 'a' is cast to int32.
Table$create(native, schema = s1)
s1 = schema(a = uint32(), b = dictionary(value_type = arrow::string(), index_type = arrow::int8()))

# Crashes R on my system because index_type is int16(), not int8()
s2 = schema(a = uint32(), b = dictionary(value_type = arrow::string(), index_type = arrow::int16()))
Table$create(native, schema = s2)
{code}


On restart, following log is in my rstudio session:


{noformat}
/private/var/folders/84/dvp0h0kn22qcx_0z_hn_b36w0000gn/T/hbtmp/apache-arrow-20200528-16757-1uok2ln/cpp/src/arrow/array.cc:1194: Check failed: (indices->type_id()) == (dict.index_type()->id()) 0 arrow.so 0x000000010dd2b3cd _ZN5arrow4util7CerrLogD2Ev + 209 1 arrow.so 0x000000010dd2b2ee _ZN5arrow4util7CerrLogD0Ev + 14 2 arrow.so 0x000000010dd2b296 _ZN5arrow4util8ArrowLogD1Ev + 34 3 arrow.so 0x000000010daf2429 _ZN5arrow15DictionaryArray10FromArraysERKNSt3__110shared_ptrINS_8DataTypeEEERKNS2_INS_5ArrayEEESA_ + 619 4 arrow.so 0x000000010d8f1eff _ZN5arrow1r19MakeFactorArrayImplINS_8Int8TypeEEENSt3__110shared_ptrINS_5ArrayEEEN4Rcpp6VectorILi13ENS7_16NoProtectStorageEEERKNS4_INS_8DataTypeEEE + 1743 5 arrow.so 0x000000010d8f1684 _ZN5arrow1r15MakeFactorArrayEN4Rcpp6VectorILi13ENS1_16NoProtectStorageEEERKNSt3__110shared_ptrINS_8DataTypeEEE + 260 6 arrow.so 0x000000010d8f40f4 _ZN5arrow1r18Array__from_vectorEP7SEXPRECRKNSt3__110shared_ptrINS_8DataTypeEEEb + 292 7 arrow.so 0x000000010d98e418 _ZZ16Table__from_dotsP7SEXPRECS0_ENK3$_1clEiS0_ + 440 8 arrow.so 0x000000010d98d15b _Z16Table__from_dotsP7SEXPRECS0_ + 1611 9 arrow.so 0x000000010d95af3b _arrow_Table__from_dots + 91 10 libR.dylib 0x0000000105466b5d R_doDotCall + 1437 11 libR.dylib 0x00000001054b2a7a bcEval + 105226 12 libR.dylib 0x0000000105498831 Rf_eval + 385 13 libR.dylib 0x00000001054b8cf1 R_execClosure + 2193 14 libR.dylib 0x00000001054b7ac9 Rf_applyClosure + 473 15 libR.dylib 0x000000010549f9a8 bcEval + 27192 16 libR.dylib 0x0000000105498831 Rf_eval + 385 17 libR.dylib 0x00000001054b71cc forcePromise + 172 18 libR.dylib 0x00000001054c2b4a getvar + 778 19 libR.dylib 0x000000010549cb8c bcEval + 15388 20 libR.dylib 0x0000000105498831 Rf_eval + 385 21 libR.dylib 0x00000001054b71cc forcePromise + 172 22 libR.dylib 0x00000001054c2b4a getvar + 778 23 libR.dylib 0x000000010549cb8c bcEval + 15388 24 libR.dylib 0x0000000105498831 Rf_eval + 385 25 libR.dylib 0x00000001054b8cf1 R_execClosure + 2193 26 libR.dylib 0x00000001054b7ac9 Rf_applyClosure + 473 27 libR.dylib 0x000000010549f9a8 bcEval + 27192 28 libR.dylib 0x0000000105498831 Rf_eval + 385 29 libR.dylib 0x00000001054b8cf1 R_execClosure + 2193 30 libR.dylib 0x00000001054b7ac9 Rf_applyClosure + 473 31 libR.dylib 0x000000010549f9a8 bcEval + 27192 32 libR.dylib 0x0000000105498831 Rf_eval + 385 33 libR.dylib 0x00000001054b8cf1 R_execClosure + 2193 34 libR.dylib 0x00000001054b7ac9 Rf_applyClosure + 473 35 libR.dylib 0x0000000105498d06 Rf_eval + 1622 36 libR.dylib 0x00000001054edcda Rf_ReplIteration + 810 37 libR.dylib 0x00000001054ef1ff run_Rmainloop + 207 38 rsession 0x0000000104bef4d0 _ZN7rstudio1r7session12runEmbeddedRERKNS_4core8FilePathES5_bb7SA_TYPERKNS1_9CallbacksEPNS1_17InternalCallbacksE + 416

{noformat}
",pull-request-available,['R'],ARROW,Bug,Minor,2020-05-28 21:30:05,4
13308043,[FlightRPC][C++] Fix flaky MacOS tests,"The gRPC MacOS tests have been flaking again.

Looking at [https://github.com/grpc/grpc/issues/20311] they may possibly have been fixed except [https://github.com/grpc/grpc/issues/13856] reports they haven't (in some configurations?) so I will try a few things in CI, or just disable the tests on MacOS.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-05-28 13:31:43,0
13307885,[Java] Support batch value appending for large varchar/varbinary vectors,Support appending values in batch for LargeVarCharVector/LargeVarBinaryVector.,pull-request-available,['Java'],ARROW,New Feature,Major,2020-05-28 02:17:41,7
13307883,[Java] Support range value comparison for large varchar/varbinary vectors,Support comparing a range of values for LargeVarCharVector and LargeVarBinaryVector.,pull-request-available,['Java'],ARROW,New Feature,Major,2020-05-28 02:15:48,7
13307840,[C++] Reduce generated code in compute/kernels/scalar_compare.cc,"We are instantiating multiple versions of templates in this module for cases that, byte-wise, do the exact same comparison. For example:

* For equals, not_equals, we can use the same 32-bit/64-bit comparison kernels for signed int / unsigned int / floating point types of the same byte width
* TimestampType can reuse int64 kernels, similarly for other date/time types
* BinaryType/StringType can share kernels

etc.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-27 20:45:47,14
13307774,[Python][Documentation] Pyarrow documentation for pip nightlies references 404'd location,"The pyarrow documentation gives to options for nightly builds, one for use with anaconda, one for use with pip. While the anaconda command works, the command for pip sends users to

[https://repo.fury.io/arrow-nightlies/,] a url which 404s. Sphinx docs need updated for correct url of gemfury.com/arrow-nightlies/",pull-request-available,['Python'],ARROW,Task,Minor,2020-05-27 15:07:57,3
13307622,[C++] Linking failure with clang-4.0,"{code:java}
FAILED: release/arrow-file-to-stream
: && /Users/uwe/miniconda3/envs/pyarrow-dev/bin/ccache /Users/uwe/miniconda3/envs/pyarrow-dev/bin/x86_64-apple-darwin13.4.0-clang++  -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -Qunused-arguments -fcolor-diagnostics -O3 -DNDEBUG  -Wall -Wno-unknown-warning-option -Wno-pass-failed -msse4.2  -O3 -DNDEBUG -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.15.sdk -Wl,-search_paths_first -Wl,-headerpad_max_install_names -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs src/arrow/ipc/CMakeFiles/arrow-file-to-stream.dir/file_to_stream.cc.o  -o release/arrow-file-to-stream  release/libarrow.a /usr/local/opt/openssl@1.1/lib/libssl.dylib /usr/local/opt/openssl@1.1/lib/libcrypto.dylib /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libbrotlienc-static.a /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libbrotlidec-static.a /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libbrotlicommon-static.a /Users/uwe/miniconda3/envs/pyarrow-dev/lib/liblz4.dylib /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libsnappy.1.1.7.dylib /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libz.dylib /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libzstd.dylib /Users/uwe/miniconda3/envs/pyarrow-dev/lib/liborc.a /Users/uwe/miniconda3/envs/pyarrow-dev/lib/libprotobuf.dylib jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a && :
Undefined symbols for architecture x86_64:
  ""arrow::internal::(anonymous namespace)::StringToFloatConverterImpl::main_junk_value_"", referenced from:
      arrow::internal::StringToFloat(char const*, unsigned long, float*) in libarrow.a(value_parsing.cc.o)
      arrow::internal::StringToFloat(char const*, unsigned long, double*) in libarrow.a(value_parsing.cc.o)
  ""arrow::internal::(anonymous namespace)::StringToFloatConverterImpl::fallback_junk_value_"", referenced from:
      arrow::internal::StringToFloat(char const*, unsigned long, float*) in libarrow.a(value_parsing.cc.o)
      arrow::internal::StringToFloat(char const*, unsigned long, double*) in libarrow.a(value_parsing.cc.o)
ld: symbol(s) not found for architecture x86_64
clang-4.0: error: linker command failed with exit code 1 (use -v to see invocation) {code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-05-27 06:43:33,8
13307590,[C++] Add utf8proc library to toolchain,"This is a minimal MIT-licensed library for UTF-8 data processing originally developed for use in Julia

https://github.com/JuliaStrings/utf8proc",pull-request-available,['C++'],ARROW,New Feature,Major,2020-05-27 02:25:13,8
13307568,[Rust] Broken build due to new benchmark crate using old API,Broken build due to new benchmark crate using old API,pull-request-available,['Rust'],ARROW,Bug,Major,2020-05-26 23:51:36,10
13307496,[C++] Fix compiler warning in compute/kernels/scalar_cast_temporal.cc,"The kernel functor can return an uninitialized value on errors

{code}
../src/arrow/compute/kernels/scalar_cast_temporal.cc: In member function OUT arrow::compute::internal::ParseTimestamp::Call(arrow::compute::KernelContext*, ARG0) const [with OUT = long int; ARG0 = nonstd::sv_lite::basic_string_view<char>]:
../src/arrow/compute/kernels/scalar_cast_temporal.cc:267:12: warning: result may be used uninitialized in this function [-Wmaybe-uninitialized]
     return result;
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-05-26 17:33:52,14
13307488,[C++] Make head optional in s3fs,"When you open an input file with the f3fs, it issues a head request to S3 to check if the file is present/authorized and get the size (https://github.com/apache/arrow/blob/f16f76ab7693ae085e82f4269a0a0bc23770bef9/cpp/src/arrow/filesystem/s3fs.cc#L407).

This call comes with a non-neglictable cost:
 * adds latency
 * priced the same as a GET request by AWS

I fail to see usecases where this call is really crucial:
 * if the file is not present/authorized, failing at first read seems to have mostly the same effect as failing on opening. I agree that it is kind of ""usual"" for an _open_ call to fail eagerly, so to avoid surprises we could add a flag indicating if we don't need to fail when running _OpenInputFile_ on an inaccessible file.
 * getting the size can be done on the first read, and could be mostly avoided on caller side if the filesystem api provided read-from-end capabilities (compatible with fs reads using _ios::end_ and on http filesystems with _bytes=-xxx_). Worst case scenario the call to _head_ could be done lazily when calling _getSize()._

I agree that it makes things a bit more complex, and I understand that you would not want to complexify the generic fs api because of blob storage behavior. But obviously there are workloads where this has a significant impact.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-26 17:13:45,2
13307465,[Python] Add tests for parquet.write_metadata metadata_collector,Follow-up on ARROW-8062: the PR added functionality to {{parquet.write_metadata}} to pass a a collection of metadata objects to be concatenated. We should add some specific tests for this.,pull-request-available,['Python'],ARROW,Test,Major,2020-05-26 14:56:33,5
13307443,[C++][Dataset] Add support for Partitioning to ParquetDatasetFactory,Follow-up on ARROW-8062: the ParquetDatasetFactory currently does not yet support specifying a Partitioning / inferring with a PartitioningFactory.,dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-26 13:42:30,13
13307412,[R] Detect compression in reading CSV/JSON,"Hi all,

Apologises if this has already been covered by another ticket. Is it possible for arrow to read in compress delimited files (for example gzip)?

Currently I get an error when trying to read in a compressed delimited file:


{code:java}
vroom::vroom_write(iris, ""iris.csv.gz"", delim = "","")
arrow::read_csv_arrow(""iris.csv.gz"")

# Error in csv__TableReader_Read(self) :
# Invalid: CSV parse error: Expected 1 columns, got 4{code}
however it can be read in by vroom and readr:
{code:java}
vroom::vroom(""iris.csv.gz"")
readr::read_csv(""iris.csv.gz"")
{code}






",pull-request-available,['R'],ARROW,New Feature,Major,2020-05-26 11:37:28,4
13307409,[C++/Python] arrow-nightlies conda repository is full,"You currently have 3 public packages and 0 packages that require to be authenticated.
Using 10.0 GB of 3.0 GB storage



We need a script to delete old packages, e.g. once a week?",pull-request-available,"['C++', 'Packaging', 'Python']",ARROW,Improvement,Major,2020-05-26 10:43:49,8
13307386,[Java] Fix the performance degradation of integration tests,"In the past, we run integration tests from main methods, and recently, we have changed this to run them by the failsafe plugin. 

This is a good change, but it also leads to significant performance degradation. In the past, it took about 10s to run {{ITTestLargeVector#testLargeDecimalVector}}, now it takes more than half an hour. 

Our investigation shows that the problem was caused by calling {{HistoricalLog#recordEvent}} repeatedly. This method is called only when {{BaseAllocator#DEBUG}} is enabled. In a unit/integration test, the flag is enabled by default. 

We solve the problem with the following steps:
1. We set system property to disable the {{BaseAllocator#DEBUG}} flag.
2. We change the logic so that the system property takes precedence over the {{AssertionUtil#isAssertionsEnabled}} method. 

This makes the integration tests as fast as before. ",pull-request-available,['Java'],ARROW,Improvement,Major,2020-05-26 09:42:42,7
13307285,[R] Provide binding for arrow::compute::CallFunction,This will drastically simplify exposing new functions to R users,pull-request-available,['R'],ARROW,New Feature,Major,2020-05-25 19:49:24,4
13307255,"[C++] Add ""parse_strptime"" function for string to timestamp conversions using the kernels framework",This should be relatively straightforward to implement using the new kernels framework,pull-request-available,['C++'],ARROW,New Feature,Major,2020-05-25 14:55:52,14
13307249,[C++] Add timestamp subtract kernel aliased to int64 subtract implementation,"We can use the same scalar exec function for int64 subtraction as well as {{(array[TIMESTAMP], array[TIMESTAMP]) -> duration}}. ",pull-request-available,['C++'],ARROW,New Feature,Major,2020-05-25 14:38:32,14
13307242,[C++] Reduce generated code in vector_hash.cc,"Since hashing doesn't need to know about logical types, we can do the following:

* Use same generated code for both BinaryType and StringType (and LargeBinary/LargeString)
* Use same generated code for primitive types having the same byte width

These two changes should reduce binary size and improve compilation speed",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-25 13:55:08,14
13307112,[C++] Change compute::Arity:VarArgs min_args default to 0,The issue of minimum number of arguments is separate from providing an {{InputType}} for input type checking. ,pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-25 01:49:52,14
13307098,[C++] Improve docstrings in new public APIs in arrow/compute and fix miscellaneous typos,I've noticed some imprecise language while reading the headers and some other opportunities for improvement,pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-24 21:10:39,14
13307084,[Rust] [DataFusion] CsvExec::schema() returns incorrect results,"CsvExec::schema() returns the underlying CSV schema and not the projected schema. Also, the documentation for the CsvExec schema field is incorrect.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-05-24 18:32:11,10
13307073,[C++] Implement example string scalar kernel function to assist with string kernels buildout per ARROW-555,I will write a patch to provide an example of creating a string-input string-output kernel for executing scalar-valued string functions,pull-request-available,['C++'],ARROW,New Feature,Major,2020-05-24 15:00:25,14
13307063,"[C++] Add ""DispatchBest"" APIs to compute::Function that selects a kernel that may require implicit casts to invoke","Currently we have ""DispatchExact"" which requires an exact match of input types. ""DispatchBest"" would permit kernel selection with implicit casts required. Since multiple kernels may be valid when allowing implicit casts, we will need to break ties by estimating the ""cost"" of the implicit casts. For example, casting int8 to int32 is ""less expensive"" than implicitly casting to int64",compute pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-24 13:09:12,6
13307061,"[C++] Add cast ""metafunction"" to FunctionRegistry that addresses dispatching to appropriate type-specific CastFunction","By setting the output type in {{CastOptions}}, we can write

{code}
call_function(""cast"", [arg], cast_options)
{code}

This simplifies use of casting for binding developers. This mimics the standard SQL

{code}
CAST(expr AS target_type)
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-24 13:00:05,14
13307060,"[C++][Compute] Formalize ""metafunction"" concept","A metafunction is a function that provides the {{Execute}} API but does not contain any kernels. Such functions can also handle non-Array/Scalar inputs like RecordBatch or Table. 

This will enable bindings to invoke such functions (like take, filter) like

{code}
call_function('take', [table, indices])
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-24 12:55:46,14
13307004,[C++] Slicing a ChunkedArray with zero chunks segfaults,"{code:python}
import pyarrow as pa
arr = pa.chunked_array([[1]])
empty = arr.filter(pa.array([False]))
print(empty)
print(empty[:]) # <- crash
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Critical,2020-05-23 23:29:11,14
13306965,[Java] Out of order writes using setSafe,"I noticed that calling setSafe on a VarCharVector with indices not in increasing order causes the lastIndex to be set to the index in the last call to setSafe.

Is this a documented and expected behavior ?

Sample code:
{code:java}
import java.util.Collections;
import lombok.extern.slf4j.Slf4j;
import org.apache.arrow.memory.RootAllocator;
import org.apache.arrow.vector.VarCharVector;
import org.apache.arrow.vector.VectorSchemaRoot;
import org.apache.arrow.vector.types.pojo.ArrowType;
import org.apache.arrow.vector.types.pojo.Field;
import org.apache.arrow.vector.types.pojo.Schema;
import org.apache.arrow.vector.util.Text;

@Slf4j
public class ATest {

  public static void main() {
    Schema schema = new Schema(Collections.singletonList(Field.nullable(""Data"", new ArrowType.Utf8())));
    try (VectorSchemaRoot vroot = VectorSchemaRoot.create(schema, new RootAllocator())) {
      VarCharVector vec = (VarCharVector) vroot.getVector(""Data"");

      for (int i = 0; i < 10; i++) {
        vec.setSafe(i, new Text(Integer.toString(i) + ""_mtest""));
      }

      vec.setSafe(7, new Text(Integer.toString(7) + ""_new""));

      log.info(""Data at index 8 Before {}"", vec.getObject(8));
      vroot.setRowCount(10);
      log.info(""Data at index 8 After {}"", vec.getObject(8));
      log.info(vroot.contentToTSVString());
    }
  }
}
{code}


If I don't set the index 7 after the loop, I get all the 0_mtest, 1_mtest, ..., 9_mtest entries.

If I set index 7 after the loop, I see 0_mtest, ..., 5_mtest, 6_mtext, 7_new,
  Before the setRowCount, the data at index 8 is ->*st8_mtest*; index 9 is*9_mtest*
 After the setRowCount, the data at index 8 is -> """" ; index 9 is """"

With a text with more chars instead of 4 with _new, it keeps eating into the data at the following indices.

",pull-request-available,['Java'],ARROW,Bug,Major,2020-05-23 13:47:33,7
13306915,[Python] Fix usages of deprecated C++ APIs related to child/field,"{code}
-- Running cmake --build for pyarrow
cmake --build . --config debug -- -j16
[19/20] Building CXX object CMakeFiles/lib.dir/lib.cpp.o
lib.cpp:20265:85: warning: 'num_children' is deprecated: Use num_fields() [-Wdeprecated-declarations]
  __pyx_t_1 = __pyx_f_7pyarrow_3lib__normalize_index(__pyx_v_i, __pyx_v_self->type->num_children()); if (unlikely(__pyx_t_1 == ((Py_ssize_t)-1L))) __PYX_ERR(1, 119, __pyx_L1_error)
                                                                                    ^
/home/wesm/local/include/arrow/type.h:263:3: note: 'num_children' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use num_fields()"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:20276:76: warning: 'child' is deprecated: Use field(i) [-Wdeprecated-declarations]
  __pyx_t_2 = __pyx_f_7pyarrow_3lib_pyarrow_wrap_field(__pyx_v_self->type->child(__pyx_v_index)); if (unlikely(!__pyx_t_2)) __PYX_ERR(1, 120, __pyx_L1_error)
                                                                           ^
/home/wesm/local/include/arrow/type.h:251:3: note: 'child' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use field(i)"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:20507:56: warning: 'num_children' is deprecated: Use num_fields() [-Wdeprecated-declarations]
  __pyx_t_1 = __Pyx_PyInt_From_int(__pyx_v_self->type->num_children()); if (unlikely(!__pyx_t_1)) __PYX_ERR(1, 139, __pyx_L1_error)
                                                       ^
/home/wesm/local/include/arrow/type.h:263:3: note: 'num_children' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use num_fields()"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:23361:44: warning: 'num_children' is deprecated: Use num_fields() [-Wdeprecated-declarations]
  __pyx_r = __pyx_v_self->__pyx_base.type->num_children();
                                           ^
/home/wesm/local/include/arrow/type.h:263:3: note: 'num_children' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use num_fields()"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:24039:44: warning: 'num_children' is deprecated: Use num_fields() [-Wdeprecated-declarations]
  __pyx_r = __pyx_v_self->__pyx_base.type->num_children();
                                           ^
/home/wesm/local/include/arrow/type.h:263:3: note: 'num_children' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use num_fields()"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:58220:37: warning: 'child' is deprecated: Use field(pos) [-Wdeprecated-declarations]
  __pyx_v_child = __pyx_v_self->ap->child(__pyx_v_child_id);
                                    ^
/home/wesm/local/include/arrow/array.h:1281:3: note: 'child' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use field(pos)"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:58956:74: warning: 'children' is deprecated: Use fields() [-Wdeprecated-declarations]
  __pyx_v_child_fields = __pyx_v_self->__pyx_base.__pyx_base.type->type->children();
                                                                         ^
/home/wesm/local/include/arrow/type.h:257:3: note: 'children' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use fields()"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
lib.cpp:78247:73: warning: 'child' is deprecated: Use field(pos) [-Wdeprecated-declarations]
  __pyx_v_result = (( arrow::UnionArray *)__pyx_v_self->__pyx_base.ap)->child(__pyx_v_pos);
                                                                        ^
/home/wesm/local/include/arrow/array.h:1281:3: note: 'child' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use field(pos)"")
  ^
/home/wesm/local/include/arrow/util/macros.h:104:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
8 warnings generated.

{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2020-05-23 03:39:30,14
13306875,[C++] Reimplement dictionary unpacking in Cast kernels using Take,As suggested by [~apitrou] this should yield less code to maintain,pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-22 21:40:59,14
13306873,"[C++] Add C++ unit tests for filter and take functions on temporal type inputs, including timestamps","These are used in R but not tested in C++, so I only found out that I had missed adding the kernels to the Filter VectorFunction when running the R test suite",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-22 21:17:33,14
13306843,[C++][CI] CI builds for MSVC do not build benchmarks,"We must ensure that our benchmarks always build on Windows

I'm fixing these errors for example in ARROW-8792

{code}
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(249): error C2220: warning treated as error - no 'object' file generated
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(256): note: see reference to function template instantiation 'void parquet::BM_PlainEncodingSpaced<parquet::BooleanType>(benchmark::State &)' being compiled
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(249): warning C4244: 'argument': conversion from 'const int64_t' to 'int', possible loss of data
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(292): warning C4244: 'argument': conversion from 'const int64_t' to 'int', possible loss of data
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(306): note: see reference to function template instantiation 'void parquet::BM_PlainDecodingSpaced<parquet::BooleanType>(benchmark::State &)' being compiled
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(299): warning C4244: 'argument': conversion from 'int64_t' to 'int', possible loss of data
C:/Users/wesmc/code/arrow/cpp/src/parquet/encoding_benchmark.cc(300): warning C4244: 'argument': conversion from 'const int64_t' to 'int', possible loss of data
[11/67] Linking CXX executable release\arrow-ipc-read-write-benchmark.exe
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-05-22 17:26:07,3
13306761,[Python] Python 3.7 SIGSEGV when comparing RecordBatch to None,"This seems to only happen for Python 3.6 and 3.7. It doesn't happen with 3.8. It seems to happen even when built from source, but I used the wheels for this reproduction.
{noformat}
> uname -a
Linux chaconne 5.6.13-arch1-1 #1 SMP PREEMPT Thu, 14 May 2020 06:52:53 +0000 x86_64 GNU/Linux
> python --version
Python 3.7.7
> pip freeze
numpy==1.18.4
pyarrow==0.17.1{noformat}
Reproduction:
{code:python}
import pyarrow as pa
table = pa.Table.from_arrays([pa.array([1,2,3])], names=[""a""])
batches = table.to_batches()
batches[0].equals(None)
{code}
{noformat}
#0  0x00007fffdf9d34f0 in arrow::RecordBatch::num_columns() const () from /home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.7/site-packages/pyarrow/libarrow.so.17
#1  0x00007fffdf9d69e9 in arrow::RecordBatch::Equals(arrow::RecordBatch const&, bool) const () from /home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.7/site-packages/pyarrow/libarrow.so.17
#2  0x00007fffe084a6e0 in __pyx_pw_7pyarrow_3lib_11RecordBatch_31equals(_object*, _object*, _object*) () from /home/lidavidm/Code/twosigma/arrow/venv2/lib/python3.7/site-packages/pyarrow/lib.cpython-37m-x86_64-linux-gnu.so
#3  0x00005555556b97e4 in _PyMethodDef_RawFastCallKeywords (method=0x7fffe0c1b760 <__pyx_methods_7pyarrow_3lib_RecordBatch+288>, self=0x7fffdefd7110, args=0x7ffff786f5c8, nargs=<optimized out>, kwnames=<optimized out>)
    at /tmp/build/80754af9/python_1585000375785/work/Objects/call.c:694
#4  0x00005555556c06af in _PyMethodDescr_FastCallKeywords (descrobj=0x7fffdefa4050, args=0x7ffff786f5c0, nargs=2, kwnames=0x0) at /tmp/build/80754af9/python_1585000375785/work/Objects/descrobject.c:288
#5  0x0000555555724add in call_function (kwnames=0x0, oparg=2, pp_stack=<synthetic pointer>) at /tmp/build/80754af9/python_1585000375785/work/Python/ceval.c:4593
#6  _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at /tmp/build/80754af9/python_1585000375785/work/Python/ceval.c:3110
#7  0x0000555555669289 in _PyEval_EvalCodeWithName (_co=0x7ffff78a68a0, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, kwnames=0x0, kwargs=0x0, kwcount=<optimized out>, kwstep=2, 
    defs=0x0, defcount=0, kwdefs=0x0, closure=0x0, name=0x0, qualname=0x0) at /tmp/build/80754af9/python_1585000375785/work/Python/ceval.c:3930
#8  0x000055555566a1c4 in PyEval_EvalCodeEx (_co=<optimized out>, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, kwdefs=0x0, 
    closure=0x0) at /tmp/build/80754af9/python_1585000375785/work/Python/ceval.c:3959
#9  0x000055555566a1ec in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at /tmp/build/80754af9/python_1585000375785/work/Python/ceval.c:524
#10 0x0000555555780cb4 in run_mod (mod=<optimized out>, filename=<optimized out>, globals=0x7ffff78d7c30, locals=0x7ffff78d7c30, flags=<optimized out>, arena=<optimized out>)
    at /tmp/build/80754af9/python_1585000375785/work/Python/pythonrun.c:1035
#11 0x000055555578b0d1 in PyRun_FileExFlags (fp=0x5555558c24d0, filename_str=<optimized out>, start=<optimized out>, globals=0x7ffff78d7c30, locals=0x7ffff78d7c30, closeit=1, flags=0x7fffffffe1b0)
    at /tmp/build/80754af9/python_1585000375785/work/Python/pythonrun.c:988
#12 0x000055555578b2c3 in PyRun_SimpleFileExFlags (fp=0x5555558c24d0, filename=<optimized out>, closeit=1, flags=0x7fffffffe1b0) at /tmp/build/80754af9/python_1585000375785/work/Python/pythonrun.c:429
#13 0x000055555578c3f5 in pymain_run_file (p_cf=0x7fffffffe1b0, filename=0x5555558e51f0 L""repro.py"", fp=0x5555558c24d0) at /tmp/build/80754af9/python_1585000375785/work/Modules/main.c:462
#14 pymain_run_filename (cf=0x7fffffffe1b0, pymain=0x7fffffffe2c0) at /tmp/build/80754af9/python_1585000375785/work/Modules/main.c:1641
#15 pymain_run_python (pymain=0x7fffffffe2c0) at /tmp/build/80754af9/python_1585000375785/work/Modules/main.c:2902
#16 pymain_main (pymain=0x7fffffffe2c0) at /tmp/build/80754af9/python_1585000375785/work/Modules/main.c:3442
#17 0x000055555578c51c in _Py_UnixMain (argc=<optimized out>, argv=<optimized out>) at /tmp/build/80754af9/python_1585000375785/work/Modules/main.c:3477
#18 0x00007ffff7dcd002 in __libc_start_main () from /usr/lib/libc.so.6
#19 0x000055555572fac0 in _start () at ../sysdeps/x86_64/elf/start.S:103
{noformat}",pull-request-available,['Python'],ARROW,Bug,Major,2020-05-22 12:16:06,0
13306711,[Python] Heuristic in dataframe_to_arrays that decides to multithread convert cause slow conversions,"When calling pa.Table.from_pandas() the code path that uses the ThreadPoolExecutor in dataframe_to_arrays (called by Table.from_pandas) the conversion is much much slower.


 I have a simple example - but the time difference is much worse with a real table.


{code:java}
Python 3.7.3 | packaged by conda-forge | (default, Dec 6 2019, 08:54:18)
 Type 'copyright', 'credits' or 'license' for more information
 IPython 7.13.0  An enhanced Interactive Python. Type '?' for help.
In [1]: import pyarrow as pa
In [2]: import pandas as pd
In [3]: df = pd.DataFrame({""A"": [0] * 10000000})
In [4]: %timeit table = pa.Table.from_pandas(df)
 577 s  15.3 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
In [5]: %timeit table = pa.Table.from_pandas(df, nthreads=1)
 106 s  1.65 s per loop (mean  std. dev. of 7 runs, 10000 loops each)
{code}
",pull-request-available,['Python'],ARROW,Bug,Minor,2020-05-22 08:19:07,14
13306606,[R] Don't include everything everywhere,I noticed that we were jamming all of our arrow #includes in one header file in the R bindings and then including that everywhere. Seemed like that was wasteful and probably causing compilation to be slower.,pull-request-available,['R'],ARROW,Improvement,Major,2020-05-21 21:51:29,4
13306549,[Rust] [Integration Testing] Enable passing tests and update spec doc,"Some of the integration test failures can be avoided by disabling unsupported tests, like large lists and nested types",pull-request-available,"['Integration', 'Rust']",ARROW,Sub-task,Major,2020-05-21 17:26:35,12
13306541,[Rust] Add large list and binary support,Rust does not yet support large lists and large binary arrays.,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-21 16:53:25,12
13306343,[R] try_download is confused when download.file.method isn't default,"Hello there and thanks again for this beautiful package!

I am trying to install {{arrow}} on linux and I got a few problematic warnings during the install. My computer is behind a firewall so not all the connections coming from rstudio are allowed.


{code:java}
> sessionInfo()
R version 3.6.1 (2019-07-05)
Platform: x86_64-ubuntu18-linux-gnu (64-bit)
Running under: Ubuntu 18.04.4 LTS
Matrix products: default
BLAS/LAPACK: /apps/intel/2019.1/compilers_and_libraries_2019.1.144/linux/mkl/lib/intel64_lin/libmkl_gf_lp64.so
locale:
 [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 
 [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 
 [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C 
[10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
attached base packages:
[1] stats graphics grDevices utils datasets methods base
other attached packages:
[1] MKLthreads_0.1
loaded via a namespace (and not attached):
[1] compiler_3.6.1 tools_3.6.1
{code}


after running {{install.packages(""arrow"")}} I get


{code:java}

installing *source* package ?arrow? ...
** package ?arrow? successfully unpacked and MD5 sums checked
** using staged installation
*** Successfully retrieved C++ source
*** Proceeding without C++ dependencies
Warning message:
In unzip(tf1, exdir = src_dir) : error 1 in extracting from zip file
./configure: line 132: cd: libarrow/arrow-0.17.1/lib: No such file or directory
------------------------- NOTE ---------------------------
After installation, please run arrow::install_arrow()
for help installing required runtime libraries
---------------------------------------------------------
{code}




However, the installation ends normally.


{code:java}
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (arrow)
{code}


So I go ahead and try to run arrow::install_arrow() and get a similar warning.


{code:java}
installing *source* package ?arrow? ...
** package ?arrow? successfully unpacked and MD5 sums checked
** using staged installation
*** Successfully retrieved C++ binaries for ubuntu-18.04
Warning messages:
1: In file(file, ""rt"") :
 URL 'https://raw.githubusercontent.com/ursa-labs/arrow-r-nightly/master/linux/distro-map.csv': status was 'Couldn't connect to server'
2: In unzip(bin_file, exdir = dst_dir) :
 error 1 in extracting from zip file
./configure: line 132: cd: libarrow/arrow-0.17.1/lib: No such file or directory
------------------------- NOTE ---------------------------
After installation, please run arrow::install_arrow()
for help installing required runtime libraries
{code}
And unfortunately I cannot read any parquet file.
{noformat}
Error in fetch(key) : lazy-load database '/mydata/R/x86_64-ubuntu18-linux-gnu-library/3.6/arrow/help/arrow.rdb' is corrupt{noformat}


Could you please tell me how to fix this? Can I just copy the zip from github and do a manual install in Rstudio?



Thanks!



",pull-request-available,['R'],ARROW,Bug,Major,2020-05-21 01:40:33,4
13306128,[CI] Travis-CI jobs fail (can't open file 'ci/detect-changes.py'),"See example here:
https://travis-ci.org/github/apache/arrow/builds/689169003

Excerpt:
{code}
$ eval ""$(python ci/detect-changes.py)""

python: can't open file 'ci/detect-changes.py': [Errno 2] No such file or directory
{code}
",pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2020-05-20 09:42:50,2
13306124,[C++] Gandiva build failure,"Looks like there was an undetected conflict between Gandiva changes to use the Arrow parsing internals, and the refactor of said parsing internals.",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Bug,Blocker,2020-05-20 09:21:22,2
13306034,[Rust] [DataFusion] Type Coercion optimizer rule does not support new scan nodes,Type Coercion optimizer rule does not support new scan nodes,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-05-20 01:49:09,10
13306018,[R] Support converting POSIXlt type,"{code:r}
f <- as.POSIXlt(Sys.time() + 1:5)
Array$create(f)
# Error in Array__from_vector(x, type) : 
#   Unknown: List vector expecting elements vector of type double but got int32
{code}

Issue #1: POSIXlt type is a struct, essentially. But because it is not a data.frame, we don't try to convert it to a struct. (We should probably convert named lists to structs and not list type in general.)

If I trick the converter into thinking it is a data.frame, it will convert to struct successfully.

{code:r}
class(f) <- c(class(f), ""data.frame"")
Array$create(f)
# StructArray
# <struct<sec: double, min: int32, hour: int32, mday: int32, mon: int32, year: int32, wday: int32, yday: int32, isdst: int32, zone: string, gmtoff: int32>>
# ...
{code}

Issue #2: round trip won't work because the attributes that tell you that this struct is a POSIXlt type, what time zone it is, etc., are dropped. This would be helped by storing those attributes as custom_metadata on the Table. (We could also implement it as an extension type, but if it's just for going back and forth between R, would that have any benefit?)",pull-request-available,['R'],ARROW,New Feature,Major,2020-05-19 22:09:24,4
13306017,[C++] Split Type::UNION into Type::SPARSE_UNION and Type::DENSE_UNION,"Similar to the recent {{Type::INTERVAL}} split, having these two array types which have different memory layouts under the same {{Type::type}} value makes function dispatch somewhat more complicated. This issue is less critical from INTERVAL so this may not be urgent but seems like a good pre-1.0 change",pull-request-available,['C++'],ARROW,Improvement,Blocker,2020-05-19 22:06:37,6
13306005,[R] Add methods to Table/RecordBatch for consistency with data.frame,Some methods identified in the Feather package test suite,pull-request-available,['R'],ARROW,New Feature,Major,2020-05-19 20:54:02,4
13306001,[C++] Array subclass constructors must set ArrayData::null_count to 0 when there is no validity bitmap,Many type-specific array constructors incorrectly set the null count to unknown. It would be better to set it to 0,pull-request-available,['C++'],ARROW,Bug,Major,2020-05-19 20:29:34,14
13305954,[C++] IPC/Feather decompression broken for nested arrays,"When writing a table with a Struct typed column, this is read back with garbage values when using compression (which is the default):

{code:python}
>>>  table = pa.table({'col': pa.StructArray.from_arrays([[0, 1, 2], [1, 2, 3]], names=[""f1"", ""f2""])})

# roundtrip through feather
>>> feather.write_feather(table, ""test_struct.feather"")
>>> table2 = feather.read_table(""test_struct.feather"")

>>> table2.column(""col"")
<pyarrow.lib.ChunkedArray object at 0x7f0b0c4d7728>
[
  -- is_valid: all not null
  -- child 0 type: int64
    [
      24,
      1261641627085906436,
      1369095386551025664
    ]
  -- child 1 type: int64
    [
      24,
      1405756815161762308,
      281479842103296
    ]
]
{code}

When not using compression, it is read back correctly:

{code:python}
>>> feather.write_feather(table, ""test_struct.feather"", compression=""uncompressed"")                                                                                                                           
>>> table2 = feather.read_table(""test_struct.feather"")                                                                                                                                                        

>>> table2.column(""col"")                                                                                                                                                                                      
<pyarrow.lib.ChunkedArray object at 0x7f0b0e466778>
[
  -- is_valid: all not null
  -- child 0 type: int64
    [
      0,
      1,
      2
    ]
  -- child 1 type: int64
    [
      1,
      2,
      3
    ]
]
{code}
",pull-request-available,['C++'],ARROW,Bug,Critical,2020-05-19 15:38:15,5
13305909,[FlightRPC] Ensure headers are uniformly exposed,"* Java: MetadataAdapter should support iterating through binary headers
* Python: binary headers need to be present in the output",pull-request-available,"['FlightRPC', 'Java', 'Python']",ARROW,Bug,Major,2020-05-19 12:01:38,0
13305804,[CI] MinGW builds break on system upgrade,"See e.g. https://github.com/apache/arrow/pull/7218/checks?check_run_id=687127263#step:7:69

Started failing sometime today.",pull-request-available,"['Continuous Integration', 'R', 'Ruby']",ARROW,Bug,Critical,2020-05-19 04:05:48,4
13305787,[Rust] [Integration Testing] Return empty batch if MessageHeader is NONE,"{code:java}
Error: IoError(""Reading types other than record batches not yet supported"") {code}",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-19 00:47:20,12
13305786,[Rust] [Integration Testing] data type Date32(Day) not supported,"{code:java}
Error: JsonError(""data type Date32(Day) not supported"") {code}",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-19 00:46:39,12
13305784,[Rust] [Integration Testing] Show output from arrow-json-integration-test,"Show output from arrow-json-integration-test. It is currently hidden, making it hard to debug the test failures.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-19 00:43:21,10
13305738,[CI][C/Glib] MinGW build error,"https://github.com/apache/arrow/pull/7216/checks?check_run_id=686415769#step:6:66
{code}
:: Processing package changes...
upgrading msys2-runtime...
      0 [main] pacman (5056) C:\hostedtoolcache\windows\Ruby\2.6.6\x64\msys64\usr\bin\pacman.exe: *** fatal error - cygheap base mismatch detected - 0x180330408/0x180317408.
This problem is probably due to using incompatible versions of the cygwin DLL.
Search for cygwin1.dll using the Windows Start->Find/Search facility
and delete all but the most recent version.  The most recent version *should*
reside in x:\cygwin\bin, where 'x' is the drive on which you have
installed the cygwin distribution.  Rebooting is also suggested if you
are unable to find another cygwin DLL.
      0 [main] pacman 246 dofork: child -1 - forked process 5056 died unexpectedly, retry 0, exit code 0xC0000142, errno 11
error: could not open file /var/cache/pacman/pkg/msys2-runtime-3.1.4-1-x86_64.pkg.tar.zst: Can't initialize filter; unable to run program ""zstd -d -qq""
error: could not commit transaction
error: failed to commit transaction (transaction aborted)
Errors occurred, no packages were upgraded.
MSYS2 could not be found. Please run 'ridk install'
or download and install MSYS2 manually from https://msys2.github.io/
{code}",pull-request-available,"['C', 'Continuous Integration', 'GLib']",ARROW,Bug,Blocker,2020-05-18 18:39:12,1
13305715,[C++] Pass task size / metrics in Executor API,"For now, our ThreadPool implementation would ignore those metrics, but other implementations may use it for custom ordering.

Example metrics:
* IO size (number of bytes)
* CPU cost (~ number of instructions)
* Priority (opaque integer? lower is more urgent)

",pull-request-available,['C++'],ARROW,Wish,Major,2020-05-18 16:28:42,2
13305546,[Rust] Add Null type,"{code:java}
thread 'main' panicked at 'not implemented: Type Null not supported', arrow/src/ipc/convert.rs:316:14
 {code}",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-17 22:03:39,12
13305499,[Rust] Implement arrow-file-to-stream for integration testing,Implement arrow-file-to-stream for integration testing,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-17 14:05:33,10
13305498,[Rust] Implement VALIDATE mode in integration test binary,"The binary arrow-json-integration-test.rs already supports converting arrow files to json, and json to arrow.

We now need to implement the VALIDATE command, which should read an Arrow and a JSON file and ensure that they contain the same batches.

See the Java implementation at java/tools/src/main/java/org/apache/arrow/tools/Integration.java for an example.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-17 14:04:53,10
13305496,[Python] AttributeError: module 'pyarrow.fs' has no attribute 'S3FileSystem',"Documentation declares that pyarrow can work in this way:
{code:python}
from pyarrow import fs
s3 = fs.S3FileSystem(region='eu-west-3')
{code}
Follow link to documentation: [https://arrow.apache.org/docs/python/filesystems.html#example]



After installing pyarrow with this command:
{noformat}
pipenv install pyarrow==0.17.0
{noformat}
And when trying to run a simple python application: [https://github.com/gumartinm/MLForFun/tree/master/playground|https://github.com/gumartinm/MLForFun/tree/cd1f2a200c74a00adfe03dd325c5b6bcaeaed99b/playground]

Application ends up with the following error:
{noformat}
AttributeError: module 'pyarrow.fs' has no attribute 'S3FileSystem'{noformat}


This line is failing[https://github.com/apache/arrow/blob/apache-arrow-0.17.0/python/pyarrow/fs.py#L42]because*pyarrow.**_s3fs* does not exist.









",pull-request-available,['Python'],ARROW,Bug,Major,2020-05-17 13:59:15,5
13305406,[Crossbow] remote URL should always have .git,"In ARROW-7803, I edited the crossbow templates for the homebrew jobs to substitute in the correct fork of arrow and append the current git SHA so that the code under test corresponds to the requested git commit. Unfortunately, this caused the nightly builds to fail. 

Comparing a successful on-demand run (https://github.com/ursa-labs/crossbow/blob/actions-266-travis-homebrew-r-autobrew/.travis.yml) with a nightly run (https://github.com/ursa-labs/crossbow/blob/nightly-2020-05-16-0-travis-homebrew-cpp/.travis.yml), it appears that the default ""remote"" URL that crossbow uses when not on a fork/PR does not contain the "".git"" suffix. And I suspect that Homebrew requires that in order to identify the source as a git repo in order to clone it correctly.",pull-request-available,"['Continuous Integration', 'Developer Tools']",ARROW,Bug,Major,2020-05-16 17:27:01,4
13305386,[Rust] [DataFusion] Add MemoryScan variant to LogicalPlan,Allow queries against Vec<RecordBatch>,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-05-16 14:16:58,10
13305180,[Dev][Release] Binary upload script should retry on unexpected bintray request error,During uploading the binaries to bintray the script exited multiple times because of unhandled HTTP errors. ,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-05-15 11:23:01,3
13305179,[Dev][Release] Binary upload script keeps raising locale warnings,The console output is filled with warnings which makes hard to follow what happens.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-05-15 11:19:38,3
13305076,[Rust] schema mismatch in integration test,"I apologize for the vagueness here, will flesh out details when I learn more but it looks like Rust is specifying an int64 as a 32 bit type somewhere.
{code:java}

diff schema1.txt schema2.txt 
15c15
<  int64_nullable: Int(32,
---
>  int64_nullable: Int(64,
17c17
<  int64_nonnullable: Int(32,
---
>  int64_nonnullable: Int(64,
 {code}",pull-request-available,['Rust'],ARROW,Bug,Major,2020-05-14 23:49:03,10
13305075,[Rust] Divide by zero in arrays/builder.rs,"Integration testing exposed a bug in cases where values_data.len() is zero. This fails with divide by zero error.
{code:java}
// check that values_data length is multiple of len
assert!(
    values_data.len() / len == self.list_len as usize,
    ""Values of FixedSizeList must have equal lengths, values have length {} and list has {}"",
    values_data.len(),
    len
); {code}",pull-request-available,['Rust'],ARROW,Bug,Major,2020-05-14 23:38:10,10
13304977,[C++][Dataset] Schema metadata are lost when reading a subset of columns,"Python example:

{code}
import pandas as pd     
import pyarrow.dataset as ds                                                                                                                                                                              

df = pd.DataFrame({'a': [1, 2, 3]})  
df.to_parquet(""test_metadata.parquet"")  

dataset = ds.dataset(""test_metadata.parquet"")                                                                                                                                                             
{code}

gives:
{code}
>>> dataset.to_table().schema 
a: int64
  -- field metadata --
  PARQUET:field_id: '1'
-- schema metadata --
pandas: '{""index_columns"": [{""kind"": ""range"", ""name"": null, ""start"": 0, ""' + 397
ARROW:schema: '/////4ACAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABAwAQAAAAAAAKAAwAAA' + 806

>>> dataset.to_table(columns=['a']).schema 
a: int64
  -- field metadata --
  PARQUET:field_id: '1'
{code}

So when specifying a subset of the columns, the additional metadata entries are lost (while those can still be informative, eg for conversion to pandas)",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Bug,Major,2020-05-14 15:44:13,13
13304967,[Python] Memory leak on read from parquet file with UTC timestamps using pandas,"Given dump.py script


{code:java}
import pandas as pd
import numpy as np


x = pd.to_datetime(np.random.randint(0, 2**32, size=2**20), unit='ms', utc=True)
pd.DataFrame({'x': x}).to_parquet('data.parquet', engine='pyarrow', compression=None)
{code}
and load.py script


{code:java}
import sys
import pandas as pd

def foo(engine):
  for _ in range(2**9):
    pd.read_parquet('data.parquet', engine=engine)
  print('Done')
  input()

foo(sys.argv[1])
{code}
running first ""python dump.py"" and then ""pythonload.py pyarrow"", on my machine python memory usage stays at 4+ GB while it waits for input. If using ""python load.py fastparquet"" instead, it is about 100 MB, so it should be a pyarrow issue instead of a pandas issue. The leak disappears if ""utc=True"" is removed from dump.py, in which case the timestamp is timezone-unaware.



",pull-request-available,['Python'],ARROW,Bug,Blocker,2020-05-14 14:53:21,14
13304959,[C++] Split arrow::ChunkedArray into arrow/chunked_array.h,"There are plenty of scenarios where ChunkedArray is used separate from Table, it would probably make sense to split up the headers, implementation, and unit tests",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-14 14:32:10,14
13304946,[C++][Dataset] Reading list column as nested dictionary segfaults,"Python example:

{code}
import pyarrow as pa
import pyarrow.parquet as pq  
from pyarrow.tests import util                                                                                                                                                                             
                                                                                                                                                                      
repeats = 10 
nunique = 5 

data = [ 
    [[util.rands(10)] for i in range(nunique)] * repeats, 
] 
table = pa.table(data, names=['f0'])                                                                                                                                                                   

pq.write_table(table, ""test_dictionary.parquet"")
{code}

Reading with the parquet code works:

{code}
>>> pq.read_table(""test_dictionary.parquet"", read_dictionary=['f0.list.item'])                                                                                                                                  
pyarrow.Table
f0: list<item: dictionary<values=string, indices=int32, ordered=0>>
  child 0, item: dictionary<values=string, indices=int32, ordered=0>
{code}

but doing the same with the datasets API segfaults:

{code}
>>> fmt = ds.ParquetFileFormat(read_options=dict(dictionary_columns=[""f0.list.item""]))
>>> dataset = ds.dataset(""test_dictionary.parquet"", format=fmt)                                                                       
>>> dataset.to_table()      
Segmentation fault (core dumped)
{code}",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-05-14 13:47:24,6
13304812,[C++] BitUtil::SetBitsTo probably doesn't need to be inline,Inlining this function probably does not yield meaningful performance benefits,pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-14 02:03:58,14
13304783,"[C++] Improved declarative compute function / kernel development framework, normalize calling conventions","I'm working on a significant revamp of the way that kernels are implemented in the project as discussed on the mailing list. PR to follow within the next week or sooner

A brief list of features:

* Kernel selection that takes into account the shape of inputs (whether Scalar or Array, so you can provide an implementation just for Arrays and a separate one just for Scalars if you want)
* More customizable / less monolithic type-to-kernel dispatch
* Standardized C++ function signature for kernel implementations (rather than every one being a little bit special)
* Multiple implementations of the same function can coexist (e.g. with / without SIMD optimizations) so that you can choose the one you want at runtime
* Browsable function registry (see all available kernels and their input type signatures)
* Central code path for type-checking and argument validation
* Central code path for kernel execution on ChunkedArray inputs

There's a lot of JIRAs in the backlog that will follow from this work so I will attach those to this issue for visibility but this issue will cover the initial refactoring work to port the existing code to the new framework without altering existing features.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-13 22:53:45,14
13304690,[Rust] Add separate crate for integration test binaries,Add separate crate for integration test binaries,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-05-13 14:34:53,10
13304675,[Packaging][rpm] Use bundled zstd in the CentOS 8 build,"There was an update in the epel-release repository in the last 12 hours, since then yum/dnf is unable to find libzstd-devel.

",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-05-13 13:38:37,3
13304672,[Python][Packaging] Build the windows wheels with MIMALLOC enabled,"Alread set the flag, but there is a typo in it ARROW_MIMA""ll""OC",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-05-13 13:35:10,2
13304665,[Rust] [DataFusion] Remove use of Arc from LogicalPlan,"The reason that the LogicalPlan currently uses Arc rather than Box is to support the ability to pass the logical plan between threads.

I am no longer sure that this is a requirement, and if it is, then it would perhaps be better to serialize the plan to JSON using the serde crate.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-05-13 13:17:59,10
13304657,[Rust] [DataFusion] Logical plan should have ParquetScan and CsvScan entries,"The LogicalPlan currently has a TableScan entry which references a Table (any logical plan registered with an ExecutionContext) and is often backed by a Parquet or CSV data source.

I am finding it increasingly inconvenient that we can't just create a logical plan referencing a Parquet or CSV file, without having to create an execution context first and register the data sources with it.

This addition will not remove any existing behavior.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-05-13 13:05:26,10
13304650,[Rust] [DataFusion] Add benchmarks based on NYC Taxi data set,"I plan on adding a new benchmarks folder beneatch the datafusion crate, containing benchmarks based on the NYC Taxi data set. The benchmark will be a CLI and will support running a number of different queries against CSV and Parquet.

The README will contain instructions for downloading the data set.

The benchmark will produce CSV files containing results.

These benchmarks will allow us to manually verify performance before major releases and on an ongoing basis as we make changes to Arrow/Parquet/DataFusion.

I will be basing this on existing benchmarks I recently built in Ballista [1] (I am the only contributor to these benchmarks so far).

A dockerfile will be provided, making it easy to restrict CPU and RAM when running these benchmarks.

[1] https://github.com/ballista-compute/ballista/tree/master/rust/benchmarks


",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-05-13 13:00:05,10
13304633,[CI][C++] Enable ccache on GHA MinGW jobs,It would be nice to enable caching with ccache on the MinGW Github Actions jobs. They're currently quite slow...,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Wish,Minor,2020-05-13 11:42:49,1
13304397,[FlightRPC][C++] Flight/C++ middleware don't receive headers on failed calls to Java servers,"For a failed call, gRPC/Java may consolidate headers with trailers, so Flight/C++ needs to check both headers and trailers to get any headers that may have been sent.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-05-12 15:55:14,0
13304352,[Python] pyarrow schema.empty_table() does not preserve nullability of fields,"Introduced by PR: [https://github.com/apache/arrow/pull/2589]



When a field in a schema is marked as not-nullable, calling empty_table() on the schema returns a table with nullable fields.



reproduction
{code:java}
>>> import pyarrow as pa
>>> s = pa.schema([pa.field('a', pa.int64(), nullable=False), pa.field('b', pa.int64())])

>>> s

a: int64 not null
b: int64

>>> e = s.empty_table()
>>> e

pyarrow.Table
a: int64
b: int64

>>> e.schema

a: int64
b: int64

>>> assert s == e.schema
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
AssertionError

{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-05-12 12:18:50,3
13304264,[C++] Add boost/process library to build support,"Some of our test source code requires the process.hpp file (and its dependent libraries). Our current build support does not include these files, causing build failures like:

fatal error: boost/process.hpp: No such file or directory",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-12 04:27:27,4
13304247,[C++] Add convenience methods to access fields by name in StructScalar,This would improve usability of this type,pull-request-available,['C++'],ARROW,Improvement,Critical,2020-05-12 01:42:42,6
13304150,[Python] A FileSystem implementation based on Python callbacks,"The new {{pyarrow.fs}} filesystems are now actual C++ objects, and no longer ""just"" a python interface. So they can't easily be expanded from the Python side, and the existing integration with {{fsspec}} filesystems is therefore also not working anymore. 

One possible solution is  to have a C++ filesystem that calls back into a python object for each of its methods (possibly similar to how you can implement a flight server in Python, I suppose). 

Such a FileSystem implementation would allow to make a {{pyarrow.fs}} wrapper for {{fsspec}} filesystems, and thus allow such filesystems to be used in pyarrow where new filesystems are expected.",dataset-dask-integration filesystem pull-request-available,['Python'],ARROW,Sub-task,Major,2020-05-11 15:44:10,2
13304139,[C++] Create RandomAccessFile::WillNeed-like API,"We need to inform RandomAccessFile that we will need a given range or number of ranges.
Also call that method from MemoryMappedFile::Read and friends.

Also perhaps write specialized ReadAsync implementations?",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-11 15:08:44,2
13304129,[C++][Gandiva] Replace Gandiva's BitmapAnd with common implementation,"Now that the arrow/util/bit_util.h implementation has been optimized, we should just use that one",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Improvement,Major,2020-05-11 14:10:03,14
13304096,[C++] Improve the performance of minmax kernel,"We improve the performance of the max-min kernel with the simple idea: if the current value is smaller than the current min value; then there is no need to compare it against the current max value, because it must be smaller than the current max value. 

This simple trick reduces the expected number of comparisons from 2n to 1.5n, which can be notable for large arrays. ",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-05-11 11:51:20,7
13303675,[Python][Packaging] Keep VS2015 with for the windows wheels,The windows wheels needs to be fixed for the release.,pull-request-available,['Packaging'],ARROW,Bug,Major,2020-05-08 15:47:05,3
13303643,[CI] Fix archery option in pandas master cron test,The --no-cache-leaf option was renamed to --no-leaf-cache.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Minor,2020-05-08 13:30:11,3
13303463,[R] improve nightly build installation,"I've tried to install / build from source (with from a git checkout and using the built-in `install_arrow()`) and when compiling I'm getting the following error reliably during the auto brew process:

{code:bash}
 x System command 'R' failed, exit status: 1, stdout + stderr:
E> * checking for file /Users/jkeane/Dropbox/arrow/r/DESCRIPTION ... OK
E> * preparing arrow:
E> * checking DESCRIPTION meta-information ... OK
E> * cleaning src
E> * running cleanup
E> * installing the package to build vignettes
E>       -----------------------------------
E> * installing *source* package arrow ...
E> ** using staged installation
E> *** Generating code with data-raw/codegen.R
E> There were 27 warnings (use warnings() to see them)
E> *** > 375 functions decorated with [[arrow|s3::export]]
E> *** > generated file `src/arrowExports.cpp`
E> *** > generated file `R/arrowExports.R`
E> *** Downloading apache-arrow
E> **** Using local manifest for apache-arrow
E> Thu May  7 13:13:42 CDT 2020: Auto-brewing apache-arrow in /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow...
E> ==> Tapping autobrew/core from https://github.com/autobrew/homebrew-core
E> Tapped 2 commands and 4639 formulae (4,888 files, 12.7MB).
E> lz4
E> openssl
E> thrift
E> snappy
E> ==> Downloading https://homebrew.bintray.com/bottles/lz4-1.8.3.mojave.bottle.tar.gz
E> Already downloaded: /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/downloads/b4158ef68d619dbf78935df6a42a70b8339a65bc8876cbb4446355ccd40fa5de--lz4-1.8.3.mojave.bottle.tar.gz
E> ==> Pouring lz4-1.8.3.mojave.bottle.tar.gz
E> ==> Skipping post_install step for autobrew...
E>   /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/Cellar/lz4/1.8.3: 22 files, 512.7KB
E> ==> Downloading https://homebrew.bintray.com/bottles/openssl-1.0.2p.mojave.bottle.tar.gz
E> Already downloaded: /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/downloads/fbb493745981c8b26c0fab115c76c2a70142bfde9e776c450277e9dfbbba0bb2--openssl-1.0.2p.mojave.bottle.tar.gz
E> ==> Pouring openssl-1.0.2p.mojave.bottle.tar.gz
E> ==> Skipping post_install step for autobrew...
E> ==> Caveats
E> openssl is keg-only, which means it was not symlinked into /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow,
E> because Apple has deprecated use of OpenSSL in favor of its own TLS and crypto libraries.
E> 
E> If you need to have openssl first in your PATH run:
E>   echo 'export PATH=""/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/bin:$PATH""' >> ~/.zshrc
E> 
E> For compilers to find openssl you may need to set:
E>   export LDFLAGS=""-L/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/lib""
E>   export CPPFLAGS=""-I/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/include""
E> 
E> For pkg-config to find openssl you may need to set:
E>   export PKG_CONFIG_PATH=""/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/lib/pkgconfig""
E> 
E> ==> Summary
E>   /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/Cellar/openssl/1.0.2p: 1,793 files, 12MB
E> ==> Downloading https://homebrew.bintray.com/bottles/thrift-0.11.0.mojave.bottle.tar.gz
E> Already downloaded: /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/downloads/7e05ea11a9f7f924dd7f8f36252ec73a24958b7f214f71e3752a355e75e589bd--thrift-0.11.0.mojave.bottle.tar.gz
E> ==> Pouring thrift-0.11.0.mojave.bottle.tar.gz
E> ==> Skipping post_install step for autobrew...
E> ==> Caveats
E> To install Ruby binding:
E>   gem install thrift
E> ==> Summary
E>   /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/Cellar/thrift/0.11.0: 102 files, 7MB
E> ==> Downloading https://homebrew.bintray.com/bottles/snappy-1.1.7_1.mojave.bottle.tar.gz
E> Already downloaded: /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/downloads/1f09938804055499d1dd951b13b26d80c56eae359aa051284bf4f51d109a9f73--snappy-1.1.7_1.mojave.bottle.tar.gz
E> ==> Pouring snappy-1.1.7_1.mojave.bottle.tar.gz
E> ==> Skipping post_install step for autobrew...
E>   /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/Cellar/snappy/1.1.7_1: 18 files, 115.8KB
E> ==> Downloading https://autobrew.github.io/bottles/apache-arrow-0.17.0.el_capitan.bottle.tar.gz
E> Already downloaded: /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/downloads/7dcf2302ba174a5efb32eaa5b8fe0ae874f4a4671f575e126c79a524830054ae--apache-arrow-0.17.0.el_capitan.bottle.tar.gz
E> ==> Pouring apache-arrow-0.17.0.el_capitan.bottle.tar.gz
E> ==> Skipping post_install step for autobrew...
E>   /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/Cellar/apache-arrow/0.17.0: 294 files, 49.7MB
E> ==> Caveats
E> ==> openssl
E> openssl is keg-only, which means it was not symlinked into /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow,
E> because Apple has deprecated use of OpenSSL in favor of its own TLS and crypto libraries.
E> 
E> If you need to have openssl first in your PATH run:
E>   echo 'export PATH=""/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/bin:$PATH""' >> ~/.zshrc
E> 
E> For compilers to find openssl you may need to set:
E>   export LDFLAGS=""-L/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/lib""
E>   export CPPFLAGS=""-I/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/include""
E> 
E> For pkg-config to find openssl you may need to set:
E>   export PKG_CONFIG_PATH=""/private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/build-apache-arrow/opt/openssl/lib/pkgconfig""
E> 
E> ==> thrift
E> To install Ruby binding:
E>   gem install thrift
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewarrow.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewarrow_dataset.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewparquet.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewlz4.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewcrypto.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewssl.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewsnappy.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewthrift.a
E> created /var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib/libbrewthriftz.a
E> PKG_CFLAGS=-I/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/include -DARROW_R_WITH_ARROW
E> PKG_LIBS=-L/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/lib -L/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/lib -lbrewparquet -lbrewarrow_dataset -lbrewarrow -lbrewthrift -lbrewlz4 -lbrewsnappy
E> ** libs
E> clang++ -mmacosx-version-min=10.13 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG -I/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/include -DARROW_R_WITH_ARROW -I'/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include' -I/usr/local/include   -fPIC  -Wall -g -O2  -c array.cpp -o array.o
E> clang++ -mmacosx-version-min=10.13 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG -I/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/include -DARROW_R_WITH_ARROW -I'/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include' -I/usr/local/include   -fPIC  -Wall -g -O2  -c array_from_vector.cpp -o array_from_vector.o
E> clang++ -mmacosx-version-min=10.13 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG -I/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/include -DARROW_R_WITH_ARROW -I'/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include' -I/usr/local/include   -fPIC  -Wall -g -O2  -c array_to_vector.cpp -o array_to_vector.o
E> clang++ -mmacosx-version-min=10.13 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG -I/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/include -DARROW_R_WITH_ARROW -I'/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include' -I/usr/local/include   -fPIC  -Wall -g -O2  -c arraydata.cpp -o arraydata.o
E> clang++ -mmacosx-version-min=10.13 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG -I/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T//build-apache-arrow/opt/apache-arrow/include -DARROW_R_WITH_ARROW -I'/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include' -I/usr/local/include   -fPIC  -Wall -g -O2  -c arrowExports.cpp -o arrowExports.o
E> arrowExports.cpp:1650:21: error: no member named 'CsvFileFormat' in namespace 'arrow::dataset'
E> std::shared_ptr<ds::CsvFileFormat> dataset___CsvFileFormat__Make(const std::shared_ptr<arrow::csv::ParseOptions>& parse_options);
E>                 ~~~~^
E> 1 error generated.
E> make: *** [arrowExports.o] Error 1
E> ERROR: compilation failed for package arrow
E> * removing /private/var/folders/45/n5gfjjtn05j877spnpbnhqqw0000gn/T/RtmpNwJEKk/Rinstd9644c71dcbf/arrow
E>       -----------------------------------
E> ERROR: package installation failed 
{code} 

I'm sure I'm missing some setup step somewhere or I'm missing a necessary step to install from source. I've tried this both under R 3.6(.1) and 4.0 to see if upgrading helped and I get the same error",pull-request-available,['R'],ARROW,Bug,Major,2020-05-07 18:29:34,4
13303427,[C++][Dataset][Python] ParquetFileFragment should provide access to parquet FileMetadata,"Related to ARROW-8062 (as there we will also need a way to expose the global FileMetadata). But independently, it would be useful to get access to the FileMetadata on each {{ParquetFileFragment}} (eg to get access to the statistics).

This would be relatively simple to code on the Python/R side, since we have access to the file path, and could read the metadata from the file backing the fragment, and return this as a FileMetadata object. 

I am wondering if we want to integrate this with ARROW-8062, since when the fragments were created from a {{_metadata}} file, a {{ParquetFileFragment.metadata}} attribute would not need to read it from the parquet file in this case, but from the global metadata (at least for eg the row group data).

Another question: what for a ParquetFileFragment that maps to a single row group?",dataset dataset-dask-integration pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-05-07 15:04:39,6
13303426,[C++] Let Futures support cancellation,"There should be a way for consumers of Futures to notify that they are not interested in the task at hand anymore. For some kinds of tasks this may allow cancelling the task in-flight (e.g. an IO task, or a task consisting of multiple steps).",pull-request-available,['C++'],ARROW,Wish,Major,2020-05-07 14:56:01,2
13303371,[Rust] Use slice instead of &Vec for function arguments,It is best practice to use slice instead of &Vec for function arguments,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-05-07 11:41:40,10
13303340,[C++][Dataset] Only selecting a partition column results in empty table,"Python reproducer:

{code}
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds
path = ""test_dataset""

table = pa.table({'part': ['a', 'a', 'b', 'b'], 'col': [1, 2, 3, 4]})
pq.write_to_dataset(table, str(path), partition_cols=[""part""])
{code}

gives

{code}
In [38]: ds.dataset(str(path), partitioning=""hive"").to_table().num_rows                                                                                                                                            
Out[38]: 4

In [39]: ds.dataset(str(path), partitioning=""hive"").to_table(columns=[""part""]).num_rows                                                                                                                            
Out[39]: 0
{code}

The schema correctly only includes the ""part"" column, but there are no rows.

cc [~bkietz]",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Bug,Major,2020-05-07 10:02:53,6
13303254,[C++] Do not require struct-initialization of StringConverter<T> to parse strings to other types,"I ran into this issue while working on refactoring kernels. {{StringConverter<T>}} must be initialized to be able to support parametric types like Timestamp, but this produces an awkwardness and possibly a performance penalty (I haven't measured yet) in inlined functions. 

In any case, I'm refactoring everything to be static non-stateful",pull-request-available,['C++'],ARROW,Improvement,Major,2020-05-07 02:43:21,14
13303239,[C++][Dataset] Mis-specified DirectoryPartitioning incorrectly uses the file name as value,"Calling filter + collect on a dataset with a mis-specified partitioning causes a segfault. Though this is clearly input error, it would be nice if there was some guidance that something was wrong with the partitioning.

{code:r}
library(arrow)
library(dplyr)

dir.create(""multi_mtcars/one"", recursive = TRUE)
dir.create(""multi_mtcars/two"", recursive = TRUE)
write_parquet(mtcars, ""multi_mtcars/one/mtcars.parquet"")
write_parquet(mtcars, ""multi_mtcars/two/mtcars.parquet"")

ds <- open_dataset(""multi_mtcars"", partitioning = c(""level"", ""nothing""))

# the following will segfault
ds %>%
  filter(cyl > 8) %>% 
  collect()
{code}",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-05-07 00:56:51,13
13303223,"[Dev] ""archery docker run -e"" doesn't work","{noformat}
$ archery docker run --dry-run -e ARROW_GANDIVA=OFF ubuntu-cpp       
docker-compose pull --ignore-pull-failures ubuntu-cpp
docker-compose build ubuntu-cpp
docker-compose run --rm ubuntu-cpp
{noformat}

{{docker-compose run}} doesn't have {{-e ARROW_GANDIVA=OFF}}.",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-05-06 23:09:09,3
13303187,[CI] Fix R build matrix,Follow-up of the docker-compose archery PR.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-05-06 18:27:31,3
13303176,[C++] Fix checked_pointer_cast,"While investigating performance, I noted that dyncast (and rtti internal methods) were showing up in the ""hot"" functions for release builds.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-05-06 17:39:02,13
13303163,[CI][Packaging] Add build dependency on boost to homebrew,"cf. https://github.com/Homebrew/homebrew-core/pull/54287

and revise the Travis jobs to uninstall boost and thrift before checking the formula",pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Improvement,Major,2020-05-06 16:49:37,4
13303000,[Python] Expose strptime timestamp parsing in read_csv conversion options,Follow up to ARROW-8111,pull-request-available,['Python'],ARROW,New Feature,Major,2020-05-06 00:40:04,2
13302950,[CI] Utilize github actions cache for docker-compose volumes,Save and restore the architecure specific docker-compose volumes to speed up the time consuming compilation steps.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-05-05 19:33:18,3
13302940,[CI] Docker push fails because of wrong dockerhub credentials,"Error: https://github.com/apache/arrow/runs/646854493#step:8:12
Introduced in commit https://github.com/apache/arrow/commit/4116516f22b680b70ab32ea28c029cc47e8d8988",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-05-05 18:13:10,3
13302833,[R] schema$metadata should be properly typed,"Currently, I try to export numeric data plus some metadata in Python into to a parquet file and read it in R. However, the metadata seems to be a dict in Python but a string in R. I would have expected a list (which is roughly a dict in Python). Am I missing something? Here is the code to demonstrate the issue:

{{import sys}}
{{import numpy as np}}
{{import pyarrow as pa}}
{{import pyarrow.parquet as pq}}
{{print(sys.version)}}
{{print(pa.__version__)}}
{{x = np.random.randint(0, 10, (10, 3))}}
{{arrays = [pa.array(x[:, i]) for i in range(x.shape[1])]}}
{{table = pa.Table.from_arrays(arrays=arrays, names=['A', 'B', 'C'],}}
{{ metadata=\{'foo': '42'})}}
{{pq.write_table(table, 'array.parquet', compression='snappy')}}
{{table = pq.read_table('array.parquet')}}
{{metadata = table.schema.metadata}}
{{print(metadata)}}
{{print(type(metadata))}}



And in R:



{{library(arrow)}}
{{print(R.version)}}
{{print(packageVersion(""arrow""))}}
{{table <- read_parquet(""array.parquet"", as_data_frame = FALSE)}}
{{metadata <- table$schema$metadata}}
{{print(metadata)}}
{{print(is(metadata))}}
{{print(metadata[""foo""])}}{{}}



Output Python:

{{3.6.8 (default, Aug 7 2019, 17:28:10) }}
{{[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)]}}
{{0.13.0}}
{{OrderedDict([(b'foo', b'42')])}}
{{<class 'collections.OrderedDict'>}}



Output R:

{{[1] 0.17.0}}
{{[1] ""\n-- metadata --\nfoo: 42""}}
{{[1] ""character"" ""vector"" ""data.frameRowLabels""}}
{{[4] ""SuperClassMethod"" }}
{{[1] NA}}

",pull-request-available,['R'],ARROW,Improvement,Major,2020-05-05 09:03:33,4
13302755,[R] Fix automatic r_to_py conversion,See https://github.com/rstudio/reticulate/issues/748,pull-request-available,['R'],ARROW,Improvement,Major,2020-05-04 21:21:47,4
13302683,[Python][Parquet] parquet.read_schema() fails when loading wide table created from Pandas DataFrame,"parquet.read_schema() fails when loading wide table schema created from Pandas DataFrame with 50,000 columns. This works ok using pyarrow 0.16.0.
{code:java}
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
print(pa.__version__)
df = pd.DataFrame(({'c' + str(i): np.random.randn(10) for i in range(50000)}))
table = pa.Table.from_pandas(df)
pq.write_table(table, ""test_wide.parquet"")
schema = pq.read_schema('test_wide.parquet'){code}
Output:

0.17.0
Traceback (most recent call last):
 File ""/GAAL/kisseri/conda_envs/blkmamba-dev/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3319, in run_code
 exec(code_obj, self.user_global_ns, self.user_ns)
 File ""<ipython-input-29-d5ef2df77263>"", line 9, in <module>
 table = pq.read_schema('test_wide.parquet')
 File ""/GAAL/kisseri/conda_envs/blkmamba-dev/lib/python3.6/site-packages/pyarrow/parquet.py"", line 1793, in read_schema
 return ParquetFile(where, memory_map=memory_map).schema.to_arrow_schema()
 File ""/GAAL/kisseri/conda_envs/blkmamba-dev/lib/python3.6/site-packages/pyarrow/parquet.py"", line 210, in __init__
 read_dictionary=read_dictionary, metadata=metadata)
 File ""pyarrow/_parquet.pyx"", line 1023, in pyarrow._parquet.ParquetReader.open
 File ""pyarrow/error.pxi"", line 100, in pyarrow.lib.check_status
OSError: Couldn't deserialize thrift: TProtocolException: Exceeded size limit

",pull-request-available,"['C++', 'Python']",ARROW,Bug,Critical,2020-05-04 16:45:35,14
13302660,[Python] Dataset.get_fragments is missing an implicit cast when filtering,"This currently segfaults:

{code}
dataset.get_fragments(filter=ds.field(""col"") > 1)
{code}

in case ""col"" is not int64 (like default inferred partition columns are int32)",pull-request-available,['Python'],ARROW,Bug,Major,2020-05-04 15:17:07,5
13302627,[Python] Clean-up dataset+parquet tests now order is determinstic,"Follow-up on ARROW-8447, we should now be able to clean-up some tests.
",pull-request-available,['Python'],ARROW,Improvement,Major,2020-05-04 12:24:26,5
13302621,[C++] S3 benchmarks fail linking,"{code}
FAILED: release/arrow-filesystem-s3fs-benchmark 
: && /usr/bin/ccache /usr/bin/g++-7  -Wno-noexcept-type  -fdiagnostics-color=always -fuse-ld=gold -O3 -DNDEBUG  -Wall -mavx2  -D_GLIBCXX_USE_CXX11_ABI=1 -D_GLIBCXX_USE_CXX11_ABI=1 -fno-omit-frame-pointer -g -O3 -DNDEBUG  -rdynamic src/arrow/filesystem/CMakeFiles/arrow-filesystem-s3fs-benchmark.dir/s3fs_benchmark.cc.o  -o release/arrow-filesystem-s3fs-benchmark  -Wl,-rpath,/home/antoine/arrow/dev/cpp/build-release/release:/home/antoine/miniconda3/envs/pyarrow/lib  gbenchmark_ep/src/gbenchmark_ep-install/lib/libbenchmark_main.a  gbenchmark_ep/src/gbenchmark_ep-install/lib/libbenchmark.a  release/libarrow_testing.so.18.0.0  /home/antoine/miniconda3/envs/pyarrow/lib/libcrypto.so  /home/antoine/miniconda3/envs/pyarrow/lib/libssl.so  /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlienc-static.a  /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlidec-static.a  /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlicommon-static.a  -ldl  /home/antoine/miniconda3/envs/pyarrow/lib/libgtest_main.so  /home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so  /home/antoine/miniconda3/envs/pyarrow/lib/libgmock.so  -ldl  release/libparquet.so.18.0.0  release/libarrow.so.18.0.0  /home/antoine/miniconda3/envs/pyarrow/lib/libssl.so  /home/antoine/miniconda3/envs/pyarrow/lib/libcrypto.so  /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlienc-static.a  /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlidec-static.a  /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlicommon-static.a  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-config.so  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-transfer.so  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-s3.so  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-core.so  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-event-stream.so.1.0.0  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-common.so.1.0.0  -lm  -lpthread  /home/antoine/miniconda3/envs/pyarrow/lib/libaws-checksums.so  -ldl  jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a  mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.0/libmimalloc-release.a  -pthread  -lrt  -Wl,-rpath-link,/home/antoine/miniconda3/envs/pyarrow/lib && :
/home/antoine/miniconda3/envs/pyarrow/include/boost/filesystem/path.hpp:792: error: undefined reference to 'boost::filesystem::path::operator/=(boost::filesystem::path const&)'
/home/antoine/miniconda3/envs/pyarrow/include/boost/filesystem/operations.hpp:461: error: undefined reference to 'boost::filesystem::detail::status(boost::filesystem::path const&, boost::system::error_code*)'
{code}",pull-request-available,"['Benchmarking', 'C++']",ARROW,Bug,Major,2020-05-04 11:39:20,2
13302553,"[Python] ""SystemError: Bad call flags in _PyMethodDef_RawFastCallDict"" in Python 3.7.7 on macOS when using pyarrow wheel","[~npr] reported this on the 0.17.0 RC0 vote thread but I have confirmed it independently. It was also reported at

https://github.com/apache/arrow/issues/7082

Here are steps to reproduce on macOS:

{code}
conda create -yn py-3.7-defaults python=3.7 -c defaults
conda activate py-3.7-defaults
pip install pyarrow
{code}

Now open the Python interpreter, run {{import pyarrow}}, then exit the interpreter ({{python -c ""import pyarrow""}} didn't trigger it for me):

{code}
$ python
Python 3.7.7 (default, Mar 26 2020, 10:32:53) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyarrow
>>> 
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""pyarrow/types.pxi"", line 2638, in pyarrow.lib._unregister_py_extension_types
SystemError: Bad call flags in _PyMethodDef_RawFastCallDict. METH_OLDARGS is no longer supported!
Segmentation fault: 11
{code}

It fails with Python 3.7.6 when using {{-c conda-forge}} also, so it is not particular to defaults.

Frustratingly, the problem doesn't exist in Python 3.7.4 but occurs for me with 3.7.5, 3.7.6, and 3.7.7. ",pull-request-available,['Python'],ARROW,Bug,Blocker,2020-05-04 01:25:30,2
13302465,[C++] Use IPC body compression metadata approved in ARROW-300 ,"This will adapt the existing code to use the new metadata, while maintaining backward compatibility code to recognize the ""experimental"" metadata written in 0.17.0",pull-request-available,['C++'],ARROW,Sub-task,Blocker,2020-05-02 23:02:35,14
13302319,[CI] Consolidate appveyor scripts,The appveyor scripts are a bit outdated and contain unreasonable amount of indirections.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-05-01 14:50:18,3
13302245,[C++][Gandiva] Reduce dependence on Boost,Remove Boost usages aside from Boost.Multiprecision,pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Improvement,Major,2020-05-01 01:49:54,14
13302187,[C++][Dataset] Implement subtree pruning for FileSystemDataset::GetFragments,"This is a very handy optimization for large datasets with multiple partition fields. For example, given a hive-style directory {{$base_dir/a=3/}} and a filter {{""a""_ == 2}} none of its files or subdirectories need be examined.

After ARROW-8318 FileSystemDataset stores only files so subtree pruning (whose implementation depended on the presence of directories to represent subtrees) was disabled. It should be possible to reintroduce this without reference to directories by examining partition expressions directly and extracting a tree structure from their subexpressions.",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-30 19:02:33,6
13302156,[Python][C++][Parquet] Forward compatibility issue from 0.16 to 0.17 when using version='2.0',"With the recent release of 0.17, the ParquetVersion is used to define the logical type interpretation of fields and the selection of the DataPage format.

As a result all parquet files that were created with ParquetVersion::V2 to get features such as unsigned int32s, timestamps with nanosecond resolution, etc are not forward compatible (cannot be read with 0.16.0). That's TBs of data in my case.

Those two concerns should be separated. Given that that DataPageV2 pages were not written prior to 0.17 and in order to allow reading existing files, the existing version property should continue to operate as in 0.16 and inform the logical type mapping.

Some consideration should be given to issue a release 0.17.1.

",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-04-30 16:59:53,15
13302151,[Python] Switch to VS2017 in the windows wheel builds,Since the recent conda-forge compiler migrations the wheel builds are failing https://mail.google.com/mail/u/0/#label/ARROW/FMfcgxwHNCsqSGKQRMZxGlWWsfmGpKdC,pull-request-available,['Python'],ARROW,Improvement,Major,2020-04-30 16:39:04,3
13302140,[C++][Dataset][Python][R] Preserve partitioning information for a discovered Dataset,"Currently, we have the {{HivePartitioning}} and {{DirectoryPartitioning}} classes that describe a partitioning used in the discovery phase. But once a dataset object is created, it doesn't know any more about this, it just has partition expressions for the fragments. And the partition keys are added to the schema, but you can't directly know which columns of the schema originated from the partitions.

However, there can be use cases where it would be useful that a dataset still ""knows"" from what kind of partitioning it was created:

- The ""read CSV write back Parquet"" use case, where the CSV was already partitioned and you want to automatically preserve that partitioning for parquet (kind of roundtripping the partitioning on read/write)
- To convert the dataset to other representation, eg conversion to pandas, it can be useful to know what columns were partition columns (eg for pandas, those columns might be good candidates to be set as the index of the pandas/dask DataFrame). I can imagine conversions to other systems can use similar information.
",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-30 16:09:29,5
13302133,[Python][Dataset] Support pickling of Dataset objects,"We alraedy made several parts of a Dataset serializable (the formats, the expressions, the filesystem). With those, it should also be possible to pickle FileFragments, and with that also Dataset.",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2020-04-30 15:36:37,3
13302122,[Rust] [Website] Add documentation to Arrow website,"The documentation page [1] on the Arrow site has links for C, C++, Java, Python, JavaScript, and R. It would be good do add Rust here as well, even if the docs here are brief and link to the rustdocs on docs.rs[2] (which are currently broken due to ARROW-8536 [3].



[1] [https://arrow.apache.org/docs/]

[2] https://docs.rs/crate/arrow/0.17.0

[3] https://issues.apache.org/jira/browse/ARROW-8536",pull-request-available,"['Rust', 'Website']",ARROW,Improvement,Major,2020-04-30 14:51:16,4
13302121,[Java] [Website] Java documentation on website is hidden,"There is some excellent Java documentation on the web site that is hard to find because the Java documentation link [1] goes straight to the generated javadocs.



[1] https://arrow.apache.org/docs/java",pull-request-available,"['Java', 'Website']",ARROW,Bug,Major,2020-04-30 14:47:09,4
13302094,[C++][Dataset] Optionally encode partition field values as dictionary type,"In the Python ParquetDataset implementation, the partition fields are returned as dictionary type columns. 

In the new Dataset API, we now use a plain type (integer or string when inferred). But, you can already manually specify that the partition keys should be dictionary type by specifying the partitioning schema (in {{Partitioning}} passed to the dataset factory). 

Since using dictionary type can be more efficient (since partition keys will typically be repeated values in the resulting table), it might be good to still have an option in the DatasetFactory to use dictionary types for the partition fields.

See also https://github.com/apache/arrow/pull/6303#discussion_r400622340",dataset dataset-dask-integration pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-30 13:35:46,6
13302037,[C++] Missing gflags dependency for plasma,"The documentation build fails because gflags is not installed and CMake doesn't build the bundled version of it.

Introduced by https://github.com/apache/arrow/commit/dfc14ef24ed54ff757c10a26663a629ce5e8cebf",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-30 09:04:15,3
13302033,[Python] Dask integration tests failing due to change in not including partition columns,"In ARROW-3861 (https://github.com/apache/arrow/pull/7050), I ""fixed"" a bug that the partition columns are always included even when the user did a manual column selection.

But apparently, this behaviour was being relied upon by dask. See the failing nightly integration tests: https://circleci.com/gh/ursa-labs/crossbow/11854?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

So the best option might be to just keep the ""old"" behaviour for the legacy ParquetDataset, when using the new datasets API ({{use_legacy_datasets=False}}), you get the new / correct behaviour.",pull-request-available,['Python'],ARROW,Bug,Major,2020-04-30 08:57:40,5
13302029,[Python] Tests with pandas master failing due to freq assertion ,"Nightly pandas master tests are failing, eg https://circleci.com/gh/ursa-labs/crossbow/11858?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

This is caused by a change in pandas, see https://github.com/pandas-dev/pandas/pull/33815#issuecomment-620820134",pull-request-available,['Python'],ARROW,Test,Major,2020-04-30 08:51:56,5
13301943,[Java] Create an example,"The Java implementation doesn't seem to have any documentation or examples on how to get started with basic operations such as creating an array. Javadocs exist but how do new users even know which class to look for?

I would like to create an examples module and one simple example as a starting point. I hope to have a PR soon.",pull-request-available,['Java'],ARROW,New Feature,Major,2020-04-29 23:23:23,10
13301904,[C++] Fix conversion error warning in array_union_test.cc,"
https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/32500007/job/c4f2kqcsm04gjd8u#L1074
",pull-request-available,['C++'],ARROW,Bug,Minor,2020-04-29 19:38:57,6
13301902,[C++][Dataset] Add ConvertOptions and ReadOptions to CsvFileFormat,"https://github.com/apache/arrow/pull/7033 does not add ConvertOptions (including alternate spellings for null/true/false, etc) or ReadOptions (block_size, column name customization, etc). These will be helpful but will require some discussion to find the optimal way to integrate them with dataset::",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-29 19:33:50,0
13301900,[C++][Dataset] Pass schema including all materialized fields to catch CSV edge cases,"see discussion here https://github.com/apache/arrow/pull/7033#discussion_r416941674

Fields filtered but not projected will revert to their inferred type, whatever their dataset's schema may be. This can cause validated filters to fail due to type disagreements",dataset,['C++'],ARROW,Improvement,Major,2020-04-29 19:29:49,6
13301865,[CI][Dev] Wrap docker-compose commands with archery,Build the image hierarchy automatically.,pull-request-available,"['Continuous Integration', 'Developer Tools']",ARROW,Improvement,Major,2020-04-29 17:10:14,3
13301664,[C++] Use distinct Type::type values for interval types,"This is a breaking API change, but {{MonthIntervalType}} and {{DayTimeIntervalType}} are different data types (and have different value sizes, which is not true of timestamps) and thus should be distinguished in the same way that DATE32 / DATE64 are distinguished, or TIME32 / TIME64 are distinguished

This may nominally simplify function / kernel dispatch",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-28 22:56:24,14
13301622,[C++] ASSIGN_OR_RAISE should move its argument,"Since {{ASSIGN_OR_RAISE}} consumes its {{Result}} argument anyway, there's no reason not to cast that argument to an rvalue reference whenever possible. This will decrease boilerplate when handling non-temporary {{Result}}s, for example when yielding from an iterator:

{code}
 for (auto maybe_batch : scan_task->Execute()) {
-  ASSIGN_OR_RAISE(auto batch, std::move(maybe_batch));
+  ASSIGN_OR_RAISE(auto batch, maybe_batch);
 }
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-28 17:29:31,6
13301590,[R] Error better and insist on RandomAccessFile in read_feather,"The use of read_feather function with CompressedInputStream fails with the error:


{code:java}
Error in ipc___feather___Reader__Open(file) : 
 Invalid: File is too small to be a well-formed file
{code}
The minimal code to get it:
{code:java}
filename <- ""x.gz""
data <- data.frame(c(1,2), c(3,4), stringsAsFactors=FALSE)
fos <- FileOutputStream$create(filename)
codec <- Codec$create(""gzip"")
cos <- CompressedOutputStream$create(fos, codec)
write_feather(data, cos)
cos$close()
fos <- ReadableFile$create(filename)
codec <- Codec$create(""gzip"")
cos <- CompressedInputStream$create(fos, codec)
read_feather(cos)
{code}
However, I can extract the file ""x.gz"" with
{code:java}
gzip -d x.gz
{code}
and then run in R:
{code:java}
fos <- ReadableFile$create(""x"")
read_feather(fos);
  c.1..2. c.3..4.
1       1       3
2       2       4
{code}
The codec gzip is available according to:
{code:java}
codec_is_available(""gzip"")
[1] TRUE{code}
Attached the generated ""x.gz""

",pull-request-available,['R'],ARROW,Bug,Major,2020-04-28 15:27:24,4
13301521,[C++][Dataset] Raise error for unparsable partition value,"Currently, when specifying a partitioning schema, but on of the partition field values cannot be parsed according to the specified type, you silently get null values for that partition field.

Python example:
{code:python}
import pathlib              
import pyarrow.parquet as pq 
import pyarrow.datasets as d

path = pathlib.Path(""."") / ""dataset_partition_schema_errors"" 
path.mkdir(exist_ok=True)                                                                                                                                                                                  

table = pa.table({""part"": [""1_2"", ""1_2"", ""3_4"", ""3_4""], ""values"": range(4)})   
pq.write_to_dataset(table, str(path), partition_cols=[""part""]) 
{code}
{code:java}
In [17]: ds.dataset(path, partitioning=""hive"").to_table().to_pandas() 
Out[17]: 
   values part
0       0  1_2
1       1  1_2
2       2  3_4
3       3  3_4

In [18]: partitioning = ds.partitioning(pa.schema([(""part"", pa.int64())]), flavor=""hive"")                                                                                                                          

In [19]: ds.dataset(path, partitioning=partitioning).to_table().to_pandas()   
Out[19]: 
   values  part
0       0   NaN
1       1   NaN
2       2   NaN
3       3   NaN
{code}

Silently ignoring such a parse error doesn't seem the best default to me (since partition keys are quite essential). I think raising an error might be better? ",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-04-28 10:40:08,6
13301377,[R][CI] Unbreak builds following R 4.0 release,Just a tourniquet to get master passing again while I work on ARROW-8604.,pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2020-04-27 19:56:15,4
13301283,[R][CI] Update CI to use R 4.0,"[Master|[https://github.com/apache/arrow/runs/622393526]] fails to compile. The C++ cmake build is not using the same [compiler|[https://github.com/apache/arrow/runs/622393526#step:8:807]] than the R extension [compiler|[https://github.com/apache/arrow/runs/622393526#step:11:141]].


{code:java}
// Files installed here
  adding: arrow-0.17.0.9000/lib-4.9.3/i386/libarrow.a (deflated 85%)
  adding: arrow-0.17.0.9000/lib-4.9.3/i386/libarrow_dataset.a (deflated 82%)
  adding: arrow-0.17.0.9000/lib-4.9.3/i386/libparquet.a (deflated 84%)
  adding: arrow-0.17.0.9000/lib-4.9.3/i386/libsnappy.a (deflated 61%)
  adding: arrow-0.17.0.9000/lib-4.9.3/i386/libthrift.a (deflated 81%)

// Linker is using `-L`
C:/Rtools/mingw_32/bin/g++ -shared -s -static-libgcc -o arrow.dll tmp.def array.o array_from_vector.o array_to_vector.o arraydata.o arrowExports.o buffer.o chunkedarray.o compression.o compute.o csv.o dataset.o datatype.o expression.o feather.o field.o filesystem.o io.o json.o memorypool.o message.o parquet.o py-to-r.o recordbatch.o recordbatchreader.o recordbatchwriter.o schema.o symbols.o table.o threadpool.o -L../windows/arrow-0.17.0.9000/lib-8.3.0/i386 -L../windows/arrow-0.17.0.9000/lib/i386 -lparquet -larrow_dataset -larrow -lthrift -lsnappy -lz -lzstd -llz4 -lcrypto -lcrypt32 -lws2_32 -LC:/R/bin/i386 -lR
C:/Rtools/mingw_32/bin/../lib/gcc/i686-w64-mingw32/4.9.3/../../../../i686-w64-mingw32/bin/ld.exe: cannot find -lparquet
C:/Rtools/mingw_32/bin/../lib/gcc/i686-w64-mingw32/4.9.3/../../../../i686-w64-mingw32/bin/ld.exe: cannot find -larrow_dataset
C:/Rtools/mingw_32/bin/../lib/gcc/i686-w64-mingw32/4.9.3/../../../../i686-w64-mingw32/bin/ld.exe: cannot find -larrow
C:/Rtools/mingw_32/bin/../lib/gcc/i686-w64-mingw32/4.9.3/../../../../i686-w64-mingw32/bin/ld.exe: cannot find -lthrift
C:/Rtools/mingw_32/bin/../lib/gcc/i686-w64-mingw32/4.9.3/../../../../i686-w64-mingw32/bin/ld.exe: cannot find -lsnappy
{code}

C++ developers, rejoice, this is almost the end of gcc-4.9.

",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2020-04-27 13:50:37,4
13301276,[Documentation] Fix Sphinx doxygen comment,See[https://github.com/apache/arrow/runs/622393532],pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Trivial,2020-04-27 13:38:58,13
13300922,[C++] Docs still list LLVM 7 as compiler used,should be LLVM 8,pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Major,2020-04-25 04:12:08,15
13300834,[C++] Compilation error when linking arrow-flight-perf-server,"I wanted to play around with Flight benchmark after seeing the discussion regarding Flight's throughput in arrow dev mailing list today.

I met the following error when trying to build the benchmark from latest source code:
{code:java}
[ 95%] Linking CXX executable ../../../debug/arrow-flight-perf-server
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::detail::canonical(boost::filesystem::path const&, boost::filesystem::path const&, boost::system::error_code*)'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::system::system_category()'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::path::parent_path() const'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `deflate'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `deflateEnd'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::system::generic_category()'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::detail::current_path(boost::system::error_code*)'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `inflateInit2_'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `inflate'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `deflateInit2_'
../../../debug/libarrow_flight.so.18.0.0: undefined reference to `inflateEnd'
../../../debug/libarrow_flight_testing.so.18.0.0: undefined reference to `boost::filesystem::path::operator/=(boost::filesystem::path const&)'
collect2: error: ld returned 1 exit status
src/arrow/flight/CMakeFiles/arrow-flight-perf-server.dir/build.make:154: recipe for target 'debug/arrow-flight-perf-server' failed
make[2]: *** [debug/arrow-flight-perf-server] Error 1
CMakeFiles/Makefile2:2609: recipe for target 'src/arrow/flight/CMakeFiles/arrow-flight-perf-server.dir/all' failed
make[1]: *** [src/arrow/flight/CMakeFiles/arrow-flight-perf-server.dir/all] Error 2
Makefile:140: recipe for target 'all' failed
make: *** [all] Error 2

{code}
I was using {{cmake .. -DCMAKE_BUILD_TYPE=Debug -DARROW_DEPENDENCY_SOURCE=AUTO -DARROW_FLIGHT=ON -DARROW_BUILD_BENCHMARKS=ON -DARROW_CXXFLAGS=""-lboost_filesystem -lboost_system""}} to configure the build.
 I noticed that there was a {{ARROW_BOOST_BUILD_VERSION: 1.71.0}} in the output, but the Boost library that I installed from the package manger was of this version: {{1.65.1.0ubuntu1}}. Could this be the cause of the problem?

PS:
I was able to build the benchmark [before|https://issues.apache.org/jira/browse/ARROW-7200]. It was on AWS with the OS being ubuntu-bionic-18.04-amd64-server-20191002, which should be very similar to the one I'm using on my laptop.",pull-request-available,"['Benchmarking', 'C++', 'FlightRPC']",ARROW,Bug,Minor,2020-04-24 16:25:15,2
13300796,[R] installation failure on CentOS 7,"Hi,

I am trying to install arrow via RStudio, but it seems like it is not working that after I installed the package, it kept asking me to run arrow::install_arrow() even after I did:

{code}
> install.packages(""arrow"")
Installing package into /home/hc/R/x86_64-redhat-linux-gnu-library/3.6
(as lib is unspecified)
trying URL 'https://cran.rstudio.com/src/contrib/arrow_0.17.0.tar.gz'
Content type 'application/x-gzip' length 242534 bytes (236 KB)
==================================================
downloaded 236 KB

* installing *source* package arrow ...
** package arrow successfully unpacked and MD5 sums checked
** using staged installation
*** Successfully retrieved C++ source
*** Building C++ libraries
**** cmake
**** arrow  
./configure: line 132: cd: libarrow/arrow-0.17.0/lib: Not a directory
------------------------- NOTE ---------------------------
After installation, please run arrow::install_arrow()
for help installing required runtime libraries
---------------------------------------------------------
** libs
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c array.cpp -o array.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c array_from_vector.cpp -o array_from_vector.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c array_to_vector.cpp -o array_to_vector.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c arraydata.cpp -o arraydata.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c arrowExports.cpp -o arrowExports.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c buffer.cpp -o buffer.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c chunkedarray.cpp -o chunkedarray.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c compression.cpp -o compression.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c compute.cpp -o compute.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c csv.cpp -o csv.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c dataset.cpp -o dataset.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c datatype.cpp -o datatype.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c expression.cpp -o expression.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c feather.cpp -o feather.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c field.cpp -o field.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c filesystem.cpp -o filesystem.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c io.cpp -o io.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c json.cpp -o json.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c memorypool.cpp -o memorypool.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c message.cpp -o message.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c parquet.cpp -o parquet.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c py-to-r.cpp -o py-to-r.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c recordbatch.cpp -o recordbatch.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c recordbatchreader.cpp -o recordbatchreader.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c recordbatchwriter.cpp -o recordbatchwriter.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c schema.cpp -o schema.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c symbols.cpp -o symbols.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c table.cpp -o table.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c threadpool.cpp -o threadpool.o
g++ -m64 -std=gnu++11 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o arrow.so array.o array_from_vector.o array_to_vector.o arraydata.o arrowExports.o buffer.o chunkedarray.o compression.o compute.o csv.o dataset.o datatype.o expression.o feather.o field.o filesystem.o io.o json.o memorypool.o message.o parquet.o py-to-r.o recordbatch.o recordbatchreader.o recordbatchwriter.o schema.o symbols.o table.o threadpool.o -L/usr/lib64/R/lib -lR
installing to /home/hc/R/x86_64-redhat-linux-gnu-library/3.6/00LOCK-arrow/00new/arrow/libs
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
  converting help for package arrow
    finding HTML links ... done
    ArrayData                               html  
    ChunkedArray                            html  
    Codec                                   html  
    CsvReadOptions                          html  
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/CsvReadOptions.Rd:61: file link read.csv in package utils does not exist and so has been treated as a topic
    CsvTableReader                          html  
    DataType                                html  
    Dataset                                 html  
    DictionaryType                          html  
    Expression                              html  
    FeatherReader                           html  
    Field                                   html  
    FileFormat                              html  
    FileInfo                                html  
    FileSelector                            html  
    FileSystem                              html  
    FixedWidthType                          html  
    InputStream                             html  
    MemoryPool                              html  
    Message                                 html  
    MessageReader                           html  
    OutputStream                            html  
    ParquetFileReader                       html  
    ParquetFileWriter                       html  
    ParquetReaderProperties                 html  
    ParquetWriterProperties                 html  
    Partitioning                            html  
    RecordBatch                             html  
    RecordBatchReader                       html  
    RecordBatchWriter                       html  
    Scanner                                 html  
    Schema                                  html  
    Table                                   html  
    array                                   html  
    arrow-package                           html  
    arrow_available                         html  
    buffer                                  html  
    cast_options                            html  
    codec_is_available                      html  
    compression                             html  
    cpu_count                               html  
    data-type                               html  
    dataset_factory                         html  
    default_memory_pool                     html  
    dictionary                              html  
    enums                                   html  
    hive_partition                          html  
    install_arrow                           html  
    install_pyarrow                         html  
    make_readable_file                      html  
    map_batches                             html  
    mmap_create                             html  
    mmap_open                               html  
    open_dataset                            html  
    read_delim_arrow                        html  
    read_feather                            html  
    read_ipc_stream                         html  
    read_json_arrow                         html  
    read_message                            html  
    read_parquet                            html  
    read_record_batch                       html  
    read_schema                             html  
    reexports                               html  
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:24: file link print.integer64 in package bit64 does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:24: file link str.integer64 in package bit64 does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link contains in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link ends_with in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link everything in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link last_col in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link matches in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link num_range in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmpEMszQs/R.INSTALL6254154cd713/arrow/man/reexports.Rd:26: file link starts_with in package tidyselect does not exist and so has been treated as a topic
    type                                    html  
    unify_schemas                           html  
    write_feather                           html  
    write_ipc_stream                        html  
    write_parquet                           html  
    write_to_raw                            html  
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (arrow)

The downloaded source packages are in
	/tmp/Rtmpo9otzy/downloaded_packages
> library(arrow)

Attaching package: arrow

The following object is masked from package:utils:

    timestamp

> df <- read_parquet('/home/hc/my.10.level.20200331.2book.parquet')
Error in io___MemoryMappedFile__Open(path, mode) : 
  Cannot call io___MemoryMappedFile__Open(). Please use arrow::install_arrow() to install required runtime libraries. 
> arrow::install_arrow()
Installing package into /home/hc/R/x86_64-redhat-linux-gnu-library/3.6
(as lib is unspecified)
trying URL 'https://cran.rstudio.com/src/contrib/arrow_0.17.0.tar.gz'
Content type 'application/x-gzip' length 242534 bytes (236 KB)
==================================================
downloaded 236 KB

* installing *source* package arrow ...
** package arrow successfully unpacked and MD5 sums checked
** using staged installation
*** No C++ binaries found for centos-7.7.1908
*** Successfully retrieved C++ source
*** Building C++ libraries
**** cmake
**** arrow  
./configure: line 132: cd: libarrow/arrow-0.17.0/lib: Not a directory
------------------------- NOTE ---------------------------
After installation, please run arrow::install_arrow()
for help installing required runtime libraries
---------------------------------------------------------
** libs
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c array.cpp -o array.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c array_from_vector.cpp -o array_from_vector.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c array_to_vector.cpp -o array_to_vector.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c arraydata.cpp -o arraydata.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c arrowExports.cpp -o arrowExports.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c buffer.cpp -o buffer.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c chunkedarray.cpp -o chunkedarray.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c compression.cpp -o compression.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c compute.cpp -o compute.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c csv.cpp -o csv.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c dataset.cpp -o dataset.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c datatype.cpp -o datatype.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c expression.cpp -o expression.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c feather.cpp -o feather.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c field.cpp -o field.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c filesystem.cpp -o filesystem.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c io.cpp -o io.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c json.cpp -o json.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c memorypool.cpp -o memorypool.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c message.cpp -o message.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c parquet.cpp -o parquet.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c py-to-r.cpp -o py-to-r.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c recordbatch.cpp -o recordbatch.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c recordbatchreader.cpp -o recordbatchreader.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c recordbatchwriter.cpp -o recordbatchwriter.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c schema.cpp -o schema.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c symbols.cpp -o symbols.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c table.cpp -o table.o
g++ -m64 -std=gnu++11 -I""/usr/include/R"" -DNDEBUG  -I""/home/hc/R/x86_64-redhat-linux-gnu-library/3.6/Rcpp/include"" -I/usr/local/include  -fpic  -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic  -c threadpool.cpp -o threadpool.o
g++ -m64 -std=gnu++11 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o arrow.so array.o array_from_vector.o array_to_vector.o arraydata.o arrowExports.o buffer.o chunkedarray.o compression.o compute.o csv.o dataset.o datatype.o expression.o feather.o field.o filesystem.o io.o json.o memorypool.o message.o parquet.o py-to-r.o recordbatch.o recordbatchreader.o recordbatchwriter.o schema.o symbols.o table.o threadpool.o -L/usr/lib64/R/lib -lR
installing to /home/hc/R/x86_64-redhat-linux-gnu-library/3.6/00LOCK-arrow/00new/arrow/libs
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
  converting help for package arrow
    finding HTML links ... done
    ArrayData                               html  
    ChunkedArray                            html  
    Codec                                   html  
    CsvReadOptions                          html  
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/CsvReadOptions.Rd:61: file link read.csv in package utils does not exist and so has been treated as a topic
    CsvTableReader                          html  
    DataType                                html  
    Dataset                                 html  
    DictionaryType                          html  
    Expression                              html  
    FeatherReader                           html  
    Field                                   html  
    FileFormat                              html  
    FileInfo                                html  
    FileSelector                            html  
    FileSystem                              html  
    FixedWidthType                          html  
    InputStream                             html  
    MemoryPool                              html  
    Message                                 html  
    MessageReader                           html  
    OutputStream                            html  
    ParquetFileReader                       html  
    ParquetFileWriter                       html  
    ParquetReaderProperties                 html  
    ParquetWriterProperties                 html  
    Partitioning                            html  
    RecordBatch                             html  
    RecordBatchReader                       html  
    RecordBatchWriter                       html  
    Scanner                                 html  
    Schema                                  html  
    Table                                   html  
    array                                   html  
    arrow-package                           html  
    arrow_available                         html  
    buffer                                  html  
    cast_options                            html  
    codec_is_available                      html  
    compression                             html  
    cpu_count                               html  
    data-type                               html  
    dataset_factory                         html  
    default_memory_pool                     html  
    dictionary                              html  
    enums                                   html  
    hive_partition                          html  
    install_arrow                           html  
    install_pyarrow                         html  
    make_readable_file                      html  
    map_batches                             html  
    mmap_create                             html  
    mmap_open                               html  
    open_dataset                            html  
    read_delim_arrow                        html  
    read_feather                            html  
    read_ipc_stream                         html  
    read_json_arrow                         html  
    read_message                            html  
    read_parquet                            html  
    read_record_batch                       html  
    read_schema                             html  
    reexports                               html  
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:24: file link print.integer64 in package bit64 does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:24: file link str.integer64 in package bit64 does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link contains in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link ends_with in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link everything in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link last_col in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link matches in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link num_range in package tidyselect does not exist and so has been treated as a topic
Rd warning: /tmp/RtmppYi3Tc/R.INSTALL7ba654278045/arrow/man/reexports.Rd:26: file link starts_with in package tidyselect does not exist and so has been treated as a topic
    type                                    html  
    unify_schemas                           html  
    write_feather                           html  
    write_ipc_stream                        html  
    write_parquet                           html  
    write_to_raw                            html  
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (arrow)

The downloaded source packages are in
	/tmp/Rtmpo9otzy/downloaded_packages
Please restart R to use the 'arrow' package.

Restarting R session...

> library(arrow)

Attaching package: arrow

The following object is masked from package:utils:

    timestamp

> read_parquet('/home/hc/my.10.level.20200331.1.book.parquet')
Error in io___MemoryMappedFile__Open(path, mode) : 
  Cannot call io___MemoryMappedFile__Open(). Please use arrow::install_arrow() to install required runtime libraries. 
> library(arrow)
> df <- read_parquet('/home/hc/my.10.level.20200331.1.book.parquet')
Error in io___MemoryMappedFile__Open(path, mode) : 
  Cannot call io___MemoryMappedFile__Open(). Please use arrow::install_arrow() to install required runtime libraries. 
{code}",pull-request-available,['R'],ARROW,Bug,Major,2020-04-24 12:21:32,4
13300772,[Packaging][C++] Protobuf link error in deb builds,"See build log 
Stretch: https://github.com/ursa-labs/crossbow/runs/614358553
Focal: https://github.com/ursa-labs/crossbow/runs/614358637

cc @kou",pull-request-available,"['C++', 'Packaging']",ARROW,Bug,Major,2020-04-24 10:23:39,1
13300770,[C++][Doc] Undocumented parameter in Dataset namespace,"See build log: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-04-24-0-circle-test-ubuntu-18.04-docs

We should build the doxygen docs on each commit, preferably in the conda-cpp build.",pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Major,2020-04-24 10:17:51,3
13300694,[Plasma] PlasmaClient::Connect() of CUDA enabled build is always failed on no CUDA device machine,"We provide CUDA enabled Plasma packages for Debian GNU/Linux and Ubuntu.
It must work on no CUDA device machine.

The original report:

---

Hi all,

Previously, I was using c_glib Plasma library (build 0.12) for creating plasma objects. It was working as expected. But now I want to use Arrow's newest build. I incurred the following error:


/build/apache-arrow-0.17.0/cpp/src/arrow/result.cc:28: ValueOrDie called on an error: IOError: Cuda error 100 in function 'cuInit': [CUDA_ERROR_NO_DEVICE] no CUDA-capable device is detected
I think plasma client options (gplasma_client_options_new()) which I am using with default settings are enabling a check for my CUDA device and I have no CUDA device attached to my system. How I can disable this check? Any help will be highly appreciated. Thanks",pull-request-available,"['C++ - Plasma', 'GPU']",ARROW,Bug,Major,2020-04-24 02:42:38,1
13300625,[Rust] Upgrade to Rust 1.44 nightly,"Rust 1.43.0 was just released, so we should update to 1.44 nightly.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-04-23 19:30:11,10
13300623,[Python] Expose UnionArray.array and other fields,"Currently in Python, you can construct a UnionArray easily, but getting the data back out (without copying) is near-impossible. We should expose the getter for UnionArray.array so we can pull out the constituent arrays. We should also expose fields like mode while we're at it.

The use case is: in Flight, we'd like to write multiple distinct datasets (with distinct schemas) in a single logical call; using UnionArrays lets us combine these datasets into a single logical dataset.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-04-23 19:19:59,0
13300608,[C++]Switch AppVeyor image to VS 2017,"conda-forge did the switch, so we should follow this.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-23 17:14:25,8
13300583,[CI] Upgrade xcode version for testing homebrew formulae,To prevent as many bottles from being built from source.,pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Improvement,Minor,2020-04-23 15:10:48,4
13300565,[C++][Python] Crash on decimal cast in debug mode,"{code:python}
>>> arr = pa.array([Decimal('123.45')])                                                                                                                                   
>>> arr                                                                                                                                                                   
<pyarrow.lib.Decimal128Array object at 0x7efbbddd4210>
[
  123.45
]
>>> arr.type                                                                                                                                                              
Decimal128Type(decimal(5, 2))
>>> arr.cast(pa.decimal128(4, 2))                                                                                                                                         
../src/arrow/util/basic_decimal.cc:626:  Check failed: (original_scale) != (new_scale) 
{code}
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-04-23 13:38:42,2
13300357,[C++][Gandiva] Stop using deprecated google::protobuf::MessageLite::ByteSize(),"It's deprecated since Protobuf 3.4.0.

https://github.com/protocolbuffers/protobuf/blob/v3.4.0/CHANGES.txt#L58-L59

{quote}
  * ByteSize() and SpaceUsed() are deprecated.Use ByteSizeLong() and
    SpaceUsedLong() instead
{quote}",pull-request-available,['C++ - Gandiva'],ARROW,Improvement,Minor,2020-04-22 21:24:14,1
13300319,[R] zstd symbol not found if there are multiple installations of zstd,"I would like to install the `arrow` R package on my Ubuntu 19.10 system. Prebuilt binaries are unavailable, and I want to enable compression, so I set the {{LIBARROW_MINIMAL=false}} environment variable. When I do so, it looks like the package is able to compile, but can't be loaded. I'm able to install correctly if I don't set the {{LIBARROW_MINIMAL}} variable.

Here's the error I get:
{code:java}
** testing if installed package can be loaded from temporary location
Error: package or namespace load failed for arrow in dyn.load(file, DLLpath = DLLpath, ...):
 unable to load shared object '~/.R/3.6/00LOCK-arrow/00new/arrow/libs/arrow.so':
  ~/.R/3.6/00LOCK-arrow/00new/arrow/libs/arrow.so: undefined symbol: ZSTD_initCStream
Error: loading failed
Execution halted
ERROR: loading failed
* removing ~/.R/3.6/arrow
{code}
",pull-request-available,['R'],ARROW,Bug,Major,2020-04-22 16:46:02,4
13300286,[FlightRPC][Java] Implement Flight DoExchange for Java,As described in the mailing list vote.,pull-request-available,"['FlightRPC', 'Java']",ARROW,New Feature,Major,2020-04-22 14:31:42,0
13300107,[CI] Don't run cron GHA jobs on forks,"It's wasteful, and I'm tired of seeing them clogging up my Actions tab and notifications. ",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-21 22:59:23,4
13299985,[Release] Fix checksum url in the website post release script,The issue was captured here https://github.com/apache/arrow-site/pull/53#discussion_r411728907,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-04-21 12:23:41,3
13299971,[Release] Don't remove previous source releases automatically,We should keep at least the last three source tarballs.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-04-21 11:35:33,3
13299966,[C++] Create memory allocation benchmark,To judge the overhead of repeated allocations and deallocations (e.g. for temporary computation results).,pull-request-available,['C++'],ARROW,Wish,Minor,2020-04-21 10:59:53,2
13299848,[Rust] Failed to locate format/Flight.proto in any parent directory,"When using Arrow 0.17.0 as a dependency, it is likely that you will get the error ""Failed to locate format/Flight.proto in any parent directory"". This is caused by the custom build script in the arrow-flight crate, which expects to find a ""format/Flight.proto"" file in a parent directory. This works when building the crate from within the Arrow source tree, but unfortunately doesn't work for the published crate, since the Flight.proto file was not published as part of the crate.

The workaround is to create a ""format"" directory in the root of your file system (or at least at a higher level than where cargo is building code) and place the Flight.proto file there (making sure to use the 0.17.0 version, which can be found in the source release [1]).

[1] [https://github.com/apache/arrow/releases/tag/apache-arrow-0.17.0]

",pull-request-available,['Rust'],ARROW,Bug,Critical,2020-04-21 02:18:38,12
13299844,[Rust] Arrow crate does not specify arrow-flight version,"Arrow Cargo.toml has:
{code:java}
arrow-flight = { path = ""../arrow-flight"", optional = true } {code}
It should be:
{code:java}
arrow-flight = { path = ""../arrow-flight"", optional = true, version = ""1.0.0-SNAPSHOT"" } {code}
Also need to update release scripts to replace this version.

",pull-request-available,['Rust'],ARROW,Bug,Critical,2020-04-21 01:48:37,12
13299678,[C++] Fix usage of NextCounts() in GetBatchWithDict[Spaced],See discussion in ARROW-8486,pull-request-available,['C++'],ARROW,Bug,Major,2020-04-20 12:42:27,2
13299589,[CI] Free up space on github actions,"We run into out of space errors in multiple builds. Github actions ship with a lot of preinstalled software which are essentially not required to run the docker-compose based builds, so try to clean up some space.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-20 07:06:05,3
13299550,[Developer] Add environment variable option to toggle whether ephemeral NodeJS is installed in release verification script,"Currently the script fails if the user does not have a new enough NodeJS and associated utilities (npm/npx) on their system. 

The code to install NodeJS exists already but it was disabled in https://github.com/apache/arrow/commit/e570db9c45ca97f77c5633e5525c02f55dbb6c4b#diff-8cc7fa3ae5de30b356c17d7a4b59fe09 because it caused problems when run in GitHub Actions",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-04-20 00:06:32,14
13299548,"[Developer] Group Sub-task, Task, Test, and Wish issue types as ""Improvement"" in Changelog","In my opinion this makes the changelog more readable. The ""Wish"" type in particular might cause people to not see improvements",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-04-19 23:47:57,3
13299541,[Developer] Use .asf.yaml to direct GitHub notifications to e-mail lists and JIRA,Context: INFRA-20149,pull-request-available,['Developer Tools'],ARROW,New Feature,Major,2020-04-19 22:34:23,14
13299535,"[C++][Packaging][deb][RPM] Linux package builds are failed by ""No space left on device"" on GitHub Actions","For example: https://github.com/ursa-labs/crossbow/runs/597759127

{noformat}
/usr/bin/ranlib: release/libarrow.a: No space left on device
{noformat}",pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2020-04-19 22:09:46,1
13299447,[C++] Bitmap ToString should have an option of grouping by bytes,ToString for anything longer than really sort bitmaps can be unreadable. Grouping by bytes makes it more useful.,pull-request-available,['C++'],ARROW,New Feature,Major,2020-04-19 07:09:08,15
13299404,[C++] Delete unused compute expr prototype code,"Most of the code added in https://github.com/apache/arrow/commit/08ca13f83f3d6dbd818c4280d619dae306aa9de5 can be deleted. I may leave some of the ""shape"" types in case we can make use of those. ",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-18 20:53:03,14
13299402,[Developer][Release] Windows release verification script does not halt if C++ compilation fails ,This made finding the issue in ARROW-8510 more difficult,pull-request-available,['C++'],ARROW,Bug,Major,2020-04-18 20:21:31,14
13299401,"[C++] arrow/dataset/file_base.cc fails to compile with internal compiler error with ""Visual Studio 15 2017 Win64"" generator","I discovered this while running the release verification on Windows. There was an obscuring issue which is that if the build fails, the verification script continues. I will fix that",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Blocker,2020-04-18 20:13:37,14
13299379,[GLib] Add low level record batch read/write functions,"Hi All,

I am working on integrating two programs, both of which are using Plasma API. For this purpose, I need to convert RecordBatches to Buffer to transfer to Plasma.

I have created GArrowRecordBatch <-> GArrowBuffer conversion functions which are working for me locally, but I am not sure if I have adopted the correct way, I want it to be integrated into c_glib. Can you people please check these functions and update/accept the pull request?



https://github.com/apache/arrow/pull/6963",pull-request-available,['GLib'],ARROW,New Feature,Major,2020-04-18 16:14:28,1
13299274,[C++] Add Run Length Reader,"For nullability data, in many cases nulls are not evenly distributed. In these cases it would be beneficial to have a mechanism to understand when runs of set/unset bits are encountered. One example of this is writing translating a bitmap to parquet definition levels .



An implementation path could be to add this as method on Bitmap that makes an adaptor callback for VisitWords but I think at least for parquet an iterator API might be more appropriate (something that is easily stoppable/resumable).





",pull-request-available,['C++'],ARROW,New Feature,Major,2020-04-18 03:32:32,15
13299231,[Packaging][RPM] Upgrade devtoolset to 8 on CentOS 6,"It seems that devtoolset-6 is removed:

https://github.com/ursa-labs/crossbow/runs/594096124#step:4:3570

{noformat}
No package devtoolset-6 available.
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-04-17 21:29:37,1
13299169,"[C++] Use selection vectors in Filter implementation for record batches, tables","The current implementation of {{Filter}} on RecordBatch, Table does redundant analysis of the filter array. It would be more efficient in most cases (i.e. whenever there are multiple columns) to convert the boolean array into a selection vector and then use {{Take}}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-17 16:25:36,14
13298953,[C++] Implement basic array-by-array  reassembly logic,"This logic would attempt to create the data necessary for each field by passing through the levels once for each field. it is expected that due to once we can put SIMD/bitmap code in place this will perform better for nested data with shallow nesting, but due to repetitive computation might perform worse for deep nested that include List-types. The SIMD/bitmap enhancements are covered in:ARROW-9985



At a high level the logic would be structured as:

{{for each field:}}

{{ for each rep/def level entry:}}

{{     update null bitmask and offsets.}}",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-04-17 04:13:13,15
13298951,[C++] Create unified schema resolution code for Array reconstruction.,Augment SchemaField in SchemaManifest to track repeated ancestor definition level.,pull-request-available,['C++'],ARROW,Sub-task,Major,2020-04-17 04:07:44,15
13298886,[R] Replace VALUE_OR_STOP with ValueOrStop,We should avoid macro as much as possible as per style guide.,pull-request-available,['R'],ARROW,Improvement,Major,2020-04-16 18:33:35,13
13298879,[FlightRPC][C++] Make it possible to target a specific payload size,"gRPC by default limits message sizes on the wire. While Flight in turn disables these by default, they're still useful to be able to control memory consumption. A well-behaved client/server may wish to split up writes to respect these limits. However, right now, there's no way to measure the memory usage of what you're about to write without serializing it.

With ARROW-5377, we can in theory avoid this by having the writer take control of serialization, producing the IpcPayload, then measuring the size and writing the payload if the size is as desired. However, Flight doesn't provide such a low-level mechanism yet - we'd need to open that up as well.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2020-04-16 18:03:30,0
13298767,[Java] Provide an allocation manager based on Unsafe API,"This is in response to the discussion in https://github.com/apache/arrow/pull/6323#issuecomment-614195070

In this issue, we provide an allocation manager that is capable of allocation large (> 2GB) buffers. In addition, it does not depend on the netty library, which is aligning with the general trend of removing netty dependencies. In the future, we are going to make it the default allocation manager. ",pull-request-available,['Java'],ARROW,New Feature,Major,2020-04-16 10:21:13,7
13298595,[C++][Integration] Regression to /u?int64/ as JSON::number,"In moving datagen.py under archery, the fix for ARROW-6310 was clobbered out resulting in representing 64 bit integers as numbers in integration JSON.",pull-request-available,"['C++', 'Integration']",ARROW,Bug,Major,2020-04-15 15:15:42,6
13298543,[Dev] Fix nightly docker tests on azure,"Need to remove pushd/popd from the azure template, see build log https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-04-15-0-azure-test-conda-python-3.7",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-04-15 11:48:50,3
13298540,[Document] Fix the incorrect null bits description,The desription about the null bits in arrays.rst is incorrect.,pull-request-available,['Documentation'],ARROW,Bug,Trivial,2020-04-15 11:45:55,7
13298424,[Packaging] The python unittests are not running in the windows wheel builds,Appveyors log swallows why those tests are not running. Requires investigation.,pull-request-available,['Packaging'],ARROW,Bug,Major,2020-04-15 00:58:20,3
13298421,[Packaging][Python] Windows py35 wheel build fails because of boost,See build log https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-04-14-3-appveyor-wheel-win-cp35m,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-04-15 00:13:40,3
13298408,"[CI] Balance the nightly test builds between CircleCI, Azure and Github","Most of our nightly docker builds are running on circleci and its queuing, so try to offload some of the builds.",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2020-04-14 22:12:14,3
13298390,[Packaging][deb] Ubuntu Focal build is failed,It seems that this is no disk space error.,pull-request-available,['Packaging'],ARROW,Bug,Major,2020-04-14 20:40:01,1
13298384,[Dev][Archery] Use a more recent cmake-format,"Reading through the cmake-format releases page it seems to contain improvements.

Additionally we should check cmake-format's version in run-cmake-format.py to have unified behaviour both locally and on the CI.",pull-request-available,['Developer Tools'],ARROW,Task,Trivial,2020-04-14 19:52:39,3
13298378,[C++] Prefer the original mirrors for the bundled thirdparty dependencies,Currently the ursa-labs mirrors have the highest priority but we should consider the original mirrors more reliable.,pull-request-available,"['C++', 'Packaging']",ARROW,Task,Major,2020-04-14 19:20:54,3
13298311,[Package] Can't build apt packages with ubuntu-focal,"While trying to debug the failing nightly (due to disk space), I encounter the following error, the tar generated by the build script does not conform to what debuilder expects. It blocks
{code}
Unable to find source-code formatter for language: shell. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlSuccessfully built ecdda7ea015d
Successfully tagged apache-arrow-ubuntu-focal:latest
docker run --rm --tty --volume /home/fsaintjacques/src/db/arrow/dev/tasks/linux-packages/apache-arrow/apt:/host:rw --env DEBUG=yes apache-arrow-ubuntu-focal /host/build.sh
This package has a Debian revision number but there does not seem to be
an appropriate original tar file or .orig directory in the parent directory;
(expected one of apache-arrow_0.16.0.orig.tar.gz, apache-arrow_0.16.0.orig.tar.bz2,
apache-arrow_0.16.0.orig.tar.lzma,  apache-arrow_0.16.0.orig.tar.xz or apache-arrow-1.0.0~dev20200414.orig)
continue anyway? (y/n) 

{code}",pull-request-available,['Packaging'],ARROW,Bug,Major,2020-04-14 15:13:14,13
13298280,[C++][Dataset] Ensure Scanner::ToTable preserve ordering of ScanTasks,"This can be refactored with a little effort in Scanner::ToTable:

# Change `batches` to `std::vector<RecordBatchVector>`
# When pushing the closure to the TaskGroup, also track an incrementing integer, e.g. scan_task_id
# In the closure, store the RecordBatches for this ScanTask in a local vector, when all batches are consumed, move the local vector in the `batches` at the right index, resizing and emplacing with mutex
# After waiting for the task group completion either
* Flatten into a single vector and call `Table::FromRecordBatch` or
* Write a RecordBatchReader that supports vector<vector<RecordBatch> and add method `Table::FromRecordBatchReader`

The later involves more work but is the clean way, the other FromRecordBatch method can be implemented from it and support ""streaming"".",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-14 13:18:51,13
13298220,[Python] NullType.to_pandas_dtype inconsisent with dtype returned in to_pandas/to_numpy,"There is this behaviour of {{to_pandas_dtype}} returning float, while all actual conversions to numpy or pandas use object dtype:

{code}
In [23]: pa.null().to_pandas_dtype()                                                                                                                                                                               
Out[23]: numpy.float64

In [24]: pa.array([], pa.null()).to_pandas()                                                                                                                                                                       
Out[24]: Series([], dtype: object)

In [25]: pa.array([], pa.null()).to_numpy(zero_copy_only=False)                                                                                                                                                    
Out[25]: array([], dtype=object)
{code}

So we should probably fix {{NullType.to_pandas_dtype}} to return object, which is used in practice.",pull-request-available,['Python'],ARROW,Bug,Major,2020-04-14 09:22:09,5
13298194,[C++] arrow-io-memory-benchmark crashes,"""arrow-io-memory-benchmark"" SIGSEGV in latest code base. It worked at least when my last commit 8 days ago: b1d4c86eb28267525c52f436c3a096e70b8ef6e0

Tested on x86 host. Build with ""cmake -GNinja -DCMAKE_BUILD_TYPE=Debug -DARROW_COMPUTE=ON -DARROW_BUILD_BENCHMARKS=ON ..""

stack backtrace attached

(gdb) r
Starting program: /home/cyb/share/debug/arrow-io-memory-benchmark 
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
(gdb) [New Thread 0x7ffff37ff700 (LWP 29065)]
2020-04-14 14:24:40
Running /home/cyb/share/debug/arrow-io-memory-benchmark
Run on (32 X 2100 MHz CPU s)
CPU Caches:
  L1 Data 32K (x16)
  L1 Instruction 64K (x16)
  L2 Unified 512K (x16)
  L3 Unified 4096K (x16)
Load Average: 2.64, 4.39, 4.28
***WARNING*** CPU scaling is enabled, the benchmark real time measurements may be noisy and will incur extra overhead.
***WARNING*** Library was built as DEBUG. Timings may be affected.

Thread 1 ""arrow-io-memory"" received signal SIGSEGV, Segmentation fault.
0x00007ffff68e67c8 in arrow::Buffer::is_mutable (this=0x0) at ../src/arrow/buffer.h:258
258	../src/arrow/buffer.h: No such file or directory.
(gdb) bt
#0  0x00007ffff68e67c8 in arrow::Buffer::is_mutable (this=0x0) at ../src/arrow/buffer.h:258
#1  0x00007ffff6c3c41a in arrow::io::FixedSizeBufferWriter::FixedSizeBufferWriterImpl::FixedSizeBufferWriterImpl (this=0x5555558921f0, buffer=std::shared_ptr<arrow::Buffer> (empty) = {...})
    at ../src/arrow/io/memory.cc:164
#2  0x00007ffff6c3a575 in arrow::io::FixedSizeBufferWriter::FixedSizeBufferWriter (this=0x7fffffffd660, buffer=std::shared_ptr<arrow::Buffer> (empty) = {...}, __in_chrg=<optimized out>, 
    __vtt_parm=<optimized out>) at ../src/arrow/io/memory.cc:227
#3  0x00005555555ebd00 in arrow::ParallelMemoryCopy (state=...) at ../src/arrow/io/memory_benchmark.cc:303
#4  0x00005555555f80d4 in benchmark::internal::FunctionBenchmark::Run (this=0x555555891290, st=...)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_register.cc:496
#5  0x000055555564bcc7 in benchmark::internal::BenchmarkInstance::Run (this=0x5555558939c0, iters=10, thread_id=0, timer=0x7fffffffd7a0, manager=0x555555894b70)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_api_internal.cc:10
#6  0x000055555562c0c8 in benchmark::internal::(anonymous namespace)::RunInThread (b=0x5555558939c0, iters=10, thread_id=0, manager=0x555555894b70)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_runner.cc:119
#7  0x000055555562c95a in benchmark::internal::(anonymous namespace)::BenchmarkRunner::DoNIterations (this=0x7fffffffddc0)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_runner.cc:214
#8  0x000055555562d0ac in benchmark::internal::(anonymous namespace)::BenchmarkRunner::DoOneRepetition (this=0x7fffffffddc0, repetition_index=0)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_runner.cc:299
#9  0x000055555562c558 in benchmark::internal::(anonymous namespace)::BenchmarkRunner::BenchmarkRunner (this=0x7fffffffddc0, b_=..., complexity_reports_=0x7fffffffdef0)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_runner.cc:161
#10 0x000055555562d47f in benchmark::internal::RunBenchmark (b=..., complexity_reports=0x7fffffffdef0)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_runner.cc:355
#11 0x00005555555f0ae6 in benchmark::internal::(anonymous namespace)::RunBenchmarks (benchmarks=std::vector of length 9, capacity 12 = {...}, display_reporter=0x555555891510, file_reporter=0x0)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark.cc:265
#12 0x00005555555f13b6 in benchmark::RunSpecifiedBenchmarks (display_reporter=0x555555891510, file_reporter=0x0)
    at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark.cc:399
#13 0x00005555555f0ef8 in benchmark::RunSpecifiedBenchmarks () at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark.cc:340
#14 0x00005555555efc64 in main (argc=1, argv=0x7fffffffe398) at /home/cyb/arrow/cpp/debug/gbenchmark_ep-prefix/src/gbenchmark_ep/src/benchmark_main.cc:17
",pull-request-available,"['Benchmarking', 'C++']",ARROW,Bug,Major,2020-04-14 06:46:52,2
13298164,[C++] Remove std::move return value from MakeRandomNullBitmap test utility,"Introduced by #6910, the builds triggered on the PR have not catched the compile error.",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-14 01:32:40,3
13298145,[C++] Ipc RecordBatchFileReader deserializes the Schema multiple times,"This extra work is redundant and should be skipped

https://github.com/apache/arrow/blob/0c5296d3353e494bbf65c8cbc6b56525db6ae084/cpp/src/arrow/ipc/reader.cc#L800",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-04-13 22:06:59,2
13298122,[R] Add feather alias for ipc format in dataset API,cf. ARROW-8416,pull-request-available,['R'],ARROW,Improvement,Minor,2020-04-13 20:22:38,4
13298105,[Python][CI] Failure to download Hadoop,"https://circleci.com/gh/ursa-labs/crossbow/11128?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

This is caused by an HTTP request failure https://github.com/apache/arrow/blob/master/ci/docker/conda-python-hdfs.dockerfile#L36

We should probably not rely on https://www.apache.org/dyn/mirrors/mirrors.cgi to get tarballs. Currently there are:

{code}
ci/docker/conda-python-hdfs.dockerfile
36:RUN wget -q -O - ""https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=hadoop/common/hadoop-${hdfs}/hadoop-${hdfs}.tar.gz"" | tar -xzf - -C /opt

ci/docker/linux-apt-docs.dockerfile
57:RUN wget -q -O - ""https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=maven/maven-3/${maven}/binaries/apache-maven-${maven}-bin.tar.gz"" | tar -xzf - -C /opt

python/manylinux1/scripts/build_thrift.sh
22:  ""https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=${THRIFT_DOWNLOAD_PATH}"" \

python/manylinux201x/scripts/build_thrift.sh
20:wget https://archive.apache.org/dist/thrift/${THRIFT_VERSION}/thrift-${THRIFT_VERSION}.tar.gz
{code}

Factor these out into a reusable script for downloading apache tarballs. It should contain hard coded apache mirrors and retry when connections fail",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2020-04-13 19:11:35,6
13298095,[CI] Configure self-hosted runners for Github Actions,Set up Ubuntu C++ ARMv8 builders and perhaps AMD64 builder to run on self-hosted github runners.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-13 18:03:33,3
13298086,[C++] Fix Buffer::CopySlice on 0-sized buffer,Found by OSS-Fuzz.,pull-request-available,['C++'],ARROW,Bug,Blocker,2020-04-13 17:34:21,2
13298085,[C++][NIGHTLY:gandiva-jar-trusty] GCC 4.8 failures in C++ unit tests,"See https://issues.apache.org/jira/browse/ARROW-8388

Not reported by the CI job added in that issue since manylinux1 doesn't currently build the c++ unit tests.",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-13 17:30:26,6
13298077,[C++][Dataset] Do not ignore file paths with underscore/dot when full path was specified,"Currently, when passing a list of file path to FileSystemDatasetFactory, the files that have one of their parent directories with a underscore or dot are skipped. Since the file paths were passed as an explicit list, we should maybe not skip them.

For example, when specifying a directory (Selector), it will only check child directories to skip, not parent directories.",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-04-13 16:26:46,6
13298044,[Rust] [Parquet] Serialize arrow schema into metadata when writing parquet,"The C++ implementation uses""ARROW:schema"" as a value to store the arrow schema as metadata. Implement same for compatibility.

Having the original Arrow schema is useful for readers as it preserves some properties like dictionary encoding.",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-04-13 14:38:16,12
13298043,[Rust] [Parquet] Implement function to convert Arrow schema to Parquet schema,Implement function to convert Arrow schema to Parquet schema,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-04-13 14:37:49,12
13298042,[Rust] [Parquet] Implement parquet writer,"This is the parent story. See subtasks for more information.

Notes from [~wesm] :

A couple of initial things to keep in mind
 * Writes of both Nullable (OPTIONAL) and non-nullable (REQUIRED) fields
 * You can optimize the special case where a nullable field's data has no nulls
 * A good amount of code is required to handle converting from the Arrow physical form of various logical types to the Parquet equivalent one, see [https://github.com/apache/arrow/blob/master/cpp/src/parquet/column_writer.cc] for details
 * It would be worth thinking up front about how dictionary-encoded data is handled both on the Arrow write and Arrow read paths. In parquet-cpp we initially discarded Arrow DictionaryArrays on write (casting e.g. Dictionary to dense String), and through real world need I was forced to revisit this (quite painfully) to enable Arrow dictionaries to survive roundtrips to Parquet format, and also achieve better performance and memory use in both reads and writes. You can certainly do a dictionary-to-dense conversion like we did, but you may someday find yourselves doing the same painful refactor that I did to make dictionary write and read not only more efficient but also dictionary order preserving.

Notes from [~sunchao] :

I roughly skimmed through the C++ implementation and think on the high level we need to do the following:
 # implement a method similar to {{WriteArrow}} in [column_writer.cc|https://github.com/apache/arrow/blob/master/cpp/src/parquet/column_writer.cc]. We can further break this up into smaller pieces such as: dictionary/non-dictionary, primitive types, booleans, timestamps, dates, so on and so forth.
 # implement an arrow writer in the parquet crate [here|https://github.com/apache/arrow/tree/master/rust/parquet/src/arrow]. This needs to offer similar APIs as [writer.h|https://github.com/apache/arrow/blob/master/cpp/src/parquet/arrow/writer.h].",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-04-13 14:36:55,12
13298039,[C++] CMake fails to configure on armv7l platform (e.g. Raspberry Pi 3),"Related to ARROW-8410, but probably will resolve the ARMv7 issues in a separate PR

{code}
$ cmake .. -DARROW_BUILD_TESTS=ON -DARROW_ORC=ON -DARROW_PARQUET=ON -DARROW_DEPENDENCY_SOURCE=BUNDLED -GNinja
-- Building using CMake version: 3.13.4
-- The C compiler identification is GNU 8.3.0
-- The CXX compiler identification is GNU 8.3.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Arrow version: 1.0.0 (full: '1.0.0-SNAPSHOT')
-- Arrow SO version: 100 (full: 100.0.0)
-- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29"") 
-- clang-tidy not found
-- clang-format not found
-- Could NOT find ClangTools (missing: CLANG_FORMAT_BIN CLANG_TIDY_BIN) 
-- infer not found
-- Found Python3: /usr/bin/python3.7 (found version ""3.7.3"") found components:  Interpreter 
-- Found cpplint executable at /home/pi/code/arrow/cpp/build-support/cpplint.py
-- Performing Test CXX_SUPPORTS_SSE4_2
-- Performing Test CXX_SUPPORTS_SSE4_2 - Failed
-- Performing Test CXX_SUPPORTS_AVX2
-- Performing Test CXX_SUPPORTS_AVX2 - Failed
-- Performing Test CXX_SUPPORTS_AVX512
-- Performing Test CXX_SUPPORTS_AVX512 - Failed
-- Arrow build warning level: PRODUCTION
CMake Error at cmake_modules/SetupCxxFlags.cmake:318 (message):
  SSE4.2 required but compiler doesn't support it.
Call Stack (most recent call first):
  CMakeLists.txt:399 (include)


-- Configuring incomplete, errors occurred!
See also ""/home/pi/code/arrow/cpp/build/CMakeFiles/CMakeOutput.log"".
See also ""/home/pi/code/arrow/cpp/build/CMakeFiles/CMakeError.log"".
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-13 14:21:44,14
13297968,"[Python] Provide a ""feather"" alias in the dataset API","I don't know what the plans are on the C++ side (ARROW-7586), but for 0.17, I think it would be nice if we can at least support {{ds.dataset(..., format=""feather"")}} (instead of needing to tell people to do {{ds.dataset(..., format=""ipc"")}} to read feather files).",pull-request-available,['Python'],ARROW,Bug,Major,2020-04-13 07:38:24,5
13297951,[C++] Refactor DefLevelsToBitmap,"The current code is should be split apart and made more efficient to consolidate logic need to support all nesting combinations.



We need to be able to pass in an arbitrary min definitions level to prune away elements that aren't included in lists.

The functionality is also somewhat replicated in reading the struct code, the two paths should be consolidated.



",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-04-13 06:13:14,15
13297916,[C++] CMake fails on aarch64 systems that do not support -march=armv8-a+crc+crypto,"I was trying to build the project on a rockpro64 system to look into something else and ran into this

{code}
-- Arrow build warning level: PRODUCTION
CMake Error at cmake_modules/SetupCxxFlags.cmake:332 (message):
  Unsupported arch flag: -march=armv8-a+crc+crypto.
Call Stack (most recent call first):
  CMakeLists.txt:398 (include)

-- Configuring incomplete, errors occurred!
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-12 22:37:11,14
13297912,"[R] Add arrow::cpu_count, arrow::set_cpu_count wrapper functions a la Python","While some people will configure these with {{$OMP_NUM_THREADS}}, it is useful to be able to configure the global thread pool dynamically",pull-request-available,['R'],ARROW,Improvement,Major,2020-04-12 21:11:46,14
13297910,[Python] Add memory_map= toggle to pyarrow.feather.read_feather,I missed this in my prior patch,pull-request-available,['Python'],ARROW,Improvement,Major,2020-04-12 20:28:50,14
13297888,[Rust] Add rustdoc for Dictionary type,Add rustdoc for Dictionary type,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-04-12 15:54:22,10
13297855,[Python] test_fs fails when run from a different drive on Windows,"{code:python}
path = ""C:\Users\VssAdministrator\AppData\Local\Temp\pytest-of-VssAdministrator\pytest-0\test_construct_from_single_fil0\single-file""
_, path = FileSystem.from_uri(path)
path == ""/Users/VssAdministrator/AppData/Local/Temp/pytest-of-VssAdministrator/pytest-0/test_construct_from_single_fil0/single-file""
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Blocker,2020-04-12 09:24:23,2
13297771,[Java] Support ValidateFull methods in Java,"We need to support ValidateFull methods in Java, just like we do in C++. 
This is required by ARROW-5926.",pull-request-available,['Java'],ARROW,New Feature,Major,2020-04-11 13:56:15,7
13297686,[Python] Remove deprecation warnings originating from python tests,See build loghttps://travis-ci.org/github/ursa-labs/crossbow/builds/673385834#L6846,pull-request-available,['Python'],ARROW,Improvement,Major,2020-04-10 19:23:34,3
13297684,[C++] Fail to compile aggregate_test.cc on Ubuntu 16.04,See build loghttps://app.circleci.com/pipelines/github/ursa-labs/crossbow/31122/workflows/b250d378-52a8-4d15-9909-96474fa38482/jobs/10840,pull-request-available,['C++'],ARROW,Bug,Major,2020-04-10 19:09:37,3
13297540,[Java] Fix overflow related corner cases for vector value comparison,"1. Fix corner cases related to overflow.
2. Provide test cases for the corner cases. ",pull-request-available,['Java'],ARROW,Bug,Major,2020-04-10 04:34:44,7
13297496,[Integration] Run tests in parallel,This follows ARROW-8176.,pull-request-available,"['Continuous Integration', 'Integration']",ARROW,Improvement,Major,2020-04-09 22:37:31,2
13297467,[C++] GCC 4.8 fails to move on return,"See https://github.com/apache/arrow/pull/6883#issuecomment-611661733

This is a recurring problem which usually shows up as a broken nightly (the gandiva nightly jobs, specifically) along with similar issues due to gcc 4.8's incomplete handling of c++11. As long as someone depends on these we should probably have an every-commit CI job which checks we haven't introduced such a breakage",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-09 19:44:18,6
13297416,[C++][Python] arrow/filesystem/hdfs.h and Python wrapper does not have an option for setting a path to a Kerberos ticket,"This feature seems to have been dropped

Is there a plan for migrating users to the new filesystem API? We have two different code paths now",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-09 15:23:11,2
13297388,[C++][Dataset] Refactor WritePlan to decouple from Fragment/Scan/Partition classes ,"WritePlan should look like the following.
{code:c++}
class ARROW_DS_EXPORT WritePlan {
 public:
  /// Execute the WritePlan and return a FileSystemDataset as a result.
 Result<FileSystemDataset> Execute(FileSystemDatasetFactory factory);

 protected:
  /// The schema of the Dataset which will be written
  std::shared_ptr<Schema> schema;

  /// The format into which fragments will be written
  std::shared_ptr<FileFormat> format;
 
  using SourceAndReader = std::pair<FIleSource, RecordBatchReader>;
  /// Files to write
  std::vector<SourceAndReader> outputs;
};
{code}
 * Refactor FileFormat::Write(FileSource destination, RecordBatchReader), not sure if it should take the output schema, or the RecordBatchReader should be already of the right schema.
 * Add a class/function that constructs SourceAndReader from Fragments, Partitioning and base path.
 * Move Write() out FIleSystemDataset into WritePlan. It could take a FileSystemDatasetFactory to recreate the FileSystemDataset. This is a bonus, not a requirement.
 * Simplify writing routine to avoid the PathTree directory structure, it shouldn't be more complex than `for task in write_tasks: task()`. Not path construction should be there.
 * Move the logic of dropping columns or any filtering into a custom RecordBatchReader.

The effects are:
 * Simplified WritePlan execution, abstracted away from path construction, and can write to multiple FileSystem and/or Buffers since it doesn't construct the FileSource.
 * By the virtue of using RecordBatchReader instead of Fragment, it isn't tied to writing from Fragment, it can take any construct that yields a RecordBatchReader. It also means that WritePlan doesn't have to know about any Scan related classes.
 * Writing can be done with or without partitioning, this logic is given to whomever generates the SourceAndReader list.
 * Should be simpler to test.",dataset,['C++'],ARROW,Improvement,Major,2020-04-09 14:04:48,6
13297372,[C++][Dataset] Dataset writing should require a writer schema,"# Dataset writing should always take an explicit writer schema instead of the first fragment's schema.
# The MakeWritePlanImpl should not try removing columns that are found in the partition, this is left to the caller by passing an explicit schema.

",dataset,['C++'],ARROW,Bug,Major,2020-04-09 12:42:43,6
13297139,"[GLib] Problems resolving gobject-introspection, arrow in Meson builds",See example failure https://github.com/apache/arrow/pull/6872/checks?check_run_id=571082161,pull-request-available,['GLib'],ARROW,Bug,Major,2020-04-08 17:26:12,1
13297121,[Crossbow] Implement and exercise sanity checks for tasks.yml ,See conversation athttps://github.com/apache/arrow/pull/6868#issuecomment-610721717,pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Improvement,Major,2020-04-08 16:20:40,3
13296968,[CI] Fix crossbow wildcard groups,This was broken in ARROW-8356,pull-request-available,['Continuous Integration'],ARROW,Bug,Minor,2020-04-08 02:25:33,4
13296941,"[Format] In C interface, clarify resource management for consumers needing only a subset of child fields in ArrowArray","The current implication of the C Interface is that only moving a single child out of an ArrowArray is allowed. 

Questions:

* Should it be allowed to move multiple children, as long as they are moved at the same time, and the parent is released after?
* In the event that children have disjoint internal resources, should there be a clarification around moved children having their resources released independently?

See mailing list discussion https://lists.apache.org/thread.html/r92b77e0fa7bed384daa377e2178bc8e8ca46103928598050341e40b1%40%3Cdev.arrow.apache.org%3E",pull-request-available,"['C++', 'Format']",ARROW,Improvement,Blocker,2020-04-07 21:48:21,2
13296899,"[C++] Deprecate Buffer::FromString(..., pool) ","From [https://github.com/apache/arrow/pull/6863#discussion_r404913683]

There are currently two overloads of {{Buffer::FromString}}, one which takes an rvalue reference to string and another which takes a const reference and a MemoryPool. In the former case the string is simply moved into a Buffer subclass while in the latter the MemoryPool is used to allocate space into which the string's contents are copied, which necessitates bubbling the potential allocation failure. This seems gratuitous given we don't use {{std::string}} to store large quantities so it should be fine to provide only
{code:java}
  static std::unique_ptr<Buffer> FromString(std::string data); 
{code}
and rely on {{std::string}}'s copy constructor when the argument is not an rvalue.

In the case of a {{std::string}} which may/does contain large data and must be copied, tracking the copied memory with a MemoryPool does not require a great deal of boilerplate:
{code:java}
ARROW_ASSIGN_OR_RAISE(auto buffer,
                      Buffer(large).CopySlice(0, large.size(), pool));
{code}",pull-request-available,['C++'],ARROW,Improvement,Trivial,2020-04-07 19:27:07,2
13296856,[Rust] Need to revert recent arrow-flight build change,"The PR [1] merged for ARROW-7794 causes problems with projects that have a dependency on this crate where the build.rs code becomes an infinite loop looking for a parent directory named ""arrow"" that doesn't exist.

This PR simply reverts that change. I will need to find a better approach to resolving the original issue.

[1] https://github.com/apache/arrow/pull/6858",pull-request-available,['Rust'],ARROW,Bug,Critical,2020-04-07 15:31:37,10
13296847,[C++] Error when writing files to S3 larger than 5 GB,"When purely using the arrow-cpp library to write to S3, I get the following error when trying to write a large Arrow table to S3 (resulting in a file size larger than 5 GB):

{{../src/arrow/io/interfaces.cc:219: Error ignored when destroying file of type N5arrow2fs12_GLOBAL__N_118ObjectOutputStreamE: IOError: When uploading part for key 'test01.parquet/part-00.parquet' in bucket 'test': AWS Error [code 100]: Unable to parse ExceptionName: EntityTooLarge Message: Your proposed upload exceeds the maximum allowed size with address : 52.219.100.32}}

I have diagnosed the problem by looking at and modifying the code in *{{s3fs.cc}}*. The code uses multipart upload, and uses 5 MB chunks for the first 100 parts. After it has submitted the first 100 parts, it is supposed to increase the size of the chunks to 10 MB (the part upload threshold or {{part_upload_threshold_}}). The issue is that the threshold is increased inside {{DoWrite}}, and {{DoWrite}} can be called multiple times before the current part is uploaded, which ultimately causes the threshold to keep getting increased indefinitely, and the last part ends up surpassing the 5 GB part upload limit of AWS/S3.

This issue where the last part is much larger than it should I'm pretty sure can happen every time a multi-part upload exceeds 100 parts, but the error is only thrown if the last part is larger than 5 GB. Therefore this is only observed with very large uploads.

I can confirm that the bug does not happen if I move this:

{{if (part_number_ % 100 == 0) {}}
 part_upload_threshold_ += kMinimumPartUpload;}}
}

and do it in a different method, right before the line that does: {{++part_number_}}

",pull-request-available,['C++'],ARROW,Bug,Blocker,2020-04-07 14:47:28,2
13296824,[Crossbow] Ensure that the locally generated version is used in the docker tasks,"Arrow fork might not have the version tags, so the scm based version generation can't work.

Pass the locally detected version to the docker builds.",pull-request-available,['Packaging'],ARROW,Task,Major,2020-04-07 13:15:37,3
13296731,[C++/Python] Enable aarch64/ppc64le build in conda recipes,"These two new arches were added in the conda recipes, we should also build them as nightlies.",pull-request-available,"['C++', 'Packaging', 'Python']",ARROW,Improvement,Major,2020-04-07 05:14:03,8
13296707,[C++] Fix -Wrange-loop-construct warnings in clang-11 ,"We might change one of our CI entries to use clang-11 so we get some more bleeding edge compiler warnings, to get out ahead of things",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-07 02:14:54,14
13296705,[Rust] [DataFusion] Dockerfile for CLI is missing format dir,"{code:java}
error: failed to run custom build command for `arrow-flight v1.0.0-SNAPSHOT (/arrow/rust/arrow-flight)`Caused by:
  process didn't exit successfully: `/arrow/rust/target/release/build/arrow-flight-a0fb14daffea70f5/build-script-build` (exit code: 1)
--- stderr
Error: Custom { kind: Other, error: ""protoc failed: ../../format: warning: directory does not exist.\nCould not make proto path relative: ../../format/Flight.proto: No such file or directory\n"" }warning: build failed, waiting for other jobs to finish...
error: failed to compile `datafusion v1.0.0-SNAPSHOT (/arrow/rust/datafusion)`, intermediate artifacts can be found at `/arrow/rust/target`Caused by:
  build failed
 {code}",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Minor,2020-04-07 02:06:17,10
13296698,"[Developer] Support * wildcards with ""crossbow submit"" via GitHub actions","While the ""group"" feature can be useful, sometimes there is a group of builds that do not fit neatly into a particular group",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-04-07 01:08:59,3
13296667,[R] Fix segfault in Table to Array conversion,See https://github.com/fsaintjacques/arrow/runs/564315427#step:6:2169,pull-request-available,['R'],ARROW,Bug,Major,2020-04-06 21:30:13,13
13296666,[C++] is_nullable maybe not initialized in parquet writer,"From the Rtools build:

{code}
[ 84%] Building CXX object src/parquet/CMakeFiles/parquet_static.dir/column_reader.cc.obj
In file included from D:/a/arrow/arrow/cpp/src/arrow/io/concurrency.h:23:0,
                 from D:/a/arrow/arrow/cpp/src/arrow/io/memory.h:25,
                 from D:/a/arrow/arrow/cpp/src/parquet/platform.h:25,
                 from D:/a/arrow/arrow/cpp/src/parquet/arrow/writer.h:23,
                 from D:/a/arrow/arrow/cpp/src/parquet/arrow/writer.cc:18:
D:/a/arrow/arrow/cpp/src/arrow/result.h: In member function 'virtual arrow::Status parquet::arrow::FileWriterImpl::WriteColumnChunk(const std::shared_ptr<arrow::ChunkedArray>&, int64_t, int64_t)':
D:/a/arrow/arrow/cpp/src/arrow/result.h:428:28: warning: 'is_nullable' may be used uninitialized in this function [-Wmaybe-uninitialized]
   auto result_name = (rexpr);                               \
                            ^
D:/a/arrow/arrow/cpp/src/parquet/arrow/writer.cc:430:10: note: 'is_nullable' was declared here
     bool is_nullable;
          ^
{code}

I'd give it a default value, but IDK that it's that simple.",pull-request-available,['C++'],ARROW,Bug,Minor,2020-04-06 21:27:02,14
13296663,[R] Add install_pyarrow(),"To facilitate installing for use with reticulate, including handling how to use the nightly packages.",pull-request-available,['R'],ARROW,New Feature,Major,2020-04-06 21:09:47,4
13296642,[R][CI] Store the Rtools-built Arrow C++ library as a build artifact,To help with debugging unexplained segfaults.,pull-request-available,"['Continuous Integration', 'R']",ARROW,New Feature,Major,2020-04-06 19:24:34,4
13296544,[CI][Ruby] GLib/Ruby macOS build fails on zlib,"See https://github.com/apache/arrow/runs/564610412 for example.

{code}
Using 'PKG_CONFIG_PATH' from environment with value: '/usr/local/lib/pkgconfig'
Run-time dependency gobject-2.0 found: YES 2.64.1
Run-time dependency gio-2.0 found: NO (tried framework and cmake)

c_glib/arrow-glib/meson.build:210:0: ERROR: Could not generate cargs for gio-2.0:
Package zlib was not found in the pkg-config search path.
Perhaps you should add the directory containing `zlib.pc'
to the PKG_CONFIG_PATH environment variable
Package 'zlib', required by 'gio-2.0', not found


A full log can be found at /Users/runner/runners/2.168.0/work/arrow/arrow/build/c_glib/meson-logs/meson-log.txt
{code}",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Bug,Major,2020-04-06 15:33:20,1
13296531,[Python] feather.read_table should not require pandas,"We still check the pandas version, while pandas is not actually needed. Will do a quick fix.",pull-request-available,['Python'],ARROW,Bug,Minor,2020-04-06 14:32:29,5
13296417,[Python] dask and kartothek integration tests are failing,"The integration tests for both dask and kartothek, and for both master and latest released version of them, started failing the last days.

Dask latest: https://circleci.com/gh/ursa-labs/crossbow/10629?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link 
Kartothek latest: https://circleci.com/gh/ursa-labs/crossbow/10604?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

I think both are related to the KeyValueMetadata changes (ARROW-8079).

The kartothek one is clearly related, as it gives: TypeError: 'pyarrow.lib.KeyValueMetadata' object does not support item assignment

And I think the dask one is related to the ""pandas"" key now being present twice, and therefore it is using the ""wrong"" one.
",pull-request-available,['Python'],ARROW,Bug,Blocker,2020-04-06 06:42:24,14
13296404,[Packaging][deb] Fail to build by no disk space,"https://github.com/ursa-labs/crossbow/runs/561083601

{noformat}
2020-04-04T22:52:18.5037888Z cp: error writing 'debian/libarrow-dataset-dev//usr/lib/x86_64-linux-gnu/libarrow_dataset.a': No space left on device
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-04-06 03:16:32,1
13296209,[Release] Add crossbow jobs to run release verification,"Workflow: edit version number and rc number in template in {{dev/release/github.verify.yml}}, make PR, and do 

* {{@github-actions crossbow submit -g verify-rc}} to run everything
* {{@github-actions crossbow submit -g verify-rc-wheel|source|binary}} to run those groups
* Other groups at {{verify-rc-wheel|source-macos|ubuntu|windows}}, {{verify-rc-source-cpp|csharp|java|etc.}}
* Individual workflows at e.g. {{verify-rc-wheel-windows}}, {{verify-rc-source-macos-csharp}}. We could break out the wheel verification by python version (maybe we should), but that requires changes to the verification scripts themselves.

Running the main {{verify-rc}} group will put a ton of workflow svg badges on the PR so we can see at a glance what is passing and failing. If things fail when running all, can push fixes to the verification script to the branch and retry just those that failed.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-04-04 17:08:59,4
13296140,[C++][CI] Always compile benchmarks in some C++ CI entry ,"As exposed in ARROW-8331, apparently we do not check. ",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-03 23:27:13,14
13296125,[C++] arrow-compute-filter-benchmark fails to compile,"Are the benchmarks not being built in CI?

{code}
../src/arrow/compute/kernels/filter_benchmark.cc:45:18: error: no matching function for call to 'Filter'
    ABORT_NOT_OK(Filter(&ctx, Datum(array), Datum(filter), &out));
                 ^~~~~~
../src/arrow/testing/gtest_util.h:109:18: note: expanded from macro 'ABORT_NOT_OK'
    auto _res = (expr);                                             \
                 ^~~~
../src/arrow/compute/kernels/filter.h:65:8: note: candidate function not viable: requires 5 arguments, but 4 were provided
Status Filter(FunctionContext* ctx, const Datum& values, const Datum& filter,
       ^
../src/arrow/compute/kernels/filter_benchmark.cc:66:18: error: no matching function for call to 'Filter'
    ABORT_NOT_OK(Filter(&ctx, Datum(array), Datum(filter), &out));
                 ^~~~~~
../src/arrow/testing/gtest_util.h:109:18: note: expanded from macro 'ABORT_NOT_OK'
    auto _res = (expr);                                             \
                 ^~~~
../src/arrow/compute/kernels/filter.h:65:8: note: candidate function not viable: requires 5 arguments, but 4 were provided
Status Filter(FunctionContext* ctx, const Datum& values, const Datum& filter,
       ^
../src/arrow/compute/kernels/filter_benchmark.cc:90:18: error: no matching function for call to 'Filter'
    ABORT_NOT_OK(Filter(&ctx, Datum(array), Datum(filter), &out));
                 ^~~~~~
../src/arrow/testing/gtest_util.h:109:18: note: expanded from macro 'ABORT_NOT_OK'
    auto _res = (expr);                                             \
                 ^~~~
../src/arrow/compute/kernels/filter.h:65:8: note: candidate function not viable: requires 5 arguments, but 4 were provided
Status Filter(FunctionContext* ctx, const Datum& values, const Datum& filter,
       ^
3 errors generated.
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-03 21:39:23,14
13296079,[Documentation] The post release script generates the documentation with a development version,See the current documentation page. Also regenerate the github page.,pull-request-available,['Documentation'],ARROW,Task,Major,2020-04-03 17:43:36,3
13296078,[Documentation][C++] Undocumented FilterOptions argument in Filter kernel,"The documentation build fails, seehttps://github.com/apache/arrow/runs/558617620#step:6:1186",pull-request-available,"['C++', 'Documentation']",ARROW,Task,Major,2020-04-03 17:40:34,3
13296036,[FlightRPC][Java] gRPC trailers may be null,This can cause spurious failures.,pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2020-04-03 14:07:06,0
13295876,[C++] Pin gRPC at v1.27 to avoid compilation error in its headers,"[gRPC 1.28|https://github.com/grpc/grpc/releases/tag/v1.28.0] includes a change which introduces an implicit size_t->int conversion in proto_utils.h: https://github.com/grpc/grpc/commit/2748755a4ff9ed940356e78c105f55f839fdf38b

Conversion warnings are treated as errors for example here: https://ci.appveyor.com/project/BenjaminKietzman/arrow/build/job/9cl0vqa8e495knn3#L1126
So IIUC we need to pin gRPC to 1.27 for now.

Upstream PR: https://github.com/grpc/grpc/pull/22557",pull-request-available,['C++'],ARROW,Bug,Major,2020-04-02 18:58:47,6
13295860,[CI] Fix C# workflow file syntax,"The github actions expression requires the enclosing ""${{ }}""",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2020-04-02 18:01:35,3
13295855,[CI] Use bundled thrift in Fedora 30 build,"After unsetting Thrift_SOURCE from AUTO it surfaced that the thrift available on Fedora 30 is older 0.10 than the minimal required version 0.11.

Build thrift_ep instead.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-02 17:54:49,3
13295853,[Documentation][Format] Clarify (lack of) alignment requirements in C data interface,This document should clarify that memory buffers need not start on aligned (8-byte or otherwise) pointer offsets. ,pull-request-available,"['Documentation', 'Format']",ARROW,Improvement,Major,2020-04-02 17:54:32,14
13295851,[CI] Install thrift compiler in the debian build,"CMake is missing thrift compiler after setting Thrift_SOURCE to empty from AUTO, 
see build: https://github.com/apache/arrow/runs/555631125?check_suite_focus=true#step:6:143",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-02 17:48:01,3
13295849,[C++][Dataset] Dataset should instantiate Fragment,"Fragments are created on the fly when invoking a Scan. This means that a lot of the auxilliary/ancilliary data must be stored by the specialised Dataset, e.g. the FileSystemDataset must hold the path and partition expression. With the venue of more complex Fragment, e.g. ParquetFileFragment, more data must be stored. ",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-02 17:45:04,13
13295837,[CI] Set docker-compose to use docker-cli instead of docker-py for building images,"The images pushed from the master branch were sometimes producing reusable layers, sometimes not. So the caching was working non-deterministically. 
The underlying issue is https://github.com/docker/compose/issues/883



",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-02 16:43:50,3
13295810,[Python][Dataset] Don't rely on ordered dict keys in test_dataset.py,"Python 3.5 does not guarantee insertion order of dict keys, so we can't rely on it when constructing tables in test_dataset.py

https://github.com/apache/arrow/pull/6809/checks?check_run_id=554945477#step:6:2166

",dataset pull-request-available,['Python'],ARROW,Bug,Minor,2020-04-02 14:06:04,2
13295786,[Python] Provide a method to select a subset of columns of a Table,"I looked through the open issues and in our API, but didn't directly find something about selecting a subset of columns of a table.

Assume you have a table like:

{code}
table = pa.table({'a': [1, 2], 'b': [.1, .2], 'c': ['a', 'b']})
{code}

You can select a single column with {{table.column('a')}} or {{table['a']}} to get a chunked array. You can add, append, remove and replace columns (with {{add_column}}, {{append_column}}, {{remove_column}}, {{set_column}}). 
But an easy way to get a subset of the columns (without the manuall removing the ones you don't want one by one) doesn't seem possible. 

I would propose something like:

{code}
table.select(['a', 'c'])
{code}",pull-request-available,['Python'],ARROW,New Feature,Major,2020-04-02 12:11:48,5
13295680,[C++] Add push style stream format reader,"The current reader API read data from stream directly. This API isn't usable with event driven style IO API.

Push style reader API don't read data from stream directly. It receive already read data by users. This style is useful with event driven style IO API. We can't read data from stream directly in event driven style IO API. We just receive already read data from event driven style IO API.

We can't use the current reader API with event driven style IO API but we can use push style reader with event driven style IO API.
",pull-request-available,['C++'],ARROW,Improvement,Major,2020-04-02 03:18:49,1
13295617,[C++] Minio's exceptions not recognized by IsConnectError(),"Minio emits an {{XMinioServerNotInitialized}} exception on failure to connect, which is recognized by {{ConnectRetryStrategy}} and used to trigger a retry instead of an error. This exception has an HTTP error 503.

However this code does not round trip through the AWS SDK, which maintains an explicit [mapping from known exception names to error codes|https://github.com/aws/aws-sdk-cpp/blob/d36c2b16c9c3caf81524ebfff1e70782b8e1a006/aws-cpp-sdk-core/source/client/CoreErrors.cpp#L37] and will demote an unrecognized exception name [to {{CoreErrors::UNKNOWN}}|https://github.com/aws/aws-sdk-cpp/blob/master/aws-cpp-sdk-core/source/client/AWSErrorMarshaller.cpp#L150]

The end result is flakiness in the test (and therefore CI) since {{ConnectRetryStrategy}} never gets a chance to operate, see for example https://github.com/apache/arrow/pull/6789/checks?check_run_id=552871444#step:6:1778

Probably {{IsConnectError}} will need to examine the error string in the event of {{CoreErrors::UNKNOWN}}.",pull-request-available,['C++'],ARROW,Bug,Minor,2020-04-01 19:50:53,2
13295599,[CI] C++/Java/Rust workflows should trigger on changes to Flight.proto,"The Flight DoExchange format change caused Rust build failures (ARROW-8308). We would have caught these in the format change patch, but the Rust builds weren't triggered on changes to {{format/Flight.proto}}.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-04-01 18:20:25,4
13295595,[Rust] [Flight] Implement DoExchange on examples,The gRPC server examples in Rust require all trait members to be exhaustively implemented. The recent `DoExchange` endpoint to the Flight service is causing failures in Rust.,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-04-01 17:46:26,12
13295459,[Java] ExtensionTypeVector should make sure underlyingVector not null,"In ExtensionTypeVector seems not check nullability for underlyingVector, and it will throw exception when it is null in some api like accept/reset/getField etc.

it's better to do null check when create ExtensionTypeVector rather than when really use it.",pull-request-available,['Java'],ARROW,Bug,Major,2020-04-01 08:26:39,16
13295432,[Flight][Python] Flight client with TLS root certificate is reporting error on do_get(),"I have started a flight's local python server with TLS support using the testing certificates present in repo:
{code:java}
python3 <arrow_dir>/python/examples/flight/server.py--host localhost --tls<arrow_dir>/testing/data/flight/cert0.pem<arrow_dir>/testing/data/flight/cert0.key{code}
This server is started successfully.

Now I started testing the python client with TLS support.

1. Client pushing a csv file to the flightendpoint server:
{code:java}
python3 <arrow_dir>/python/examples/flight/client.py put --tls --tls-roots <arrow_dir>/testing/data/flight/root-ca.pem localhost:5005 /sharedFolder/dataset/iris.csv{code}
Fileiris.csv is pushed successfully.

2. List the flights available on the server
{code:java}
python3 <arrow_dir>/python/examples/flight/client.py list --tls --tls-roots <arrow_dir>/testing/data/flight/root-ca.pem localhost:5005{code}
It is listing the flight which is pushed in above step 1.

3. Get/Retrieve the specific flight(eg. /sharedFolder/dataset/iris.csv) from the server
{code:java}
python3 <arrow_dir>/python/examples/flight/client.py get --tls --tls-roots <arrow_dir>/testing/data/flight/root-ca.pem -p /sharedFolder/dataset/iris.csv localhost:5005{code}
It is failing with following errors:
{quote}Ticket: <Ticket b""(1, None, (b'/sharedFolder/dataset/iris.csv',))"">

<Location b'grpc+tls://localhost:5005'>

{color:#ff0000}E0401 06:43:30.164324553  1055 ssl_transport_security.cc:1238] Handshake failed with fatal error SSL_ERROR_SSL: error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed.{color}

Traceback (most recent call last):

 File ""<arrow_dir>/python/examples/flight/client.py"", line 178, in <module>

  main()

 File ""<arrow_dir>/python/examples/flight/client.py"", line 174, in main

  commands[args.action](args, client)

 File ""<arrow_dir>/python/examples/flight/client.py"", line 98, in get_flight

  reader = get_client.do_get(endpoint.ticket)

 File ""pyarrow/_flight.pyx"", line 1144, in pyarrow._flight.FlightClient.do_get

 File ""pyarrow/_flight.pyx"", line 73, in pyarrow._flight.check_flight_status

pyarrow._flight.FlightUnavailableError: gRPC returned unavailable error, with message: Connect Failed
{quote}
Python client.py is working for functions like_list_flights(), do_action(),push_data()_ but failing on

_get_flight()_ function forcode line.
{code:java}
reader = get_client.do_get(endpoint.ticket) {code}",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2020-04-01 07:14:58,2
13295324,[R] Handle ChunkedArray and Table in C data interface,"Currently the C data interface does Array and RecordBatch, but we're also going to need ChunkedArray and Table.",pull-request-available,['R'],ARROW,Improvement,Critical,2020-03-31 18:10:14,4
13295322,"[C++] Reusable ""optional ParallelFor"" function for optional use of multithreading","We often see code like

{code}
    if (use_threads) {
      return ::arrow::internal::ParallelFor(n, Func);
    } else {
      for (size_t i = 0; i < n; ++i) {
        RETURN_NOT_OK(Func(i));
      }
      return Status::OK();
{code}

It might be nice to have a helper function to do this. It doesn't even need to be an inline template, it could be a precompiled function accepting {{std::function<Status(int)>}}",pull-request-available,['C++'],ARROW,New Feature,Major,2020-03-31 18:05:03,7
13295311,[C++][CI] MinGW builds fail building grpc,"See e.g. https://github.com/apache/arrow/pull/6781/checks?check_run_id=548816924
{code}
CMake Error at D:/a/arrow/arrow/build/cpp/grpc_ep-prefix/src/grpc_ep-stamp/grpc_ep-build-RELEASE.cmake:62 (message):
  Command failed: 2

   'D:/a/arrow/msys64/usr/bin/make'

  See also

    D:/a/arrow/arrow/build/cpp/grpc_ep-prefix/src/grpc_ep-stamp/grpc_ep-build-*.log


make[2]: *** [CMakeFiles/grpc_ep.dir/build.make:130: grpc_ep-prefix/src/grpc_ep-stamp/grpc_ep-build] Error 1
make[1]: *** [CMakeFiles/Makefile2:1012: CMakeFiles/grpc_ep.dir/all] Error 2
make: *** [Makefile:158: all] Error 2
{code}
",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Critical,2020-03-31 16:54:06,1
13295310,[FlightRPC][C++] Implement Flight DoExchange for C++,As described in the mailing list vote.,pull-request-available,"['C++', 'FlightRPC']",ARROW,New Feature,Major,2020-03-31 16:53:22,0
13295295,[C++][Dataset] IpcFileFormat should expliclity push down column projection,{{IpcReadOptions::included_fields}} allows explicit skipping read/decompression of fields.,pull-request-available,['C++'],ARROW,Improvement,Minor,2020-03-31 15:58:15,6
13295294,[Format][Flight] Add DoExchange RPC to Flight protocol,Per mailing list discussion and vote,pull-request-available,"['FlightRPC', 'Format']",ARROW,New Feature,Major,2020-03-31 15:57:24,0
13295289,[Python][Dataset] Passthrough schema to Factory.finish() in dataset() function,"This is already a very simple fix to allow manually specifying the schema, without exposing any other options (as described in ARROW-8221)",pull-request-available,['Python'],ARROW,Sub-task,Major,2020-03-31 15:29:30,5
13295286,[Packaging] Conda nightly builds can't locate Numpy,See build error https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-03-30-1-azure-conda-linux-gcc-py36,pull-request-available,[],ARROW,Improvement,Major,2020-03-31 15:11:31,3
13295276,[Python][Dataset] Improve ergonomy of the FileSystemDataset constructor,"Currently, to manually create a FileSystemDataset, you can do something like:

{code}
dataset = ds.FileSystemDataset(
        schema, None, ds.ParquetFileFormat(), pa.fs.LocalFileSystem(),
        [""data_file1.parquet"", ""data_file2.parquet""],
        [ds.field('file') == 1, ds.field('file') == 2])
{code}

There are some usibility improvements we can do though:

- Allow passing the arguments by name to improve readability of the calling code (now they all need to be passed positionally, due to the way they are implemented in cython as {{not None}})
- I would maybe change the order of the arguments (eg start with the paths, we don't need to match the order of the C++ constructor)
- Potentially allow {{partitions}} to be optional, in which case they need to be set to a list of ScalarExpression(True) values.",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-31 14:47:45,5
13295273,[Rust] [Parquet] Implement minimal Arrow Parquet writer as starting point for full writer,Implement a minimal Arrow writer for Parquet so that RecordBatches can be written to a Parquet file. Ths initial version will only support i32 data type and separate JIRAs will be created for each data type or additional feature to support.,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-03-31 14:32:54,10
13295272,[Python]Expose with_ modifiers on DataType,We have several {{WithX}} functions defined on {{DataType}} in C++ but only {{WithMetadata}} is yet exposed in Python. We should expose the rest of them.,pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-31 14:32:24,8
13295264,[Python] Creating dataset from pathlib results in UnionDataset instead of FileSystemDataset,"{code}
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds

table = pa.table({'a': np.random.randn(10), 'b': range(10), 'c': ['a', 'b'] * 5})
pq.write_table(table, ""test.parquet"")

import pathlib

ds.dataset(pathlib.Path(""./test.parquet""))
# gives UnionDataset

ds.dataset(str(pathlib.Path(""./test.parquet"")))
# correctly gives FileSystemDataset
{code}

and since those two dataset classes have different API, this is important to give FileSystemDataset",dataset pull-request-available,['Python'],ARROW,Bug,Major,2020-03-31 13:48:56,5
13295246,[Python][Dataset] Non-existent files are silently dropped in pa.dataset.FileSystemDataset,"When passing a list of files to the constructor of {{pyarrow.dataset.FileSystemDataset}}, all files that don't exist are silently dropped immediately (i.e. no fragments are created for them).

Instead, I would expect that fragments will be created for them but an error is thrown when one tries to read the fragment with the non-existent file.",dataset pull-request-available,['Python'],ARROW,Improvement,Critical,2020-03-31 12:40:26,5
13295135,[C++] MinGW builds failing due to CARES-related toolchain issue,"This just started occurring today I think

Example run https://github.com/apache/arrow/pull/6774/checks?check_run_id=547420903

{code}
CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
Please set them or make sure they are set and tested correctly in the CMake files:
CARES_INCLUDE_DIR
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-31 02:30:43,1
13295129,"[C++] Do not export symbols from Codec implementations, remove need for PIMPL pattern",This is a small bit of code tidying that I noticed while reviewing the recent compression patch,pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-31 01:50:38,14
13295048,[C++][Dataset] Scanning a Fragment does not take into account the partition columns,"Follow-up on ARROW-8061, the {{to_table}} method doesn't work for fragments created from a partitioned dataset.

(will add a reproducer later)

cc [~bkietz]",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-03-30 16:35:55,6
13295043,"[Python][Docs] Review Feather + IPC file documentation per ""Feather V2"" changes",Bring documentation up to date with what's in master,pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2020-03-30 16:23:52,14
13295041,"[C++] Use LZ4 frame format for ""LZ4"" compression in IPC write",Currently the non-frame format is being used. ,pull-request-available,['C++'],ARROW,Bug,Major,2020-03-30 16:22:50,2
13294973,[CI][Python] Test failure on Ubuntu 16.04,See https://github.com/pitrou/arrow/runs/545291564,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Critical,2020-03-30 12:21:54,3
13294967,[Packaging] Allow wheel upload failures to gemfury,"If we run multiple nightly/scheduled jobs per day for the same arrow commit then gemfury's API will refuse the upload because of conflicting versions, see [build|https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=9053&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=b525c197-f769-5e52-d38a-e6301f5260f2&l=27].

Sadly gemfury doesn't have a force update like parameter.",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-03-30 11:53:41,3
13294888,"[Python] Failure in ""nopandas"" build in test_parquet_row_group_fragments",https://github.com/apache/arrow/runs/544301393,pull-request-available,['Python'],ARROW,Bug,Major,2020-03-30 03:52:32,14
13294886,[Ruby] Test failure due to lack of built ZSTD support,"This was uncovered after the boost_ep issue was resolved

https://github.com/apache/arrow/runs/544301326

{code}
===============================================================================
Error: test: compression(TestTable::instance methods::#write_as_feather): Arrow::Error::NotImplemented: [feather-write-file]: NotImplemented: ZSTD codec support not built
/var/lib/gems/2.3.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/loader.rb:600:in `invoke'
/var/lib/gems/2.3.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/loader.rb:600:in `invoke'
/var/lib/gems/2.3.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/loader.rb:514:in `block in define_method'
/arrow/c_glib/test/test-table.rb:264:in `block (3 levels) in <class:TestTable>'
/arrow/c_glib/test/test-table.rb:234:in `setup'
{code}",pull-request-available,['Ruby'],ARROW,Bug,Major,2020-03-30 03:44:15,1
13294867,[CI][GLib] Failed to build on Ubuntu 16.04,Because Meson 0.54.0 requires Ninja 1.7 but Ubuntu 16.04 provides Ninja 1.5.1 by default.,pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Improvement,Major,2020-03-29 21:10:58,1
13294866,[C++] Add backup mirrors for external project source downloads,"As we've seen a number of times, most recently with boost, our builds sometimes fail because of a failure to download bundled dependencies. To reduce this risk, we can add alternate URLs to the cmake externalprojects, so that it will attempt to download from the second location if the first fails (https://cmake.org/cmake/help/latest/module/ExternalProject.html). This feature is available in cmake >=3.7. ",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-03-29 20:47:33,6
13294863,[Rust] [DataFusion] Create utility for printing record batches,"It is too difficult to write examples that print record batches and it would be good to have a utility method to print a batch or to get rows from a batch as a Vec<String>. We already have code in the CSV writer that could be repurposed.

Another option is to modify the csv writer to be able to print to a string rather than a file.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-03-29 19:05:07,10
13294850,[Rust] [DataFusion] LogicalPlanBuilder.limit() should take a literal argument,"LogicalPlanBuilder.limit() should take a literal argument rather than requiring an expression representing a literal value, or maybe we have two versions of this method.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-03-29 17:55:22,9
13294847,[Rust] [DataFusion] ProjectionPushDownRule does not rewrite LIMIT,ProjectionPushDownRule does not rewrite LIMIT,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-03-29 17:38:22,10
13294844,[Rust] [DataFusion] Update CLI documentation for 0.17.0 release,Update CLI documentation for 0.17.0 release,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-03-29 17:07:37,10
13294843,[Rust] [DataFusion] COUNT(*) results in confusing error,COUNT(*) is not supported and results in a confusing error. We should implement this support or at least provide an error saying that it isn't supported.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-03-29 17:02:05,10
13294706,[Python] pandas.ExtensionDtype does not survive round trip with write_to_dataset,"write_to_dataset with pandas fields using pandas.ExtensionDtype nullable int or string produce parquet file which when read back in has different dtypes than original df
{code:java}
import pandas as pd 
import pyarrow as pa 
import pyarrow.parquet as pq 
parquet_dataset = 'partquet_dataset/' 
parquet_file = 'test.parquet' 

df = pd.DataFrame([{'str_col':'abc','int_col':1,'part':1}, {'str_col':np.nan,'int_col':np.nan,'part':1}]) 
df['str_col'] = df['str_col'].astype(pd.StringDtype()) 
df['int_col'] = df['int_col'].astype(pd.Int64Dtype()) 

table = pa.Table.from_pandas(df) 

pq.write_to_dataset(table, root_path=parquet_dataset, partition_cols=['part'] ) pq.write_table(table, where=parquet_file) {code}
write_table handles schema correctly, pandas.ExtensionDtype survive round trip:
{code:java}
pq.read_table(parquet_file).to_pandas().dtypes 
str_col string 
int_col Int64 
part int64 {code}
However, write_to_dataset reverts back to object/float:
{code:java}
pq.read_table(parquet_dataset).to_pandas().dtypes 
str_col object 
int_col float64 
part category {code}
I have also tried writing common metadata at the top-level directory of a partitioned dataset and then passing metadata to read_table, but results are the same as without metadata
{code:java}
pq.write_metadata(table.schema, parquet_dataset+'_common_metadata', version='2.0') meta = pq.read_metadata(parquet_dataset+'_common_metadata') pq.read_table(parquet_dataset,metadata=meta).to_pandas().dtypes {code}
This also affects pandas to_parquet when partition_cols is specified:
{code:java}
df.to_parquet(path = parquet_dataset, partition_cols=['part']) pd.read_parquet(parquet_dataset).dtypes 
str_col object 
int_col float64 
part category {code}
",pull-request-available,['Python'],ARROW,Bug,Major,2020-03-28 10:19:17,5
13294572,[Rust] [DataFusion] Make Table and LogicalPlanBuilder APIs more consistent,"We now have two similar APIs with Table and LogicalPlanBuilder and although they are similar, there are some differences and it would be good to unify them. There is also code duplication and it most likely makes sense for the Table API to delegate to the query builder API to build logical plans.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-03-27 23:19:12,10
13294535,"[C++] Add -Wa,-mbig-obj when compiling with MinGW to avoid linking errors","See https://digitalkarabela.com/mingw-w64-how-to-fix-file-too-big-too-many-sections/

This seems to be the MinGW equivalent of {{/bigobj}} in MSVC",pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-27 18:44:57,14
13294485,"[Python][Parquet] Add `write_to_dataset` option to populate the ""file_path"" metadata fields","Prior to [dask#6023|[https://github.com/dask/dask/pull/6023]], Dask has been using the `write_to_dataset` API to write partitioned parquet datasets. This PR is switching to a (hopefully temporary) custom solution, because that API makes it difficult to populate the the ""file_path"" column-chunk metadata fields that are returned within the optional `metadata_collector` kwarg.Dask needs to set these fields correctly in order to generate a proper global `""_metadata""` file.

Possible solutions to this problem:
 # Optionally populate the file-path fields within`write_to_dataset`
 # Always populate the file-path fields within`write_to_dataset`
 # Return the file paths for the data written within`write_to_dataset` (up to the user to manually populate the file-path fields)",parquet pull-request-available,['Python'],ARROW,Wish,Minor,2020-03-27 14:51:34,5
13294474,[Rust] [DataFusion] Fix inconsistent API in LogicalPlanBuilder,LogicalPlanBuilder project method takes a &Vec whereas other methods take a Vec. It makes sense to take Vec and take ownership of these inputs since they are being used to build the plan.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2020-03-27 14:01:35,10
13294470,[C++] Flight fails to compile on GCC 4.8,See recent build log https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=8944&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=2186,pull-request-available,['C++'],ARROW,Improvement,Blocker,2020-03-27 13:51:57,3
13294468,[Rust] Add convenience methods to Schema,"I would like to add the following methods to Schema to make it easier to work with.


{code:java}
pub fn field_with_name(&self, name: &str) -> Result<&Field>;

pub fn index_of(&self, name: &str) -> Result<usize>;
{code}",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-03-27 13:46:40,10
13294314,[Python] Review Developer build instructions for conda and non-conda users,See discussion in ARROW-3329. These instructions should be reviewed and either fixed (if possible) or removed if they cannot be maintained,pull-request-available,['Python'],ARROW,Bug,Major,2020-03-26 22:14:11,14
13294275,"[CI] Build timeouts on ""AMD64 Windows MinGW 64 GLib & Ruby ""","See for example:
https://github.com/apache/arrow/runs/535319644?check_suite_focus=true
https://github.com/apache/arrow/runs/535245619?check_suite_focus=true
",pull-request-available,"['C', 'Continuous Integration', 'GLib', 'Ruby']",ARROW,Bug,Critical,2020-03-26 18:39:42,1
13294261,[Python] Deprecate pa.open_file and pa.open_stream in favor of pa.ipc.open_file/open_stream,Thought we did this already. For the avoidance of ambiguity we should encourage the use of {{pyarrow.ipc.open_file}} instead of {{pyarrow.open_file}}. So the latter will print a deprecation warning. Since many people use these methods we'd need to keep the deprecated versions around for a while,pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-26 17:42:58,14
13294164,[Java] Move ArrowBuf into the Arrow package,"After ARROW-7505 and ARROW-7935 are done, we are ready to move ArrowBuf into Arrow's package, and make it independent of Netty library.",pull-request-available,['Java'],ARROW,Improvement,Major,2020-03-26 10:15:14,7
13294035,[Python] Schema.from_pandas breaks with pandas nullable integer dtype,"
{code:java}
import pandas as pd
import pyarrow as pa
df = pd.DataFrame([{'int_col':1},
 {'int_col':2}])
df['int_col'] = df['int_col'].astype(pd.Int64Dtype())

schema = pa.Schema.from_pandas(df)
{code}
producesArrowTypeError: Did not pass numpy.dtype object



However, this works fine
{code:java}
schema = pa.Table.from_pandas(df).schema{code}",easyfix,['Python'],ARROW,Bug,Minor,2020-03-25 22:41:46,8
13294033,[C++] Use bcp to make a slim boost for bundled build,"We don't use much of Boost (just system, filesystem, and regex), but when we do a bundled build, we still download and extract all of boost. The tarball itself is 113mb, expanded is over 700mb. This can be slow, and it requires a lot of free disk space that we don't really need.

[bcp|https://www.boost.org/doc/libs/1_72_0/tools/bcp/doc/html/index.html] is a boost tool that lets you extract a subset of boost, resolving any of its necessary dependencies across boost. The savings for us could be huge:

{code}
mkdir test
./bcp system.hpp filesystem.hpp regex.hpp test
tar -czf test.tar.gz test/
{code}

The resulting tarball is 885K (kilobytes!). 

{{bcp}} also lets you re-namespace, so this would (IIUC) solve ARROW-4286 as well.

We would need a place to host this tarball, and we would have to updated it whenever we (1) bump the boost version or (2) add a new boost library dependency. This patch would of course include a script that would generate the tarball. Given the small size, we could also consider just vendoring it.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-25 22:34:11,4
13293922,[Python][Dataset] Expose schema inference / validation options in the factory,"ARROW-8058 added options related to schema inference / validation for the Dataset factory. We should expose this in Python in the {{dataset(..)}} factory function:

- Add ability to pass a user-specified schema with a {{schema}} keyword, instead of inferring the schema from (one of) the files (to be passed to the factory finish method)
- Add {{validate_schema}} option to toggle whether the schema is validated against the actual files or not.
- Expose in some way the number of fragments to be inspected when inferring or validating the schema. Not sure yet what the best API for this would be. 

Some relevant notes from the original PR: https://github.com/apache/arrow/pull/6687#issuecomment-604394407",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-25 19:56:49,5
13293908,[Python] Make dataset FileFormat objects serializable,"Similar to ARROW-8060, ARROW-8059, also the FileFormats need to be pickleable.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-25 19:08:50,3
13293888,[C++] Parallelize decompression at field level in experimental IPC compression code,"This is follow up work to ARROW-7979, a minor amount of refactoring will be required to move the decompression step out of {{ArrayLoader}}",pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-25 17:39:29,14
13293886,[R][C++] Fix crashing test in test-dataset.R on 32-bit Windows from ARROW-7979,If we can obtain a gdb backtrace from the failed test in https://github.com/apache/arrow/pull/6638 then we can sort out what's wrong. ,pull-request-available,"['C++', 'R']",ARROW,Bug,Major,2020-03-25 17:37:37,4
13293884,[R][C++][Dataset] Filtering returns all-missing rows where the filtering column is missing,"

I have just noticed some slightly odd behaviour with the filter method for Dataset.


{code:java}
library(arrow)
library(dplyr)
packageVersion(""arrow"")
#> [1] '0.16.0.20200323'
## Make sample parquet
starwars$hair_color[starwars$hair_color == ""brown""] <- """"
dir <- tempdir()
fpath <- file.path(dir, ""data.parquet"")
write_parquet(starwars, fpath)
## df in memory
df_mem <- starwars %>%
 filter(hair_color == """")
## reading from the parquet
df_parquet <- read_parquet(fpath) %>%
 filter(hair_color == """")
## using open_dataset
df_dataset <- open_dataset(dir) %>%
 filter(hair_color == """") %>%
 collect()
identical(df_mem, df_parquet)
#> [1] TRUE
identical(df_mem, df_dataset)
#> [1] FALSE
{code}




I'm pretty sure all these should return the same data.frame. Am I missing something?

",pull-request-available,['R'],ARROW,Bug,Minor,2020-03-25 17:26:15,6
13293868,[CI][GLib] Meson install fails in the macOS build,"It also happens in the pull request builds, see build log https://github.com/apache/arrow/runs/533168517#step:5:1230

cc @kou",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Improvement,Major,2020-03-25 16:39:20,1
13293851,[Python][Dataset] Opening a dataset with a local incorrect path gives confusing error message,"Even after the previous PRs related to local paths (https://github.com/apache/arrow/pull/6643, https://github.com/apache/arrow/pull/6655), I don't think the user experience optimal in case you are working with local files, and pass a wrong, non-existent path (eg due to a typo).

Currently, you get this error:

{code}
>>> dataset = ds.dataset(""data_with_typo.parquet"", format=""parquet"")
...
ArrowInvalid: URI has empty scheme: 'data_with_typo.parquet'
{code}

where ""URI has empty scheme"" is rather confusing for the user in case of a non-existent path.  I think ideally we should raise a ""No such file or directory"" error.

I am not fully sure what the best solution is, as {{FileSystem.from_uri}} can also give other errors that we do want to propagate to the user. 
The most straightforward that I am now thinking of is checking if ""URI has empty scheme"" is in the error message, and then rewording it, but that's not very clean ..",dataset pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-03-25 15:33:01,3
13293839,[Python] Accessing duplicate column of Table by name gives wrong error,"When you have a table with duplicate column names and you try to access this column, you get an error about the column not existing:

{code}
>>> table = pa.table([pa.array([1, 2, 3]), pa.array([4, 5, 6]), pa.array([7, 8, 9])], names=['a', 'b', 'a']) 

>>> table.column('a')                                                                                                                                                                                          
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-6-14fad86d3142> in <module>
----> 1 table.column('a')

~/scipy/repos/arrow/python/pyarrow/table.pxi in pyarrow.lib.Table.column()

KeyError: 'Column a does not exist in table'
{code}

It should rather give an error message about the column name being duplicate.",beginner pull-request-available,['Python'],ARROW,Bug,Minor,2020-03-25 14:58:05,14
13293687,[R] Minor fix for backwards compatibility on Linux installation,"In 0.16, the recommendation was to set {{LIBARROW_DOWNLOAD=true}} to install with dependencies, and this would include getting a binary. But the recent refactor to Linux installation didn't carry this setting forward correctly.",pull-request-available,['R'],ARROW,Bug,Minor,2020-03-25 00:54:03,4
13293681,[Rust] [DataFusion] DataFusion should enforce unique field names in a schema,"There does not seem to be any validation to avoid schemas being created with duplicate field names. We should add this along with unit tests.

This will require changing the signature of the constructors to try_new with a Result return type.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-03-25 00:31:07,9
13293678,"[C#] ""dotnet pack"" is failed","https://pipelines.actions.githubusercontent.com/CKhEcbzaHKPSiCoVWyWCFardoFMA9UN2zDnXYiBlRYO31o0IIv/_apis/pipelines/1/runs/47768/signedlogcontent/5?urlExpires=2020-03-24T23%3A57%3A59.7937731Z&urlSigningMethod=HMACV1&urlSignature=xtA9W3a9mJgo8WU3mKXh3r5dmqN9PgWM3gkfMEP5NxQ%3D

{noformat}
2020-03-24T23:52:40.0535661Z ....E
2020-03-24T23:52:40.0536281Z ===============================================================================
2020-03-24T23:52:40.0536773Z Error: test_csharp_git_commit_information(SourceTest):
2020-03-24T23:52:40.0538133Z   CommandRunnable::Error: Failed to run: dotnet pack -c Release
2020-03-24T23:52:40.0538277Z   stdout:
2020-03-24T23:52:40.0538379Z    
2020-03-24T23:52:40.0538501Z   Welcome to .NET Core 3.1!
2020-03-24T23:52:40.0538779Z   ---------------------
2020-03-24T23:52:40.0538903Z   SDK Version: 3.1.200
2020-03-24T23:52:40.0539022Z   
2020-03-24T23:52:40.0539139Z   Telemetry
2020-03-24T23:52:40.0539388Z   ---------
2020-03-24T23:52:40.0539984Z   The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to '1' or 'true' using your favorite shell.
2020-03-24T23:52:40.0540205Z   
2020-03-24T23:52:40.0540591Z   Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry
2020-03-24T23:52:40.0540744Z   
2020-03-24T23:52:40.0540993Z   ----------------
2020-03-24T23:52:40.0541293Z   Explore documentation: https://aka.ms/dotnet-docs
2020-03-24T23:52:40.0541445Z   Report issues and find source on GitHub: https://github.com/dotnet/core
2020-03-24T23:52:40.0541776Z   Find out what's new: https://aka.ms/dotnet-whats-new
2020-03-24T23:52:40.0542133Z   Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https
2020-03-24T23:52:40.0542506Z   Use 'dotnet --help' to see available commands or visit: https://aka.ms/dotnet-cli-docs
2020-03-24T23:52:40.0542842Z   Write your first app: https://aka.ms/first-net-core-app
2020-03-24T23:52:40.0543189Z   --------------------------------------------------------------------------------------
2020-03-24T23:52:40.0543339Z   Microsoft (R) Build Engine version 16.5.0+d4cbfca49 for .NET Core
2020-03-24T23:52:40.0543763Z   Copyright (C) Microsoft Corporation. All rights reserved.
2020-03-24T23:52:40.0543894Z   
2020-03-24T23:52:40.0544332Z     Restore completed in 10.5 sec for /tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/test/Apache.Arrow.Tests/Apache.Arrow.Tests.csproj.
2020-03-24T23:52:40.0544753Z     Restore completed in 11.52 sec for /tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj.
2020-03-24T23:52:40.0545260Z     Restore completed in 4.4 sec for /tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/test/Apache.Arrow.Benchmarks/Apache.Arrow.Benchmarks.csproj.
2020-03-24T23:52:40.0545871Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: The ""Microsoft.Build.Tasks.Git.LocateRepository"" task failed unexpectedly. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0546749Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: System.TypeInitializationException: The type initializer for 'Microsoft.Build.Tasks.Git.TaskImplementation' threw an exception. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0547603Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  ---> System.IO.FileNotFoundException: Could not load file or assembly 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35'. The system cannot find the file specified. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0548212Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0548884Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: File name: 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35' [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0549663Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  ---> System.IO.FileNotFoundException: Could not load file or assembly 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35'. The system cannot find the file specified. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0550234Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0551270Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: File name: 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35' [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0552054Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.nLoad(AssemblyName fileName, String codeBase, RuntimeAssembly assemblyContext, StackCrawlMark& stackMark, Boolean throwOnFileNotFound, AssemblyLoadContext assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0552932Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.InternalLoadAssemblyName(AssemblyName assemblyRef, StackCrawlMark& stackMark, AssemblyLoadContext assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0553857Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.Assembly.Load(AssemblyName assemblyRef, StackCrawlMark& stackMark, AssemblyLoadContext assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0554639Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Runtime.Loader.AssemblyLoadContext.LoadFromAssemblyName(AssemblyName assemblyName) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0555559Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.Tasks.Git.GitLoaderContext.Load(AssemblyName assemblyName) in /_/src/Microsoft.Build.Tasks.Git/GitLoaderContext.cs:line 24 [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0556412Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Runtime.Loader.AssemblyLoadContext.ResolveUsingLoad(AssemblyName assemblyName) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0557311Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Runtime.Loader.AssemblyLoadContext.Resolve(IntPtr gchManagedAssemblyLoadContext, AssemblyName assemblyName) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0557882Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0558415Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0559183Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.GetType(QCallAssembly assembly, String name, Boolean throwOnError, Boolean ignoreCase, ObjectHandleOnStack type, ObjectHandleOnStack keepAlive, ObjectHandleOnStack assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0559872Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.GetType(String name, Boolean throwOnError, Boolean ignoreCase) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0560518Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.Assembly.GetType(String name, Boolean throwOnError) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0561191Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.Tasks.Git.TaskImplementation..cctor() in /_/src/Microsoft.Build.Tasks.Git/TaskImplementation.cs:line 35 [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0561864Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0562399Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0563228Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    --- End of inner exception stack trace --- [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0563989Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.Tasks.Git.LocateRepository.Execute() in /_/src/Microsoft.Build.Tasks.Git/LocateRepository.cs:line 18 [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0564669Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.BackEnd.TaskExecutionHost.Microsoft.Build.BackEnd.ITaskExecutionHost.Execute() [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0565472Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.BackEnd.TaskBuilder.ExecuteInstantiatedTask(ITaskExecutionHost taskExecutionHost, TaskLoggingContext taskLoggingContext, TaskHost taskHost, ItemBucket bucket, TaskExecutionMode howToExecuteTask) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0566154Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: The ""Microsoft.Build.Tasks.Git.LocateRepository"" task failed unexpectedly. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0566886Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: System.TypeInitializationException: The type initializer for 'Microsoft.Build.Tasks.Git.TaskImplementation' threw an exception. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0567717Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  ---> System.IO.FileNotFoundException: Could not load file or assembly 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35'. The system cannot find the file specified. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0568320Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0569001Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: File name: 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35' [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0570000Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  ---> System.IO.FileNotFoundException: Could not load file or assembly 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35'. The system cannot find the file specified. [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0570580Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0571498Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018: File name: 'Microsoft.Build.Tasks.Git, Version=1.0.0.27001, Culture=neutral, PublicKeyToken=31bf3856ad364e35' [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0572686Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.nLoad(AssemblyName fileName, String codeBase, RuntimeAssembly assemblyContext, StackCrawlMark& stackMark, Boolean throwOnFileNotFound, AssemblyLoadContext assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0575446Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.InternalLoadAssemblyName(AssemblyName assemblyRef, StackCrawlMark& stackMark, AssemblyLoadContext assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0577652Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.Assembly.Load(AssemblyName assemblyRef, StackCrawlMark& stackMark, AssemblyLoadContext assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0580392Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Runtime.Loader.AssemblyLoadContext.LoadFromAssemblyName(AssemblyName assemblyName) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0581164Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.Tasks.Git.GitLoaderContext.Load(AssemblyName assemblyName) in /_/src/Microsoft.Build.Tasks.Git/GitLoaderContext.cs:line 24 [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0581895Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Runtime.Loader.AssemblyLoadContext.ResolveUsingLoad(AssemblyName assemblyName) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0583013Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Runtime.Loader.AssemblyLoadContext.Resolve(IntPtr gchManagedAssemblyLoadContext, AssemblyName assemblyName) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0583626Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0587571Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0588404Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.GetType(QCallAssembly assembly, String name, Boolean throwOnError, Boolean ignoreCase, ObjectHandleOnStack type, ObjectHandleOnStack keepAlive, ObjectHandleOnStack assemblyLoadContext) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0589252Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.RuntimeAssembly.GetType(String name, Boolean throwOnError, Boolean ignoreCase) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0590105Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at System.Reflection.Assembly.GetType(String name, Boolean throwOnError) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0590783Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.Tasks.Git.TaskImplementation..cctor() in /_/src/Microsoft.Build.Tasks.Git/TaskImplementation.cs:line 35 [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0591350Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0591904Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:  [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0592493Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    --- End of inner exception stack trace --- [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0593174Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.Tasks.Git.LocateRepository.Execute() in /_/src/Microsoft.Build.Tasks.Git/LocateRepository.cs:line 18 [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0594037Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.BackEnd.TaskExecutionHost.Microsoft.Build.BackEnd.ITaskExecutionHost.Execute() [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0594843Z   /home/runner/.nuget/packages/microsoft.build.tasks.git/1.0.0-beta2-19270-01/build/Microsoft.Build.Tasks.Git.targets(16,5): error MSB4018:    at Microsoft.Build.BackEnd.TaskBuilder.ExecuteInstantiatedTask(ITaskExecutionHost taskExecutionHost, TaskLoggingContext taskLoggingContext, TaskHost taskHost, ItemBucket bucket, TaskExecutionMode howToExecuteTask) [/tmp/d20200324-5667-vnxt4i/apache-arrow-1.0.0/csharp/src/Apache.Arrow/Apache.Arrow.csproj]
2020-03-24T23:52:40.0595075Z   
2020-03-24T23:52:40.0595304Z   stderr:
2020-03-24T23:52:40.0595409Z    
2020-03-24T23:52:40.0595762Z /home/runner/work/arrow/arrow/dev/release/test-helper.rb:45:in `sh'
2020-03-24T23:52:40.0596157Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:71:in `block in test_csharp_git_commit_information'
2020-03-24T23:52:40.0596317Z      68:     source
2020-03-24T23:52:40.0596453Z      69:     Dir.chdir(""#{@tag_name}/csharp"") do
2020-03-24T23:52:40.0596592Z      70:       FileUtils.mv(""dummy.git"", ""../.git"")
2020-03-24T23:52:40.0597238Z   => 71:       sh(""dotnet"", ""pack"", ""-c"", ""Release"")
2020-03-24T23:52:40.0597555Z      72:       FileUtils.mv(""../.git"", ""dummy.git"")
2020-03-24T23:52:40.0597755Z      73:       Dir.chdir(""artifacts/Apache.Arrow/Release"") do
2020-03-24T23:52:40.0597907Z      74:         sh(""unzip"", ""Apache.Arrow.#{@snapshot_version}.nupkg"")
2020-03-24T23:52:40.0598252Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:69:in `chdir'
2020-03-24T23:52:40.0598629Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:69:in `test_csharp_git_commit_information'
2020-03-24T23:52:40.0599088Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:30:in `block (2 levels) in setup'
2020-03-24T23:52:40.0599455Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:29:in `chdir'
2020-03-24T23:52:40.0599809Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:29:in `block in setup'
2020-03-24T23:52:40.0600165Z /opt/hostedtoolcache/Ruby/2.6.5/x64/lib/ruby/2.6.0/tmpdir.rb:93:in `mktmpdir'
2020-03-24T23:52:40.0600505Z /home/runner/work/arrow/arrow/dev/release/02-source-test.rb:28:in `setup'
2020-03-24T23:52:40.1909864Z ===============================================================================
{noformat}",pull-request-available,['C#'],ARROW,Improvement,Major,2020-03-25 00:15:14,1
13293628,"[GLib] Rename garrow_file_system_target_info{,s}() to ..._file_info{,s}()",Because C++ renamed them by ARROW-8145.,pull-request-available,['GLib'],ARROW,Improvement,Minor,2020-03-24 20:27:59,1
13293624,[C++] Add support for multi-column sort on Table,"I'm just coming up to speed with Arrow and am noticing a dearth of examples ... maybe I can help here.

I'd like to implement multi-column sorting for Tables and just want to ensure that I'm not duplicating existing work or proposing a bad design.

My thought was to create a Table-specific version of SortToIndices() where you can specify the columns and sort order.

Then I'd create Array ""views"" that use the Indices to remap from the original Array values to the values in sorted order. (Original data is not sorted, but could be as a second step.) I noticed some of the array list variants keep offsets, but didn't see anything that supports remapping per a list of indices, but this may just be my oversight?

Thanks in advance, Scott",pull-request-available,['C++'],ARROW,Wish,Minor,2020-03-24 19:58:24,1
13293605,[C++] Diffing should handle null arrays,"When {{AssertArraysEqual}} fails between arrays of type null, you get the rather unhelpful message:
{code}
../src/arrow/compare.cc:964:  Check failed: _s.ok() Operation failed: PrintDiff(left, right, opts.diff_sink())
Bad status: NotImplemented: formatting diffs between arrays of type null
In ../src/arrow/array/diff.cc, line 453, code: VisitTypeInline(type, this)
In ../src/arrow/array/diff.cc, line 825, code: (_error_or_value10).status()
In ../src/arrow/compare.cc, line 955, code: (_error_or_value4).status()
{code}",pull-request-available,['C++'],ARROW,Improvement,Trivial,2020-03-24 17:18:20,6
13293591,"[Rust] DataFusion ""create_physical_plan"" returns incorrect schema?","I am using DataFusion in a situation where I know there will only be a single file. DataFusion currently collects all batches into a vector.

As I am writing the data back out I want to work with an iterator instead of a vector.

I have something as follows:
{code:java}
let plan = ctx.create_logical_plan(&sql).unwrap();
let plan = ctx.optimize(&plan).unwrap();
dbg!(plan.schema());  // Returns field names
let plan = ctx.create_physical_plan(&plan, batch_size).unwrap();
dbg!(plan.schema()); // Returns c0, c1, etc{code}
Maybe this is expected after turning the plan into a physical plan?

I can change the schema of the returned batches, would this be the recommended way to address this or is there something in DataFusion I should leverage to do this?",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Minor,2020-03-24 16:10:27,10
13293508,[CI] Remove Boost download step in Github Actions,"According to https://github.com/actions/virtual-environments/issues/370, the full version of Boost should now be properly installed on the GHA Windows 2019 image. We should try to remove the Boost download step, which is quite slow (it installs 2GB worth of files).",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Minor,2020-03-24 10:47:41,1
13293502,[CI] Github Actions Windows job should run tests in parallel,"Currently, the GHA Windows job runs tests using {{-j 1}}. But IIRC GHA exposes two CPU cores.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Wish,Trivial,2020-03-24 10:40:30,2
13293424,[C++] arrow-future-test fails to compile on gcc 4.8,"{code}
In file included from /usr/include/c++/4.8/memory:64:0,
                 from /home/wesm/code/arrow/cpp/src/arrow/util/future.h:22,
                 from /home/wesm/code/arrow/cpp/src/arrow/util/future_test.cc:18:
/usr/include/c++/4.8/bits/stl_construct.h: In instantiation of void std::_Construct(_T1*, _Args&& ...) [with _T1 = arrow::MoveOnlyDataType; _Args = {const arrow::MoveOnlyDataType&}]:
/usr/include/c++/4.8/bits/stl_uninitialized.h:75:53:   required from static _ForwardIterator std::__uninitialized_copy<_TrivialValueTypes>::__uninit_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = __gnu_cxx::__normal_iterator<const arrow::MoveOnlyDataType*, std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> > >; _ForwardIterator = arrow::MoveOnlyDataType*; bool _TrivialValueTypes = false]
/usr/include/c++/4.8/bits/stl_uninitialized.h:117:41:   required from _ForwardIterator std::uninitialized_copy(_InputIterator, _InputIterator, _ForwardIterator) [with _InputIterator = __gnu_cxx::__normal_iterator<const arrow::MoveOnlyDataType*, std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> > >; _ForwardIterator = arrow::MoveOnlyDataType*]
/usr/include/c++/4.8/bits/stl_uninitialized.h:258:63:   required from _ForwardIterator std::__uninitialized_copy_a(_InputIterator, _InputIterator, _ForwardIterator, std::allocator<_Tp>&) [with _InputIterator = __gnu_cxx::__normal_iterator<const arrow::MoveOnlyDataType*, std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> > >; _ForwardIterator = arrow::MoveOnlyDataType*; _Tp = arrow::MoveOnlyDataType]
/usr/include/c++/4.8/bits/stl_vector.h:316:32:   required from std::vector<_Tp, _Alloc>::vector(const std::vector<_Tp, _Alloc>&) [with _Tp = arrow::MoveOnlyDataType; _Alloc = std::allocator<arrow::MoveOnlyDataType>]
/home/wesm/code/arrow/cpp/src/arrow/result.h:417:5:   required from void arrow::Result<T>::ConstructValue(U&&) [with U = std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> >&; T = std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> >]
/home/wesm/code/arrow/cpp/src/arrow/result.h:167:42:   required from arrow::Result<T>::Result(U&&) [with U = std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> >&; E = void; T = std::vector<arrow::MoveOnlyDataType, std::allocator<arrow::MoveOnlyDataType> >]
/home/wesm/code/arrow/cpp/src/arrow/util/iterator.h:159:12:   required from arrow::Result<std::vector<FutureType> > arrow::Iterator<T>::ToVector() [with T = arrow::MoveOnlyDataType]
/home/wesm/code/arrow/cpp/src/arrow/testing/gtest_util.h:419:3:   required from std::vector<FutureType> arrow::IteratorToVector(arrow::Iterator<T>) [with T = arrow::MoveOnlyDataType]
/home/wesm/code/arrow/cpp/src/arrow/util/future_test.cc:610:61:   required from void arrow::FutureTestBase<T>::TestBasicAsCompleted() [with T = arrow::MoveOnlyDataType]
/home/wesm/code/arrow/cpp/src/arrow/util/future_test.cc:708:52:   required from void arrow::FutureIteratorTest_BasicAsCompleted_Test<gtest_TypeParam_>::TestBody() [with gtest_TypeParam_ = arrow::MoveOnlyDataType]
/home/wesm/code/arrow/cpp/src/arrow/type.h:756:20:   required from here
/usr/include/c++/4.8/bits/stl_construct.h:75:7: error: use of deleted function arrow::MoveOnlyDataType::MoveOnlyDataType(const arrow::MoveOnlyDataType&)
     { ::new(static_cast<void*>(__p)) _T1(std::forward<_Args>(__args)...); }
       ^
/home/wesm/code/arrow/cpp/src/arrow/util/future_test.cc:70:3: error: declared here
   MoveOnlyDataType(const MoveOnlyDataType& other) = delete;
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-24 02:04:01,14
13293413,[Packaging][APT] Fix cmake removal in Debian GNU/Linux Stretch,"https://pipelines.actions.githubusercontent.com/vq3K6L47kuiwb0u8UPIOur4viIWCTlJTS2oa3tJBsNRPmNG4Rr/_apis/pipelines/1/runs/543/signedlogcontent/3?urlExpires=2020-03-24T00%3A32%3A47.1136259Z&urlSigningMethod=HMACV1&urlSignature=v6IKrOVpNsPWF0FtAP2lMUFZOgTw5qcKJuOdP5k8Z5s%3D

{noformat}
2020-03-23T08:02:46.0936180Z The following packages will be REMOVED:
2020-03-23T08:02:46.0957074Z    cmake (3.7.2-1)
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-03-24 00:43:05,1
13293411,[C++][Flight] Allow setting IpcWriteOptions and IpcReadOptions in Flight IPC message reader and writer classes,Follow up work to ARROW-7979,pull-request-available,"['C++', 'FlightRPC']",ARROW,New Feature,Major,2020-03-24 00:29:19,0
13293329,[R] Adapt to latest checks in R-devel,"See https://github.com/ursa-labs/crossbow/runs/526813242 for example.

1. checkbashisms now is complaining about a few things
2. Latest R-devel actually runs the donttest examples with --as-cran, and one fails. ",pull-request-available,['R'],ARROW,Bug,Blocker,2020-03-23 15:49:54,4
13293328,[R] Make test assertions robust to i18n,"{code}
 1. Failure: codec_is_available (@test-compressed.R#22)  
`codec_is_available(""sdfasdf"")` threw an error with unexpected message.
Expected match: ""'arg' should be one of""
Actual message: ""'arg' doit tre un de UNCOMPRESSED, SNAPPY, GZIP, BROTLI, ZSTD, LZ4, LZO, BZ2""
Backtrace:
  1. testthat::expect_error(codec_is_available(""sdfasdf""), ""'arg' should be one of"") testthat/test-compressed.R:22:2
  6. arrow::codec_is_available(""sdfasdf"")
  8. arrow:::compression_from_name(type)
  9. purrr::map_int(...)
 10. arrow:::.f(.x[[i]], ...)
 11. base::match.arg(toupper(.x), names(CompressionType))

 2. Failure: time type unit validation (@test-data-type.R#298)  
`time32(""years"")` threw an error with unexpected message.
Expected match: ""'arg' should be one of""
Actual message: ""'arg' doit tre un de ms, s""
Backtrace:
 1. testthat::expect_error(time32(""years""), ""'arg' should be one of"") testthat/test-data-type.R:298:2
 6. arrow::time32(""years"")
 7. base::match.arg(unit)

 3. Failure: time type unit validation (@test-data-type.R#305)  
`time64(""years"")` threw an error with unexpected message.
Expected match: ""'arg' should be one of""
Actual message: ""'arg' doit tre un de ns, us""
Backtrace:
 1. testthat::expect_error(time64(""years""), ""'arg' should be one of"") testthat/test-data-type.R:305:2
 6. arrow::time64(""years"")
 7. base::match.arg(unit)

 4. Failure: decimal type and validation (@test-data-type.R#387)  
`decimal()` threw an error with unexpected message.
Expected match: ""argument \""precision\"" is missing, with no default""
Actual message: ""l'argument \""precision\"" est manquant, avec aucune valeur par dfaut""
Backtrace:
 1. testthat::expect_error(decimal(), ""argument \""precision\"" is missing, with no default"") testthat/test-data-type.R:387:2
 6. arrow::decimal()

 5. Failure: decimal type and validation (@test-data-type.R#389)  
`decimal(4)` threw an error with unexpected message.
Expected match: ""argument \""scale\"" is missing, with no default""
Actual message: ""l'argument \""scale\"" est manquant, avec aucune valeur par dfaut""
Backtrace:
 1. testthat::expect_error(decimal(4), ""argument \""scale\"" is missing, with no default"") testthat/test-data-type.R:389:2
 6. arrow::decimal(4)
{code}",pull-request-available,['R'],ARROW,Improvement,Minor,2020-03-23 15:47:28,4
13293309,[Python] Dataset expression != returns bool instead of expression for invalid value,"It's a bit a strange case, but eg when doing {{!= \{3\}}} you get a boolean result instead of an expression:

{code}
In [8]: ds.field('col') != 3                                                                                                                                                                                       
Out[8]: <pyarrow.dataset.ComparisonExpression (col != 3:int64)>

In [9]: ds.field('col') != {3}                                                                                                                                                                                     
Out[9]: True
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-03-23 13:45:43,5
13293271,[Packaging] Document the available nightly wheels and conda packages,"The packaging scripts are uploading the artifacts to package manager specific hosting services like Anaconda and Gemfury. We should document this in a form which conforms the [ASF Policy|https://www.apache.org/dev/release-distribution.html#unreleased].

For more information see the conversation at https://github.com/apache/arrow/pull/6669#issuecomment-601947006",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-03-23 10:46:18,3
13293269,[Packaging] Use arrow-nightlies organization name on Anaconda and Gemfury to host the nightlies,"Currently I've set up the scripts to use Ursa Labs's accounts, but we should prefer a more neutral org.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-03-23 10:42:43,3
13293175,[Packaging] Increment the version number detected from the latest git tag,"The packaging artifacts are tagged with the following version numer ""{latest-tag}.dev{distance}"" e.g. 0.16.0.dev1, 0.16.0.dev2, ...

Once we make nightly artifacts available for development purposes depending on the package manager 0.16.0 can be considered newer than 0.16.0.dev1. 

For example to install a nightly wheel via pip the user must pin the exact version of the produced wheel, there are no options to prefer 0.16.0.devN over 0.16.0.

One way to resolve this to increment the version number's minor component, so instead of generating 0.16.0.devN version number after the 0.16 tag, create 0.16.1.devN version which will be considered newer than 0.16.0.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-03-22 15:10:44,3
13293040,[R] Windows build script tweaking for nightly packaging on GHA,"Following ARROW-7675. Nightly R packages are built on https://github.com/ursa-labs/arrow-r-nightly, and in porting that to use GHA too, some minor modifications are needed to the build scripts.",pull-request-available,"['Packaging', 'R']",ARROW,Improvement,Major,2020-03-20 22:31:48,4
13293030,[C++] Upgrade to Flatbuffers 1.12,This will pick up a UBSan fix in https://github.com/google/flatbuffers/pull/5355,pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-20 21:37:02,14
13292991,[FlightRPC][Integration] Have Flight services bind to port 0 in integration,"In integration tests, instead of allocating a port and then trying to bind to it, we should have the Flight server bind to port 0, then have the test runner parse out the port. This avoids flakiness due to port collisions. This also will give us the ability to know when the Flight server has actually started.",pull-request-available,"['FlightRPC', 'Integration']",ARROW,Bug,Major,2020-03-20 18:18:43,0
13292989,[Python]Setup type checking with mypy,"Get mypy checks running, activate things like {{check_untyped_defs}}later.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2020-03-20 17:47:52,8
13292820,[Java] Improve the performance of JDBC adapter by allocating memory proactively,"The current implementation use {{setSafe}} methods to dynamically allocate memory if necessary. For fixed width vectors (which are frequently used in JDBC), however, we can allocate memory proactively, since the vector size is known as a configuration parameter. So for fixed width vectors, we can use {{set}} methods instead.

This change leads to two benefits:
1. When processing each value, we no longer have to check vector capacity and reallocate memroy if needed. This leads to better performance.
2. If we allow the memory to expand automatically (each time by 2x), the amount of memory usually ends up being more than necessary. By allocating memory by the configuration parameter, we allocate no more, or no less. 

Benchmark results show notable performance improvements:

Before:

Benchmark                                   Mode  Cnt    Score   Error  Units
JdbcAdapterBenchmarks.consumeBenchmark      avgt    5  521.700  4.837  us/op

After:

Benchmark                                   Mode  Cnt    Score   Error  Units
JdbcAdapterBenchmarks.consumeBenchmark      avgt    5  430.523  9.932  us/op",pull-request-available,['Java'],ARROW,Improvement,Major,2020-03-20 02:55:11,7
13292764,[CI] Add support for skipping builds with skip pattern in pull request title,Github actions doesn't support to skip builds marked as [skip ci] by default.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-03-19 18:47:20,3
13292732,[C++][Dataset] Let datasets be viewable with non-identical schema,"It would be useful to allow some schema unification capability after discovery has completed. For example, if a FileSystemDataset is being wrapped into a UnionDataset with another and their schemas are unifiable then there is no reason we can't create the UnionDataset (rather than emitting an error because the schemas are not identical).

I think this behavior will be most naturally expressed in C++ like so:

{code}
virtual Result<Dataset> Dataset::ReplaceSchema(std::shared_ptr<Schema> schema) const = 0;
{code}

which will raise an error if the provided schema is not unifiable with the current dataset schema.

If this needs to be extended to non trivial projections then this will probably warrant a separate class, {{ProjectedDataset}} or so. Definitely follow up material (if desired)",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-19 15:55:19,6
13292730,[C++][Dataset] Allow FileSystemDataset's file list to be lazy,"A FileSystemDataset currently requires a full listing of files it contains on construction, so a scan cannot start until all files in the dataset are discovered. Instead it would be ideal if a large dataset could be constructed with a lazy file listing so that scans can start immediately.",dataset,['C++'],ARROW,Improvement,Major,2020-03-19 15:41:54,6
13292614,[Java] Getting length of data buffer and base variable width vector,"For string data buffer and base variable width vector can we have a way to get length of the data?

For instance, in ArrowColumnVector in StringAccessor we use stringResult.start and stringResult.end, instead we would like to get length of the data through an exposed function.",pull-request-available,['Java'],ARROW,Improvement,Minor,2020-03-19 03:45:46,16
13292587,[C++][Gandiva] Support building with LLVM 9,"Now that LLVM 9 has already been released.

LLVM branch 10 has been created on https://apt.llvm.org/
LLVM branch 9 has already been promoted to the old-stable branch.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-18 23:33:08,1
13292557,[Packaging] Update the conda feedstock files and upload artifacts to Anaconda,"The windows builds were failing, so the feedstock files must be updated.

Under the same hat add support for uploading the produced artifacts to Anaconda labeled as nightly.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-03-18 20:16:47,3
13292546,[Benchmarking][Dataset] Benchmark Parquet read performance with S3File,We should establish a performance baseline with the current S3File implementation and Parquet reader before proceeding with work like PARQUET-1698.,dataset pull-request-available,"['Benchmarking', 'C++']",ARROW,Improvement,Major,2020-03-18 18:29:42,0
13292523,[C++/Python] Enable CUDA Support in conda recipes,"See the changes in[https://github.com/conda-forge/arrow-cpp-feedstock/pull/123], we need to copy this into the Arrow repository and also test CUDA in these recipes.",pull-request-available,"['C++', 'Packaging']",ARROW,New Feature,Major,2020-03-18 16:23:51,8
13292478,[C++] Rename GetTargetInfos,"Sorry, but I think I'm irked by the new ""GetTargetInfos"" spelling.
I suggest either ""GetTargetInfo"" or ""GetFileInfo"" (both singular).",pull-request-available,"['C++', 'Python']",ARROW,Wish,Trivial,2020-03-18 13:40:30,2
13292467,[CI] Cmake 3.2 nightly build fails,In the LLVM 8 Migration PR wget was [removed|https://github.com/apache/arrow/commit/58ec1bc3984b8453011ba6ca45c727ff6ceed78c#diff-0a4bf63085865017969bbbdac6f66880L29] so the build is [missing|https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2020-03-18-0-circle-test-ubuntu-18.04-cpp-cmake32] wget.,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-03-18 13:19:55,3
13292404,[C++] Casting a chunked array with 0 chunks critical failure,"When casting a schema of an empty table from dict encoded to non-dict encoded type a critical error is raised and not handled causing the interpreter to shut down.

This only happens after a parquet roundtrip


{code:python}
import pyarrow as pa
import pandas as pd
import pyarrow.parquet as pq

df = pd.DataFrame({""col"": [""a""]}).astype({""col"": ""category""}).iloc[:0]
table = pa.Table.from_pandas(df)
field = table.schema[0]
new_field = pa.field(field.name, field.type.value_type, field.nullable, field.metadata)

buf = pa.BufferOutputStream()
pq.write_table(table, buf)
reader = pa.BufferReader(buf.getvalue().to_pybytes())
table = pq.read_table(reader)

schema = table.schema.remove(0).insert(0, new_field)
new_table = table.cast(schema)
assert new_table.schema == schema
 {code}


Output
{code:java}
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0318 09:55:14.266649 299722176 table.cc:47] Check failed: (chunks.size()) > (0) cannot construct ChunkedArray from empty vector and omitted type {code}
",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-18 08:59:27,6
13292306,[Developer] Follow NullType -> NullField change,The lint CI job is failed since ARROW-8101 merge because ARROW-8101 uses old class name (NullType). The old class name was renamed to NullField by ARROW-2255.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-03-18 00:33:57,1
13292272,[C++] FileSystem enum causes attributes warning,"See e.g. https://github.com/apache/arrow/runs/512427577?check_suite_focus=true#step:7:996

{code}
In file included from /arrow/r/check/arrow.Rcheck/00_pkg_src/arrow/libarrow/arrow-0.16.0.9000/include/arrow/dataset/discovery.h:31:0,
                 from /arrow/r/check/arrow.Rcheck/00_pkg_src/arrow/libarrow/arrow-0.16.0.9000/include/arrow/dataset/api.h:21,
                 from ./arrow_types.h:203,
                 from array_to_vector.cpp:18:
/arrow/r/check/arrow.Rcheck/00_pkg_src/arrow/libarrow/arrow-0.16.0.9000/include/arrow/filesystem/filesystem.h:65:1: warning: type attributes ignored after type is already defined [-Wattributes]
{code}

This isn't new but I've been staring at the R Linux builds a lot and wanted to clean this up.",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-17 20:31:45,4
13292201,[C++][Python] Creating dataset from relative path no longer working,"Since https://github.com/apache/arrow/pull/6597, local relative paths don't work anymore:

{code}
In [1]: import pyarrow.dataset as ds  

In [2]: ds.dataset(""test.parquet"")  
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-2-23ecfce52d13> in <module>
----> 1 ds.dataset(""test.parquet"")

~/scipy/repos/arrow/python/pyarrow/dataset.py in dataset(paths_or_factories, filesystem, partitioning, format)
    327 
    328     if isinstance(paths_or_factories, str):
--> 329         return factory(paths_or_factories, **kwargs).finish()
    330 
    331     if not isinstance(paths_or_factories, list):

~/scipy/repos/arrow/python/pyarrow/dataset.py in factory(path_or_paths, filesystem, partitioning, format)
    246     factories = []
    247     for path in path_or_paths:
--> 248         fs, paths_or_selector = _ensure_fs_and_paths(path, filesystem)
    249         factories.append(FileSystemDatasetFactory(fs, paths_or_selector,
    250                                                   format, options))

~/scipy/repos/arrow/python/pyarrow/dataset.py in _ensure_fs_and_paths(path, filesystem)
    165     from pyarrow.fs import FileType, FileSelector
    166 
--> 167     filesystem, path = _ensure_fs(filesystem, _stringify_path(path))
    168     infos = filesystem.get_target_infos([path])[0]
    169     if infos.type == FileType.Directory:

~/scipy/repos/arrow/python/pyarrow/dataset.py in _ensure_fs(filesystem, path)
    158     if filesystem is not None:
    159         return filesystem, path
--> 160     return FileSystem.from_uri(path)
    161 
    162 

~/scipy/repos/arrow/python/pyarrow/_fs.pyx in pyarrow._fs.FileSystem.from_uri()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: URI has empty scheme: 'test.parquet'

{code}

[~apitrou] Is this something that should be fixed in {{FileSystemFromUriOrPath}} or rather on the python side? ({{FileSystem.from_uri}} ensures to get the absolute path for Pathlib objects, but not for strings)",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-03-17 13:49:33,5
13292029,[CI] Github Actions sometimes fail to checkout Arrow,Failing build https://github.com/apache/arrow/pull/6632/checks?check_run_id=511663097,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-03-16 18:01:38,3
13291961,[C++] arrow-s3fs-test failing on master,"Log:

[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/branch/master/job/9dgr7xl635yuwh7y#L1917]",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-16 12:25:37,2
13291862,"[C++] ""arrow-tests"" target broken with ninja build","{code}
$ ninja arrow-tests
ninja: no work to do.
{code}

According to git bisect this was introduced by 

{code}
$ git bisect bad
7db3855cd4a2e2f704b8715af3a36cbef0bb2a27 is the first bad commit
commit 7db3855cd4a2e2f704b8715af3a36cbef0bb2a27
Author: Benjamin Kietzman <bengilgit@gmail.com>
Date:   Mon Mar 9 16:40:21 2020 +0100

    ARROW-8014: [C++] Provide CMake targets exercising tests with a label
    
    To run a subset of the tests, use:
    ```shell-session
    $ ninja -C ~/arrow/cpp/debug-build test-arrow_dataset
    ```
    
    Closes #6547 from bkietz/8014-Provide-CMake-targets-to- and squashes the following commits:
    
    cf9bbb06a <Benjamin Kietzman> test-lable- => test-
    90a1a7f3b <Benjamin Kietzman> ARROW-8014:  Provide Cmake targets exercising tests with a label
    
    Authored-by: Benjamin Kietzman <bengilgit@gmail.com>
    Signed-off-by: Antoine Pitrou <antoine@python.org>

 cpp/cmake_modules/BuildUtils.cmake | 15 +++++++++++++++
 cpp/src/arrow/CMakeLists.txt       |  2 --
 2 files changed, 15 insertions(+), 2 deletions(-)
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-16 01:40:03,14
13291827,[Rust] [DataFusion] Create LogicalPlanBuilder,"Building logical plans is arduous and a builder would make this nicer. Example:
{code:java}
let plan = LogicalPlanBuilder::new()
    .scan(
        ""default"",
        ""employee.csv"",
        &employee_schema(),
        Some(vec![0, 3]),
    )?
    .filter(col(1).eq(&lit_str(""CO"")))?
    .project(vec![col(0)])?
    .build()?; {code}
Note that I am already working on this and will have a PR shortly.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-03-15 15:05:08,10
13291700,"[Java] Enhance code style checking for Java code (add space after commas, semi-colons and type casts)","This is in response to a discussion in https://github.com/apache/arrow/pull/6039#discussion_r375161992

We found the current style checking for Java code is not sufficient. So we want to enhace it in a series of ""small"" steps, in order to avoid having to change too many files at once.

In this issue, we add spaces after commas, semi-colons and type casts.",pull-request-available,['Java'],ARROW,Improvement,Major,2020-03-14 08:50:32,7
13291657,[Dev] Make Yaml optional dependency for archery,"It is required to parse the output of crossbow, but only needed to run the comment bot, so make this dependency optional.",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-03-13 22:42:22,3
13291560,[C++] Implement a lighter-weight variant,"{{util::variant}} is an extremely useful structure but its header slows compilation significantly, so using it in public headers is questionable https://github.com/apache/arrow/pull/6545#discussion_r388406246

I'll try writing a lighter weight version.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-13 14:13:59,6
13291548,[FlightRPC][C++] Some status codes don't round-trip through gRPC,"KeyError and AlreadyExists don't fully round-trip, instead becoming UNKNOWN. There are others, but we don't attempt to map all the Arrow status to a gRPC status, only the ones that closely correspond to a gRPC error.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-03-13 12:50:34,0
13291469,[Java] Extract a common interface for dictionary encoders,"In this issue, we extract a common interfaces from existing dictionary encoders. This can be useful for scenarios when the client does not care about the encoder implementations. ",pull-request-available,['Java'],ARROW,New Feature,Minor,2020-03-13 02:42:53,7
13291464,[Python] Builds on master broken by pandas 1.0.2 release,"See example failing build

https://github.com/apache/arrow/runs/504483158

{code}
test_conversion_extensiontype_to_extensionarray

>   ???
E   pyarrow.lib.ArrowNotImplementedError: No cast implemented from extension<arrow.py_extension_type> to int64
{code}",nightly pull-request-available,['Python'],ARROW,Bug,Blocker,2020-03-13 01:44:44,5
13291435,[R] Make default Linux build more minimal,"So that we can build on CRAN as quickly as possible, and thus make the default experience for users installing the package better--no environment variable required to get something functional.",pull-request-available,"['Packaging', 'R']",ARROW,New Feature,Major,2020-03-12 22:27:32,4
13291426,[Dev] Crossbow's version detection doesn't work in the comment bot's scenario,"Because the tags are not fetched, see build https://ci.appveyor.com/project/Ursa-Labs/crossbow/builds/31432389#L2641",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-03-12 21:36:04,3
13291423,[FlightRPC][Java] Can't read/write only an empty null array,"This is rather an edge case, but Java/Flight fails with a table consisting of only an empty null array, since it has no buffers, and Java assumes this can never happen.

{noformat}
Exception in thread ""main"" org.apache.arrow.flight.FlightRuntimeException: CallStatus{code=CANCELLED, cause=java.lang.RuntimeException: Unexpected IO Exception, description='Failed to stream message'}
	at org.apache.arrow.flight.CallStatus.toRuntimeException(CallStatus.java:113)
	at org.apache.arrow.flight.grpc.StatusUtils.fromGrpcRuntimeException(StatusUtils.java:134)
	at org.apache.arrow.flight.grpc.StatusUtils.fromThrowable(StatusUtils.java:142)
	at org.apache.arrow.flight.FlightClient$SetStreamObserver.onError(FlightClient.java:315)
	at io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:442)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at org.apache.arrow.flight.grpc.ClientInterceptorAdapter$FlightClientCallListener.onClose(ClientInterceptorAdapter.java:117)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:700)
	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)
	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)
	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)
	at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:399)
	at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:510)
	at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:66)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:630)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:518)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:692)
	at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:681)
	at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Unexpected IO Exception
	at org.apache.arrow.flight.ArrowMessage.asInputStream(ArrowMessage.java:334)
	at org.apache.arrow.flight.ArrowMessage.access$000(ArrowMessage.java:64)
	at org.apache.arrow.flight.ArrowMessage$ArrowMessageHolderMarshaller.stream(ArrowMessage.java:382)
	at org.apache.arrow.flight.ArrowMessage$ArrowMessageHolderMarshaller.stream(ArrowMessage.java:372)
	at io.grpc.MethodDescriptor.streamRequest(MethodDescriptor.java:290)
	at io.grpc.internal.ClientCallImpl.sendMessageInternal(ClientCallImpl.java:473)
	at io.grpc.internal.ClientCallImpl.sendMessage(ClientCallImpl.java:457)
	at io.grpc.ForwardingClientCall.sendMessage(ForwardingClientCall.java:37)
	at io.grpc.ForwardingClientCall.sendMessage(ForwardingClientCall.java:37)
	at io.grpc.ForwardingClientCall.sendMessage(ForwardingClientCall.java:37)
	at io.grpc.stub.ClientCalls$CallToStreamObserverAdapter.onNext(ClientCalls.java:341)
	at org.apache.arrow.flight.FlightClient$PutObserver.putNext(FlightClient.java:354)
	at org.apache.arrow.flight.example.integration.IntegrationTestClient.testStream(IntegrationTestClient.java:132)
	at org.apache.arrow.flight.example.integration.IntegrationTestClient.run(IntegrationTestClient.java:96)
	at org.apache.arrow.flight.example.integration.IntegrationTestClient.main(IntegrationTestClient.java:69)
Caused by: java.lang.IllegalArgumentException
	at org.apache.arrow.util.Preconditions.checkArgument(Preconditions.java:122)
	at org.apache.arrow.flight.ArrowMessage.asInputStream(ArrowMessage.java:294)
	... 14 more
{noformat}",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2020-03-12 20:37:37,0
13291408,[Dev] Comment bot's crossbow command acts on the master branch,Rather than the branch of the PR which has triggered it.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-03-12 19:02:36,3
13291391,[CI][Crossbow] Nightly turbodbc job fails,"Turbodbc fails to compile (both ""master"" and ""latest"" versions with this error):

{code}
FAILED: cpp/turbodbc_arrow/Library/CMakeFiles/turbodbc_arrow_support.dir/src/arrow_result_set.cpp.o 
/opt/conda/envs/arrow/bin/x86_64-conda_cos6-linux-gnu-c++  -Dturbodbc_arrow_support_EXPORTS -I/turbodbc/cpp/turbodbc_arrow/Library -I/turbodbc/cpp/turbodbc_arrow/../cpp_odbc/Library -I/turbodbc/cpp/turbodbc_arrow/../turbodbc/Library -I/turbodbc/pybind11/include -isystem /opt/conda/envs/arrow/include -isystem /opt/conda/envs/arrow/include/python3.7m -isystem /opt/conda/envs/arrow/lib/python3.7/site-packages/numpy/core/include -fvisibility-inlines-hidden -Wall -Wextra -g -O0 -pedantic -fPIC -fvisibility=hidden   -std=c++11 -std=c++14 -MD -MT cpp/turbodbc_arrow/Library/CMakeFiles/turbodbc_arrow_support.dir/src/arrow_result_set.cpp.o -MF cpp/turbodbc_arrow/Library/CMakeFiles/turbodbc_arrow_support.dir/src/arrow_result_set.cpp.o.d -o cpp/turbodbc_arrow/Library/CMakeFiles/turbodbc_arrow_support.dir/src/arrow_result_set.cpp.o -c /turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp
/turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp: In member function 'arrow::Status turbodbc_arrow::{anonymous}::StringDictionaryBuilderProxy::AppendProxy(const char*, int32_t)':
/turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:67:36: error: no matching function for call to 'turbodbc_arrow::{anonymous}::StringDictionaryBuilderProxy::Append(const char*&, int32_t&)'
         return Append(value, length);
                                    ^
In file included from /opt/conda/envs/arrow/include/arrow/builder.h:26:0,
                 from /opt/conda/envs/arrow/include/arrow/api.h:26,
                 from /turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:6:
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:143:10: note: candidate: arrow::Status arrow::internal::DictionaryBuilderBase<BuilderType, T>::Append(const Scalar&) [with BuilderType = arrow::AdaptiveIntBuilder; T = arrow::StringType; arrow::internal::DictionaryBuilderBase<BuilderType, T>::Scalar = nonstd::sv_lite::basic_string_view<char>]
   Status Append(const Scalar& value) {
          ^~~~~~
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:143:10: note:   candidate expects 1 argument, 2 provided
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:156:43: note: candidate: template<class T1> arrow::enable_if_fixed_size_binary<T1, arrow::Status> arrow::internal::DictionaryBuilderBase<BuilderType, T>::Append(const uint8_t*) [with T1 = T1; BuilderType = arrow::AdaptiveIntBuilder; T = arrow::StringType]
   enable_if_fixed_size_binary<T1, Status> Append(const uint8_t* value) {
                                           ^~~~~~
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:156:43: note:   template argument deduction/substitution failed:
/turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:67:36: note:   candidate expects 1 argument, 2 provided
         return Append(value, length);
                                    ^
In file included from /opt/conda/envs/arrow/include/arrow/builder.h:26:0,
                 from /opt/conda/envs/arrow/include/arrow/api.h:26,
                 from /turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:6:
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:162:43: note: candidate: template<class T1> arrow::enable_if_fixed_size_binary<T1, arrow::Status> arrow::internal::DictionaryBuilderBase<BuilderType, T>::Append(const char*) [with T1 = T1; BuilderType = arrow::AdaptiveIntBuilder; T = arrow::StringType]
   enable_if_fixed_size_binary<T1, Status> Append(const char* value) {
                                           ^~~~~~
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:162:43: note:   template argument deduction/substitution failed:
/turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:67:36: note:   candidate expects 1 argument, 2 provided
         return Append(value, length);
                                    ^
In file included from /opt/conda/envs/arrow/include/arrow/builder.h:26:0,
                 from /opt/conda/envs/arrow/include/arrow/api.h:26,
                 from /turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:6:
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:168:37: note: candidate: template<class T1> arrow::enable_if_binary_like<T1, arrow::Status> arrow::internal::DictionaryBuilderBase<BuilderType, T>::Append(const uint8_t*, int32_t) [with T1 = T1; BuilderType = arrow::AdaptiveIntBuilder; T = arrow::StringType]
   enable_if_binary_like<T1, Status> Append(const uint8_t* value, int32_t length) {
                                     ^~~~~~
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:168:37: note:   template argument deduction/substitution failed:
/turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:67:36: note:   cannot convert 'value' (type 'const char*') to type 'const uint8_t* {aka const unsigned char*}'
         return Append(value, length);
                                    ^
In file included from /opt/conda/envs/arrow/include/arrow/builder.h:26:0,
                 from /opt/conda/envs/arrow/include/arrow/api.h:26,
                 from /turbodbc/cpp/turbodbc_arrow/Library/src/arrow_result_set.cpp:6:
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:174:37: note: candidate: template<class T1> arrow::enable_if_binary_like<T1, arrow::Status> arrow::internal::DictionaryBuilderBase<BuilderType, T>::Append(const char*, int32_t) [with T1 = T1; BuilderType = arrow::AdaptiveIntBuilder; T = arrow::StringType]
   enable_if_binary_like<T1, Status> Append(const char* value, int32_t length) {
                                     ^~~~~~
/opt/conda/envs/arrow/include/arrow/array/builder_dict.h:174:37: note:   template argument deduction/substitution failed:

[151/156] Building CXX object cpp/turbodbc_arrow/Library/CMakeFiles/turbodbc_arrow_support.dir/src/set_arrow_parameters.cpp.o
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp: In member function 'void turbodbc_arrow::{anonymous}::string_converter::rebind_to_maximum_length(const arrow::BinaryArray&, std::size_t, std::size_t)':
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp:101:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
           for (int64_t i = 0; i != elements; ++i) {
                               ~~^~~~~~~~~~~
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp: In member function 'void turbodbc_arrow::{anonymous}::string_converter::set_batch_utf16(std::size_t, std::size_t)':
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp:140:31: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
         for (int64_t i = 0; i != elements; ++i) {
                             ~~^~~~~~~~~~~
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp: In instantiation of 'void turbodbc_arrow::{anonymous}::string_converter::set_batch_of_type(std::size_t, std::size_t) [with String = std::__cxx11::basic_string<char>; std::size_t = long unsigned int]':
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp:173:57:   required from here
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp:121:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
           for (int64_t i = 0; i != elements; ++i) {
                               ~~^~~~~~~~~~~
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp: In function 'std::vector<std::unique_ptr<turbodbc_arrow::{anonymous}::parameter_converter> > turbodbc_arrow::{anonymous}::make_converters(const arrow::Table&, turbodbc::bound_parameter_set&)':
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp:363:41: warning: this statement may fall through [-Wimplicit-fallthrough=]
                 converters.emplace_back(new null_converter(data, parameters, i));
                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/turbodbc/cpp/turbodbc_arrow/Library/src/set_arrow_parameters.cpp:364:15: note: here
               case arrow::Type::INT8:
               ^~~~

[152/156] Building CXX object cpp/turbodbc_arrow/Library/CMakeFiles/turbodbc_arrow_support.dir/src/python_bindings.cpp.o
ninja: build stopped: subcommand failed.

Exited with code exit status 1
{code}

https://circleci.com/gh/ursa-labs/crossbow/9165",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2020-03-12 17:42:52,1
13291386,[CI][Crossbow] OSX wheels fail on bundled bzip2,"See e.g.

[https://travis-ci.org/github/ursa-labs/crossbow/builds/661245916#L6104]

[https://travis-ci.org/github/ursa-labs/crossbow/builds/661246751#L6103]

",pull-request-available,"['Continuous Integration', 'Packaging', 'Python']",ARROW,Bug,Blocker,2020-03-12 17:30:32,3
13291385,[CI][Crossbow] Fix nightly homebrew and R failures,"R: [https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=8156&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=d9b15392-e4ce-5e4c-0c8c-b69645229181&l=127]

Homebrew: [https://travis-ci.org/github/ursa-labs/crossbow/builds/661245549#L3392]",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2020-03-12 17:27:38,4
13291313,[C++][Dataset] Partition columns with specified dictionary type result in all nulls,"When specifying an explicit schema for the Partitioning, and when using a dictionary type, the materialization of the partition keys goes wrong: you don't get an error, but you get columns with all nulls.

Python example:

{code:python}
foo_keys = [0, 1]
bar_keys = ['a', 'b', 'c']
N = 30

df = pd.DataFrame({
    'foo': np.array(foo_keys, dtype='i4').repeat(15),
    'bar': np.tile(np.tile(np.array(bar_keys, dtype=object), 5), 2),
    'values': np.random.randn(N)
})

pq.write_to_dataset(pa.table(df), ""test_order"", partition_cols=['foo', 'bar'])
{code}

When reading with discovery, all is fine:

{code:python}
>>> ds.dataset(""test_order"", format=""parquet"", partitioning=""hive"").to_table().schema
values: double
bar: string
foo: int32
>>> ds.dataset(""test_order"", format=""parquet"", partitioning=""hive"").to_table().to_pandas().head(2)
     values bar  foo
0  2.505903   a    0
1 -1.760135   a    0
{code}

But when specifying the partition columns to be dictionary type with explicit {{HivePartitioning}}, you get no error but all null values:

{code:python}
>>> partitioning = ds.HivePartitioning(pa.schema([
...     (""foo"", pa.dictionary(pa.int32(), pa.int64())),
...     (""bar"", pa.dictionary(pa.int32(), pa.string()))
... ]))
>>> ds.dataset(""test_order"", format=""parquet"", partitioning=partitioning).to_table().schema
values: double
foo: dictionary<values=int64, indices=int32, ordered=0>
bar: dictionary<values=string, indices=int32, ordered=0>
>>> ds.dataset(""test_order"", format=""parquet"", partitioning=partitioning).to_table().to_pandas().head(2)
     values  foo  bar
0  2.505903  NaN  NaN
1 -1.760135  NaN  NaN
{code}",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-03-12 12:54:42,6
13291301,[C++][Dataset] Order of keys with HivePartitioning is lost in resulting schema,"Currently, when reading a partitioned dataset with hive partitioning, it seems that the partition columns get sorted alphabetically when appending them to the schema (while the old ParquetDataset implementation keeps the order as it is present in the paths).  
For a regular partitioning this order is consistent for all fragments.

So for example for the typical NYC Taxi data example, with datasets, the schema ends with columns ""month, year"", while the ParquetDataset appends them as ""year, month"".

Python example:

{code}
foo_keys = [0, 1]
bar_keys = ['a', 'b', 'c']
N = 30

df = pd.DataFrame({
    'foo': np.array(foo_keys, dtype='i4').repeat(15),
    'bar': np.tile(np.tile(np.array(bar_keys, dtype=object), 5), 2),
    'values': np.random.randn(N)
})

pq.write_to_dataset(pa.table(df), ""test_order"", partition_cols=['foo', 'bar'])
{code}

{code}
>>> pq.read_table(""test_order"").schema
values: double
foo: dictionary<values=int64, indices=int32, ordered=0>
bar: dictionary<values=string, indices=int32, ordered=0>

>>> ds.dataset(""test_order"", format=""parquet"", partitioning=""hive"").schema
values: double
bar: string
foo: int32
{code}

so ""foo, bar"" vs ""bar, foo"" (the fact that it are dictionaries is something else)",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-12 12:15:27,6
13291180,"[Python] Implement a wrapper for KeyValueMetadata, duck-typing dict where relevant","Per mailing list discussion, it may be better to not return the metadata always as a dict and instead wrap the KeyValueMetadata methods. We can make {{__getitem__}} lookup a key in it of course",pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-11 21:27:25,3
13291159,[Python] Add wheel build script and Crossbow configuration for Windows on Python 3.5,"Because conda-forge isn't building Python 3.5 packages anymore we need to build the 3.5 wheel in a different way, with most C++ dependencies built in BUNDLED mode. For this build we can disable Gandiva and Flight to keep things simple",pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-11 19:49:29,14
13291060,[C++][Dataset] Support for file-like objects (buffers) in FileSystemDataset?,"The current {{pyarrow.parquet.read_table}}/{{ParquetFile}} can work with buffer (reader) objects (file-like objects, pyarrow.Buffer, pyarrow.BufferReader) as input when dealing with single files. This functionality is for example being used by pandas and kartothek (in addition to being extensively used in our own tests as well).

While we could keep the old implementation to handle single files (which is different from the ParquetDataset logic), there are also some advantages of being able to handle this in the Datasets API.  
For example, this would enable to filtering functionality of the datasets API, also for this single-file buffers use case, which would be a nice enhancement (currently, {{read_table}} does not support {{filters}} in case of single files, which is eg why kartothek implements this themselves).

Would this be possible to support?

The {{arrow::dataset::FileSource}} already has PATH and BUFFER enum types (https://github.com/apache/arrow/blob/08f8bff05af37921ff1e5a2b630ce1e7ec1c0ede/cpp/src/arrow/dataset/file_base.h#L46-L49), so it seems in principle possible to create a FileSource (for a FileSystemDataset / FileFragment) from a buffer instead of from a path?",dataset dataset-dask-integration pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-03-11 11:31:06,6
13290957,[GLib] Build error with configure,This is introduced by ARROW-7444.,pull-request-available,['GLib'],ARROW,Bug,Major,2020-03-11 02:04:21,1
13290951,[C++] Cast segfaults on unsupported cast from list<binary> to utf8,"Was messing around with some nested arrays and found a pretty easy to reproduce segfault:


{code:java}
Python 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:33:48)
[GCC 7.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np, pyarrow as pa
>>> pa.__version__
'0.16.0'
>>> np.__version__
'1.18.1'
>>> x=[np.array([b'a',b'b'])]
>>> a = pa.array(x,pa.list_(pa.binary()))
>>> a
<pyarrow.lib.ListArray object at 0x7fd948b108a0>
[
  [
    61,
    62
  ]
]
>>> a.cast(pa.string())
Segmentation fault
{code}

I don't know if that cast makes sense, but I left the checks on, so I would not expect a segfault from it.",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-11 01:11:27,3
13290899,"[C++][Dataset] Untangle Dataset, Fragment and ScanOptions","Currently: a fragment is a product of a scan; it is a lazy collection of scan tasks corresponding to a data source which is logically singular (like a single file, a single row group, ...). It would be more useful if instead a fragment were the direct object of a scan; one scans a fragment (or a collection of fragments):

 # Remove {{ScanOptions}} from Fragment's properties and move it into {{Fragment::Scan}} parameters.
 # Remove {{ScanOptions}} from {{Dataset::GetFragments}}. We can provide an overload to support predicate pushdown in FileSystemDataset and UnionDataset {{Dataset::GetFragments(std::shared_ptr<Expression> predicate)}}.
 # Expose lazy accessor to Fragment::physical_schema()
 # Consolidate ScanOptions and ScanContext

This will lessen the cognitive dissonance between fragments and files since fragments will no longer include references to scan properties.
",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-10 19:03:44,13
13290897,[Dev] Implement Comment bot via Github actions,Ala {{@ursabot}}.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-03-10 18:59:57,3
13290889,[Python] Add user guide documentation for Datasets API,"Currently, we only have API docs (https://arrow.apache.org/docs/python/api/dataset.html), but we also need prose docs explaining what the dataset module does with examples.

This can also include guidelines on how to use this instead of the ParquetDataset API (depending on how we end up doing ARROW-8039), this aspect is also covered by ARROW-8047",documentation parquet pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2020-03-10 18:24:25,5
13290885,[C++][Dataset] Parquet Dataset factory from a _metadata/_common_metadata file,"Partitioned parquet datasets sometimes come with {{_metadata}} / {{_common_metadata}} files. Those files include information about the schema of the full dataset and potentially all RowGroup metadata as well (for {{_metadata}}).

Using those files during the creation of a parquet {{Dataset}} can give a more efficient factory (using the stored schema instead of inferring the schema from unioning the schemas of all files + using the paths to individual parquet files instead of crawling the directory).

Basically, based those files, the schema, list of paths and partition expressions (the information that is needed to create a Dataset) could be constructed.   
Such logic could be put in a different factory class, eg {{ParquetManifestFactory}} (as suggestetd by [~fsaintjacques]).",dataset dataset-dask-integration pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-03-10 18:15:51,13
13290882,[C++][Dataset] Ability to specify granularity of ParquetFileFragment (support row groups),"Specifically for parquet (not sure if it will be relevant for other file formats as well, for IPC/feather potentially ther record batch), it would be useful to target row groups instead of files as fragments.

Quoting the original design documents: _""In datasets consisting of many fragments, the dataset API must expose the granularity of fragments in a public way to enable parallel processing, if desired. ""._   
And a comment from Wes on that: _""a single Parquet file can ""export"" one or more fragments based on settings. The default might be to split fragments based on row group""_

Currently, the level on which fragments are defined (at least in the typical partitioned parquet dataset) is ""1 file == 1 fragment"".

Would it be possible or desirable to make this more fine grained, where you could also opt to have a fragment per row group?   
We could have a ParquetFragment that has this option, and a ParquetFileFormat specific option to say what the granularity of a fragment is (file vs row group)?

cc [~fsaintjacques] [~bkietz]",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-10 18:00:31,6
13290879,[Python] Make dataset Expression objects serializable,It would be good to be able to pickle pyarrow.dataset.Expression objects (eg for use in dask.distributed),pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-10 17:48:24,3
13290878,[Python] Make FileSystem objects serializable,"It would be good to be able to pickle {{pyarrow.fs.FileSystem}} objects (eg for use in dask.distributed)

cc [~apitrou]",pull-request-available,['Python'],ARROW,Improvement,Major,2020-03-10 17:47:00,3
13290804,[C++][Python][Dataset] Provide an option to toggle validation and schema inference in FileSystemDatasetFactoryOptions,"This can be costly and is not always necessary.

At the same time we could move file validation into the scan tasks; currently all files are inspected as the dataset is constructed, which can be expensive if the filesystem is slow. We'll be performing the validation multiple times but the check will be cheap since at scan time we'll be reading the file into memory anyway.",dataset pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-03-10 13:21:00,13
13290771,[Python] Don't check Schema metadata in __eq__ and __ne__,"When performing schema roundtrips, the equality check for fields break. This is a regression from PyArrow 0.16.0

The equality check for entire schemas has never worked (but should from my POV)
{code:python}
import pyarrow.parquet as pq
import pyarrow as pa
print(pa.__version__)
fields = [
    pa.field(""bool"", pa.bool_()),
    pa.field(""byte"", pa.binary()),
    pa.field(""date"", pa.date32()),
    pa.field(""datetime64"", pa.timestamp(""us"")),
    pa.field(""float32"", pa.float64()),
    pa.field(""float64"", pa.float64()),
    pa.field(""int16"", pa.int64()),
    pa.field(""int32"", pa.int64()),
    pa.field(""int64"", pa.int64()),
    pa.field(""int8"", pa.int64()),
    pa.field(""null"", pa.null()),
    pa.field(""uint16"", pa.uint64()),
    pa.field(""uint32"", pa.uint64()),
    pa.field(""uint64"", pa.uint64()),
    pa.field(""uint8"", pa.uint64()),
    pa.field(""unicode"", pa.string()),
    pa.field(""array_float32"", pa.list_(pa.float64())),
    pa.field(""array_float64"", pa.list_(pa.float64())),
    pa.field(""array_int16"", pa.list_(pa.int64())),
    pa.field(""array_int32"", pa.list_(pa.int64())),
    pa.field(""array_int64"", pa.list_(pa.int64())),
    pa.field(""array_int8"", pa.list_(pa.int64())),
    pa.field(""array_uint16"", pa.list_(pa.uint64())),
    pa.field(""array_uint32"", pa.list_(pa.uint64())),
    pa.field(""array_uint64"", pa.list_(pa.uint64())),
    pa.field(""array_uint8"", pa.list_(pa.uint64())),
    pa.field(""array_unicode"", pa.list_(pa.string())),
]

schema = pa.schema(fields)

buf = pa.BufferOutputStream()
pq.write_metadata(schema, buf)
reader = pa.BufferReader(buf.getvalue().to_pybytes())
reconstructed_schema = pq.read_schema(reader)

assert reconstructed_schema == reconstructed_schema
assert reconstructed_schema[0] == reconstructed_schema[0]
# This breaks on master / regression from 0.16.0 
assert schema[0] == reconstructed_schema[0]

# This never worked but should
assert reconstructed_schema == schema
assert schema == reconstructed_schema
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-03-10 10:35:17,14
13290673,[C++] Upgrade bundled Thrift version to 0.13.0,Follow up to discussion in ARROW-6821,pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-09 23:18:19,4
13290648,[Python][Documentation] Document migration from ParquetDataset to pyarrow.datasets,"We need documentation describing a migration path from ParquetDataset, at least for the basic user facing API of ParquetDataset (As I read it, that's: construction, projection, filtering, threading, for a first pass). Following this we could mark ParquetDataset as deprecated, building features needed by power users like dask and adding those to the migration document",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2020-03-09 21:11:04,5
13290641,[CI][NIGHTLY:gandiva-jar-osx] pygit2 needs libgit2 v1.0.x,"pygit2 (required by crossbow itself) is failing to build because it doesn't support the latest version of libgit2 https://travis-ci.org/ursa-labs/crossbow/builds/659892661#L4542

for now, we'll have to pin it at 1.0.x https://github.com/libgit2/pygit2/blob/42df4cb3eb95272cb7bdb76fdd4f127d370e3096/src/types.h#L36",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-03-09 20:33:41,6
13290633,[Developer] Provide better visibility for failed nightly builds,"Emails reporting nightly failures are unsatisfactory in two ways: there is a large click/scroll distance between the links presented in that email and the actual error message. Worse, once one is there it's not clear what JIRAs have been made or which of them are in progress.

One solution would be to replace or augment the [NIGHTLY] email with a page (https://ursa-labs.github.org/crossbow would be my favorite) which shows how many nights it has failed, a shortcut to the actual error line in CI's logs, and useful views of JIRA. We could accomplish this with:
- dedicated JIRA tags; one for each nightly job so a JIRA can be easily associated with specific jobs
- client side JavaScript to scrape JIRA and update the page dynamically as soon as JIRAs are opened
- provide automatic and expedited creation of correctly labelled JIRAs, so that viewers can quickly organize/take ownership of a failed nightly job. JIRA supports reading form fields from URL parameters, so this would be fairly straightforward: 
https://issues.apache.org/jira/secure/CreateIssueDetails!init.jspa?pid=12319525&issuetype=1&summary=[NIGHTLY:gandiva-jar-osx,gandiva-jar-trusty]&versions=12340948&fixVersions=12347769&components=12334626",pull-request-available,"['Continuous Integration', 'Developer Tools']",ARROW,Improvement,Major,2020-03-09 20:10:56,6
13290595,[Python] pyarrow.ChunkedArray docstring is incorrect regarding zero-length ChunkedArray having no chunks,"As part of fixing this, we should ensure that there are appropriate unit tests for {{pyarrow.chunked_array([], type=t)}}

https://github.com/apache/arrow/blob/master/python/pyarrow/table.pxi#L23",pull-request-available,['Python'],ARROW,Bug,Major,2020-03-09 17:09:48,14
13290584,[Python][Dataset] Support using dataset API in pyarrow.parquet with a minimal ParquetDataset shim,"Assemble a minimal ParquetDataset shim backed by {{pyarrow.dataset.*}}. Replace the existing ParquetDataset with the shim by default, allow opt-out for users who need the current ParquetDataset

This is mostly exploratory to see which of the python tests fail",dataset pull-request-available,"['C++', 'Python']",ARROW,Sub-task,Major,2020-03-09 16:43:34,5
13290552,[C++] Compilation failure with gtest 1.10.0,"{code}
../src/arrow/array_test.cc:641:1: error: 'TypedTestCaseIsDeprecated' is deprecated: TYPED_TEST_CASE is deprecated, please use TYPED_TEST_SUITE [-Werror,-Wdeprecated-declarations]
TYPED_TEST_CASE(TestPrimitiveBuilder, Primitives);
^
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-09 14:44:47,2
13290537,[C++] example parquet-arrow project includes broken FindParquet.cmake,"The example project at [https://github.com/apache/arrow/tree/master/cpp/examples/parquet/parquet-arrow/cmake_modules] includes a broken version of FindParquet.cmake ( [https://github.com/apache/arrow/blob/master/cpp/examples/parquet/parquet-arrow/cmake_modules/FindParquet.cmake] )

The other module is, correctly, a link to FindArrow.cmake in [https://github.com/apache/arrow/tree/master/cpp/cmake_modules]

For the curious, the broken part is assuming that FindPkgConfig variables will be set if the module is found - this can be false if the include directory is /usr/include. This can be controlled by one of FindPkgConfig's config variables, but the default behaviour changes as of CMake 3.10. It then erroneously reports that Parquet has not been found.

This is not a major bug, but I based my build files off of those in the example directory and it took me a LONG time to figure out the error. It can be really confusing for new users and is simple to fix.",beginner easyfix,['C++'],ARROW,Bug,Minor,2020-03-09 14:04:22,3
13290316,[Developer][Integration] Add integration tests for duplicate field names,"Schemas and nested types whose fields' names are not unique are permitted, so the integration tests should include a case which exercises these.",pull-request-available,['Integration'],ARROW,Improvement,Major,2020-03-07 17:10:29,6
13290171,[C++] Implement cast to Binary and FixedSizeBinary,It appears you can cast from Binary to String but not the other way. ,pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-06 19:13:39,3
13290169,[R] Bindings for BinaryType and FixedBinaryType,Prerequisite for ARROW-6235 (converting BinaryArray data to R). ,pull-request-available,['R'],ARROW,Improvement,Major,2020-03-06 19:09:41,4
13290088,[Python] Appveyor does not appear to be including pandas in test runs,"See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/31283406/job/j687pun52n9yamhx#L1854

This is caused by installing requirements.txt instead of requirements-test.txt

https://github.com/apache/arrow/blob/master/ci/cpp-msvc-build-main.bat#L106",pull-request-available,['Python'],ARROW,Bug,Major,2020-03-06 14:16:48,14
13289985,[Java] Implement vector validate functionality ,"In C++ side, we already have array validate functionality but no similar functionality in Java side.

This issue is about to implement this functionality.",pull-request-available,['Java'],ARROW,New Feature,Major,2020-03-06 08:19:19,16
13289902,[Developer] Fix deprecation warning in PR merge tool,"{code}
/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/jira/client.py:405: DeprecationWarning: Old API usage, use JIRA(url) or JIRA(options={'server': url}, when using dictionary always use named parameters.
  DeprecationWarning)
{code}",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-03-05 20:00:22,14
13289865,[C++] Provide CMake targets to test only within a given label,Tests are labelled but this feature is not easily accessible from Ninja or Make. Provide targets like {{test-label-arrow_dataset}} which exercises only the tests labelled {{arrow_dataset}}.,pull-request-available,['C++'],ARROW,Improvement,Major,2020-03-05 16:53:08,6
13289856,[Python][Packaging] Fix manylinux wheels,"The manylinux build jobs are failing currently because of ARROW-7917. See for example:
https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=7890&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=188
",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2020-03-05 16:12:17,2
13289834,[C++] Some buffers not resized when reading from Parquet,"This may leak uninitialized data:
{code:python}
>>> table = pa.Table.from_pydict({""a"": pa.array([0, None, None])})                                                                                                            
>>> table.column(""a"").chunk(0).buffers()[1].to_pybytes()                                                                                                                      
b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
>>> bio = io.BytesIO()                                                                                                                                                        
>>> pq.write_table(table, bio, use_dictionary=False)                                                                                                                          
>>> bio.seek(0)                                                                                                                                                               
0
>>> table = pq.read_table(bio)                                                                                                                                                
>>> table.column(""a"").chunk(0).buffers()[1].to_pybytes()                                                                                                                      
b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-05 14:53:13,2
13289817,[Java] Fix the hash code methods for BitVector,"The current hash code methods of BitVector are based on implementations in BaseFixedWidthVector, which rely on the type width of the vector. 
For BitVector, the type width is 0, so the underlying data is not actually used when computing the hash code. That means, the hash code will always be 0, no matter if the underlying data is null or not, and no matter if the underlying bit is 0 or 1. 

We fix this by overriding the methods in BitVector. ",pull-request-available,['Java'],ARROW,Bug,Major,2020-03-05 14:02:17,7
13289809,[C++/Python] Framework Python is preferred even though not the activated one,Currently the framework Python is preferred on macOS eventhough development happens in a completely different Python runtime.,pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-03-05 13:45:27,8
13289695,[C++] Unsafe arrow dictionary recovered from parquet,"When an arrow dictionary of values=strings and indices=intx is written to parquet and recovered, the indices that correspond to null positions are not written. This causes two problems:
 * when transposing the dictionary, the code encounters indices that are out of bounds with the existing dictionary. This does cause crashes.
 * a potential security risk because it's unclear whether bytes can be read back inadvertently.

I traced using GDB and found that:
 # My dictionary indices were decoded by RleDecoder::GetBatchSpaced. When the valid bit is unset, that function increments ""out"" but does not set it. I think it should write a 0.[https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/rle_encoding.h#L396]
 # The recovered data ""out"" array is written to the dictionary builder using an AppendIndices which moves the memory as a bulk move without checking for nulls. Hence we end-up with the indices buffer holding the ""out"" from above.[https://github.com/apache/arrow/blob/master/cpp/src/parquet/encoding.cc#L1670|https://github.com/apache/arrow/blob/master/cpp/src/parquet/encoding.cc#L1670]When transpose runs on this ([https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/int_util.cc#L406]), it may attempt to access memory out of bounds.

While is would be possible to fix ""transpose"" and other functions that process dictionary indices (e.g. compare for sorting), it seems safer to initialize to 0. Also that's the default behavior for the arrow dict builder when appending one or more nulls.

Incidentally the code recovers the dict with indices int32 instead of the original int8 but I guess that this is covered by another activity.

",pull-request-available,['C++'],ARROW,Bug,Critical,2020-03-05 03:21:01,2
13289682,[Website] Review and adjust any usages of Apache dist system from website / tools,"ASF Infra has communicated

""As of March 2020, we are deprecating www.apache.org/dist/ in favor of
https://downloads.apache.org/ for backup downloads as well as signature
and checksum verification. The primary driver has been splitting up web
site visits and downloads to gain better control and offer a better
service for both downloads and web site visits.

As stated, this does not impact end-users, and should have a minimal
impact on projects, as our download selectors as well as visits to
www.apache.org/dist/ have been adjusted to make use of
downloads.apache.org instead. We do however ask that projects, in their
own time-frame, change references on their own web sites from
www.apache.org/dist/ to downloads.apache.org wherever such references
may exist, to complete the switch in full. We will NOT be turning off
www.apache.org/dist/ in the near future, but would greatly appreciate if
projects could help us transition away from the old URLs in their
documentation and on their download pages.

The standard way of uploading releases[1] will STILL apply, however
there may be a short delay (<= 15 minutes) between releasing and
releases showing up on downloads.apache.org for technical reasons.
""

We should adjust our website and if necessary our release scripts based on this",pull-request-available,['Website'],ARROW,Improvement,Major,2020-03-05 01:22:02,13
13289611,[C++] -DBZip2_SOURCE=BUNDLED fails when building with clang,"I found this while working on something else (ARROW-8000)

{code}
$ ninja bzip2_ep
[2/8] Performing download step (download, verify and extract) for 'bzip2_ep'
-- bzip2_ep download command succeeded.  See also /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-download-*.log
[6/8] Performing build step for 'bzip2_ep'
FAILED: bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-build bzip2_ep-install/lib/libbz2.a 
cd /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep && /home/wesm/cpp-toolchain/bin/cmake -P /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-build-DEBUG.cmake && /home/wesm/cpp-toolchain/bin/cmake -E touch /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-build
CMake Error at /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-build-DEBUG.cmake:49 (message):
  Command failed: 2

   '/home/wesm/cpp-toolchain/bin/make' '-j16' 'CFLAGS= -Qunused-arguments -fuse-ld=gold -ggdb -O0 -g -fPIC'

  See also

    /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-build-*.log


ninja: build stopped: subcommand failed.
(arrow-3.7) 13:03 ~/code/arrow/cpp/build-gcc48  (gcc48-fixes)$ cat /home/wesm/code/arrow/cpp/build-gcc48/bzip2_ep-prefix/src/bzip2_ep-stamp/bzip2_ep-build-err.log 
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:120: huffman.o] Error 1
make: *** Waiting for unfinished jobs....
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:124: randtable.o] Error 1
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:128: decompress.o] Error 1
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:118: blocksort.o] Error 1
make: *** [Makefile:132: bzip2.o] Error 1
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:134: bzip2recover.o] Error 1
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:122: crctable.o] Error 1
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
gcc: error: unrecognized command line option -Qunused-arguments; did you mean -Wunused-parameter?
make: *** [Makefile:126: compress.o] Error 1
make: *** [Makefile:130: bzlib.o] Error 1
{code}

full output here https://gist.github.com/wesm/0fb18e0be7a7c6b7d383d891fd8b6c6e

it seems that CMAKE_C_COMPILER (which is {{clang-7}} here) is not being propagated to bzip2_ep, so the CMAKE_C_FLAGS that are assembled for Clang do not work properly",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-04 19:05:59,14
13289605,[R][Dataset] Bindings for dataset writing,This was started in ARROW-8002 but there's more to implement and test,dataset pull-request-available,['R'],ARROW,Improvement,Major,2020-03-04 18:49:14,4
13289586,[C++] gcc 4.8 build failures,"{code}
[219/530] Building CXX object src/arrow/c/CMakeFiles/arrow-c-bridge-test.dir/bridge_test.cc.o
FAILED: src/arrow/c/CMakeFiles/arrow-c-bridge-test.dir/bridge_test.cc.o 
/usr/bin/ccache /usr/bin/g++-4.8  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_HDFS -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR="""" -DARROW_NO_DEPRECATED_API -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_TIMING_TESTS -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DBOOST_ALL_NO_LIB -DBOOST_FILESYSTEM_DYN_LINK -DBOOST_SYSTEM_DYN_LINK -DURI_STATIC_BUILD -Isrc -I../src -I../src/generated -isystem ../thirdparty/flatbuffers/include -isystem /home/wesm/cpp-toolchain/include -isystem jemalloc_ep-prefix/src -isystem ../thirdparty/hadoop/include -fuse-ld=gold -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -Wno-attributes -msse4.2 -fno-omit-frame-pointer -g -fPIE   -pthread -std=gnu++11 -MD -MT src/arrow/c/CMakeFiles/arrow-c-bridge-test.dir/bridge_test.cc.o -MF src/arrow/c/CMakeFiles/arrow-c-bridge-test.dir/bridge_test.cc.o.d -o src/arrow/c/CMakeFiles/arrow-c-bridge-test.dir/bridge_test.cc.o -c ../src/arrow/c/bridge_test.cc
../src/arrow/c/bridge_test.cc:50:38: error: field initializer is not constant
   static constexpr auto ExportFunc = ExportType;
                                      ^
../src/arrow/c/bridge_test.cc:55:38: error: field initializer is not constant
   static constexpr auto ExportFunc = ExportField;
                                      ^
../src/arrow/c/bridge_test.cc:60:38: error: field initializer is not constant
   static constexpr auto ExportFunc = ExportSchema;
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-04 17:23:52,14
13289295,[C++] IO: coalescing and caching read ranges,This will be useful in order to improve Parquet reading performance on remote / high-latency filesystems.,pull-request-available,['C++'],ARROW,Wish,Major,2020-03-03 16:42:17,2
13289268,[CI][C++] Move AppVeyor MinGW builds to GitHub Actions,"To lighten a bit the load on AppVeyor (where we often have queues building up), it would be nice to move the MinGW builds to GitHub Actions.",pull-request-available,"['C++', 'Continuous Integration', 'Ruby']",ARROW,Wish,Minor,2020-03-03 14:50:54,1
13289062,"[C++][Developer] Add ""archery lint"" option for running ""iwyu.sh all""","I noticed when running {{archery lint --iwyu}} that it failed silently for me (Ubuntu 18.04 / clang 7)

{code}
-- Configuring done
-- Generating done
-- Build files have been written to: /tmp/arrow-lint-bbwghazv/cpp-build
[1/1] cd /tmp/arrow-lint-bbwghazv/cpp-bu...ode/arrow/cpp/build-support/iwyu/iwyu.sh
{code}

I believe the root cause of the silent failure is ARROW-7989, but in any case such a failure should make itself visible",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2020-03-02 22:58:23,14
13289048,[CI][R] Fix for verbose nightly builds,Followup to ARROW-7983,pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2020-03-02 21:31:34,4
13288983,[C++] ListBuilder.Finish fails if underlying value builder is empty and .Reserve'd,"Here's a reproduction:
{code:java}
#include <arrow/builder.h>
#include <arrow/status.h>
#include <iostream>

int main() {
        arrow::ListBuilder lb(arrow::default_memory_pool(), std::unique_ptr<arrow::ArrayBuilder>(new arrow::Int32Builder()));
        lb.value_builder()->Reserve(100); // bug
        lb.Append();
        std::shared_ptr<arrow::Array> ar;
        arrow::Status st = lb.Finish(&ar);
        if (!st.ok()) {
                std::cerr << st << '\n';
                return 1;
        }
}
{code}
The output is
{noformat}
Invalid: Resize cannot downsize{noformat}
The Resize call is made at builder_nested.h, line 115. There's a note there about ARROW-2744. Perhaps the fix is to look at capacity rather than length?",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-02 19:02:50,2
13288979,[R] Check for valid inputs in more places,"In trying to reproduce bug reports, I typically hit code paths I don't usually use, and I often give some input that I expect should work and instead cause a segfault. That's no good.",pull-request-available,['R'],ARROW,Improvement,Major,2020-03-02 18:49:29,4
13288928,[C++] Let ArrayDataVisitor accept void-returning functions,"It would be nice if {{ArrayDataVisitor}} accepted a visitor struct with void-returning (instead of Status-returning) methods. Always-ok Status may not be entirely optimized away by the compiler in some situations.
",pull-request-available,['C++'],ARROW,Wish,Major,2020-03-02 15:50:52,2
13288881,[C++][Dataset] Fails to compile on gcc 5.4,See github actions cron build https://github.com/apache/arrow/runs/479486173?check_suite_focus=true#step:5:824,dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-03-02 13:45:39,3
13288631,[Python] Deserialization with pyarrow fails for certain Timestamp-based data frame,"When following the [procedure outlined here|https://stackoverflow.com/a/57986261/5085211] to use {{pyarrow}} to serialize/deserialize pandas data frames, the below example fails with the given traceback (apologies for the broken formatting; I spent 10 minutes wrestling Jira with limited luck):


{code}
import pandas as pd                                                                      
import pyarrow as pa                                                                     
df = pd.DataFrame([{'Minutes5UTC': '2020-02-25T21:15:00+00:00', 'Minutes5DK': '2020-02-25T22:15:00'}])                                                        
df['Minutes5DK'] = pd.to_datetime(df.Minutes5DK)                                         
df['Minutes5UTC'] = pd.to_datetime(df.Minutes5UTC)                                       
context = pa.default_serialization_context()                                             
pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())


--------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-9-6f75cc47c6d5> in <module>
----> 1 pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.deserialize()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.deserialize_from()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.SerializedPyObject.deserialize()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.pxi in pyarrow.lib.SerializationContext._deserialize_callback()

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/serialization.py in _deserialize_pandas_dataframe(data)
    167 
    168     def _deserialize_pandas_dataframe(data):
--> 169         return pdcompat.serialized_dict_to_dataframe(data)
    170 
    171     def _serialize_pandas_series(obj):

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in serialized_dict_to_dataframe(data)
    661 def serialized_dict_to_dataframe(data):
    662     import pandas.core.internals as _int
--> 663     reconstructed_blocks = [_reconstruct_block(block)
    664                             for block in data['blocks']]
    665 

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in <listcomp>(.0)
    661 def serialized_dict_to_dataframe(data):
    662     import pandas.core.internals as _int
--> 663     reconstructed_blocks = [_reconstruct_block(block)
    664                             for block in data['blocks']]
    665 

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in _reconstruct_block(item, columns, extension_columns)
    707                                 klass=_int.CategoricalBlock)
    708     elif 'timezone' in item:
--> 709         dtype = make_datetimetz(item['timezone'])
    710         block = _int.make_block(block_arr, placement=placement,
    711                                 klass=_int.DatetimeTZBlock,

~/miniconda3/envs/emission/lib/python3.8/site-packages/pyarrow/pandas_compat.py in make_datetimetz(tz)
    734 def make_datetimetz(tz):
    735     tz = pa.lib.string_to_tzinfo(tz)
--> 736     return _pandas_api.datetimetz_type('ns', tz=tz)
    737 
    738 

TypeError: 'NoneType' object is not callable
{code}

Perhaps interestingly, if I comment out the two `pd.to_datetime` lines, the thing works (perhaps unsurprisingly), but if I then include them again, the original reproducing example all of a sudden works. That is, this works:

{code}
import pandas as pd                                                                      
import pyarrow as pa                                                                     
df = pd.DataFrame([{'Minutes5UTC': '2020-02-25T21:15:00+00:00', 'Minutes5DK': '2020-02-25T22:15:00'}])
context = pa.default_serialization_context()
pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())

df = pd.DataFrame([{'Minutes5UTC': '2020-02-25T21:15:00+00:00', 'Minutes5DK': '2020-02-25T22:15:00'}])
df['Minutes5DK'] = pd.to_datetime(df.Minutes5DK)
df['Minutes5UTC'] = pd.to_datetime(df.Minutes5UTC)
context = pa.default_serialization_context()
pa.deserialize(pa.serialize(df).to_buffer().to_pybytes())
{code}

The issue occurs with pyarrow 0.16.0, and in both pandas 0.25.3 and 1.0.1.",pull-request-available,['Python'],ARROW,Bug,Major,2020-03-02 08:02:14,5
13288615,[C++] Implement experimental buffer compression in IPC messages,The idea is that this can be used for experiments and bespoke applications (e.g. in the context of ARROW-5510). If this is adopted formally into the IPC format then the experimental implementation can be altered to match the specification,pull-request-available,['C++'],ARROW,Sub-task,Major,2020-03-02 06:02:04,14
13288613,[C++] Rename fs::FileStats to fs::FileInfo,"If we use FileInfo instead of FileStats, we can use singular form
""info"" and plural form ""infos"" as variable names instead of ""stats""
and ""stats_vector"". It will help writing readable code.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-03-02 05:43:06,1
13288611,"[C++] Do not include padding bytes in ""Buffer"" IPC metadata accounting","At this line, we include the padding bytes into the IPC metadata

https://github.com/apache/arrow/blob/apache-arrow-0.16.0/cpp/src/arrow/ipc/writer.cc#L192

The effect of this is that buffer sizes are modified by an IPC roundtrip. According to the Format, the padding bytes do not need to be accounted for in the metadata. 

https://github.com/apache/arrow/blob/master/format/Schema.fbs#L330

The Java implementation, for example, does not.

I ran into this when working on a prototype implementation of ARROW-300, where it is important to have the exact unpadded size of the original buffer that was written. ",pull-request-available,['C++'],ARROW,Bug,Major,2020-03-02 05:17:10,14
13288604,"[Developer][C++] ResourceWarning in ""make check-format""","Related to ARROW-7973, I also see

{code}
$ ninja check-format
[1/1] cd /home/wesm/code/arrow/cpp/preflight...ce_dir /home/wesm/code/arrow/cpp/src --quiet
/home/wesm/code/arrow/cpp/build-support/run_clang_format.py:77: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/wesm/code/arrow/cpp/build-support/lint_exclusions.txt' mode='r' encoding='UTF-8'>
  for line in open(arguments.exclude_globs):
ResourceWarning: Enable tracemalloc to get the object allocation traceback
{code}",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2020-03-02 03:27:29,14
13288462,[Packaging][Python] Use system boost to build the macos wheels,It'll reduce the build time and hopefully will fix flaky nightly builds https://travis-ci.org/ursa-labs/crossbow/builds/656165410,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-02-29 18:15:18,3
13288452,[Packaging] Use cURL to upload artifacts,"The asset uploading error originates from the base python http libraries, see https://github.com/sigmavirus24/github3.py/issues/779#issuecomment-379470626

So switch to curl which seems to handle the asset uploading properly.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-02-29 15:03:35,3
13288406,[CI][Crossbow] Pin macOS version in autobrew job to match CRAN,"Followup to ARROW-7923. After hopefully fixing the underlying issue somewhere in Travis, revert the changes in that issue so that we're still testing on old macOS.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2020-02-28 22:29:51,4
13288404,[Integration][Flight][C++] Client should verify each batch independently,"Currently the C++ Flight test client in {{test_integration_client.cc}} reads all batches from JSON into a Table, reads all batches in the flight stream from the server into a Table, then compares the Tables for equality.  This is potentially a problem because a record batch might have specific information that is then lost in the conversion to a Table. For example, if the server sends empty batches, the resulting Table would not be different from one with no empty batches.

Instead, the client should check each record batch from the JSON file against each record batch from the server independently. ",pull-request-available,"['C++', 'FlightRPC', 'Integration']",ARROW,Improvement,Major,2020-02-28 22:08:50,0
13288340,[Python] Refine higher level dataset API,"Provide a more intuitive way to construct nested dataset:

```python
# instead of using confusing factory function
dataset([
     factory(""s3://old-taxi-data"", format=""parquet""),
     factory(""local/path/to/new/data"", format=""csv"")
])

# let the user to construct a new dataset directly from dataset objects
dataset([ 
    dataset(""s3://old-taxi-data"", format=""parquet""),
    dataset(""local/path/to/new/data"", format=""csv"")
])
```

In the future we might want to introduce a new Dataset class which wraps functionality of both the dataset actory and the materialized dataset enabling optimizations over rediscovery of already materialized datasets. ",pull-request-available,['Python'],ARROW,Improvement,Major,2020-02-28 16:49:28,3
13288305,[C++][Python][Dataset] Expose listing fragments,"It would be useful to able to list the fragments, to get their paths / partition expressions.",dataset pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-02-28 14:30:29,6
13288285,"[R][Dataset] Followup to ""Consolidate Source and Dataset classes""",This was pushed to ARROW-7886 but it got dropped in a force push.,dataset pull-request-available,"['C++', 'R']",ARROW,Bug,Major,2020-02-28 13:16:40,4
13288204,[C++][Parquet] Add support for schema translation from parquet nodes back to arrow for missing types,Map seems to be the most obvious one missing. Without additional metadata I don't think FixedSizeList is possible. LargeList would probably have to also be could be determined empirically while parsing if there are any entries that exceed the int32 range (or with medata). Need to also double check that struct is supported,parquet pull-request-available,['C++'],ARROW,Sub-task,Major,2020-02-28 06:45:33,15
13288201,[Ruby] Add support for Ruby 2.3 again,"Ruby 2.3 reached EOL but Ubuntu 16.04 LTS ships Ruby 2.3. So supporting Ruby 2.3 again is valuable.

Note that Red Arrow 0.15.1 works with Ruby 2.3.",pull-request-available,['Ruby'],ARROW,Improvement,Major,2020-02-28 06:17:27,1
13288127,[Python] ParquetDataset cannot take HadoopFileSystem as filesystem,"{{from pyarrow.fs import HadoopFileSystem}}
 {{import pyarrow.parquet as pq}}



{{file_name = ""hdfs://localhost:9000/test/file_name.pq""}}
 {{hdfs, path = HadoopFileSystem.from_uri(file_name)}}
 {{dataset = pq.ParquetDataset(file_name, filesystem=hdfs)}}



has error:
 {{OSError: Unrecognized filesystem: <class 'pyarrow._hdfs.HadoopFileSystem'>}}



When I tried using the deprecated {{HadoopFileSystem}}:

{{import pyarrow}}
 {{import pyarrow.parquet as pq}}



{{file_name = ""hdfs://localhost:9000/test/file_name.pq""}}

{{hdfs = pyarrow.hdfs.connect('localhost', 9000)}}

{{dataset = pq.ParquetDataset(file_names, filesystem=hdfs)}}

{{pa_schema = dataset.schema.to_arrow_schema()}}

{{pieces = dataset.pieces}}

{{for piece in pieces:}}

{{  print(piece.path)}}



{{piece.path}} lose the {{hdfs://localhost:9000}} prefix.



I think {{ParquetDataset}} should accept{{pyarrow.fs.}}{{HadoopFileSystem as filesystem?}}

And {{piece.path}} should have the prefix?",pull-request-available,['Python'],ARROW,Bug,Critical,2020-02-27 18:57:16,5
13287985,[Java] Support large buffer for file/stream IPC,"After supporting 64-bit ArrowBuf, we need to make file/stream IPC work.",pull-request-available,['Java'],ARROW,Improvement,Major,2020-02-27 09:50:53,7
13287780,[Rust] [Flight] [DataFusion] Implement example for get_schema,Implement example for get_schema and implement the required helper methods.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-02-26 16:00:05,10
13287756,[C++][Dataset] Implement InMemoryDatasetFactory,"This will allow in memory datasets (such as tables) to participate in discovery through {{UnionDatasetFactory}}. This class will be trivial since Inspect will do nothing but return the table's schema, but is necessary to ensure that the resulting {{UnionDataset}}'s unified schema accommodates the table's schema (for example including fields present only in the table's schema or emitting an error when unification is not possible)",dataset,['C++'],ARROW,Improvement,Major,2020-02-26 14:20:06,6
13287687,[C++][Parquet] Add a new level builder capable of handling nested data,There will be one or two more steps to integrate this with the existing higher level APIs,pull-request-available,['C++'],ARROW,Sub-task,Major,2020-02-26 08:57:53,15
13287613,[Rust] [DataFusion] Logical plan should support unresolved column references,"It should be possible to build a logical plan using colum names rather than indices since it is more intuitive. There should be an optimizer rule that resolves the columns and replaces these unresolved columns with column indices.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2020-02-25 21:46:32,10
13287507,[Python][Packaging] Remove boost from the macos wheels,"Only boost_regex is required for libarrow but only on gcc < 4.9, see

https://github.com/apache/arrow/blob/f609298f8f00783a6704608ca8493227a552abab/cpp/src/parquet/metadata.cc#L38

so we can remove the bundled boost libraries from the macos wheels as well.",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-02-25 14:01:36,3
13287503,[Python] FileSystem.from_uri test fails on python 3.5,See build failure at https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=7535&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=6c939d89-0d1a-51f2-8b30-091a7a82e98c&l=288,pull-request-available,['Python'],ARROW,Bug,Major,2020-02-25 13:51:57,3
13287498,[Java] Remove Netty dependency for BufferAllocator and ReferenceManager,"With previous work (ARROW-7329 and ARROW-7505), Netty based allocation is only one of the possible implementations. So we need to revise BufferAllocator and ReferenceManager, to make them general, and independent of Netty libraries.",pull-request-available,['Java'],ARROW,Improvement,Minor,2020-02-25 13:34:49,7
13287209,[Python][CI] Test jpype integration in CI,"We used to test jpype integration on Travis-CI, but this wasn't transferred to the GHA setup.

Perhaps we need a nightly build or crossbow task for it.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Task,Major,2020-02-24 13:26:20,2
13287108,"[Developer] ""archery lint"" target is not ergonomic for running a single check like IWYU",It might be useful to have a second lint CLI target with everything disabled by default so that a single lint target can be toggled on. How should this be used via docker-compose? See ARROW-7925,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-02-24 00:32:27,2
13287107,[C++][Documentation] Instructions about running IWYU and other tasks in cpp/development.rst have gone stale,"For example

https://github.com/apache/arrow/blob/master/docs/source/developers/cpp/development.rst#cleaning-includes-with-include-what-you-use-iwyu",pull-request-available,['C++'],ARROW,Bug,Major,2020-02-24 00:23:16,14
13287100,[Rust] Add sort for float types,"Floats need a different sort approach than other primitives, and this ticket will implement them separately",pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-02-23 20:37:51,12
13287078,[CI][Crossbow] macOS autobrew fails on homebrew-versions,"See e.g. https://travis-ci.org/ursa-labs/crossbow/builds/653768049#L97. According to https://github.com/Homebrew/brew/issues/5734, there needs to be {{brew untap homebrew-versions}} before {{brew update}}, except this is happening in the Travis workflow in the setup stage, so we can't. Will need to change the travis-build config or base image upstream, or look for a different workaround.",pull-request-available,"['Continuous Integration', 'Packaging', 'R']",ARROW,Bug,Major,2020-02-23 16:41:25,4
13287077,[CI][Crossbow] Nightly macOS wheel builds fail (brew bundle edition),See e.g. https://travis-ci.org/ursa-labs/crossbow/builds/653768373#L129. Apparently a new Homebrew release changed some dependency of {{brew bundle}} so we need to be sure to {{brew update}} first: https://travis-ci.community/t/macos-build-fails-because-of-homebrew-bundle-unknown-command/7296/6.,pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Major,2020-02-23 16:26:58,4
13286992,[R] Fill in some missing input validation,I hit some segfaults trying to reproduce an issue because of missing input validation.,pull-request-available,['R'],ARROW,Improvement,Minor,2020-02-22 15:32:44,4
13286912,[R] install_arrow() should conda install if appropriate,"Like, check {{if (grepl(""conda"", R.Version()$platform))}} and if so then {{system(""conda install ..."")}}. Error if nightly == TRUE because we don't host conda nightlies yet.

This would help with issues like https://github.com/apache/arrow/issues/6448",pull-request-available,['R'],ARROW,Improvement,Major,2020-02-21 23:42:20,4
13286867,[CMake] FindPythonInterp should check for python3,On ubuntu 18.04 it'll pick python2 by default.,pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-21 18:34:54,2
13286860,[C++][Dataset] Project IPC record batches to materialized fields,"If batches mmaped from disk are projected before post filtering, unreferenced columns will never be accessed (so the memory map shouldn't do I/O on them).

At the same time, it'd probably be wise to explicitly document that batches yielded directly from fragments rather than from a Scanner will not be filtered or projected (so they will not match the fragment's schema and will include columns referenced by the filter even if they were not projected).",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-21 18:09:11,6
13286858,[CI] [Python] Run tests with Python development mode enabled,"Python's ""development mode"" enable a few runtime checks and warnings, see the docs for ""{{-X dev}}"": https://docs.python.org/3/using/cmdline.html#id5",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Trivial,2020-02-21 18:08:20,2
13286852,[C++][Python][R] C++ implementation of C data interface,See ARROW-7912,pull-request-available,"['C++', 'Python', 'R']",ARROW,Improvement,Major,2020-02-21 17:26:00,2
13286851,[Format] C data interface,"Apache Arrow is designed to be a universal in-memory format for the representation
of tabular (""columnar"") data. However, some projects may face a difficult
choice between either depending on a fast-evolving project such as the
Arrow C++ library, or having to reimplement adapters for data interchange,
which may require significant, redundant development effort.

The Arrow C data interface defines a very small, stable set of C definitions
that can be easily *copied* in any project's source code and used for columnar
data interchange in the Arrow format.  For non-C/C++ languages and runtimes,
it should be almost as easy to translate the C definitions into the
corresponding C FFI declarations.

Applications and libraries can therefore work with Arrow memory without
necessarily using Arrow libraries or reinventing the wheel. Developers can
choose between tight integration
with the Arrow *software project* (benefitting from the growing array of
facilities exposed by e.g. the C++ or Java implementations of Apache Arrow,
but with the cost of a dependency) or minimal integration with the Arrow
*format* only.

",pull-request-available,['Format'],ARROW,Improvement,Major,2020-02-21 17:25:00,2
13286821,[Python] Conversion to pandas of empty table with timestamp type aborts,"Creating an empty table:

{code}
In [1]: table = pa.table({'a': pa.array([], type=pa.timestamp('us'))})                                                                                                                                             

In [2]: table['a']                                                                                                                                                                                                 
Out[2]: 
<pyarrow.lib.ChunkedArray object at 0x7fbb783e8098>
[
  []
]

In [3]: table.to_pandas()                                                                                                                                                                                          
Out[3]: 
Empty DataFrame
Columns: [a]
Index: []
{code}

the above works. But the ChunkedArray still has 1 empty chunk. When filtering data, you can actually get no chunks, and this fails:


{code}
In [4]: table2 = table.slice(0, 0)                                                                                                                                                                                 

In [5]: table2['a']                                                                                                                                                                                                
Out[5]: 
<pyarrow.lib.ChunkedArray object at 0x7fbb783aa4a8>
[

]

In [6]: table2.to_pandas()                                                                                                                                                                                         
../src/arrow/table.cc:48:  Check failed: (chunks.size()) > (0) cannot construct ChunkedArray from empty vector and omitted type
...
Aborted (core dumped)
{code}

and this seems to happen specifically for timestamp type, and specifically with non-ns unit (eg with us as above, which is the default in arrow).

I noticed this when reading a parquet file of the taxi dataset, where the filter I used resulted in an empty batch.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-02-21 15:49:33,14
13286800,[C++] Decide about Field/Schema metadata printing parameters and how much to show by default,See discussion in https://github.com/apache/arrow/pull/6472 for follow up discussions to ARROW-7063,pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-21 14:09:49,14
13286640,[Rust] [DataFusion] Upgrade SQLParser dependency for DataFusion,"We've been running into a couple issues that seem to stem from the sqlparser crate, such as it not supporting columns that begin with a leading underscore.



Unfortunately the upgrade for DataFusion to sqlparser-0.5 (or even 0.3) seems to be non-trivial.



Is this planned?",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2020-02-21 04:00:24,10
13286540,[Python] Reduce the number docstring violations using numpydoc,"This is going to require more than one patch, because we have more than a thousand violations, but we need to start somewhere.",pull-request-available,"['Documentation', 'Python']",ARROW,Task,Major,2020-02-20 18:25:05,3
13286536,[Packaging] Temporarily disable artifact uploading until we fix the deployment issues,This will filter out the false negatives from the nightly build report until we fix the deployment errors in https://github.com/apache/arrow/pull/6458,pull-request-available,['Packaging'],ARROW,Task,Major,2020-02-20 17:51:07,3
13286532,[C++] Refactor from #include guards to #pragma once,"All compilers we support handle {{#pragma once}} correctly, and it reduces our header boilerplate.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-20 17:34:50,6
13286518,[C++] DefineOptions should invoke add_definitions,"Several build options are mirrored as preprocessor definitions, for example \{{ARROW_JEMALLOC}}. This could be made more consistent by requiring that every option in DefineOptions should also define a preprocessor macro with {{add_definitions}}.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-20 16:22:18,6
13286340,[C++] RecordBatch->Equals should also have a check_metadata argument,"Followup to ARROW-7720 and ARROW-7786. Table and Schema both have it, so it stands to reason that RecordBatch should too.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-19 22:16:48,14
13286312,[C++] Add Promise / Future implementation,{{std::future}} is unfortunately not featureful enough: there is no way to wait on several futures at once.,pull-request-available,['C++'],ARROW,Wish,Major,2020-02-19 19:34:28,2
13286263,[C++][Dataset] Consolidate Source and Dataset,Source and Dataset are very similar concepts (collections of multiple data fragments). Consolidating them would decrease doc burden without reducing our flexibility.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-19 15:51:40,6
13286203,[C++][Python] Crash in pq.read_table(),"The following crashes:
{code:python}
>>> import pyarrow.parquet as pq                                                                                                                                        
>>> tab = pq.read_table(""../cpp/submodules/parquet-testing/data/nation.dict-malformed.parquet"")
{code}

Here is the backtrace:
{code}
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007ffff7805801 in __GI_abort () at abort.c:79
#2  0x00007fffb8a18e42 in arrow::util::CerrLog::~CerrLog (this=0x7fff84001690, __in_chrg=<optimized out>) at ../src/arrow/util/logging.cc:50
#3  0x00007fffb8a18e5e in arrow::util::CerrLog::~CerrLog (this=0x7fff84001690, __in_chrg=<optimized out>) at ../src/arrow/util/logging.cc:52
#4  0x00007fffb8a18c9f in arrow::util::ArrowLog::~ArrowLog (this=0x7fffaeffbf60, __in_chrg=<optimized out>) at ../src/arrow/util/logging.cc:228
#5  0x00007fffb89e1607 in arrow::io::internal::SharedExclusiveChecker::LockExclusive (this=0x555555db1338) at ../src/arrow/io/interfaces.cc:287
#6  0x00007fffb89b0b10 in arrow::io::internal::ExclusiveLockGuard<arrow::io::internal::SharedExclusiveChecker>::ExclusiveLockGuard (this=0x7fffaeffbff8, 
    lock=0x555555db1338) at ../src/arrow/io/concurrency.h:47
#7  0x00007fffb89ad20f in arrow::io::internal::SharedExclusiveChecker::exclusive_guard (this=0x555555db1338) at ../src/arrow/io/concurrency.h:74
#8  0x00007fffb89cb74b in arrow::io::internal::RandomAccessFileConcurrencyWrapper<arrow::io::ReadableFile>::GetSize (this=0x555555db1320)
    at ../src/arrow/io/concurrency.h:200
#9  0x00007fffb4ca61f3 in parquet::SerializedRowGroup::GetColumnPageReader (this=0x7fff840013e0, i=2) at ../src/parquet/file_reader.cc:117
#10 0x00007fffb4ca2b0e in parquet::RowGroupReader::GetColumnPageReader (this=0x7fff840014b0, i=2) at ../src/parquet/file_reader.cc:75
#11 0x00007fffb4b03296 in parquet::arrow::FileColumnIterator::NextChunk (this=0x7fff84000c10) at ../src/parquet/arrow/reader_internal.h:81
#12 0x00007fffb4b06ccc in parquet::arrow::LeafReader::NextRowGroup (this=0x7fff84000ef0) at ../src/parquet/arrow/reader.cc:452
#13 0x00007fffb4b0677e in parquet::arrow::LeafReader::LeafReader (this=0x7fff84000ef0, ctx=std::shared_ptr<parquet::arrow::ReaderContext> (empty) = {...}, 
    field=std::shared_ptr<arrow::Field> (empty) = {...}, input=std::unique_ptr<parquet::arrow::FileColumnIterator> = {...}) at ../src/parquet/arrow/reader.cc:407
#14 0x00007fffb4afbdac in parquet::arrow::GetReader (field=..., ctx=std::shared_ptr<parquet::arrow::ReaderContext> (use count 2, weak count 0) = {...}, 
    out=0x7fffaeffc580) at ../src/parquet/arrow/reader.cc:709
#15 0x00007fffb4b0425a in parquet::arrow::FileReaderImpl::GetFieldReader (this=0x555555dbf480, i=2, 
    included_leaves=std::shared_ptr<std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> >> (use count 5, weak count 0) = {...}, 
    row_groups=std::vector of length 1, capacity 1 = {...}, out=0x7fffaeffc580) at ../src/parquet/arrow/reader.cc:173
#16 0x00007fffb4b04451 in parquet::arrow::FileReaderImpl::ReadSchemaField (this=0x555555dbf480, i=2, 
    included_leaves=std::shared_ptr<std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> >> (use count 5, weak count 0) = {...}, 
    row_groups=std::vector of length 1, capacity 1 = {...}, out_field=0x555555dce870, out=0x555555dce790) at ../src/parquet/arrow/reader.cc:186
#17 0x00007fffb4afcf7f in parquet::arrow::FileReaderImpl::<lambda(int)>::operator()(int) const (__closure=0x555555dd4e08, i=2) at ../src/parquet/arrow/reader.cc:810
#18 0x00007fffb4b01151 in std::__invoke_impl<arrow::Status, parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>&, int&>(std::__invoke_other, parquet::arrow::FileReaderImpl::<lambda(int)> &, int &) (__f=..., __args#0=@0x555555dd4e38: 2)
    at /usr/include/c++/7/bits/invoke.h:60
#19 0x00007fffb4b010dc in std::__invoke<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>&, int&>(parquet::arrow::FileReaderImpl::<lambda(int)> &, int &) (__fn=..., __args#0=@0x555555dd4e38: 2) at /usr/include/c++/7/bits/invoke.h:96
#20 0x00007fffb4b00fe1 in std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>::__call<arrow::Status, 0>(std::tuple<> &&, std::_Index_tuple<0>) (this=0x555555dd4e08, __args=...) at /usr/include/c++/7/functional:469
#21 0x00007fffb4b00b0d in std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>::operator()<>(void) (this=0x555555dd4e08) at /usr/include/c++/7/functional:551
#22 0x00007fffb4b00742 in std::__invoke_impl<arrow::Status, std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>&>(std::__invoke_other, std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)> &) (__f=...) at /usr/include/c++/7/bits/invoke.h:60
#23 0x00007fffb4b004c5 in std::__invoke<std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>&>(std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)> &) (__fn=...) at /usr/include/c++/7/bits/invoke.h:96
#24 0x00007fffb4b001bb in std::__future_base::_Task_state<std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>, std::allocator<int>, arrow::Status()>::<lambda()>::operator()(void) const (__closure=0x7fffaeffcc28)
    at /usr/include/c++/7/future:1421
#25 0x00007fffb4b00bf9 in std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<arrow::Status>, std::__future_base::_Result_base::_Deleter>, std::__future_base::_Task_state<_Fn, _Alloc, _Res(_Args ...)>::_M_run(_Args&& ...) [with _Fn = std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>; _Alloc = std::allocator<int>; _Res = arrow::Status; _Args = {}]::<lambda()>, arrow::Status>::operator()(void) const (this=0x7fffaeffcc30) at /usr/include/c++/7/future:1339
#26 0x00007fffb4b0080a in std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter>(), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<arrow::Status>, std::__future_base::_Result_base::_Deleter>, std::__future_base::_Task_state<_Fn, _Alloc, _Res(_Args ...)>::_M_run(_Args&& ...) [with _Fn = std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>; _Alloc = std::allocator<int>; _Res = arrow::Status; _Args = {}]::<lambda()>, arrow::Status> >::_M_invoke(const std::_Any_data &) (
    __functor=...) at /usr/include/c++/7/bits/std_function.h:302
#27 0x00007fffb4b0861e in std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>::operator()() const (
    this=0x7fffaeffcc30) at /usr/include/c++/7/bits/std_function.h:706
#28 0x00007fffb4b028e6 in std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*) (this=0x555555dd4de0, __f=0x7fffaeffcc30, __did_set=0x7fffaeffcb9f) at /usr/include/c++/7/future:561
#29 0x00007fffb4b126c5 in std::__invoke_impl<void, void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::__invoke_memfun_deref, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&) (__f=
    @0x7fffaeffcbe0: (void (std::__future_base::_State_baseV2::*)(std::__future_base::_State_baseV2 * const, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter>()> *, bool *)) 0x7fffb4b028b0 <std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)>, __t=@0x7fffaeffcbb0: 0x555555dd4de0, __args#0=@0x7fffaeffcbb8: 0x7fffaeffcc30, 
---Type <return> to continue, or q <return> to quit---
    __args#1=@0x7fffaeffcbc0: 0x7fffaeffcb9f) at /usr/include/c++/7/bits/invoke.h:73
#30 0x00007fffb4b0c9f2 in std::__invoke<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&) (__fn=
    @0x7fffaeffcbe0: (void (std::__future_base::_State_baseV2::*)(std::__future_base::_State_baseV2 * const, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter>()> *, bool *)) 0x7fffb4b028b0 <std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)>, __args#0=@0x7fffaeffcbb0: 0x555555dd4de0, __args#1=@0x7fffaeffcbb8: 0x7fffaeffcc30, 
    __args#2=@0x7fffaeffcbc0: 0x7fffaeffcb9f) at /usr/include/c++/7/bits/invoke.h:95
#31 0x00007fffb4b0819e in void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)::{lambda()#1}::operator()() const (__closure=0x7fffaeffcb20) at /usr/include/c++/7/mutex:672
#32 0x00007fffb4b08209 in void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)::{lambda()#2}::operator()() const (__closure=0x0) at /usr/include/c++/7/mutex:677
#33 0x00007fffb4b0821a in void std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&)::{lambda()#2}::_FUN() () at /usr/include/c++/7/mutex:677
#34 0x00007ffff7bc5827 in __pthread_once_slow (once_control=0x555555dd4df8, init_routine=0x7fffb6d52782 <std::__once_proxy()>) at pthread_once.c:116
#35 0x00007fffb4afa2fa in __gthread_once (__once=0x555555dd4df8, __func=0x7fffb6d52782 <std::__once_proxy()>)
    at /usr/include/x86_64-linux-gnu/c++/7/bits/gthr-default.h:699
#36 0x00007fffb4b082d3 in std::call_once<void (std::__future_base::_State_baseV2::*)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*>(std::once_flag&, void (std::__future_base::_State_baseV2::*&&)(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*), std::__future_base::_State_baseV2*&&, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*&&, bool*&&) (__once=..., __f=
    @0x7fffaeffcbe0: (void (std::__future_base::_State_baseV2::*)(std::__future_base::_State_baseV2 * const, std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter>()> *, bool *)) 0x7fffb4b028b0 <std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)>, __args#0=@0x7fffaeffcbb0: 0x555555dd4de0, __args#1=@0x7fffaeffcbb8: 0x7fffaeffcc30, 
    __args#2=@0x7fffaeffcbc0: 0x7fffaeffcb9f) at /usr/include/c++/7/mutex:684
#37 0x00007fffb4b02488 in std::__future_base::_State_baseV2::_M_set_result(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>, bool) (this=0x555555dd4de0, __res=..., __ignore_failure=false) at /usr/include/c++/7/future:401
#38 0x00007fffb4b00241 in std::__future_base::_Task_state<std::_Bind<parquet::arrow::FileReaderImpl::ReadRowGroups(const std::vector<int>&, const std::vector<int>&, std::shared_ptr<arrow::Table>*)::<lambda(int)>(int)>, std::allocator<int>, arrow::Status()>::_M_run(void) (this=0x555555dd4de0) at /usr/include/c++/7/future:1423
#39 0x00007fffb708834d in std::packaged_task<arrow::Status ()>::operator()() (this=0x555555dd4e90) at /usr/include/c++/7/future:1556
#40 0x00007fffb7080078 in arrow::internal::detail::packaged_task_wrapper<arrow::Status>::operator() (this=0x555555dd4eb0) at ../src/arrow/util/thread_pool.h:70
#41 0x00007fffb4b15d78 in std::_Function_handler<void (), arrow::internal::detail::packaged_task_wrapper<arrow::Status> >::_M_invoke(std::_Any_data const&) (
    __functor=...) at /usr/include/c++/7/bits/std_function.h:316
#42 0x00007fffb8a2c73e in std::function<void ()>::operator()() const (this=0x7fffaeffcd50) at /usr/include/c++/7/bits/std_function.h:706
#43 0x00007fffb8a2a644 in arrow::internal::WorkerLoop (state=std::shared_ptr<arrow::internal::ThreadPool::State> (use count 49, weak count 0) = {...}, it={
  _M_id = {
    _M_thread = 140736129390336
  }
}) at ../src/arrow/util/thread_pool.cc:88
#44 0x00007fffb8a2b032 in arrow::internal::ThreadPool::<lambda()>::operator()(void) const (__closure=0x555555dd2578) at ../src/arrow/util/thread_pool.cc:225
#45 0x00007fffb8a2bc93 in std::__invoke_impl<void, arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::<lambda()> >(std::__invoke_other, arrow::internal::ThreadPool::<lambda()> &&) (__f=...) at /usr/include/c++/7/bits/invoke.h:60
#46 0x00007fffb8a2b9d4 in std::__invoke<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::<lambda()> >(arrow::internal::ThreadPool::<lambda()> &&) (__fn=...)
    at /usr/include/c++/7/bits/invoke.h:95
#47 0x00007fffb8a2bec6 in std::thread::_Invoker<std::tuple<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::<lambda()> > >::_M_invoke<0>(std::_Index_tuple<0>)
    (this=0x555555dd2578) at /usr/include/c++/7/thread:234
#48 0x00007fffb8a2be82 in std::thread::_Invoker<std::tuple<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::<lambda()> > >::operator()(void) (
    this=0x555555dd2578) at /usr/include/c++/7/thread:243
#49 0x00007fffb8a2be52 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::<lambda()> > > >::_M_run(void) (this=0x555555dd2570) at /usr/include/c++/7/thread:186
#50 0x00007fffb6d53163 in std::execute_native_thread_routine (__p=0x555555dd2570)
    at /home/conda/feedstock_root/build_artifacts/ctng-compilers_1578638331887/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:80
#51 0x00007ffff7bbd6db in start_thread (arg=0x7fffaeffd700) at pthread_create.c:463
---Type <return> to continue, or q <return> to quit---
#52 0x00007ffff78e688f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
{code}
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-02-19 10:14:36,2
13286121,[C++] Fix pedantic warnings,"Saw this while working on ARROW-7880:

{code}
In file included from /arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/compute/kernel.h:27,
                 from /arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/compute/api.h:22,
                 from ./arrow_types.h:199,
                 from chunkedarray.cpp:18:
/arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/scalar.h:399:2: warning: extra ; [-Wpedantic]
 };  // namespace internal
  ^
In file included from /arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/compute/api.h:31,
                 from ./arrow_types.h:199,
                 from chunkedarray.cpp:18:
/arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/compute/kernels/mean.h:66:2: warning: extra ; [-Wpedantic]
 };  // namespace arrow
  ^
In file included from /arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/dataset/file_base.h:29,
                 from /arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/dataset/api.h:22,
                 from ./arrow_types.h:201,
                 from chunkedarray.cpp:18:
/arrow/r/libarrow/arrow-0.16.0.9000/include/arrow/dataset/scanner.h:40:2: warning: extra ; [-Wpedantic]
 };
  ^
In file included from /arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/encryption.h:28,
                 from /arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/properties.h:29,
                 from /arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/metadata.h:29,
                 from /arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/file_reader.h:26,
                 from /arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/arrow/reader.h:25,
                 from ./arrow_types.h:217,
                 from chunkedarray.cpp:18:
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:319:36: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(Boolean, BOOLEAN);
                                    ^
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:320:32: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(Int32, INT32);
                                ^
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:321:32: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(Int64, INT64);
                                ^
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:322:32: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(Int96, INT96);
                                ^
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:323:32: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(Float, FLOAT);
                                ^
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:324:34: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(Double, DOUBLE);
                                  ^
/arrow/r/libarrow/arrow-0.16.0.9000/include/parquet/schema.h:325:41: warning: extra ; [-Wpedantic]
 PRIMITIVE_FACTORY(ByteArray, BYTE_ARRAY);
{code}",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-02-18 23:36:57,4
13286119,[CI][R] R sanitizer job is not really working,"It's not failing, but it's not doing useful things. It's building the C++ library, then installing the R package, but it's not finding the C++ library that was built, and then the rest of the build is not erroring but not actually working, just burning electricity.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2020-02-18 23:12:08,4
13286014,[Packaging] Fix crossbow deployment to github artifacts,"Our artifact deploying scripts have started to fail with network errors.
We need to overcome this issue before cutting the next release, so marking as blocker.
",pull-request-available,"['Developer Tools', 'Packaging']",ARROW,Task,Blocker,2020-02-18 15:02:05,3
13286009,[R] Installation fails in the documentation generation image,https://github.com/apache/arrow/runs/452378996#step:5:11843,pull-request-available,['R'],ARROW,Improvement,Major,2020-02-18 14:46:16,3
13285990,[Python][Archery] Validate docstrings with numpydoc,"Numpydoc's master have support for validating numpy flavored docstrings. 
This enables to check the docstrings automatically from CI. 
",pull-request-available,"['Developer Tools', 'Python']",ARROW,Improvement,Major,2020-02-18 13:51:40,3
13285925,[Python] Support conversion of list-of-struct in Array/Table.to_pandas,"STRUCT is not a supported value type for conversion yet

https://github.com/apache/arrow/blob/apache-arrow-0.16.0/cpp/src/arrow/python/arrow_to_pandas.cc#L137

See also https://github.com/apache/arrow/issues/6442",pull-request-available,['Python'],ARROW,Improvement,Major,2020-02-18 08:55:21,15
13285855,[Python] Expose more compute kernels,"Currently only the sum kernel is exposed.

Or consider to deprecate/remove the pyarrow.compute module, and bind the compute kernels as methods instead.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-02-17 20:59:45,2
13285805,[Python] Boost::system and boost::filesystem not necessary anymore in Python wheels,Unfortunately it seems we still need boost::regex due to Parquet.,pull-request-available,"['Packaging', 'Python']",ARROW,Task,Major,2020-02-17 15:37:18,2
13285766,[Crossbow] Reduce GitHub API query parallelism ,"With 4 parallel queries we trigger github's abuse detection mechanism, need to reduce the maximum number of parallel builds to one https://ci.ursalabs.org/#/builders/99/builds/261",pull-request-available,['Developer Tools'],ARROW,Task,Major,2020-02-17 12:01:32,3
13285530,[R] Test builds on latest Linux versions,See https://github.com/apache/arrow/issues/6435. CRAN might use old/stable versions but not everyone is so nostalgic.,pull-request-available,['R'],ARROW,Improvement,Major,2020-02-15 22:57:46,4
13285529,[R] Make sure bundled installation works even if there are system packages,"Among the issues:

* In https://github.com/apache/arrow/issues/6435: 0.15 system packages didn't have libarrow_dataset, so if they're installed and you try to install 0.16, pkg-config probably reports that the packages aren't available and it tries to build from source. That's fine except that in the linking step, apparently the system packages are being picked up instead of the static libs we just built, so installation fails (presumably until you either upgrade the system packages or delete them). In general, if we've decided to build/download static libs to match the R package, we should make sure those are the ones that get picked up.
* Whenever pkg-config does find packages, check the version and make sure it matches the R version, and if not, don't use them because they almost certainly won't work.",pull-request-available,['R'],ARROW,Improvement,Major,2020-02-15 22:55:42,4
13285448,[R] Linux installation should run quieter by default,No need to blow up the console by default. Also this solves an {{R CMD check}} warning that surfaced on CRAN.,pull-request-available,['R'],ARROW,Improvement,Major,2020-02-14 22:44:22,4
13285290,[C++][Python] Support casting an Extension type to its storage type,"Currently, casting an extension type will always fail: ""No cast implemented from extension<arrow.py_extension_type> to ..."".

However, for casting, we could fall back to the storage array's casting rules?

",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-02-14 09:32:24,3
13285284,[Python] Failing test with pandas master for extension type conversion,"The pandas master test build has one failure


{code}
_______________ test_conversion_extensiontype_to_extensionarray ________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fcd6c580bd0>

    def test_conversion_extensiontype_to_extensionarray(monkeypatch):
        # converting extension type to linked pandas ExtensionDtype/Array
        import pandas.core.internals as _int
    
        storage = pa.array([1, 2, 3, 4], pa.int64())
        arr = pa.ExtensionArray.from_storage(MyCustomIntegerType(), storage)
        table = pa.table({'a': arr})
    
        if LooseVersion(pd.__version__) < ""0.26.0.dev"":
            # ensure pandas Int64Dtype has the protocol method (for older pandas)
            monkeypatch.setattr(
                pd.Int64Dtype, '__from_arrow__', _Int64Dtype__from_arrow__,
                raising=False)
    
        # extension type points to Int64Dtype, which knows how to create a
        # pandas ExtensionArray
>       result = table.to_pandas()

opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/tests/test_pandas.py:3560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyarrow/ipc.pxi:559: in pyarrow.lib.read_message
    ???
pyarrow/table.pxi:1369: in pyarrow.lib.Table._to_pandas
    ???
opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/pandas_compat.py:764: in table_to_blockmanager
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/pandas_compat.py:1102: in _table_to_blocks
    for item in result]
opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/pandas_compat.py:1102: in <listcomp>
    for item in result]
opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/pandas_compat.py:723: in _reconstruct_block
    pd_ext_arr = pandas_dtype.__from_arrow__(arr)
opt/conda/envs/arrow/lib/python3.7/site-packages/pandas/core/arrays/integer.py:108: in __from_arrow__
    array = array.cast(pyarrow_type)
pyarrow/table.pxi:240: in pyarrow.lib.ChunkedArray.cast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   pyarrow.lib.ArrowNotImplementedError: No cast implemented from extension<arrow.py_extension_type> to int64
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-02-14 09:20:08,5
13285185,[Python] 0.16.0 wheels not compatible with older numpy,"Using python 3.7.5 and numpy 1.14.6, I am unable to import pyarrow 0.16.0 (see below for error). Updating numpy to the most recent version fixes this, and I'm wondering if pyarrow needs update its requirements.txt.


{code:java}
  ~ ipython
Python 3.7.5 (default, Nov  7 2019, 10:50:52)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.9.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import numpy as npIn [2]: np.__version__
Out[2]: '1.14.6'

In [3]: import pyarrow
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-3-f1048abcb32d> in <module>
----> 1 import pyarrow~/.local/lib/python3.7/site-packages/pyarrow/__init__.py in <module>
     47 import pyarrow.compat as compat
     48
---> 49 from pyarrow.lib import cpu_count, set_cpu_count
     50 from pyarrow.lib import (null, bool_,
     51                          int8, int16, int32, int64,~/.local/lib/python3.7/site-packages/pyarrow/lib.pyx in init pyarrow.lib()ImportError: numpy.core.multiarray failed to import

In [4]: import pyarrow
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-f1048abcb32d> in <module>
----> 1 import pyarrow~/.local/lib/python3.7/site-packages/pyarrow/__init__.py in <module>
     47 import pyarrow.compat as compat
     48
---> 49 from pyarrow.lib import cpu_count, set_cpu_count
     50 from pyarrow.lib import (null, bool_,
     51                          int8, int16, int32, int64,~/.local/lib/python3.7/site-packages/pyarrow/ipc.pxi in init pyarrow.lib()AttributeError: type object 'pyarrow.lib.Message' has no attribute '__reduce_cython__'

{code}",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Blocker,2020-02-13 20:04:40,3
13285096,[Packaging][Python] Remove the remaining py27 crossbow wheel tasks from the nightlies,"The nightly tasks are referencing deleted py27 wheel tasks, so the nightly submission has failed: https://ci.ursalabs.org/#/builders/98/builds/536",pull-request-available,"['Packaging', 'Python']",ARROW,Task,Major,2020-02-13 14:17:22,3
13285044,[Website] Write a blog post about fuzzing,At some point we should probably write a blog post about the current fuzzing setup. Perhaps when we have fixed all reported crashes :-),pull-request-available,['Website'],ARROW,Task,Major,2020-02-13 10:38:16,2
13285041,[Python][Dev] Remove last dependencies on six,"Looks like {{six}} (the Python 2-3 compatibility library) is still being used and referenced in a couple of places, notably {{archery}}.",pull-request-available,"['Developer Tools', 'Python']",ARROW,Task,Minor,2020-02-13 10:29:24,2
13284938,[R] array_to_vector is not thread safe,"See [https://travis-ci.org/ursa-labs/arrow-r-nightly/jobs/649649349#L373-L375]for an example on public CI. I was seeing this locally this week but figured I'd screwed up my env somehow.

{code}
 1. Failure: Lists are preserved when writing/reading from Parquet (@test-parq
  `object` not equivalent to `expected`.
  Component ""num"": Component 1: target is numeric, current is character
{code}

It's not always the same column in the data.frame that is affected. Also strange that it's only one column. You'd think that if it were transposing the order somehow, you'd get two that were swapped.

The test itself is straightforward (https://github.com/apache/arrow/blob/master/r/tests/testthat/test-parquet.R#L124-L137) so this is somewhat troubling.",pull-request-available,['R'],ARROW,Bug,Major,2020-02-12 22:48:04,13
13284909,[Rust] [Parquet] Implement array reader for list type,Currently array reader does not support list or map types. The initial PR implementing array readerhttps://issues.apache.org/jira/browse/ARROW-4218says that list and map support will come later. Is it known when support for list types might be implemented?,pull-request-available,['Rust'],ARROW,Improvement,Major,2020-02-12 19:40:56,12
13284886,[C++] HADOOP_HOME doesn't work to find libhdfs.so,"I have my env variable setup correctly according to the pyarrow README
{code:java}
$ ls $HADOOP_HOME/lib/native
libhadoop.a  libhadooppipes.a  libhadoop.so  libhadoop.so.1.0.0  libhadooputils.a  libhdfs.a  libhdfs.so  libhdfs.so.0.0.0 {code}
Use the following script to reproduce
{code:java}
import pyarrow
pyarrow.hdfs.connect('hdfs://localhost'){code}
With pyarrow version 0.15.1 it is fine.

However, version 0.16.0 will give error
{code:java}
Traceback (most recent call last):
  File ""<string>"", line 2, in <module>
  File ""/home/jackwindows/anaconda2/lib/python2.7/site-packages/pyarrow/hdfs.py"", line 215, in connect
    extra_conf=extra_conf)
  File ""/home/jackwindows/anaconda2/lib/python2.7/site-packages/pyarrow/hdfs.py"", line 40, in __init__
    self._connect(host, port, user, kerb_ticket, driver, extra_conf)
  File ""pyarrow/io-hdfs.pxi"", line 89, in pyarrow.lib.HadoopFileSystem._connect
  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
IOError: Unable to load libhdfs: /opt/hadoop/latest/libhdfs.so: cannot open shared object file: No such file or directory {code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-02-12 18:13:49,1
13284784,[Python][Dataset] Add IPC format to python bindings,"The C++ / R was done in ARROW-7415, we should add bindings for it in Python as well.",pull-request-available,['Python'],ARROW,Improvement,Major,2020-02-12 10:58:30,5
13284776,[C++] Installed plasma-store-server fails finding Boost,"In my build directory I have:
{code}
$ ldd build-test/debug/plasma-store-server 
	linux-vdso.so.1 (0x00007ffc0001f000)
	libplasma.so.100 => /home/antoine/arrow/dev/cpp/build-test/debug/libplasma.so.100 (0x00007efbff629000)
	libarrow_cuda.so.100 => /home/antoine/arrow/dev/cpp/build-test/debug/libarrow_cuda.so.100 (0x00007efbff58d000)
	libarrow.so.100 => /home/antoine/arrow/dev/cpp/build-test/debug/libarrow.so.100 (0x00007efbfcbae000)
	libssl.so.1.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libssl.so.1.1 (0x00007efbfcb1e000)
	libcrypto.so.1.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libcrypto.so.1.1 (0x00007efbfc870000)
	libaws-cpp-sdk-config.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-config.so (0x00007efbfc6be000)
	libaws-cpp-sdk-transfer.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-transfer.so (0x00007efbff557000)
	libaws-cpp-sdk-s3.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-s3.so (0x00007efbfc478000)
	libaws-cpp-sdk-core.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-core.so (0x00007efbfc37b000)
	libaws-c-event-stream.so.0unstable => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-event-stream.so.0unstable (0x00007efbff54e000)
	libaws-c-common.so.0unstable => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-common.so.0unstable (0x00007efbff52d000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007efbfbfa2000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007efbfbd83000)
	libaws-checksums.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-checksums.so (0x00007efbff51d000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007efbfbb7b000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007efbfb977000)
	libcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007efbfadd7000)
	libstdc++.so.6 => /home/antoine/miniconda3/envs/pyarrow/lib/libstdc++.so.6 (0x00007efbfac63000)
	libgcc_s.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libgcc_s.so.1 (0x00007efbff507000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007efbfa872000)
	/lib64/ld-linux-x86-64.so.2 (0x00007efbff4d7000)
	libbz2.so.1.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libbz2.so.1.0 (0x00007efbfa85e000)
	liblz4.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/liblz4.so.1 (0x00007efbfa829000)
	libsnappy.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libsnappy.so.1 (0x00007efbfa81e000)
	libz.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libz.so.1 (0x00007efbfa804000)
	libzstd.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libzstd.so.1 (0x00007efbfa748000)
	libboost_filesystem.so.1.68.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libboost_filesystem.so.1.68.0 (0x00007efbfa72a000)
	libboost_system.so.1.68.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libboost_system.so.1.68.0 (0x00007efbff4fe000)
	libcurl.so.4 => /home/antoine/miniconda3/envs/pyarrow/lib/./libcurl.so.4 (0x00007efbfa6a4000)
	libnvidia-fatbinaryloader.so.390.116 => /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.390.116 (0x00007efbfa456000)
	libssh2.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/././libssh2.so.1 (0x00007efbfa423000)
	libgssapi_krb5.so.2 => /home/antoine/miniconda3/envs/pyarrow/lib/././libgssapi_krb5.so.2 (0x00007efbfa3d4000)
	libkrb5.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libkrb5.so.3 (0x00007efbfa2fd000)
	libk5crypto.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libk5crypto.so.3 (0x00007efbfa2de000)
	libcom_err.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libcom_err.so.3 (0x00007efbfa2d6000)
	libkrb5support.so.0 => /home/antoine/miniconda3/envs/pyarrow/lib/./././libkrb5support.so.0 (0x00007efbfa2c8000)
	libresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007efbfa0ad000)
{code}

However, once installed it seems the Boost resolution fails:
{code}
$ ldd /home/antoine/miniconda3/envs/pyarrow/bin/plasma-store-server
	linux-vdso.so.1 (0x00007ffc0001f000)
	libplasma.so.100 => /home/antoine/miniconda3/envs/pyarrow/lib/libplasma.so.100 (0x00007efbff629000)
	libarrow_cuda.so.100 => /home/antoine/miniconda3/envs/pyarrow/lib/libarrow_cuda.so.100 (0x00007efbff58d000)
	libarrow.so.100 => /home/antoine/miniconda3/envs/pyarrow/lib/libarrow.so.100 (0x00007efbfcbae000)
	libssl.so.1.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libssl.so.1.1 (0x00007efbfcb1e000)
	libcrypto.so.1.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libcrypto.so.1.1 (0x00007efbfc870000)
	libaws-cpp-sdk-config.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-config.so (0x00007efbfc6be000)
	libaws-cpp-sdk-transfer.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-transfer.so (0x00007efbff557000)
	libaws-cpp-sdk-s3.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-s3.so (0x00007efbfc478000)
	libaws-cpp-sdk-core.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-core.so (0x00007efbfc37b000)
	libaws-c-event-stream.so.0unstable => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-event-stream.so.0unstable (0x00007efbff54e000)
	libaws-c-common.so.0unstable => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-common.so.0unstable (0x00007efbff52d000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007efbfbfa2000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007efbfbd83000)
	libaws-checksums.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-checksums.so (0x00007efbff51d000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007efbfbb7b000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007efbfb977000)
	libcuda.so.1 => /usr/lib/x86_64-linux-gnu/libcuda.so.1 (0x00007efbfadd7000)
	libstdc++.so.6 => /home/antoine/miniconda3/envs/pyarrow/lib/libstdc++.so.6 (0x00007efbfac63000)
	libgcc_s.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libgcc_s.so.1 (0x00007efbff507000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007efbfa872000)
	/lib64/ld-linux-x86-64.so.2 (0x00007efbff4d7000)
	libbz2.so.1.0 => /lib/x86_64-linux-gnu/libbz2.so.1.0 (0x00007efbfa662000)
	liblz4.so.1 => /usr/lib/x86_64-linux-gnu/liblz4.so.1 (0x00007efbfa446000)
	libsnappy.so.1 => /usr/lib/x86_64-linux-gnu/libsnappy.so.1 (0x00007efbfa23e000)
	libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007efbfa021000)
	libzstd.so.1 => /usr/lib/x86_64-linux-gnu/libzstd.so.1 (0x00007efbf9da6000)
	libboost_filesystem.so.1.68.0 => not found
	libboost_system.so.1.68.0 => not found
	libcurl.so.4 => /home/antoine/miniconda3/envs/pyarrow/lib/./libcurl.so.4 (0x00007efbf9d20000)
	libnvidia-fatbinaryloader.so.390.116 => /usr/lib/x86_64-linux-gnu/libnvidia-fatbinaryloader.so.390.116 (0x00007efbf9ad4000)
	libssh2.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/././libssh2.so.1 (0x00007efbf9aa1000)
	libgssapi_krb5.so.2 => /home/antoine/miniconda3/envs/pyarrow/lib/././libgssapi_krb5.so.2 (0x00007efbf9a52000)
	libkrb5.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libkrb5.so.3 (0x00007efbf997b000)
	libk5crypto.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libk5crypto.so.3 (0x00007efbf995c000)
	libcom_err.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libcom_err.so.3 (0x00007efbf9956000)
	libkrb5support.so.0 => /home/antoine/miniconda3/envs/pyarrow/lib/./././libkrb5support.so.0 (0x00007efbf9948000)
	libresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007efbf972d000)
{code}

Other libraries in the same directory are found, so I don't know what's happening here.
",pull-request-available,"['C++', 'C++ - Plasma']",ARROW,Bug,Major,2020-02-12 10:27:16,2
13284694,[Release] Post release task for updating the documentations,Use the ubuntu-docs docker container to build the documentations and add a post-release script.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-02-12 00:11:32,3
13284602,[R] Patches to 0.16.0 release,CRAN did not like 0.16.0 as originally submitted. This contains the patches in the 0.16.0.1 resubmission.,pull-request-available,['R'],ARROW,Bug,Major,2020-02-11 14:55:26,4
13284425,[R] Test R bindings on clang,Followup to ARROW-7817. We're generating warnings that will fail a CRAN submission but due to Travis stuff we aren't catching them.,pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2020-02-10 22:25:36,4
13284424,[Release] Remove SSH keys for internal use,"{{dev/release/binary/id_rsa*}} SSH keys are only used to login to local Docker container for
releasing binary artifacts. They aren't used over network. So putting
them to this public repository is safe.

But there are people who report ""they may be danger"" to us. We should
remove them from this repository and generate them locally instead of
describing why they aren't danger to reporters.",pull-request-available,['Packaging'],ARROW,Improvement,Minor,2020-02-10 22:23:18,1
13284380,[R] Update docs to clarify that stringsAsFactors isn't relevant for parquet/feather,"Same issue as reported for feather::read_feather (https://issues.apache.org/jira/browse/ARROW-7823);



For the R arrow package, the ""read_parquet()"" function currently does not respect ""options(stringsAsFactors = FALSE)"", leading to unexpected/inconsistent behavior.



*Example:*




{code:java}
library(arrow)
library(readr)
options(stringsAsFactors = FALSE)
write_tsv(head(iris), 'test.tsv')
write_parquet(head(iris), 'test.parquet')
head(read.delim('test.tsv', sep='\t')$Species)
# [1] ""setosa"" ""setosa"" ""setosa"" ""setosa"" ""setosa"" ""setosa""
head(read_tsv('test.tsv', col_types = cols())$Species)
# [1] ""setosa"" ""setosa"" ""setosa"" ""setosa"" ""setosa"" ""setosa""
head(read_parquet('test.parquet')$Species)
# [1] setosa setosa setosa setosa setosa setosa
# Levels: setosa versicolor virginica
{code}




*Versions:*

- R 3.6.2

- arrow_0.15.1.9000",R parquet,['R'],ARROW,Improvement,Major,2020-02-10 17:27:48,4
13284377,[C++][Dataset] Provide Dataset writing to IPC format,"Begin with writing to IPC format since it is simpler than parquet and to efficiently support the ""locally cached extract"" workflow.",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-10 17:20:59,6
13284363,[C++][Gandiva] Add DumpIR to Filter/Projector classes,"The tool should take a protobuf expression from stdin and dump the IR to stdout. This might require some though as the schema is not always known. It could mean a refactor to support plain array, especially for the Filter kernel.",pull-request-available,['C++ - Gandiva'],ARROW,Sub-task,Major,2020-02-10 16:26:45,13
13284348,[CI] macOS R autobrew nightly failed on installing dependency from source,"Failing build: https://travis-ci.org/ursa-labs/crossbow/builds/648308138
Probably an OSX SDK issue.

cc [~npr]",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-02-10 15:19:05,4
13284339,[C++] Fix crashes on corrupt IPC input (OSS-Fuzz),"More issues have been discovered with OSS-Fuzz, we need to enhance input validation.",pull-request-available,['C++'],ARROW,Bug,Major,2020-02-10 15:09:14,2
13284056,[C++][R] Compile error on macOS 10.11,"So, yeah, macOS 10.11 is from 2015, but that's what CRAN (R) builds on. [~jeroenooms] found this error when building binaries for 0.16:

{code}
==> cmake ../cpp -DCMAKE_C_FLAGS_RELEASE=-DNDEBUG -DCMAKE_CXX_FLAGS_RELEASE=-DNDEBUG -DCMAKE_INSTALL_PREFIX=/usr/local/Cellar/apache-arrow/
==> make
Last 15 lines from /Users/builder/Library/Logs/Homebrew/apache-arrow/02.make:
/tmp/apache-arrow-20200208-1254-r2avjm/apache-arrow-0.16.0/cpp/src/parquet/stream_writer.h:233:22: error: default initialization of an object of const type 'const parquet::EndRowType' without a user-provided default constructor
constexpr EndRowType EndRow;
                     ^
                           {}
/tmp/apache-arrow-20200208-1254-r2avjm/apache-arrow-0.16.0/cpp/src/parquet/stream_writer.h:236:27: error: default initialization of an object of const type 'const parquet::EndRowGroupType' without a user-provided default constructor
constexpr EndRowGroupType EndRowGroup;
                          ^
                                     {}
2 errors generated.
{code}",pull-request-available,"['C++', 'Continuous Integration', 'R']",ARROW,Improvement,Major,2020-02-08 04:06:19,4
13284052,[R][CI] Autobrew/homebrew tests should not always install from master,Figure out how to get the formula to check out a branch when building {{--head}},pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-02-08 01:53:56,4
13284019,[Developer] Add issue_comment workflow to fix lint/style/codegen,"Like https://github.com/r-lib/actions/tree/master/examples#render-readme. 

* If changes to r/README.Rmd, render readme
* If changes to r/R, render docs
* If changes to r/src, lint.sh --fix",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-02-07 21:52:42,4
13283994,[R][CI] Remove flatbuffers from homebrew formulae,ARROW-6634 vendored flatbuffers so these shouldn't be needed anymore.,pull-request-available,"['Continuous Integration', 'Packaging', 'R']",ARROW,Improvement,Major,2020-02-07 19:05:34,4
13283953,[Rust] cargo publish fails for arrow-flight due to relative path to Flight.proto,"Running ""cargo publish"" for the arrow-flight crate resulted in this error:
{code:java}
error: failed to run custom build command for `arrow-flight v0.16.0 (/home/andy/apache-arrow-0.16.0/rust/target/package/arrow-flight-0.16.0)`Caused by:
  process didn't exit successfully: `/home/andy/apache-arrow-0.16.0/rust/target/package/arrow-flight-0.16.0/target/debug/build/arrow-flight-1b2906a3933d2832/build-script-build` (exit code: 1)
--- stderr
Error: Custom { kind: Other, error: ""protoc failed: ../../format: warning: directory does not exist.\nCould not make proto path relative: ../../format/Flight.proto: No such file or directory\n"" }
 {code}
The workaround was to edit the build.rs and make the path absolute and then run ""cargo publish --allow-dirty"", but we should find a better solution before the next release.",pull-request-available,['Rust'],ARROW,Bug,Major,2020-02-07 14:33:27,10
13283905,[R] read_* functions should close connection to file,"x = as.data.frame(seq(1:100))

pbFilename <- file.path(getwd(), ""reproduceBug.feather"")

arrow::write_feather(x = x, sink = pbFilename)
file.exists(pbFilename)
file.remove(pbFilename)

arrow::write_feather(x = x, sink = pbFilename)
tempDX <- arrow::read_feather(file = pbFilename, as_data_frame = T)

file.exists(pbFilename)
file.remove(pbFilename)

>Warning message:
>In file.remove(pbFilename) :
>cannot remove file 'C:/Martin/Repo/ReinforcementLearner/reproduceBug.feather', reason

> 'Permission denied'

",pull-request-available,['R'],ARROW,Bug,Major,2020-02-07 10:24:46,4
13283867,[R] Can't initialize arrow objects when R.oo package is loaded,"Unknown error when usingarrow::write_feather() inR 3.5.3

pb = as.data.frame(seq(1:100))

pbFilename <- file.path(getwd(), ""reproduceBug.feather"")
 arrow::write_feather(x = pb, sink = pbFilename)

>Error in exists(name, envir = envir, inherits = FALSE) : 
 > use of NULL environment is defunct



packageVersion('arrow')
[1] 0.15.1.1",pull-request-available,['R'],ARROW,Bug,Minor,2020-02-07 08:12:12,4
13283845,[C++] Add schema conversion support for map type,"there is also some other cleanup that is probably worth doing:

1. Adding ""large types""

2. Adding a flag to support parquet spec required naming for list types.",pull-request-available,['C++'],ARROW,Sub-task,Major,2020-02-07 06:46:27,15
13283757,[R] Wire up check_metadata in Table.Equals method,See https://github.com/apache/arrow/pull/6318/files#r375404306. Followup to ARROW-7720.,pull-request-available,['R'],ARROW,Bug,Major,2020-02-06 18:09:11,4
13283732,[C++] diff.cc is extremely slow to compile,"This comes up especially when doing an optimized build. {{diff.cc}} is always enabled even if all components are disabled, and it takes multiple seconds to compile. ",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-02-06 15:54:53,14
13283725,[C++] ARROW_DATASET should enable ARROW_COMPUTE,"Currenty, passing {{-DARROW_DATASET=ON}} to CMake doesn't enable ARROW_COMPUTE, which leads to linker errors.",pull-request-available,['C++'],ARROW,Bug,Major,2020-02-06 15:17:18,14
13283722,[C++][Dataset] Filtering on a non-existent column gives a segfault,"Example with python code:

{code}
In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'a': [1, 2, 3]})

In [3]: df.to_parquet(""test-filter-crash.parquet"")

In [4]: import pyarrow.dataset as ds

In [5]: dataset = ds.dataset(""test-filter-crash.parquet"")

In [6]: dataset.to_table(filter=ds.field('a') > 1).to_pandas()
Out[6]:
   a
0  2
1  3

In [7]: dataset.to_table(filter=ds.field('b') > 1).to_pandas()
../src/arrow/dataset/filter.cc:929:  Check failed: _s.ok() Operation failed: maybe_value.status()
Bad status: Invalid: attempting to cast non-null scalar to NullScalar
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.16(+0x11f744c)[0x7fb1390f444c]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.16(+0x11f73ca)[0x7fb1390f43ca]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.16(+0x11f73ec)[0x7fb1390f43ec]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow.so.16(_ZN5arrow4util8ArrowLogD1Ev+0x57)[0x7fb1390f4759]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.16(+0x169fc6)[0x7fb145594fc6]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.16(+0x16b9be)[0x7fb1455969be]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.16(_ZN5arrow7dataset15VisitExpressionINS0_23InsertImplicitCastsImplEEEDTclfp0_fp_EERKNS0_10ExpressionEOT_+0x2ae)[0x7fb1455a0dee]
/home/joris/miniconda3/envs/arrow-dev/lib/libarrow_dataset.so.16(_ZN5arrow7dataset19InsertImplicitCastsERKNS0_10ExpressionERKNS_6SchemaE+0x44)[0x7fb145596d4e]
/home/joris/scipy/repos/arrow/python/pyarrow/_dataset.cpython-37m-x86_64-linux-gnu.so(+0x48286)[0x7fb1456dd286]
/home/joris/scipy/repos/arrow/python/pyarrow/_dataset.cpython-37m-x86_64-linux-gnu.so(+0x49220)[0x7fb1456de220]
/home/joris/miniconda3/envs/arrow-dev/bin/python(+0x170f37)[0x55e5127e1f37]
/home/joris/scipy/repos/arrow/python/pyarrow/_dataset.cpython-37m-x86_64-linux-gnu.so(+0x22bd6)[0x7fb1456b7bd6]
/home/joris/scipy/repos/arrow/python/pyarrow/_dataset.cpython-37m-x86_64-linux-gnu.so(+0x33b81)[0x7fb1456c8b81]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyMethodDef_RawFastCallKeywords+0x305)[0x55e5127d9c75]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyCFunction_FastCallKeywords+0x21)[0x55e5127d9cf1]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x5460)[0x55e512847c40]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyEval_EvalCodeEx+0x44)[0x55e512789064]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyEval_EvalCode+0x1c)[0x55e51278908c]
/home/joris/miniconda3/envs/arrow-dev/bin/python(+0x1e1650)[0x55e512852650]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyMethodDef_RawFastCallKeywords+0xe9)[0x55e5127d9a59]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyCFunction_FastCallKeywords+0x21)[0x55e5127d9cf1]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x48e4)[0x55e5128470c4]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyGen_Send+0x2a2)[0x55e5127e31a2]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x1a83)[0x55e512844263]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyGen_Send+0x2a2)[0x55e5127e31a2]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x1a83)[0x55e512844263]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyGen_Send+0x2a2)[0x55e5127e31a2]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyMethodDef_RawFastCallKeywords+0x8c)[0x55e5127d99fc]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyMethodDescr_FastCallKeywords+0x4f)[0x55e5127e1fdf]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x4ddc)[0x55e5128475bc]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0xfb)[0x55e5127d915b]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x416)[0x55e512842bf6]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0xfb)[0x55e5127d915b]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x6f3)[0x55e512842ed3]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0x387)[0x55e5127d93e7]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x14e4)[0x55e512843cc4]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0x325)[0x55e5127d9385]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x6f3)[0x55e512842ed3]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0x325)[0x55e5127d9385]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x6f3)[0x55e512842ed3]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0xfb)[0x55e5127d915b]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x6f3)[0x55e512842ed3]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallDict+0x400)[0x55e5127894a0]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyObject_Call_Prepend+0x63)[0x55e5127a8393]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyObject_Call+0x6e)[0x55e51279adce]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x1f5b)[0x55e51284473b]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyFunction_FastCallKeywords+0x387)[0x55e5127d93e7]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalFrameDefault+0x416)[0x55e512842bf6]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_PyEval_EvalCodeWithName+0x2f9)[0x55e5127881a9]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyEval_EvalCodeEx+0x44)[0x55e512789064]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyEval_EvalCode+0x1c)[0x55e51278908c]
/home/joris/miniconda3/envs/arrow-dev/bin/python(+0x230344)[0x55e5128a1344]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyRun_FileExFlags+0xa1)[0x55e5128ab5c1]
/home/joris/miniconda3/envs/arrow-dev/bin/python(PyRun_SimpleFileExFlags+0x1c3)[0x55e5128ab7b3]
/home/joris/miniconda3/envs/arrow-dev/bin/python(+0x23b8cf)[0x55e5128ac8cf]
/home/joris/miniconda3/envs/arrow-dev/bin/python(_Py_UnixMain+0x3c)[0x55e5128ac9ec]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7)[0x7fb1576bfb97]
/home/joris/miniconda3/envs/arrow-dev/bin/python(+0x1e171d)[0x55e51285271d]
Aborted (core dumped)
{code}

which is not very nice ;) ",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-02-06 15:00:39,13
13283708,"[Release] Fix Windows wheel RC verification script given lack of ""m"" ABI tag in Python 3.8","Python 3.8 wheels don't have the ""m"" postfix in their ABI tag.",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-02-06 14:03:10,14
13283607,[C++] Support nested dictionaries in JSON integration format,The {{generate_nested_dictionary_case}} is disabled for all library implementations. We support dictionaries-within-dictionaries in IPC (I believe) and so need to integration test this,pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-06 00:56:45,2
13283559,[Packaging][Python] Update macos and windows wheel filenames,After https://github.com/apache/arrow/commit/67e34c53b3be4c88348369f8109626b4a8a997aa the macosx wheels have different platform tag and the windows wheel tag needs to be fixed as well.,pull-request-available,['Packaging'],ARROW,Task,Major,2020-02-05 19:34:01,3
13283327,[R][C++][Dataset] Unable to filter on date32 object with date64 scalar,"I am trying to filter on a date column using `open_dataset()` and `dplyr::filter()`:

{code}
library(arrow)
library(dplyr)

tmp <- tempfile()
dir.create(tmp)
df <- data.frame(date = Sys.Date())
write_parquet(df, file.path(tmp, ""file.parquet""))

ds <- open_dataset(tmp)


ds %>%
 filter(date > as.Date(""2020-02-02"")) %>%
 collect()
{code}


This code crashes R with this error message:
{quote}/private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/hbtmp/apache-arrow-20200203-29929-1uoyri7/cpp/src/arrow/result.cc:28: ValueOrDie called on an error: NotImplemented: casting scalarsof type date64[ms] to type date32[day]
0 arrow.so 0x0000000104461f1d _ZN5arrow4util7CerrLogD2Ev + 209
1 arrow.so 0x0000000104461e3e _ZN5arrow4util7CerrLogD0Ev + 14
2 arrow.so 0x0000000104461de6 _ZN5arrow4util8ArrowLogD1Ev + 34
3 arrow.so 0x000000010436c57f _ZN5arrow8internal14DieWithMessageERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE + 63
4 arrow.so 0x000000010436d384 _ZNR5arrow6ResultINSt3__110shared_ptrINS_6ScalarEEEE10ValueOrDieEv + 192
5 arrow.so 0x000000010426ee1a _ZN5arrow7dataset23InsertImplicitCastsImpl4CastENSt3__110shared_ptrINS_8DataTypeEEEPNS3_INS0_10ExpressionEEE + 186
6 arrow.so 0x000000010426e033 _ZN5arrow7dataset23InsertImplicitCastsImplclERKNS0_20ComparisonExpressionE + 757
7 arrow.so 0x0000000104269de7 _ZN5arrow7dataset15VisitExpressionINS0_23InsertImplicitCastsImplEEEDTclfp0_fp_EERKNS0_10ExpressionEOT_ + 317
8 arrow.so 0x0000000104269c76 _ZN5arrow7dataset19InsertImplicitCastsERKNS0_10ExpressionERKNS_6SchemaE + 36
9 arrow.so 0x0000000104134e04 _Z32dataset___ScannerBuilder__FilterRKNSt3__110shared_ptrIN5arrow7dataset14ScannerBuilderEEERKNS0_INS2_10ExpressionEEE + 52
10 arrow.so 0x00000001040f35b7 _arrow_dataset___ScannerBuilder__Filter + 135
11 libR.dylib 0x00000001001f375b R_doDotCall + 955
12 libR.dylib 0x000000010023d46a bcEval + 99306
13 libR.dylib 0x000000010022494d Rf_eval + 445
14 libR.dylib 0x0000000100243209 R_execClosure + 2153
15 libR.dylib 0x000000010024210a Rf_applyClosure + 346
16 libR.dylib 0x0000000100224e7d Rf_eval + 1773
17 libR.dylib 0x0000000100245880 do_begin + 432
18 libR.dylib 0x0000000100224b40 Rf_eval + 944
19 libR.dylib 0x0000000100243209 R_execClosure + 2153
20 libR.dylib 0x000000010024210a Rf_applyClosure + 346
21 libR.dylib 0x000000010022bd71 bcEval + 27889
22 libR.dylib 0x000000010022494d Rf_eval + 445
23 libR.dylib 0x0000000100243209 R_execClosure + 2153
24 libR.dylib 0x000000010024210a Rf_applyClosure + 346
25 libR.dylib 0x0000000100287675 dispatchMethod + 757
26 libR.dylib 0x0000000100287332 Rf_usemethod + 738
27 libR.dylib 0x0000000100287926 do_usemethod + 646
28 libR.dylib 0x000000010022c369 bcEval + 29417
29 libR.dylib 0x000000010022494d Rf_eval + 445
30 libR.dylib 0x0000000100243209 R_execClosure + 2153
31 libR.dylib 0x000000010024210a Rf_applyClosure + 346
32 libR.dylib 0x0000000100224e7d Rf_eval + 1773
33 libR.dylib 0x0000000100243209 R_execClosure + 2153
34 libR.dylib 0x000000010024210a Rf_applyClosure + 346
35 libR.dylib 0x000000010022bd71 bcEval + 27889
36 libR.dylib 0x000000010022494d Rf_eval + 445
37 libR.dylib 0x00000001002418c3 forcePromise + 179
38 libR.dylib 0x0000000100224c30 Rf_eval + 1184
39 libR.dylib 0x0000000100247241 do_withVisible + 49
40 libR.dylib 0x0000000100286603 do_internal + 339
41 libR.dylib 0x000000010022c369 bcEval + 29417
42 libR.dylib 0x000000010022494d Rf_eval + 445
43 libR.dylib 0x0000000100243209 R_execClosure + 2153
44 libR.dylib 0x000000010024210a Rf_applyClosure + 346
45 libR.dylib 0x000000010022bd71 bcEval + 27889
46 libR.dylib 0x000000010022494d Rf_eval + 445
47 libR.dylib 0x0000000100243209 R_execClosure + 2153
48 libR.dylib 0x000000010024210a Rf_applyClosure + 346
49 libR.dylib 0x0000000100224e7d Rf_eval + 1773
50 libR.dylib 0x0000000100243209 R_execClosure + 2153
51 libR.dylib 0x000000010024210a Rf_applyClosure + 346
52 libR.dylib 0x0000000100224e7d Rf_eval + 1773
53 libR.dylib 0x0000000100246bc6 do_eval + 646
54 libR.dylib 0x000000010022c186 bcEval + 28934
55 libR.dylib 0x000000010022494d Rf_eval + 445
56 libR.dylib 0x0000000100243209 R_execClosure + 2153
57 libR.dylib 0x000000010024210a Rf_applyClosure + 346
58 libR.dylib 0x000000010022bd71 bcEval + 27889
59 libR.dylib 0x000000010022494d Rf_eval + 445
60 libR.dylib 0x00000001002418c3 forcePromise + 179
61 libR.dylib 0x0000000100224c30 Rf_eval + 1184
62 libR.dylib 0x0000000100247241 do_withVisible + 49
63 libR.dylib 0x0000000100286603 do_internal + 339
64 libR.dylib 0x000000010022c369 bcEval + 29417
65 libR.dylib 0x000000010022494d Rf_eval + 445
66 libR.dylib 0x0000000100243209 R_execClosure + 2153
67 libR.dylib 0x000000010024210a Rf_applyClosure + 346
68 libR.dylib 0x000000010022bd71 bcEval + 27889
69 libR.dylib 0x000000010022494d Rf_eval + 445
70 libR.dylib 0x0000000100243209 R_execClosure + 2153
71 libR.dylib 0x000000010024210a Rf_applyClosure + 346
72 libR.dylib 0x0000000100224e7d Rf_eval + 1773
73 libR.dylib 0x000000010027506a Rf_ReplIteration + 794
74 libR.dylib 0x000000010027658f run_Rmainloop + 207
75 R 0x000000010016cf5b main + 27
76 libdyld.dylib 0x00007fff7de493d5 start + 1
Abort trap: 6
{quote}


Thanks to Neal Richardson for help with the above reprex and putting words to the issue - Neal says:
{quote}""the 3 bugs are: (1) that should not crash; (2) R should translate Date in the filter expression to date32, just as it does for vectors; (3) the C++ layer should probably support that date64 to date32 cast (up for debate and technically not a bug, just not yet implemented I guess)"".
{quote}


session_info():
{quote} Session info  setting value version R version 3.6.2 (2019-12-12) os macOS Mojave 10.14.6 system x86_64, darwin15.6.0 ui RStudio language (EN) collate en_CA.UTF-8 ctype en_CA.UTF-8 tz America/Vancouver date 2020-02-04  Packages  package * version date lib source arrow * 0.15.1.20200203 2020-02-03 [1] local assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) backports 1.1.5 2019-10-02 [1] CRAN (R 3.6.0) bit 1.1-15.1 2020-01-14 [1] CRAN (R 3.6.0) bit64 0.9-7 2017-05-08 [1] CRAN (R 3.6.0) bortles 0.0.0.9000 2019-11-12 [1] Github (adam-gruer/bortles@068a1b1) callr 3.4.1 2020-01-24 [1] CRAN (R 3.6.0) cli 2.0.1 2020-01-08 [1] CRAN (R 3.6.0) clisymbols 1.2.0 2017-05-21 [1] CRAN (R 3.6.0) crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) desc 1.2.0 2018-05-01 [1] CRAN (R 3.6.0) devtools * 2.2.1 2019-09-24 [1] CRAN (R 3.6.1) digest 0.6.23 2019-11-23 [1] CRAN (R 3.6.0) dplyr * 0.8.4 2020-01-31 [1] CRAN (R 3.6.0) ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) fs 1.3.1 2019-05-06 [1] CRAN (R 3.6.0) glue 1.3.1 2019-03-12 [1] CRAN (R 3.6.0) here * 0.1 2017-05-28 [1] CRAN (R 3.5.0) htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) knitr 1.27 2020-01-16 [1] CRAN (R 3.6.2) magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) memoise 1.1.0 2017-04-21 [1] CRAN (R 3.6.0) packrat 0.5.0 2018-11-14 [1] CRAN (R 3.6.0) pillar 1.4.3 2019-12-20 [1] CRAN (R 3.6.0) pkgbuild 1.0.6 2019-10-09 [1] CRAN (R 3.6.0) pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) pkgload 1.0.2 2018-10-29 [1] CRAN (R 3.6.0) praise 1.0.0 2015-08-11 [1] CRAN (R 3.5.0) prettyunits 1.1.1 2020-01-24 [1] CRAN (R 3.6.0) processx 3.4.1 2019-07-18 [1] CRAN (R 3.6.0) prompt 1.0.0 2020-01-13 [1] Github (gaborcsardi/prompt@b332c42) ps 1.3.0 2018-12-21 [1] CRAN (R 3.6.0) purrr 0.3.3 2019-10-18 [1] CRAN (R 3.6.0) R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.0) Rcpp 1.0.3 2019-11-08 [1] CRAN (R 3.6.0) remotes 2.1.0 2019-06-24 [1] CRAN (R 3.6.0) reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) rlang 0.4.4 2020-01-28 [1] CRAN (R 3.6.0) rmarkdown 2.1 2020-01-20 [1] CRAN (R 3.6.0) rprojroot 1.3-2 2018-01-03 [1] CRAN (R 3.6.0) rstudioapi 0.10 2019-03-19 [1] CRAN (R 3.6.0) sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) testthat * 2.3.1 2019-12-01 [1] CRAN (R 3.6.0) tibble 2.1.3 2019-06-06 [1] CRAN (R 3.6.0) tictoc * 1.0 2014-06-17 [1] CRAN (R 3.6.0) tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) usethis * 1.5.1 2019-07-04 [1] CRAN (R 3.6.0) vctrs 0.2.2 2020-01-24 [1] CRAN (R 3.6.0) whisker 0.4 2019-08-28 [1] CRAN (R 3.6.0) withr 2.1.2 2018-03-15 [1] CRAN (R 3.6.0) xfun 0.12 2020-01-13 [1] CRAN (R 3.6.0)
{quote}",dataset newbie pull-request-available,"['C++', 'R']",ARROW,Bug,Major,2020-02-04 22:01:16,6
13283326,[Developer] Use ARROW_TMPDIR environment variable in the verification scripts instead of TMPDIR,See discussion https://github.com/apache/arrow/pull/6344#issuecomment-582128686,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-02-04 21:57:50,3
13283301,[Python][Packaging] Windows py38 wheels are built with wrong ABI tag,"File paths have {{cp38m}} in them, which confuses pip.",pull-request-available wheel,"['Packaging', 'Python']",ARROW,Bug,Major,2020-02-04 18:19:40,4
13283271,[C++] Builders allocate a null bitmap buffer even if there is no nulls,This is an optimization where we can coalesce to nullptr if there's no null in the array.,pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-04 15:51:30,2
13283260,[Python] Exceptions in ParquetWriter get ignored,"For example:

{code:python}
In [43]: table = pa.table({'a': [1, 2, 3]}) 

In [44]: pq.write_table(table, ""test.parquet"", version=""2.2"")                                                                                                                                                      
---------------------------------------------------------------------------
ArrowException                            Traceback (most recent call last)
ArrowException: Unsupported Parquet format version
Exception ignored in: 'pyarrow._parquet.ParquetWriter._set_version'
pyarrow.lib.ArrowException: Unsupported Parquet format version
{code}

",parquet pull-request-available,['Python'],ARROW,Bug,Major,2020-02-04 14:37:47,5
13283241,[C++] Add S3 support to fs::FileSystemFromUri,FileSystemFromUri doesn't support S3. This would give almost immediate support for S3 in python/R.,pull-request-available,['C++'],ARROW,Improvement,Minor,2020-02-04 13:07:30,2
13283136,[C++][Dataset] Add CsvFileFormat for CSV support,This should be a minimal implementation that binds 1-1 file and ScanTask for now. Streaming optimizations  can be done in ARROW-3410.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-04 03:04:03,6
13283130,[Python] Wrong conversion of timestamps that are out of bounds for pandas (eg 0000-01-01),"Using pandas.read_parquet() with pyarrow as the engine produces ValueError when the parquet file contains a date column with the value 0000-01-01.

PySpark can read the same parquet with no issues and PyArrow up to version 0.11.1 could read it as well.


{code:java}
// code placeholder

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-7-06e3cce13e18> in <module>
----> 1 df_init_df = read_parquet_files('{}/DebtFacility'.format(ext_path))

<ipython-input-4-f12125c1c8fe> in read_parquet_files(folder_path)
      2     files = [f for f in os.listdir(folder_path) if f.endswith('parquet')]
      3 
----> 4     df_list = [pd.read_parquet(os.path.join(folder_path, f)) for f in files]
      5 
      6     print(files)

<ipython-input-4-f12125c1c8fe> in <listcomp>(.0)
      2     files = [f for f in os.listdir(folder_path) if f.endswith('parquet')]
      3 
----> 4     df_list = [pd.read_parquet(os.path.join(folder_path, f)) for f in files]
      5 
      6     print(files)

/opt/conda/lib/python3.6/site-packages/pandas/io/parquet.py in read_parquet(path, engine, columns, **kwargs)
    294 
    295     impl = get_engine(engine)
--> 296     return impl.read(path, columns=columns, **kwargs)

/opt/conda/lib/python3.6/site-packages/pandas/io/parquet.py in read(self, path, columns, **kwargs)
    123         kwargs[""use_pandas_metadata""] = True
    124         result = self.api.parquet.read_table(
--> 125             path, columns=columns, **kwargs
    126         ).to_pandas()
    127         if should_close:

/opt/conda/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()

/opt/conda/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table._to_pandas()

/opt/conda/lib/python3.6/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, categories, ignore_metadata)
    702 
    703     _check_data_column_metadata_consistency(all_columns)
--> 704     blocks = _table_to_blocks(options, table, categories)
    705     columns = _deserialize_column_index(table, all_columns, column_indexes)
    706 

/opt/conda/lib/python3.6/site-packages/pyarrow/pandas_compat.py in _table_to_blocks(options, block_table, categories)
    974 
    975     # Convert an arrow table to Block from the internal pandas API
--> 976     result = pa.lib.table_to_blocks(options, block_table, categories)
    977 
    978     # Defined above

/opt/conda/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.table_to_blocks()

ValueError: year -1 is out of range

{code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-02-04 02:27:02,5
13283044,[C++] Result<T> is slow,"When converting a short performance-critical function to return a Result (instead of returning a Status and filling a out-parameter), I noticed a catastrophic performance regression (around 2x or 3x slower).

It seems the current Result implementation is very slow, for several reasons:
- it imposes ""safety"" features even in release mode, for example on the critical path of move operators
- the underlying mpark variant implementation is not optimized for performance-critical data structures
",pull-request-available,['C++'],ARROW,Improvement,Major,2020-02-03 20:50:43,2
13283040,[Release] Enable and test dataset in the verification script,We're not testing the dataset feature during the release verification.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-02-03 19:42:43,3
13283037,[Release] macOS wheel verification also needs arrow-testing,"Without it, Flight tests error (with a misleading message)",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-02-03 19:18:17,4
13283028,[Release] Make the source release verification script restartable,"Executing the verification script can take quite some time, so creating a new environment in case if anything fails is time consuming. 

Let the script reuse the same build directory for source release verification.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-02-03 18:15:52,3
13283027,[C++] Link some more tests together,"With unity builds (ARROW-7725) it may become more beneficial to reduce the number of test executables, as several C++ files could be compiled together so as to reduce build times.",pull-request-available,['C++'],ARROW,Improvement,Minor,2020-02-03 18:15:25,2
13282814,[C++][Parquet] Incorporate new level generation logic in parquet write path with a flag to revert back to old logic,This is likely going to be a decent amount of changes we should isolate them behind a feature flag.,pull-request-available,['C++'],ARROW,Sub-task,Minor,2020-02-02 05:47:37,15
13282775,[C++] Array internals corruption in StructArray::Flatten,"Reading a nested ndjson file using arrow::read_json_arrow with the default `as_data_frame=TRUE` causes an immediate session crash, but switching to `as_data_frame=FALSE` works fine and the resulting arrow object schema is correct.
{code:java}
library(tidyr)
library(arrow)
library(jsonlite)
# Create two test datasets: long_df and a variant that nests long_df into
# a dataframe with a list-column 'nest_level1' containing a dataframe
long_df <- tidyr::expand_grid(ABC = LETTERS[1:3], xyz = letters[24:26], num = 1:3)
long_df[[""ftr1""]] <- runif(nrow(long_df))
long_df[[""ftr2""]] <- rpois(nrow(long_df), 100)
nested_frame_level1 <- tidyr::nest(long_df, nest_level1 = c(num, ftr1, ftr2))
# Write and validate nested ndjson
jsonlite::stream_out(nested_frame_level1, con = file(""nested_frame_level1.json""))
readLines(""nested_frame_level1.json"", n = 2) # check we have valid ndjson here
# This does not cause a session crash
nested_arrow <- arrow::read_json_arrow(file = ""nested_frame_level1.json"", as_data_frame = FALSE)
nested_arrow$schema # correctly interprets 'nest_level1` as `list<item: struct<num: int64, ftr1: double, ftr2: int64>>`
# This causes a session crash
nested_df <- arrow::read_json_arrow(file = ""nested_frame_level1.json"", as_data_frame = TRUE)

{code}
The R package version of Arrow is latest CRAN release (arrow * 0.15.1.1, 2019-11-05, CRAN (R 3.5.2)). I'm running this code in a slightly older R version (3.5.1), macOS10.14.6,x86_64, darwin15.6.0, via RStudio 1.2.5001.

[edit: formatting fix]",pull-request-available,['C++'],ARROW,Bug,Critical,2020-02-01 16:28:12,13
13282753,[Release] conda-forge channel is missing for verifying wheels,"{noformat}
+ conda install -y --file /home/kou/work/cpp/arrow.kou/ci/conda_env_python.yml pandas
+ '[' 5 -lt 1 ']'
+ local cmd=install
+ shift
+ case ""$cmd"" in
+ OLDPATH=/tmp/arrow-0.16.0.j9Uct/test-miniconda/envs/_verify_wheel-2.7mu/bin:/tmp/arrow-0.16.0.j9Uct/test-miniconda/condabin:/home/kou/work/go/bin:/bin:/home/kou/local/bin:/home/kou/.config/composer/vendor/bin:/var/lib/gems/2.5.0/bin:/usr/local/bin:/usr/bin:/usr/games
+ __add_sys_prefix_to_path
+ '[' -n '' ']'
++ dirname /tmp/arrow-0.16.0.j9Uct/test-miniconda/bin/conda
+ SYSP=/tmp/arrow-0.16.0.j9Uct/test-miniconda/bin
++ dirname /tmp/arrow-0.16.0.j9Uct/test-miniconda/bin
+ SYSP=/tmp/arrow-0.16.0.j9Uct/test-miniconda
+ '[' -n '' ']'
+ PATH=/tmp/arrow-0.16.0.j9Uct/test-miniconda/bin:/tmp/arrow-0.16.0.j9Uct/test-miniconda/envs/_verify_wheel-2.7mu/bin:/tmp/arrow-0.16.0.j9Uct/test-miniconda/condabin:/home/kou/work/go/bin:/bin:/home/kou/local/bin:/home/kou/.config/composer/vendor/bin:/var/lib/gems/2.5.0/bin:/usr/local/bin:/usr/bin:/usr/games
+ export PATH
+ /tmp/arrow-0.16.0.j9Uct/test-miniconda/bin/conda install -y --file /home/kou/work/cpp/arrow.kou/ci/conda_env_python.yml pandas
Collecting package metadata (current_repodata.json): ...working... done
Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - pytest-faulthandler
  - pytest-lazy-fixture

Current channels:

  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-02-01 11:21:32,3
13282720,[C++] Segfault when comparing status with and without detail,I noticed this while working on Flight integration tests. The equality operator for Status doesn't check whether the status detail is nullptr before dereferencing it.,pull-request-available,['C++'],ARROW,Bug,Major,2020-02-01 00:14:24,0
13282685,[Developer] Install locally a new enough version of Go for release verification script,This will ensure that if a developer has a too-old version of Go installed on their system that the release verification will still work,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-01-31 19:19:48,14
13282576,[C++][Parquet] Support LargeListArray,"For now it's not possible to write a pyarrow.Table containing a LargeListArray in parquet. The lines
{code:java}
from pyarrow import parquet
import pyarrow as pa

indices = [1, 2, 3]
indptr = [0, 1, 2, 3]
q = pa.lib.LargeListArray.from_arrays(indptr, indices) 
table = pa.Table.from_arrays([q], names=['no']) 

parquet.write_table(table, '/test'){code}
yields the error
{code:java}
ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: large_list<item: int64>

{code}
",parquet,['C++'],ARROW,Improvement,Major,2020-01-31 10:00:41,15
13282435,[Python][CI] Pin pandas version to 0.25 in the dask integration test,Failing nightly build https://app.circleci.com/jobs/github/ursa-labs/crossbow/7693,pull-request-available,['Python'],ARROW,Task,Major,2020-01-30 16:48:16,3
13282395,[CI] [C++] Use boost binaries on Windows GHA build,"In the Github Actions ""AMD64 Windows 2019 C++"" build, around 10 minutes are spent compiling the bundled Boost library from source. We should probably find a way to reuse some existing binaries.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Minor,2020-01-30 14:46:22,2
13282387,[C++] Add infrastructure for unity builds and precompiled headers,"Unity builds and precompiled headers can be enabled ""easily"" in CMake 3.16:
https://cmake.org/cmake/help/v3.16/prop_tgt/UNITY_BUILD.html
https://cmake.org/cmake/help/v3.16/command/target_precompile_headers.html

They can make builds faster in some conditions, especially on CI with little parallelism and caching.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-30 14:33:14,2
13282274,[Release][Yum] Ignore some arm64 verifications,Because we can't build some arm64 binaries by Crossbow for now.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-01-29 22:06:26,1
13282259,[Python] StructArray  timestamp type with timezone to_pandas convert error,"When a {{StructArray}} has a child that is a timestamp with a timezone, the {{to_pandas}} conversion outputs an int64 instead of a timestamp
{code:java}
In [1]: import pyarrow as pa 
   ...: import pandas as pd 
   ...: arr = pa.array([{'start': pd.Timestamp.now(), 'end': pd.Timestamp.now()}]) 
   ...:                                                                                                                      

In [2]: arr.to_pandas()                                                                                   
Out[2]: 
0    {'end': 2020-01-29 11:38:02.792681, 'start': 2...
dtype: object

In [3]: ts = pd.Timestamp.now()                                                                                              

In [4]: arr2 = pa.array([ts], type=pa.timestamp('us', tz='America/New_York'))                                                

In [5]: arr2.to_pandas()                                                                                  
Out[5]: 
0   2020-01-29 06:38:47.848944-05:00
dtype: datetime64[ns, America/New_York]

In [6]: arr = pa.StructArray.from_arrays([arr2, arr2], ['start', 'stop'])                                                    

In [7]: arr.to_pandas()                                                                                   
Out[7]: 
0    {'start': 1580297927848944000, 'stop': 1580297...
dtype: object

{code}
from https://github.com/apache/arrow/pull/6312",pull-request-available,['Python'],ARROW,Bug,Blocker,2020-01-29 19:54:58,14
13282191,[Java][FlightRPC] Memory leak ,"Seems like there is a memory leak detected: https://github.com/apache/arrow/runs/415037585#step:5:1564

cc [~davidli]",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2020-01-29 13:45:59,0
13282170,[C++][Python] Add check_metadata argument to Table.equals,Currently we need to use {{table.replace_schema_metadata().equals(other)}} which is inconvenient. Pass and forward check_metadata argument to the schema equality check.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-01-29 12:39:22,3
13282164,[Python][Dataset] Table equality check occasionally fails,"Failing build https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=5843&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=6c939d89-0d1a-51f2-8b30-091a7a82e98c&l=265

{code}
    def _check_dataset_from_path(path, table, **kwargs):
        import pathlib
    
        # pathlib object
        assert isinstance(path, pathlib.Path)
        dataset = ds.dataset(ds.source(path, **kwargs))
        assert dataset.schema.equals(table.schema, check_metadata=False)
        result = dataset.to_table()
        assert result.replace_schema_metadata().equals(table)
    
        # string path
        dataset = ds.dataset(ds.source(str(path), **kwargs))
        assert dataset.schema.equals(table.schema, check_metadata=False)
        result = dataset.to_table()
>       assert result.replace_schema_metadata().equals(table)
E       AssertionError: assert False
E        +  where False = <built-in method equals of pyarrow.lib.Table object at 0x7fecacf0cab0>(pyarrow.Table\na: int64\nb: double)
E        +    where <built-in method equals of pyarrow.lib.Table object at 0x7fecacf0cab0> = pyarrow.Table\na: int64\nb: double.equals
E        +      where pyarrow.Table\na: int64\nb: double = <built-in method replace_schema_metadata of pyarrow.lib.Table object at 0x7fecacf0c990>()
E        +        where <built-in method replace_schema_metadata of pyarrow.lib.Table object at 0x7fecacf0c990> = pyarrow.Table\na: int64\nb: double\nmetadata\n--------\nOrderedDict([(b'ARROW:schema',\n              b'/////7AAAAAQAAAAAAAK...AAQAAAAAAAB'\n              b'AiQAAAAUAAAABAAAAAAAAAAIAAwACAAHAAgAAAAAAAABQAAAAAEAAABhAAAA'\n              b'AAAAAA==')]).replace_schema_metadata
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2020-01-29 12:02:41,3
13282159,[Release] Fix auto-retry in the binary release script,An SystemCallError not catched. ,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2020-01-29 11:26:49,3
13282144,"[Packaging][APT] Use the ""main"" component for Ubuntu 19.10","Because we always use ""main"" component in apache-arrow-archive-keyring.
",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-01-29 10:15:24,1
13282143,[Release][APT] Ignore some arm64 verifications,Because we can't build some arm64 binaries by Crossbow for now.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-01-29 10:12:37,1
13282114,[Java] TastLeak was put at the wrong location,"Seems {{TestLeak.java}}was put at the wrong place, we should move it into {{flight-core}}.",pull-request-available,['Java'],ARROW,Bug,Major,2020-01-29 06:12:24,16
13282090,[CI][Crossbow] Fix or delete fuzzit jobs,"Not sure we need them now that we're using the OSS-Fuzz project, but they're broken.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2020-01-29 00:35:51,2
13282088,[C#] Date32 test depends on system timezone,"The following failure was occurred on 2020-01-29:08:47:33+09:00:

{noformat}
Starting test execution, please wait...
[xUnit.net 00:00:00.53]     Apache.Arrow.Tests.Date32ArrayTests+Set.SetAndGet [FAIL]
  X Apache.Arrow.Tests.Date32ArrayTests+Set.SetAndGet [19ms]
  Error Message:
   Assert.Equal() Failure
Expected: 2020-01-28T00:00:00.0000000
Actual:   2020-01-27T00:00:00.0000000
  Stack Trace:
     at Apache.Arrow.Tests.Date32ArrayTests.Set.SetAndGet() in /tmp/arrow-0.16.0.mrKfP/apache-arrow-0.16.0/csharp/test/Apache.Arrow.Tests/Date32ArrayTests.cs:line 38
{noformat}",pull-request-available,['C#'],ARROW,Improvement,Major,2020-01-29 00:31:11,1
13282082,[Release][C#] .NET download URL is redirected,"https://gist.github.com/pitrou/5c4a98387153ef415ef64b8aa2457e63

{noformat}
++ curl https://dotnet.microsoft.com/download/thank-you/dotnet-sdk-2.2.300-linux-x64-binaries
++ grep 'window\.open'
++ grep -E -o '[^""]+'
++ sed -n 2p
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
+ local dotnet_download_url=
+ curl
+ tar xzf - -C /tmp/arrow-0.16.0.iRp8b/apache-arrow-0.16.0/csharp/bin
curl: try 'curl --help' or 'curl --manual' for more information

gzip: stdin: unexpected end of file
tar: Child returned status 1
tar: Error is not recoverable: exiting now
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-01-28 23:40:44,1
13282071,[Python] Conversion from Table Column to Pandas loses name for Timestamps,"When converting a Table timestamp column to Pandas, the name of the column is lost in the resulting series.
{code:java}
In [23]: a1 = pa.array([pd.Timestamp.now()])                                                                                 

In [24]: a2 = pa.array([1])                                                                                                  

In [25]: t = pa.Table.from_arrays([a1, a2], ['ts', 'a'])                                                                     

In [26]: for c in t: 
    ...:     print(c.to_pandas()) 
    ...:                                                                                                                     
0   2020-01-28 13:17:26.738708
dtype: datetime64[ns]
0    1
Name: a, dtype: int64 {code}",pull-request-available,['Python'],ARROW,Bug,Major,2020-01-28 22:25:58,5
13281989,[Rust] Initial sort implementation,An initial sort implementation that allows sorting an array by various options (e.g. sort order). This is mainly to iterate on the design and inner workings of a sort algorithm.,pull-request-available,['Rust'],ARROW,Sub-task,Major,2020-01-28 16:38:37,12
13281967,[C++][Dataset] Provide (optional) deterministic order of batches,"Example with python:

{code}
import pyarrow as pa
import pyarrow.parquet as pq

table = pa.table({'a': range(12)}) 
pq.write_table(table, ""test_chunks.parquet"", chunk_size=3) 

# reading with dataset
import pyarrow.dataset as ds
ds.dataset(""test_chunks.parquet"").to_table().to_pandas()
{code}

gives non-deterministic result (order of the row groups in the parquet file):

{code}
In [25]: ds.dataset(""test_chunks.parquet"").to_table().to_pandas()                                                                                                                                                  
Out[25]: 
     a
0    0
1    1
2    2
3    3
4    4
5    5
6    6
7    7
8    8
9    9
10  10
11  11

In [26]: ds.dataset(""test_chunks.parquet"").to_table().to_pandas()                                                                                                                                                  
Out[26]: 
     a
0    0
1    1
2    2
3    3
4    8
5    9
6   10
7   11
8    4
9    5
10   6
11   7

{code}",dataset,"['C++', 'Python']",ARROW,Bug,Major,2020-01-28 15:54:14,13
13281926,[C++] [CI] Flight test error on macOS,"See e.g. https://github.com/apache/arrow/pull/6295/checks?check_run_id=412748673

{code}
[ RUN      ] TestTls.DoAction
E0128 12:02:52.140841000 4447722944 ssl_security_connector.cc:275]     Handshaker factory creation failed with TSI_INVALID_ARGUMENT.
E0128 12:02:52.142590000 4447722944 server_secure_chttp2.cc:81]        {""created"":""@1580212972.142576000"",""description"":""Unable to create secure server with credentials of type Ssl."",""file"":""/Users/runner/runners/2.164.0/work/arrow/arrow/build/cpp/grpc_ep-prefix/src/grpc_ep/src/core/ext/transport/chttp2/server/secure/server_secure_chttp2.cc"",""file_line"":63}
/Users/runner/runners/2.164.0/work/arrow/arrow/cpp/build-support/run-test.sh: line 97: 32477 Segmentation fault: 11  $TEST_EXECUTABLE ""$@"" 2>&1
     32478 Done                    | $ROOT/build-support/asan_symbolize.py
     32479 Done                    | ${CXXFILT:-c++filt}
     32480 Done                    | $ROOT/build-support/stacktrace_addr2line.pl $TEST_EXECUTABLE
     32481 Done                    | $pipe_cmd 2>&1
     32482 Done                    | tee $LOGFILE
~/runners/2.164.0/work/arrow/arrow/build/cpp/src/arrow/flight
{code}
",pull-request-available,"['C++', 'Continuous Integration', 'FlightRPC']",ARROW,Bug,Major,2020-01-28 12:33:51,0
13281888,[Java] Support concating dense union vectors in batch,"After supporting the dense union vector, we need to support concating dense union vectors in batch. ",pull-request-available,['Java'],ARROW,New Feature,Major,2020-01-28 09:11:26,7
13281826,[Release] Unit test on release branch is failed,"https://github.com/kszucs/arrow/runs/410980755

{noformat}
8 tests, 6 assertions, 1 failures, 2 errors, 0 pendings, 0 omissions, 0 notifications
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-01-28 00:33:35,1
13281806,[Packaging][deb][RPM] Can't build repository packages for RC,"apache-arrow-archive-keyring failure:

https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=5737&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=13284

{noformat}
2020-01-27T16:02:31.2221451Z /host/build.sh: 27: cd: can't cd to apache-arrow-archive-keyring-0.16.0/
{noformat}

apache-arrow-release failure:

https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=5774&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=10330

{noformat}
/var/tmp/rpm-tmp.IfEC8a: line 39: cd: apache-arrow-release-0.16.0: No such file or directory
{noformat}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2020-01-27 21:35:34,1
13281749,[C++] Verify missing fields when walking Flatbuffers data,This will fix some of the issues detected by OSS-Fuzz.,pull-request-available,['C++'],ARROW,Task,Major,2020-01-27 16:20:55,2
13281685,[C++] Sporadic Flight test crash on macOS,"See this build:
https://github.com/apache/arrow/pull/6288/checks?check_run_id=409993893

{code}
[----------] 2 tests from TestTls
[ RUN      ] TestTls.DoAction
E0127 01:40:23.871120000 123145508859904 tls_pthread.cc:26]            assertion failed: 0 == pthread_setspecific(tls->key, (void*)value)
/Users/runner/runners/2.164.0/work/arrow/arrow/cpp/build-support/run-test.sh: line 97: 32496 Abort trap: 6           $TEST_EXECUTABLE ""$@"" 2>&1
     32497 Done                    | $ROOT/build-support/asan_symbolize.py
     32498 Done                    | ${CXXFILT:-c++filt}
     32499 Done                    | $ROOT/build-support/stacktrace_addr2line.pl $TEST_EXECUTABLE
     32500 Done                    | $pipe_cmd 2>&1
     32501 Done                    | tee $LOGFILE
~/runners/2.164.0/work/arrow/arrow/build/cpp/src/arrow/flight
{code}

This is a gRPC issue, reported here:
https://github.com/grpc/grpc/issues/20311

We should try to bump bundled gRPC version to see if that fixes the issue.

Side note: why aren't we using the homebrew-provided gRPC?",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2020-01-27 10:48:08,0
13281622,[C++] C++ developer document links in README are broken,"https://github.com/apache/arrow/issues/6261

> Link to the documentation : https://github.com/apache/arrow/blob/master/docs/source/developers/cpp.rst

https://github.com/apache/arrow/issues/6261#issuecomment-578456549

>  * https://github.com/apache/arrow/blob/master/cpp/README.md
>  * https://github.com/apache/arrow/blob/master/cpp/thirdparty/README.md
",pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-27 01:25:07,1
13281616,[Rust] Provide example of Flight server for DataFusion,"Now that IPC is in place and we have the Flight crate, it should be possible to build a working Flight server in Rust and call it from other languages such as Java.

This PR is for creating a DataFusion example that creates a Flight server capable of running SQL queries.",pull-request-available,['Rust'],ARROW,Improvement,Major,2020-01-26 23:01:59,10
13281555,[C++][Dataset] Partition discovery is not working with windows path,See discussion in https://github.com/apache/arrow/pull/6242,dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-01-25 17:40:33,2
13281513,[R] Cleaner interface for creating UnionDataset,Followup to ARROW-7380:[https://github.com/apache/arrow/pull/6232#discussion_r370824412],pull-request-available,['R'],ARROW,Improvement,Major,2020-01-25 00:46:25,4
13281464,[Packaging][Python] Ensure that the static libraries are not built in the wheel scripts,Even though we don't bundle them with the wheels.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2020-01-24 18:44:15,3
13281423,[C++][Dataset] Revisit File discovery failure mode,"Currently, the default `FileSystemFactoryOptions::exclude_invalid_files` will silently ignore unsupported files (either IO error, not of the valid format, corruption, missing compression codecs, etc...) when creating a `FileSystemSource`.

We should change this behavior to propagate an error in the Inspect/Finish calls by default and allow the user to toggle `exclude_invalid_files`. The error should contain at least the file path and a decipherable error (if possible).",dataset,['C++'],ARROW,Improvement,Major,2020-01-24 15:57:07,13
13281377,[Python][Dataset] Add bindings for the DatasetFactory,depends on https://issues.apache.org/jira/browse/ARROW-7380,pull-request-available,['Python'],ARROW,Improvement,Major,2020-01-24 13:03:14,3
13281375,[Python][Dataset] Better ergonomics for the filter expressions,"The current expression API is rather verbose, so let's have a bit more ergonomical API.",pull-request-available,['Python'],ARROW,New Feature,Major,2020-01-24 12:44:26,3
13281273,[R] linuxLibs.R should build in parallel,"It currently seems to compile everything in one thread, which is ghastinly slow.",pull-request-available,['R'],ARROW,Wish,Major,2020-01-23 19:25:53,2
13281267,[C++] Extract localfs default from FileSystemFromUri,"[https://github.com/apache/arrow/pull/6257#pullrequestreview-347506792]

The argument to FileSystemFromUri should always be rfc3986 formatted. The current fallback to localfs can be recovered by adding{{static string Uri::FromPath(string)}} which wraps [uriWindowsFilenameToUriStringA|https://uriparser.github.io/doc/api/latest/Uri_8h.html#a422dc4a2b979ad380a4dfe007e3de845] and the corresponding unix path function.
{code:java}
FileSystemFromUri(Uri::FromPath(R""(E:\dir\file.txt)""), &sanitized) {code}
This is a little more boilerplate but I think it's worthwhile to be explicit here.",pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-23 18:53:27,2
13281171,[R] Support creating ListArray from R list,"{code:r}
DF = data.frame(a = 1:10)
DF$b = as.list(DF$a)
arrow::write_parquet(DF, 'test.parquet')
# Error in Table__from_dots(dots, schema) : cannot infer type from data
{code}

This appears to be supported naturally already in Python:

{code:python}
import pandas as pd
pd.DataFrame({'a': [1, 2, 3], 'b': [[1, 2], [3, 4], [5, 6]]}).to_parquet('test.parquet')
{code}",pull-request-available,['R'],ARROW,Improvement,Major,2020-01-23 11:28:55,13
13281167,[Python] Non-optimal CSV chunking when no newline at end,"We are reading a very simple csv (see below).
The file is only 245 bytes so way below the default _block_size_ in the _ReadOptions_. Thus we would expect the resulting table to have only one batch. At least, if I understand correctly that a _block_ refers to the number of lines of certain byte size?

The docs state:_This will determine multi-threading granularity as well as the size of individual chunks in the Table._For me, that means also the size of individual batches?

Previously, we thought by fixing the block_size to the total file size, we would ensure that even for files larger than 1MB we get a pa.Table with only one batch. This mini file seems to prove us wrong?

Additionally, if I convert back and forth to pandas we get only one batch.



To reproduce:
{code:java}
import os
from pyarrow import csv as pc
import pyarrow as pa
path = ""test.csv""
read_options = pc.ReadOptions(block_size=os.stat(path).st_size)
df = pc.read_csv(path, read_options=read_options)
print(len(df.to_batches()))
# returns 2
print(pa.Table.from_batches([df.to_batches()[1]]).to_pandas())
# returns the last line of the file
pdf = df.to_pandas()
ndf = pa.Table.from_pandas(pdf)
print(len(ndf.to_batches()))
# returns 1{code}
test.csv:
{code:java}
""Name"",""Month"",""Change in %""
""Surrey Quays"",""Sep 18"",""1.01""
""Surrey Quays"",""Oct 18"",""0.38""
""Surrey Quays"",""Nov 18"",""0.97""
""Surrey Quays"",""Dec 18"",""1.28""
""Surrey Quays"",""Jan 19"",""2.43""
""Surrey Quays"",""Feb 19"",""2.49""
""Surrey Quays"",""Mar 19"",""0.81""
{code}




",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-01-23 11:11:21,2
13281090,[R] Support dplyr filtering on date/time,Plus some NSE refactoring suggested by Hadley. ,pull-request-available,['R'],ARROW,New Feature,Major,2020-01-23 00:14:17,4
13281017,[Python] Ability to set column_types to a Schema in csv.ConvertOptions is undocumented,"Originally mentioned in:[https://github.com/apache/arrow/issues/6243]

High level description:
 * As of [this commit|https://github.com/apache/arrow/commit/df54da211448b5202aa08ed2b245eb78cfd1e50c]support to supply a Schema to ConvertOptions in the csv module module was added (I'll add, extremely useful!). Marked as affected in at least 0.12.0 based on the commit history, as well as 0.15.1 (I cannot verify anything between but would assume it is true over the whole version range).
 * As of 0.15.1 the [published documentation|https://arrow.apache.org/docs/python/generated/pyarrow.csv.ConvertOptions.html#pyarrow.csv.ConvertOptions] only explains that a dictionary from field name to DataType can be supplied.

Minimal reproduction: N/A, see link.",csv pull-request-available,"['Documentation', 'Python']",ARROW,Bug,Minor,2020-01-22 16:59:37,3
13281005,[CI][Crossbow] Nightly macOS wheel builds fail,"See https://travis-ci.org/ursa-labs/crossbow/builds/640350008 for example

{code}
$ install_wheel arrow
~/build/ursa-labs/crossbow/arrow ~/build/ursa-labs/crossbow
ERROR: You must give at least one requirement to install (see ""pip help install"")
{code}

cc [~kszucs] [~apitrou]",pull-request-available,"['Continuous Integration', 'Packaging', 'Python']",ARROW,Bug,Blocker,2020-01-22 16:06:09,3
13280998,[C++] Dataset tests not built on Windows,"They are explicitly disabled in {{cpp/src/arrow/dataset/CMakeLists.txt}}. Also, if re-enable them, there are many compile errors (on VS 2017).",dataset pull-request-available,['C++'],ARROW,Bug,Blocker,2020-01-22 15:42:29,6
13280967,[C++] Sanitize local paths on Windows,"One way or the other, we should try to sanitize local filesystem paths on Windows, by converting backslashes into regular slahes.

One place to do it is {{FileSystemFromUri}}. One complication is that \-separated paths can fail parsing as a URI, but we only want to sanitize a path if we detected it's a local path (by parsing the URI). Perhaps trying on error would work.",pull-request-available,['C++'],ARROW,Wish,Major,2020-01-22 12:55:46,2
13280963,[C++] JSON reader fails to read arrays with few values,"Hi! I'm trying to load some nested JSON data and am running into a problem with arrays. I can reproduce it with a slightly modified example from the documentation:
{code:python}
from pyarrow import json
import pyarrow as pa

with open(""test.json"", ""w"") as f:
    test_json = """"""{""a"": [1], ""b"": {""c"": true, ""d"": ""1991-02-03""}}
{""a"": [], ""b"": {""c"": false, ""d"": ""2019-04-01""}}
""""""
    f.write(test_json)

json.read_json(""test.json"")
{code}
Running this code with pyarrow 0.15.1 (I also tried 0.14) gives the following error:
{code:java}
Traceback (most recent call last):
  File ""issue.py"", line 11, in <module>
    ccs = json.read_json(""test.json"")
  File ""pyarrow/_json.pyx"", line 195, in pyarrow._json.read_json
  File ""pyarrow/public-api.pxi"", line 285, in pyarrow.lib.pyarrow_wrap_table
  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column 0 named a expected length 2 but got length 1
{code}
I've tried various combinations and it seems like the error only appears when the *total* number of elements in all the ""a"" arrays is less than the number of *rows* in the file. I did not expect there to be any relationship between those things and have found nothing in the documentation about it. Is this intentional? If not, I'd suspect there's some problem in the validation step.",json pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2020-01-22 12:06:27,6
13280860,[C++][Dataset] segfault when reading compressed Parquet files if build didn't include support for codec,"I was testing something out with a lighter build and built the C++ without snappy compression. Then I tried to read our sample nyc-taxi Parquet files, which turn out to be snappy compressed. Instead of a helpful error message, I got a segfault.

cc [~bkietz] [~fsaintjacques]",dataset pull-request-available,['C++'],ARROW,Bug,Blocker,2020-01-21 23:42:59,13
13280827,[R]Cannot convert Dictionary Array to R when values aren't strings,"I got an error in R when reading a feather file using arrow::read_feather() prepared in python.
{code:r}
#' Error in Table__to_dataframe(x, use_threads = option_use_threads()) :
#' Cannot convert Dictionary Array of type `dictionary<values=double, indices=int8, ordered=0>` to R{code}
I could reproduce the issue with a minimal example:

In python:
{code:python}
import pandas as pd
import pyarrow as pa
df = pd.DataFrame({""float"": [0.1, .2, 0.5, .001]})
df[""category""] = df[""float""].astype('category')
df.dtypes
#' float float64
#' A object
#' category category
#' dtype: object
df.to_feather(""series.feather"")
pa.__version__
#''0.15.1'
{code}
From R:
{code:r}
arrow::read_feather(""series.feather"")
#' Error in Table__to_dataframe(x, use_threads = option_use_threads()) :
#' Cannot convert Dictionary Array of type `dictionary<values=double, indices=int8, ordered=0>` to R
#' Backtrace:
#' 
#' 1. arrow::read_feather(""series.feather"")
#' 2. [ base::as.data.frame(...) ]
#' 3. arrow:::as.data.frame.Table(out)
#' 4. arrow:::Table__to_dataframe(x, use_threads = option_use_threads())
{code}
The feather file is read correctly back in python
{code:python}
ft = pd.read_feather(""series.feather"")
ft.dtypes
#' float        float64
#' A             object
#' category    category
#' dtype: object
{code}

{code:r}
sessionInfo()
#' R version 3.5.1 (2018-07-02)
#' Platform: x86_64-conda_cos6-linux-gnu (64-bit)
#' Running under: Ubuntu 16.04.5 LTS
#' 
#' Matrix products: default
#' BLAS/LAPACK: /misc/DLshare/home/etbellem/miniconda3/lib/R/lib/libRblas.so
#' 
#' locale:
#' [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C
#' [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8
#' [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8
#' [7] LC_PAPER=en_US.UTF-8 LC_NAME=C
#' [9] LC_ADDRESS=C LC_TELEPHONE=C
#' [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
#' 
#' attached base packages:
#' [1] stats graphics grDevices utils datasets methods base
#' 
#' loaded via a namespace (and not attached):
#' [1] Rcpp_1.0.3 arrow_0.15.1 crayon_1.3.4 assertthat_0.2.1
#' [5] R6_2.4.1 magrittr_1.5 rlang_0.4.2 rstudioapi_0.10
#' [9] bit64_0.9-7 glue_1.3.1 purrr_0.3.3 bit_1.1-15.1
#' [13] compiler_3.5.1 tidyselect_0.2.5{code}",pull-request-available,['R'],ARROW,Bug,Major,2020-01-21 19:16:39,4
13280802,[Python] Segfault when inspecting dataset.Source with invalid file/partitioning,"Getting a segfault with:

{code}
In [1]: import pyarrow.dataset as ds                                                                                                                                                                               

In [2]: !touch test_empty.txt                                                                                                                                                                                      

In [3]: source_factory = ds.source(""test_empty.txt"", partitioning=ds.partitioning(field_names=['a', 'b']))                                                                                                         

In [4]: source_factory.inspect()                                                                                                                                                                                   
Segmentation fault (core dumped)
{code}

Didn't yet further investigate what might be the reason (there are several ""wrong"" things here: it's an empty file, it's not a valid file for the parquet format, the partitioning does not match the files, etc)",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-01-21 16:17:55,13
13280791,[Python] Clean-up the pyarrow.dataset.partitioning() API,A left-over review comment at https://github.com/apache/arrow/pull/6022#discussion_r367016454 on the API of {{partitioning()}},dataset pull-request-available,['Python'],ARROW,Improvement,Major,2020-01-21 15:23:19,5
13280785,[Python] Dataset tests failing on Windows to parse file path,"See eg https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=5217&view=logs&j=4c86bc1b-1091-5192-4404-c74dfaad23e7&t=ec99a26b-0264-5e86-36fb-9cfd0ca0f9f3&l=4066

Failing on the backward slashes of the pathlib file paths, and clearly not run in CI since this was not catched.",pull-request-available,['Python'],ARROW,Bug,Major,2020-01-21 14:56:36,5
13280754,[C++] [CI] Improve fuzzing seed corpus,"The coverage stats produced by OSS-Fuzz instruct us to guide the fuzzing process towards the following areas:
- extension arrays
- tensors
- sparse tensors

",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2020-01-21 13:14:47,2
13280716,[Python] Better document some read_csv corner cases,"Hi, I have found two problematic cases, possibly bugs, in pyarrow *read_csv* module. I have written the following piece of code and run a test on the attached CSV file. 

The code compares pandas read_csv with pyarrow csv to show that the second is not behaving correctly with the following set of parameters:

1. change parameter skip_rows = 10, 
{code:python}
Traceback (most recent call last):
  File ""/home/athan/anaconda3/envs/TRIADB/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-21-8c5c88b190c4>"", line 4, in <module>
    read_options=csv.ReadOptions(skip_rows=skip_rows, autogenerate_column_names=False, use_threads=True, column_names=column_names)
  File ""pyarrow/_csv.pyx"", line 541, in pyarrow._csv.read_csv
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowKeyError: Column 'catcost' in include_columns does not exist in CSV file
{code}

2. change parameters skip_rows = 12, columns = None
In this case you don't get the error above, all columns are fetched, but compare the two dataframes, the one from pyarrow with to_pandas() and the one from the output of pandas read_csv(). You will notice that the first one has not parsed correctly the null values ('\\N') in the last column catname. On the contrary pandas read_csv managed to parse all the null values correctly.

{code:python}
Out[28]: 
   1082  991   16.5    200 2014-09-10  1  bar
0  1082  997   0.55  100.0 2014-09-10  1  bar
1  1082  998   7.95  200.0 2014-03-03  0   \N
2  1083  998  12.50    NaN        NaT  0  bar
3  1083  999   1.00    NaN        NaT  0  foo
4  1084  994  57.30  100.0 2014-12-20  1   \N
5  1084  995  22.20    NaN        NaT  0  foo
6  1084  998  48.60  200.0 2014-12-20  1  foo

{code}

Python code to test the attached CSV file for the bugs reported above


{code:python}
from pyarrow import csv
import pyarrow as pa
import pandas as pd

file_location = 'spc_catalog.tsv'

sep = '\t'
nulls=['\\N']

columns = ['catcost', 'catqnt', 'catdate', 'catchk', 'catname']
column_names = None
column_types = None

skip_rows = None
nrecords = None

csv.read_csv(file_location,
    parse_options=csv.ParseOptions(delimiter=sep),
    convert_options=csv.ConvertOptions(include_columns=columns, column_types=column_types, null_values=nulls),
    read_options=csv.ReadOptions(skip_rows=skip_rows, autogenerate_column_names=False, use_threads=True, column_names=column_names)
).to_pandas()

pd.read_csv(file_location, sep=sep, na_values='\\N', usecols=columns, nrows=nrecords, names=column_names, dtype=column_types)

{code}",csv pull-request-available pyarrow,['Python'],ARROW,Bug,Minor,2020-01-21 10:17:04,5
13280609,[C++] Update generated flatbuffers files,The field added in  ARROW-6836 should be reflected in the generated C++ code.,pull-request-available,['C++'],ARROW,Task,Major,2020-01-20 19:29:06,2
13280605,[Format] Mark Tensor and SparseTensor fields required,"The Tensor and SparseTensor parts of the format are currently marked experimental. This presumably means that they are still allowed to change (and indeed they did change one month ago, in ARROW-4225). 

I suggest we take the opportunity to mark some fields required in {{Tensor.fbs}} and {{SparseTensor.fbs}}, to make input validation more robust.

cc [~mrkn], [~jacques]  and [~wesm] for opinions.
",pull-request-available,['Format'],ARROW,Wish,Major,2020-01-20 19:11:20,2
13280602,[Doc] Doc build fails,"{code}
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/docs/source/conf.py"", line 422, in <module>
    import pyarrow.flight
  File ""/home/antoine/arrow/dev/python/pyarrow/flight.py"", line 25, in <module>
    from pyarrow._flight import (  # noqa
ModuleNotFoundError: No module named 'pyarrow._flight'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/sphinx/config.py"", line 368, in eval_config_file
    execfile_(filename, namespace)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/sphinx/util/pycompat.py"", line 81, in execfile_
    exec(code, _globals)
  File ""/home/antoine/arrow/dev/docs/source/conf.py"", line 426, in <module>
    pyarrow.flight = sys.modules['pyarrow.flight'] = mock.Mock()
NameError: name 'mock' is not defined
{code}",pull-request-available,['Documentation'],ARROW,Bug,Major,2020-01-20 18:51:01,2
13280582,[Rust] Windows builds failing due to flatbuffer compile error,"I've noticed now on a few PRs whose tests should otherwise pass, that the Rust Windows tests are failing due to `*_generated.rs` not being found while trying to rename the generated flatbuffer files.

An example is at[https://github.com/apache/arrow/pull/6227/checks?check_run_id=397505832]



 + flatc --rust -o arrow/src/ipc/gen/ ../format/File.fbs ../format/Message.fbs ../format/Schema.fbs ../format/SparseTensor.fbs ../format/Tensor.fbs

 + find arrow/src/ipc/gen/ -name '*_generated.rs' -exec sed -i s/type__type/type_type/g '{}' ';'

 File not found - *_generated.rs",pull-request-available,['Rust'],ARROW,Bug,Blocker,2020-01-20 15:42:59,10
13280569,[C++] Fix crashes or undefined behaviour on corrupt IPC input,More input validation issues detected by OSS-Fuzz.,pull-request-available,['C++'],ARROW,Bug,Blocker,2020-01-20 14:28:21,2
13280525,[Java] Support comparing value ranges for dense union vector,"After we support dense union vectors, we should support range value comparisons for them.",pull-request-available,['Java'],ARROW,New Feature,Major,2020-01-20 10:52:36,7
13280476,[Python] Slow performance in test_parquet.py::test_set_data_page_size,"Slowest test durations:

{code}
3.73s call     pyarrow/tests/test_parquet.py::test_set_data_page_size
0.05s call     pyarrow/tests/test_parquet.py::test_nested_list_nonnullable_roundtrip_bug
0.03s call     pyarrow/tests/test_parquet.py::test_compression_level
0.03s call     pyarrow/tests/test_parquet.py::test_dictionary_array_automatically_read
0.02s call     pyarrow/tests/test_parquet.py::test_statistics_convert_logical_types
0.02s call     pyarrow/tests/test_parquet.py::test_large_list_records
0.01s call     pyarrow/tests/test_parquet.py::test_write_nested_zero_length_array_chunk_failure
0.01s call     pyarrow/tests/test_parquet.py::test_parquet_write_disable_statistics
0.01s call     pyarrow/tests/test_parquet.py::test_single_pylist_column_roundtrip[int]
0.01s call     pyarrow/tests/test_parquet.py::test_special_chars_filename
{code}

This suggests to me there is something wrong with the feature (possibly doing unnecessary work)",pull-request-available,['Python'],ARROW,Bug,Major,2020-01-20 04:37:52,5
13280268,[Java] Finish support for 64 bit int allocations ,"1. Add an allocator capable of allocating larger then 2GB of data.

2. Do end-to-end round trip trip on a larger vector/record batch size.",pull-request-available,['Java'],ARROW,Bug,Major,2020-01-18 04:31:49,7
13280199,[C++][Dataset] Expose more informational properties,"In thinking about what I'd want a useful print method for a Dataset in R to include, there are a few things that come to mind, and by skimming dataset.h, they're not available: 

* How many Sources it has
* For a Source, what kind (local filesystem, other filesystem, etc.), base path (at least where we didn't provide a list of files), how many files, what file format",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-17 18:31:36,13
13280191,[C++] Add to cpp/examples minimal examples of using Arrow as a dependency of another CMake project,It would be helpful to third party developers to have a working example of how we are expecting CMake users to build and use Arrow as an external project in their CMake C++ projects,pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-17 17:21:35,2
13280189,[C++] Create and install static library containing all dependencies built by Arrow,"If ARROW_JEMALLOC=ON, then currently the libarrow.a cannot be used for static linking without also obtaining libjemalloc_pic.a",pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-17 17:15:22,14
13280172,[CI][Crossbow] Nightly centos 8 job fails,"See [https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=4984&view=logs&j=0da5d1d9-276d-5173-c4c4-9d4d4ed14fdb&t=5b4cc83a-7bb0-5664-5bb1-588f7e4dc05b&l=1527]

{code}
CMake Error at cmake_modules/FindLLVM.cmake:34 (find_package):
  Could not find a configuration file for package ""LLVM"" that is compatible
  with requested version ""7.1"".

  The following configuration files were considered but not accepted:

    /usr/lib64/cmake/llvm/LLVMConfig.cmake, version: 8.0.1
    /lib64/cmake/llvm/LLVMConfig.cmake, version: 8.0.1

Call Stack (most recent call first):
  src/gandiva/CMakeLists.txt:31 (find_package)
{code}

[~kou] [~kszucs]",pull-request-available,"['C++ - Gandiva', 'Continuous Integration', 'Packaging']",ARROW,Bug,Blocker,2020-01-17 16:05:35,1
13280120,[Doc] [C++] Update fuzzing documentation,The doc should probably explain how to reproduce issues locally.,pull-request-available,"['C++', 'Documentation']",ARROW,Sub-task,Major,2020-01-17 11:42:18,2
13280008,[C++] Improvements to CMake configuration console summary,Making output more compact and easier to copy-and-paste options. Based on discussion and patch in https://github.com/apache/arrow/pull/6193,pull-request-available,['C++'],ARROW,Improvement,Major,2020-01-17 00:18:48,14
13279997,[Python] Only apply zero-copy DataFrame block optimizations when split_blocks=True,Follow up to ARROW-3789 since there is downstream code that assumes that the DataFrame produced always has all mutable blocks,pull-request-available,['Python'],ARROW,Bug,Blocker,2020-01-16 22:49:40,14
13279988,[R][CI] R appveyor job fails due to pacman compression change,"It looks like Appveyor is escaping/shell quoting things it didn't before. This line is failing now:[https://github.com/apache/arrow/blob/master/ci/appveyor-build-r.sh#L47]
 * before (passing):[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/30150558/job/48s5uf3750yhvfqf#L2614]
 * after (failing):[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/30166850/job/t4nuy7xqwu8pr4ph#L2632]",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Blocker,2020-01-16 21:27:40,4
13279840,[C++] Fix crashes on corrupt IPC input,"Fix the following issues spotted by OSS-Fuzz:
https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=20117
https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=20124
https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=20127

Those are basic missing sanity checks when reading an IPC file.
",pull-request-available,['C++'],ARROW,Bug,Major,2020-01-16 14:18:08,2
13279801,[Python] DictionaryArray.to_numpy returns dict of parts instead of numpy array,"Currently, the {{to_numpy}} method doesn't return an ndarray incase of dictionaryd type data:

{code}
In [54]: a = pa.array(pd.Categorical([""a"", ""b"", ""a""]))                                                                                                                                                             

In [55]: a                                                                                                                                                                                                         
Out[55]: 
<pyarrow.lib.DictionaryArray object at 0x7f5c63d98f28>

-- dictionary:
  [
    ""a"",
    ""b""
  ]
-- indices:
  [
    0,
    1,
    0
  ]

In [57]: a.to_numpy(zero_copy_only=False)                                                                                                                                                                          
Out[57]: 
{'indices': array([0, 1, 0], dtype=int8),
 'dictionary': array(['a', 'b'], dtype=object),
 'ordered': False}
{code}

This is actually just an internal representation that is passed from C++ to Python so on the Python side a {{pd.Categorical}} / {{CategoricalBlock}} can be constructed, but it's not something we should return as such to the user. Rather, I think we should return a decoded / dense numpy array (or at least error instead of returning this dict)

(also, if the user wants those parts, they are already available from the dictionary array as {{a.indices}}, {{a.dictionary}} and {{a.type.ordered}})",pull-request-available,['Python'],ARROW,Bug,Major,2020-01-16 11:07:49,5
13279561,[C++][Flight] Auth handler tests fragile on Windows,"This occurs often on AppVeyor:
{code}
[----------] 3 tests from TestAuthHandler
[ RUN      ] TestAuthHandler.PassAuthenticatedCalls
[       OK ] TestAuthHandler.PassAuthenticatedCalls (4 ms)
[ RUN      ] TestAuthHandler.FailUnauthenticatedCalls
..\src\arrow\flight\flight_test.cc(1126): error: Value of: status.message()
Expected: has substring ""Invalid token""
  Actual: ""Could not write record batch to stream: ""
[  FAILED  ] TestAuthHandler.FailUnauthenticatedCalls (3 ms)
[ RUN      ] TestAuthHandler.CheckPeerIdentity
[       OK ] TestAuthHandler.CheckPeerIdentity (2 ms)
[----------] 3 tests from TestAuthHandler (10 ms total)
[----------] 3 tests from TestBasicAuthHandler
[ RUN      ] TestBasicAuthHandler.PassAuthenticatedCalls
[       OK ] TestBasicAuthHandler.PassAuthenticatedCalls (4 ms)
[ RUN      ] TestBasicAuthHandler.FailUnauthenticatedCalls
..\src\arrow\flight\flight_test.cc(1224): error: Value of: status.message()
Expected: has substring ""Invalid token""
  Actual: ""Could not write record batch to stream: ""
[  FAILED  ] TestBasicAuthHandler.FailUnauthenticatedCalls (4 ms)
[ RUN      ] TestBasicAuthHandler.CheckPeerIdentity
[       OK ] TestBasicAuthHandler.CheckPeerIdentity (3 ms)
[----------] 3 tests from TestBasicAuthHandler (11 ms total)
{code}

See e.g. https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/30110376/job/vbtd22813g5hlgfl#L2252",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Minor,2020-01-15 10:39:03,0
13279459,[Rust][Flight] Unable to compile arrow.flight.protocol.rs,"Not sure exactly why, perhaps it has something to do with the recently updated dependencies: https://github.com/apache/arrow/runs/389937707

cc [~andygrove] ",pull-request-available,['Rust'],ARROW,Bug,Major,2020-01-14 23:15:08,3
13279453,[R] Documentation/polishing for 0.16 release,Includes updating NEWS.md,pull-request-available,['R'],ARROW,Improvement,Major,2020-01-14 22:43:17,4
13279416,[C++][CI] Check fuzzer setup in CI,"It is desirable to check that there is no regression for compiling and running the fuzz targets and assorted utilities.
Perhaps as a cron job.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Sub-task,Major,2020-01-14 18:10:34,2
13279402,[R] Linux binary packaging followup,"After ARROW-6793 merged, I set up some nightly binary building CI and need to iterate on the install script and documentation to reflect what is available there.",pull-request-available,['R'],ARROW,Improvement,Major,2020-01-14 17:12:38,4
13279321,[Python] Add API to map Arrow types to pandas ExtensionDtypes for to_pandas conversions,"ARROW-2428 was about adding such a mapping, and described three use cases (see this [comment|https://issues.apache.org/jira/browse/ARROW-2428?focusedCommentId=16914231&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16914231] for details):

* Basic roundtrip based on the pandas_metadata (in {{to_pandas}}, we check if the pandas_metadata specify pandas extension dtypes, and if so, use this as the target dtype for that column)
* Conversion for pyarrow extension types that can define their equivalent pandas extension dtype
* A way to override default conversion (eg for the built-in types, or in absence of pandas_metadata in the schema). This would require the user to be able to specify some mapping of pyarrow type or column name to the pandas extension dtype to use.

The PR that closed ARROW-2428 (https://github.com/apache/arrow/pull/5512) only covered the first two cases, and not the third case.

I think it is still interesting to also cover the third case in some way.  

An example use case are the new nullable dtypes that are introduced in pandas (eg the nullable integer dtype).  Assume I want to read a parquet file into a pandas DataFrame using this nullable integer dtype. The pyarrow Table has no pandas_metadata indicating to use this dtype (unless it was created from a pandas DataFrame that was already using this dtype, but that will often not be the case), and the pyarrow.int64() type is also not an extension type that can define its equivalent pandas extension dtype. 
Currently, the only solution is first read it into pandas DataFrame (which will use floats for the integers if there are nulls), and then afterwards to convert those floats back to a nullable integer dtype. 

A possible API for this could look like:

{code}
table.to_pandas(types_mapping={pa.int64(): pd.Int64Dtype()})
{code}

to indicate that you want to convert all columns of the pyarrow table with int64 type to a pandas column using the nullable Int64 dtype.
 




",pull-request-available,['Python'],ARROW,Improvement,Major,2020-01-14 11:04:24,5
13279308,[CI] Use more recent Miniconda on AppVeyor,A newer conda might improve setup speed because of the new package format.,pull-request-available,['Continuous Integration'],ARROW,Wish,Minor,2020-01-14 10:24:27,2
13279271,[Website] Add support for download URL redirect,"https://github.com/apache/arrow/issues/3476#issuecomment-572795744

Bintray starts using ""302 Found"" and ""Location"" HTTP response header.",pull-request-available,['Website'],ARROW,Improvement,Major,2020-01-14 07:07:46,1
13279245,[Rust] failed to select a version for `byteorder`,"*Description*
builds is required by package `thrift v0.12.0` is not found on crate.io, supported version are: 1.2.7, 1.2.6, 1.2.5, 1.2.4, 1.2.3, 1.2.2, 1.2.1.

*Error Message*
C:\...\decision_tree>cargo build
    Updating git repository `https://github.com/apache/arrow`
    Updating crates.io index
error: failed to select a version for `byteorder`.
    ... required by package `thrift v0.12.0`
    ... which is depended on by `parquet v1.0.0-SNAPSHOT (https://github.com/apache/arrow#469e9cbc)`
    ... which is depended on by `datafusion v1.0.0-SNAPSHOT (https://github.com/apache/arrow#469e9cbc)`
    ... which is depended on by `decision_tree v0.1.0 (C:\...\decision_tree)`
versions that meet the requirements `~1.2.1` are: 1.2.7, 1.2.6, 1.2.5, 1.2.4, 1.2.3, 1.2.2, 1.2.1

all possible versions conflict with previously selected packages.

  previously selected package `byteorder v1.3.2`
    ... which is depended on by `base64 v0.10.1`
    ... which is depended on by `rust-argon2 v0.5.1`
    ... which is depended on by `redox_users v0.3.1`
    ... which is depended on by `dirs v1.0.5`
    ... which is depended on by `rustyline v4.1.0`
    ... which is depended on by `datafusion v1.0.0-SNAPSHOT (https://github.com/apache/arrow#469e9cbc)`
    ... which is depended on by `decision_tree v0.1.0 (C:\...\decision_tree)`

failed to select a version for `byteorder` which could resolve this conflict

Repo steps
1. added `datafusion = { git = ""https://github.com/apache/arrow"" }` to `[dependences]` section;
2. in the console, run `cargo build`.

*Relative issue*
https://issues.apache.org/jira/browse/ARROW-7562",win10 windows,['Rust - DataFusion'],ARROW,Bug,Blocker,2020-01-14 02:39:45,10
13278881,"[C++] Unknown CMake command ""externalproject_add"".","The configure stage fails on FreeBSD:


{code:java}
- Building (vendored) jemalloc from source CMake Error at cmake_modules/ThirdpartyToolchain.cmake:1328 (externalproject_add): Unknown CMake command ""externalproject_add"".
{code}
Problems:
 * externalproject_add isn't available
 * jemalloc shouldn't be needed on BSD because BSDs already use jemalloc

0.15.0-552-gce4fa0166

",pull-request-available,['C++'],ARROW,Bug,Major,2020-01-11 20:23:31,1
13278830,[C++] TestSlowInputStream is flaky,"See https://github.com/apache/arrow/pull/6160/checks?check_run_id=384146741#step:5:1556 for example

{code}
[ RUN      ] TestSlowInputStream.Basics
/arrow/cpp/src/arrow/io/memory_test.cc:308: Failure
Expected: (dt) < (latency * 3), actual: 4.96068 vs 1.8
[  FAILED  ] TestSlowInputStream.Basics (4961 ms)
[----------] 1 test from TestSlowInputStream (4961 ms total)
{code}

Tests that rely on timing are pretty tough to do on public CI. We should consider moving this somewhere that doesn't run on CI.",pull-request-available,['C++'],ARROW,Bug,Major,2020-01-11 01:28:14,2
13278823,[FlightRPC][C++] Flight test on macOS fails due to Homebrew gRPC,"See[https://github.com/apache/arrow/runs/380443548#step:5:179]for example.

{code}
64/96 Test #64: arrow-flight-test .........................***Failed    0.46 sec
Running arrow-flight-test, redirecting output into /Users/runner/runners/2.163.1/work/arrow/arrow/build/cpp/build/test-logs/arrow-flight-test.txt (attempt 1/1)
Running main() from /Users/runner/runners/2.163.1/work/arrow/arrow/build/cpp/googletest_ep-prefix/src/googletest_ep/googletest/src/gtest_main.cc
[==========] Running 42 tests from 11 test cases.
[----------] Global test environment set-up.
[----------] 2 tests from TestFlightDescriptor
[ RUN      ] TestFlightDescriptor.Basics
[       OK ] TestFlightDescriptor.Basics (0 ms)
[ RUN      ] TestFlightDescriptor.ToFromProto
[       OK ] TestFlightDescriptor.ToFromProto (0 ms)
[----------] 2 tests from TestFlightDescriptor (0 ms total)

[----------] 6 tests from TestFlight
[ RUN      ] TestFlight.UnknownLocationScheme
[       OK ] TestFlight.UnknownLocationScheme (0 ms)
[ RUN      ] TestFlight.ConnectUri
Server running with pid 15977
/Users/runner/runners/2.163.1/work/arrow/arrow/cpp/build-support/run-test.sh: line 97: 15971 Segmentation fault: 11  $TEST_EXECUTABLE ""$@"" 2>&1
     15972 Done                    | $ROOT/build-support/asan_symbolize.py
     15973 Done                    | ${CXXFILT:-c++filt}
     15974 Done                    | $ROOT/build-support/stacktrace_addr2line.pl $TEST_EXECUTABLE
     15975 Done                    | $pipe_cmd 2>&1
     15976 Done                    | tee $LOGFILE
~/runners/2.163.1/work/arrow/arrow/build/cpp/src/arrow/flight
{code}

It's not failing every time but I'm seeing it fail frequently.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Critical,2020-01-11 00:19:27,0
13278780,[R][CI] Run donttest examples in CI,"We wrap the examples in {{\donttest{}}} so that they aren't run on CRAN, where the Arrow C++ library isn't present and thus the examples would error. But locally and on our CI, where we *are* testing with the C++ library, we should run the examples to ensure that they're valid. (As ARROW-7543 revealed, they weren't.)",pull-request-available,['R'],ARROW,Improvement,Major,2020-01-10 18:51:04,4
13278764,[Java] Reorganize Flight modules to keep top level clean/organized,"Lets create a flight parent module and then create the following below:

flight-core (existing flight module)
flight-grpc (existing flight-grpc module)
",pull-request-available,['Java'],ARROW,Task,Major,2020-01-10 17:09:50,16
13278688,[C++] [Python] [Dataset] Additional reader options in ParquetFileFormat,"[looking into using the datasets machinery in the current python parquet code]

In the current python API, we expose several options that influence reading the parquet file (eg {{read_dictionary}} to indicate to read certain BYTE_ARRAY columns directly into a dictionary type, or {{memory_map}}, {{buffer_size}}).

Those could be added to {{ParquetFileFormat}}.",dataset pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2020-01-10 10:43:10,6
13278685,[Java] Use new implementation to concat vectors values in batch,"Per discussionhttps://github.com/apache/arrow/pull/5945#discussion_r365108806.

In ARROW-7284, we write a simple method to concat vectors. However, ARROW-7073 is about to concat vector values efficiently, after this PR merged, we should use this new implementation in {{ArrowReader}}.",pull-request-available,['Java'],ARROW,Bug,Major,2020-01-10 10:31:48,16
13278679,[C++] [Dataset] Scanning dataset with dictionary type hangs,"I assume it is an issue on the C++ side of the datasets code, but reproducer in Python. 

I create a small parquet file with a single column of dictionary type. Reading it with {{pq.read_table}} works fine, reading it with the datasets machinery hangs when scanning:

{code:python}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

df = pd.DataFrame({'a': pd.Categorical(['a', 'b']*10)})
arrow_table = pa.Table.from_pandas(df)

filename = ""test.parquet""
pq.write_table(arrow_table, filename)

from pyarrow.fs import LocalFileSystem
from pyarrow.dataset import ParquetFileFormat, Dataset, FileSystemDataSourceDiscovery, FileSystemDiscoveryOptions

filesystem = LocalFileSystem()
format = ParquetFileFormat()
options = FileSystemDiscoveryOptions()

discovery = FileSystemDataSourceDiscovery(
        filesystem, [filename], format, options)
inspected_schema = discovery.inspect()
dataset = Dataset([discovery.finish()], inspected_schema)

# dataset.schema works fine and gives correct schema
dataset.schema

scanner_builder = dataset.new_scan()
scanner = scanner_builder.finish()
# this hangs
scanner.to_table()
{code}",dataset,['C++'],ARROW,Bug,Critical,2020-01-10 10:12:27,13
13278628,[CI][C++] nproc isn't available on macOS,"https://github.com/apache/arrow/runs/382855556#step:5:32

{noformat}
ci/scripts/cpp_test.sh: line 31: nproc: command not found
{noformat}",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2020-01-10 07:22:45,1
13278610,[Java] FieldVector getFieldBuffers API should not set reader/writer indices,"Per discussion [https://github.com/apache/arrow/pull/6133#discussion_r364906302].

The fact that we have reader/writer settings in {{getFieldBuffers}}is wrong. To clarify, {{getFieldBuffers}}is distinct from {{getBuffers}}. The former should be for getting access to underlying data for higher-performance algorithms. The latter is for sending the data over the wire. Seems we've mixed up use of both.



Currently in {{VectorUnloader}}, we used {{getFieldBuffers}}to create {{ArrowRecordBatch}}thats why we keep writer/reader indices in {{getFieldBuffers}}, we should use {{getBuffers}}instead.",pull-request-available,['Java'],ARROW,Bug,Major,2020-01-10 04:22:49,16
13278566,[CI][R] Nightly macOS autobrew job should be more verbose if it fails,Things like https://travis-ci.org/ursa-labs/crossbow/builds/634643469#L673-L676 are hard to debug because the installation log is not printed.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Minor,2020-01-10 00:27:41,4
13278525,[C++] ASAN failure in validation,"See https://github.com/apache/arrow/runs/376565647#step:5:2035. This is a cron GHA job, so it doesn't show up in our nightly crossbow email. 

It looks like it's been failing since ARROW-7435 merged.",pull-request-available,['C++'],ARROW,Bug,Blocker,2020-01-09 19:47:22,2
13278514,[Java] Create a new java/contrib module,"To better clarify the status of java sub-modules, create a contrib module and move the following modules underneath it.

* algorithm
* adapter
* plasma",pull-request-available,['Java'],ARROW,Task,Major,2020-01-09 18:36:29,7
13278513,[Java] Move ArrowBufPointer out of the java the memory package,The memory package is focused on memory access and management. ArrowBufPointer should be moved to algorithm package as it isn't core to the Arrow memory management primitives. I would further suggest that is an anti-pattern.,pull-request-available,['Java'],ARROW,Task,Major,2020-01-09 18:34:36,7
13278511,[CI] Unskip brew test after Homebrew fixes it upstream,Followup to ARROW-7492. See https://github.com/Homebrew/brew/issues/6908.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2020-01-09 18:31:06,4
13278491,[C++] Investigate header cost reduction,Using https://github.com/aras-p/ClangBuildAnalyzer we could create to find out the worst offenders in terms of header file parsing cost when compiling.,pull-request-available,['C++'],ARROW,Task,Major,2020-01-09 16:18:27,2
13278447,[Developer] Do not include list of commits from PR in squashed summary message,We might assess whether these messages add useful information to the project's commit history. Other projects like Apache Spark have stopped preserving this information. This came up in https://github.com/apache/arrow/pull/6136,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-01-09 13:25:00,14
13278418,[Python] The pandas.datetime class (import of datetime.datetime) and pandas.np are deprecated,"The {{pd.datetime}} was actually just an import from {{datetime.datetime}}, and is being removed from pandas (to use the stdlib one directly). And {{pd.np}} was an import of numpy.",pull-request-available,['Python'],ARROW,Bug,Major,2020-01-09 09:51:59,5
13278415,[Python] pandas/feather tests failing on pandas master,"Because I merged a PR in pandas to support Period dtype, some tests in pyarrow are now failing (they were using period dtype to test ""unsupported"" dtypes)",pull-request-available,['Python'],ARROW,Test,Major,2020-01-09 09:32:07,5
13278273,[C++][CI] Build parquet support in the VS2019 GitHub Actions job,"Enable ARROW_PARQUET cmake flag. Additional patching might be required, see https://github.com/microsoft/vcpkg/pull/8263/files",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2020-01-09 00:09:48,2
13278270,[Developer] Relax clang-tidy check,This is a very invasive check added in recent clang-tidy.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2020-01-08 23:44:33,13
13278248,[Rust] Remove tuple on FixedSizeList datatype,"The FixedSizeList datatype takes a tuple of Box<DataType> and length, but this could be simplified to take the two values without a tuple.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2020-01-08 21:20:45,12
13278230,"[Python] Build wheels, conda packages with dataset support",We should make sure our package builds work with this enabled ,pull-request-available,['Python'],ARROW,Improvement,Critical,2020-01-08 18:46:12,3
13278229,"[Python] Use PYARROW_WITH_HDFS when building wheels, conda packages",This new module is not enabled in the package builds,pull-request-available,['Python'],ARROW,Bug,Blocker,2020-01-08 18:45:16,3
13277981,[C++] Array::null_count() is not thread-compatible,"ArrayData has a mutable member null_count, that can be updated in a const function. However null_count is not atomic, so it's subject to data race.



I guess Arrays are not thread-safe (which is reasonable), but at least they should be thread-compatible so that concurrent access to const member functions are fine.

(The race looks ""benign"", but see [1][2])

[https://github.com/apache/arrow/blob/dbe708c7527a4aa6b63df7722cd57db4e0bd2dc7/cpp/src/arrow/array.cc#L123]



[1][https://software.intel.com/en-us/blogs/2013/01/06/benign-data-races-what-could-possibly-go-wrong]

[2][https://bartoszmilewski.com/2014/10/25/dealing-with-benign-data-races-the-c-way/]",pull-request-available,['C++'],ARROW,Bug,Minor,2020-01-07 17:50:42,2
13277922,[Rust] Bump Thrift version to 0.13 in parquet-format and parquet,"*Problem Description*

Currently, `byteorder` crate changes is not incorporated in both `parquet-format` and `parquet` crates. Both should have consistently updated to the thrift 0.13 in reverse order(first parquet-format then parquet) to update the dependencies which are using older versions.

This makes clashing versions from other crates that are following the upstream.",parquet,['Rust'],ARROW,Bug,Major,2020-01-07 12:52:50,10
13277873,[Java] JMH benchmarks should be called from main methods,"Some benchmarks are called as unit tests in our current code base. They should be called from main methods, because:

1. This is the recommended way of writing JMH benchmarks. The automatically generated benchmarks are called from main, and sample benchmarks provided by JMH [1] are also called from main.

2. Some compiler does not support calling JMH as unit test. For example, the ""javac with error prone"" reports the following error:

Error:(100, 15) java: [JUnit4TearDownNotRun] tearDown() method will not be run; please add JUnit's @After annotation
    (see https://errorprone.info/bugpattern/JUnit4TearDownNotRun)
  Did you mean '@After'?

3. When run as a unit test, enable assert flag will be turned on by default, so some test/debug operations will be performed. This will distort the benchmark result data. For example, a related discussion can be found in [2].

[1] https://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/
[2] https://github.com/apache/arrow/pull/5842#issuecomment-558082914",pull-request-available,['Java'],ARROW,Bug,Major,2020-01-07 07:16:10,7
13277830,[Java] Remove Netty dependency for ArrowBuf,"This is part of the first step of issue ARROW-4526. 
In this step, we remove netty dependency for ArrowBuf, BufferAllocator and ReferenceManager. 

In this issue, we remove the dependency for ArrowBuf. 
The task for BufferAllocator and ReferenceManager will not start until ARROW-7329 is finished.",pull-request-available,['Java'],ARROW,Improvement,Major,2020-01-07 02:41:52,7
13277783,[Rust] Rust builds are failing on master,"See[https://github.com/apache/arrow/runs/374130594#step:5:1506]for example:

{code}
...
---- schema::types::tests::test_schema_type_thrift_conversion_err stdout ----
thread 'schema::types::tests::test_schema_type_thrift_conversion_err' panicked at 'assertion failed: `(left == right)`
  left: `""description() is deprecated; use Display""`,
 right: `""Root schema must be Group type""`', parquet/src/schema/types.rs:1760:13


failures:
    column::writer::tests::test_column_writer_error_when_writing_disabled_dictionary
    column::writer::tests::test_column_writer_inconsistent_def_rep_length
    column::writer::tests::test_column_writer_invalid_def_levels
    column::writer::tests::test_column_writer_invalid_rep_levels
    column::writer::tests::test_column_writer_not_enough_values_to_write
    file::writer::tests::test_file_writer_error_after_close
    file::writer::tests::test_row_group_writer_error_after_close
    file::writer::tests::test_row_group_writer_error_not_all_columns_written
    file::writer::tests::test_row_group_writer_num_records_mismatch
    schema::types::tests::test_primitive_type
    schema::types::tests::test_schema_type_thrift_conversion_err

test result: FAILED. 325 passed; 11 failed; 0 ignored; 0 measured; 0 filtered out
{code}",pull-request-available,['Rust'],ARROW,Bug,Blocker,2020-01-06 20:56:09,12
13277754,[C++][Dataset] regex_error in hive partition on centos7 and opensuse42,"See[https://github.com/apache/arrow/runs/373769666#step:5:3301]and[https://github.com/apache/arrow/runs/373769676#step:5:3297]:

{code}
 Failed 
 1. Error: Hive partitioning (@test-dataset.R#89)  
regex_error
Backtrace:
  1. arrow::open_dataset(...) testthat/test-dataset.R:89:2
 12. dsd$Finish(schema)
 15. arrow:::dataset___DSDiscovery__Finish2(self, schema)
{code}
",dataset pull-request-available,['C++'],ARROW,Bug,Major,2020-01-06 18:30:17,6
13277749,[C++][Dataset] Rename DataFragment/DataSource/PartitionScheme,"DataFragment -> Fragment
DataSource -> Source
PartitionScheme -> PartitionSchema
*Discovery -> *Manifest",dataset pull-request-available,['C++'],ARROW,Wish,Critical,2020-01-06 17:46:03,13
13277748,"[Python] Test asserts: pandas.util.testing is deprecated, use pandas.testing instead","The nightly pandas-master tests are failing (eg https://circleci.com/gh/ursa-labs/crossbow/6815?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link) due to the deprecation of {{pandas.util.testing}} in pandas. 

This deprecation gives a lot of warnings (which we should solve), but also some errors because the deprecations was not fully done properly on the pandas side, opened https://github.com/pandas-dev/pandas/issues/30735 for this (will be fixed shortly)",pull-request-available,['Python'],ARROW,Bug,Major,2020-01-06 17:41:40,5
13277438,"[Java] Remove ""empty"" concept from ArrowBuf, replace with custom referencemanager","With the introduction of ReferenceManager in the codebase, the need for a separate ArrowBuf is no longer necessary. Instead, once can create a new reference manager that is used for the empty ArrowBuf. For reminder/review, empty arrowbufs have a special behavior in that they don't actually have any reference counting semantics and always stay at one. This allow us to better troubleshoot unallocated memory than what would otherwise be an NPE after calling ValueVector.clear()",pull-request-available,['Java'],ARROW,Task,Major,2020-01-03 23:13:43,16
13277437,[Java] Remove reader index and writer index from ArrowBuf,"Reader and writer index and functionality doesn't belong on a chunk of memory and is due to inheritance from ByteBuf. As part of removing ByteBuf inheritance, we should also remove reader and writer indexes from ArrowBuf functionality. It wastes heap memory for rare utility. In general, a slice can be used instead of a reader/writer index pattern.",pull-request-available,['Java'],ARROW,Task,Critical,2020-01-03 23:11:36,16
13277391,[CI][Crossbow] Nightly homebrew-cpp job fails on Python installation,"Oddly, it looks like the formula builds successfully (https://travis-ci.org/ursa-labs/crossbow/builds/631795636#L3698)

{code}
  /usr/local/Cellar/apache-arrow/HEAD-096c78c: 366 files, 57.5MB, built in 9 minutes 46 seconds
{code}

But it exits with status 1. AFAICT the error is here: https://travis-ci.org/ursa-labs/crossbow/builds/631795636#L1745

{code}
==> Pouring python-3.7.6_1.high_sierra.bottle.tar.gz
tar xof /Users/travis/Library/Caches/Homebrew/downloads/3d94ccb6613548e55aed20c7ee59f0e0b7fb045a5d9c8885aa3504ea31f8ab0e--python-3.7.6_1.high_sierra.bottle.tar.gz -C /var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/d20200102-25789-k5jazs
cp -pR /var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/d20200102-25789-k5jazs/python/. /usr/local/Cellar/python
chmod -Rf +w /var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/d20200102-25789-k5jazs
Error: The `brew link` step did not complete successfully
{code}

According to https://github.com/ursa-labs/crossbow/branches/all?utf8=%E2%9C%93&query=-0-travis-homebrew-cpp, this started failing on 2019-12-28.

",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2020-01-03 16:13:38,4
13277153,[Java] Improve the performance of aligning,"Aligning is an important and frequent operation when writing IPC data. It writes no more than 7 0 bytes to the output. 
The current implementation creates a new byte array each time, leading to performance overhead, and increases the GC pressure. 

We improve it by means of a shared byte array. Benchmark evaluation shows a 11% performance gain. ",pull-request-available,['Java'],ARROW,Improvement,Minor,2020-01-02 07:42:19,7
13277146,[Java] Avro converter should convert attributes and props to FieldType metadata,"Currently in Avro converter, some attributes are used when creating vectors such as name, size etc, others are discarded.

For named type like Record, Enum and Fixed, they may have attributes like doc aliased which should keep in metadata for potential further use.

Besides, properties are also not converted properly in some cases.",pull-request-available,['Java'],ARROW,Sub-task,Major,2020-01-02 07:03:05,16
13276891,[FlightRPC][Java] Flight gRPC service is missing reflection info,"When setting up the gRPC service, we mangle the gRPC [service descriptor|https://github.com/apache/arrow/blob/master/java/flight/src/main/java/org/apache/arrow/flight/FlightBindingService.java], removing reflection information. This means things like gRPC reflection don't work, which is necessary for debugging/development tools like [grpcurl|https://github.com/fullstorydev/grpcurl/]. Reflection information is also useful to do things like authorization/access control based on RPC method.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Improvement,Major,2019-12-30 14:25:29,0
13276549,[Ruby] Save CSV files faster,"Hi developers

Saving Arrow::Table in CSV format may be slow.

Ad hoc benchmarks...


{code:ruby}

require 'arrow'
require 'csv'
require 'gr/plot'
t = Arrow::Table.load('some_nice.tsv', format: :csv, delimiter: ""\t"".ord)
n = 1.step(1000, 100).to_a
arrow_save_times = []
csv_save_times = []
n.each do |i|
 t2 = t.slice(0, i)
start = Time.now
 t2.save('test.csv')
 arrow_save_times << p(Time.now - start)
t2 = t.raw_records
start = Time.now
 CSV.open('test2.csv', 'w') do |csv|
 t2.each do |r|
 csv << r
 end
 end
 csv_save_times << p(Time.now - start)
end
GR.stem([n, arrow_save_times], [n, csv_save_times],
 labels: [""arrow"", ""CSV""], xlabel: ""lines"", ylabel: ""time"", location: 2)
GR.savefig(""arrow.png"")
gets
{code}


",pull-request-available,['Ruby'],ARROW,Improvement,Minor,2019-12-27 06:47:36,1
13276295,[Java] Fix some incorrect behavior in UnionListWriter,Currently the {{UnionListWriter/UnionFixedSizeListWriter}} {{getField/close}}APIs seems incorrect.,pull-request-available,['Java'],ARROW,Bug,Major,2019-12-25 04:43:06,16
13276264,[Python] Cython flake8 failures,"Observed on master

{code}
0
pyarrow/_dataset.pyx:216:80: E501 line too long (84 > 79 characters)
pyarrow/_dataset.pyx:232:80: E501 line too long (96 > 79 characters)
pyarrow/_dataset.pyx:239:80: E501 line too long (83 > 79 characters)
pyarrow/_dataset.pyx:317:35: E251 unexpected spaces around keyword / parameter equals
pyarrow/_dataset.pyx:317:37: E251 unexpected spaces around keyword / parameter equals
pyarrow/includes/libarrow_dataset.pxd:261:38: E126 continuation line over-indented for hanging indent
pyarrow/includes/libarrow_dataset.pxd:293:80: E501 line too long (87 > 79 characters)
pyarrow/includes/libarrow_dataset.pxd:313:80: E501 line too long (85 > 79 characters)
8
{code}

It seems the Cython flake8 checks aren't being performed in CI",pull-request-available,['Python'],ARROW,Bug,Major,2019-12-24 20:15:30,2
13276181,[C++] Improve division related bit operations,"Improve some operations in bit_util:

1. Eliminate one division for CeilDiv
2. Avoid overflow for RoundUp
3. Add a utility for CeilDiv(value, 8)",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-12-24 06:29:34,7
13276034,[Java] ComplexCopier does incorrect copy for Map nullable info,"The {{MapVector}}and its 'value' vector are nullable, and its {{structVector}}and 'key' vector are non-nullable.

However, the {{MapVector}}generated by ComplexCopier has all nullable fields which is not correct.",pull-request-available,['Java'],ARROW,Bug,Major,2019-12-23 12:06:49,16
13275991,[CI][Java] Fix gandiva-jar-osx nightly build failure,Gandiva-jar-osx nightly build has been failing for the past few days. From [https://github.com/google/error-prone/issues/1441]the issue seems to be error-prone version 2.3.3 currently used is incompatible with java 13 that is being used in the nightly build. Updating it to 2.3.4 should fix this.,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-12-23 07:08:37,3
13275929,[Rust] Improve some kernels with autovectorisation,"In a comment to an open ticket for optimising a cast kernel by using SIMD, [~andy-thomason]mentioned that LLVM does autovec well for Rust.

I'd like to explore whether we could improve the kernel performance by simplifying the loops enough to allow the compiler to vectorise.",pull-request-available,['Rust'],ARROW,Improvement,Major,2019-12-22 13:29:47,12
13275908,[Python] Documentation lint is failed,"This is included by e519853fbfd0241d87572d8b333a8ab34f02b401.

I missed this. Sorry.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-21 23:10:40,1
13275855,[C++] Add support for YYYY-MM-DDThh and YYYY-MM-DDThh:mm timestamp formats,"Because ISO 8601 accepts them.
See also: https://en.wikipedia.org/wiki/ISO_8601#Times
",pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-21 08:05:00,1
13275741,[CI][C++] test-ubuntu-18.04-cpp-static failing with linking error in arrow-io-hdfs-test,"see https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-12-20-0-circle-test-ubuntu-18.04-cpp-static

{code}
FAILED: debug/arrow-io-hdfs-test
: && /usr/bin/ccache /usr/bin/c++  -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -g  -rdynamic src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o  -o debug/arrow-io-hdfs-test  -Wl,-rpath,/build/cpp/debug debug/libarrow_testing.a debug/libarrow.a /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libssl.so /usr/lib/x86_64-linux-gnu/libbrotlienc.so /usr/lib/x86_64-linux-gnu/libbrotlidec.so /usr/lib/x86_64-linux-gnu/libbrotlicommon.so /usr/lib/x86_64-linux-gnu/libprotobuf.so orc_ep-install/lib/liborc.a /usr/lib/x86_64-linux-gnu/libglog.so -ldl debug//libgtest_maind.so debug//libgtestd.so debug//libgmockd.so /usr/lib/x86_64-linux-gnu/libcrypto.so /usr/lib/x86_64-linux-gnu/libbz2.so /usr/lib/x86_64-linux-gnu/liblz4.so /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7 /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libzstd.so jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt && :
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `__static_initialization_and_destruction_0(int, int)':
/usr/include/boost/system/error_code.hpp:206: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:208: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:210: undefined reference to `boost::system::system_category()'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::system::error_category::std_category::equivalent(int, std::error_condition const&) const':
/usr/include/boost/system/error_code.hpp:656: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:659: undefined reference to `boost::system::generic_category()'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::system::error_category::std_category::equivalent(std::error_code const&, int) const':
/usr/include/boost/system/error_code.hpp:686: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:689: undefined reference to `boost::system::generic_category()'
/usr/include/boost/system/error_code.hpp:701: undefined reference to `boost::system::generic_category()'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::filesystem::operator/(boost::filesystem::path const&, boost::filesystem::path const&)':
/usr/include/boost/filesystem/path.hpp:792: undefined reference to `boost::filesystem::path::operator/=(boost::filesystem::path const&)'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::filesystem::temp_directory_path()':
/usr/include/boost/filesystem/operations.hpp:716: undefined reference to `boost::filesystem::detail::temp_directory_path(boost::system::error_code*)'
src/arrow/io/CMakeFiles/arrow-io-hdfs-test.dir/hdfs_test.cc.o: In function `boost::filesystem::unique_path(boost::filesystem::path const&)':
/usr/include/boost/filesystem/operations.hpp:723: undefined reference to `boost::filesystem::detail::unique_path(boost::filesystem::path const&, boost::system::error_code*)'
collect2: error: ld returned 1 exit status
{code}

possibly related to ARROW-6742",pull-request-available,['C++'],ARROW,Bug,Major,2019-12-20 14:35:09,14
13275655,[Ruby] Specifying column type as time causes segmentation fault,"Hi, I might find a bug.

When we specify column type as time via Arrow::Table.load's column_types named argument, it causes segmentation fault:

I think Ruby's any extension libraries should not cause SEGV.

Script to reproduce this issue(timedata.rb):
{code:java}
require ""arrow""
CSV_FILE = File.join(__dir__, ""timedata.csv"")
table = Arrow::Table.load(CSV_FILE, column_types: [[""datetime"", :time]]){code}
Sample CSV file(timedata.csv) is:
{code:java}
id,datetime,name
1,2019-12-19T18:30,
,,
{code}
Result:
{code:java}
% ruby -v ./timedata.rb
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/data-type.rb:88: [BUG] Segmentation fault at 0x0000000000000008
ruby 2.6.5p114 (2019-10-01 revision 67812) [x86_64-linux]-- Control frame information -----------------------------------------------
c:0018 p:---- s:0097 e:000096 CFUNC  :initialize
c:0017 p:---- s:0094 e:000093 CFUNC  :new
c:0016 p:0125 s:0090 e:000089 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/data-type.rb:88
c:0015 p:0015 s:0080 e:000077 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-read-options.rb:22
c:0014 p:0011 s:0072 e:000071 BLOCK  /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:110 [FINISH]
c:0013 p:---- s:0067 e:000066 CFUNC  :each
c:0012 p:0161 s:0063 e:000062 BLOCK  /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:109 [FINISH]
c:0011 p:---- s:0057 e:000056 CFUNC  :each
c:0010 p:0019 s:0053 e:000052 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:93
c:0009 p:0004 s:0048 e:000047 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:160
c:0008 p:0042 s:0042 e:000041 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:39
c:0007 p:0018 s:0038 e:000037 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:26
c:0006 p:0078 s:0032 e:000031 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb:152
c:0005 p:0139 s:0027 e:000026 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb:50
c:0004 p:0015 s:0019 e:000018 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb:22
c:0003 p:0018 s:0013 e:000012 METHOD /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table.rb:29
c:0002 p:0054 s:0007 E:0006f8 EVAL   ./timedata.rb:5 [FINISH]
c:0001 p:0000 s:0003 E:002070 (none) [FINISH]-- Ruby level backtrace information ----------------------------------------
./timedata.rb:5:in `<main>'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table.rb:29:in `load'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb:22:in `load'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb:50:in `load'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb:152:in `load_as_csv'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:26:in `load'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:39:in `load'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:160:in `load_from_path'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:93:in `reader_options'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:93:in `each'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:109:in `block in reader_options'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:109:in `each'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb:110:in `block (2 levels) in reader_options'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-read-options.rb:22:in `add_column_type'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/data-type.rb:88:in `resolve'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/data-type.rb:88:in `new'
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/data-type.rb:88:in `initialize'-- Machine register context ------------------------------------------------
 RIP: 0x00007f4617b82f1e RBP: 0x00007fffd292f670 RSP: 0x00007fffdac318c0
 RAX: 0x0000000000000000 RBX: 0x00007fffd292f660 RCX: 0x0000000000000001
 RDX: 0x0000000000000001 RDI: 0x00007fffdac31940 RSI: 0x00007fffd2cc8640
  R8: 0x00007fffd2cc8640  R9: 0x0000000000000001 R10: 0x0000000000000032
 R11: 0x00007f461e3bc6f0 R12: 0x0000000000000000 R13: 0x00007fffdac31940
 R14: 0x00007fffd2cc8b00 R15: 0x00007fffd2b75590 EFL: 0x0000000000010246-- C level backtrace information -------------------------------------------
/usr/local/bin/ruby(rb_vm_bugreport+0x7d3) [0x7f4622c89493] vm_dump.c:715
/usr/local/bin/ruby(rb_bug_context+0xe4) [0x7f4622c7c4f4] error.c:609
/usr/local/bin/ruby(sigsegv+0x42) [0x7f4622b4d832] signal.c:998
/lib/x86_64-linux-gnu/libpthread.so.0(__restore_rt+0x0) [0x7f46221d2890]
/usr/lib/x86_64-linux-gnu/libarrow-glib.so.15(0x7f4617b82f1e) [0x7f4617b82f1e]
/usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0(0x7f461e395a7a) [0x7f461e395a7a]
/usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0(g_object_newv+0x2ad) [0x7f461e3971bd]
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so(rbgobj_gobject_new+0x11c) [0x7f461e9392bc] rbgobj_object.c:397
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so(rbgobj_gobject_new) (null):0
/home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so(rg_initialize+0x12b) [0x7f461e93942b] rbgobj_object.c:867
/usr/local/bin/ruby(vm_call0_body+0x2d8) [0x7f4622bd48e8] vm_eval.c:86
/usr/local/bin/ruby(rb_vm_call0+0x35) [0x7f4622bd592f] vm_eval.c:60
/usr/local/bin/ruby(rb_call0) vm_eval.c:308
/usr/local/bin/ruby(rb_class_s_new+0x21) [0x7f4622abe811] object.c:2187
/usr/local/bin/ruby(vm_call_cfunc+0x10a) [0x7f4622bc73ea] vm_insnhelper.c:1908
/usr/local/bin/ruby(vm_call_method+0xe3) [0x7f4622bd3333] vm_insnhelper.c:2400
/usr/local/bin/ruby(vm_exec_core+0x12f) [0x7f4622bd949f] insns.def:765
/usr/local/bin/ruby(rb_vm_exec+0xac) [0x7f4622bcf85c] vm.c:1885
/usr/local/bin/ruby(rb_yield+0x18e) [0x7f4622bd045e] vm.c:1092
/usr/local/bin/ruby(rb_ary_each+0x3c) [0x7f4622bf3e0c] array.c:2087
/usr/local/bin/ruby(vm_call_cfunc+0x10a) [0x7f4622bc73ea] vm_insnhelper.c:1908
/usr/local/bin/ruby(vm_call_method+0xe3) [0x7f4622bd3333] vm_insnhelper.c:2400
/usr/local/bin/ruby(vm_exec_core+0x1d3) [0x7f4622bd9543] insns.def:750
/usr/local/bin/ruby(rb_vm_exec+0xac) [0x7f4622bcf85c] vm.c:1885
/usr/local/bin/ruby(invoke_block_from_c_bh+0x1f4) [0x7f4622bd09e4] vm.c:1092
/usr/local/bin/ruby(each_pair_i_fast+0x3b) [0x7f4622a4af9b] hash.c:2775
/usr/local/bin/ruby(hash_foreach_call+0x75) [0x7f4622a4c7d5] hash.c:1136
/usr/local/bin/ruby(rb_ensure+0xd3) [0x7f4622a2c013] eval.c:1076
/usr/local/bin/ruby(rb_hash_foreach+0x7e) [0x7f4622a5371e] hash.c:1229
/usr/local/bin/ruby(rb_hash_each_pair+0x28) [0x7f4622a540e8] hash.c:2806
/usr/local/bin/ruby(vm_call_cfunc+0x10a) [0x7f4622bc73ea] vm_insnhelper.c:1908
/usr/local/bin/ruby(vm_call_method+0xe3) [0x7f4622bd3333] vm_insnhelper.c:2400
/usr/local/bin/ruby(vm_exec_core+0x1d3) [0x7f4622bd9543] insns.def:750
/usr/local/bin/ruby(rb_vm_exec+0x6cf) [0x7f4622bcfe7f] vm.c:1894
/usr/local/bin/ruby(ruby_exec_internal+0xd3) [0x7f4622a26213] eval.c:262
/usr/local/bin/ruby(ruby_exec_node+0x11) [0x7f4622a2afcb] eval.c:326
/usr/local/bin/ruby(ruby_run_node) eval.c:318
/usr/local/bin/ruby(main+0x5b) [0x7f4622a25f0b] ./main.c:42-- Other runtime information -----------------------------------------------* Loaded script: ./timedata.rb* Loaded features:    0 enumerator.so
    1 thread.rb
    2 rational.so
    3 complex.so
    4 /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/encdb.so
    5 /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/trans/transdb.so
    6 /usr/local/lib/ruby/2.6.0/x86_64-linux/rbconfig.rb
    7 /usr/local/lib/ruby/2.6.0/rubygems/compatibility.rb
    8 /usr/local/lib/ruby/2.6.0/rubygems/defaults.rb
    9 /usr/local/lib/ruby/2.6.0/rubygems/deprecate.rb
   10 /usr/local/lib/ruby/2.6.0/rubygems/errors.rb
   11 /usr/local/lib/ruby/2.6.0/rubygems/version.rb
   12 /usr/local/lib/ruby/2.6.0/rubygems/requirement.rb
   13 /usr/local/lib/ruby/2.6.0/rubygems/platform.rb
   14 /usr/local/lib/ruby/2.6.0/rubygems/basic_specification.rb
   15 /usr/local/lib/ruby/2.6.0/rubygems/stub_specification.rb
   16 /usr/local/lib/ruby/2.6.0/delegate.rb
   17 /usr/local/lib/ruby/2.6.0/uri/rfc2396_parser.rb
   18 /usr/local/lib/ruby/2.6.0/uri/rfc3986_parser.rb
   19 /usr/local/lib/ruby/2.6.0/uri/common.rb
   20 /usr/local/lib/ruby/2.6.0/uri/generic.rb
   21 /usr/local/lib/ruby/2.6.0/uri/file.rb
   22 /usr/local/lib/ruby/2.6.0/uri/ftp.rb
   23 /usr/local/lib/ruby/2.6.0/uri/http.rb
   24 /usr/local/lib/ruby/2.6.0/uri/https.rb
   25 /usr/local/lib/ruby/2.6.0/uri/ldap.rb
   26 /usr/local/lib/ruby/2.6.0/uri/ldaps.rb
   27 /usr/local/lib/ruby/2.6.0/uri/mailto.rb
   28 /usr/local/lib/ruby/2.6.0/uri.rb
   29 /usr/local/lib/ruby/2.6.0/rubygems/specification_policy.rb
   30 /usr/local/lib/ruby/2.6.0/rubygems/util/list.rb
   31 /usr/local/lib/ruby/2.6.0/x86_64-linux/stringio.so
   32 /usr/local/lib/ruby/2.6.0/rubygems/specification.rb
   33 /usr/local/lib/ruby/2.6.0/rubygems/exceptions.rb
   34 /usr/local/lib/ruby/2.6.0/rubygems/util.rb
   35 /usr/local/lib/ruby/2.6.0/rubygems/bundler_version_finder.rb
   36 /usr/local/lib/ruby/2.6.0/rubygems/dependency.rb
   37 /usr/local/lib/ruby/2.6.0/rubygems/core_ext/kernel_gem.rb
   38 /usr/local/lib/ruby/2.6.0/monitor.rb
   39 /usr/local/lib/ruby/2.6.0/rubygems/core_ext/kernel_require.rb
   40 /usr/local/lib/ruby/2.6.0/rubygems/core_ext/kernel_warn.rb
   41 /usr/local/lib/ruby/2.6.0/rubygems.rb
   42 /usr/local/lib/ruby/2.6.0/rubygems/path_support.rb
   43 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/version.rb
   44 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/core_ext/name_error.rb
   45 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/levenshtein.rb
   46 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/jaro_winkler.rb
   47 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checker.rb
   48 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checkers/name_error_checkers/class_name_checker.rb
   49 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checkers/name_error_checkers/variable_name_checker.rb
   50 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checkers/name_error_checkers.rb
   51 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checkers/method_name_checker.rb
   52 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checkers/key_error_checker.rb
   53 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/spell_checkers/null_checker.rb
   54 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean/formatters/plain_formatter.rb
   55 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/did_you_mean-1.3.1/lib/did_you_mean.rb
   56 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/extpp-0.0.8/lib/extpp/setup.rb
   57 /usr/local/lib/ruby/2.6.0/x86_64-linux/pathname.so
   58 /usr/local/lib/ruby/2.6.0/pathname.rb
   59 /usr/local/lib/ruby/2.6.0/English.rb
   60 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2/deprecatable.rb
   61 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
   62 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2/version.rb
   63 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2/regex.rb
   64 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2/deprecated.rb
   65 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.rb
   66 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject_introspection.so
   67 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/arg-info.rb
   68 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/collection-reader.rb
   69 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/boxed-info.rb
   70 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/callable-info.rb
   71 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/function-info.rb
   72 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/interface-info.rb
   73 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/object-info.rb
   74 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/registered-type-info.rb
   75 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/repository.rb
   76 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/struct-info.rb
   77 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/type-info.rb
   78 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/type-tag.rb
   79 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/union-info.rb
   80 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/version.rb
   81 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection/loader.rb
   82 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject-introspection.rb
   83 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/loader.rb
   84 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.so
   85 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/action.rb
   86 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/action-map.rb
   87 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/application-command-line.rb
   88 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/content-type.rb
   89 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/file.rb
   90 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/icon.rb
   91 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/inet-address.rb
   92 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/input-stream.rb
   93 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/menu-item.rb
   94 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/pollable-input-stream.rb
   95 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/pollable-output-stream.rb
   96 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/resources.rb
   97 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/settings.rb
   98 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/settings-schema-source.rb
   99 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/simple-action.rb
  100 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2/deprecated.rb
  101 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.rb
  102 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/version.rb
  103 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/block-closable.rb
  104 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/loader.rb
  105 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/array.rb
  106 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date_core.so
  107 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date.rb
  108 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/array-builder.rb
  109 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/binary-array-builder.rb
  110 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/chunked-array.rb
  111 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/column.rb
  112 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/compression-type.rb
  113 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/forwardable-1.3.1/lib/forwardable/impl.rb
  114 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/forwardable-1.3.1/lib/forwardable/version.rb
  115 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/forwardable-1.3.1/lib/forwardable.rb
  116 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/fields_converter.rb
  117 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/match_p.rb
  118 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/strscan-1.0.3/lib/strscan.so
  119 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/delete_suffix.rb
  120 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/row.rb
  121 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/table.rb
  122 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/parser.rb
  123 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/writer.rb
  124 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/version.rb
  125 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/core_ext/array.rb
  126 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv/core_ext/string.rb
  127 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/csv-3.1.2/lib/csv.rb
  128 /usr/local/lib/ruby/2.6.0/time.rb
  129 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-loader.rb
  130 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/csv-read-options.rb
  131 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/data-type.rb
  132 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/date32-array.rb
  133 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/date32-array-builder.rb
  134 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/date64-array.rb
  135 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/date64-array-builder.rb
  136 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/decimal128.rb
  137 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/decimal128-array.rb
  138 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.so
  139 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.rb
  140 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/bigdecimal-extension.rb
  141 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/decimal128-array-builder.rb
  142 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/decimal128-data-type.rb
  143 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/dense-union-data-type.rb
  144 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/dictionary-data-type.rb
  145 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/field.rb
  146 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/file-output-stream.rb
  147 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/list-array-builder.rb
  148 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/list-data-type.rb
  149 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/null-array-builder.rb
  150 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/path-extension.rb
  151 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/record.rb
  152 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/column-containable.rb
  153 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/record-containable.rb
  154 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/record-batch.rb
  155 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/record-batch-builder.rb
  156 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/record-batch-file-reader.rb
  157 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/record-batch-stream-reader.rb
  158 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/rolling-window.rb
  159 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/field-containable.rb
  160 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/schema.rb
  161 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/slicer.rb
  162 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/sparse-union-data-type.rb
  163 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/struct-array.rb
  164 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/struct-array-builder.rb
  165 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/struct-data-type.rb
  166 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/group.rb
  167 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table.rb
  168 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-formatter.rb
  169 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-list-formatter.rb
  170 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-table-formatter.rb
  171 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-loader.rb
  172 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/table-saver.rb
  173 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/tensor.rb
  174 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time.rb
  175 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time32-array.rb
  176 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time32-array-builder.rb
  177 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time32-data-type.rb
  178 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time64-array.rb
  179 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time64-array-builder.rb
  180 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/time64-data-type.rb
  181 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/timestamp-array.rb
  182 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/timestamp-array-builder.rb
  183 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/timestamp-data-type.rb
  184 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow/writable.rb
  185 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.so
  186 /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.rb* Process memory map:7f460c000000-7f460c021000 rw-p 00000000 00:00 0
7f460c021000-7f4610000000 ---p 00000000 00:00 0
7f46117b0000-7f46119a0000 r--s 00000000 00:00 33307              /lib/x86_64-linux-gnu/libc-2.27.so
7f46119a0000-7f46119ea000 rw-p 00000000 00:00 0
7f46119ee000-7f4611aff000 r--s 00000000 00:00 1661697            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
7f4611aff000-7f4612d50000 r--s 00000000 00:00 453996             /usr/local/bin/ruby
7f4612d50000-7f4612d56000 r-xp 00000000 00:00 1659347            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/extpp-0.0.8/ext/extpp/libruby-extpp.so
7f4612d56000-7f4612d88000 ---p 00006000 00:00 1659347            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/extpp-0.0.8/ext/extpp/libruby-extpp.so
7f4612d88000-7f4612f55000 ---p 00000038 00:00 1659347            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/extpp-0.0.8/ext/extpp/libruby-extpp.so
7f4612f55000-7f4612f56000 r--p 00005000 00:00 1659347            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/extpp-0.0.8/ext/extpp/libruby-extpp.so
7f4612f56000-7f4612f57000 rw-p 00006000 00:00 1659347            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/extpp-0.0.8/ext/extpp/libruby-extpp.so
7f4612f60000-7f4612f78000 r-xp 00000000 00:00 2006190            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.so
7f4612f78000-7f4613017000 ---p 00018000 00:00 2006190            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.so
7f4613017000-7f4613178000 ---p 000000b7 00:00 2006190            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.so
7f4613178000-7f4613179000 r--p 00018000 00:00 2006190            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.so
7f4613179000-7f461317a000 rw-p 00019000 00:00 2006190            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/red-arrow-0.15.1/lib/arrow.so
7f4613180000-7f4613195000 r-xp 00000000 00:00 279300             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.so
7f4613195000-7f46131be000 ---p 00015000 00:00 279300             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.so
7f46131be000-7f4613394000 ---p 0000003e 00:00 279300             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.so
7f4613394000-7f4613395000 r--p 00014000 00:00 279300             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.so
7f4613395000-7f4613396000 rw-p 00015000 00:00 279300             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/bigdecimal-1.4.4/lib/bigdecimal.so
7f46133a0000-7f46133a8000 r-xp 00000000 00:00 428789             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/strscan-1.0.3/lib/strscan.so
7f46133a8000-7f46133c2000 ---p 00008000 00:00 428789             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/strscan-1.0.3/lib/strscan.so
7f46133c2000-7f46135a7000 ---p 00000022 00:00 428789             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/strscan-1.0.3/lib/strscan.so
7f46135a7000-7f46135a8000 r--p 00007000 00:00 428789             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/strscan-1.0.3/lib/strscan.so
7f46135a8000-7f46135a9000 rw-p 00008000 00:00 428789             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/strscan-1.0.3/lib/strscan.so
7f46135b0000-7f46135e5000 r-xp 00000000 00:00 656316             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date_core.so
7f46135e5000-7f4613641000 ---p 00035000 00:00 656316             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date_core.so
7f4613641000-7f46137e4000 ---p 00000091 00:00 656316             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date_core.so
7f46137e4000-7f46137e5000 r--p 00034000 00:00 656316             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date_core.so
7f46137e5000-7f46137e6000 rw-p 00035000 00:00 656316             /home/kitaitimakoto/.gem/ruby/2.6.0/gems/date-3.0.0/lib/date_core.so
7f46137e6000-7f46137e7000 rw-p 00000000 00:00 0
7f46137f0000-7f46137f1000 ---p 00000000 00:00 0
7f46137f1000-7f4613ff1000 rw-p 00000000 00:00 0
7f4614000000-7f46148c8000 rw-p 00000000 00:00 0
7f46148d5000-7f4614990000 r--s 00000000 00:00 647808             /usr/lib/x86_64-linux-gnu/libarrow-glib.so.15.1.0
7f4614990000-7f46149b4000 r-xp 00000000 00:00 33379              /lib/x86_64-linux-gnu/liblzma.so.5.2.2
7f46149b4000-7f46149b6000 ---p 00024000 00:00 33379              /lib/x86_64-linux-gnu/liblzma.so.5.2.2
7f46149b6000-7f4614bb4000 ---p 00000026 00:00 33379              /lib/x86_64-linux-gnu/liblzma.so.5.2.2
7f4614bb4000-7f4614bb5000 r--p 00024000 00:00 33379              /lib/x86_64-linux-gnu/liblzma.so.5.2.2
7f4614bb5000-7f4614bb6000 rw-p 00025000 00:00 33379              /lib/x86_64-linux-gnu/liblzma.so.5.2.2
7f4614bc0000-7f4614bcc000 r-xp 00000000 00:00 138042             /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1
7f4614bcc000-7f4614bcd000 ---p 0000c000 00:00 138042             /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1
7f4614bcd000-7f4614dcb000 ---p 0000000d 00:00 138042             /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1
7f4614dcb000-7f4614dcc000 r--p 0000b000 00:00 138042             /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1
7f4614dcc000-7f4614dcd000 rw-p 0000c000 00:00 138042             /usr/lib/x86_64-linux-gnu/libunwind.so.8.0.1
7f4614dcd000-7f4614ddb000 rw-p 00000000 00:00 0
7f4614de0000-7f4614e04000 r-xp 00000000 00:00 2002700            /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.1
7f4614e04000-7f4614e05000 ---p 00024000 00:00 2002700            /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.1
7f4614e05000-7f4615003000 ---p 00000025 00:00 2002700            /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.1
7f4615003000-7f4615004000 r--p 00023000 00:00 2002700            /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.1
7f4615004000-7f4615005000 rw-p 00024000 00:00 2002700            /usr/lib/x86_64-linux-gnu/libgflags.so.2.2.1
7f4615010000-7f461502f000 r-xp 00000000 00:00 2002028            /usr/lib/x86_64-linux-gnu/libbrotlicommon.so.1.0.4
7f461502f000-7f4615030000 ---p 0001f000 00:00 2002028            /usr/lib/x86_64-linux-gnu/libbrotlicommon.so.1.0.4
7f4615030000-7f461522e000 ---p 00000020 00:00 2002028            /usr/lib/x86_64-linux-gnu/libbrotlicommon.so.1.0.4
7f461522e000-7f461522f000 r--p 0001e000 00:00 2002028            /usr/lib/x86_64-linux-gnu/libbrotlicommon.so.1.0.4
7f461522f000-7f4615230000 rw-p 0001f000 00:00 2002028            /usr/lib/x86_64-linux-gnu/libbrotlicommon.so.1.0.4
7f4615230000-7f4615234000 r-xp 00000000 00:00 2001795            /usr/lib/x86_64-linux-gnu/libboost_system.so.1.65.1
7f4615234000-7f4615235000 ---p 00004000 00:00 2001795            /usr/lib/x86_64-linux-gnu/libboost_system.so.1.65.1
7f4615235000-7f4615433000 ---p 00000005 00:00 2001795            /usr/lib/x86_64-linux-gnu/libboost_system.so.1.65.1
7f4615433000-7f4615434000 r--p 00003000 00:00 2001795            /usr/lib/x86_64-linux-gnu/libboost_system.so.1.65.1
7f4615434000-7f4615435000 rw-p 00004000 00:00 2001795            /usr/lib/x86_64-linux-gnu/libboost_system.so.1.65.1
7f4615440000-7f4615459000 r-xp 00000000 00:00 2001919            /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.65.1
7f4615459000-7f461545a000 ---p 00019000 00:00 2001919            /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.65.1
7f461545a000-7f4615658000 ---p 0000001a 00:00 2001919            /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.65.1
7f4615658000-7f4615659000 r--p 00018000 00:00 2001919            /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.65.1
7f4615659000-7f461565a000 rw-p 00019000 00:00 2001919            /usr/lib/x86_64-linux-gnu/libboost_filesystem.so.1.65.1
7f4615660000-7f46156da000 r-xp 00000000 00:00 137406             /usr/lib/x86_64-linux-gnu/libzstd.so.1.3.3
7f46156da000-7f46156db000 ---p 0007a000 00:00 137406             /usr/lib/x86_64-linux-gnu/libzstd.so.1.3.3
7f46156db000-7f46158d9000 ---p 0000007b 00:00 137406             /usr/lib/x86_64-linux-gnu/libzstd.so.1.3.3
7f46158d9000-7f46158da000 r--p 00079000 00:00 137406             /usr/lib/x86_64-linux-gnu/libzstd.so.1.3.3
7f46158da000-7f46158db000 rw-p 0007a000 00:00 137406             /usr/lib/x86_64-linux-gnu/libzstd.so.1.3.3
7f46158e0000-7f46158e7000 r-xp 00000000 00:00 1001124            /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7
7f46158e7000-7f46158e8000 ---p 00007000 00:00 1001124            /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7
7f46158e8000-7f4615ae6000 ---p 00000008 00:00 1001124            /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7
7f4615ae6000-7f4615ae7000 r--p 00006000 00:00 1001124            /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7
7f4615ae7000-7f4615ae8000 rw-p 00007000 00:00 1001124            /usr/lib/x86_64-linux-gnu/libsnappy.so.1.1.7
7f4615af0000-7f4615b0b000 r-xp 00000000 00:00 137962             /usr/lib/x86_64-linux-gnu/liblz4.so.1.7.1
7f4615b0b000-7f4615b0c000 ---p 0001b000 00:00 137962             /usr/lib/x86_64-linux-gnu/liblz4.so.1.7.1
7f4615b0c000-7f4615d0a000 ---p 0000001c 00:00 137962             /usr/lib/x86_64-linux-gnu/liblz4.so.1.7.1
7f4615d0a000-7f4615d0b000 r--p 0001a000 00:00 137962             /usr/lib/x86_64-linux-gnu/liblz4.so.1.7.1
7f4615d0b000-7f4615d0c000 rw-p 0001b000 00:00 137962             /usr/lib/x86_64-linux-gnu/liblz4.so.1.7.1
7f4615d10000-7f4615d2f000 r-xp 00000000 00:00 2003623            /usr/lib/x86_64-linux-gnu/libglog.so.0.0.0
7f4615d2f000-7f4615d31000 ---p 0001f000 00:00 2003623            /usr/lib/x86_64-linux-gnu/libglog.so.0.0.0
7f4615d31000-7f4615f2f000 ---p 00000021 00:00 2003623            /usr/lib/x86_64-linux-gnu/libglog.so.0.0.0
7f4615f2f000-7f4615f30000 r--p 0001f000 00:00 2003623            /usr/lib/x86_64-linux-gnu/libglog.so.0.0.0
7f4615f30000-7f4615f31000 rw-p 00020000 00:00 2003623            /usr/lib/x86_64-linux-gnu/libglog.so.0.0.0
7f4615f31000-7f4615f41000 rw-p 00000000 00:00 0
7f4615f50000-7f4615f5b000 r-xp 00000000 00:00 2002035            /usr/lib/x86_64-linux-gnu/libbrotlidec.so.1.0.4
7f4615f5b000-7f4615f5c000 ---p 0000b000 00:00 2002035            /usr/lib/x86_64-linux-gnu/libbrotlidec.so.1.0.4
7f4615f5c000-7f461615a000 ---p 0000000c 00:00 2002035            /usr/lib/x86_64-linux-gnu/libbrotlidec.so.1.0.4
7f461615a000-7f461615b000 r--p 0000a000 00:00 2002035            /usr/lib/x86_64-linux-gnu/libbrotlidec.so.1.0.4
7f461615b000-7f461615c000 rw-p 0000b000 00:00 2002035            /usr/lib/x86_64-linux-gnu/libbrotlidec.so.1.0.4
7f4616160000-7f46161e6000 r-xp 00000000 00:00 2002185            /usr/lib/x86_64-linux-gnu/libbrotlienc.so.1.0.4
7f46161e6000-7f46161e7000 ---p 00086000 00:00 2002185            /usr/lib/x86_64-linux-gnu/libbrotlienc.so.1.0.4
7f46161e7000-7f46163e5000 ---p 00000087 00:00 2002185            /usr/lib/x86_64-linux-gnu/libbrotlienc.so.1.0.4
7f46163e5000-7f46163e6000 r--p 00085000 00:00 2002185            /usr/lib/x86_64-linux-gnu/libbrotlienc.so.1.0.4
7f46163e6000-7f46163e7000 rw-p 00086000 00:00 2002185            /usr/lib/x86_64-linux-gnu/libbrotlienc.so.1.0.4
7f46163f0000-7f4616400000 r-xp 00000000 00:00 2002344            /usr/lib/x86_64-linux-gnu/libdouble-conversion.so.1.0
7f4616400000-7f4616401000 ---p 00010000 00:00 2002344            /usr/lib/x86_64-linux-gnu/libdouble-conversion.so.1.0
7f4616401000-7f46165ff000 ---p 00000011 00:00 2002344            /usr/lib/x86_64-linux-gnu/libdouble-conversion.so.1.0
7f46165ff000-7f4616600000 r--p 0000f000 00:00 2002344            /usr/lib/x86_64-linux-gnu/libdouble-conversion.so.1.0
7f4616600000-7f4616601000 rw-p 00010000 00:00 2002344            /usr/lib/x86_64-linux-gnu/libdouble-conversion.so.1.0
7f4616610000-7f4616627000 r-xp 00000000 00:00 34041              /lib/x86_64-linux-gnu/libgcc_s.so.1
7f4616627000-7f4616628000 ---p 00017000 00:00 34041              /lib/x86_64-linux-gnu/libgcc_s.so.1
7f4616628000-7f4616826000 ---p 00000018 00:00 34041              /lib/x86_64-linux-gnu/libgcc_s.so.1
7f4616826000-7f4616827000 r--p 00016000 00:00 34041              /lib/x86_64-linux-gnu/libgcc_s.so.1
7f4616827000-7f4616828000 rw-p 00017000 00:00 34041              /lib/x86_64-linux-gnu/libgcc_s.so.1
7f4616830000-7f46169a9000 r-xp 00000000 00:00 34137              /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25
7f46169a9000-7f46169b6000 ---p 00179000 00:00 34137              /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25
7f46169b6000-7f4616ba9000 ---p 00000186 00:00 34137              /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25
7f4616ba9000-7f4616bb3000 r--p 00179000 00:00 34137              /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25
7f4616bb3000-7f4616bb5000 rw-p 00183000 00:00 34137              /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.25
7f4616bb5000-7f4616bb9000 rw-p 00000000 00:00 0
7f4616bc0000-7f46176da000 r-xp 00000000 00:00 2003880            /usr/lib/x86_64-linux-gnu/libarrow.so.15.1.0
7f46176da000-7f4617717000 ---p 00b1a000 00:00 2003880            /usr/lib/x86_64-linux-gnu/libarrow.so.15.1.0
7f4617717000-7f46178d9000 ---p 00000b57 00:00 2003880            /usr/lib/x86_64-linux-gnu/libarrow.so.15.1.0
7f46178d9000-7f4617913000 r--p 00b19000 00:00 2003880            /usr/lib/x86_64-linux-gnu/libarrow.so.15.1.0
7f4617913000-7f4617916000 rw-p 00b53000 00:00 2003880            /usr/lib/x86_64-linux-gnu/libarrow.so.15.1.0
7f4617916000-7f4617b2b000 rw-p 00000000 00:00 0
7f4617b30000-7f4617be7000 r-xp 00000000 00:00 647808             /usr/lib/x86_64-linux-gnu/libarrow-glib.so.15.1.0
7f4617be7000-7f4617beb000 ---p 000b7000 00:00 647808             /usr/lib/x86_64-linux-gnu/libarrow-glib.so.15.1.0
7f4617beb000-7f4617de6000 ---p 000000bb 00:00 647808             /usr/lib/x86_64-linux-gnu/libarrow-glib.so.15.1.0
7f4617de6000-7f4617de9000 r--p 000b6000 00:00 647808             /usr/lib/x86_64-linux-gnu/libarrow-glib.so.15.1.0
7f4617de9000-7f4617dea000 rw-p 000b9000 00:00 647808             /usr/lib/x86_64-linux-gnu/libarrow-glib.so.15.1.0
7f4617dea000-7f4617deb000 rw-p 00000000 00:00 0
7f4617df0000-7f4617df2000 r-xp 00000000 00:00 2012571            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.so
7f4617df2000-7f4617e18000 ---p 00002000 00:00 2012571            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.so
7f4617e18000-7f4617ff1000 ---p 00000028 00:00 2012571            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.so
7f4617ff1000-7f4617ff2000 r--p 00001000 00:00 2012571            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.so
7f4617ff2000-7f4617ff3000 rw-p 00002000 00:00 2012571            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gio2-3.4.1/lib/gio2.so
7f4618000000-7f4618021000 rw-p 00000000 00:00 0
7f4618021000-7f461c000000 ---p 00000000 00:00 0
7f461c000000-7f461c04b000 rw-p 00000000 00:00 0
7f461c058000-7f461c0ac000 r--s 00000000 00:00 144187             /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0.5600.4
7f461c0ac000-7f461c0d0000 r--s 00000000 00:00 33454              /lib/x86_64-linux-gnu/libpthread-2.27.so
7f461c0d0000-7f461c131000 rw-p 00000000 00:00 0
7f461c140000-7f461c141000 ---p 00000000 00:00 0
7f461c141000-7f461c941000 rw-p 00000000 00:00 0
7f461c950000-7f461c956000 r-xp 00000000 00:00 142890             /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f461c956000-7f461c957000 ---p 00006000 00:00 142890             /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f461c957000-7f461cb55000 ---p 00000007 00:00 142890             /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f461cb55000-7f461cb56000 r--p 00005000 00:00 142890             /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f461cb56000-7f461cb57000 rw-p 00006000 00:00 142890             /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f461cb60000-7f461cba7000 r-xp 00000000 00:00 142895             /lib/x86_64-linux-gnu/libblkid.so.1.1.0
7f461cba7000-7f461cbad000 ---p 00047000 00:00 142895             /lib/x86_64-linux-gnu/libblkid.so.1.1.0
7f461cbad000-7f461cda7000 ---p 0000004d 00:00 142895             /lib/x86_64-linux-gnu/libblkid.so.1.1.0
7f461cda7000-7f461cdab000 r--p 00047000 00:00 142895             /lib/x86_64-linux-gnu/libblkid.so.1.1.0
7f461cdab000-7f461cdac000 rw-p 0004b000 00:00 142895             /lib/x86_64-linux-gnu/libblkid.so.1.1.0
7f461cdac000-7f461cdad000 rw-p 00000000 00:00 0
7f461cdb0000-7f461ce01000 r-xp 00000000 00:00 146446             /lib/x86_64-linux-gnu/libmount.so.1.1.0
7f461ce01000-7f461ce04000 ---p 00051000 00:00 146446             /lib/x86_64-linux-gnu/libmount.so.1.1.0
7f461ce04000-7f461d000000 ---p 00000054 00:00 146446             /lib/x86_64-linux-gnu/libmount.so.1.1.0
7f461d000000-7f461d002000 r--p 00050000 00:00 146446             /lib/x86_64-linux-gnu/libmount.so.1.1.0
7f461d002000-7f461d003000 rw-p 00052000 00:00 146446             /lib/x86_64-linux-gnu/libmount.so.1.1.0
7f461d003000-7f461d004000 rw-p 00000000 00:00 0
7f461d010000-7f461d027000 r-xp 00000000 00:00 33460              /lib/x86_64-linux-gnu/libresolv-2.27.so
7f461d027000-7f461d029000 ---p 00017000 00:00 33460              /lib/x86_64-linux-gnu/libresolv-2.27.so
7f461d029000-7f461d227000 ---p 00000019 00:00 33460              /lib/x86_64-linux-gnu/libresolv-2.27.so
7f461d227000-7f461d228000 r--p 00017000 00:00 33460              /lib/x86_64-linux-gnu/libresolv-2.27.so
7f461d228000-7f461d229000 rw-p 00018000 00:00 33460              /lib/x86_64-linux-gnu/libresolv-2.27.so
7f461d229000-7f461d22b000 rw-p 00000000 00:00 0
7f461d230000-7f461d255000 r-xp 00000000 00:00 33467              /lib/x86_64-linux-gnu/libselinux.so.1
7f461d255000-7f461d256000 ---p 00025000 00:00 33467              /lib/x86_64-linux-gnu/libselinux.so.1
7f461d256000-7f461d454000 ---p 00000026 00:00 33467              /lib/x86_64-linux-gnu/libselinux.so.1
7f461d454000-7f461d455000 r--p 00024000 00:00 33467              /lib/x86_64-linux-gnu/libselinux.so.1
7f461d455000-7f461d456000 rw-p 00025000 00:00 33467              /lib/x86_64-linux-gnu/libselinux.so.1
7f461d456000-7f461d458000 rw-p 00000000 00:00 0
7f461d460000-7f461d5f5000 r-xp 00000000 00:00 144184             /usr/lib/x86_64-linux-gnu/libgio-2.0.so.0.5600.4
7f461d5f5000-7f461d5fd000 ---p 00195000 00:00 144184             /usr/lib/x86_64-linux-gnu/libgio-2.0.so.0.5600.4
7f461d5fd000-7f461d7f5000 ---p 0000019d 00:00 144184             /usr/lib/x86_64-linux-gnu/libgio-2.0.so.0.5600.4
7f461d7f5000-7f461d7fc000 r--p 00195000 00:00 144184             /usr/lib/x86_64-linux-gnu/libgio-2.0.so.0.5600.4
7f461d7fc000-7f461d7fd000 rw-p 0019c000 00:00 144184             /usr/lib/x86_64-linux-gnu/libgio-2.0.so.0.5600.4
7f461d7fd000-7f461d7ff000 rw-p 00000000 00:00 0
7f461d800000-7f461d803000 r-xp 00000000 00:00 144186             /usr/lib/x86_64-linux-gnu/libgmodule-2.0.so.0.5600.4
7f461d803000-7f461d804000 ---p 00003000 00:00 144186             /usr/lib/x86_64-linux-gnu/libgmodule-2.0.so.0.5600.4
7f461d804000-7f461da02000 ---p 00000004 00:00 144186             /usr/lib/x86_64-linux-gnu/libgmodule-2.0.so.0.5600.4
7f461da02000-7f461da03000 r--p 00002000 00:00 144186             /usr/lib/x86_64-linux-gnu/libgmodule-2.0.so.0.5600.4
7f461da03000-7f461da04000 rw-p 00003000 00:00 144186             /usr/lib/x86_64-linux-gnu/libgmodule-2.0.so.0.5600.4
7f461da10000-7f461da43000 r-xp 00000000 00:00 137883             /usr/lib/x86_64-linux-gnu/libgirepository-1.0.so.1.0.0
7f461da43000-7f461da44000 ---p 00033000 00:00 137883             /usr/lib/x86_64-linux-gnu/libgirepository-1.0.so.1.0.0
7f461da44000-7f461dc42000 ---p 00000034 00:00 137883             /usr/lib/x86_64-linux-gnu/libgirepository-1.0.so.1.0.0
7f461dc42000-7f461dc43000 r--p 00032000 00:00 137883             /usr/lib/x86_64-linux-gnu/libgirepository-1.0.so.1.0.0
7f461dc43000-7f461dc44000 rw-p 00033000 00:00 137883             /usr/lib/x86_64-linux-gnu/libgirepository-1.0.so.1.0.0
7f461dc50000-7f461dc70000 r-xp 00000000 00:00 1999784            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject_introspection.so
7f461dc70000-7f461dcc9000 ---p 00020000 00:00 1999784            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject_introspection.so
7f461dcc9000-7f461de70000 ---p 00000079 00:00 1999784            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject_introspection.so
7f461de70000-7f461de71000 r--p 00020000 00:00 1999784            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject_introspection.so
7f461de71000-7f461de72000 rw-p 00021000 00:00 1999784            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/gobject-introspection-3.4.1/lib/gobject_introspection.so
7f461de80000-7f461dee1000 rw-p 00000000 00:00 0
7f461def0000-7f461def7000 r-xp 00000000 00:00 137867             /usr/lib/x86_64-linux-gnu/libffi.so.6.0.4
7f461def7000-7f461def8000 ---p 00007000 00:00 137867             /usr/lib/x86_64-linux-gnu/libffi.so.6.0.4
7f461def8000-7f461e0f6000 ---p 00000008 00:00 137867             /usr/lib/x86_64-linux-gnu/libffi.so.6.0.4
7f461e0f6000-7f461e0f7000 r--p 00006000 00:00 137867             /usr/lib/x86_64-linux-gnu/libffi.so.6.0.4
7f461e0f7000-7f461e0f8000 rw-p 00007000 00:00 137867             /usr/lib/x86_64-linux-gnu/libffi.so.6.0.4
7f461e100000-7f461e170000 r-xp 00000000 00:00 33443              /lib/x86_64-linux-gnu/libpcre.so.3.13.3
7f461e170000-7f461e172000 ---p 00070000 00:00 33443              /lib/x86_64-linux-gnu/libpcre.so.3.13.3
7f461e172000-7f461e370000 ---p 00000072 00:00 33443              /lib/x86_64-linux-gnu/libpcre.so.3.13.3
7f461e370000-7f461e371000 r--p 00070000 00:00 33443              /lib/x86_64-linux-gnu/libpcre.so.3.13.3
7f461e371000-7f461e372000 rw-p 00071000 00:00 33443              /lib/x86_64-linux-gnu/libpcre.so.3.13.3
7f461e380000-7f461e3d2000 r-xp 00000000 00:00 144187             /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0.5600.4
7f461e3d2000-7f461e3d4000 ---p 00052000 00:00 144187             /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0.5600.4
7f461e3d4000-7f461e5d2000 ---p 00000054 00:00 144187             /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0.5600.4
7f461e5d2000-7f461e5d3000 r--p 00052000 00:00 144187             /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0.5600.4
7f461e5d3000-7f461e5d4000 rw-p 00053000 00:00 144187             /usr/lib/x86_64-linux-gnu/libgobject-2.0.so.0.5600.4
7f461e5e0000-7f461e6f4000 r-xp 00000000 00:00 144185             /usr/lib/x86_64-linux-gnu/libglib-2.0.so.0.5600.4
7f461e6f4000-7f461e6f6000 ---p 00114000 00:00 144185             /usr/lib/x86_64-linux-gnu/libglib-2.0.so.0.5600.4
7f461e6f6000-7f461e8f4000 ---p 00000116 00:00 144185             /usr/lib/x86_64-linux-gnu/libglib-2.0.so.0.5600.4
7f461e8f4000-7f461e8f5000 r--p 00114000 00:00 144185             /usr/lib/x86_64-linux-gnu/libglib-2.0.so.0.5600.4
7f461e8f5000-7f461e8f6000 rw-p 00115000 00:00 144185             /usr/lib/x86_64-linux-gnu/libglib-2.0.so.0.5600.4
7f461e8f6000-7f461e8f7000 rw-p 00000000 00:00 0
7f461e900000-7f461e95b000 r-xp 00000000 00:00 1661697            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
7f461e95b000-7f461ea11000 ---p 0005b000 00:00 1661697            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
7f461ea11000-7f461eb5b000 ---p 00000111 00:00 1661697            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
7f461eb5b000-7f461eb60000 r--p 0005b000 00:00 1661697            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
7f461eb60000-7f461eb62000 rw-p 00060000 00:00 1661697            /home/kitaitimakoto/.gem/ruby/2.6.0/gems/glib2-3.4.1/lib/glib2.so
7f461eb62000-7f461eb63000 rw-p 00000000 00:00 0
7f461eb70000-7f461eb78000 r-xp 00000000 00:00 737330             /usr/local/lib/ruby/2.6.0/x86_64-linux/pathname.so
7f461eb78000-7f461eb93000 ---p 00008000 00:00 737330             /usr/local/lib/ruby/2.6.0/x86_64-linux/pathname.so
7f461eb93000-7f461ed77000 ---p 00000023 00:00 737330             /usr/local/lib/ruby/2.6.0/x86_64-linux/pathname.so
7f461ed77000-7f461ed78000 r--p 00007000 00:00 737330             /usr/local/lib/ruby/2.6.0/x86_64-linux/pathname.so
7f461ed78000-7f461ed79000 rw-p 00008000 00:00 737330             /usr/local/lib/ruby/2.6.0/x86_64-linux/pathname.so
7f461ed80000-7f461ed89000 r-xp 00000000 00:00 737334             /usr/local/lib/ruby/2.6.0/x86_64-linux/stringio.so
7f461ed89000-7f461eda6000 ---p 00009000 00:00 737334             /usr/local/lib/ruby/2.6.0/x86_64-linux/stringio.so
7f461eda6000-7f461ef88000 ---p 00000026 00:00 737334             /usr/local/lib/ruby/2.6.0/x86_64-linux/stringio.so
7f461ef88000-7f461ef89000 r--p 00008000 00:00 737334             /usr/local/lib/ruby/2.6.0/x86_64-linux/stringio.so
7f461ef89000-7f461ef8a000 rw-p 00009000 00:00 737334             /usr/local/lib/ruby/2.6.0/x86_64-linux/stringio.so
7f461ef90000-7f461ef92000 r-xp 00000000 00:00 688271             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/trans/transdb.so
7f461ef92000-7f461ef96000 ---p 00002000 00:00 688271             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/trans/transdb.so
7f461ef96000-7f461f192000 ---p 00000006 00:00 688271             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/trans/transdb.so
7f461f192000-7f461f193000 r--p 00002000 00:00 688271             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/trans/transdb.so
7f461f193000-7f461f194000 rw-p 00003000 00:00 688271             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/trans/transdb.so
7f461f1a0000-7f461f1a2000 r-xp 00000000 00:00 688267             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/encdb.so
7f461f1a2000-7f461f1b4000 ---p 00002000 00:00 688267             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/encdb.so
7f461f1b4000-7f461f3a1000 ---p 00000014 00:00 688267             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/encdb.so
7f461f3a1000-7f461f3a2000 r--p 00001000 00:00 688267             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/encdb.so
7f461f3a2000-7f461f3a3000 rw-p 00002000 00:00 688267             /usr/local/lib/ruby/2.6.0/x86_64-linux/enc/encdb.so
7f461f3b0000-7f46213b9000 rw-p 00000000 00:00 0
7f46213c0000-7f46215a7000 r-xp 00000000 00:00 33307              /lib/x86_64-linux-gnu/libc-2.27.so
7f46215a7000-7f46215b0000 ---p 001e7000 00:00 33307              /lib/x86_64-linux-gnu/libc-2.27.so
7f46215b0000-7f46217a7000 ---p 000001f0 00:00 33307              /lib/x86_64-linux-gnu/libc-2.27.so
7f46217a7000-7f46217ab000 r--p 001e7000 00:00 33307              /lib/x86_64-linux-gnu/libc-2.27.so
7f46217ab000-7f46217ad000 rw-p 001eb000 00:00 33307              /lib/x86_64-linux-gnu/libc-2.27.so
7f46217ad000-7f46217b1000 rw-p 00000000 00:00 0
7f46217c0000-7f462195d000 r-xp 00000000 00:00 33383              /lib/x86_64-linux-gnu/libm-2.27.so
7f462195d000-7f4621960000 ---p 0019d000 00:00 33383              /lib/x86_64-linux-gnu/libm-2.27.so
7f4621960000-7f4621b5c000 ---p 000001a0 00:00 33383              /lib/x86_64-linux-gnu/libm-2.27.so
7f4621b5c000-7f4621b5d000 r--p 0019c000 00:00 33383              /lib/x86_64-linux-gnu/libm-2.27.so
7f4621b5d000-7f4621b5e000 rw-p 0019d000 00:00 33383              /lib/x86_64-linux-gnu/libm-2.27.so
7f4621b60000-7f4621b69000 r-xp 00000000 00:00 33317              /lib/x86_64-linux-gnu/libcrypt-2.27.so
7f4621b69000-7f4621b6a000 ---p 00009000 00:00 33317              /lib/x86_64-linux-gnu/libcrypt-2.27.so
7f4621b6a000-7f4621d68000 ---p 0000000a 00:00 33317              /lib/x86_64-linux-gnu/libcrypt-2.27.so
7f4621d68000-7f4621d69000 r--p 00008000 00:00 33317              /lib/x86_64-linux-gnu/libcrypt-2.27.so
7f4621d69000-7f4621d6a000 rw-p 00009000 00:00 33317              /lib/x86_64-linux-gnu/libcrypt-2.27.so
7f4621d6a000-7f4621d98000 rw-p 00000000 00:00 0
7f4621da0000-7f4621da3000 r-xp 00000000 00:00 33331              /lib/x86_64-linux-gnu/libdl-2.27.so
7f4621da3000-7f4621da4000 ---p 00003000 00:00 33331              /lib/x86_64-linux-gnu/libdl-2.27.so
7f4621da4000-7f4621fa2000 ---p 00000004 00:00 33331              /lib/x86_64-linux-gnu/libdl-2.27.so
7f4621fa2000-7f4621fa3000 r--p 00002000 00:00 33331              /lib/x86_64-linux-gnu/libdl-2.27.so
7f4621fa3000-7f4621fa4000 rw-p 00003000 00:00 33331              /lib/x86_64-linux-gnu/libdl-2.27.so
7f4621fb0000-7f4621fb7000 r-xp 00000000 00:00 33463              /lib/x86_64-linux-gnu/librt-2.27.so
7f4621fb7000-7f4621fb8000 ---p 00007000 00:00 33463              /lib/x86_64-linux-gnu/librt-2.27.so
7f4621fb8000-7f46221b6000 ---p 00000008 00:00 33463              /lib/x86_64-linux-gnu/librt-2.27.so
7f46221b6000-7f46221b7000 r--p 00006000 00:00 33463              /lib/x86_64-linux-gnu/librt-2.27.so
7f46221b7000-7f46221b8000 rw-p 00007000 00:00 33463              /lib/x86_64-linux-gnu/librt-2.27.so
7f46221c0000-7f46221da000 r-xp 00000000 00:00 33454              /lib/x86_64-linux-gnu/libpthread-2.27.so
7f46221da000-7f46221e4000 ---p 0001a000 00:00 33454              /lib/x86_64-linux-gnu/libpthread-2.27.so
7f46221e4000-7f46223d9000 ---p 00000024 00:00 33454              /lib/x86_64-linux-gnu/libpthread-2.27.so
7f46223d9000-7f46223da000 r--p 00019000 00:00 33454              /lib/x86_64-linux-gnu/libpthread-2.27.so
7f46223da000-7f46223db000 rw-p 0001a000 00:00 33454              /lib/x86_64-linux-gnu/libpthread-2.27.so
7f46223db000-7f46223df000 rw-p 00000000 00:00 0
7f46223e0000-7f46223fc000 r-xp 00000000 00:00 33495              /lib/x86_64-linux-gnu/libz.so.1.2.11
7f46223fc000-7f46223fd000 ---p 0001c000 00:00 33495              /lib/x86_64-linux-gnu/libz.so.1.2.11
7f46223fd000-7f46225fb000 ---p 0000001d 00:00 33495              /lib/x86_64-linux-gnu/libz.so.1.2.11
7f46225fb000-7f46225fc000 r--p 0001b000 00:00 33495              /lib/x86_64-linux-gnu/libz.so.1.2.11
7f46225fc000-7f46225fd000 rw-p 0001c000 00:00 33495              /lib/x86_64-linux-gnu/libz.so.1.2.11
7f4622600000-7f4622626000 r-xp 00000000 00:00 33283              /lib/x86_64-linux-gnu/ld-2.27.so
7f4622626000-7f4622627000 r-xp 00026000 00:00 33283              /lib/x86_64-linux-gnu/ld-2.27.so
7f4622639000-7f462264f000 r--p 00000000 00:00 2001779            /usr/lib/x86_64-linux-gnu/girepository-1.0/Arrow-1.0.typelib
7f462264f000-7f462267d000 r--p 00000000 00:00 137788             /usr/lib/x86_64-linux-gnu/girepository-1.0/GLib-2.0.typelib
7f462267d000-7f462268c000 r--p 00000000 00:00 137790             /usr/lib/x86_64-linux-gnu/girepository-1.0/GObject-2.0.typelib
7f462268c000-7f4622827000 r--p 00000000 00:00 35322              /usr/lib/locale/locale-archive
7f4622827000-7f4622828000 r--p 00027000 00:00 33283              /lib/x86_64-linux-gnu/ld-2.27.so
7f4622828000-7f4622829000 rw-p 00028000 00:00 33283              /lib/x86_64-linux-gnu/ld-2.27.so
7f4622829000-7f462282a000 rw-p 00000000 00:00 0
7f462282c000-7f4622880000 r--p 00000000 00:00 137791             /usr/lib/x86_64-linux-gnu/girepository-1.0/Gio-2.0.typelib
7f4622880000-7f4622981000 rw-p 00000000 00:00 0
7f462298f000-7f46229c0000 r--p 00000000 00:00 35311              /usr/lib/locale/C.UTF-8/LC_CTYPE
7f46229c0000-7f46229c3000 rw-p 00000000 00:00 0
7f46229d0000-7f46229d2000 rw-p 00000000 00:00 0
7f46229e0000-7f46229e2000 rw-p 00000000 00:00 0
7f46229ef000-7f46229f0000 r--p 00000000 00:00 35315              /usr/lib/locale/C.UTF-8/LC_MESSAGES/SYS_LC_MESSAGES
7f46229f0000-7f46229f7000 r--s 00000000 00:00 137779             /usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache
7f4622a00000-7f4622d78000 r-xp 00000000 00:00 453996             /usr/local/bin/ruby
7f4622d78000-7f4622d79000 r-xp 00378000 00:00 453996             /usr/local/bin/ruby
7f4622f78000-7f4622f7e000 r--p 00378000 00:00 453996             /usr/local/bin/ruby
7f4622f7e000-7f4622f7f000 rw-p 0037e000 00:00 453996             /usr/local/bin/ruby
7f4622f7f000-7f4622f90000 rw-p 00000000 00:00 0
7fffd22e4000-7fffd2ef6000 rw-p 00000000 00:00 0                  [heap]
7fffda434000-7fffdac34000 rw-p 00000000 00:00 0                  [stack]
7fffdaf55000-7fffdaf56000 r-xp 00000000 00:00 0                  [vdso]
[NOTE]
You may have encountered a bug in the Ruby interpreter or extension libraries.
Bug reports are welcome.
For details: https://www.ruby-lang.org/bugreport.htmll{code}",pull-request-available,['Ruby'],ARROW,Bug,Major,2019-12-20 05:09:57,1
13275452,[Java] ReadChannel#readFully does not set writer index correctly,"1. The writer index should be incremented by the amount of data actually read.
2. When EOS is encounterned, the number of bytes read should be incremented before returning.",pull-request-available,['Java'],ARROW,Bug,Trivial,2019-12-19 09:42:37,7
13275385,[Archery] Fix benchmark default configuration,Compute module is not being built since the slim default cmake configuration.,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-12-19 01:39:26,13
13275312,Security issue: ValidateOffsets() does not prevent buffer over-read,"Skimming through {{Validate()}} code in both 0.15 and master, I noticed an oversight in {{BinaryArray}} validation in C++ (and Python).

{{ValidateOffsets()}} checks that the first offset is 0, but it doesn't check that the offsets all point within the data buffer. A nefarious Arrow file could write {{offsets=[0,999999]}} and {{data=[]}}. If a caller reads the first value in that array, that will produce a buffer over-read.

Validation is cheap, since Arrow already validates that offsets are monotonically increasing. One need only test that the last offset is less than or equal to the size of the data buffer.

We at Workbench are letting untrusted programs write Arrow files that we then validate and read. We're keen to ensure Arrow files don't allow untrusted programs to plant data that leads to arbitrary code execution or arbitrary reads. We wrote a validation tool that checks this buffer over-read I describe here: https://github.com/CJWorkbench/arrow-tools/blob/005fe582b428c1ab6a9ed5f6dc968387d77e9a80/src/arrow-validate.cc#L27. But it feels to me like Arrow's {{Validate()}} should be checking this.
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-12-18 18:52:30,2
13275223,[Python] Add higher-level datasets functions,"From [~kszucs]: We need to define a more pythonic API for the dataset bindings, because the current one is pretty low-level.

One option is to provide a ""open_dataset"" function similar as what is available in R.

A short-cut to go from a Dataset to a Table might also be useful.",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-18 10:26:42,5
13275222,[Python] Add dataset API to reference docs,Add dataset to python API docs.,dataset pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-18 10:23:23,5
13275208,[Java] Enhance code style checking for Java code (remove consecutive spaces),"This issue is opened in response to a discussion in https://github.com/apache/arrow/pull/5861#discussion_r348917065.

We found the current style checking for Java code is not sufficient. So we want to enhace it in a series of ""small"" steps, in order to avoid having to change too many files at once. 

In this issue, we remove consecutive spaces between tokens, so that tokens are separated by single spaces. ",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-12-18 09:27:43,7
13275194,[Java] PromotableWriter support writing FixedSizeList type data,"We have introduced writer API for {{FixedSizeListVector}}via ARROW-6079, but {{PromotableWriter}}s support for it is incomplete.

For example, using {{UnionListWriter}}we could simply write {{List<List>}}type data, but for {{List<FixedSizeList>}}or {{FixedSizeList<FixedSizeList>}}it doesnt work.

This issue is about to enhance the {{PromotableWriter}}support for {{FixedSizeList}}type and add tests to verify the cases mentioned above.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-12-18 08:42:45,16
13275102,[C++] Can't build with g++ 5.4.0 on Ubuntu 16.04,"Full log: https://circleci.com/gh/ursa-labs/crossbow/6109

Formatted error message:

{noformat}
FAILED: /usr/bin/ccache /usr/lib/ccache/g++ \
  -DARROW_JEMALLOC \
  -DARROW_JEMALLOC_INCLUDE_DIR="""" \
  -DARROW_USE_GLOG \
  -DARROW_USE_SIMD \
  -DARROW_WITH_BOOST_FILESYSTEM \
  -DARROW_WITH_BROTLI \
  -DARROW_WITH_BZ2 \
  -DARROW_WITH_LZ4 \
  -DARROW_WITH_SNAPPY \
  -DARROW_WITH_ZLIB \
  -DGTEST_LINKED_AS_SHARED_LIBRARY=1 \
  -DURI_STATIC_BUILD \
  -isystem /arrow/cpp/thirdparty/flatbuffers/include \
  -isystem boost_ep-prefix/src/boost_ep \
  -isystem thrift_ep/src/thrift_ep-install/include \
  -isystem /arrow/cpp/thirdparty/protobuf_ep-install/include \
  -isystem jemalloc_ep-prefix/src \
  -isystem googletest_ep-prefix/src/googletest_ep/include \
  -isystem rapidjson_ep/src/rapidjson_ep-install/include \
  -isystem /arrow/cpp/thirdparty/hadoop/include \
  -Isrc \
  -I/arrow/cpp/src \
  -I/arrow/cpp/src/generated \
  -fdiagnostics-color=always \
  -ggdb \
  -O0 \
  -Wall \
  -Wno-conversion \
  -Wno-sign-conversion \
  -Wno-unused-variable \
  -Werror \
  -Wno-attributes \
  -msse4.2 \
  -g \
  -fPIE \
  -pthread \
  -std=gnu++11 \
  -MMD \
  -MT src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o \
  -MF src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o.d \
  -o src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o \
  -c /arrow/cpp/src/arrow/dataset/dataset_test.cc
/arrow/cpp/src/arrow/dataset/dataset_test.cc: In member function
  'virtual void arrow::dataset::TestSchemaUnification_SelectStar_Test::TestBody()':
/arrow/cpp/src/arrow/dataset/dataset_test.cc:531:3: error:
  converting to '
    std::tuple<nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int>,
               nonstd::optional_lite::optional<int> >'
  from initializer list would use explicit constructor '
    constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...)
      [with
       _UElements = {
         int,
         int,
         const nonstd::optional_lite::nullopt_t&,
         const nonstd::optional_lite::nullopt_t&,
         int,
         int
       };
       <template-parameter-2-2> = void;
       _Elements = {
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>,
         nonstd::optional_lite::optional<int>
       }]'
   };
   ^
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-17 21:51:09,1
13275073,[R][Nightly] Fix macos-r-autobrew build on R 3.6.2,"The nightly build started failing when R 3.6.2 came out. Ive done some digging and debugging.

Here is the install log from the failure on 3.6.2: https://dl.bintray.com/ursalabs/arrow-r/logs/bin/macosx/el-capitan/contrib/3.6/20191216-00install.out

Here is the log from success on 3.6.0: https://dl.bintray.com/ursalabs/arrow-r/logs/bin/macosx/el-capitan/contrib/3.6/20191217-00install.out

On 3.6.2, theres a longer temp dir path, with working_dir in it. I traced that to this change: https://github.com/wch/r-source/commit/956ea7f7d4db25ad6a2fb32f177e9ec5995d350d
When it fails, theres a bunch of Note: The post-install step did not complete successfully messages, and then later Error: too long unix socket path (123bytes given but 104bytes max). Which makes me think that the extra `/Rtmpyp98FZ/working_dir/` temp path is pushing us over some limit?",pull-request-available,"['Continuous Integration', 'R']",ARROW,New Feature,Major,2019-12-17 18:52:02,4
13275069,[C++][Dataset] Implement IpcFormat for sources composed of ipc files,Currently only parquet is supported. IPC files make a nice test case for multiple file formats since they also have a completely unambiguous physical schema (unlike CSV) and support for reading/writing is already present.,dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-12-17 18:26:15,6
13275061,[C++][Dataset] Ensure that dataset code is robust to schemas with duplicate field names,"Fields in a schema don't have to have unique names, so we should make sure we're not assuming that.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-12-17 17:26:32,6
13274967,[C++] Reference benchmarks fail compiling,"{code}
../src/arrow/util/compression_benchmark.cc:72:23: warning: 'MakeCompressor' is deprecated: Use Result-returning version [-Wdeprecated-declarations]
  ABORT_NOT_OK(codec->MakeCompressor(&compressor));
                      ^
../src/arrow/util/compression.h:189:3: note: 'MakeCompressor' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use Result-returning version"")
  ^
../src/arrow/util/macros.h:96:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
../src/arrow/util/compression_benchmark.cc:83:61: error: too many arguments to function call, expected 4, have 6
                                      output_buffer.data(), &bytes_read, &bytes_written));
                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/arrow/testing/gtest_util.h:108:18: note: expanded from macro 'ABORT_NOT_OK'
    auto _res = (expr);                                             \
                 ^~~~
../src/arrow/util/compression.h:62:3: note: 'Compress' declared here
  virtual Result<CompressResult> Compress(int64_t input_len, const uint8_t* input,
  ^
../src/arrow/util/compression_benchmark.cc:101:34: error: too many arguments to function call, expected 2, have 4
                                 &bytes_written, &should_retry));
                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/arrow/testing/gtest_util.h:108:18: note: expanded from macro 'ABORT_NOT_OK'
    auto _res = (expr);                                             \
                 ^~~~
../src/arrow/util/compression.h:77:3: note: 'End' declared here
  virtual Result<EndResult> End(int64_t output_len, uint8_t* output) = 0;
  ^
../src/arrow/util/compression_benchmark.cc:121:45: error: variable 'codec' declared with deduced type 'auto' cannot appear in its own initializer
  auto codec = *Codec::Create(compression, &codec);
                                            ^
../src/arrow/util/compression_benchmark.cc:143:23: warning: 'Create' is deprecated: Use Result-returning version [-Wdeprecated-declarations]
  ABORT_NOT_OK(Codec::Create(compression, &codec));
                      ^
../src/arrow/util/compression.h:170:3: note: 'Create' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use Result-returning version"")
  ^
../src/arrow/util/macros.h:96:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
../src/arrow/util/compression_benchmark.cc:152:25: warning: 'MakeDecompressor' is deprecated: Use Result-returning version [-Wdeprecated-declarations]
    ABORT_NOT_OK(codec->MakeDecompressor(&decompressor));
                        ^
../src/arrow/util/compression.h:193:3: note: 'MakeDecompressor' has been explicitly marked deprecated here
  ARROW_DEPRECATED(""Use Result-returning version"")
  ^
../src/arrow/util/macros.h:96:48: note: expanded from macro 'ARROW_DEPRECATED'
#  define ARROW_DEPRECATED(...) __attribute__((deprecated(__VA_ARGS__)))
                                               ^
../src/arrow/util/compression_benchmark.cc:163:67: error: too many arguments to function call, expected 4, have 7
                                            output_buffer.data(), &bytes_read,
                                                                  ^~~~~~~~~~~~
../src/arrow/testing/gtest_util.h:108:18: note: expanded from macro 'ABORT_NOT_OK'
    auto _res = (expr);                                             \
                 ^~~~
../src/arrow/util/compression.h:99:3: note: 'Decompress' declared here
  virtual Result<DecompressResult> Decompress(int64_t input_len, const uint8_t* input,
  ^
3 warnings and 4 errors generated.
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-12-17 11:20:11,2
13274916,[Python] Failed to install pyarrow 0.15.1 on Python 3.8,"Hi, I cannot install pyarrow 0.15.1

Steps:
{noformat}
$ python3 -m venv virtualenv
$ source virtualenv/bin/activate
$ pip install numpy
$ virtualenv/bin/python3 -c 'import numpy as n; print(n.__version__); print(n.get_include());'
1.17.4
/tmp/virtualenv/lib/python3.8/site-packages/numpy/core/include

$ pip install pyarrow==0.15.1{noformat}
Log: see attached log",pull-request-available,['Python'],ARROW,Bug,Critical,2019-12-17 07:19:34,8
13274899,[Java] NonNullableStructVector#hashCode should pass hasher to child vectors,"This was introduced by ARROW-6866 making parameter hasher useless in hashCode(int index, {{ArrowBufHasher}}hasher), and the child vectors would calculate hashCode using default hasher which is not correct.

This issue should be fixed by passing hasher to child vector when calculating hashCode.",pull-request-available,['Java'],ARROW,Bug,Major,2019-12-17 05:15:50,16
13274883,[Java] ListVector isEmpty API is incorrect,"Currently {{isEmpty}}API is always return false in {{BaseRepeatedValueVector}}, and its subclass {{ListVector}}did not overwrite this method.

This will lead to incorrect result, for example, a {{ListVector}}with data [1,2], null, [], [5,6] should get [false, false, true, false] with this API, but now it would return [false, false, false, false].",pull-request-available,['Java'],ARROW,Bug,Minor,2019-12-17 03:02:31,16
13274744,[Java] Avoids the worst case for quick sort,"This issue is in response of a discussion in: https://github.com/apache/arrow/pull/5540#discussion_r329487232.

The quick sort algorithm can degenerate to an O(n^2) algorithm, if the pivot is selected poorly. This is an important problem, as the worst case can happen, if the input vector is alrady sorted, which is frequently encountered in practice.

After some investigation, we solve the problem with a simple but effective approach: take 3 samples and choose the median (with at most 3 comparisons) as the pivot. This sorts the vector which is already sorted in O(nlogn) time. ",pull-request-available,['Java'],ARROW,Improvement,Major,2019-12-16 12:08:35,7
13274462,[Packaging] Add conda packaging tasks for python 3.8,Conda-forge now supports python 3.8 so we should build the appropriate packages.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-12-13 17:51:31,3
13274455,[Python] Remove unnecessary classes from the binding layer,"Several Python classes introduced by https://github.com/apache/arrow/pull/5237 are unnecessary and can be removed in favor of simple functions which produce opaque pointers, including the PartitionScheme and Expression classes. These should be removed to reduce cognitive overhead of the Python datasets API and to loosen coupling between Python and C++.",dataset pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-13 17:19:17,6
13274451,[C++][Dataset] Concurrency race in Projector::Project ,"When a DataFragment is invoked by 2 scan tasks of the same DataFragment, there's a race to invoke SetInputSchema. Note that ResizeMissingColumns also suffers from this race. The ideal goal is to make Project a const method.",dataset pull-request-available,['C++'],ARROW,Bug,Major,2019-12-13 16:56:06,13
13274424,[Python][Packaging] Remove pyarrow.s3fs import check from the recipe,"The nightly conda packaging tests are failing because of the recently removed s3fs module in favor of pyarrow.fs.
https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-12-13-0-azure-conda-linux-gcc-py27",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-12-13 14:22:24,3
13274423,[Python] Skip HDFS tests if libhdfs cannot be located,CI is failing because libhdfs is not installed. We should skip the HDFS tests in this case.,pull-request-available,['Python'],ARROW,Bug,Major,2019-12-13 14:17:52,3
13274214,[C++][Dataset] Refactor FsDsDiscovery constructors,"This constructor should not take a vector of filestats. Instead, provide a convenience constructor taking paths and a filesystem pointer. Also, ensure that missing parent directories are injected (otherwise partition scheme application will fail).",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-12 17:58:58,6
13274192,[C++][Packaging] Iterator change broke manylinux1 wheels,"{code}
[171/235] Building CXX object src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/scanner.cc.o
FAILED: src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/scanner.cc.o 
/usr/bin/ccache /opt/rh/devtoolset-2/root/usr/bin/c++  -DARROW_DS_EXPORTING -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR="""" -DARROW_USE_SIMD -DARROW_WITH_BOOST_FILESYSTEM -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DURI_STATIC_BUILD -Isrc -I/arrow/cpp/src -I/arrow/cpp/src/generated -I/arrow/cpp/src/parquet -isystem /arrow/cpp/thirdparty/flatbuffers/include -isystem /arrow_boost_dist/include -isystem jemalloc_ep-prefix/src -isystem /usr/local/include -isystem /arrow/cpp/thirdparty/hadoop/include -O3 -DNDEBUG  -Wall -Wno-attributes -msse4.2  -O3 -DNDEBUG -fPIC   -std=gnu++11 -MD -MT src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/scanner.cc.o -MF src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/scanner.cc.o.d -o src/arrow/dataset/CMakeFiles/arrow_dataset_objlib.dir/scanner.cc.o -c /arrow/cpp/src/arrow/dataset/scanner.cc
In file included from /arrow/cpp/src/arrow/dataset/dataset_internal.h:30:0,
                 from /arrow/cpp/src/arrow/dataset/scanner.cc:25:
/arrow/cpp/src/arrow/util/iterator.h: In instantiation of arrow::Iterator<T>::RangeIterator arrow::Iterator<T>::end() [with T = std::shared_ptr<arrow::RecordBatch>]:
/arrow/cpp/src/arrow/dataset/scanner.cc:173:29:   required from here
/arrow/cpp/src/arrow/util/iterator.h:113:5: error: conversion from std::shared_ptr<arrow::RecordBatch> to non-scalar type arrow::Result<std::shared_ptr<arrow::RecordBatch> > requested
     RangeIterator() = default;
     ^
/arrow/cpp/src/arrow/util/iterator.h:147:46: note: synthesized method arrow::Iterator<T>::RangeIterator::RangeIterator() [with T = std::shared_ptr<arrow::RecordBatch>] first required here 
   RangeIterator end() { return RangeIterator(); }
                                              ^
/arrow/cpp/src/arrow/util/iterator.h: In instantiation of arrow::Iterator<T>::RangeIterator arrow::Iterator<T>::end() [with T = std::shared_ptr<arrow::dataset::ScanTask>]:
/arrow/cpp/src/arrow/dataset/scanner.cc:190:31:   required from here
/arrow/cpp/src/arrow/util/iterator.h:113:5: error: conversion from std::shared_ptr<arrow::dataset::ScanTask> to non-scalar type arrow::Result<std::shared_ptr<arrow::dataset::ScanTask> > requested
     RangeIterator() = default;
     ^
/arrow/cpp/src/arrow/util/iterator.h:147:46: note: synthesized method arrow::Iterator<T>::RangeIterator::RangeIterator() [with T = std::shared_ptr<arrow::dataset::ScanTask>] first required here 
   RangeIterator end() { return RangeIterator(); }
                                              ^
{code}",pull-request-available,"['C++', 'Packaging']",ARROW,Bug,Major,2019-12-12 16:11:52,6
13274190,[C++][Dataset] Implement DatasetFactory,Takes a list of DataSourceDiscovery and yields a Dataset.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-12 16:08:11,13
13274188,[C++] Introduce SchemaBuilder companion class and Field::IsCompatibleWith,The methods verifies if fields/schemas are compatible with regards to naming and type. This is a partly extracted from `UnifySchemas`.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-12 16:04:24,13
13273962,[C++][Dataset] Simplify parquet column projection,This is a minor makeup.,dataset pull-request-available,['C++'],ARROW,Improvement,Minor,2019-12-11 21:00:02,13
13273946,[Python] Expose C++ MakeArrayOfNull,This would allow creating an array of nulls efficiently. Perhaps expose it as {{pa.Array.all_nulls}} or something.,pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-11 19:19:42,3
13273943,[Dev] [C++] cuda-cpp docker image fails compiling Arrow,"{code}
[2/9] Linking CXX executable debug/plasma-store-server
FAILED: debug/plasma-store-server 
: && /usr/bin/ccache /usr/lib/ccache/c++  -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -fsanitize=address -DADDRESS_SANITIZER -fsanitize=undefined -fno-sanitize=alignment,vptr -fno-sanitize-recover=all -fPIC -g  -rdynamic src/plasma/CMakeFiles/plasma-store-server.dir/external_store.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/hash_table_store.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/dlmalloc.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/events.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/eviction_policy.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/quota_aware_policy.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/plasma_allocator.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/store.cc.o src/plasma/CMakeFiles/plasma-store-server.dir/thirdparty/ae/ae.c.o  -o debug/plasma-store-server  debug/libplasma.a debug/libarrow_cuda.a debug/libarrow.a /usr/local/cuda/lib64/libcudart_static.a -ldl -lrt -lCUDA_CUDA_LIBRARY-NOTFOUND /usr/lib/x86_64-linux-gnu/libglog.so /usr/lib/x86_64-linux-gnu/libboost_filesystem.so /usr/lib/x86_64-linux-gnu/libboost_system.so jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt && :
/usr/bin/ld: cannot find -lCUDA_CUDA_LIBRARY-NOTFOUND
collect2: error: ld returned 1 exit status
{code}",pull-request-available,"['C++', 'Developer Tools', 'GPU']",ARROW,Bug,Major,2019-12-11 19:10:28,2
13273903,[C++] Allow creating dictionary array from simple JSON,Would make writing some tests slightly easier.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-11 14:59:53,2
13273785,[C++] Old Protobuf with AUTO detection is failed,"{noformat}
-- Could NOT find Protobuf: Found unsuitable version ""3.6.1"", but required is at least ""3.7.0"" (found /usr/lib/x86_64-linux-gnu/libprotobuf.so;-pthread)
Building Protocol Buffers from source
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:1179 (add_library):
  add_library cannot create imported target ""protobuf::libprotobuf"" because
  another target with the same name already exists.
Call Stack (most recent call first):
  cmake_modules/ThirdpartyToolchain.cmake:147 (build_protobuf)
  cmake_modules/ThirdpartyToolchain.cmake:178 (build_dependency)
  cmake_modules/ThirdpartyToolchain.cmake:1204 (resolve_dependency_with_version)
  CMakeLists.txt:428 (include)
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-11 07:55:31,1
13273678,[C++][Dataset] Use PartitionSchemeDiscovery in DataSourceDiscovery,"https://github.com/apache/arrow/pull/5950 introduces {{PartitionSchemeDiscovery}}, but ideally it would be supplied as an option to data source discovery and the partition schema automatically discovered based on the file paths accumulated then.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-12-10 18:08:08,6
13273622,[Python] Support FixedSizeList type in conversion to numpy/pandas,"Follow-up on ARROW-7261, still need to add support for FixedSizeListType in the arrow -> python conversion (arrow_to_pandas.cc)",pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-10 14:01:35,14
13273468,[Rust] Build directory is not passed to ci/scripts/rust_test.sh,See build https://github.com/apache/arrow/runs/340751277,pull-request-available,['Rust'],ARROW,Bug,Major,2019-12-09 21:02:32,3
13273463,[R] Can't use dplyr filter() with variables defined in parent scope,"The following will generate an error


{code:r}
test_that(""filtering with expression"", {
  char_sym <- ""b""                       
  expect_dplyr_equal(                   
    input %>%                           
      filter(chr == char_sym) %>%       
      select(string = chr, int) %>%     
      collect(),                        
    tbl                                 
  )                                     
})                                      

{code}
",pull-request-available,['R'],ARROW,Bug,Major,2019-12-09 20:24:32,4
13273400,[CI] Environment variables are defined twice for the fuzzit builds,"https://github.com/apache/arrow/commit/7102d7eeef60fd0fb4fb7b06ed092d63db961b15#diff-4e5e90c6228fd48698d074241c2ba760R916 broke the fuzzit builds

The environment variables should be merged.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-12-09 14:19:49,3
13273388,[C++] TestHadoopFileSystem::ThreadSafety fails with sigabort,"The regression has been introduced recently:
https://github.com/ursa-labs/crossbow/branches/all?utf8=&query=hdfs
Most certainly with commit:
https://github.com/apache/arrow/commit/6758b24fdd4525dda0f9b2760d016753015a948d

{code}
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007f9988f69801 in __GI_abort () at abort.c:79
#2  0x00007f998654abf5 in os::abort(bool) () from /opt/conda/envs/arrow/jre/lib/amd64/server/libjvm.so
#3  0x00007f99866dce03 in VMError::report_and_die() () from /opt/conda/envs/arrow/jre/lib/amd64/server/libjvm.so
#4  0x00007f9986551622 in JVM_handle_linux_signal () from /opt/conda/envs/arrow/jre/lib/amd64/server/libjvm.so
#5  0x00007f9986546c93 in signalHandler(int, siginfo*, void*) () from /opt/conda/envs/arrow/jre/lib/amd64/server/libjvm.so
#6  <signal handler called>
#7  0x000055f59f107870 in arrow::Buffer::data (this=0x726162) at /arrow/cpp/src/arrow/buffer.h:181
#8  0x000055f59f121bbe in arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::TestBody()::{lambda()#1}::operator()() const (__closure=0x55f5a03a1ba8) at /arrow/cpp/src/arrow/io/hdfs_test.cc:470
#9  0x000055f59f130d76 in std::__invoke_impl<void, arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::TestBody()::{lambda()#1}>(std::__invoke_other, arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDri
ver>::TestBody()::{lambda()#1}&&) (__f=...) at /opt/conda/envs/arrow/x86_64-conda_cos6-linux-gnu/include/c++/7.3.0/bits/invoke.h:60
#10 0x000055f59f12f9ee in std::__invoke<arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::TestBody()::{lambda()#1}>(std::__invoke_result&&, (arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::T
estBody()::{lambda()#1}&&)...) (__fn=...) at /opt/conda/envs/arrow/x86_64-conda_cos6-linux-gnu/include/c++/7.3.0/bits/invoke.h:95
#11 0x000055f59f134148 in std::thread::_Invoker<std::tuple<arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::TestBody()::{lambda()#1}> >::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0x55f5a03a1ba8)
    at /opt/conda/envs/arrow/x86_64-conda_cos6-linux-gnu/include/c++/7.3.0/thread:234
#12 0x000055f59f1340f5 in std::thread::_Invoker<std::tuple<arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::TestBody()::{lambda()#1}> >::operator()() (this=0x55f5a03a1ba8)
    at /opt/conda/envs/arrow/x86_64-conda_cos6-linux-gnu/include/c++/7.3.0/thread:243
#13 0x000055f59f1340b4 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<arrow::io::TestHadoopFileSystem_ThreadSafety_Test<arrow::io::JNIDriver>::TestBody()::{lambda()#1}> > >::_M_run() (this=0x55f5a03a1ba0)
    at /opt/conda/envs/arrow/x86_64-conda_cos6-linux-gnu/include/c++/7.3.0/thread:186
#14 0x00007f99893e2163 in std::execute_native_thread_routine (__p=0x55f5a03a1ba0)
    at /home/conda/feedstock_root/build_artifacts/ctng-compilers_1574978377740/work/.build/x86_64-conda_cos6-linux-gnu/src/gcc/libstdc++-v3/src/c++11/thread.cc:80
#15 0x00007f99894956db in start_thread (arg=0x7f99039ff700) at pthread_create.c:463
#16 0x00007f998904a88f in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
{code}",hdfs pull-request-available,['C++'],ARROW,Bug,Major,2019-12-09 13:57:21,2
13273378,[C++] Disable -Wmissing-braces when building with clang,"I found this fails the build with Xcode 8.3.3. It seems that it is advised to ignore this warning

https://stackoverflow.com/questions/13905200/is-it-wise-to-ignore-gcc-clangs-wmissing-braces-warning",pull-request-available,['C++'],ARROW,Bug,Major,2019-12-09 13:29:27,14
13273325,[Developer] Only suggest cpp-* fix versions when merging Parquet patches,"The default fix version (1.11.0) for Parquet issues is resulting in the wrong fix version being set sometimes when committers merge PARQUET-* patches. Since we only have C++ Parquet issues, I think we can safely only suggest the cpp-* fix versions",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-12-09 11:06:46,14
13273315,[C++] Fix the bug of parsing string hex values,"std::lower_bound returns the end of the search range, when failing to find a match. 

The end of the search range is one position after the last valid position. So the value in this position is undefined, and we should not reference the value here to compare it with the target value. ",pull-request-available,['C++'],ARROW,Bug,Minor,2019-12-09 10:26:36,7
13272968,[C++] Update bundled Boost to 1.71.0,Because we can't build Boost 1.67.0 with Visual Studio 2019.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-07 22:29:10,1
13272956,[CI] Explicit usage of ccache across the builds,ccache is implicitly enabled and configured. We should prefer explicit configurations.,pull-request-available,['Continuous Integration'],ARROW,Task,Major,2019-12-07 18:10:36,3
13272861,[Packaging][Python] Build manylinux2014 wheels,"See https://www.python.org/dev/peps/pep-0599/

https://github.com/pypa/manylinux/issues/338 tracks the standard's progress

https://quay.io/organization/pypa now has manylinux2014 wheels on it

I've been experimenting with it and have made https://dl.bintray.com/nealrichardson/pyarrow-dev/pyarrow-0.15.1.dev427+ga309da790.d20191206-cp37-cp37m-linux_x86_64.whl",pull-request-available wheel,"['Packaging', 'Python']",ARROW,New Feature,Major,2019-12-06 22:24:00,4
13272855,[Java] Memory leak in Flight DoGet when client cancels,"I believe this causes things like ARROW-4765.

-If a stream is interrupted or otherwise not drained by the client, the serialized form of the ArrowMessage (DrainableByteBufInputStream) will sit around forever, leaking memory.-",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-12-06 21:31:42,0
13272836,[CI] Unbreak nightly Conda R job,ARROW-7146 fixed a number of issues in the regular R docker setup and made the testing more rigorous. At least one of those fixes also should have been added to the R conda setup.,pull-request-available,['Continuous Integration'],ARROW,Bug,Minor,2019-12-06 19:11:08,4
13272781,[C++] Improve InMemoryDataSource to support generator instead of static list,"The constructor should take a generator

{code:c++}
// Some comments here
class InMemoryDataSource : public DataSource {
  public:
    using Generator = std::function<Iterator<std::shared_ptr<RecordBatch>()>;

    InMemoryDataSource(Generator&& generator);
    // Convenience constructor to support a fixed list of RecordBatch
    InMemoryDataSource(std::shared_ptr<RecordBatch>);
    InMemoryDataSource(std::vector<std::shared_ptr<RecordBatch>>);

private:
  Generator generator;
}
{code}
",dataset,['C++'],ARROW,Improvement,Major,2019-12-06 15:31:18,6
13272656,[CI][Python] macOS uses Python 2,We should use Python 3.,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2019-12-06 05:34:34,1
13272600,[CI][Rust] Remove duplicated nightly job,"We run the same test on each push with GitHub Actions.
",pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Improvement,Major,2019-12-05 21:07:47,1
13272583,[C++][Parquet] Explicitly catch status exceptions in PARQUET_CATCH_NOT_OK,"PARQUET_THROW_NOT_OK throws a ParquetStatusException, which contains a full Status rather than just an error string. These could be caught explicitly in PARQUET_CATCH_NOT_OK and the original status returned rather than creating a new status:

{code}
  } catch (const ::parquet::ParquetStatusException& e) { \
    return e.status(); \
  } catch (const ::parquet::ParquetException& e) { \
    return Status::IOError(e.what()) \
{code}

This will retain the original StatusCode rather than overwriting it with IOError.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-12-05 19:54:05,2
13272425,[CI] GitHub Actions should trigger on changes to GitHub Actions configuration,"Currently, when you only change something in the {{.github/workflows}} directory, most GitHub Actions don't get run.",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-12-05 09:15:20,3
13272403,[Rust] Add Timezone to Timestamp,Proposal to add timestamp to timezone type,pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-12-05 07:46:12,12
13272356,[CI][Rust] Nightly CI is failed by different toolchain,"https://circleci.com/gh/ursa-labs/crossbow/5685

{noformat}
Step 8/8 : RUN rustup component add rustfmt --toolchain nightly-2019-11-14-x86_64-unknown-linux-gnu
 ---> Running in b28e33fbf36d
error: toolchain 'nightly-2019-11-14-x86_64-unknown-linux-gnu' is not installed
ERROR: Service 'debian-rust' failed to build: The command '/bin/sh -c rustup component add rustfmt --toolchain nightly-2019-11-14-x86_64-unknown-linux-gnu' returned a non-zero code: 1
{noformat}",pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Improvement,Major,2019-12-05 00:06:29,1
13272344,[CI][Python] Fall back to arrowdev dockerhub organization for manylinux images,Until https://issues.apache.org/jira/browse/INFRA-19499 is resolved.,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Task,Major,2019-12-04 23:28:47,3
13272327,[CI][GLib] Failed to build with GLib warning,"https://circleci.com/gh/ursa-labs/crossbow/5681

{noformat}
FAILED: arrow-glib/20f505c@@arrow-glib@sha/file.cpp.o
ccache c++ -Iarrow-glib/20f505c@@arrow-glib@sha -Iarrow-glib -I../../arrow/c_glib/arrow-glib -I. -I../../arrow/c_glib/ -I/usr/local/include -I/usr/include/glib-2.0 -I/usr/lib/x86_64-linux-gnu/glib-2.0/include -fdiagnostics-color=always -pipe -D_FILE_OFFSET_BITS=64 -Wall -Winvalid-pch -Wnon-virtual-dtor -std=c++11 -g -Werror -DARROW_NO_DEPRECATED_API -fPIC -pthread -MD -MQ 'arrow-glib/20f505c@@arrow-glib@sha/file.cpp.o' -MF 'arrow-glib/20f505c@@arrow-glib@sha/file.cpp.o.d' -o 'arrow-glib/20f505c@@arrow-glib@sha/file.cpp.o' -c ../../arrow/c_glib/arrow-glib/file.cpp
In file included from /usr/include/glib-2.0/gobject/gobject.h:24:0,
                 from /usr/include/glib-2.0/gobject/gbinding.h:29,
                 from /usr/include/glib-2.0/glib-object.h:23,
                 from ../../arrow/c_glib/arrow-glib/error.h:22,
                 from ../../arrow/c_glib/arrow-glib/error.hpp:24,
                 from ../../arrow/c_glib/arrow-glib/file.cpp:26:
../../arrow/c_glib/arrow-glib/file.cpp: In function 'GType garrow_file_get_type()':
/usr/include/glib-2.0/gobject/gtype.h:219:50: error: '<<' in boolean context, did you mean '<' ? [-Werror=int-in-bool-context]
 #define G_TYPE_MAKE_FUNDAMENTAL(x) ((GType) ((x) << G_TYPE_FUNDAMENTAL_SHIFT))
                                             ~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~
/usr/include/glib-2.0/gobject/gtype.h:2015:11: note: in definition of macro '_G_DEFINE_INTERFACE_EXTENDED_BEGIN'
       if (TYPE_PREREQ) \
           ^~~~~~~~~~~
/usr/include/glib-2.0/gobject/gtype.h:1756:47: note: in expansion of macro 'G_DEFINE_INTERFACE_WITH_CODE'
 #define G_DEFINE_INTERFACE(TN, t_n, T_P)      G_DEFINE_INTERFACE_WITH_CODE(TN, t_n, T_P, ;)
                                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
../../arrow/c_glib/arrow-glib/file.cpp:40:1: note: in expansion of macro 'G_DEFINE_INTERFACE'
 G_DEFINE_INTERFACE(GArrowFile,
 ^~~~~~~~~~~~~~~~~~
/usr/include/glib-2.0/gobject/gtype.h:178:25: note: in expansion of macro 'G_TYPE_MAKE_FUNDAMENTAL'
 #define G_TYPE_OBJECT   G_TYPE_MAKE_FUNDAMENTAL (20)
                         ^~~~~~~~~~~~~~~~~~~~~~~
../../arrow/c_glib/arrow-glib/file.cpp:42:20: note: in expansion of macro 'G_TYPE_OBJECT'
                    G_TYPE_OBJECT)
                    ^~~~~~~~~~~~~
cc1plus: all warnings being treated as errors
{noformat}",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Improvement,Major,2019-12-04 22:10:53,1
13272296,[Python] Compiler warning in pyarrow,"Saw this while building a wheel locally

{code}
-- Running cmake --build for pyarrow
cmake --build . --config release -- -j3
[23/24] Building CXX object CMakeFiles/lib.dir/lib.cpp.o
lib.cpp: In function PyObject* __pyx_pf_7pyarrow_3lib_90union(PyObject*, PyObject*, PyObject*, PyObject*):
lib.cpp:37184:29: warning: comparison of integer expressions of different signedness: Py_ssize_t {aka long int} and std::vector<std::shared_ptr<arrow::Field> >::size_type {aka long unsigned int} [-Wsign-compare]
     __pyx_t_2 = ((__pyx_t_6 != __pyx_v_c_fields.size()) != 0);
                   ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2019-12-04 18:25:27,5
13272221,[Python] Return filesystem and path from URI,The C++ API {{FileSystemFromUri}} should be exposed in Python.,pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-04 13:14:29,3
13272217,[Python] Expose HDFS implementation for pyarrow.fs,There's a C++ implementation for HDFS (see {{arrow/filesystem/hdfs.h}}) but it's not exposed from Python.,pull-request-available,['Python'],ARROW,Improvement,Major,2019-12-04 13:10:30,3
13272149,[CI][GLib] Documentation isn't generated,"https://github.com/apache/arrow/runs/332154195#step:5:12092

{noformat}
 + rsync -a --delete '/usr/local/share/gtk-doc/html/*' /build/docs/c_glib
rsync: change_dir ""/usr/local/share/gtk-doc/html"" failed: No such file or directory (2)
rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1196) [sender=3.1.2]
{noformat}",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Improvement,Major,2019-12-04 08:28:34,1
13272045,[C++] Refactor benchmarks to use new Result APIs,"When building benchmarks, I get the following error:
{code}
../src/arrow/csv/converter_benchmark.cc:83:64: error: too many arguments to function call, expected 2, have 3
    ABORT_NOT_OK(converter->Convert(parser, 0 /* col_index */, &result));
{code}

This was introduced by ARROW-7236. I guess the CI didn't catch it because we don't currently build benchmarks? [~apitrou] [~kszucs]",pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-03 19:35:02,6
13271959,[C++] CSV: allow converting a column to a specific dictionary type,"We can probably limit ourselves to {{dictionary(int32, utf8)}}.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-12-03 14:05:54,2
13271884,[Java] Sql type DATE should correspond to DateDayVector,"According to SQL convertion, sql type DATE should correspond to a format of YYYY-MM-DD, without the components for hour/minute/second/millis

Therefore, JDBC type DATE should correspond to DateDayVector, with a type width of 4, instead of 8. ",pull-request-available,['Java'],ARROW,Bug,Minor,2019-12-03 07:48:18,7
13271837,[C++] cpp/thirdparty/download-dependencies.sh is broken,This occurred during some of the recent vendoring work,pull-request-available,['C++'],ARROW,Bug,Major,2019-12-03 01:11:05,14
13271586,[C++][R] read_parquet() freezes on Windows with Japanese locale,"The following example on read_parquet()'s doc freezes (seems to wait for the result forever) on my Windows.

df <- read_parquet(system.file(""v0.7.1.parquet"", package=""arrow""))

The CRAN checks are all fine, which means the example is successfully executed on the CRAN Windows. So, I have no idea why it doesn't work on my local.

[https://cran.r-project.org/web/checks/check_results_arrow.html]

Here's my session info in case it helps:
{code:java}
> sessioninfo::session_info()

- Session info ---------------------------------------------------------------------------------
 setting  value
 version  R version 3.6.1 (2019-07-05)
 os       Windows 10 x64
 system   x86_64, mingw32
 ui       RStudio
 language en
 collate  Japanese_Japan.932
 ctype    Japanese_Japan.932
 tz       Asia/Tokyo
 date     2019-12-01

- Packages -------------------------------------------------------------------------------------
 package     * version  date       lib source
 arrow       * 0.15.1.1 2019-11-05 [1] CRAN (R 3.6.1)
 assertthat    0.2.1    2019-03-21 [1] CRAN (R 3.6.0)
 bit           1.1-14   2018-05-29 [1] CRAN (R 3.6.0)
 bit64         0.9-7    2017-05-08 [1] CRAN (R 3.6.0)
 cli           1.1.0    2019-03-19 [1] CRAN (R 3.6.0)
 crayon        1.3.4    2017-09-16 [1] CRAN (R 3.6.0)
 fs            1.3.1    2019-05-06 [1] CRAN (R 3.6.0)
 glue          1.3.1    2019-03-12 [1] CRAN (R 3.6.0)
 magrittr      1.5      2014-11-22 [1] CRAN (R 3.6.0)
 purrr         0.3.3    2019-10-18 [1] CRAN (R 3.6.1)
 R6            2.4.1    2019-11-12 [1] CRAN (R 3.6.1)
 Rcpp          1.0.3    2019-11-08 [1] CRAN (R 3.6.1)
 reprex        0.3.0    2019-05-16 [1] CRAN (R 3.6.0)
 rlang         0.4.2    2019-11-23 [1] CRAN (R 3.6.1)
 rstudioapi    0.10     2019-03-19 [1] CRAN (R 3.6.0)
 sessioninfo   1.1.1    2018-11-05 [1] CRAN (R 3.6.0)
 tidyselect    0.2.5    2018-10-11 [1] CRAN (R 3.6.0)
 withr         2.1.2    2018-03-15 [1] CRAN (R 3.6.0)

[1] C:/Users/hiroaki-yutani/Documents/R/win-library/3.6
[2] C:/Program Files/R/R-3.6.1/library
{code}",parquet pull-request-available,"['C++', 'R']",ARROW,Bug,Critical,2019-12-02 01:54:00,1
13271582,[C++] ensure C++ implementation meets clarified dictionary spec,"see parent issue.



CC [~tianchen92]",pull-request-available,['C++'],ARROW,Sub-task,Major,2019-12-02 01:18:52,7
13271581,[Java] ensure java implementation meets clarified dictionary spec,"see parent issue.



CC [~tianchen92]",pull-request-available,['Java'],ARROW,Sub-task,Major,2019-12-02 01:18:26,16
13271573,[Python] IO functions should raise FileNotFoundError when appropriate,"I get the following error when trying to open a file that does not exist.

```
 pyarrow.lib.ArrowIOError: Failed to open local file 'filename', error: No such file or directory

```

In my opinion, this particular error should also subclass from Python FileNotFoundError. It currently only inherits from IOError (which is a superclass of FileNotFoundError).

```
 >>> import pyarrow
 >>> try:
 ... raise pyarrow.lib.ArrowIOError
 ... except IOError:
 ... pass
 ... 
 >>> try:
 ... raise pyarrow.lib.ArrowIOError
 ... except FileNotFoundError:
 ... print('caught')
 ... 
 Traceback (most recent call last):
 File ""<stdin>"", line 2, in <module>
 pyarrow.lib.ArrowIOError
 >>> try:
 ... raise pyarrow.lib.ArrowIOError
 ... except IOError:
 ... print('caught')
 ... 
 caught
 ```",exception-reporting pull-request-available pyarrow,"['C++', 'Python']",ARROW,Bug,Minor,2019-12-01 20:28:10,2
13271479,[C++] AdaptiveIntBuilder::length() does not consider pending_pos_.,"{code:c++}
arrow::AdaptiveIntBuilder builder(arrow::default_memory_pool());
builder.Append(1);
std::cout << builder.length() << std::endl;
{code}

Expected output: {{1}}
Actual output: {{0}}

I imagine this regression came with https://github.com/apache/arrow/pull/3040

My use case: I'm building a JSON parser that appends ""records"" (JSON Objects mapping key=>value) to Arrow columns (each key gets an ArrayBuilder). Not all JSON Objects contain all keys; so {{builder.Append()}} isn't always called. So on a subsequent row, I want to add nulls for every append that was skipped: {{builder.AppendNulls(row - builder.length()); builder.Append(value)}}. This fails because {{builder.length()}} is wrong.

Annoying but simple workaround: I maintain a separate {{length}} value alongside {{builder}}.",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-29 22:43:19,2
13271362,[C++] Rename UnionArray::type_ids to UnionArray::type_codes,"This would be consistent with {{UnionType::type_codes}}. Furthermore, ""type_id"" already means something else in the C++ API, so it would be less confusing as well.",pull-request-available,['C++'],ARROW,Wish,Minor,2019-11-29 10:42:42,2
13271306,[Document] Add discussion about vector lifecycle,"As discussed in https://issues.apache.org/jira/browse/ARROW-7254?focusedCommentId=16983284&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16983284, we need a discussion about the lifecycle of a vector.

Each vector has a lifecycle, and different operations should be performed in particular phases of the lifecycle. If we violate this, some unexpected results may be produced. This may cause some confusion for Arrow users. So we want to add a new section to the prose document, to make it clear and explicit.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-11-29 02:53:34,7
13271223,[Python] Non-nullable null field is allowed / crashes when writing to parquet,"It seems to be possible to create a ""non-nullable null field"". While this does not make any sense (so already a reason to disallow this I think), this can also lead to crashed in further operations, such as writing to parquet:

{code}
In [18]: table = pa.table([pa.array([None, None], pa.null())], schema=pa.schema([pa.field('a', pa.null(), nullable=False)]))

In [19]: table
Out[19]:
pyarrow.Table
a: null not null

In [20]: pq.write_table(table, ""test_null.parquet"")
WARNING: Logging before InitGoogleLogging() is written to STDERR
F1128 14:08:30.267439 27560 column_writer.cc:837]  Check failed: (nullptr) != (values)
*** Check failure stack trace: ***
Aborted (core dumped)
{code}",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-11-28 13:12:58,14
13271005,[C++][Flight] Use the single parameter version of SetTotalBytesLimit,"With the recent protobuf update on CF deprecated error is triggered during compilation.
See build error https://app.circleci.com/jobs/github/ursa-labs/crossbow/5418",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-27 13:42:40,3
13270913,[C++] Fix arrow::parquet compiler warning,"Encountered the compiler warning when building:

[WARNING:/arrow/cpp/src/parquet/parquet.thrift:297] The ""byte"" type is a compatibility alias for ""i8"". Use ""i8"" to emphasize the signedness of this type.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-11-27 08:23:04,14
13270733,[Python] dictionary_encode() of a slice gives wrong result,"Steps to reproduce:

{code:python}
import pyarrow as pa
arr = pa.array([""a"", ""b"", ""b"", ""b""])[1:]
arr.dictionary_encode()
{code}

Expected results:

{code}
-- dictionary:
  [
    ""b""
  ]
-- indices:
  [
    0,
    0,
    0
  ]
{code}

Actual results:

{code}
-- dictionary:
  [
    ""b"",
    """"
  ]
-- indices:
  [
    0,
    0,
    1
  ]
{code}

I don't know a workaround. Converting to pylist and back is too slow. Is there a way to copy the slice to a new offset-0 StringArray that I could then dictionary-encode? Otherwise, I'm considering building buffers by hand....",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-11-26 14:34:06,2
13270676,[Java] RangeEqualsVisitor type check is not correct,"Currently {{RangeEqualsVisitor}}generally only checks type once and keep the result to avoid repeated type checking, see
{code:java}
typeCompareResult = left.getField().getType().equals(right.getField().getType());
{code}
This only compares {{ArrowType}} and for complex type, this may cause unexpected behavior, for example {{List<Int>}}and {{List<BigInt>}}would be type equals which not consider their child field.

We should compare Field here instead and to make it more extendable, we use {{TypeEqualsVisitor}}to compare Field, in this way, one could choose whether checks names or metadata either.



Also provide a test for ListVector to validate this change.",pull-request-available,['Java'],ARROW,Bug,Major,2019-11-26 09:57:30,16
13270661,[Python] Python support for fixed size list type,"I didn't see any issue about this, but {{FixedSizeListArray}} (ARROW-1280) is not yet exposed in Python.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-11-26 08:33:36,5
13270659,[CI] Ubuntu 14.04 test is failed by user defined literal,"https://circleci.com/gh/ursa-labs/crossbow/5329

{noformat}
  /arrow/cpp/src/arrow/dataset/filter_test.cc:80:194: error: invalid suffix on literal; C++11 requires a space between literal and identifier [-Werror=literal-suffix]
     ASSERT_EQ(""a""_.ToString(), ""a"");
                  ^
{noformat}",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-11-26 08:20:06,1
13270637,[Java] Support subfield encoder use different hasher,"Currently {{ListSubFieldEncoder/StructSubFieldEncoder}}use default hasher for calculating hashCode.

This issue enables them to use different hasher or even user-defined hasher for their own use cases just like {{DictionaryEncoder}}does.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-11-26 06:48:18,16
13270588,[CI] Fuzzit job is failed by nonexistent directory,"https://circleci.com/gh/ursa-labs/crossbow/5320

{noformat}
+ source_dir=/arrow/cpp
+ build_dir=/arrow/cpp/build
+ pushd /arrow/cpp/build/relwithdebinfo
/arrow/ci/scripts/fuzzit_build.sh: line 26: pushd: /arrow/cpp/build/relwithdebinfo: No such file or directory
{noformat}",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-11-26 03:18:32,1
13270584,[CI] Homebrew formula is failed by openssl formula name update,"https://travis-ci.org/ursa-labs/crossbow/builds/616575964#L3501

{noformat}
Error: 2 problems in 1 formula detected
apache-arrow:
  * Dependency 'openssl' is an alias; use the canonical name 'openssl@1.1'.
The command ""brew audit $ARROW_FORMULA"" failed and exited with 1 during .
{noformat}",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-11-26 02:37:36,1
13270582,[C++] Remove ARROW_MEMORY_POOL_DEFAULT macro,"As mentioned elsewhere in a JIRA I recall, we aren't testing adequately the CMake option for ""no default memory pool"", so it would either be better to require explicit memory pools or pass the default, rather than having a build-time option to set whether a default will be passed",pull-request-available,['C++'],ARROW,Improvement,Major,2019-11-26 02:22:29,3
13270539,[CI] Run source release test on pull request,"If we don't run source release test on pull request, it's difficult that we detect source release test failure on pull request.

For example,
https://github.com/apache/arrow/pull/5852 introduced source release test failure but we can't detect it on pull request.
Then
https://github.com/apache/arrow/pull/5889 tried to fix the source release test failure and merged but it didn't fix the failure. We couldn't detect it because we didn't run source release test on pull request.
Then
https://github.com/apache/arrow/pull/5893 will really fix the source release test failure.

Why did we not run source release test?

https://github.com/apache/arrow/pull/5889#issuecomment-558096239

> The reason why I've removed them from running on each PR to reduce the number of the triggered builds. See the mailing list thread about the queueing concerns. The docs build takes fairly long time, in contrary of the source release test.

The mailing list discussion:
[CI] Docker-compose refactor and GitHub Actions
https://lists.apache.org/thread.html/8fefdebd76742fe4ffb2ee7d0fa88ddf2b3229c55935c146b0d97e52@%3Cdev.arrow.apache.org%3E",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-11-25 20:52:50,1
13270528,BaseVariableWidthVector#setSafe appears to make value offsets inconsistent,"The following program writes a file which PyArrow either segfaults (0.14.1) or rejects with an error (0.15.1) {{pyarrow.lib.ArrowInvalid: Column 0: Offset invariant failure at: 2 inconsistent value_offsets for null slot0!=4}} on reading.

Calling {{setRowCount}} again, or calling {{setSafe}} with a higher index fixes it. While it seems from the new documentation that we should (must?) call {{VectorSchemaRoot#setRowCount}} at the end, I wouldn't have expected to get an invalid file by calling using {{setSafe}}, either. 

Full traceback:
{noformat}
> python3 -c 'import pyarrow as pa; print(pa.ipc.open_stream(open(""./test.bin"", ""rb"")).read_pandas())'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/lidavidm/Flight/arrow-5137-auth/java/venv/lib/python3.7/site-packages/pyarrow/ipc.py"", line 46, in read_pandas
    table = self.read_all()
  File ""pyarrow/ipc.pxi"", line 330, in pyarrow.lib._CRecordBatchReader.read_all
  File ""pyarrow/public-api.pxi"", line 321, in pyarrow.lib.pyarrow_wrap_table
  File ""pyarrow/error.pxi"", line 78, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column 0: Offset invariant failure at: 2 inconsistent value_offsets for null slot0!=4
{noformat}

Full program:
{code:java}
import java.io.OutputStream;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.Collections;
import org.apache.arrow.memory.BufferAllocator;
import org.apache.arrow.memory.RootAllocator;
import org.apache.arrow.vector.VarCharVector;
import org.apache.arrow.vector.VectorSchemaRoot;
import org.apache.arrow.vector.ipc.ArrowStreamWriter;
import org.apache.arrow.vector.types.pojo.ArrowType;
import org.apache.arrow.vector.types.pojo.Field;
import org.apache.arrow.vector.types.pojo.Schema;

public class AsdfTest {

  public static void main(String[] args) throws Exception {
    Schema schema = new Schema(Collections.singletonList(Field.nullable(""a"", new ArrowType.Utf8())));

    try (BufferAllocator allocator = new RootAllocator(Integer.MAX_VALUE);
        VectorSchemaRoot root = VectorSchemaRoot.create(schema, allocator)) {
      root.setRowCount(2);
      VarCharVector v = (VarCharVector) root.getVector(""a"");
      v.setSafe(0, ""asdf"".getBytes(StandardCharsets.UTF_8));
      try (OutputStream output = Files.newOutputStream(Paths.get(""./test.bin""))) {
        ArrowStreamWriter writer = new ArrowStreamWriter(root, null, output);
        writer.writeBatch();
        writer.close();
      }
    }
  }
}
{code}

{{v.setNull(1)}} after {{v.setSafe(0, ""asdf"")}} does not fix it. Using {{set}} instead of {{setSafe}} will fail in Java.
",pull-request-available,['Java'],ARROW,Bug,Minor,2019-11-25 19:29:33,7
13270521,[CI] Fix master failure with release test,Fix master failure with release test,pull-request-available,['CI'],ARROW,Bug,Major,2019-11-25 18:29:08,10
13270374,[C++] Undefined symbols for StringToFloatConverter::Impl with clang 4.x,"{code:java}
Undefined symbols for architecture x86_64:
  ""arrow::internal::StringToFloatConverter::Impl::main_junk_value_"", referenced from:
      arrow::internal::StringToFloatConverter::StringToFloat(char const*, unsigned long, float*) in libarrow.a(parsing.cc.o)
      arrow::internal::StringToFloatConverter::StringToFloat(char const*, unsigned long, double*) in libarrow.a(parsing.cc.o)
  ""arrow::internal::StringToFloatConverter::Impl::fallback_junk_value_"", referenced from:
      arrow::internal::StringToFloatConverter::StringToFloat(char const*, unsigned long, float*) in libarrow.a(parsing.cc.o)
      arrow::internal::StringToFloatConverter::StringToFloat(char const*, unsigned long, double*) in libarrow.a(parsing.cc.o)
ld: symbol(s) not found for architecture x86_64{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-25 07:07:50,8
13270319,[CI] Release test fails in master due to new arrow-flight Rust crate,See https://github.com/apache/arrow/runs/318192961,pull-request-available,['CI'],ARROW,Bug,Major,2019-11-24 17:18:09,10
13270209,[CI][Python] wheel can't be built by wget and OpenSSL error,"{noformat}
dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib
  Referenced from: /usr/local/bin/wget
  Reason: image not found
{noformat}",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2019-11-23 02:31:08,1
13270188,[CI][Python] wheel can't be built by SSL_ST_INIT error,"{noformat}
$ docker-compose pull $BUILD_IMAGE
Traceback (most recent call last):
  File ""/usr/local/bin/docker-compose"", line 6, in <module>
    from compose.cli.main import main
  File ""/usr/local/lib/python2.7/dist-packages/compose/cli/main.py"", line 18, in <module>
    import docker
  File ""/usr/local/lib/python2.7/dist-packages/docker/__init__.py"", line 2, in <module>
    from .api import APIClient
  File ""/usr/local/lib/python2.7/dist-packages/docker/api/__init__.py"", line 2, in <module>
    from .client import APIClient
  File ""/usr/local/lib/python2.7/dist-packages/docker/api/client.py"", line 5, in <module>
    import requests
  File ""/usr/local/lib/python2.7/dist-packages/requests/__init__.py"", line 95, in <module>
    from urllib3.contrib import pyopenssl
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py"", line 46, in <module>
    import OpenSSL.SSL
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/__init__.py"", line 8, in <module>
    from OpenSSL import rand, crypto, SSL
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"", line 118, in <module>
    SSL_ST_INIT = _lib.SSL_ST_INIT
AttributeError: 'module' object has no attribute 'SSL_ST_INIT'
{noformat}",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2019-11-22 22:46:49,1
13270022,[C++] Add Result<T> APIs to IPC module,src/arrow/ipc,pull-request-available,['C++'],ARROW,Sub-task,Major,2019-11-22 05:56:54,14
13269605,[CI][Python] Install pickle5 in the conda-python docker image for python version 3.6,See conversation https://github.com/apache/arrow/pull/5873#discussion_r348510729,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2019-11-20 14:44:48,2
13269595,[Python] Conversion from boolean numpy scalars not working,"In general, we are fine to accept a list of numpy scalars:

{code}
In [12]: type(list(np.array([1, 2]))[0])                                                                                                                                                                           
Out[12]: numpy.int64

In [13]: pa.array(list(np.array([1, 2])))                                                                                                                                                                          
Out[13]: 
<pyarrow.lib.Int64Array object at 0x7f51a493e2e8>
[
  1,
  2
]
{code}

But for booleans, this doesn't work:

{code}
In [14]: pa.array(list(np.array([True, False])))                                                                                                                                                                   
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-14-0d82cdce82e8> in <module>
----> 1 pa.array(list(np.array([True, False])))

~/scipy/repos/arrow/python/pyarrow/array.pxi in pyarrow.lib.array()

~/scipy/repos/arrow/python/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()

~/scipy/repos/arrow/python/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()

ArrowInvalid: Could not convert True with type numpy.bool_: tried to convert to boolean
{code}",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-11-20 14:00:25,3
13269566,ARROW-7217: [CI][Python] Use correct python version in Github Actions,"The ""AMD64 Conda Python 2.7"" build is actually using Python 3.6. 

This python 3.6 version is written in the conda-python.dockerfile: https://github.com/apache/arrow/blob/master/ci/docker/conda-python.dockerfile#L24 
and I am not fully sure how the ENV variable overrides that or not

cc [~kszucs]
",pull-request-available,"['CI', 'Python']",ARROW,Test,Major,2019-11-20 10:38:41,3
13269564,[Java] Improve the performance of setting/clearing individual bits,"Setting/clearing individual bits are key operations for Arrow. In this issue, we improve the performance these operations by:

1. replacing arithmetic operations with bit-wise operations
2. remove unnecessary casts between int/byte
3. provide new API to remove the if branch

Benchmark results show that for clearing a bit, the performance improve by 11%, and for general set/clear operation, the performance improve by 4.7%:

before:
BitVectorHelperBenchmarks.setValidityBitBenchmark        avgt    5  4.524  0.015  us/op

after:
BitVectorHelperBenchmarks.setValidityBitBenchmark        avgt    5  4.313  0.011  us/op
BitVectorHelperBenchmarks.setValidityBitToZeroBenchmark  avgt    5  4.020  0.016  us/op

",pull-request-available,['Java'],ARROW,Improvement,Major,2019-11-20 10:28:39,7
13269525,[Python] unpickling a pyarrow table with dictionary fields crashes,"The following code crashes on this check:
{code:java}
F1120 07:51:37.523720 12432 array.cc:773]  Check failed: (data->dictionary) != (nullptr) 
{code}

{code:java}
import cPickle as pickle
import pandas as pd
import pyarrow as pa

df = pd.DataFrame([{""cat"": ""a"", ""val"":1},{""cat"": ""b"", ""val"":2} ])
df[""cat""] = df[""cat""].astype('category')index_table = pa.Table.from_pandas(df, preserve_index=False)

with open('/tmp/zz.pickle', 'wb') as f:
    pickle.dump(index_table, f, protocol=2)

with open('/tmp/zz.pickle', 'rb') as f:
   index_table = pickle.load(f)
{code}


Used Python2 with the following environment:
{code:java}
Package         Version
--------------- -------
enum34          1.1.6  
futures         3.3.0  
numpy           1.16.5 
pandas          0.24.2 
pip             19.3.1 
pyarrow         0.14.1 (0.14.0 and up suffer from this issue)
python-dateutil 2.8.1  
pytz            2019.3 
setuptools      41.6.0 
six             1.13.0 
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-11-20 07:53:10,5
13269509,[Java] Represent a data element of a vector as a tree of ArrowBufPointer,"For a fixed/variable width vector, each of its data element can be represented as an ArrowBufPointer object, which represents a contiguous memory segment. This makes many tasks easier and more efficient (without memory copy): calculating hash code, comparing values, etc.

This cannot be achieved for complex vectors, because their values often reside in more than one contiguous memory regions. However, it can be seen that the contiguous memory regions for each data element forms a tree-like structure, whose leaf nodes are the contiguous memory regions. For example, a data element for a struct vector forms a tree, whose root corresponds to the struct vector, while the child vectors corresponds to the child nodes of the tree root. 

In this issue, we provide a data structure that represents each data element of a vector as a tree, whose leaf nodes are ArrowBufPointers, representing contiguous memory regions for the data element. 

With this data structure, many tasks also becomes easier and more efficient: calculating hash code, comparing vector elements (ordering & equality). In addition, we can do something that could not have been done in the past, like placing data elements into a hash table/hash set, etc. 
",pull-request-available,['Java'],ARROW,New Feature,Major,2019-11-20 05:54:41,7
13269365,[C++] Scalar cast should support time-based types,This would allow supporting a minimum of expression evaluation on time-based arrays.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-11-19 18:10:14,13
13269346,[Python] tests with pandas master are failing now __from_arrow__ support landed in pandas,"I implemented pandas <-> arrow roundtrip for pandas' integer+string dtype in https://github.com/pandas-dev/pandas/pull/29483, which is now merged. But our tests where assuming this did not yet work in pandas, and thus need to be updated.",pull-request-available,['Python'],ARROW,Test,Major,2019-11-19 16:23:49,5
13269344,[Python] Passing directory to ParquetFile class gives confusing error message,"Somehow have the same errors. We are working with pyarrow 0.15.1, trying to access a folder of `parquet` files generated with Amazon Athena.

```python
table2 = pq.read_table('C:/Data/test-parquet')
```

works fine in contrast to

```python
parquet_file = pq.ParquetFile('C:/Data/test-parquet')
# parquet_file.read_row_group(0)
```

which raises

`ArrowIOError: Failed to open local file 'C:/Data/test-parquet', error: Access is denied.`",parquet pull-request-available,['Python'],ARROW,Bug,Major,2019-11-19 16:16:47,14
13269321,[Rust] Update Generated Flatbuffer Files,"We last built the fbs files early in the year, and since then there have been some changes like LargeLists. We should update the generated Rust files to incorporate these changes",pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-11-19 14:14:31,12
13269201,[C++][Dataset] In expression should not require exact type match,Similar to ARROW-7047. I encountered this on ARROW-7185 (https://github.com/apache/arrow/pull/5858/files#diff-1d8a97ca966e8446ef2ae4b7b5a96ed1R125),dataset pull-request-available,['C++'],ARROW,Improvement,Critical,2019-11-19 00:01:14,6
13268884,[Rust] Implement Flight crate,Implement a flight crate that can run a gRPC server that understands the flight protocol. This will be a library and the user will need to provide implementations of one or more traits to plug in their logic.,pull-request-available,['Rust'],ARROW,New Feature,Major,2019-11-16 22:44:54,10
13268850,[C++][Doc] doxygen broken on master: missing param implicit_casts,https://circleci.com/gh/ursa-labs/crossbow/4991,dataset,"['C++', 'Documentation']",ARROW,Bug,Minor,2019-11-16 16:37:31,13
13268763,[C++][Doc] doxygen broken on master because of @,See https://github.com/apache/arrow/runs/305621760#step:5:985,dataset pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Minor,2019-11-16 01:22:01,4
13268630,[CI][Crossbow] Re-skip r-sanitizer nightly tests,They're broken (ARROW-6957) and were skipped (ARROW-7034) but that skipping must have been lost in the docker-compose refactor.,pull-request-available,['Continuous Integration'],ARROW,Bug,Minor,2019-11-15 17:12:36,4
13268623,[Python][Nightly] Wheel builds could NOT find ArrowPython,"See for example https://travis-ci.org/ursa-labs/crossbow/builds/612285565#L1088. I wonder if ARROW-5575 (https://github.com/apache/arrow/pull/5798) disrupted this?

[~kou] ",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Critical,2019-11-15 16:59:09,1
13268606,[C++][Compute] Array support for fill_null,"Add kernels to support which replacing null values in an array with values taken from corresponding slots in another array:

{code}
fill_null([1, null, null, 3], [5, 6, null, 8]) -> [1, 6, null, 3]
{code}",analytics,['C++'],ARROW,Improvement,Major,2019-11-15 16:04:00,6
13268569,[Java] Provide a utility to improve the performance of vector loading/unloading,"Vector loading/unloading transforms a set of vectors to and from a set of buffers with meta data. It is heavily used in flight/IPC.

In the loading/unloading operations, only the number of type buffers are really needed. However, the current code logic gets a copy of the type buffers, which is not necessary.

In this issue, we provide a utility to get the number of type buffers, given an arrow type. It improves the performance because it removes the following overhead:

1. creating type buffer objects unnecessarily.
2. creating a list and copying list contents (in TypeLayout#getBufferTypes) for vector unloading.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-11-15 12:58:44,7
13268477,[Integration] Add test to verify Map field names can be arbitrary,"A Map has child fields and the format spec only recommends that they be named ""entries"", ""key"", and ""value"" but could be named anything. Currently, integration tests for Map arrays verify the exchanged schema is equal, so the child fields are always named the same. There should be tests that use different names to verify implementations can accept this.",pull-request-available,['Integration'],ARROW,Test,Major,2019-11-15 05:51:27,2
13268389,[C++][Dataset] Improve format of Expression::ToString,"Instead of {{GREATER(FIELD(b), SCALAR<int32>(3))}}, these could just read {{""b""_ > int32(3)}}",dataset pull-request-available,['C++'],ARROW,Improvement,Minor,2019-11-14 17:51:00,6
13268346,[C++] Bundled ORC fails linking,"This shows up when building the tests as well:
{code}
[1/2] Linking CXX executable debug/orc-adapter-test
FAILED: debug/orc-adapter-test 
: && /usr/bin/ccache /usr/bin/clang++-7  -Qunused-arguments -fcolor-diagnostics -fuse-ld=gold -ggdb -O0  -Wall -Wextra -Wdocumentation         -Wno-unused-parameter -Wno-unknown-warning-option -Werror -Wno-unknown-warning-option -msse4.2 -maltivec  -D_GLIBCXX_USE_CXX11_ABI=1 -D_GLIBCXX_USE_CXX11_ABI=1 -fno-omit-frame-pointer -g  -rdynamic src/arrow/adapters/orc/CMakeFiles/orc-adapter-test.dir/adapter_test.cc.o  -o debug/orc-adapter-test  -Wl,-rpath,/home/antoine/arrow/dev/cpp/build-test/debug:/home/antoine/miniconda3/envs/pyarrow/lib /home/antoine/miniconda3/envs/pyarrow/lib/libgtest_main.so /home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so -lpthread -ldl debug/libarrow_testing.so.100.0.0 debug/libarrow.so.100.0.0 orc_ep-install/lib/liborc.a /home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so -ldl double-conversion_ep/src/double-conversion_ep/lib/libdouble-conversion.a /home/antoine/miniconda3/envs/pyarrow/lib/libssl.so /home/antoine/miniconda3/envs/pyarrow/lib/libcrypto.so /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlienc-static.a /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlidec-static.a /home/antoine/miniconda3/envs/pyarrow/lib/libbrotlicommon-static.a /home/antoine/miniconda3/envs/pyarrow/lib/libprotobuf.so /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-config.so /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-transfer.so /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-s3.so /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-core.so /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-event-stream.so.1.0.0 /home/antoine/miniconda3/envs/pyarrow/lib/libaws-c-common.so.1.0.0 -lm -lpthread /home/antoine/miniconda3/envs/pyarrow/lib/libaws-checksums.so jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a mimalloc_ep/src/mimalloc_ep/lib/mimalloc-1.0/libmimalloc-debug.a -pthread -lrt -Wl,-rpath-link,/home/antoine/miniconda3/envs/pyarrow/lib && :
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:284: error: undefined reference to 'deflateInit2_'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:232: error: undefined reference to 'deflateReset'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:254: error: undefined reference to 'deflate'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:291: error: undefined reference to 'deflateEnd'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:405: error: undefined reference to 'inflateInit2_'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:430: error: undefined reference to 'inflateEnd'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:471: error: undefined reference to 'inflateReset'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:477: error: undefined reference to 'inflate'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:820: error: undefined reference to 'snappy::GetUncompressedLength(char const*, unsigned long, unsigned long*)'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:828: error: undefined reference to 'snappy::RawUncompress(char const*, unsigned long, char*)'
/home/antoine/arrow/dev/cpp/build-test/orc_ep-prefix/src/orc_ep/c++/src/Compression.cc:894: error: undefined reference to 'LZ4_decompress_safe'
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-14 15:08:48,2
13268328,[C++] Vendor uriparser library,"The [uriparser C library|https://github.com/uriparser/uriparser]  is used internally for URI parsing. Instead of having an explicit dependency, we could simply vendor it.",pull-request-available,['C++'],ARROW,Wish,Major,2019-11-14 14:20:15,2
13268293,[Python] pa.array() doesn't respect specified dictionary type,"This might be related toARROW-6548 and others dealing with all NaN columns. When creating a dictionary array, even when fully specifying the desired type, this type is not respected when the data contains only NaNs:


{code:python}
# This may look a little artificial but easily occurs when processing categorial data in batches and a particular batch containing only NaNs
ser = pd.Series([None, None]).astype('object').astype('category')
typ = pa.dictionary(index_type=pa.int8(), value_type=pa.string(), ordered=False)
pa.array(ser, type=typ).type
{code}

results in

{noformat}
>> DictionaryType(dictionary<values=null, indices=int8, ordered=0>)
{noformat}

which means that one cannot e.g. serialize batches of categoricals if the possibility of all-NaN batches exists, even when trying to enforce that each batch has the same schema (because the schema is not respected).

I understand that inferring the type in this case would be difficult, but I'd imagine that a fully specified type should be respected in this case?

In the meantime, is there a workaround to manually create a dictionary array of the desired type containing only NaNs?
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-11-14 12:21:15,5
13268206,[Java] Remove redundant code for Jdbc adapters,"As discussed in https://github.com/apache/arrow/pull/5508#issuecomment-543011016, we need a separate issue to extract common logic to a common super class. 

This makes the code clearer, and we need to make sure we have no performance regression. 
",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-11-14 06:36:33,7
13268194,[CI] Dev cron github action is failing every 15 minutes,See https://github.com/apache/arrow/actions?query=workflow%3A%22Dev+Cron%22,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-11-14 04:23:24,4
13268102,[C++] Cleanup warnings in cmake_modules/SetupCxxFlags.cmake,For clang we currently disable a lot of warnings explicitly. This dates back to when we enabled {{-Weverything}}. We should probably remove most or all of these flags now.,pull-request-available,"['C++', 'Developer Tools']",ARROW,Wish,Minor,2019-11-13 18:18:15,2
13268081,[C++] Update string_view backport,"This will include the following changeset:
https://github.com/martinmoene/string-view-lite/commit/d96175999ebc81b08f027f7e6e2074f77efba4e0
",pull-request-available,['C++'],ARROW,Task,Minor,2019-11-13 16:33:16,2
13268059,[CI] Run HDFS tests as cron task,Probably simply by using the {{conda-python-hdfs}} docker-compose entry.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2019-11-13 15:32:06,2
13268053,[C++][Visual Studio]Build config Error on non English Version visual studio.,"* Build Config Error on Non English OS
 * always show

{code:java}
 Not supported MSVC compiler  {code}
 * [https://github.com/apache/arrow/blob/master/cpp/cmake_modules/CompilerInfo.cmake#L44]

There is a bug in the code below.

{code:java}
if(MSVC)
 set(COMPILER_FAMILY ""msvc"")
 if(""${COMPILER_VERSION_FULL}"" MATCHES
 "".*Microsoft ?\\(R\\) C/C\\+\\+ Optimizing Compiler Version 19.*x64""){code}

 * In my compiler the version display contains Korean.

{code:java}
Microsoft (R) C/C++    19.00.24215.1(x64){code}

 * Regular expression seems to need to be changed.",pull-request-available,['C++'],ARROW,Bug,Minor,2019-11-13 15:21:54,1
13268052,"[R] Add validation, helpful error message to Object$new()","I have a 30 gig arrow file - using record batch reader crashes RStudio

arrow::RecordBatchFileReader$new(""file.arrow"")",pull-request-available,['R'],ARROW,Bug,Blocker,2019-11-13 15:17:14,4
13267891,[Java] Delete useless class DiffFunction,"{{DiffFunction}} was used in the initial implementation of visitors, since currently visitors logic has been refactored and this class no longer useful.

",pull-request-available,['Java'],ARROW,Bug,Major,2019-11-13 02:54:29,16
13267777,[C++] Remove experimental status on filesystem APIs,At some point we should declare that the C++ filesystem APIs (or some of them) are not experimental anymore.,pull-request-available,['C++'],ARROW,Task,Major,2019-11-12 16:46:57,2
13267769,[C++][Dataset] Refactor dataset's API to use Result<T>,We should make this switch before the API settles,dataset,['C++'],ARROW,Sub-task,Major,2019-11-12 16:23:36,13
13267734,[R][CI] Various fixes and speedups for the R docker-compose setup,"Was ""Pre-install the R dependencies in the dockerfile"" but there were a number of things that needed fixing/that got lost in the move from Travis-CI.

Here: https://github.com/apache/arrow/blob/master/ci/docker/linux-apt-r.dockerfile#L61
Using the https://github.com/apache/arrow/blob/master/ci/scripts/r_deps.sh script",pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2019-11-12 14:54:05,4
13267721,[C++] Compile error with GCC 5.4.0 ,"If I recall correctly it was template related.

Reproduce it with:
export UBUNTU=16.04 
docker-compose build ubuntu-cpp
docker-compose run --rm ubuntu-cpp

cc [~fsaintjacques]",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-12 13:46:04,6
13267705,[Rust][CI] Pre-install the rust dependencies in the dockerfile,"I'd be nice to pre-install the rust dependencies similarly like we does with go, see the comment https://github.com/apache/arrow/blob/master/ci/docker/debian-10-rust.dockerfile#L41-L44",pull-request-available,"['CI', 'Rust']",ARROW,Improvement,Major,2019-11-12 13:26:36,9
13267699,[CI] Allow GH Actions to run on all branches,"When working from feature branches, it is good to have CI results as well (especially from a fork, before submitting a PR).",pull-request-available,"['Continuous Integration', 'Developer Tools']",ARROW,Bug,Major,2019-11-12 13:09:06,2
13267691,[CI] Fedora cron jobs are failing because of wrong fedora version,The requested fedora version is 10 (Debian) instead of 29: https://github.com/apache/arrow/runs/299223601,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-11-12 12:19:09,3
13267676,[C++][CI] Use scripts/util_coredump.sh to show automatic backtraces,"The script was previously used on Travis, we should enable it in docker and on GitHub actions to speed up the debugging process.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-11-12 11:32:55,3
13267670,[C++][CI] Fix the hanging C++ tests in Windows 2019,"C++ tests hang on windows-latest GitHub actions host, so the tests are disabled.



We should fix it and re-enable the testshttps://github.com/apache/arrow/blob/master/.github/workflows/cpp.yml#L171",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-11-12 11:16:31,2
13267669,[CI] Use the docker repository provided by apache organisation,"This is required to push the outdated, but rebuilt images from GitHub actions for reuse.",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2019-11-12 11:14:05,3
13267390,[Java] Fix the problem that flight perf test hangs endlessly,"Flight performance test (org.apache.arrow.flight.perf.TestPerf) is an important tool for tracking the current throughput of IPC. In this issue, we improve it in two ways:

1. We fix the problem that the test hangs endlessly after all runs have been finished. This is because the thread pool is not released.

2. We add a summary to the output report, so that we can easily evaluate the overall results for all runs. 
",pull-request-available,['Java'],ARROW,Bug,Minor,2019-11-11 02:45:32,7
13267373,[CI][Crossbow] Nightly homebrew-cpp job fails,"Not clear (to me) which is the real error, but https://travis-ci.org/ursa-labs/crossbow/builds/609138711#L6672 seems to implicate aws-sdk-cpp, which was upgraded on homebrew-core a couple of days ago: https://github.com/Homebrew/homebrew-core/commits/master/Formula/aws-sdk-cpp.rb.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2019-11-10 21:52:47,4
13267209,[R] Various minor cleanups,"Removes some detritus following ARROW-6743 and ARROW-7067, as well as a few other things that have come up.",pull-request-available,['R'],ARROW,New Feature,Minor,2019-11-08 23:04:07,4
13267025,[Java] Improve the performance of comparing two memory blocks,"We often use the 8-4-1 paradigm to compare two blocks of memory:
1. First compare by 8-byte blocks in a loop
2. Then compare by 4-byte blocks in a loop
3. Last compare by 1-byte blocks in a loop

It can be proved that the second loop runs at most once. So we can replace the loop with a if statement, which will save us a comparison and two jump operations. 

According to the discussion in https://github.com/apache/arrow/pull/5508#discussion_r343973982, loop can be expensive. ",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-11-08 04:58:13,7
13267007,[Rust][CI] Builds failing due to rust nightly formatting,see e.g. https://github.com/apache/arrow/runs/293573608 on master,pull-request-available,['Rust'],ARROW,Bug,Major,2019-11-08 02:55:51,13
13266974,[R] Better handling of unsupported filter and mutate expressions in dplyr methods,Followup to ARROW-6340 and ARROW-7186. See if there's a safer way to defer evaluation that may allow for less data to be pulled down to R to filter after. Likewise for mutate.,pull-request-available,['R'],ARROW,New Feature,Major,2019-11-07 23:55:34,4
13266973,[C++] FileSystemDataSource should use an owning pointer for fs::Filesystem,Followup to ARROW-6340,dataset,"['C++', 'R']",ARROW,Sub-task,Major,2019-11-07 23:52:54,13
13266969,[R] Add vignette for dplyr and datasets,Followup to ARROW-6340,pull-request-available,['R'],ARROW,New Feature,Critical,2019-11-07 23:47:11,4
13266893,[C++] Move all factories to type_fwd.h,"There's no particular reason why parameter-less factories are in {{type_fwd.h}}, but the others in their respective implementation headers. By putting more factories in {{type_fwd.h}}, we may be able to avoid importing the heavier headers in some places.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-11-07 18:49:24,6
13266767,[C++]  ArrayRangeEquals should check for full type equality?,"It looks like ArrayRangeEquals in compare.cc only checks type IDs before doing comparison actual values. This is inconsistent with ArrayEquals which checks for type equality and also seems incorrect for cases like Decimal128.



I presume this was an oversight when fixingARROW-2567 but maybe it was intentional?

[~uwe]?",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-07 09:06:33,2
13266653,[Python][Parquet][C++] Expose parquet field_id in Schema objects,"I'm in the process of adding parquet read support to Iceberg([https://iceberg.apache.org/]), and we use the parquet field_ids as a consistent id when reading a parquet file to create a map between the current schema and theschema of the file being read. Unless I've missed something, it appears that field_id is not exposed in the python APIs in pyarrow._parquet.ParquetSchema nor is it available in pyarrow.lib.Schema.

Would it be possible to add this to either of those two objects?",parquet pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2019-11-06 18:50:52,14
13266615,[C++] Unsupported Dict->T cast crashes instead of returning error,"{code:python}
>>> arr = pa.array([""foo"", ""bar""])                                                                                                                                    
>>> arr.dictionary_encode().cast('binary')                                                                                                                            
F1106 16:53:22.411165 28503 cast.cc:919]  Check failed: values_type.Equals(*output->type) Dictionary type: string target type: binary
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2019-11-06 15:54:15,2
13266565,[C++] ASSERT_OK_AND_ASSIGN crashes when failing,"Instead of simply failing the test, the {{ASSERT_OK_AND_ASSIGN}} macro crashes when the operation failed, e.g.:
{code}
Value of: _st.ok()
  Actual: false
Expected: true
WARNING: Logging before InitGoogleLogging() is written to STDERR
F1106 12:53:32.882110  4698 result.cc:28] ValueOrDie called on an error:  XXX
{code}
",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2019-11-06 11:55:19,13
13266526,[Java] Support concating vectors values in batch,"We need a way to copy vector values in batch. Currently, we have copyFrom and copyFromSafe APIs. However, they are not enough, as copying values individually is not performant. ",pull-request-available,['Java'],ARROW,New Feature,Major,2019-11-06 08:52:32,7
13266525,[Java] Support concating validity bits efficiently,"For scenarios when we need to concate vectors (like the scenario in ARROW-7048, and delta dictionary), we need a way to concat validity bits. 

Currently, we have bit level API to read/write individual validity bit. However, it is not efficient , and we need a way to copy more bits at a time. ",pull-request-available,['Java'],ARROW,New Feature,Major,2019-11-06 08:48:07,7
13266402,[C++][Dataset] Replace ConstantPartitionScheme with PrefixDictionaryPartitionScheme,"ConstantPartitionScheme is not very useful, it'd be better to provide a dictionary of prefixes which maps to provided partition expressions",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-11-05 19:05:24,6
13266391,[C++] Expose the offsets of a ListArray as a Int32Array,"As follow-up on ARROW-7031 (https://github.com/apache/arrow/pull/5759), we can move this into C++ and use that implementation from Python.



Cfr [https://github.com/apache/arrow/pull/5759#discussion_r342244521,] this could be a \{{ListArray::value_offsets_array}}",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-11-05 17:54:44,14
13266359,[CI] Disable code coverage on Travis-CI,"Computing and uploading code coverage data probably makes Travis-CI builds even slower. Since the resulting data isn't currently exploited (and furthermore partial builds can upload bogus data, e.g. showing some C++ uncovered if only R is rebuilt), we should just disable it.",pull-request-available,"['C++', 'Continuous Integration', 'Python', 'R']",ARROW,Wish,Major,2019-11-05 14:46:42,2
13266336,[Python] support returning ChunkedArray from __arrow_array__ ?,"The {{\_\_arrow_array\_\_}} protocol was added so that custom objects can define how they should be converted to a pyarrow Array (similar to numpy's {{\_\_array\_\_}}). This is then also used to support converting pandas DataFrames with columns using pandas' ExtensionArrays to a pyarrow Table (if the pandas ExtensionArray, such as nullable integer type, implements this {{\_\_arrow_array\_\_}} method).

This last use case could also be useful for fletcher (https://github.com/xhochy/fletcher/, a package that implements pandas ExtensionArrays that wrap pyarrow arrays, so they can be stored as is in a pandas DataFrame).  
However, fletcher stores ChunkedArrays in ExtensionArry / the columns of a pandas DataFrame (to have a better mapping with a Table, where the columns also consist of chunked arrays). While we currently require that the return value of {{\_\_arrow_array\_\_}} is a pyarrow.Array.

So I was wondering: could we relax this constraint and also allow ChunkedArray as return value? 
However, this protocol is currently called in the {{pa.array(..)}} function, which probably should keep returning an Array (and not ChunkedArray in certain cases).

cc [~uwe]",pull-request-available,['Python'],ARROW,Improvement,Major,2019-11-05 12:52:58,5
13266214,[C++] Schema print method prints too much metadata,"I loaded some taxi data in a Dataset and printed the schema. This is what was printed:

{code}
vendor_id: string
pickup_at: timestamp[us]
dropoff_at: timestamp[us]
passenger_count: int8
trip_distance: float
pickup_longitude: float
pickup_latitude: float
rate_code_id: null
store_and_fwd_flag: string
dropoff_longitude: float
dropoff_latitude: float
payment_type: string
fare_amount: float
extra: float
mta_tax: float
tip_amount: float
tolls_amount: float
total_amount: float
-- metadata --
pandas: {""index_columns"": [{""kind"": ""range"", ""name"": null, ""start"": 0, ""stop"": 14387371, ""step"": 1}], ""column_indexes"": [{""name"": null, ""field_name"": null, ""pandas_type"": ""unicode"", ""numpy_type"": ""object"", ""metadata"": {""encoding"": ""UTF-8""}}], ""columns"": [{""name"": ""vendor_id"", ""field_name"": ""vendor_id"", ""pandas_type"": ""unicode"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": ""pickup_at"", ""field_name"": ""pickup_at"", ""pandas_type"": ""datetime"", ""numpy_type"": ""datetime64[ns]"", ""metadata"": null}, {""name"": ""dropoff_at"", ""field_name"": ""dropoff_at"", ""pandas_type"": ""datetime"", ""numpy_type"": ""datetime64[ns]"", ""metadata"": null}, {""name"": ""passenger_count"", ""field_name"": ""passenger_count"", ""pandas_type"": ""int8"", ""numpy_type"": ""int8"", ""metadata"": null}, {""name"": ""trip_distance"", ""field_name"": ""trip_distance"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""pickup_longitude"", ""field_name"": ""pickup_longitude"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""pickup_latitude"", ""field_name"": ""pickup_latitude"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""rate_code_id"", ""field_name"": ""rate_code_id"", ""pandas_type"": ""empty"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": ""store_and_fwd_flag"", ""field_name"": ""store_and_fwd_flag"", ""pandas_type"": ""unicode"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": ""dropoff_longitude"", ""field_name"": ""dropoff_longitude"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""dropoff_latitude"", ""field_name"": ""dropoff_latitude"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""payment_type"", ""field_name"": ""payment_type"", ""pandas_type"": ""unicode"", ""numpy_type"": ""object"", ""metadata"": null}, {""name"": ""fare_amount"", ""field_name"": ""fare_amount"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""extra"", ""field_name"": ""extra"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""mta_tax"", ""field_name"": ""mta_tax"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""tip_amount"", ""field_name"": ""tip_amount"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""tolls_amount"", ""field_name"": ""tolls_amount"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}, {""name"": ""total_amount"", ""field_name"": ""total_amount"", ""pandas_type"": ""float32"", ""numpy_type"": ""float32"", ""metadata"": null}], ""creator"": {""library"": ""pyarrow"", ""version"": ""0.15.1""}, ""pandas_version"": ""0.25.3""}
ARROW:schema: /////3gOAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABAwAQAAAAAAAKAAwAAAAEAAgACgAAAFQKAAAEAAAAAQAAAAwAAAAIAAwABAAIAAgAAAAsCgAABAAAAB8KAAB7ImluZGV4X2NvbHVtbnMiOiBbeyJraW5kIjogInJhbmdlIiwgIm5hbWUiOiBudWxsLCAic3RhcnQiOiAwLCAic3RvcCI6IDE0Mzg3MzcxLCAic3RlcCI6IDF9XSwgImNvbHVtbl9pbmRleGVzIjogW3sibmFtZSI6IG51bGwsICJmaWVsZF9uYW1lIjogbnVsbCwgInBhbmRhc190eXBlIjogInVuaWNvZGUiLCAibnVtcHlfdHlwZSI6ICJvYmplY3QiLCAibWV0YWRhdGEiOiB7ImVuY29kaW5nIjogIlVURi04In19XSwgImNvbHVtbnMiOiBbeyJuYW1lIjogInZlbmRvcl9pZCIsICJmaWVsZF9uYW1lIjogInZlbmRvcl9pZCIsICJwYW5kYXNfdHlwZSI6ICJ1bmljb2RlIiwgIm51bXB5X3R5cGUiOiAib2JqZWN0IiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJwaWNrdXBfYXQiLCAiZmllbGRfbmFtZSI6ICJwaWNrdXBfYXQiLCAicGFuZGFzX3R5cGUiOiAiZGF0ZXRpbWUiLCAibnVtcHlfdHlwZSI6ICJkYXRldGltZTY0W25zXSIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAiZHJvcG9mZl9hdCIsICJmaWVsZF9uYW1lIjogImRyb3BvZmZfYXQiLCAicGFuZGFzX3R5cGUiOiAiZGF0ZXRpbWUiLCAibnVtcHlfdHlwZSI6ICJkYXRldGltZTY0W25zXSIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAicGFzc2VuZ2VyX2NvdW50IiwgImZpZWxkX25hbWUiOiAicGFzc2VuZ2VyX2NvdW50IiwgInBhbmRhc190eXBlIjogImludDgiLCAibnVtcHlfdHlwZSI6ICJpbnQ4IiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJ0cmlwX2Rpc3RhbmNlIiwgImZpZWxkX25hbWUiOiAidHJpcF9kaXN0YW5jZSIsICJwYW5kYXNfdHlwZSI6ICJmbG9hdDMyIiwgIm51bXB5X3R5cGUiOiAiZmxvYXQzMiIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAicGlja3VwX2xvbmdpdHVkZSIsICJmaWVsZF9uYW1lIjogInBpY2t1cF9sb25naXR1ZGUiLCAicGFuZGFzX3R5cGUiOiAiZmxvYXQzMiIsICJudW1weV90eXBlIjogImZsb2F0MzIiLCAibWV0YWRhdGEiOiBudWxsfSwgeyJuYW1lIjogInBpY2t1cF9sYXRpdHVkZSIsICJmaWVsZF9uYW1lIjogInBpY2t1cF9sYXRpdHVkZSIsICJwYW5kYXNfdHlwZSI6ICJmbG9hdDMyIiwgIm51bXB5X3R5cGUiOiAiZmxvYXQzMiIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAicmF0ZV9jb2RlX2lkIiwgImZpZWxkX25hbWUiOiAicmF0ZV9jb2RlX2lkIiwgInBhbmRhc190eXBlIjogImVtcHR5IiwgIm51bXB5X3R5cGUiOiAib2JqZWN0IiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJzdG9yZV9hbmRfZndkX2ZsYWciLCAiZmllbGRfbmFtZSI6ICJzdG9yZV9hbmRfZndkX2ZsYWciLCAicGFuZGFzX3R5cGUiOiAidW5pY29kZSIsICJudW1weV90eXBlIjogIm9iamVjdCIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAiZHJvcG9mZl9sb25naXR1ZGUiLCAiZmllbGRfbmFtZSI6ICJkcm9wb2ZmX2xvbmdpdHVkZSIsICJwYW5kYXNfdHlwZSI6ICJmbG9hdDMyIiwgIm51bXB5X3R5cGUiOiAiZmxvYXQzMiIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAiZHJvcG9mZl9sYXRpdHVkZSIsICJmaWVsZF9uYW1lIjogImRyb3BvZmZfbGF0aXR1ZGUiLCAicGFuZGFzX3R5cGUiOiAiZmxvYXQzMiIsICJudW1weV90eXBlIjogImZsb2F0MzIiLCAibWV0YWRhdGEiOiBudWxsfSwgeyJuYW1lIjogInBheW1lbnRfdHlwZSIsICJmaWVsZF9uYW1lIjogInBheW1lbnRfdHlwZSIsICJwYW5kYXNfdHlwZSI6ICJ1bmljb2RlIiwgIm51bXB5X3R5cGUiOiAib2JqZWN0IiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJmYXJlX2Ftb3VudCIsICJmaWVsZF9uYW1lIjogImZhcmVfYW1vdW50IiwgInBhbmRhc190eXBlIjogImZsb2F0MzIiLCAibnVtcHlfdHlwZSI6ICJmbG9hdDMyIiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJleHRyYSIsICJmaWVsZF9uYW1lIjogImV4dHJhIiwgInBhbmRhc190eXBlIjogImZsb2F0MzIiLCAibnVtcHlfdHlwZSI6ICJmbG9hdDMyIiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJtdGFfdGF4IiwgImZpZWxkX25hbWUiOiAibXRhX3RheCIsICJwYW5kYXNfdHlwZSI6ICJmbG9hdDMyIiwgIm51bXB5X3R5cGUiOiAiZmxvYXQzMiIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAidGlwX2Ftb3VudCIsICJmaWVsZF9uYW1lIjogInRpcF9hbW91bnQiLCAicGFuZGFzX3R5cGUiOiAiZmxvYXQzMiIsICJudW1weV90eXBlIjogImZsb2F0MzIiLCAibWV0YWRhdGEiOiBudWxsfSwgeyJuYW1lIjogInRvbGxzX2Ftb3VudCIsICJmaWVsZF9uYW1lIjogInRvbGxzX2Ftb3VudCIsICJwYW5kYXNfdHlwZSI6ICJmbG9hdDMyIiwgIm51bXB5X3R5cGUiOiAiZmxvYXQzMiIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAidG90YWxfYW1vdW50IiwgImZpZWxkX25hbWUiOiAidG90YWxfYW1vdW50IiwgInBhbmRhc190eXBlIjogImZsb2F0MzIiLCAibnVtcHlfdHlwZSI6ICJmbG9hdDMyIiwgIm1ldGFkYXRhIjogbnVsbH1dLCAiY3JlYXRvciI6IHsibGlicmFyeSI6ICJweWFycm93IiwgInZlcnNpb24iOiAiMC4xNS4xIn0sICJwYW5kYXNfdmVyc2lvbiI6ICIwLjI1LjMifQAGAAAAcGFuZGFzAAASAAAAxAMAAHgDAABEAwAAAAMAAMgCAACMAgAAVAIAACACAADoAQAArAEAAHABAAA8AQAACAEAANgAAACoAAAAdAAAADwAAAAEAAAAlPz//wAAAQMYAAAADAAAAAQAAAAAAAAAyvz//wAAAQAMAAAAdG90YWxfYW1vdW50AAAAAMj8//8AAAEDGAAAAAwAAAAEAAAAAAAAAP78//8AAAEADAAAAHRvbGxzX2Ftb3VudAAAAAD8/P//AAABAxgAAAAMAAAABAAAAAAAAAAy/f//AAABAAoAAAB0aXBfYW1vdW50AAAs/f//AAABAxgAAAAMAAAABAAAAAAAAABi/f//AAABAAcAAABtdGFfdGF4AFj9//8AAAEDGAAAAAwAAAAEAAAAAAAAAI79//8AAAEABQAAAGV4dHJhAAAAhP3//wAAAQMYAAAADAAAAAQAAAAAAAAAuv3//wAAAQALAAAAZmFyZV9hbW91bnQAtP3//wAAAQUUAAAADAAAAAQAAAAAAAAApP3//wwAAABwYXltZW50X3R5cGUAAAAA5P3//wAAAQMYAAAADAAAAAQAAAAAAAAAGv7//wAAAQAQAAAAZHJvcG9mZl9sYXRpdHVkZQAAAAAc/v//AAABAxgAAAAMAAAABAAAAAAAAABS/v//AAABABEAAABkcm9wb2ZmX2xvbmdpdHVkZQAAAFT+//8AAAEFFAAAAAwAAAAEAAAAAAAAAET+//8SAAAAc3RvcmVfYW5kX2Z3ZF9mbGFnAACI/v//AAABARQAAAAMAAAABAAAAAAAAAB4/v//DAAAAHJhdGVfY29kZV9pZAAAAAC4/v//AAABAxgAAAAMAAAABAAAAAAAAADu/v//AAABAA8AAABwaWNrdXBfbGF0aXR1ZGUA7P7//wAAAQMYAAAADAAAAAQAAAAAAAAAIv///wAAAQAQAAAAcGlja3VwX2xvbmdpdHVkZQAAAAAk////AAABAxgAAAAMAAAABAAAAAAAAABa////AAABAA0AAAB0cmlwX2Rpc3RhbmNlAAAAWP///wAAAQIkAAAAFAAAAAQAAAAAAAAACAAMAAgABwAIAAAAAAAAAQgAAAAPAAAAcGFzc2VuZ2VyX2NvdW50AJj///8AAAEKGAAAAAwAAAAEAAAAAAAAAM7///8AAAMACgAAAGRyb3BvZmZfYXQAAMj///8AAAEKIAAAABQAAAAEAAAAAAAAAAAABgAIAAYABgAAAAAAAwAJAAAAcGlja3VwX2F0AAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFGAAAABAAAAAEAAAAAAAAAAQABAAEAAAACQAAAHZlbmRvcl9pZAAAAA==
{code}

I'd argue that extra metadata, if it's not part of the Arrow format and can be whatever an application wants to put in there, should not be printed as part of the schema's ToString method. It should be viewable some way, just not always. And IDK what to do with this {{ARROW:schema: }} business but it's clearly not readable as is.",dataset parquet pull-request-available,"['C++', 'Python']",ARROW,Improvement,Minor,2019-11-04 23:17:47,14
13266213,[C++] Parquet file parse error messages should include the file name,ARROW-7061 was harder to diagnose than it should have been because the error message was opaque and didn't tell me where to look.,dataset parquet pull-request-available,['C++'],ARROW,Improvement,Major,2019-11-04 23:13:59,13
13266211,[C++][Dataset] FileSystemDiscovery with ParquetFileFormat should ignore files that aren't Parquet,"I got {{Invalid parquet file. Corrupt footer.}} trying to read real data. Turned out it was because I had opened the directory in macOS Finder and it had added the junk .DS_Store files. Once I deleted them, the Dataset created fine. 

If we're creating a DataSource with Parquet files, we should ignore any non-Parquet files we encounter when scanning.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-11-04 23:11:54,13
13266203,[R] Post-0.15.1 cleanup,"This includes some backfilling of changelog, as well as some updates to meet new CRAN requirements.",pull-request-available,['R'],ARROW,New Feature,Major,2019-11-04 22:13:53,4
13266201,[Python] Reading parquet file with many columns is much slower in 0.15.x versus 0.14.x,"Reading Parquet files with large number of columns still seems to be very slow in 0.15.1 compared to 0.14.1. I using the same test used inARROW-6876 except I set {{use_threads=False}} to make for an apples-to-apples comparison with respect to # of CPUs.


{code}
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
table = pa.table(\{'c' + str(i): np.random.randn(10) for i in range(10000)})
pq.write_table(table, ""test_wide.parquet"")
res = pq.read_table(""test_wide.parquet"")
print(pa.__version__)
%time res = pq.read_table(""test_wide.parquet"", use_threads=False)
{code}

*In 0.14.1 with use_threads=False:*

{{0.14.1}}
{{CPU times: user 515 ms, sys: 9.3 ms, total: 524 ms}}
{{Wall time: 525 ms}}
**

*In 0.15.1 with* *use_threads=False**:*

{{0.15.1}}
{{CPU times: user 9.89 s, sys: 37.8 ms, total: 9.93 s}}
{{Wall time: 9.93 s}}",parquet performance pull-request-available,['Python'],ARROW,Bug,Major,2019-11-04 22:07:50,14
13266177,[C++] FileSystemDataSourceDiscovery should apply partition schemes relative to the base_dir of its selector,"Currently, the absolute path of each fragment is used which leads to erroneous parse errors unless the Discovery's base directory also happens to be root of a FileSystem.",dataset pull-request-available,['C++'],ARROW,Improvement,Minor,2019-11-04 20:03:12,6
13266136,[C++] Add API to parse URI query strings,"We already expose an API to parse URIs, but it doesn't handle query strings. Parsing query strings is required for ARROW-6720.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-11-04 15:14:08,2
13266122,[Python] Test errors without S3,"When running the tests I get errors like this:
{code}
_______________________________________________________ ERROR at setup of test_non_path_like_input_raises[S3FileSystem] ________________________________________________________
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_fs.py"", line 57, in s3fs
    from pyarrow.s3fs import S3Options, S3FileSystem
  File ""/home/antoine/arrow/dev/python/pyarrow/s3fs.py"", line 20, in <module>
    from pyarrow._s3fs import (  # noqa
ModuleNotFoundError: No module named 'pyarrow._s3fs'
{code}

It seems that those tests should be automatically skipped if S3 support isn't enabled.",pull-request-available,['Python'],ARROW,Bug,Major,2019-11-04 13:50:14,2
13265920,[Docs] Add option to override displayed docs version with an environment variable,Work around ARROW-7053,pull-request-available,['Documentation'],ARROW,Improvement,Major,2019-11-02 20:51:08,14
13265917,[C++] Datasets example fails to build with ARROW_SHARED=OFF,"This turned up in nightly tests

https://circleci.com/gh/ursa-labs/crossbow/4335",pull-request-available,['C++'],ARROW,New Feature,Major,2019-11-02 20:36:56,14
13265908,[R] Fix compiler warnings in R bindings,"As reported by gcc 8.3.0 on Windows. Maybe you should CI with -Werror?

{code}
array_from_vector.cpp: In member function 'arrow::Status arrow::r::TypedVectorConverter<Type, Derived>::Ingest(SEXP) [with Type = arrow::DoubleType; Derived = arrow::r::NumericVectorConverter<arrow::DoubleType>]':
array_from_vector.cpp:383:16: warning: 'value' may be used uninitialized in this function [-Wmaybe-uninitialized]
         double value;
                ^~~~~
{code}",pull-request-available,['R'],ARROW,Bug,Minor,2019-11-02 16:43:59,4
13265897,[C++] warnings building on mingw-w64,"Two warnings when building libarrow 0.15.1 on mingw-w64:

{code}
[  2%] Running thrift compiler on parquet.thrift
[WARNING:C:/msys64/home/mingw-packages/mingw-w64-arrow/src/apache-arrow-0.15.1/cpp/src/parquet/parquet.thrift:297] The ""byte"" type is a compatibility alias for ""i8"". Use ""i8"" to emphasize the signedness of this type.
{code} 

And later:

{code}
 81%] Building CXX object src/parquet/CMakeFiles/parquet_static.dir/column_reader.cc.obj
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/apache-arrow-0.15.1/cpp/src/parquet/arrow/writer.cc: In member function 'virtual arrow::Status parquet::arrow::FileWriterImpl::WriteColumnChunk(const std::shared_ptr<arrow::ChunkedArray>&, int64_t, int64_t)':
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/apache-arrow-0.15.1/cpp/src/parquet/arrow/writer.cc:79:41: warning: 'schema_field' may be used uninitialized in this function [-Wmaybe-uninitialized]
         schema_manifest_(schema_manifest) {}
                                         ^
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/apache-arrow-0.15.1/cpp/src/parquet/arrow/writer.cc:466:24: note: 'schema_field' was declared here
     const SchemaField* schema_field;
{code}

Maybe CI with `CXXFLAGS += -Werror` ?",pull-request-available,['C++'],ARROW,Bug,Minor,2019-11-02 14:59:50,6
13265866,[Java] Support for combining multiple vectors under VectorSchemaRoot,"Hi,



pyarrow.Table.combine_chunks provides a nice functionality of combining multiple batch records under a singlepyarrow.Table.



I am currently working on a downstream application which reads data from BigQuery. BigQuery storage api supports data output in Arrow format but streams data in many batches of size 1024 or less number of rows.

It would be really nice to have Arrow Java api provide this functionality under an abstraction like VectorSchemaRoot.

After getting guidance from [~emkornfield@gmail.com], I tried to write my own implementation by copying data vector by vector usingTransferPair's copyValueSafe

But, unless I am missing some thing obvious, turns out it only copies one value at a time. That means a lot of looping tryingcopyValueSafe millions of rows from source vector index to target vector index. Ideally I would want to concatenate/link the underlying buffers rather than copying one cell at a time.



Eg, if I have :
{code:java}
List<VectorSchemaRoot> batchList = new ArrayList<>();
try (ArrowStreamReader reader = new ArrowStreamReader(new ByteArrayInputStream(out.toByteArray()), allocator)) {
    Schema schema = reader.getVectorSchemaRoot().getSchema();
    for (int i = 0; i < 5; i++) {
        // This will be loaded with new values on every call to loadNextBatch
        VectorSchemaRoot readBatch = reader.getVectorSchemaRoot();
        reader.loadNextBatch();
        batchList.add(readBatch);
    }
}

//VectorSchemaRoot.combineChunks(batchList, newVectorSchemaRoot);{code}


A method like VectorSchemaRoot.combineChunks(List<VectorSchemaRoot>)?

I did read the VectorSchemaRoot discussion onhttps://issues.apache.org/jira/browse/ARROW-6896and am not sure if its the right thing to use here.





PS. Feel free to update the title of this feature request with more appropriate wordings.



Cheers,

Yogesh



",pull-request-available,['Java'],ARROW,New Feature,Major,2019-11-02 02:48:48,7
13265850,[C++][Dataset] Filter expressions should not require exact type match,"It's not trivial for users to be able to ensure that scalars are of identical type to the fields they relate to in Expressions. For one, FieldExpressions don't contain a type reference, so at the time when I construct {{field_ref(""col1"") > scalar(42)}}, I don't know exactly what type col1 is to be able to ensure that scalar(42) matches. Even if it were available, I wouldn't be able to determine what type to make it if the expression were {{(field_ref(""col1"") + field_ref(""col2"")) > scalar(42)}}.

We should allow CompareExpressions to cast the inputs as necessary. This should be among integer types and floating point types, and across integers and floats too. Likewise among date/timestamp types, and probably if comparing a string scalar against a date/timestamp column, the string should be parsed as a datetime. We also need to think about DictionaryTypes (though in practice this is moot until we have a comparison kernels that work on strings).

[~fsaintjacques][~bkietz]",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-11-01 22:05:21,6
13265523,[Python] Typecheck expects pandas to be installed,"See nightly build failure: https://circleci.com/gh/ursa-labs/crossbow/4285?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

The following patch fixes it:

{code}
diff --git a/python/pyarrow/table.pxi b/python/pyarrow/table.pxi
index b65dac4cb..e0a82fd76 100644
--- a/python/pyarrow/table.pxi
+++ b/python/pyarrow/table.pxi
@@ -17,6 +17,7 @@

 import warnings

+
 cdef class ChunkedArray(_PandasConvertible):
     """"""
     Array backed via one or more memory chunks.
@@ -1579,7 +1580,7 @@ def record_batch(data, names=None, schema=None, metadata=None):
     if isinstance(data, (list, tuple)):
         return RecordBatch.from_arrays(data, names=names, schema=schema,
                                        metadata=metadata)
-    elif isinstance(data, _pandas_api.pd.DataFrame):
+    elif _pandas_api.is_data_frame(data):
         return RecordBatch.from_pandas(data, schema=schema)
     else:
         return TypeError(""Expected pandas DataFrame or python dictionary"")
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-10-31 12:14:58,5
13265385,[CI][Crossbow] Skip known nightly failures,The failures are ticketed. There's no point in running them if we know they're failing. The patches that fix the builds can add them back to the nightly list.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Minor,2019-10-30 21:07:31,4
13265378,[C++] Error in./configure step for jemalloc when building on OSX 10.14.6,"Hello. I'm trying to build the C++ part of Apache Arrow (as a first step to possible contributions). I'm following the C++ Development instructions, but running into an error early. I also looked at ARROW-4935, but the cause there seems different, so I'm opening a new bug report.

I'm on MacOS 10.14.6. I have the XCode cli tools installed (via xcode-select), and installed the other dependencies with Homebrew, giving it the cpp/Brewfile. I want to be able to run the tests, so I'm configuring a debug build with:

 cmake -DCMAKE_BUILD_TYPE=Debug -DARROW_BUILD_TESTS=ON ..

from an out-of-source build, in a cpp/debug directory. Then, running make, I get very quickly the following error:

{{$ make}}
{{[ 0%] Performing configure step for 'jemalloc_ep'}}
{{CMake Error at /Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-DEBUG.cmake:49 (message):}}
{{ Command failed: 1}}{{'./configure' 'AR=/Library/Developer/CommandLineTools/usr/bin/ar' 'CC=/Library/Developer/CommandLineTools/usr/bin/cc' '--prefix=/Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep/dist/' '--with-jemalloc-prefix=je_arrow_' '--with-private-namespace=je_arrow_private_' '--without-export' '--disable-cxx' '--disable-libdl' '--disable-initial-exec-tls'}}{{See also}}{{/Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-*.log}}
{{make[2]: *** [jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure] Error 1}}
{{make[1]: *** [CMakeFiles/jemalloc_ep.dir/all] Error 2}}
{{make: *** [all] Error 2}}

{{Looking into the log file as suggested, I see:}}

configure: error: in `/Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep':
configure: error: cannot run C compiled programs.
If you meant to cross compile, use `--host'.
See `config.log' for more details

... which seems a bit suspicuous. Running the ./configure invocation manually, I get the same error:

{{$ './configure' 'AR=/Library/Developer/CommandLineTools/usr/bin/ar' 'CC=/Library/Developer/CommandLineTools/usr/bin/cc' '--prefix=/Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep/dist/' '--with-jemalloc-prefix=je_arrow_' '--with-private-namespace=je_arrow_private_' '--without-export' '--disable-cxx' '--disable-libdl' '--disable-initial-exec-tls'}}
{{checking for xsltproc... /usr/bin/xsltproc}}
{{checking for gcc... /Library/Developer/CommandLineTools/usr/bin/cc}}
{{checking whether the C compiler works... yes}}
{{checking for C compiler default output file name... a.out}}
{{checking for suffix of executables...}}
{{checking whether we are cross compiling... configure: error: in `/Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep':}}
{{configure: error: cannot run C compiled programs.}}
{{If you meant to cross compile, use `--host'.}}
{{See `config.log' for more details}}{{}}

{{Digging into config.log, I see:}}

configure:3213: checking whether we are cross compiling
*configure:3221: /Library/Developer/CommandLineTools/usr/bin/cc -o conftest conftest.c >&5*
*conftest.c:9:10: fatal error: 'stdio.h' file not found*
#include <stdio.h>
 ^~~~~~~~~
1 error generated.
configure:3225: $? = 1
configure:3232: ./conftest
./configure: line 3234: ./conftest: No such file or directory
configure:3236: $? = 127
configure:3243: error: in `/Users/chrish/Code/arrow/cpp/debug/jemalloc_ep-prefix/src/jemalloc_ep':
configure:3245: error: cannot run C compiled programs.
If you meant to cross compile, use `--host'.

(Relevant bit in bold.) Well, that would make more sense, at least. I create a close-enough conftest.c by hand:

{{#include <stdio.h>}}

{{int main(void) \{ return 0; }}}

and try to compile it with the same command-line invocation:

{{$ /Library/Developer/CommandLineTools/usr/bin/cc -o conftest conftest.c}}

{{I get that same error:}}

conftest.c:1:10: fatal error: 'stdio.h' file not found
#include <stdio.h>
 ^~~~~~~~~
1 error generated.

However, I also have a cc in /usr/bin. If I try that one instead, things works:

{{$ /usr/bin/cc -o conftest conftest.c}}
{{$ ls -l conftest}}
{{-rwxr-xr-x 1 chrish staff 4,2K oct 30 16:03 conftest*}}
{{$ ./conftest}}

{{(No error compiling or running conftest.c)}}

The two executable seem to be the same compiler (or at least the exact same version):

{{$ /usr/bin/cc --version
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin}}

{{$ /Library/Developer/CommandLineTools/usr/bin/cc --version
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin}}{{}}

So... I guess at least for me, Apple's Clang works correctly only if invoked from the /usr/bin path. Any reason why the configure, etc. scripts make a special effort to invoke it from /Library/Developer/CommandLineTools/usr/bin, if it's also available in /usr/bin (which is even in the path by default). How do I fix this, so others don't run into that problem too? Thanks!",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-30 20:11:07,1
13265331,[Release] Run the python unit tests in the release verification script,"For linux wheels use docker, otherwise setup a virtualenv and install the wheel supported on the host's platform. 
Testing should include the imports for the optional modules and perhaps running the unit tests, but the import testing should catch most of the wheel issues.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-10-30 16:17:18,3
13265306,[Python] Expose the offsets of a ListArray in python,"Assume the following ListArray:

{code}
In [1]: arr = pa.ListArray.from_arrays(offsets=[0, 3, 5], values=[1, 2, 3, 4, 5])                                                                                                                                  

In [2]: arr                                                                                                                                                                                                        
Out[2]: 
<pyarrow.lib.ListArray object at 0x7f11de71c708>
[
  [
    1,
    2,
    3
  ],
  [
    4,
    5
  ]
]
{code}

You can get the actual values as a flat array through {{.values}} / {{.flatten()}}, but there is currently no easy way to get back to the offsets (except from interpreting the buffers manually). 

We should probably add an {{offsets}} attribute (there is actually also a TODO comment for that).",pull-request-available,['Python'],ARROW,Improvement,Major,2019-10-30 13:44:58,5
13265244,[Python] pa.table(..) returns instead of raises error if passing invalid object,"When passing eg a Series instead of a DataFrame, you get:

{code}
In [4]: df = pd.DataFrame({'a': [1, 2, 3]})                                                                                                                                                                        

In [5]: table = pa.table(df['a'])                                                                                                                                                                                  

In [6]: table                                                                                                                                                                                                      
Out[6]: TypeError('Expected pandas DataFrame or python dictionary')

In [7]: type(table)                                                                                                                                                                                                
Out[7]: TypeError
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2019-10-30 09:33:36,5
13265215,[Java] Remove assertions in MessageSerializer/vector/writer/reader,"Currently assertions exists in many classes like {{MessagaSerializer/JsonReader/JsonWriter/ListVector}}etc.

i. If jvm arguments are not specified, these checks will skipped and lead to potential problems.

ii. Java errors produced by failed assertions are not caught by traditional catch clauses.

To fix this, use {{Preconditions}}instead.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-10-30 07:14:45,16
13265089,[CI][R] Update R dependencies for Conda build,"Conda makes you keep a separate list of package dependencies and install them differently from how R normally does this, so when adding an R dependency, you have to add it there too.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Minor,2019-10-29 16:58:58,4
13265078,"[Python] pa.array does not use ""from_pandas"" semantics for pd.Index","{code}
In [15]: idx = pd.Index([1, 2, np.nan], dtype=object)                                                                                                                                                              

In [16]: pa.array(idx)                                                                                                                                                                                             
Out[16]: 
<pyarrow.lib.DoubleArray object at 0x7f2e24300780>
[
  1,
  2,
  nan
]

In [17]: pa.array(idx, from_pandas=True)                                                                                                                                                                           
Out[17]: 
<pyarrow.lib.Int64Array object at 0x7f2e242d3678>
[
  1,
  2,
  null
]

In [18]: pa.array(pd.Series(idx))                                                                                                                                                                                  
Out[18]: 
<pyarrow.lib.Int64Array object at 0x7f2e242d3780>
[
  1,
  2,
  null
]
{code}

We should probably handle Series and Index the same in this regard.",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-29 16:09:20,5
13265041,[Python] __arrow_array__ does not work for ExtensionTypes in Table.from_pandas,"When someone has a custom ExtensionType defined in Python, and an array class that gets converted to that (through {{\_\_arrow_array\_\_}}), the conversion in pyarrow works with the array class, but not yet for the array stored in a pandas DataFrame.

Eg using my definition of ArrowPeriodType in https://github.com/pandas-dev/pandas/pull/28371, I see:

{code}
In [15]: pd_array = pd.period_range(""2012-01-01"", periods=3, freq=""D"").array                                                                                                                                       

In [16]: pd_array                                                                                                                                                                                                  
Out[16]: 
<PeriodArray>
['2012-01-01', '2012-01-02', '2012-01-03']
Length: 3, dtype: period[D]

In [17]: pa.array(pd_array)                                                                                                                                                                                        
Out[17]: 
<pyarrow.lib.ExtensionArray object at 0x7f657cf78768>
[
  15340,
  15341,
  15342
]

In [18]: df = pd.DataFrame({'periods': pd_array})                                                                                                                                                                  

In [19]: pa.table(df)                                                                                                                                                                                              
...
ArrowInvalid: ('Could not convert 2012-01-01 with type Period: did not recognize Python value type when inferring an Arrow data type', 'Conversion failed for column periods with type period[D]')
{code}

(this is working correctly for array objects whose {{\_\_arrow_array\_\_}} is returning a built-in pyarrow Array).",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-29 13:45:28,5
13265027,[Java] UnionFixedSizeListWriter decimal type should check writer index,"{{UnionFixedSizeListWriter}} should check writer index for decimal type (just as other types) to ensure the values written not exceed listSize.

Otherwise, the writer may continue to write data into its underlying vector quietly even the the writer.idx() > listSize * index.",pull-request-available,['Java'],ARROW,Bug,Major,2019-10-29 12:46:57,16
13265025,[Java] Fix the bugs when calculating vector hash code,"When calculating the hash code for a value in the vector, the validity bit must be taken into account.",pull-request-available,['Java'],ARROW,Bug,Major,2019-10-29 12:42:52,7
13265020,[Java] Improve the performance of loading validity buffers,"At the receiver side of flighting, loading validity buffer is an important operation, as each vector has a validity buffer. 

For non-nullable vectors, the current implementation of loading the validity buffer is inefficient.  We improve the performance of this operation by efficiently setting the bits of a memory region to 1. 

Benchmark results show that the changes leads to a 35% performance improvement:

Before:
BitVectorHelperBenchmarks.loadValidityBufferAllOne  avgt    5  748.916  23.290  ns/op

After:
BitVectorHelperBenchmarks.loadValidityBufferAllOne  avgt    5  487.352  15.046  ns/op
",pull-request-available,['Java'],ARROW,Improvement,Major,2019-10-29 12:35:53,7
13264980,[R] Non-UTF-8 data in Arrow <--> R conversion,"Hello.
I'm new to the arrow package in R and I'm having a trouble regarding special characters (Icelandic). I have a large data set and everything is fine until I write the file to disk and read it in again (i.e. I use write_parquet() and then read_parquet()). When I read the data back in to R special characters turn into question mark. I.e. Veitingastair becomes Veitingastair.

This does not happen when I use .csv.

Is there anything I can do when I write the .parquet file to disk or when I read it in to prevent this?",pull-request-available,['R'],ARROW,Bug,Critical,2019-10-29 08:08:10,4
13264922,[Developer][Python] Write script to verify Windows wheels given local environment with conda,Windows version of ARROW-7014,pull-request-available,"['Developer Tools', 'Python']",ARROW,New Feature,Major,2019-10-28 22:23:47,14
13264920,[Developer] Write script to verify Linux wheels given local environment with conda or virtualenv,Facilitate testing RC wheels. Also test checksum and sig,pull-request-available,"['Developer Tools', 'Python']",ARROW,New Feature,Major,2019-10-28 22:22:08,14
13264914,[C++] arrow-dataset pkgconfig is incomplete,"Unlike the other *.pc.in files, it doesn't include a {{Libs}} field, so passing the result of what is found by pkgconfig results in the lib still not being found.",dataset pull-request-available,['C++'],ARROW,Bug,Major,2019-10-28 22:01:06,4
13264901,[C++] Clarify ChunkedArray chunking strategy and policy,"See discussion onARROW-6784 and[https://github.com/apache/arrow/pull/5686]. Among the questions:
 * Do Arrow users control the chunking, or is it an internal implementation detail they should not manage?
 * If users control it, how do they control it? E.g. if I call Take and use a ChunkedArray for the indices to take, does the chunking follow how the indices are chunked? Or should we attempt to preserve the mapping of data to their chunks in the input table/chunked array?
 * If it's an implementation detail, what is the optimal chunk size? And when is it worth reshaping (concatenating, slicing) input data to attain this optimal size?",pull-request-available,['C++'],ARROW,New Feature,Minor,2019-10-28 20:14:28,14
13264899,[C++] Implement casts from float/double to decimal128,"see also ARROW-5905, ARROW-7010",pull-request-available,['C++'],ARROW,New Feature,Major,2019-10-28 20:09:58,2
13264897,[C++] Support lossy casts from decimal128 to float32 and float64/double,I do not believe such casts are implemented. This can be helpful for people analyzing data where the precision of decimal128 is not needed,pull-request-available,['C++'],ARROW,New Feature,Major,2019-10-28 20:08:23,2
13264818,[Python] pyarrow.chunked_array([array]) fails on array with all-None buffers,"Minimal reproducer:

{code}
import pyarrow as pa

pa.chunked_array([pa.array([], type=pa.string()).dictionary_encode().dictionary])
{code}

Traceback

{code}
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x20)
  * frame #0: 0x0000000112cd5d0e libarrow.15.dylib`arrow::Status arrow::internal::ValidateVisitor::ValidateOffsets<arrow::BinaryArray const>(arrow::BinaryArray const&) + 94
    frame #1: 0x0000000112cc79a3 libarrow.15.dylib`arrow::Status arrow::VisitArrayInline<arrow::internal::ValidateVisitor>(arrow::Array const&, arrow::internal::ValidateVisitor*) + 915
    frame #2: 0x0000000112cc747d libarrow.15.dylib`arrow::Array::Validate() const + 829
    frame #3: 0x0000000112e3ea19 libarrow.15.dylib`arrow::ChunkedArray::Validate() const + 89
    frame #4: 0x0000000112b8eb7d lib.cpython-37m-darwin.so`__pyx_pw_7pyarrow_3lib_135chunked_array(_object*, _object*, _object*) + 3661
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-28 13:47:24,14
13264706,[Format] [Rust] Generate flatbuffers files in build script,We should generate the flatbuffers files rather than check them in.,pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-10-27 19:10:40,10
13264614,[Python] KeyError: '__index_level_0__' passing Table.from_pandas its own schema,"Steps to reproduce:
 # Generate any DataFrame's pyarrow Schema usingTable.from_pandas
 # Pass the generated schema as input into Table.from_pandas
 # Causes KeyError: '__index_level_0__'

We did not have this issue with pyarrow==0.11.0 which we used to write many partitions across years. Our goal now is to use pyarrow==0.15.0 and produce schema going forward that are *backwards compatible* (i.e. also have'__index_level_0__'), so we should not need to re-generate all prior years' partitions when we migrate to 0.15.0.

We cannot set _preserve_index=False_, since that effectively deletes'__index_level_0__', causing inconsistent schema across earlier partitions that had been written using pyarrow==0.11.0.


{code:java}
import pandas as pd
import pyarrow as pa
df = pd.DataFrame() 
schema = pa.Table.from_pandas(df).schema
pa_table = pa.Table.from_pandas(df, schema=schema)

{code}
{noformat}
Traceback (most recent call last):
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3078, in get_loc
    return self._engine.get_loc(key)
  File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: '__index_level_0__'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 408, in _get_columns_to_convert_given_schema
    col = df[name]
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__
    return self._getitem_column(key)
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column
    return self._get_item_cache(key)
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache
    values = self._data.get(item)
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get
    loc = self.items.get_loc(item)
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: '__index_level_0__'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-36-6711a2fcec96>"", line 5, in <module>
    pa_table = pa.Table.from_pandas(df, schema=pa.Table.from_pandas(df).schema)
  File ""pyarrow/table.pxi"", line 1057, in pyarrow.lib.Table.from_pandas
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 517, in dataframe_to_arrays
    columns)
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 337, in _get_columns_to_convert
    return _get_columns_to_convert_given_schema(df, schema, preserve_index)
  File ""/GAAR/FIAG/sandbox/software/miniconda3/envs/rc_sfi_2019.1/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 426, in _get_columns_to_convert_given_schema
    ""in the columns or index"".format(name))
KeyError: ""name '__index_level_0__' present in the specified schema is not found in the columns or index""
{noformat}",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-26 16:34:38,5
13264549,[Packaging] Add support for RHEL,"We need symbolic links to {{${VERSION}Server}} from {{${VERSION}}} such as {{7Server}} from {{7}}. (Is it available on BinTray?)

We also need to update install information. We can't install {{epel-release}} by {{yum install -y epel-release}}. We need to specify URL explicitly: {{yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm}}. See https://fedoraproject.org/wiki/EPEL for details.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-10-25 21:03:23,1
13264534,[Python] Expose boolean filter kernel on Table,This is currently only implemented for Array but would also be useful on Tables and ChunkedArrays.,iceberg pull-request-available,['Python'],ARROW,Improvement,Major,2019-10-25 19:43:10,5
13264492,[C++] Research jemalloc memory page reclamation configuration on macOS when background_thread option is unavailable,"In ARROW-6977, this was disabled on macOS, but this will potentially have negative performance and memory implications that were intended to have been fixed in ARROW-6910",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-25 15:06:01,14
13264413,[Python][C++] Assert is triggered when decimal type inference occurs on a value with out of range precision,"Example:
pa.array([decimal.Decimal(123.234)])

The problem is that inference.cc calls the direct constructor for decimal types instead using Make.",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-25 05:27:45,5
13264360,[R] Add basic Expression class,"I started this as part of ARROW-6980 but it proved not necessary. This will be a foundation for ARROW-6982, in addition to being useful on its own.",pull-request-available,['R'],ARROW,New Feature,Major,2019-10-24 21:57:37,4
13264251,[C++] Update LZ4 to 1.9.2 for CVE-2019-17543,"There is a reported CVEthatLZ4 before 1.9.2 has a heap-based buffer overflow in LZ4_write32 (More details in here -[https://nvd.nist.gov/vuln/detail/CVE-2019-17543] ). I see that Apache Arrow uses *v1.8.3* version ( [https://github.com/apache/arrow/blob/47e5ecafa72b70112a64a1174b29b9db45f803ef/cpp/thirdparty/versions.txt#L38]).

We need to bump up the dependency version of LZ4 to *1.9.2* to get past the reported CVE. Thank you!",pull-request-available,['C++'],ARROW,Wish,Major,2019-10-24 13:11:45,3
13264169,[C++] Threaded task group crashes sometimes,"You can give this a more descriptive title :)

See discussion on ARROW-6977. https://gist.github.com/pitrou/87f3091c226db3306c45b2c32dd9aea8 seems to fix it.",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-24 03:33:55,2
13264120,[R] Add bindings for compare and boolean kernels,"See cpp/src/arrow/compute/kernels/compare.h and boolean.h. ARROW-6980 introduces an Expression class that works on Arrow Arrays, but to evaluate the expressions, it has to pull the data into R first. This would enable us to do the work in C++ and only pull in the result.",pull-request-available,['R'],ARROW,Improvement,Major,2019-10-23 21:17:29,4
13264108,[R] Enable jemalloc in autobrew formula,See https://github.com/apache/arrow/blob/59a6788c76330cf055bdbcbc7bdae7b0106c6656/dev/tasks/homebrew-formulae/autobrew/apache-arrow.rb#L47,pull-request-available,['R'],ARROW,Improvement,Blocker,2019-10-23 19:42:40,4
13264082,[C++] Only enable jemalloc background_thread if feature is supported,"Followup to ARROW-6910. When loading the R package after that patch merged, I get this new message:

{code}
$ R
> library(arrow)
<jemalloc>: option background_thread currently supports pthread only

{code}

https://github.com/jemalloc/jemalloc/blob/3d84bd57f4954a17059bd31330ec87d3c1876411/src/background_thread.c#L884-L887 is where the message comes from. Tracing that further, {{have_background_thread}} comes from https://github.com/jemalloc/jemalloc/blob/21cfe59ff7b10a61dabe26cd3dbfb7a255e1f5e8/include/jemalloc/internal/jemalloc_preamble.h.in#L205-L211, which gets set in {{configure.ac}} here: https://github.com/jemalloc/jemalloc/blob/d2dddfb82aac9f2212922eb90324e84790704bfe/configure.ac#L2155-L2157

In sum, on my system, that flag doesn't get set, so {{have_background_thread}} is false, and when that is false and the {{background_thread}} option is true, I get that message printed. And I do not want to see that message.

cc [~wesm]",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-23 17:46:51,2
13264033,[C++] Put make_unique in its own header,{{arrow/util/stl.h}} carries other stuff that is almost never necessary.,pull-request-available,['C++'],ARROW,Wish,Major,2019-10-23 14:23:08,2
13263858,[C++][Dataset] ParquetScanTask eagerly load file ,"The file content should only be read when invoking ParquetScanTask::Scan, not on construction. This blocks reading in a true streaming fashion with memory constraints.",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-22 18:13:49,13
13263844,"[C++] Add filter expressions for IN, IS_VALID","Implement filter expressions for {{IN, IS_VALID}}

{{IN}} should be backed in TreeEvaluator by the IsIn kernel. {{IS_VALID}} should be implementable by wrapping null bitmap into a BooleanArray.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-22 17:11:11,6
13263835,[C++][Dataset] Optionally expose partition keys as materialized columns,This would be exposed in the DataSourceDiscovery as an option.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-22 16:42:54,6
13263815,[Packaging][Wheel][OSX] Use crossbow's command to deploy artifacts from travis builds,"Travis starts to fail more often during artefact deployment to GitHub releases.
Crossbow has a builtin command to upload the artifacts which is more reliable.

All of the travis builds should use the crossbow script instead of relying on travis's deployment feature.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-10-22 14:11:17,3
13263791,[C++] [CI] Stop compiling with -Weverything,"We should simply use {{-Wall}} instead.

[https://quuxplusone.github.io/blog/2018/12/06/dont-use-weverything/]",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-10-22 12:15:36,2
13263652,[C++] Clarify what signatures are preferred for compute kernels,"Many of the compute kernels feature functions which accept only array inputs in addition to functions which accept Datums. The former seems implicitly like a convenience wrapper around the latter but I don't think this is explicit anywhere. Is there a preferred overload for bindings to use? Is it preferred that C++ implementers provide convenience wrappers for different permutations of argument type? (for example, Filter now provides an overload for record batch input as well as array input)",compute,['C++'],ARROW,Improvement,Minor,2019-10-21 21:03:18,14
13263625,[CI][Crossbow] Nightly R with sanitizers build fails installing dependencies,"See https://circleci.com/gh/ursa-labs/crossbow/4006 for example. Failure looks like this:

{code}
...
g++ -fsanitize=address,undefined,bounds-strict -fno-omit-frame-pointer -std=gnu++11 -I""/usr/local/RDsan/lib/R/include"" -DNDEBUG  -I'/usr/local/RDsan/lib/R/site-library/Rcpp/include' -I'/usr/local/RD/lib/R/library/BH/include' -I/usr/local/include  -I. -Ircon -fpic  -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g -O0 -Wall -pedantic  -c TokenizerWs.cpp -o TokenizerWs.o

cc1plus: out of memory allocating 65536 bytes after a total of 31866880 bytes

cc1plus: out of memory allocating 65536 bytes after a total of 52080640 bytes
virtual memory exhausted: Cannot allocate memory
virtual memory exhausted: Cannot allocate memory
/usr/local/RDsan/lib/R/etc/Makeconf:175: recipe for target 'Collector.o' failed
make[1]: *** [Collector.o] Error 1
make[1]: *** Waiting for unfinished jobs....
/usr/local/RDsan/lib/R/etc/Makeconf:175: recipe for target 'CollectorGuess.o' failed
make[1]: *** [CollectorGuess.o] Error 1
/usr/local/RDsan/lib/R/etc/Makeconf:175: recipe for target 'Reader.o' failed
make[1]: *** [Reader.o] Error 1
/usr/local/RDsan/lib/R/etc/Makeconf:175: recipe for target 'TokenizerWs.o' failed
make[1]: *** [TokenizerWs.o] Error 1
make[1]: Leaving directory '/tmp/RtmpvSXO7m/R.INSTALL992bafde57/readr/src'
ERROR: compilation failed for package readr
* removing /usr/local/RDsan/lib/R/site-library/readr
Error: Failed to install 'decor' from GitHub:
  (converted from warning) installation of one or more packages failed,
  probably readr
Execution halted
ERROR: Service 'r-sanitizer' failed to build: The command '/bin/sh -c RDsan -e ""remotes::install_github('romainfrancois/decor')""' returned a non-zero code: 1
{code}

Some thoughts: 

* This dependency is not needed to run the tests--it is a developer dependency, only needed when you're editing the Rcpp code. So we could conditionally not install it if we're on CI.
* The docker code around this changed in ARROW-6918 last week, around the time this started failing. The installation of this package was moved to a different part of the job, and it appears that {{MAKEVARS=-j8}} was added. Maybe that's causing more memory to be used concurrently? (Also, I would be shocked if CircleCI gave us a VM with 8 cores, so this seems wasteful and should probably be tuned to whatever the current system has.)

cc [~apitrou][~fsaintjacques]",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2019-10-21 18:31:29,4
13263572,[C++][Dataset] Ensure expression filter is passed ParquetDataFragment,We should be able to prune RowGroups based on the expression and the statistics.,dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-21 14:43:35,13
13263568,[C++][Dataset] Add example/benchmark for reading parquet files with dataset,Create an executable that load a directory with a known partition scheme with a filter and a projection. This will be used as a baseline for future performance improvement but also to show various feature of the dataset API.,dataset pull-request-available,['C++'],ARROW,Test,Major,2019-10-21 14:41:33,13
13263430,[Rust] [DataFusion] Add support for scalar UDFs,"As a user, I would like to be able to define my own functions and then use them in SQL statements.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2019-10-20 19:47:28,10
13263335,[Rust] Enable integration tests,Use docker-compose to generate test files using the Java implementation and then have Rust tests read them.,pull-request-available,"['Integration', 'Rust']",ARROW,Sub-task,Major,2019-10-19 15:14:36,4
13263323,[Rust] Add StringType,"Create a separate String type which uses UTF8, and restrict the BinaryArray to opaque binary data",pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-10-19 11:16:49,12
13263310,[Developer] Add support for Parquet in pull request check by GitHub Actions,"* title check should work with parquet and arrow Jira issues.
",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-10-19 06:28:27,1
13263303,[C++] Unpin gtest in build environment,Follow up to failure triaged in ARROW-6834,pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-19 03:22:31,3
13263263,"[Python] Windows wheel depends on zstd.dll and libbz2.dll, which are not bundled","I found this out by using Dependency Walker on the wheel at

https://ci.appveyor.com/project/Ursa-Labs/crossbow/builds/28212818/artifacts

This simplest mitigation is to disable these codecs",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-18 20:22:31,14
13263108,[Python] Improve error message when object of wrong type is given,"In the PyArrow {{unwrap_xxx}} functions, the error messages when an object of the wrong type is given are not very informative (e.g. ""Could not unwrap Buffer from the passed Python object."").",pull-request-available,['Python'],ARROW,Improvement,Major,2019-10-18 13:41:36,2
13263099,[Java] Improve the performance of comparing two blocks of heap data,"Implement methods to compare data word by word, instead of byte by byte.
Benchmarks shows that there is a 4.5x performance improvement:

ByteFunctionHelpersBenchmarks.builtInByteArrayEquals  avgt    5  437.504  1.120  ns/op
ByteFunctionHelpersBenchmarks.byteArrayEquals         avgt    5   97.700  0.178  ns/op",pull-request-available,['Java'],ARROW,Improvement,Major,2019-10-18 12:54:01,7
13263087,[Java] Suppor linear dictionary encoder,"For many scenarios, the distribution of dictionary entries is highly skewed. In other words, a few dictionary entries occurs much more frequently than others. If we can sort the dictionary by the non-increasing order of entry frequencies, and compare each value to encode from the beginning of the dictionary, we get the following benefits:

1)      We need no extra memory space or data structure.
2)      The search is extremely efficient, as we are likely to find a match in the first few entries of the dictionary.

This is the basic idea behind the linear dictionary encoder. When the scenario is right (highly skewed dictionary distribution), it outperforms both search based encoder and hash table based encoders. 
",pull-request-available,['Java'],ARROW,New Feature,Major,2019-10-18 11:46:16,7
13262999,[Java] Create utility class for populating vector values used for test purpose only,"There is a lot of verbosity in the construction of Arrays for testing purposes (multiple lines of setSafe(...) or set(...).
We should start adding a utility class to make test setup clearer and more concise, note this class should be located in arrow-vector test package and could be used in other modules testing by adding dependency:

{{<dependency>}}
{{<groupId>org.apache.arrow</groupId>}}
{{<artifactId>arrow-vector</artifactId>}}
{{<version>${project.version}</version>}}
{{<classifier>tests</classifier>}}
{{<type>test-jar</type>}}
{{<scope>test</scope>}}
{{</dependency>}}

Usage would be something like:
{quote}try (IntVector vector = new IntVector(vector, allocator)) {
ValueVectorPopulator.setVector(vector, 1, 2, null, 4, 5);
output = doSomethingWith(input);
assertThat(output).isEqualTo(expected);
}
{quote}",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-10-18 04:21:05,16
13262998,[C++] ValidateArray is out of sync with the ListArray IPC specification,"* It appears to check that null values take zero space
 * It still checks for a begin offset of 0 if the array isn't sliced (technically this doesn't seem necessary and it could be non-zero even if the array wasn't sliced.)
 * I think it also fails if an array is sliced to truncate it since it should compare length to data_extent instead of last_offset.",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-18 04:08:04,6
13262973,[Rust] Add FixedSizeList type,"Support FixedSizeList, which is required for integration testing",pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-10-17 23:30:05,12
13262955,[C++] Add gRPC version check,We need gRPC++ 1.17.0 or later.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-17 21:41:24,1
13262953,[Python] Support __sizeof__ protocol for Python objects,"It would be helpful if PyArrow objects implemented the `__sizeof__` protocol to give other libraries hints about how much data they have allocated.  This helps systems like Dask, which have to make judgements about whether or not something is cheap to move or taking up a large amount of space.",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-10-17 21:34:58,5
13262919,[Python] Pandas master build is failing (MultiIndex.levels change),"https://circleci.com/gh/ursa-labs/crossbow/3933?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

Caused by https://github.com/pandas-dev/pandas/pull/27242",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-17 18:10:42,5
13262900,[Python] create manylinux wheels for python3.8,"There are currently no wheels available in pypi for python3.8. This means it's not possible to install later versions of arrow from pypi as the latest uploaded source release is 0.14.0.

It would also be useful to upload source releases to pypi...",pull-request-available,['Python'],ARROW,Improvement,Major,2019-10-17 16:28:52,1
13262876,"ARROW-6917: [Archery][Release] Add support for JIRA curation, changelog generation and commit cherry-picking for maintenance releases ","For 0.14.1, I maintained this script by hand. It would be less failure-prone (maybe) to generate it based on the fix versions set in JIRA",pull-request-available,"['Archery', 'Developer Tools']",ARROW,Improvement,Major,2019-10-17 14:10:36,3
13262875,[Developer] Alphabetize task names in nightly Crossbow report,It's a nuisance to have related tasks appearing in different places in the list when they could appear next to each other if places in alphabetical order,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-10-17 14:08:20,14
13262874,"[Developer] Do not overwrite minor release version with merge script, even if not specified by committer","Not every committer knows to write ""$MAJOR_VERSION,$MINOR_VERSION"" for the fix version when merging",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-10-17 14:05:15,14
13262872,[R] Potential bug in compute.cc,"Just spotted this compiler warning which looks like a real issue:
{code}
compute.cpp: In function 'std::shared_ptr<arrow::ChunkedArray> ChunkedArray__Take(const std::shared_ptr<arrow::ChunkedArray>&, Rcpp::IntegerVector&)':
compute.cpp:141:15: warning: suggest parentheses around comparison in operand of '&' [-Wparentheses]
     if (min_i >= offset & max_i < offset + len) {
         ~~~~~~^~~~~~~~~
{code}

In C++ the ""&"" operator has a lower priority than comparisons. You probably want to use the ""&&"" operator instead.

cc [~romainfrancois] [~npr]",pull-request-available,['R'],ARROW,Bug,Major,2019-10-17 13:59:41,4
13262774,[Java] Extract a common base class for avro converter consumers,Currently Avro converter consumers have some common variables and methods which could be eliminated by extracting a common class.,pull-request-available,['Java'],ARROW,Sub-task,Major,2019-10-17 06:29:51,16
13262766,[Java] Provide composite comparator,"A composite comparator is a sub-class of VectorValueComparator that contains an array of inner comparators, with each comparator corresponding to one column for comparison. It can be used to support sort/comparison operations for VectorSchemaRoot/StructVector.

The composite comparator works like this: it first uses the first internal comparator (for the primary sort key) to compare vector values. If it gets a non-zero value, we just return it; otherwise, we use the second comparator to break the tie, and so on, until a non-zero value is produced by some internal comparator, or all internal comparators have been used. 
",pull-request-available,['Java'],ARROW,New Feature,Major,2019-10-17 04:15:11,7
13262735,[Python] pyarrow.parquet.read_table(...) takes up lots of memory which is not released until program exits,"I realize that when I read up a lot of Parquet files using pyarrow.parquet.read_table(...), my program's memory usage becomes very bloated, although I don't keep the table objects after converting them to Pandas DFs.

You can try this in an interactive Python shell to reproduce this problem:

```{python}
from tqdm import tqdm
from pyarrow.parquet import read_table

PATH = '/tmp/big.snappy.parquet'

for _ in tqdm(range(10)):
    read_table(PATH, use_threads=False, memory_map=False)
    (note that I'm not assigning the read_table(...) result to anything, so I'm not creating any new objects at all)

```

During the For loop above, if you view the memory usage (e.g. using htop program), you'll see that it keeps creeping up. Either the program crashes during the 10 iterations, or if the 10 iterations complete, the program will still occupy a huge amount of memory, although no objects are kept. That memory is only released when you exit() from Python.

This problem means that my compute jobs using PyArrow currently need to use bigger server instances than I think is necessary, which translates to significant extra cost.

",pull-request-available,"['C++', 'Python']",ARROW,Bug,Critical,2019-10-16 23:49:30,14
13262690,[C++] Add support for Bazel,"I would like to use Arrow in a C++ project that uses Bazel.



Would it be possible to add support for building Arrow using Bazel?",pull-request-available,['C++'],ARROW,New Feature,Major,2019-10-16 18:13:15,15
13262668,[Packaging][OSX] Nightly builds on MacOS are failing because of brew compile timeouts,"Home-brew in our packaging builds has recently started to compile the dependencies instead of installing precompiled binaries. I'm not sure what's the issue, perhaps it is because the too old Xcode.",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-10-16 16:37:34,4
13262632,[Python] Wheels broken after ARROW-6860 changes,I forgot to handle the .so bundling issues. ,pull-request-available,['Python'],ARROW,Bug,Major,2019-10-16 13:40:17,14
13262558,"[Python] to_pandas() not implemented on list<dictionary<values=string, indices=int32>","Hi,

{{pyarrow.Table.to_pandas()}} fails on an Arrow List Vector where the data vector is of type ""dictionary encoded string"". Here is the table schema as printed by pyarrow:
{code:java}
pyarrow.Table
encodedList: list<$data$: dictionary<values=string, indices=int32, ordered=0> not null> not null
  child 0, $data$: dictionary<values=string, indices=int32, ordered=0> not null
metadata
--------
OrderedDict() {code}
and the data (also attached in a file to this ticket)
{code:java}
<pyarrow.lib.ChunkedArray object at 0x7f7ea6a748b8>
[
  [

    -- dictionary:
      [
        ""a"",
        ""b"",
        ""c"",
        ""d""
      ]
    -- indices:
      [
        0,
        1,
        2
      ],

    -- dictionary:
      [
        ""a"",
        ""b"",
        ""c"",
        ""d""
      ]
    -- indices:
      [
        0,
        3
      ]
  ]
] {code}
and the exception I got
{code:java}
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-10-5f865bc01df1> in <module>
----> 1 df.to_pandas()

~/.local/share/virtualenvs/jupyter-BKbz0SEp/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()

~/.local/share/virtualenvs/jupyter-BKbz0SEp/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table._to_pandas()

~/.local/share/virtualenvs/jupyter-BKbz0SEp/lib/python3.6/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, categories, ignore_metadata)
    700 
    701     _check_data_column_metadata_consistency(all_columns)
--> 702     blocks = _table_to_blocks(options, table, categories)
    703     columns = _deserialize_column_index(table, all_columns, column_indexes)
    704 

~/.local/share/virtualenvs/jupyter-BKbz0SEp/lib/python3.6/site-packages/pyarrow/pandas_compat.py in _table_to_blocks(options, block_table, categories)
    972 
    973     # Convert an arrow table to Block from the internal pandas API
--> 974     result = pa.lib.table_to_blocks(options, block_table, categories)
    975 
    976     # Defined above

~/.local/share/virtualenvs/jupyter-BKbz0SEp/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.table_to_blocks()

~/.local/share/virtualenvs/jupyter-BKbz0SEp/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: Not implemented type for list in DataFrameBlock: dictionary<values=string, indices=int32, ordered=0> {code}
Note that the data vector itself can be loaded successfully by to_pandas.

It'd be great if this would be addressed in the next version of pyarrow. For now, is there anything I can do on my end to bypass this unimplemented conversion?

Thanks,

Razvan",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-16 08:36:45,14
13262525,[Java] Fix potential memory leak in ArrowWriter and several test classes,"ARROW-6040 fixed the problem that dictionary entries are required in IPC streams even when empty, which only writes dictionaries when there are at least one batch. In this way, if we write empty stream and invoke ArrowWriter#close, the dictionaries are not closed leading to memory leak (they are closed after the write operation), and its really hard to debug, this problem was found by {{TestArrowReaderWriter#testEmptyStreamInStreamingIPC}}when I tried to close allocator after the test.



Besides, there are several test classes have potential memory leak without closing allocator/vector/buf etc.",pull-request-available,['Java'],ARROW,Bug,Major,2019-10-16 06:51:48,16
13262496,[Java] Vector schema root should not share vectors,"Vector schema root should not share vectors. Otherwise, unexpectd behavior would happen. 

Please note that VectorSchemaRoot is not just a container for vectors, it is also a resource (it implements the AutoClosable interface), and it manages the life cycle of its inner vectors.

When two VectorSchemaRoots share vectors, something unexpected may happen. Consider the following scenario, which is frequently encountered in a SQL engine.

1. We create a batch:
VectorSchemaRoot oldBatch = ...

2. We add a vector to it, which results in a new batch
VectorSchemaRoot newBatch = oldBatch.addVector(vector);

3. We are done with the old batch, and release the resource
oldBatch.close();

4. We continue to use the new batch, but gets an exception, because some inner vectors have been released by the old batch. 

",pull-request-available,['Java'],ARROW,Bug,Major,2019-10-16 02:39:52,7
13262354,[Java] ComplexCopier enable FixedSizeList type & fix RangeEualsVisitor StackOverFlow,"i. Enable {{ComplexCopier}}copy {{FixedSizeListVector}} value, add related tests

ii. Fix {{RangeEqualsVisitor#compareFixedSizeListVectors}}StackOverFlow",pull-request-available,['Java'],ARROW,Improvement,Major,2019-10-15 12:42:36,16
13262347,[Java] Support copy operation for vector value comparators,"In this issue, we provide copy operations for vector value comparators. This operation creates another comparator with the same type and comparison logic.

This feature is useful in multi-threading scenarios where multiple threads uses the comparator to perform their own task. In this scenario, we have no way of making sure the compare method is thread safe. So a safe way is to create a new comparator for each thread. The copy operation will support this.

An immediate application of this is the parallel searcher for ordering semantics. ",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-10-15 12:00:04,7
13262290,[Java] Create prose documentation for using ValueVectors,"We should create documentation (in restructured text) for the library that demonstrates:

1. Basic construction of ValueVectors. Highlighting:

  * ValueVector lifecycle

  * Reading by rows using Readers (mentioning that it is not as efficient as direct access).

  * Populating with Writers

2. Reading and writing IPC stream format and file formats.",pull-request-available,"['Documentation', 'Java']",ARROW,Improvement,Major,2019-10-15 05:40:20,16
13262244,[C++] arrow::io header nvcc compiler warnings,"Seeing the following compiler warnings statically linking the arrow::io headers with nvcc:

{noformat}
arrow/install/include/arrow/io/file.h(189): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::MemoryMappedFile""

arrow/install/include/arrow/io/memory.h(98): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::MockOutputStream""

arrow/install/include/arrow/io/memory.h(116): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::FixedSizeBufferWriter""

arrow/install/include/arrow/io/file.h(189): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::MemoryMappedFile""

arrow/install/include/arrow/io/memory.h(98): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::MockOutputStream""

arrow/install/include/arrow/io/memory.h(116): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::FixedSizeBufferWriter""

arrow/install/include/arrow/io/file.h(189): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::MemoryMappedFile""

arrow/install/include/arrow/io/memory.h(98): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::MockOutputStream""

arrow/install/include/arrow/io/memory.h(116): warning: overloaded virtual function ""arrow::io::Writable::Write"" is only partially overridden in class ""arrow::io::FixedSizeBufferWriter""
{noformat}
",pull-request-available,['C++'],ARROW,Bug,Trivial,2019-10-14 22:52:19,2
13262243,[Python] Remove superfluous skipped timedelta test,"Now that we support timedelta / duration type, there is an old xfailed test that can be removed.",pull-request-available,['Python'],ARROW,Test,Major,2019-10-14 22:32:33,5
13262236,[Python][Flight] Make server-side RPC exceptions more friendly?,"Here is what an error looks like when a client RPC fails in the server

{code}
E   pyarrow.lib.ArrowException: Unknown error: gRPC returned unknown error, with message: a bytes-like object is required, not 'str'
E   In ../src/arrow/python/flight.cc, line 201, code: CheckPyError(). Detail: Python exception: TypeError
{code}

The ""line 201, code:"" business is added by -DARROW_EXTRA_ERROR_CONTEXT=ON so the normal use won't see this

It might be nice to re-raise the same exception type in the client with some extra context added to make clear that it is a server-side error",pull-request-available,"['FlightRPC', 'Python']",ARROW,Improvement,Major,2019-10-14 21:46:57,14
13262227,[C++] Support sending delta DictionaryBatch or replacement DictionaryBatch in IPC stream writer class,"I didn't see other JIRA issues about this, but this is one significant matter to have complete columnar format coverage in the C++ library.

This functionality will flow through to the various bindings, so it would be helpful to add unit tests to assert that things work correctly e.g. in Python from an end-user perspective",pull-request-available,['C++'],ARROW,New Feature,Major,2019-10-14 20:54:37,2
13262225,[Python] cannot create a chunked_array from dictionary_encoding result,"I've experienced a strange error raise when trying to apply `pa.chunked_array` directly on the indices of dictionary_encoding (code is below). Making a memory view solves the problem.
{code:python}
import pyarrow as pa
ca = pa.array(['a', 'a', 'b', 'b', 'c'])                                                                                           
fca = ca.dictionary_encode()                                                                                                       
fca.indices                                                                                                                        
<pyarrow.lib.Int32Array object at 0x1250fb888>
[
  0,
  0,
  1,
  1,
  2
]

pa.chunked_array([fca.indices])                                                                                                    
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-44-71ca3b877e1c> in <module>
----> 1 pa.chunked_array([fca.indices])

~/Projects/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.chunked_array()

~/Projects/miniconda3/envs/pyarrow/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Unexpected dictionary values in array of type int32

# with another memory view it's  OK
pa.chunked_array([fca.indices.view(fca.indices.type)])                 
Out[45]: 
<pyarrow.lib.ChunkedArray object at 0x12508dc78>
[
  [
    0,
    0,
    1,
    1,
    2
  ]
]
 {code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-14 20:39:39,5
13262204,[Python] pa.array() does not handle list of dicts with bytes keys correctly under python3,"It creates sub-arrays with nulls filled, instead of the provided values.

$ python

Python 3.6.8 (default, Jan 3 2019, 03:42:36) 
[GCC 8.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyarrow as pa
>>> pa.__version__
'0.15.0'
>>> a = pa.array([\{b""a"": [1, 2, 3]}])
>>> a
<pyarrow.lib.StructArray object at 0x7fdcb4c28168>
-- is_valid: all not null
-- child 0 type: list<item: int64>
 [
 null
 ]
>>> a = pa.array([\{""a"": [1, 2, 3]}])
>>> a
<pyarrow.lib.StructArray object at 0x7fdcb4c28108>
-- is_valid: all not null
-- child 0 type: list<item: int64>
 [
 [
 1,
 2,
 3
 ]
 ]



It works under python2.",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-14 18:46:41,2
13262196,[C++] Boost not found from the correct environment,"My local dev build started to fail, due to cmake founding a wrong boost (it found {{-- Found Boost 1.70.0 at /home/joris/miniconda3/lib/cmake/Boost-1.70.0}} while building in a different conda environment.

I can reproduce this with creating a new conda env from scratch following our documentation.

By specifying {{-DBOOST_ROOT=/home/joris/miniconda3/envs/arrow-dev/lib}} it works fine.",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-14 17:58:38,14
13262186,[Python] Reading parquet file with many columns becomes slow for 0.15.0,"Hi,



I just noticed that reading a parquet file becomes really slow after I upgraded to 0.15.0 when using pandas.



Example:

*With 0.14.1*
 In [4]: %timeit df = pd.read_parquet(path)
 2.02 s  47.5 ms per loop (mean  std. dev. of 7 runs, 1 loop each)

*With 0.15.0*
 In [5]: %timeit df = pd.read_parquet(path)
 22.9 s  478 ms per loop (mean  std. dev. of 7 runs, 1 loop each)



The file is about 15MB in size. I am testing on the same machine using the same version of python and pandas.



Have you received similar complain? What could be the issue here?



Thanks a lot.





Edit1:

Some profiling I did:

0.14.1:

!image-2019-10-14-18-12-07-652.png!



0.15.0:

!image-2019-10-14-18-10-42-850.png!

",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-14 17:07:38,14
13262177,[FlightRPC] Implement Criteria for ListFlights RPC / list_flights method,We should work through how to pass a custom Criteria to ListFlights,pull-request-available,"['C++', 'FlightRPC', 'Java', 'Python']",ARROW,Improvement,Major,2019-10-14 16:27:42,0
13262167,[Python] Memory leak in Table.to_pandas() when conversion to object dtype,"I upgraded from pyarrow 0.14.1 to 0.15.0 and during some testing my python interpreterran out of memory.

I narrowed the issue down to the pyarrow.Table.to_pandas() call, which appears to have a memory leak in the latest version. See details below to reproduce this issue.


{code:java}
import numpy as np
import pandas as pd
import pyarrow as pa

# create a table with one nested array column
nested_array = pa.array([np.random.rand(1000) for i in range(500)])
nested_array.type  # ListType(list<item: double>)
table = pa.Table.from_arrays(arrays=[nested_array], names=['my_arrays'])

# convert it to a pandas DataFrame in a loop to monitor memory consumption
num_iterations = 10000
# pyarrow v0.14.1: Memory allocation does not grow during loop execution
# pyarrow v0.15.0: ~550 Mb is added to RAM, never garbage collected
for i in range(num_iterations):
    df = pa.Table.to_pandas(table)


# When the table column is not nested, no memory leak is observed
array = pa.array(np.random.rand(500 * 1000))
table = pa.Table.from_arrays(arrays=[array], names=['numbers'])
# no memory leak:
for i in range(num_iterations):
    df = pa.Table.to_pandas(table){code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-14 15:10:01,2
13262161,[Python]Stale CColumn reference break Cython cimport pyarrow,"Traceback:

{code}
Error compiling Cython file:
------------------------------------------------------------
...
# under the License.

from __future__ import absolute_import

from libcpp.memory cimport shared_ptr
from pyarrow.includes.libarrow cimport (CArray, CBuffer, CColumn, CDataType,
^
------------------------------------------------------------

/lib/python3.7/site-packages/pyarrow/__init__.pxd:21:0: 'pyarrow/includes/libarrow/CColumn.pxd' not found

Error compiling Cython file:
------------------------------------------------------------
...
    cdef object wrap_tensor(const shared_ptr[CTensor]& sp_tensor)
    cdef object wrap_sparse_tensor_coo(
        const shared_ptr[CSparseTensorCOO]& sp_sparse_tensor)
    cdef object wrap_sparse_tensor_csr(
        const shared_ptr[CSparseTensorCSR]& sp_sparse_tensor)
    cdef object wrap_column(const shared_ptr[CColumn]& ccolumn)
                                                   ^
------------------------------------------------------------

/lib/python3.7/site-packages/pyarrow/__init__.pxd:39:52: unknown type in template argument

Error compiling Cython file:
------------------------------------------------------------
...

from pyarrow cimport Int64ArrayBuilder
^
------------------------------------------------------------

/Users/uwe/.ipython/cython/_cython_magic_3eb31dd63fb578b618cc8e98a60dbdf5.pyx:2:0: 'pyarrow/Int64ArrayBuilder.pxd' not found
---------------------------------------------------------------------------
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-14 13:42:02,8
13262134,[C++][Python] Empty table with dictionary-columns raises ArrowNotImplementedError,"h2. Abstract
As a pyarrow user, I would expect that I can create an empty table out of every schema that I created via pandas. This does not work for dictionary types (e.g. {{""category""}} dtypes).

h2. Test Case
This code:

{code:python}
import pandas as pd
import pyarrow as pa

df = pd.DataFrame({""x"": pd.Series([""x"", ""y""], dtype=""category"")})
table = pa.Table.from_pandas(df)
schema = table.schema
table_empty = schema.empty_table()  # boom
{code}

produces this exception:

{noformat}
Traceback (most recent call last):
  File ""arrow_bug.py"", line 8, in <module>
    table_empty = schema.empty_table()
  File ""pyarrow/types.pxi"", line 860, in __iter__
  File ""pyarrow/array.pxi"", line 211, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 36, in pyarrow.lib._sequence_to_array
  File ""pyarrow/error.pxi"", line 86, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: Sequence converter for type dictionary<values=string, indices=int8, ordered=0> not implemented
{noformat}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2019-10-14 10:40:45,5
13262059,[Java] Enhance TransferPair related parameters check and tests,"{{TransferPair}} related param checks in different classes have potential problems:

i. {{copyValueSafe}}do not check from index, if from > valueCount, no error is shown.

ii. {{splitAndTansfer}}has no indices check in classes like {{VarcharVector}}

iii. {{splitAndTranser}}indices check in classes like UnionVector is not correct(Preconditions.checkArgument(startIndex + length <= valueCount)), should check params separately.

iv. some assert usages should be replaced with {{Preconditions}}.

v. should add more UT to cover corner cases.",pull-request-available,['Java'],ARROW,Bug,Major,2019-10-14 04:31:49,16
13262042,"[C++] Dictionary ""delta"" building logic in builder_dict.h produces invalid arrays",Looking at the unit tests for the dictionary delta logic -- the arrays that are produced by subsequent invocations of {{Finish}} yield DictionaryArray instances with partial dictionaries. I think this is misleading (I was surprised to find this while working on ARROW-6861). We should develop a different approach to computing dictionary delta. ,pull-request-available,['C++'],ARROW,Bug,Blocker,2019-10-14 01:19:06,14
13261952,[FlightRPC][Java] Flight server can hang JVM on shutdown,"I noticed this while working on Flight integration tests. FlightService keeps an executor, which can hang the JVM on shutdown if the executor itself is not shut down.

It's used by Handshake and DoPut.

I think this surfaced because I wrote an AuthHandler that threw an exception.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-10-12 14:52:54,0
13261947,[Java] Improve the performance of calculating hash code for struct vector,"Improve the performance of hashCode(int) method for StructVector:
1. We can get the child vectors directly, so there is no need to get the name from the child vector and then use the name to get the vector. 
2. The child vectors cannot be null, so there is no need to check it.

The performance improvement depends on the complexity of the hash algorithm. For computational intensive hash algorithms, the improvement can be small; while for simple hash algorithms, the improvement can be notable. ",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-10-12 12:33:08,7
13261941,[Java] Improve the performance of comparing an ArrowBuf against a byte array,"We change the way of comparing an ArrowBuf against a byte array from byte wise comparison to comparison by long/int/byte.

Benchmark shows that there is a 6.7x performance improvement. ",pull-request-available,['Java'],ARROW,Improvement,Major,2019-10-12 11:50:31,7
13261940,[C++] bz2 / zstd tests not enabled,"When passing {{-DARROW_WITH_ZSTD=on}} and {{-DARROW_WITH_BZ2=on}}, the relevant tests in {{arrow-compression-test}} and {{arrow-io-compressed-test}} are still not enabled.",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-12 11:38:51,14
13261926,[Java] Provide parallel searcher,"For scenarios where the vector is large and the a low response time is required, we need to search the vector in parallel to improve the responsiveness.

This issue tries to provide a parallel searcher for the equality semantics (the support for ordering semantics is not ready yet, as we need a way to distribute the comparator).

The implementation is based on multi-threading.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-10-12 09:24:42,7
13261844,[Python] arrow-0.15.0 reading arrow-0.14.1-output Parquet dictionary column: Failure reading column: IOError: Arrow error: Invalid: Resize cannot downsize,"I'll need to jump through hoops to upload the (seemingly-valid) Parquet file that triggers this bug. In the meantime, here's the error I get, reading the Parquet file with read_dictionary=true. I'll start with the stack trace:

{{Failure reading column: IOError: Arrow error: Invalid: Resize cannot downsize}}

{{#0 0x0000000000b9fffd in __cxa_throw ()}}
 {{#1 0x00000000004ce7b5 in parquet::PlainByteArrayDecoder::DecodeArrow (this=0x555556612e50, num_values=67339, null_count=0, valid_bits=0x7f39a764b780 '\377' <repeats 200 times>..., valid_bits_offset=748544,}}
 \{{ builder=0x555556616330) at /src/apache-arrow-0.15.0/cpp/src/parquet/encoding.cc:886}}
 {{#2 0x000000000046d703 in parquet::internal::ByteArrayDictionaryRecordReader::ReadValuesSpaced (this=0x555556616260, values_to_read=67339, null_count=0)}}
 \{{ at /src/apache-arrow-0.15.0/cpp/src/parquet/column_reader.cc:1314}}
 {{#3 0x00000000004a13f8 in parquet::internal::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)6> >::ReadRecordData (this=0x555556616260, num_records=67339)}}
 \{{ at /src/apache-arrow-0.15.0/cpp/src/parquet/column_reader.cc:1096}}
 {{#4 0x0000000000493876 in parquet::internal::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)6> >::ReadRecords (this=0x555556616260, num_records=815883)}}
 \{{ at /src/apache-arrow-0.15.0/cpp/src/parquet/column_reader.cc:875}}
 {{#5 0x0000000000413955 in parquet::arrow::LeafReader::NextBatch (this=0x555556615640, records_to_read=815883, out=0x7ffd4b5afab0) at /src/apache-arrow-0.15.0/cpp/src/parquet/arrow/reader.cc:413}}
 {{#6 0x0000000000412081 in parquet::arrow::FileReaderImpl::ReadColumn (this=0x5555566067a0, i=7, row_groups=..., out=0x7ffd4b5afab0) at /src/apache-arrow-0.15.0/cpp/src/parquet/arrow/reader.cc:218}}
 {{#7 0x00000000004121b0 in parquet::arrow::FileReaderImpl::ReadColumn (this=0x5555566067a0, i=7, out=0x7ffd4b5afab0) at /src/apache-arrow-0.15.0/cpp/src/parquet/arrow/reader.cc:223}}
 {{#8 0x0000000000405fbd in readParquet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()}}

And now a report of my gdb adventures:

In Arrow 0.15.0, when reading a particular dictionary column ({{read_dictionaries=true}}) with815883 rows that was written by Arrow 0.14.1, {{arrow::Dictionary32Builder<arrow::BinaryType>::AppendIndices(...)}} is called twice (once with 493568 values, once with 254976 values); and then {{PlainByteArrayDecoder::DecodeArrow()}} is called. (I'm a novice; I don't know why this column comes in three batches.) On first {{AppendIndices()}} call, the buffer capacity is equal to the number of values. On second call, that's no longer the case: the buffer grows using {{BufferBuilder::GrowByFactor}}, so its capacity is987136.

But there's a bug: the 987136-capacity buffer is in {{Dictionary32Builder::indices_builder_}}; so987136 is stored in {{Dictionary32Builder::indices_builder_.capacity_}}. {{Dictionary32Builder::capacity_}} does not change when {{AppendIndices()}}is called. (Dictionary32Builder behaves like a proxy for its {{indices_builder_}}; but its {{capacity()}} method is not virtual, so things are messy.)

So {{builder.capacity_}} is 0. Then comes the final batch of67339 values, via {{DecodeArrow()}}. It calls {{builder->Reserve(num_values)}}. But {{builder->Reserve(num_values)}} tries to increase the capacity from 0 (its wrong, cached value) to {{length_ + num_values}} (815883). Since{{indicies_builder->capacity_}} is987136, that's a downsize  which throws an exception.

The only workaround I can find: use {{read_dictionaries=false}}.

This affects Python, too.

I've attached a patch that fixes the issue for my file. I don't know how to formulate a reduction, though, so I haven't contributed unit tests. I'm also not certain how FinishInternal is meant to work, so this definitely needs expert review. (FinishInternal was _definitely_ buggy before my patch; after my patch it _might_ be buggy but I don't know.)",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-10-11 17:20:12,14
13261841,[Python] Only link libarrow_flight.so to pyarrow._flight,See BEAM-8368. We need to find a strategy to mitigate protobuf static linking issues with teh Beam community,pull-request-available,['Python'],ARROW,Bug,Major,2019-10-11 17:14:48,14
13261828,[CI][Nightly] Disable docker layer caching for CircleCI tasks,CircleCI builds are failing because the layer caching is not available for free plans.,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-10-11 15:22:25,3
13261819,[Python][C++] Segfault for dictionary_encode on empty chunked_array (edge case),"a reproducer is here :
{code:python}
import pyarrow as pa
aa = pa.chunked_array([pa.array(['a', 'b', 'c'])])
aa[:0].dictionary_encode()  
# Segmentation fault: 11
{code}
For pyarrow=0.14, I could not reproduce. 
 I use a conda version : ""pyarrow 0.15.0 py37hdca360a_0 conda-forge""",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-11 14:50:16,2
13261813,[C++][Python][Flight] Implement Flight middleware,C++/Python side of ARROW-6074,pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2019-10-11 14:20:41,0
13261765,[Java] Support vector and dictionary encoder use different hasher for calculating hashCode,"Hasher interface was introduce in ARROW-5898 and now have two different implementations ({{MurmurHasher and SimpleHasher}}) and it could be more in the future.

And currently {{ValueVector#hashCode}}and {{DictionaryHashTable}}only use {{SimpleHasher}}for calculating hashCode. This issue enables them to use different hasher or even user-defined hasher for their own use cases.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-10-11 10:31:48,16
13261727,[Java] Jdbc converter support Null type,"java.sql.Types.Null is not supported yet since we have no NullVector in Java code before.

This could be implemented after ARROW-1638 merged (IPC roundtrip for null type).",pull-request-available,['Java'],ARROW,New Feature,Major,2019-10-11 08:15:20,16
13261643,[C++] Specify -std=c++11 instead of -std=gnu++11 when building,"Relevant discussion:

[https://lists.apache.org/thread.html/5807e65d865c1736b3a7a32653ca8bb405d719eb13b8a10b6fe0e904@%3Cdev.arrow.apache.org%3E]

in addition to

set(CMAKE_CXX_STANDARD 11)

, we also need to

set(CMAKE_CXX_EXTENSIONS OFF)

in order to turn off compiler-specific extensions (with GCC, it's -std=gnu++11)



This is supposed to be a no-op, because Arrow builds fine with other compilers (Clang-LLVM / MSCV). But opening this bug to track any issues with flipping the switch.

",pull-request-available,['C++'],ARROW,Bug,Minor,2019-10-10 20:06:29,14
13261625,[C++] Add a range_expression interface to Iterator<>,"Iterator provides the Visit() method for visiting each element, but idiomatic C++ uses a range for loop",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-10 18:35:05,6
13261489,[C++][Parquet][Python] List<scalar type> columns read broken with 0.15.0,"Columns of type {{array<primitive type>}} (such as `array<int32>`, `array<int64>`...) are not readable anymore using {{pyarrow == 0.15.0}} (but were with {{pyarrow == 0.14.1}}) when the original writer of the parquet file is {{parquet-mr 1.9.1}}.

{code}
import pyarrow.parquet as pq

pf = pq.ParquetFile('sample.gz.parquet')

print(pf.read(columns=['profile_ids']))
{code}

with 0.14.1:

{code}
pyarrow.Table
profile_ids: list<element: int64>
 child 0, element: int64

...
{code}

with 0.15.0:

{code}
Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
 File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyarrow/parquet.py"", line 253, in read
 use_threads=use_threads)
 File ""pyarrow/_parquet.pyx"", line 1131, in pyarrow._parquet.ParquetReader.read_all
 File ""pyarrow/error.pxi"", line 78, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column data for field 0 with type list<item: int64> is inconsistent with schema list<element: int64>
{code}

I've tested parquet files coming from multiple tables (with various schemas) created with `parquet-mr`, couldn't read any `array<primitive type>` column anymore.



I _think_ the bug was introduced with[this commit|[https://github.com/apache/arrow/commit/06fd2da5e8e71b660e6eea4b7702ca175e31f3f5]].

I think the root of the issue comes from the fact that `parquet-mr` writes the inner struct name as `""element""` by default (see [here|[https://github.com/apache/parquet-mr/blob/b4198be200e7e2df82bc9a18d54c8cd16aa156ac/parquet-column/src/main/java/org/apache/parquet/schema/ConversionPatterns.java#L33]]), whereas `parquet-cpp` (or `pyarrow`?) assumes `""item""` (see for example [this test|[https://github.com/apache/arrow/blob/c805b5fadb548925c915e0e130d6ed03c95d1398/python/pyarrow/tests/test_schema.py#L74]]). The round-tripping tests write/read in pyarrow only obviously won't catch this.



",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-10-10 04:44:23,14
13261446,[Website] Jekyll error building website,"I'm getting the following error locally on a fresh checkout and {{bundle install --path vendor/bundle}}

{code}
$ bundle exec jekyll serve
Configuration file: /home/wesm/code/arrow-site/_config.yml
            Source: /home/wesm/code/arrow-site
       Destination: build
 Incremental build: disabled. Enable with --incremental
      Generating... 
jekyll 3.8.4 | Error:  wrong number of arguments (given 2, expected 1)
{code}

Never seen this so not sure how to debug",pull-request-available,['Website'],ARROW,Bug,Major,2019-10-09 22:26:30,1
13261431,"[Java] Add APIs to read and write ""custom_metadata"" field of IPC file footer",Access custom_metadata from ARROW-6836,pull-request-available,['Java'],ARROW,New Feature,Major,2019-10-09 20:37:37,16
13261428,[C++/Python] access File Footer custom_metadata,Access custom_metadata from ARROW-6836,pull-request-available,"['C++', 'Python']",ARROW,New Feature,Minor,2019-10-09 20:35:34,14
13261423,[Archery][CMake] Restore ARROW_LINT_ONLY  ,"This is used by developers to fasten the cmake build creation and loosen the required installed toolchains (notably libraries). This was yanked because ARROW_LINT_ONLY effectively exit-early and doesn't generate `compile_commands.json`.

Restore this option, but ensure that archery toggles accordingly to the usage of iwyu or clang-tidy.",pull-request-available,['Archery'],ARROW,Bug,Minor,2019-10-09 20:12:22,13
13261417,[C++] Pin gtest to 1.8.1 to triage failing Appveyor / MSVC build,"Not sure what introduced this

https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/27992011/job/cj247lfl0s48xrsl

{code}
LINK: command ""C:\PROGRA~2\MI0E91~1.0\VC\bin\amd64\link.exe /nologo src\arrow\CMakeFiles\arrow-public-api-test.dir\public_api_test.cc.obj /out:release\arrow-public-api-test.exe /implib:release\arrow-public-api-test.lib /pdb:release\arrow-public-api-test.pdb /version:0.0 /machine:x64 /NODEFAULTLIB:LIBCMT /INCREMENTAL:NO /subsystem:console release\arrow_testing.lib release\arrow.lib C:\Miniconda36-x64\envs\arrow\Library\lib\double-conversion.lib C:\Miniconda36-x64\envs\arrow\Library\lib\libcrypto.lib C:\Miniconda36-x64\envs\arrow\Library\lib\libssl.lib C:\Miniconda36-x64\envs\arrow\Library\lib\brotlienc-static.lib C:\Miniconda36-x64\envs\arrow\Library\lib\brotlidec-static.lib C:\Miniconda36-x64\envs\arrow\Library\lib\brotlicommon-static.lib C:\Miniconda36-x64\envs\arrow\Library\bin\aws-cpp-sdk-config.lib C:\Miniconda36-x64\envs\arrow\Library\bin\aws-cpp-sdk-transfer.lib C:\Miniconda36-x64\envs\arrow\Library\bin\aws-cpp-sdk-s3.lib C:\Miniconda36-x64\envs\arrow\Library\bin\aws-cpp-sdk-core.lib C:\Miniconda36-x64\envs\arrow\Library\lib\double-conversion.lib C:\Miniconda36-x64\envs\arrow\Library\lib\libboost_filesystem.lib C:\Miniconda36-x64\envs\arrow\Library\lib\libboost_system.lib googletest_ep-prefix\src\googletest_ep\lib\gtest_main.lib googletest_ep-prefix\src\googletest_ep\lib\gtest.lib googletest_ep-prefix\src\googletest_ep\lib\gmock.lib C:\Miniconda36-x64\envs\arrow\Library\lib\libcrypto.lib C:\Miniconda36-x64\envs\arrow\Library\lib\aws-c-event-stream.lib C:\Miniconda36-x64\envs\arrow\Library\lib\aws-c-common.lib BCrypt.lib Kernel32.lib Ws2_32.lib C:\Miniconda36-x64\envs\arrow\Library\lib\aws-checksums.lib mimalloc_ep\src\mimalloc_ep\lib\mimalloc-1.0\mimalloc-static-release.lib Ws2_32.lib kernel32.lib user32.lib gdi32.lib winspool.lib shell32.lib ole32.lib oleaut32.lib uuid.lib comdlg32.lib advapi32.lib /MANIFEST /MANIFESTFILE:release\arrow-public-api-test.exe.manifest"" failed (exit code 1120) with the following output:
public_api_test.cc.obj : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: static void __cdecl testing::Test::SetUpTestSuite(void)"" (__imp_?SetUpTestSuite@Test@testing@@SAXXZ) referenced in function ""public: static void (__cdecl*__cdecl testing::internal::SuiteApiResolver<class testing::Test>::GetSetUpCaseOrSuite(char const *,int))(void)"" (?GetSetUpCaseOrSuite@?$SuiteApiResolver@VTest@testing@@@internal@testing@@SAP6AXXZPEBDH@Z)
public_api_test.cc.obj : error LNK2019: unresolved external symbol ""__declspec(dllimport) public: static void __cdecl testing::Test::TearDownTestSuite(void)"" (__imp_?TearDownTestSuite@Test@testing@@SAXXZ) referenced in function ""public: static void (__cdecl*__cdecl testing::internal::SuiteApiResolver<class testing::Test>::GetTearDownCaseOrSuite(char const *,int))(void)"" (?GetTearDownCaseOrSuite@?$SuiteApiResolver@VTest@testing@@@internal@testing@@SAP6AXXZPEBDH@Z)
release\arrow-public-api-test.exe : fatal error LNK1120: 2 unresolved externals
[205/515] Building CXX object src\arrow\CMakeFiles\arrow-array-test.dir\array_test.cc.obj
[206/515] Building CXX object src\arrow\CMakeFiles\arrow-array-test.dir\array_dict_test.cc.obj
ninja: build stopped: subcommand failed.
(arrow) C:\projects\arrow\cpp\build>goto scriptexit 
{code}",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-10-09 19:30:28,14
13261402,[R][CI] Add crossbow job for full R autobrew macOS build,"I have a separate nightly job that runs this on multiple R versions, but it would be nice to be able to have crossbow check this on a PR. As it turns out, the ARROW_S3 feature doesn't work with autobrew in practice--aws-sdk-cpp doesn't seem to ship static libs via Homebrew, so the autobrew packaging doesn't work, even though the formula builds and {{brew audit}} is clean.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2019-10-09 18:17:42,4
13261401,[R] Implement Codec::IsAvailable,New in ARROW-6631,pull-request-available,['R'],ARROW,Improvement,Minor,2019-10-09 18:15:20,4
13261387,[R] Update R macOS/Windows builds for change in cmake compression defaults,ARROW-6631 changed the defaults for including compressions but did not update these build scripts.,pull-request-available,['R'],ARROW,Improvement,Major,2019-10-09 17:24:48,4
13261278,[C++] Rework CSV reader IO around readahead iterator,"Following ARROW-6764, we should try to remove the custom ReadaheadSpooler and use the generic readahead iteration facility instead. This will require reworking the blocking / chunking logic to mimick what is done in the JSON reader.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-09 09:55:54,2
13261250,[C++][Python][R] Support metadata in the feather format?,"This might need to wait / could be enabled by the feather v2 (ARROW-5510), but thought to open a specific issue about it: do we want to support saving metadata in feather files?

With Parquet files, you can have file-level metadata (which we currently use to eg store the pandas_metadata). I think it would be useful to have a similar mechanism for Feather files.

A use case where this came up is in GeoPandas where we would like to store the Coordinate Reference System identifier of the geometry data inside the file, to avoid needing a sidecar file just for that.

In a v2 world (using the IPC format), I suppose this could be the metadata of the Schema.",feather,"['C++', 'Python', 'R']",ARROW,Improvement,Major,2019-10-09 06:56:20,14
13261223,[Website] merge_pr.py is published,We can download merge_pr.py at https://arrow.apache.org/merge_pr.py,pull-request-available,['Website'],ARROW,Improvement,Minor,2019-10-09 01:29:44,1
13261195,[C++][Parquet] Do not require Thrift compiler when building (but still require library),"Building Thrift from source carries extra toolchain dependencies (bison and flex). If we check in the files produced by compiling parquet.thrift, then the EP can be simplified to only build the Thrift C++ library and not the compiler. This also results in a simpler build for third parties",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-08 20:53:32,4
13260932,[Ruby] Ensure requiring suitable MSYS2 package,"C:\Users\Dominic E Sisneros>gem update red-arrow
Updating installed gems
Updating red-arrow
Temporarily enhancing PATH for MSYS/MINGW...
Installing required msys2 packages: mingw-w64-x86_64-arrow
warning: mingw-w64-x86_64-arrow-0.14.0-2 is up to date -- skipping
Building native extensions. This could take a while...
ERROR:  Error installing red-arrow:
        ERROR: Failed to build gem native extension.

    current directory: c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/red-arrow-0.15.0/ext/arrow
c:/Ruby26-x64/bin/ruby.exe -I c:/Ruby26-x64/lib/ruby/site_ruby/2.6.0 -r ./siteconf20191007-20416-1m6q3x3.rb extconf.rb
checking --enable-debug-build option... no
checking C++ compiler... x86_64-w64-mingw32-g++
checking g++ version... 9.2 (gnu++14)
checking for --enable-debug-build option... no
checking for -Wall option to compiler... yes
checking for -Waggregate-return option to compiler... yes
checking for -Wcast-align option to compiler... yes
checking for -Wextra option to compiler... no
checking for -Wformat=2 option to compiler... yes
checking for -Winit-self option to compiler... yes
checking for -Wlarger-than-65500 option to compiler... yes
checking for -Wmissing-declarations option to compiler... yes
checking for -Wmissing-format-attribute option to compiler... yes
checking for -Wmissing-include-dirs option to compiler... yes
checking for -Wmissing-noreturn option to compiler... yes
checking for -Wmissing-prototypes option to compiler... yes
checking for -Wnested-externs option to compiler... no
checking for -Wold-style-definition option to compiler... yes
checking for -Wpacked option to compiler... yes
checking for -Wp,-D_FORTIFY_SOURCE=2 option to compiler... yes
checking for -Wpointer-arith option to compiler... yes
checking for -Wswitch-default option to compiler... yes
checking for -Wswitch-enum option to compiler... yes
checking for -Wundef option to compiler... yes
checking for -Wout-of-line-declaration option to compiler... no
checking for -Wunsafe-loop-optimizations option to compiler... yes
checking for -Wwrite-strings option to compiler... yes
checking for Homebrew... no
checking for arrow... yes
checking for arrow-glib... yes
creating Makefile

current directory: c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/red-arrow-0.15.0/ext/arrow
make ""DESTDIR="" clean

current directory: c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/red-arrow-0.15.0/ext/arrow
make ""DESTDIR=""
generating arrow-x64-mingw32.def
compiling arrow.cpp
compiling converters.cpp
compiling raw-records.cpp
raw-records.cpp: In lambda function:
raw-records.cpp:61:52: error: 'class arrow::Column' has no member named 'chunks'
   61 |             for (const auto array : chunked_array->chunks()) {
      |                                                    ^~~~~~
make: *** [Makefile:236: raw-records.o] Error 1

make failed, exit code 2

Gem files will remain installed in c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/red-arrow-0.15.0 for inspection.
Results logged to c:/Ruby26-x64/lib/ruby/gems/2.6.0/extensions/x64-mingw32/2.6.0/red-arrow-0.15.0/gem_make.out
Gems updated: red-arrow",pull-request-available,['Ruby'],ARROW,Bug,Major,2019-10-07 17:17:42,1
13260918,[C++] Segfault deserializing ListArray containing null/empty list,"The following code segfaults for me (Windows and Linux, pyarrow 0.15):


{code:java}
import pyarrow as pa
from io import BytesIO
x = b'\xdc\x00\x00\x00\x10\x00\x00\x00\x0c\x00\x0e\x00\x06\x00\r\x00\x08\x00\x00\x00\x0c\x00\x00\x00\x00\x00\x03\x00\x10\x00\x00\x00\x00\x01\n\x00\x0c\x00\x00\x00\x08\x00\x04\x00\n\x00\x00\x00\x08\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x18\x00\x00\x00\x00\x00\x12\x00\x18\x00\x14\x00\x13\x00\x12\x00\x0c\x00\x00\x00\x08\x00\x04\x00\x12\x00\x00\x00\x14\x00\x00\x00\x14\x00\x00\x00`\x00\x00\x00\x00\x00\x0c\x01\\\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x18\x00\x00\x00\x00\x00\x12\x00\x18\x00\x14\x00\x00\x00\x13\x00\x0c\x00\x00\x00\x08\x00\x04\x00\x12\x00\x00\x00\x14\x00\x00\x00\x14\x00\x00\x00\x14\x00\x00\x00\x00\x00\x00\x05\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf0\xff\xff\xff\x06\x00\x00\x00$data$\x00\x00\x04\x00\x04\x00\x04\x00\x00\x00\x10\x00\x00\x00exchangeCodeList\x00\x00\x00\x00\xcc\x00\x00\x00\x14\x00\x00\x00\x00\x00\x00\x00\x0c\x00\x16\x00\x0e\x00\x15\x00\x10\x00\x04\x00\x0c\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00\x10\x00\x00\x00\x00\x03\n\x00\x18\x00\x0c\x00\x08\x00\x04\x00\n\x00\x00\x00\x14\x00\x00\x00h\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
r = pa.RecordBatchStreamReader(BytesIO(x))
r.read_all()
{code}
I *think* what should happen instead is that I should get a Table with a single column named ""exchangeCodeList"", where the column is a ChunkedArray with a single chunk, where that chunk is a ListArray containing just a single element (a null). Failing that (i.e. if the bytestring is actually malformed), pyarrow should maybe throw an error instead of segfaulting?

I'm not 100% sure how the bytestring was generated: I think it comes from a Java-based server. I can deserialize the server response fine if all the records have at least one element in the ""exchangeCodeList"" column, but not if at least one of them is null. I've tried to reproduce the failure by generating the bytestring with pyarrow but can't trigger the segfault.

",pull-request-available,['C++'],ARROW,Bug,Critical,2019-10-07 15:44:39,2
13260910,[CI] [Rust] Migrate Travis Rust job to Github Actions,"Should be easy, as it's already using docker-compose.",pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Improvement,Major,2019-10-07 14:58:46,2
13260869,[Rust] [DataFusion] Aggregate queries are slower with new physical query plan,"executing direct from logical plan:
{code:java}
 aggregate_query_no_group_by                                                                             
                        time:   [13.096 us 13.187 us 13.294 us]
                        change: [-88.712% -88.554% -88.398%] (p = 0.00 < 0.05)
                        Performance has improved.
Found 5 outliers among 100 measurements (5.00%)
  5 (5.00%) high mildaggregate_query_group_by                                                                             
                        time:   [44.153 us 44.816 us 45.541 us]
                        change: [-77.984% -77.485% -77.009%] (p = 0.00 < 0.05)
                        Performance has improved.
Found 4 outliers among 100 measurements (4.00%)
  4 (4.00%) high mildaggregate_query_group_by_with_filter                                                                            
                        time:   [75.383 us 76.076 us 76.817 us]
                        change: [-72.345% -71.811% -71.097%] (p = 0.00 < 0.05)
                        Performance has improved.
Found 16 outliers among 100 measurements (16.00%)
{code}
executing from physical plan:
{code:java}
aggregate_query_no_group_by                                                                            
                        time:   [112.13 us 113.63 us 115.26 us]
                        change: [-3.8005% -2.0342% -0.3584%] (p = 0.02 < 0.05)
                        Change within noise threshold.
Found 1 outliers among 100 measurements (1.00%)
  1 (1.00%) high mildaggregate_query_group_by                                                                            
                        time:   [195.12 us 198.63 us 202.39 us]
                        change: [-1.3814% +1.0612% +3.5732%] (p = 0.40 > 0.05)
                        No change in performance detected.
Found 6 outliers among 100 measurements (6.00%)
  4 (4.00%) high mild
  2 (2.00%) high severeaggregate_query_group_by_with_filter                                                                            
                        time:   [270.69 us 272.18 us 273.63 us]
                        change: [-2.1583% -0.4877% +1.0161%] (p = 0.56 > 0.05)
                        No change in performance detected.
 {code}",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-10-07 11:39:28,10
13260799,[Rust] Arrow source release tarball is missing benchmarks,Arrow source tarball is missing benchmarks but references the benchmarks in the Cargo.toml causing the release script to fail. We need to include the benchmarks in the source tarball in the future to avoid having to manually release the crates.,pull-request-available,['Rust'],ARROW,Bug,Blocker,2019-10-06 23:43:22,4
13260791,[C++] Add CMake option to build libraries targeting a C++14 or C++17 toolchain environment,Such option would cause public APIs involving e.g. {{string_view}} to use the STL versions rather than our vendored backports,pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-06 20:43:54,14
13260781,[CI] [Rust] Improve build times by caching dependencies in the Docker image,Improve Rust build times by caching dependencies in the Docker image,pull-request-available,"['CI', 'Rust']",ARROW,Test,Major,2019-10-06 17:11:03,10
13260618,[R] Arrow C++ binary packaging for Linux,"Our current installation experience on Linux isn't ideal. Unless you've already installed the Arrow C++ library, when you install the R package, you get a shell that tells you to install the C++ library. That was a useful approach to allow us to get the package on CRAN, which makes it easy for macOS and Windows users to install, but it doesn't improve the installation experience for Linux users. This is an impediment to adoption of arrow not only by users but also by package maintainers who might want to depend on arrow.

macOS and Windows have a better experience because at installation time, the configure scripts download and statically link a prebuilt C++ library. CRAN bundles the whole thing up and delivers that as a binary R package.

Python wheels do a similar thing: they're binaries that contain all external dependencies. And there are pyarrow wheels for Linux. This suggests that we could do something similar for R: build a generic Linux binary of the C++ library and download it in the R package configure script at install time.

I experimented with using the Arrow C++ binaries included in the Python wheels in R. See discussion at the end of ARROW-5956. This worked on macOS (not useful for R, but it proved the concept) and almost worked on Linux, but it turned out that the ""manylinux2010"" standard is too archaic to work with contemporary Rcpp.

Proposal: do a similar workflow to what the manylinux2010 pyarrow build does, just with slightly more modern compiler/settings. Publish that C++ binary package to bintray. Then download it in the R configure scriptif a local/system package isn't found.

Once we have a basic version working, test against various distros on [R-hub|https://builder.r-hub.io/advanced]to make sure we're solid everywhere and/or ensure the current fallback behavior when we encounter a distro that this doesn't work for. If necessary, we can make multiple flavors of this C++ binary for debian, centos, etc.",pull-request-available,['R'],ARROW,Improvement,Critical,2019-10-04 18:06:52,4
13260479,[Python] Automatically box bytes/buffer-like values yielded from `FlightServerBase.do_action` in Result values,This will help with less boilerplate for server implementations,pull-request-available,['Python'],ARROW,Improvement,Major,2019-10-04 00:36:02,14
13260448,[CI] Migrate Travis CI lint job to GitHub Actions,Depends on ARROW-5802. As far as I can tell GitHub Actions jobs run more or less immediately so this will give more prompt feedback to contributors,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-10-03 21:33:34,3
13260446,"[CI] Decommission ""C++ with clang 7 and system packages"" Travis CI job","Now that this is running in GitHub Actions, we can probably skip it in Travis CI?

Any other barriers to turning this off and saving the CI build time?",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-10-03 21:22:18,2
13260348,[C++][R] Move filter and take code from Rcpp to C++ library,"Followup to ARROW-3808 and some other previous work. Of particular interest:
 * Filter and Take methods for ChunkedArray, in r/src/compute.cpp
 * Methods for that and some other things that apply Array and ChunkedArray methods across the columns of a RecordBatch or Table, respectively
 * RecordBatch__select and Table__select to take columns",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-03 17:50:09,4
13260342,[C++] Build minimal core Arrow libraries without any Boost headers,We have a couple of places where these are used. It would be good to be able to build without any Boost headers available,pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-03 16:43:18,14
13260332,"[C++] Improve and consolidate ARROW_CHECK, DCHECK macros","Currently we have multiple macros like {{DCHECK_EQ}} and {{DCHECK_LT}} which check various comparisons but don't report anything about their operands. Furthermore, the ""stream to assertion"" pattern for appending extra info has proven fragile. I propose a new unified macro which can capture operands of comparisons and report them:

{code:cpp}
  int three = 3;
  int five = 5;
  DCHECK(three == five, ""extra: "", 1, 2, five);
{code}

Results in check failure messages like:
{code}
F1003 11:12:46.174767  4166 logging_test.cc:141]  Check failed: three == five
  LHS: 3
  RHS: 5
extra: 125
{code}
",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-10-03 15:23:45,6
13260302,[C++] Support DurationType in Cast kernel,"Currently, duration is not yet supported in basic cast operations (using the python binding from ARROW-5855, currently from my branch, not yet merged):

{code}
In [25]: arr = pa.array([1, 2])

In [26]: arr.cast(pa.duration('s'))  
...
ArrowNotImplementedError: No cast implemented from int64 to duration[s]

In [27]: arr = pa.array([1, 2], pa.duration('s'))  

In [28]: arr.cast(pa.duration('ms'))
...
ArrowNotImplementedError: No cast implemented from duration[s] to duration[ms]
{code}
",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-03 13:32:01,5
13260206,[C++] [Python] Proposal for several Array utility functions,"Hi,

We developed several utilities that computes / accesses certain properties of Arrays and wonder if they make sense to get them into the upstream (into both the C++ API and pyarrow) and assuming yes, where is the best place to put them?

Maybe I have overlooked existing APIs that already do the same.. in that case please point out.



1/ ListLengthFromListArray(ListArray&)

Returns lengths of lists in a ListArray, as a Int32Array (or Int64Array for large lists). For example:

[[1, 2, 3], [], None] => [3, 0, 0] (or [3, 0, None], but we hope the returned array can be converted to numpy)



2/ GetBinaryArrayTotalByteSize(BinaryArray&)

Returns the total byte size of a BinaryArray (basically offset[len - 1] - offset[0]).

Alternatively, a BinaryArray::Flatten() -> Uint8Array would work.



3/ GetArrayNullBitmapAsByteArray(Array&)

Returns the array's null bitmap as a UInt8Array (which can be efficiently converted to a bool numpy array)



4/ GetFlattenedArrayParentIndices(ListArray&)

Makes a int32 array of the same length as the flattened ListArray. returned_array[i] == j means i-th element in the flattened ListArray came from j-th list in the ListArray.


For example [[1,2,3], [], None, [4,5]] => [0, 0, 0, 3, 3]

",pull-request-available,"['C++', 'Python']",ARROW,Wish,Minor,2019-10-02 22:42:07,14
13260160,[C++] Filter kernel returns invalid data when filtering with an Array slice,"See ARROW-3808. This failing test reproduces the issue:
{code:java}
--- a/cpp/src/arrow/compute/kernels/filter_test.cc
+++ b/cpp/src/arrow/compute/kernels/filter_test.cc
@@ -151,6 +151,12 @@ TYPED_TEST(TestFilterKernelWithNumeric, FilterNumeric) {
   this->AssertFilter(""[7, 8, 9]"", ""[null, 1, 0]"", ""[null, 8]"");
   this->AssertFilter(""[7, 8, 9]"", ""[1, null, 1]"", ""[7, null, 9]"");
 
+  this->AssertFilterArrays(
+    ArrayFromJSON(this->type_singleton(), ""[7, 8, 9]""),
+    ArrayFromJSON(boolean(), ""[0, 1, 1, 1, 0, 1]"")->Slice(3, 3),
+    ArrayFromJSON(this->type_singleton(), ""[7, 9]"")
+  );
+
{code}
{code:java}
arrow/cpp/src/arrow/testing/gtest_util.cc:82: Failure
Failed

@@ -2, +2 @@
+0

[  FAILED  ] TestFilterKernelWithNumeric/9.FilterNumeric, where TypeParam = arrow::DoubleType (0 ms)
{code}


",pull-request-available,['C++'],ARROW,Bug,Major,2019-10-02 18:13:58,4
13260141,[Packaging][Python] Missing pytest dependency from conda and wheel builds,"Multiple python packaging nightlies are failing:

{code}
Failed Tasks:
- conda-osx-clang-py36:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-osx-clang-py36
- conda-osx-clang-py37:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-osx-clang-py37
- conda-win-vs2015-py36:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-win-vs2015-py36
- wheel-manylinux1-cp27mu:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-wheel-manylinux1-cp27mu
- conda-linux-gcc-py27:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-linux-gcc-py27
- wheel-osx-cp27m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-wheel-osx-cp27m
- docker-spark-integration:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-circle-docker-spark-integration
- wheel-win-cp35m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-appveyor-wheel-win-cp35m
- conda-win-vs2015-py37:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-win-vs2015-py37
- conda-linux-gcc-py37:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-linux-gcc-py37
- wheel-manylinux2010-cp27mu:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-wheel-manylinux2010-cp27mu
- conda-linux-gcc-py36:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-linux-gcc-py36
- wheel-win-cp37m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-appveyor-wheel-win-cp37m
- wheel-win-cp36m:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-appveyor-wheel-win-cp36m
- gandiva-jar-osx:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-travis-gandiva-jar-osx
- conda-osx-clang-py27:
  URL: https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-10-02-0-azure-conda-osx-clang-py27
{code}

Because of missing, recently introduced pytest-lazy-fixture test dependency:
{code}
+ pytest -m 'not requires_testing_data' --pyargs pyarrow
============================= test session starts ==============================
platform linux -- Python 3.7.3, pytest-5.2.0, py-1.8.0, pluggy-0.13.0
hypothesis profile 'default' ->
database=DirectoryBasedExampleDatabase('$SRC_DIR/.hypothesis/examples')
rootdir: $SRC_DIR
plugins: hypothesis-4.38.1
collected 1437 items / 1 errors / 3 deselected / 5 skipped / 1428 selected

==================================== ERRORS ====================================
______________________ ERROR collecting tests/test_fs.py _______________________
../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol/lib/python3.7/site-packages/pyarrow/tests/test_fs.py:91:
in <module>
pytest.lazy_fixture('localfs'),
E AttributeError: module 'pytest' has no attribute 'lazy_fixture'
=============================== warnings summary ===============================
$PREFIX/lib/python3.7/site-packages/_pytest/mark/structures.py:324
$PREFIX/lib/python3.7/site-packages/_pytest/mark/structures.py:324:
PytestUnknownMarkWarning: Unknown pytest.mark.s3 - is this a typo? You
can register custom marks to avoid this warning - for details, see
https://docs.pytest.org/en/latest/mark.html
PytestUnknownMarkWarning,

-- Docs: https://docs.pytest.org/en/latest/warnings.html
!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!
{code}",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2019-10-02 16:36:37,3
13260124,[CI][Travis] Download Minio quietly,To remove verbose output https://travis-ci.org/pitrou/arrow/jobs/592577525#L191,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-10-02 14:52:14,3
13260123,[C++][Dataset] End to End dataset integration test case,"1. Create a DataSource from a knowndirectory and a PartitionScheme. 
2. Create a Dataset from the previous DataSource. 
3. Request a ScannerBuilder from previous Dataset. 
4. Add filter expression to ScannerBuilder (and other options). 
5. Finalize into a Scan operation. 
6. Materialize into an arrow::Table.

",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-10-02 14:44:32,13
13260122,[C++][Dataset] Implement dataset::Scan to Table helper function,The Scan interface exposes classes (ScanTask/Iterator) which are not of interest to all callers. This would implement`Status Scan::Materialize(std::shared_ptr<arrow::Table>* out)` so consumers cancall this function instead of consuming and dispatching the streaming interface.,dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-10-02 14:43:09,13
13260090,[C++] Add readahead iterator,"This could replace the current ad-hoc ReadaheadSpooler, at least for JSON.
CSV currently uses non-zero padding, but it could switch to the same strategy as JSON (i.e. keep track of partial / completion blocks).
",pull-request-available,['C++'],ARROW,Improvement,Major,2019-10-02 10:56:25,2
13260058,[C++] JSON reader segfaults on newline,"Using the {{SampleRecord.jl}} attachment from ARROW-6737, I notice that trying to read this file on master results in a segfault:

{code}
In [1]: from pyarrow import json 
   ...: import pyarrow.parquet as pq 
   ...:  
   ...: r = json.read_json('SampleRecord.jl') 
WARNING: Logging before InitGoogleLogging() is written to STDERR
F1002 09:56:55.362766 13035 reader.cc:93]  Check failed: (string_view(*next_partial).find_first_not_of("" \t\n\r"")) == (string_view::npos) 
*** Check failure stack trace: ***
Aborted (core dumped)
{code}

while with 0.14.1 this works fine:

{code}
In [24]: from pyarrow import json 
    ...: import pyarrow.parquet as pq 
    ...:  
    ...: r = json.read_json('SampleRecord.jl')                                                                                                                                                                     

In [25]: r                                                                                                                                                                                                         
Out[25]: 
pyarrow.Table
_type: string
provider_name: string
arrival: timestamp[s]
berthed: timestamp[s]
berth: null
cargoes: list<item: struct<movement: string, product: string, volume: string, volume_unit: string, buyer: null, seller: null>>
  child 0, item: struct<movement: string, product: string, volume: string, volume_unit: string, buyer: null, seller: null>
      child 0, movement: string
      child 1, product: string
      child 2, volume: string
      child 3, volume_unit: string
      child 4, buyer: null
      child 5, seller: null
departure: timestamp[s]
eta: null
installation: null
port_name: string
next_zone: null
reported_date: timestamp[s]
shipping_agent: null
vessel: struct<beam: null, build_year: null, call_sign: null, dead_weight: null, dwt: null, flag_code: null, flag_name: null, gross_tonnage: null, imo: string, length: int64, mmsi: null, name: string, type: null, vessel_type: null>
  child 0, beam: null
  child 1, build_year: null
  child 2, call_sign: null
  child 3, dead_weight: null
  child 4, dwt: null
  child 5, flag_code: null
  child 6, flag_name: null
  child 7, gross_tonnage: null
  child 8, imo: string
  child 9, length: int64
  child 10, mmsi: null
  child 11, name: string
  child 12, type: null
  child 13, vessel_type: null

In [26]: pa.__version__                                                                                                                                                                                            
Out[26]: '0.14.1'
{code}

cc [~apitrou] [~bkietz]",json pull-request-available,['C++'],ARROW,Bug,Major,2019-10-02 08:03:27,2
13260046,[Rust] Travis CI builds not respecting rust-toolchain,"Travis builds recently started failing with a Rust ICE (Internal Compiler Error) which has been reported to the Rust compiler team ([https://github.com/rust-lang/rust/issues/64908]).

",pull-request-available,['Rust'],ARROW,Bug,Major,2019-10-02 05:50:54,10
13260037,[C++] JSON: improve error message when column changed type,"When a column accidentally changes type in a JSON file (which is not supported), it would be nice to get the column name that gives this problem in the error message.

---

I am trying to parse a simple json file. While doing so, am getting the error {{JSON parse error: A column changed from string to number}}

{code}

from pyarrow import json
r = json.read_json('dummy.jl')

{code}

",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-02 03:40:39,6
13260014,[Release] Install ephemeral node/npm/npx in release verification script,Installing node with nvm isn't terribly difficult; to add this to the release verification script would make it easier for people to verify more of the release,pull-request-available,"['Developer Tools', 'JavaScript']",ARROW,Improvement,Major,2019-10-02 00:28:41,14
13260010,"[Python] Creating csv.ParseOptions() causes ""Windows fatal exception: access violation"" with Visual Studio 2017","I encountered this when trying to verify the release with MSVC 2017. It may be particular to this machine or build (though it's 100% reproducible for me). I will check the Windows wheels to see if it occurs there, too

{code}
(C:\tmp\arrow-verify-release\conda-env)  python
Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 22:01:29) [MSC v.1900 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyarrow.csv as pc
>>> pc.ParseOptions()
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-01 23:27:43,14
13259985,[Release] Improvements to Windows release verification script,"* Only build dynamic libraries (we don't need the static libs to verify, and I got ""compiler is out of heap space"" errors when I built locally just now, will have to investigate that some more later)
* Maybe some other things",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-10-01 19:38:53,14
13259938,[CI] ccache doesn't cache on Travis-CI,Looks like our ccache setup on Travis is broken. This is seen by the cache hits statistics printed at the end of each job.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-10-01 15:13:54,2
13259913,[Python] Silence S3 error logs by default,"Some errors get displayed at the end of {{test_fs.py}}:
{code}
$ python -m pytest --tb=native pyarrow/tests/test_fs.py 
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.7.3, pytest-5.1.1, py-1.8.0, pluggy-0.12.0
hypothesis profile 'dev' -> max_examples=10, database=DirectoryBasedExampleDatabase('/home/antoine/arrow/dev/python/.hypothesis/examples')
rootdir: /home/antoine/arrow/dev/python, inifile: setup.cfg
plugins: timeout-1.3.3, repeat-0.8.0, hypothesis-3.82.1, lazy-fixture-0.5.2, forked-1.0.2, xdist-1.28.0
collected 90 items                                                                                                                                                         

pyarrow/tests/test_fs.py ..........................................................................................                                                  [100%]

============================================================================ 90 passed in 1.33s ============================================================================
19-08-07T01-59-21Z
vary : Origin
x-amz-request-id : 15C988FDC2359A9C
x-xss-protection : 1; mode=block
[ERROR] 2019-10-01 13:29:28.597 AWSClient [139765563750208] HTTP response code: 409
Exception name: BucketAlreadyOwnedByYou
Error message: Your previous request to create the named bucket succeeded and you already own it.
9 response headers:
accept-ranges : bytes
content-length : 366
content-security-policy : block-all-mixed-content
content-type : application/xml
date : Tue, 01 Oct 2019 13:29:28 GMT
server : MinIO/RELEASE.2019-08-07T01-59-21Z
vary : Origin
x-amz-request-id : 15C988FDC317620E
x-xss-protection : 1; mode=block

[etc.]
{code}
",pull-request-available,['Python'],ARROW,Bug,Minor,2019-10-01 13:30:37,2
13259863,[Python] Conversion of non-ns timestamp array to numpy gives wrong values,"{code}
In [25]: np_arr = np.arange(""2012-01-01"", ""2012-01-06"", int(1e6)*60*60*24, dtype=""datetime64[us]"")                                                                                                                 

In [26]: np_arr                                                                                                                                                                                                    
Out[26]: 
array(['2012-01-01T00:00:00.000000', '2012-01-02T00:00:00.000000',
       '2012-01-03T00:00:00.000000', '2012-01-04T00:00:00.000000',
       '2012-01-05T00:00:00.000000'], dtype='datetime64[us]')

In [27]: arr = pa.array(np_arr)                                                                                                                                                                                    

In [28]: arr                                                                                                                                                                                                       
Out[28]: 
<pyarrow.lib.TimestampArray object at 0x7f0b2ef07ee8>
[
  2012-01-01 00:00:00.000000,
  2012-01-02 00:00:00.000000,
  2012-01-03 00:00:00.000000,
  2012-01-04 00:00:00.000000,
  2012-01-05 00:00:00.000000
]

In [29]: arr.type                                                                                                                                                                                                  
Out[29]: TimestampType(timestamp[us])

In [30]: arr.to_numpy()                                                                                                                                                                                            
Out[30]: 
array(['1970-01-16T08:09:36.000000000', '1970-01-16T08:11:02.400000000',
       '1970-01-16T08:12:28.800000000', '1970-01-16T08:13:55.200000000',
       '1970-01-16T08:15:21.600000000'], dtype='datetime64[ns]')
{code}

So it seems to simply interpret the integer microsecond values as nanoseconds when converting to numpy.",pull-request-available,['Python'],ARROW,Bug,Major,2019-10-01 10:07:48,5
13259691,[C++] Completely remove usage of boost::filesystem (except in hdfs_internal),"We probably want to do this at some point. It will remove one fragility in the build system (currently, we select whether we need {{boost::filesystem}} or not depending on the selected build options, then set the {{ARROW_WITH_BOOST_FILESYSTEM}} macro that triggers compiling different versions of a couple utility functions).",pull-request-available,['C++'],ARROW,Task,Major,2019-09-30 14:57:44,2
13259690,[C++] Remove usage of boost::filesystem::path from arrow/io/hdfs_internal.cc,This is the only usage of boost::filesystem in this file. Low priority,pull-request-available,['C++'],ARROW,Improvement,Minor,2019-09-30 14:55:06,2
13259679,[Python] Unable to delete closed MemoryMappedFile on Windows,"{code:java}
import os
import pyarrow as pa

# Create a file and memory-map it
file_name = 'path-to-a-new-file'
mmap = pa.create_memory_map(file_name, 100)

# or open an existing file
# file_name = 'path-to-an-existing-file'
# mmap = pa.memory_map(file_name)

# close it
mmap.close()
mmap.closed  # True

# try to delete it (can't delete until the python interpreter is killed)
os.remove(file_name)  # PermissionError

# Note: opening an existing file as `pa.input_stream` works as expected{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-09-30 14:11:42,2
13259638,[Java] Fix problems with current union comparison logic,"There are some problems with the current union comparison logic. For example:
 1. For type check, we should not require fields to be equal. It is possible that two vectors' value ranges are equal but their fields are different.",pull-request-available,['Java'],ARROW,Bug,Major,2019-09-30 10:57:37,7
13259556,[Rust] [DataFusion] Aggregate expressions get evaluated repeatedly,"There is a design flaw in the new aggregate expression traits and implementations where the input to the aggregate expression gets evaluated against the whole batch once for each row in the batch. For example, if the batch has 1024 rows then the expression gets evaluated 1024 times instead of once.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2019-09-30 00:07:01,10
13259551,[C++] Suppress sign-compare warning with g++ 9.2.1,"This was introduced by ARROW-6402 but removed accidentally by ARROW-5935.

{noformat}
In file included from ../src/arrow/status.h:24,
                 from ../src/arrow/memory_pool.h:26,
                 from ../src/arrow/buffer.h:28,
                 from ../src/arrow/array.h:28,
                 from ../src/arrow/array/builder_union.h:25,
                 from ../src/arrow/array/builder_union.cc:18:
../src/arrow/array/builder_union.cc: In constructor 'arrow::BasicUnionBuilder::BasicUnionBuilder(arrow::MemoryPool*, arrow::UnionMode::type, const std::vector<std::shared_ptr<arrow::ArrayBuilder> >&, const std::shared_ptr<arrow::DataType>&)':
../src/arrow/util/logging.h:86:55: error: comparison of integer expressions of different signedness: 'std::vector<arrow::ArrayBuilder*>::size_type' {aka 'long unsigned int'} and 'signed char' [-Werror=sign-compare]
   86 | #define ARROW_CHECK_LT(val1, val2) ARROW_CHECK((val1) < (val2))
      |                                                ~~~~~~~^~~~~~~~
../src/arrow/util/macros.h:43:52: note: in definition of macro 'ARROW_PREDICT_TRUE'
   43 | #define ARROW_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
      |                                                    ^
../src/arrow/util/logging.h:86:36: note: in expansion of macro 'ARROW_CHECK'
   86 | #define ARROW_CHECK_LT(val1, val2) ARROW_CHECK((val1) < (val2))
      |                                    ^~~~~~~~~~~
../src/arrow/util/logging.h:135:19: note: in expansion of macro 'ARROW_CHECK_LT'
  135 | #define DCHECK_LT ARROW_CHECK_LT
      |                   ^~~~~~~~~~~~~~
../src/arrow/array/builder_union.cc:63:3: note: in expansion of macro 'DCHECK_LT'
   63 |   DCHECK_LT(type_id_to_children_.size(), std::numeric_limits<int8_t>::max());
      |   ^~~~~~~~~
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-29 22:01:56,1
13259514,[Java] Implement quick sort in a non-recursive way to avoid stack overflow,"The current quick sort algorithm in implemented by a recursive algorithm. The problem is that for the worst case, the number of recursive layers is equal to the length of the vector.  For large vectors, this will cause stack overflow.

To solve this problem, we implement the quick sort algorithm as a non-recursive algorithm.",pull-request-available,['Java'],ARROW,Improvement,Critical,2019-09-29 12:05:50,7
13259392,[CI] [Rust] Set up Github Action to run Rust tests,Set up Github Action to run Rust tests,pull-request-available,"['CI', 'Rust']",ARROW,Improvement,Major,2019-09-27 23:17:57,10
13259278,[CI] Disable 3rdparty fuzzit nightly builds,"Docker-cpp-fuzzit docker-compose task fails currently, probably misses 
parameters like version, git object id and credentials. 
Disable it until we have a solid solution for running them regularly.",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2019-09-27 13:59:04,3
13259265,[C++] Add simpler static ctor for BufferOutputStream than the current Create function,Not a major rough edge but the current {{Create}} function strikes me as a bit awkward since a size and memory pool must be explicitly passed,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-27 12:54:09,2
13259247,[Java] Reduce the range of synchronized block when releasing an ArrowBuf,"When releasing an ArrowBuf, we will run the following piece of code:

  private int decrement(int decrement) {
    allocator.assertOpen();
    final int outcome;
    synchronized (allocationManager) {
      outcome = bufRefCnt.addAndGet(-decrement);
      if (outcome == 0) {
        lDestructionTime = System.nanoTime();
        allocationManager.release(this);
      }
    }
    return outcome;
  }

It can be seen that we need to acquire the lock for allocation manager lock, no matter if we need to release the buffer. In addition, the operation of decrementing refcount is only carried out after the lock is acquired. This leads to unnecessary resource contention, and may degrade performance. 

We propose to change the code like this:

  private int decrement(int decrement) {
    allocator.assertOpen();
    final int outcome;
    outcome = bufRefCnt.addAndGet(-decrement);
    if (outcome == 0) {
      lDestructionTime = System.nanoTime();
      synchronized (allocationManager) {
        allocationManager.release(this);
      }
    }
    return outcome;
  }

Note that this change can be dangerous, as it lies in the core of our code base, so we should be careful with it. On the other hand, it may have non-trivial performance implication. As far as I know, when a distributed task is getting closed, a large number of ArrowBuf will be closed simultaneously. If we reduce the range of the synchronization block, we can significantly improve the performance. 

What do you think?",pull-request-available,['Java'],ARROW,Improvement,Major,2019-09-27 11:37:09,7
13259235,[Java] Provide a uniform way to get vector name,"Currently, the getName method is defined in BaseValueVector, as an abstract class. However, some vector does not extend the BaseValueVector, like StructVector, UnionVector, ZeroVector.
In this issue, we move the method to ValueVector interface, the base interface for all vectors.
This makes it easier to get a vector's name without checking its type.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-09-27 10:40:28,7
13259217,[JAVA] Avro adapter benchmark only runs once in JMH,"The current {{AvroAdapterBenchmark}}actually only run once during JMH evaluation, since the decoder was consumed for the first time and the follow-up invokes will directly return.

To solve this, we use {{BinaryDecoder}}explicitly in benchmark and reset its inner stream first when the test method is invoked.",pull-request-available,['Java'],ARROW,Sub-task,Minor,2019-09-27 09:09:35,16
13259145,[CI] [Rust] New 1.40.0 nightly causing builds to fail,"So much for pinning the nightly version ... that doesn't work when there is a new major version of a nightly apparently.

Travis is now using:
{code:java}
rustc 1.40.0-nightly (37538aa13 2019-09-25) {code}
Despite rust-toolchain containing:
{code:java}
nightly-2019-07-30 {code}


",pull-request-available,"['CI', 'Rust']",ARROW,Bug,Major,2019-09-26 22:24:26,10
13259131,[R] Fix untested RecordBatchWriter case,Passing a data.frame to RecordBatchWriter$write() would trigger a segfault,pull-request-available,['R'],ARROW,Bug,Major,2019-09-26 20:39:51,4
13259069,[C++] Consolidate Filter and Expression classes,"There is unnecessary boilerplate required when using the Filter/Expression classes. Filter is no longer necessary; it (and FilterVector) can be replaced with Expression. Expression is sufficiently general that it can be subclassed to provide any custom functionality which would have been added through a GenericFilter (add some tests for this).

Additionally rows within RecordBatches yielded from a scan are not currently filtered using Expression::Evaluate(). (Add tests ensuring both row filtering and pruning obey Kleene logic)

Add some comments on the mechanism of {{Assume()}} too, and refactor it not to return a Result (its failure modes are covered by {{Validate()}})",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-26 14:23:39,6
13259067,[Java] Add JDBC adapter test to cover cases which contains some null values,"The current JDBC adapter tests only cover the cases that values are all non-null or all null.

However, the cases that ResultSet has some null values are not covered (ARROW-6709).",pull-request-available,['Java'],ARROW,Improvement,Major,2019-09-26 14:06:53,16
13259050,[JAVA] Jdbc adapter currentIndex should increment when value is null,"Currently, in several consumers, currentIndex only increments when {{ResultSet}}was not null.

However, if {{ResultSet}}contains null values, the Arrow vector valueCount is not correct.",pull-request-available,['Java'],ARROW,Bug,Major,2019-09-26 13:09:48,16
13259044,"[C++] ""cannot find -lboost_filesystem_static""","I'm trying a fresh build on another machine and get this error when using the {{boost-cpp}} conda package:

{code}
/usr/bin/ld.gold: error: cannot find -lboost_filesystem_static
/usr/bin/ld.gold: error: cannot find -lboost_system_static
{code}

Note that Boost static libraries are installed, but they are named {{libboost_filesystem.a}} and {{libboost_system.a}} (no ""_static"" suffix).",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-26 12:54:38,2
13259034,[Java] Improve the performance of JDBC adapters by using nullable information,"JDBC meta data has a field that indicates if a column can contain null. We can make use of this information when transforming jdbc data to arrow vectors.

In particular, if the column cannot have null, there is no need to call the JDBC API for each value to check if the last value is null. 

This will improve the performance of transforming JDBC data to arrow vectors. ",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-09-26 12:19:37,7
13259024,[Rust] [DataFusion] README has invalid github URL,"README has:

git clone [https://github/apache/arrow]

but should be:

git clone https://github.com/apache/arrow",pull-request-available,['Rust'],ARROW,Bug,Major,2019-09-26 11:44:42,10
13258991,[C++] Cast from timestamp to higher resolution does not check out of bounds timestamps,"When casting eg {{timestamp('s')}} to {{timestamp('ns')}}, we do not check for out of bounds timestamps, giving ""garbage"" timestamps in the result:

{code}
In [74]: a_np = np.array([""2012-01-01"", ""2412-01-01""], dtype=""datetime64[s]"")                                                                                                                                      

In [75]: arr = pa.array(a_np)                                                                                                                                                                                      

In [76]: arr                                                                                                                                                                                                       
Out[76]: 
<pyarrow.lib.TimestampArray object at 0x7f3d1f07cb88>
[
  2012-01-01 00:00:00,
  2412-01-01 00:00:00
]

In [77]: arr.cast(pa.timestamp('ns'))                                                                                                                                                                              
Out[77]: 
<pyarrow.lib.TimestampArray object at 0x7f3d1f07cfa8>
[
  2012-01-01 00:00:00.000000000,
  1827-06-13 00:25:26.290448384
]
{code}

Now, this is the same behaviour as numpy, so not sure we should do this. However, since we have a {{safe=True/False}}, I would expect that for {{safe=True}} we check this and for {{safe=False}} we do not check this.  
(numpy has a similiar {{casting='safe'}} but also does not raise an error in that case).
",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-26 08:08:47,5
13258981,[Packaging][Linux] Restore ARROW_VERSION environment variable,"{{ARROW_VERSION}} is needed to use correct download URL for RC.
",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-09-26 07:25:25,1
13258976,[C++][R] Lint failing on R cpp code,[See as an example https://travis-ci.org/apache/arrow/jobs/589772132#L695|https://travis-ci.org/apache/arrow/jobs/589772132#L695],pull-request-available,"['C++', 'Continuous Integration', 'R']",ARROW,Bug,Major,2019-09-26 07:00:31,4
13258823,[Rust] [DataFusion] Remove execution of logical plan,Remove execution of logical plan,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-25 14:02:39,10
13258822,[Rust] [DataFusion] Update integration tests to use physical plan,Update integration tests to use physical query plan (once all features are supported),pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-25 14:01:58,10
13258820,[Rust] [DataFusion] Update examples to use physical query plan,Update examples to use physical query plan,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-25 14:00:39,10
13258817,[Rust] [DataFusion] HashAggregate without GROUP BY should use SIMD,"Currently the implementation of HashAggregate in the new physical plan uses the same logic regardless of whether a grouping expression is used.

For the case where there is no grouping expression, such as ""SELECT SUM(a) FROM b"" we can use the compute kernels to perform an aggregate operation on each batch rather than iterating over each row and accumulating individual values.

This optimization already exists in the original implementation of aggregate queries direct from the logical plan.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-25 13:58:20,10
13258803,[Rust] [DataFusion] Query returns incorrect row count,"Creating a DataFrame using Pandas (0.25.1 + pyarrow 0.14.1) which includes NaN value as a last entry in the column results in wrong ResultBatch num_rows()

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2019-09-25 13:11:43,10
13258794,[C++/Python] S3 FileStat object's base_path and type depends on trailing slash,"The current behaviour is:

{code:python}
s3fs.create_dir('bucket/directory/')
stats = s3fs.get_target_stats(['bucket/directory/'])
stats[0].type == FileType.File
stats[0].base_name == '/'
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-25 12:35:13,2
13258692,[Python] Add unit tests that validate cross-compatibility with pyarrow.parquet when fastparquet is installed,This will help prevent such issues as ARROW-6678 from reocurring,pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-25 05:07:17,5
13258659,[RELEASE] autobrew license in LICENSE.txt is not acceptable,"{code}
This project includes code from the autobrew project.

* r/tools/autobrew and dev/tasks/homebrew-formulae/autobrew/apache-arrow.rb
  are based on code from the autobrew project.

Copyright: Copyright (c) 2017 - 2019, Jeroen Ooms.
All rights reserved.
Homepage: https://github.com/jeroen/autobrew
{code}

This code needs to be made available under a Category A license

https://apache.org/legal/resolved.html#category-a",pull-request-available,['R'],ARROW,Bug,Blocker,2019-09-24 23:18:27,4
13258657,[C++] Regression in Parquet file compatibility introduced by ARROW-3246,"I randomly discovered that this script fails after applying the patch for ARROW-3246

https://github.com/apache/arrow/commit/2ba0566b29312e84fafc987fd8dc9664748be96a

{code}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import fastparquet as fp

df = pd.util.testing.makeDataFrame()

pq.write_table(pa.table(df), 'test.parquet')

fp.ParquetFile('test.parquet')
{code}

with 

{code}
Traceback (most recent call last):
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/fastparquet/api.py"", line 110, in __init__
    with open_with(fn2, 'rb') as f:
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/fastparquet/util.py"", line 38, in default_open
    return open(f, mode)
NotADirectoryError: [Errno 20] Not a directory: 'test.parquet/_metadata'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 10, in <module>
    fp.ParquetFile('test.parquet')
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/fastparquet/api.py"", line 116, in __init__
    self._parse_header(f, verify)
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/fastparquet/api.py"", line 135, in _parse_header
    fmd = read_thrift(f, parquet_thrift.FileMetaData)
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/fastparquet/thrift_structures.py"", line 25, in read_thrift
    obj.read(pin)
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/fastparquet/parquet_thrift/parquet/ttypes.py"", line 1929, in read
    iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 0: invalid start byte
{code}

I don't recall making any metadata-related changes but I'm going to review the patch to see if I can narrow down where the problem is to see whether it's a bug with Arrow/parquet-cpp or with the third party library",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-09-24 23:02:03,14
13258635,[FlightRPC][C++] Document using Flight in C++,"Similarly to ARROW-6390 for Python, we should have C++ documentation for Flight.",pull-request-available,"['Documentation', 'FlightRPC']",ARROW,Bug,Major,2019-09-24 19:33:23,0
13258571,[Python] Fix or ignore the test warnings,"Currently when running the python tests, we have a bunch of warnings. Some of them can be ignored, some of them can be fixed. But it is better to do that explicitly, so that new warnings (which can be potentially important to see) get more attention.",pull-request-available,['Python'],ARROW,Bug,Minor,2019-09-24 14:35:45,5
13258537,[Java] Extract a common interface for dictionary builders,"We need a common interface for dictionary builders to support more sophisticated scenarios, like collecting dictionary statistics.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-09-24 11:57:34,7
13258371,[Rust] [DataFusion] Implement physical expression for binary expressions,"Implement comparison operators (<, <=, >, >=, =, !=) as well as binary operators AND and OR.",pull-request-available,[],ARROW,Sub-task,Major,2019-09-23 17:55:30,10
13258356,[Rust] [DataFusion] Implement CAST expression,Implement CAST expression,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-23 17:01:50,10
13258310,[Rust] [DataFusion] Implement numeric literal expressions,Implement numeric literal expressions,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-23 14:09:28,10
13258219,[C++] Add option to build without SSE4.2,Child task of ARROW-5381,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-23 04:22:22,14
13258217,[Java] Implement equals/approxEquals API for VectorSchemaRoot,"Currently with the new added visitor APIs(ARROW-6211), we could implement equals/approxEquals for VectorSchemaRoot.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-09-23 04:09:42,16
13258216,[Java] Implement APIs like slice to enhance VectorSchemaRoot,"Currently in Java Implementation there is no APIs like slice for record batch like C++/Python.

This issue is about to implement slice/getVector/addVector/removeVector.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-09-23 04:09:04,16
13258171,[Rust] [DataFusion] Minor docs update for 0.15.0 release,Minor docs update for 0.15.0 release,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2019-09-22 18:30:53,10
13258145,[Python] Filesystem bindings for S3,Follow-up work of ARROW-5494: [Python] Create FileSystem bindings,pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-22 11:18:51,3
13258124,[Developer] Add support for auto JIRA link on pull request,"https://lists.apache.org/thread.html/7bb9e646832390d207393f064d7934e54c6cb010e30ea9f39f3ed1ce@%3Cdev.arrow.apache.org%3E

I frequently do the following little bit bothersome steps for opening
JIRA tickets when I watch a GitHub pull-request:

1. Select the ""ARROW-XXXX"" text in the title and copy it
2. Open JIRA if I haven't open it
3. Select a ticket to open it
4. Alter the URL by pasting text that copied at the step-1
5. Hit the enter key

I think it is better if these steps become easier.

We already have a mechanism to inject a GitHub pull-request URL into
the corresponding JIRA ticket. How about making the similar mechanism
for the reverse link?  I guess it is possible to automate making a
comment of JIRA ticket URL to the pull-request when the ""ARROW-XXXX""
text is injected in the title field by using GitHub Actions.",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-09-21 22:53:11,1
13258109,[Python] to_pandas conversion removes timezone from type,"Calling {{to_pandas}} on a {{pyarrow.Array}} with a timezone aware timestamp type, removes the timezone in the resulting {{pandas.Series}}.

{code}
>>> import pyarrow as pa
>>> a = pa.array([1], type=pa.timestamp('us', tz='America/Los_Angeles'))
>>> a.to_pandas()
0   1970-01-01 00:00:00.000001
dtype: datetime64[ns]
{code}

Previous behavior from 0.14.1 of converting a {{pyarrow.Column}} {{to_pandas}} retained the timezone.
{code}
In [4]: import pyarrow as pa 
   ...: a = pa.array([1], type=pa.timestamp('us', tz='America/Los_Angeles'))  
   ...: c = pa.Column.from_array('ts', a) 

In [5]: c.to_pandas()                                                                                                        
Out[5]: 
0   1969-12-31 16:00:00.000001-08:00
Name: ts, dtype: datetime64[ns, America/Los_Angeles]
{code}",pull-request-available,['Python'],ARROW,Bug,Critical,2019-09-21 16:58:02,5
13258104,[R] Fix R conda job,ARROW-6214 touched the build scripts it uses and now the nightly job is failing.,pull-request-available,['Continuous Integration'],ARROW,Bug,Minor,2019-09-21 15:41:05,4
13258076,[Rust] [Integration] Create methods to test Arrow files against Integration JSON,"[~emkornfield@gmail.com]recommended that we use the integration IPC files. To be able to compare against the JSON files that are used, we need to be able to generate a JSON represention of Arrow data in Rust.

We can already do this for schemas, and this ticket is for supporting converting RecordBatch to JSON.",pull-request-available,"['Integration', 'Rust']",ARROW,Sub-task,Major,2019-09-21 04:06:15,12
13258063,"[R] print() methods for Table, RecordBatch, etc.","Inspired by tibble: show schema, head of data, etc.",pull-request-available,['R'],ARROW,Improvement,Major,2019-09-20 23:35:01,4
13258043,[C++] Can't build with g++ 4.8.5 on CentOS 7 by member initializer for shared_ptr,"{noformat}
% g++ --version
g++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
{noformat}

Error message:

{noformat}
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc: In instantiation of 'arrow::Status arrow::py::GetConverterFlat(const std::shared_ptr<arrow::DataType>&, bool, std::unique_ptr<arrow::py::SeqConverter>*) [with arrow::py::NullCoding null_coding = (arrow::py::NullCoding)1]':
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:1001:5:   required from here
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:864:7: error: conversion from 'std::nullptr_t' to non-scalar type 'std::shared_ptr<arrow::DecimalType>' requested
 class DecimalConverter
       ^
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:894:10: note: synthesized method 'arrow::py::DecimalConverter<(arrow::py::NullCoding)1>::DecimalConverter()' first required here 
     *out = std::unique_ptr<SeqConverter>(new TYPE_CLASS<null_coding>); \
          ^
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:915:5: note: in expansion of macro 'SIMPLE_CONVERTER_CASE'
     SIMPLE_CONVERTER_CASE(DECIMAL, DecimalConverter);
     ^
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc: In instantiation of 'arrow::Status arrow::py::GetConverterFlat(const std::shared_ptr<arrow::DataType>&, bool, std::unique_ptr<arrow::py::SeqConverter>*) [with arrow::py::NullCoding null_coding = (arrow::py::NullCoding)0]':
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:1004:5:   required from here
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:864:7: error: conversion from 'std::nullptr_t' to non-scalar type 'std::shared_ptr<arrow::DecimalType>' requested
 class DecimalConverter
       ^
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:894:10: note: synthesized method 'arrow::py::DecimalConverter<(arrow::py::NullCoding)0>::DecimalConverter()' first required here 
     *out = std::unique_ptr<SeqConverter>(new TYPE_CLASS<null_coding>); \
          ^
/root/rpmbuild/BUILD/apache-arrow-0.15.0/cpp/src/arrow/python/python_to_arrow.cc:915:5: note: in expansion of macro 'SIMPLE_CONVERTER_CASE'
     SIMPLE_CONVERTER_CASE(DECIMAL, DecimalConverter);
     ^
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 21:49:56,1
13258001,[Python] Faster boundschecking of dictionary indices when converting to Categorical,This was added at some point to fix a bug. I suspect we might want to move this check somewhere else rather than do it every time {{to_pandas}} is called,pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-20 18:00:37,14
13257918,[Python] chained access of ParquetDataset's metadata segfaults,"Creating and reading a parquet dataset:

{code}
table = pa.table({'a': [1, 2, 3]})

import pyarrow.parquet as pq
pq.write_table(table, '__test_statistics_segfault.parquet')
dataset = pq.ParquetDataset('__test_statistics_segfault.parquet')
dataset_piece = dataset.pieces[0]
{code}

If you access the metadata and a column's statistics in steps, this works fine:

{code}
meta = dataset_piece.get_metadata()
row = meta.row_group(0)
col = row.column(0)
{code}

but doing it chained in one step, this segfaults:

{code}
dataset_piece.get_metadata().row_group(0).column(0)
{code}

{{dataset_piece.get_metadata().row_group(0)}} still works, but additionally with {{.column(0)}} then it segfaults. ",parquet pull-request-available,['Python'],ARROW,Bug,Major,2019-09-20 10:38:48,5
13257818,[Packaging][RPM] Add support for CentOS 7 on aarch64,"apt:build rake task supports architecture to run [1], but it is not true
 for yum task.

[1] [https://github.com/apache/arrow/blob/master/dev/tasks/linux-packages/package-task.rb#L276]

It is useful yum task also supports architecture (ex. i386) too. (even though CentOS 6 i386 EOL reaches 2020/11)

",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-09-20 00:58:46,1
13257813,[C++] Zero-dependency default core build,This is a tracking JIRA for items relating to having few or no dependencies for minimal out-of-the-box builds,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 00:32:07,14
13257812,[C++] Do not build C++ command line utilities by default,"This means to change {{ARROW_BUILD_UTILITIES}} to be off by default. These are mostly used for integration testing, so building unit or integration tests should toggle this on automatically. ",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 00:31:15,2
13257811,[C++] Do not require glog for default build,We should change the default for {{ARROW_USE_GLOG}} to be off,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 00:30:12,2
13257810,[C++] Do not require flatbuffers or flatbuffers_ep to build,Flatbuffers is small enough that we can vendor {{flatbuffers/flatbuffers.h}} and check in the compiled files to make flatbuffers_ep unneeded,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 00:29:10,14
13257809,[C++] Do not require double-conversion for default build,"This library is only needed in core builds if

* ARROW_JSON=on or
* ARROW_CSV=on (option to be added) or
* ARROW_BUILD_TESTS=on 

The double conversion headers leak into 

* arrow/util/decimal.h",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 00:28:20,2
13257807,[C++] Do not build with any compression library dependencies by default,Numerous packaging scripts will have to be updated if we decide to do this. ,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-20 00:24:34,14
13257802,[Doc][C++] Document the FileSystem API,"In ARROW-6622, I was looking for a place in the docs to add about path normalization, and I couldn't find filesystem docs at all. ",pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Major,2019-09-19 23:27:54,2
13257760,[C++][R] SubTreeFileSystem path error on Windows,"On ARROW-6438, we got this error on Windows testing out the subtree:

{code}
> test_check(""arrow"")
  -- 1. Error: SubTreeFilesystem (@test-filesystem.R#86)  ------------------------
  Unknown error: Underlying filesystem returned path 'C:/Users/appveyor/AppData/Local/Temp/1/RtmpqWFbxi/working_dir/Rtmp2Dfa6d/file2904934312d/DESCRIPTION', which is not a subpath of 'C:/Users/appveyor/AppData/Local/Temp/1\RtmpqWFbxi/working_dir\Rtmp2Dfa6d\file2904934312d/'
  1: st_fs$GetTargetStats(c(""DESCRIPTION"", ""test"", ""nope"", ""DESC.txt"")) at testthat/test-filesystem.R:86
  2: map(fs___FileSystem__GetTargetStats_Paths(self, x), shared_ptr, class = FileStats)
  3: fs___FileSystem__GetTargetStats_Paths(self, x)
  
  == testthat results  ===========================================================
  [ OK: 992 | SKIPPED: 2 | WARNINGS: 0 | FAILED: 1 ]
{code}

Notice the mixture of forward slashes and backslashes in the paths so that they don't match up. 

I'm not sure which layer is doing the wrong thing.",filesystem pull-request-available,"['C++', 'R']",ARROW,Bug,Major,2019-09-19 19:08:00,4
13257744,[Rust][DataFusion] Examples for DataFusion are not executed in CI,"See the CI scripts, we already test the examples for the Arrow sub-crate",beginner pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2019-09-19 16:57:25,10
13257631,[Python] Reading a zero-size buffer can segfault,"Simplest reproducible code is:

{code}
pa.read_message(b'')
{code}

which gives a segfault. 

You can easily run into this interactively when eg by accident passing a already-read buffer to it, like:

{code}
serialized = pa.schema([('a', pa.int64())]).serialize().to_pybytes()
buffer = pa.BufferReader(serialized)
pa.read_message(buffer)
pa.read_message(buffer)
{code}

And for example, if you compare to {{read_schema}}, this gives an error on the second time / empty buffer:

{code}
>>> pa.read_schema(buffer)
>>> pa.read_schema(buffer)
...
ArrowInvalid: Tried reading schema message, was null or length 0
{code}

I know this is not proper usage of Buffer(Reader), but since it is easy to accidentally do this, we should try to protect users from this I think.",pull-request-available,['Python'],ARROW,Bug,Major,2019-09-19 09:46:54,2
13257509,[C++][Dataset] Implement FileSystemDataSourceDiscovery,DataSourceDiscovery is what allows InferingSchema and constructing a DataSource with PartitionScheme.,dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-09-18 21:14:20,13
13257497,[C++] Remove dependency on boost::filesystem,"See ARROW-2196 for details.
boost::filesystem should not be required for base functionality at least (including filesystems, probably).",pull-request-available,['C++'],ARROW,Wish,Major,2019-09-18 20:39:01,2
13257492,[C++] Add ARROW_FILESYSTEM=ON/OFF CMake configuration flag,"Building this code should not be required in order to take advantage of the columnar core (memory allocation, data structures, IPC)",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-18 20:31:58,2
13257470,[C++] Add minimal build Dockerfile example,This will also help developers test a minimal build configuration,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-18 19:06:04,14
13257439,[C++] Construct tree structure from std::vector<fs::FileStats>,This will be used by FileSystemDataSource for pushdown predicate pruning of branches.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-18 17:29:34,13
13257438,[C++] Add recursion depth control to fs::Selector,"This is similar to the recursive options, but also control the depth.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-09-18 17:20:20,13
13257415,[Doc] Add feature / implementation matrix,We have many different implementations and each implementation makes a different set of features available. It would be nice to have a top-level doc page making it clear which implementation supports what.,pull-request-available,['Documentation'],ARROW,Improvement,Major,2019-09-18 15:26:41,2
13257408,[Java] Improve JDBC adapter performance & add benchmark,"Add a performance test as well to get a baseline number, to avoid performance regression when we change related code.

",pull-request-available,['Java'],ARROW,Task,Critical,2019-09-18 15:00:59,16
13257405,[Java] Implement dictionary-encoded subfields for Union type,"Implement dictionary-encoded subfields for {{Union}}type. Each child vector could be encodable or not.



Meanwhile extra common logic into {{DictionaryEncoder}}as well as refactor List subfield encoding to keep consistent with {{Struct/Union}}type.

",pull-request-available,['Java'],ARROW,Sub-task,Major,2019-09-18 14:50:19,16
13257383,[Rust] [DataFusion] Implement SUM aggregate expression,Implement the SUM aggregate function in the new physical query plan,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-18 13:22:27,10
13257373,[Java] Sort the code for ApproxEqualsVisitor,"As a follow up issue of ARROW-6458, we finalize the code for ApproxEqualsVisitor.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-09-18 12:58:46,7
13257330,[Python] Segfault in test_pandas with Python 2.7,"I get a segfault in test_pandas with Python 2.7.

gdb stack trace (excerpt):
{code}
Thread 27 ""python"" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffb7fff700 (LWP 17725)]
0x00007fffcac1a9f9 in arrow::py::internal::PyDate_from_int (val=10957, unit=arrow::DateUnit::DAY, out=0x555555e1b9b0) at ../src/arrow/python/datetime.cc:229
229	  *out = PyDate_FromDate(static_cast<int32_t>(year), static_cast<int32_t>(month),
(gdb) bt
#0  0x00007fffcac1a9f9 in arrow::py::internal::PyDate_from_int (val=10957, unit=arrow::DateUnit::DAY, out=0x555555e1b9b0) at ../src/arrow/python/datetime.cc:229
#1  0x00007fffcabaed34 in arrow::Status arrow::py::ConvertDates<arrow::Date32Type>(arrow::py::PandasOptions const&, arrow::ChunkedArray const&, _object**)::{lambda(int, _object**)#1}::operator()(int, _object**) const (this=0x7fffb7ffde90, value=10957, out=0x555555e1b9b0) at ../src/arrow/python/arrow_to_pandas.cc:657
#2  0x00007fffcabaeb8c in arrow::Status arrow::py::ConvertAsPyObjects<arrow::Date32Type, arrow::Status arrow::py::ConvertDates<arrow::Date32Type>(arrow::py::PandasOptions const&, arrow::ChunkedArray const&, _object**)::{lambda(int, _object**)#1}&>(arrow::py::PandasOptions const&, arrow::ChunkedArray const&, arrow::Status arrow::py::ConvertDates<arrow::Date32Type>(arrow::py::PandasOptions const&, arrow::ChunkedArray const&, _object**)::{lambda(int, _object**)#1}&, _object**)::{lambda(int const&, _object**)#1}::operator()(int const, _object**) const (this=0x7fffb7ffdd88, value=@0x7fffb7ffdcbc: 10957, out_values=0x555555e1b9b0)
    at ../src/arrow/python/arrow_to_pandas.cc:417
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2019-09-18 09:16:18,2
13257292,"[R] Getting ""Cannot call io___MemoryMappedFile__Open()"" error while reading a parquet file","I am usingr/Dockerfile to get all the R dependency and following back to get everything to get the arrow/r work in linux (either ubuntu/debian) but it is continuously giving me this error:

Error in io___MemoryMappedFile__Open(fs::path_abs(path), mode) : 

 Cannot call io___MemoryMappedFile__Open()

I have installed all the required cpp libraries as mentioned here:[https://arrow.apache.org/install/]under ""Ubuntu 18.04 LTS or later"". I have also tried to use[cpp/Dockerfile|https://github.com/apache/arrow/blob/master/cpp/Dockerfile]and then followed backwards without any luck. The error is consistent and doesn't go away.

I am trying to build a docker image with dockerfile containing everything that arrow needs, all the cpp libraries etc.",Docker R arrow parquet,['R'],ARROW,Bug,Major,2019-09-18 06:18:43,14
13257289,"[Java] Avro - Experiment with ""compiled"" consumer delegates for performance.","All consumers that rely on delegates (e.g. struct, composite, list, union, ...) require megamorphic lookups which can't be inlined well by JIT.



We should verify the performance different of a hand-coded consumer vs an existing delegate consumer



i.e. something like:



void consume(Decoder d) {

 ((IntConsumer)delegate).consume(d);

}



compared to the existing implementation. It is expected we will see a decent amount of performance improvement from this approach. If we do, we should add an option to converter to generate new custom classes on the fly, that mimic the hand-coded option.

",avro,['Java'],ARROW,Sub-task,Major,2019-09-18 05:55:52,16
13257288,[Java] Support logical type encodings from Avro,"Avro supports some logical types that overlap with Arrow logical types([http://avro.apache.org/docs/current/spec.html#Logical+Types) |http://avro.apache.org/docs/current/spec.html#Logical+Types]



For the ones that overlap, we should use the appropriate Arrow Logical type array instead of the raw values.



it potentially makes sense to break this down further into sub-tasks for each logical type.

",avro pull-request-available,['Java'],ARROW,Sub-task,Major,2019-09-18 05:50:54,16
13257284,[Java] Experiment with performance difference of avoiding the use of Avro Decoder,"It has been posited that the Decoder object (and on-heap work in general) is potentially slow for decoding.



The scope of this Jira is to add a new method that instead of consuming from Decoder, consumes directly from a ByteBuffer. In order to this we there needs to be utility classes for zig-zag decoding (one might existing in avro) from a ByteBuffer.



This is esentially rewriting logic in the decoder to work directly against a bytebuffer and then measure if there is a meaningful performance impact.



",avro,['Java'],ARROW,Sub-task,Major,2019-09-18 05:43:46,16
13257282,[Java] Add support for skipping decoding of columns/field in Avro converter,"Users should be able to pass in a set of fields they wish to decode from Avro and the converter should avoid creating Vectors in the returned ArrowSchemaRoot. This would ideally support nested columns so if there was:



Struct A {

  int B;

  int C;

}



The use could choose to only read A.B or A.C or both.",avro pull-request-available,['Java'],ARROW,Sub-task,Major,2019-09-18 05:42:14,16
13257225,[C++] Do not require ARROW_JSON=ON when ARROW_IPC=ON,"arrow/CMakeLists.txt currently has

{code}
if(ARROW_IPC AND NOT ARROW_JSON)
  message(FATAL_ERROR ""JSON support is required for Arrow IPC"")
endif()
{code}

Building the JSON scanner component should not be a pre-requisite of building IPC support",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-17 21:49:51,14
13257217,[C++] Support BinaryType in MakeArrayOfNull,This function does not even return an error Status when passed an instance of BinaryType,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-17 21:16:18,6
13257215,[C++] Suppress class-memaccess warning with g++ 9.2.1,"{noformat}
[1/8] Building CXX object src/parquet/CMakeFiles/parquet_objlib.dir/encoding.cc.o
FAILED: src/parquet/CMakeFiles/parquet_objlib.dir/encoding.cc.o 
/usr/bin/c++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR="""" -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_ZSTD -DHAVE_INTTYPES_H -DHAVE_NETDB_H -DHAVE_NETINET_IN_H -DPARQUET_EXPORTING -Isrc -I../src -isystem jemalloc_ep-prefix/src -isystem flatbuffers_ep-prefix/src/flatbuffers_ep-install/include -isystem ../thirdparty/cares_ep-install/include -isystem ../thirdparty/hadoop/include -isystem orc_ep-install/include -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -g -fPIC   -std=gnu++11 -MD -MT src/parquet/CMakeFiles/parquet_objlib.dir/encoding.cc.o -MF src/parquet/CMakeFiles/parquet_objlib.dir/encoding.cc.o.d -o src/parquet/CMakeFiles/parquet_objlib.dir/encoding.cc.o -c ../src/parquet/encoding.cc
In file included from ../src/parquet/encoding.cc:33:
../src/arrow/util/rle_encoding.h: In instantiation of 'int arrow::util::RleDecoder::GetBatchWithDictSpaced(const T*, T*, int, int, const uint8_t*, int64_t) [with T = parquet::FixedLenByteArray; uint8_t = unsigned char; int64_t = long int]':
../src/parquet/encoding.cc:1079:20:   required from 'int parquet::DictDecoderImpl<Type>::DecodeSpaced(parquet::DictDecoderImpl<Type>::T*, int, int, const uint8_t*, int64_t) [with Type = parquet::PhysicalType<parquet::Type::FIXED_LEN_BYTE_ARRAY>; parquet::DictDecoderImpl<Type>::T = parquet::FixedLenByteArray; uint8_t = unsigned char; int64_t = long int]'
../src/parquet/encoding.cc:1076:7:   required from here
../src/arrow/util/rle_encoding.h:440:9: error: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'struct parquet::FixedLenByteArray'; use assignment or value-initialization instead [-Werror=class-memaccess]
  440 |   memset(&zero, 0, sizeof(T));
      |   ~~~~~~^~~~~~~~~~~~~~~~~~~~~
In file included from ../src/parquet/encoding.h:27,
                 from ../src/parquet/encoding.cc:18:
../src/parquet/types.h:515:8: note: 'struct parquet::FixedLenByteArray' declared here
  515 | struct FixedLenByteArray {
      |        ^~~~~~~~~~~~~~~~~
In file included from ../src/parquet/encoding.cc:33:
../src/arrow/util/rle_encoding.h: In instantiation of 'int arrow::util::RleDecoder::GetBatchWithDictSpaced(const T*, T*, int, int, const uint8_t*, int64_t) [with T = parquet::ByteArray; uint8_t = unsigned char; int64_t = long int]':
../src/parquet/encoding.cc:1079:20:   required from 'int parquet::DictDecoderImpl<Type>::DecodeSpaced(parquet::DictDecoderImpl<Type>::T*, int, int, const uint8_t*, int64_t) [with Type = parquet::PhysicalType<parquet::Type::BYTE_ARRAY>; parquet::DictDecoderImpl<Type>::T = parquet::ByteArray; uint8_t = unsigned char; int64_t = long int]'
../src/parquet/encoding.cc:1076:7:   required from here
../src/arrow/util/rle_encoding.h:440:9: error: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'struct parquet::ByteArray'; use assignment or value-initialization instead [-Werror=class-memaccess]
  440 |   memset(&zero, 0, sizeof(T));
      |   ~~~~~~^~~~~~~~~~~~~~~~~~~~~
In file included from ../src/parquet/encoding.h:27,
                 from ../src/parquet/encoding.cc:18:
../src/parquet/types.h:495:8: note: 'struct parquet::ByteArray' declared here
  495 | struct ByteArray {
      |        ^~~~~~~~~
cc1plus: all warnings being treated as errors
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-17 21:04:35,1
13257170,[Python][Wheel] Bundle zlib again with the windows wheels,"The PR https://github.com/apache/arrow/pull/4886 aimed to link zlib statically with the wheels broke the windows wheels. This issue was hidden by the AppVeyor wheel build script, but recent conda changes have reproduced the issue caused the broken windows wheels for 0.14.1 https://github.com/ursa-labs/crossbow/branches/all?query=nightly-2019-09-17-0-appveyor-wheel-win-cp37m
The root cause was, thad ZLIB was linked dynamically in the windows wheels despite `zlib_SOURCE=BUNDLED`. ",pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-17 16:50:41,3
13257143,[R] Arrow to R fails with embedded nuls in strings,"Apologies if this issue isn't categorized or documented appropriately.  Please be gentle! :)

As a heavy R user that normally interacts with parquet files using SparklyR, I have recently decided to try to use arrow::read_parquet() on a few parquet files that were on my local machine rather than in hadoop.  I was not able to proceed after several various attempts due to embedded nuls.  For example:

try({df <- read_parquet('out_2019-09_data_1.snappy.parquet') })
Error in Table__to_dataframe(x, use_threads = option_use_threads()) : 
  embedded nul in string: 'INSTALL BOTH LEFT FRONT AND RIGHT FRONT  TORQUE ARMS\0 ARMS'

Is there a solution to this?

I have also hit roadblocks with embedded nuls in the past with csvs using data.table::fread(), but readr::read_delim() seems to handle them gracefully with just a warning after proceeding.

Apologies that I do not have a handy reprex. I don't know if I can even recreate a parquet file with embedded nuls using arrow if it won't let me read one in, and I can't share this file due to company restrictions.

Please let me know how I can be of any more help!",pull-request-available,['R'],ARROW,Bug,Major,2019-09-17 14:32:54,4
13257137,[C++] Fix fuzzit job submission,See [https://circleci.com/gh/ursa-labs/crossbow/2978],pull-request-available,['C++'],ARROW,Bug,Major,2019-09-17 14:25:37,2
13257115,[Java] Support comparison for unsigned integers,"In this issue, we support the comparison of unsigned integer vectors, including UInt1Vector, UInt2Vector, UInt4Vector, and UInt8Vector.
With support for comparison for these vectors, the sort for them is also supported automatically.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-09-17 13:06:53,7
13257080,[C++] Casting int64 to string columns,"I wanted to cast a list of a tables to the same schema so I could use concat_tables later. However, I encountered ArrowNotImplementedError:
{code:java}
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-11-bd4916c221bf> in <module>
----> 1 list_tb = [i.cast(mts_schema, safe = True) for i in list_tb]

<ipython-input-11-bd4916c221bf> in <listcomp>(.0)
----> 1 list_tb = [i.cast(mts_schema, safe = True) for i in list_tb]

~\AppData\Local\Continuum\miniconda3\envs\cyclone\lib\site-packages\pyarrow\table.pxi in itercolumns()

~\AppData\Local\Continuum\miniconda3\envs\cyclone\lib\site-packages\pyarrow\table.pxi in pyarrow.lib.Column.cast()

~\AppData\Local\Continuum\miniconda3\envs\cyclone\lib\site-packages\pyarrow\error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: No cast implemented from int64 to string
{code}
Some context: I want to read and concatenate a bunch of csv files that come from partitioning of the same table. Using cast after reading csv is usually significantly faster than specifying column_types in ConvertOptions. There are string columns that are mostly populated with integer-like values so a particular file can have an integer-only column. This situation is rather common so having an option to cast int64 column to string column would be helpful.

",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2019-09-17 10:38:13,2
13256954,[Python] Segfault when writing to parquet,"When attempting to write out a pyarrow table to parquet I am observing a segfault when there is a mismatch between the schema and the datatypes.

Here is a reproducible example:


{code:java}
import pyarrow as pa
import pyarrow.parquet as pq

data = dict()
data[""key""] = [0, 1, 2, 3] # segfault
#data[""key""] = [""0"", ""1"", ""2"", ""3""] # no segfault

schema = pa.schema({""key"" : pa.string()})

table = pa.Table.from_pydict(data, schema = schema)
print(""now writing out test file"")
pq.write_table(table, ""test.parquet"")
{code}
This results in a segfault when writing the table. Running


{code:java}
gdb -ex r --args python test.py 
{code}
Yields




{noformat}
Program received signal SIGSEGV, Segmentation fault. 0x00007fffe8173917 in virtual thunk to parquet::DictEncoderImpl<parquet::DataType<(parquet::Type::type)6> >::Put(parquet::ByteArray const*, int) () from /net/fantasia/home/jweinstk/anaconda3/lib/python3.7/site-packages/pyarrow/libparquet.so.14
{noformat}




Thanks for all of your arrow work,

Josh",pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2019-09-16 19:01:36,14
13256908,[C++] Reading some Parquet data can return uninitialized memory,"In https://github.com/apache/arrow/pull/5365 we found via a UBSAN issue that Parquet decoding of RLE-encoded dict-encoded Parquet data with nulls does not initialize the output data array for null entries. Since the output data array is generally a freshly-allocated memory buffer, this means it will contain uninitialized memory.",pull-request-available,['C++'],ARROW,Bug,Critical,2019-09-16 15:23:56,2
13256864,[Python] Use MemoryPool to allocate memory for NumPy arrays in to_pandas calls,"It occurred to me that we can likely improve the performance and scalability of {{Table.to_pandas}} or other {{to_pandas}} methods by using the active MemoryPool to allocate memory for the array rather than letting NumPy use the system allocator. We would need to use the {{PyCapsule}} approach to setting a {{shared_ptr<Buffer>}} as the base of the created NumPy arrays

This has the additional benefit of tracking NumPy-related allocations in the MemoryPool so we will have a more precise accounting of allocated memory. ",pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-16 12:17:06,14
13256745,[C++][Python][Parquet] pyarrow.parquet crash writing zero-chunk dictionary-type column,"Trying to write a zero-RecordBatch file to parquet:

{code:python}
import pyarrow
import pyarrow.parquet
table = pyarrow.Table.from_batches([], pyarrow.schema([('A', pyarrow.dictionary(pyarrow.int32(), pyarrow.string()))]))
pyarrow.parquet.write_table(table, 'x.parquet')
{code}

... I receive an error and Python exits with exit code {{139}}:

{noformat}
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0915 18:37:23.099939     1 table.cc:64]  Check failed: (chunks.size()) > (0) cannot construct ChunkedArray from empty vector and omitted type
*** Check failure stack trace: ***
{noformat}

",pull-request-available,['Python'],ARROW,Bug,Major,2019-09-15 18:47:05,6
13256743,[Rust] [DataFusion] SQL aggregate query execution assume grouping expressions precede aggregate expressions,"Aggregate SQL queries produce incorrect results unless the query is written with grouping expressions followed by aggregate expressions.

For example, the following queries return the exact same result set:
{code:java}
SELECT a, SUM(b) FROM test GROUP BY a 
SELECT SUM(b), a FROM test GROUP BY a{code}",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2019-09-15 18:17:56,10
13256738,[Rust] [DataFusion] Intermittent test failure due to temp dir already existing,"Brittle code when creating temp files causing this error
{code:java}
---- execution::physical_plan::projection::tests::project_first_column stdout ----
thread 'execution::physical_plan::projection::tests::project_first_column' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 17, kind: AlreadyExists, message: ""File exists"" }', src/libcore/result.rs:1084:5
 {code}",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2019-09-15 16:21:34,10
13256736,[Python] Do not require pandas for invoking Array.__array__,See ARROW-6560,pull-request-available,['Python'],ARROW,Bug,Major,2019-09-15 15:38:43,5
13256734,"[Rust] [DataFusion] Create ""merge"" execution plan",The MergeExec plan simply executes multiple partitions in parallel and combines the result into a single partition.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-09-15 15:15:29,10
13256693,[Python] pandas-master integration test failure,"{code}
=================================== FAILURES ===================================
_____________________________ test_array_protocol ______________________________

    def test_array_protocol():
        if LooseVersion(pd.__version__) < '0.24.0':
            pytest.skip('IntegerArray only introduced in 0.24')
    
        def __arrow_array__(self, type=None):
            return pa.array(self._data, mask=self._mask, type=type)
    
        df = pd.DataFrame({'a': pd.Series([1, 2, None], dtype='Int64')})
    
        # with latest pandas/arrow, trying to convert nullable integer errors
        with pytest.raises(TypeError):
>           pa.table(df)
E           Failed: DID NOT RAISE <class 'TypeError'>

opt/conda/lib/python3.6/site-packages/pyarrow/tests/test_pandas.py:3035: Failed
{code}

https://circleci.com/gh/ursa-labs/crossbow/2896?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link",pull-request-available,['Python'],ARROW,Bug,Major,2019-09-14 14:43:06,5
13256651,"[Developer][C++] Add ""archery"" option to specify system toolchain for C++ builds","I use toolchain directories that are found outside of conda environments. It's a bit awkward to use archery to do benchmark comparisons with this arrangement. I suggest adding a ""--cpp-package-prefix"" option or similar that will set {{ARROW_DEPENDENCY_SOURCE=SYSTEM}} and the correct {{ARROW_PACKAGE_PREFIX}} so this works properly",pull-request-available,"['C++', 'Developer Tools']",ARROW,Improvement,Major,2019-09-14 01:46:40,14
13256600,[C++] Refactor Iterator to a type erased handle,Since {{Iterator<T>}} is used as a base class for exported classes and is instantiated in both arrow.dll and arrow_dataset.dll we get multiple definition errors. The solution taken by MSVC's stl implementation in the similar case of {{std::function}} is type erasure. Since it's not a base class MSVC doesn't force public visibility of its members. ,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-13 19:18:46,6
13256569,"[Python] Always return pandas.Series from Array/ChunkedArray.to_pandas, propagate field names to Series from RecordBatch, Table",One part of ARROW-6429,pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-13 15:39:17,14
13256473,[Python] Prepare for pandas release without SparseDataFrame,"We still have a few places where we use SparseDataFrame. An upcoming release of pandas will remove this class, so we should already make sure it works for that.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-13 08:27:43,5
13256354,[C++] Switch back to latest jemalloc 5.x,"In ARROW-6478, we switched back to jemallox 4.x to fix a large performance regression in micro-benchmarks which appeared when bumping the vendored jemalloc version to 5.2.0.

Since then, the issue was [discussed|https://github.com/jemalloc/jemalloc/issues/1621] with the jemalloc maintainers and we found out that customizing some options allowed regaining all (or most) of the performance on micro-benchmarks with 5.2.x.",pull-request-available,['C++'],ARROW,Wish,Major,2019-09-12 14:14:13,2
13256203,[Format][C++] Use two-part EOS and amend Format documentation,Per mailing list discussion,pull-request-available,"['C++', 'Format']",ARROW,Improvement,Blocker,2019-09-11 21:58:28,14
13256144,[Python] Add detach() method to buffered streams,"{{BufferedOutputStream::Close}} closes the raw file handle unconditionally. This may be undesirable in some circumstances.

Some alternatives:

* Do not close it
* Only close it if the {{use_count}} of the {{shared_ptr}} is 1, so we know that no one else has a copy of the shared_ptr",pull-request-available,['Python'],ARROW,Improvement,Major,2019-09-11 15:51:03,2
13256135,[CI][Crossbow][R] Nightly R job doesn't install all dependencies,"https://circleci.com/gh/ursa-labs/crossbow/2802

{code}
* checking for file './DESCRIPTION' ... OK
* preparing 'arrow':
* checking DESCRIPTION meta-information ... OK
* cleaning src
* running 'cleanup'
Error in loadVignetteBuilder(pkgdir, TRUE) : 
  vignette builder 'knitr' not found
Execution halted
Exited with code 1
{code}",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Minor,2019-09-11 14:49:25,4
13256117,[C++] Feather: slow writing of NullArray,"From https://stackoverflow.com/questions/57877017/pandas-feather-format-is-slow-when-writing-a-column-of-none

Smaller example with just using pyarrow, it seems that writing an array of nulls takes much longer than an array of for example ints, which seems a bit strange:

{code}
In [93]: arr = pa.array([None]*1000, type='int64')

In [94]: %%timeit 
    ...: w = pyarrow.feather.FeatherWriter('__test.feather') 
    ...: w.writer.write_array('x', arr) 
    ...: w.writer.close() 

31.4 s  464 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)

In [95]: arr = pa.array([None]*1000)  

In [96]: arr    
Out[96]: 
<pyarrow.lib.NullArray object at 0x7fa47a23ca40>
1000 nulls

In [97]: %%timeit 
    ...: w = pyarrow.feather.FeatherWriter('__test.feather') 
    ...: w.writer.write_array('x', arr) 
    ...: w.writer.close() 

3.75 ms  64.1 s per loop (mean  std. dev. of 7 runs, 100 loops each)
{code}

So writing the same length NullArray takes ca 100x more time compared to an array of nulls but with Integer type.",feather,['C++'],ARROW,Bug,Major,2019-09-11 13:35:16,14
13256085,[C++] Add OutputStream::Write() variant taking an owned buffer,"When Write() takes an arbitrary data pointer and needs to buffer it, it is mandatory to copy the data because the pointer may go stale, or the data may be overwritten.

Buf if the user has an immutable Buffer, then it should be enough to store the Buffer as necessary, without doing a memory copy. We could add a special Write() variant for that.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-11 10:00:34,2
13256082,[C++] Poison data in PoolBuffer destructor,"In debug mode, we could poison data (at least the first and last bytes?) in the PoolBuffer destructor so as to easily detect buffer lifetime issues.

(ASAN also helps, but this would act as a first defense barrier, e.g. for local development)",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-11 09:56:21,2
13256067,[C++] CloseFromDestructor() should perhaps not crash,"When a stream object fails to close in its destructor, CloseFromDestructor() will abort the process with a fatal error. This may not be desirable on e.g. networked filesystems where failing to closing isn't uncommon. Perhaps we should just log an error instead.

(stream users should generally call Close() explicitly, but in some cases they may fail to do so, e.g. when an error interrupts processing)",pull-request-available,['C++'],ARROW,Wish,Major,2019-09-11 09:14:58,2
13256034,[Developer][Packaging] Nightly build report's subject should contain Arrow,"Something like: ""[NIGHTLY] Arrow build report for job <job-id>""",pull-request-available,"['Developer Tools', 'Packaging']",ARROW,Improvement,Major,2019-09-11 06:49:39,3
13256007,"[Python] Test suite fails with pandas 0.23.4, pytest 3.8.1","{code}
_____________________________ test_array_protocol _____________________________
    def test_array_protocol():
        if LooseVersion(pd.__version__) < '0.24.0':
>           pytest.skip(reason='IntegerArray only introduced in 0.24')
E           TypeError: unexpected keyword arguments: ['reason']
C:\Miniconda3-x64\envs\wheel-test\lib\site-packages\pyarrow\tests\test_pandas.py:2934: TypeError
=========================== short test summary info ===========================
{code}

See https://ci.appveyor.com/project/Ursa-Labs/crossbow/builds/27310212

[~jorisvandenbossche] can you have a look?",pull-request-available,['Python'],ARROW,Bug,Major,2019-09-11 00:54:14,5
13255991,[Python] Segmentation fault on writing tables with fixed size binary fields ,"I'm not sure if this should be reported to Parquet or here.

When I tried to serialize a pyarrow table with a fixed size binary field (holds 16 byte UUID4 information) to a parquet file, segmentation fault occurs.

Here is the minimal example to reproduce:

{{import pyarrow as pa}}
{{from pyarrow import parquet as pq}}
{{data = \{""col"": pa.array([b""1234"" for _ in range(10)])}}}
{{fields = [(""col"", pa.binary(4))]}}
{{schema = pa.schema(fields)}}
{{table = pa.table(data, schema)}}
{{pq.write_table(table, ""test.parquet"")}}
{{segmentation fault (core dumped) ipython}}



Yet, it works if I don't specify the size of the binary field.

{{import pyarrow as pa}}
{{from pyarrow import parquet as pq}}
{{data = \{""col"": pa.array([b""1234"" for _ in range(10)])}}}
{{fields = [(""col"", pa.binary())]}}
{{schema = pa.schema(fields)}}
{{table = pa.table(data, schema)}}
{{pq.write_table(table, ""test.parquet"")}}

Thanks,",newbie pull-request-available,['Python'],ARROW,Bug,Critical,2019-09-10 22:43:54,5
13255974,[Packaging][Python] Flight failing in OSX Python wheel builds,"See example failure

https://travis-ci.org/ursa-labs/crossbow/builds/583167489?utm_source=github_status&utm_medium=notification

{code}
[ 30%] Generating Flight.pb.cc, Flight.pb.h, Flight.grpc.pb.cc, Flight.grpc.pb.h
dyld: Library not loaded: /usr/local/opt/gperftools/lib/libprofiler.0.dylib
  Referenced from: /usr/local/Cellar/grpc/1.23.0_2/bin/grpc_cpp_plugin
  Reason: image not found
--grpc_out: protoc-gen-grpc: Plugin killed by signal 6.
make[2]: *** [src/arrow/flight/Flight.pb.cc] Error 1
make[2]: *** Deleting file `src/arrow/flight/Flight.pb.cc'
make[1]: *** [src/arrow/flight/CMakeFiles/flight_grpc_gen.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
{code}

I suggest disabling Flight in the wheel builds",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Blocker,2019-09-10 21:15:36,3
13255908,[C++] Clean type_traits.h definitions,"{{IsSignedInt}} takes either an array or a type as a type argument, which is surprisingly atypical for traits. Furthermore whereas {{is_signed_integer}} returns false for date and other types which are represented by but not identical to integers {{IsSignedInt}} returns true by checking only the {{c_type}}, which leads to {{static_assert(IsSignedInt<HalfFloatType>::value, """")}}. Finally the declaration of {{IsSignedInt}} is far from readable due to nested macro usage.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-10 14:46:10,13
13255905,[CI] The conda environment files arrow/ci/conda_env_*.yml should have .txt extension,"The files `arrow/ci/conda_env_*.yml` files are not yaml files, we should rename them to use txt extension.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Trivial,2019-09-10 14:35:50,3
13255873,[Python][Filesystem] Expose nanosecond resolution mtime,"FileStats.mtime returns a microsecond resolution datetime object. At some point we should also expose a mtime_ns attribute that gives the exact nanoseconds as an integer, like os.stat does: https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.as_posix",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-09-10 12:34:44,2
13255807,[C++] Validation of ExtensionType with nested type fails,"A reproducer using the Python ExtensionType:

{code}
class MyStructType(pa.ExtensionType): 

    def __init__(self): 
        storage_type = pa.struct([('a', pa.int64()), ('b', pa.int64())]) 
        pa.ExtensionType.__init__(self, storage_type, 'my_struct_type') 

    def __arrow_ext_serialize__(self): 
        return b'' 

    @classmethod 
    def __arrow_ext_deserialize__(self, storage_type, serialized): 
        return MyStructType() 

ty = MyStructType()
storage_array = pa.array([{'a': 1, 'b': 2}], ty.storage_type) 
arr = pa.ExtensionArray.from_storage(ty, storage_array) 
{code}

then validating this array fails because it expects no children (the extension array itself has no children, only the storage array):

{code}
In [8]: arr.validate()   
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-8-13783ce9f25e> in <module>
----> 1 arr.validate()

~/scipy/repos/arrow/python/pyarrow/array.pxi in pyarrow.lib.Array.validate()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Expected 0 child arrays in array of type extension<my_struct_type>, got 2
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-10 07:36:48,5
13255777,[GLib][CI] MinGW failure in CI,"This failure seems to have crept in to master

https://ci.appveyor.com/project/wesm/arrow/build/job/ocfkn9m0a3ux1ur5#L2288",pull-request-available,['GLib'],ARROW,Bug,Blocker,2019-09-10 01:22:00,1
13255702,"[Website] On change to master branch, automatically make PR to asf-site","I added a build/deploy script to arrow-site that would enable automatically publishing to asf-site when there is a commit to the master branch. However, ASF won't let us add a deploy key to enable this publishing (INFRA-18924). 

I have a workaround that's not automatic but as close as we can get. On commits to apache/arrow-site's master branch, Travis builds the site and pushes it to a fork of arrow-site (where there is no restriction on deploy keys), and then it makes a PR from there back to the asf-site branch of apache/arrow-site using [hub|https://hub.github.com/hub-pull-request.1.html]. So it's ""semiautomatic"": the asf-site PR is made automatically, but a committer will need to merge it. ",pull-request-available,['Website'],ARROW,Improvement,Major,2019-09-09 17:34:47,4
13255661,[C++][Dataset] Implement basic PartitionScheme,"The PartitionScheme interface parses paths and yields the partition expressions which are encoded in those paths. For example, the Hive partition scheme would yield {{""a""_ = 2 and ""b""_ = 3}} from ""a=2/b=3/*.parquet"".",dataset pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-09 14:39:09,6
13255624,[Python] file written with latest fastparquet cannot be read with latest pyarrow,"From report on the pandas issue tracker: https://github.com/pandas-dev/pandas/issues/28252

With the latest released versions of fastparquet (0.3.2) and pyarrow (0.14.1), writing a file with pandas using the fastparquet engine cannot be read with the pyarrow engine:

{code}
df = pd.DataFrame({'A': [1, 2, 3]})
df.to_parquet(""test.parquet"", engine=""fastparquet"", compression=None)                                                                                                                                     
pd.read_parquet(""test.parquet"", engine=""pyarrow"")   
{code}

gives the following error when reading:

{code}
----> 1 pd.read_parquet(""test.parquet"", engine=""pyarrow"")

~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py in read_parquet(path, engine, columns, **kwargs)
    292 
    293     impl = get_engine(engine)
--> 294     return impl.read(path, columns=columns, **kwargs)

~/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py in read(self, path, columns, **kwargs)
    123         kwargs[""use_pandas_metadata""] = True
    124         result = self.api.parquet.read_table(
--> 125             path, columns=columns, **kwargs
    126         ).to_pandas()
    127         if should_close:

~/miniconda3/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()

~/miniconda3/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table._to_pandas()

~/miniconda3/lib/python3.7/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, categories, ignore_metadata)
    642         column_indexes = pandas_metadata.get('column_indexes', [])
    643         index_descriptors = pandas_metadata['index_columns']
--> 644         table = _add_any_metadata(table, pandas_metadata)
    645         table, index = _reconstruct_index(table, index_descriptors,
    646                                           all_columns)

~/miniconda3/lib/python3.7/site-packages/pyarrow/pandas_compat.py in _add_any_metadata(table, pandas_metadata)
    965                 raw_name = 'None'
    966 
--> 967         idx = schema.get_field_index(raw_name)
    968         if idx != -1:
    969             if col_meta['pandas_type'] == 'datetimetz':

~/miniconda3/lib/python3.7/site-packages/pyarrow/types.pxi in pyarrow.lib.Schema.get_field_index()

~/miniconda3/lib/python3.7/site-packages/pyarrow/lib.cpython-37m-x86_64-linux-gnu.so in string.from_py.__pyx_convert_string_from_py_std__in_string()

TypeError: expected bytes, dict found
{code}",parquet pull-request-available,['Python'],ARROW,Bug,Major,2019-09-09 12:39:37,5
13255514,[Python] pyarrow.NULL equals to itself,"Somewhat related to ARROW-6386 on the interpretation of nulls, we currently have the following behaviour:

{code}
In [28]: pa.NULL == pa.NULL                                                                                                                                                                                        
Out[28]: True
{code}

Which I think is certainly unexpected for a null / missing value. I still need to check what the array-level compare kernel does (NULL or False? ideally NULL I think), but we should follow that.",pull-request-available,['Python'],ARROW,Bug,Major,2019-09-08 19:43:41,5
13255503,[Rust] [DataFusion] Create test utils module,I've been learning how to better organize unit test code in Rust and would like to introduce a test utils module containing common test helper functions. This code will use {{#[cfg(test)]}} to make sure it doesn't ship with the production code.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-09-08 15:47:29,10
13255466,[Java] Enable create indexType for DictionaryEncoding according to dictionary value count,"Currently, when create {{DictionaryEncoding}}, we need to specify indexType, and it use Int(32, true)as default if this value is null.

Actually, when dictionary valueCount is small, we should use Int(8,true)/Int(16,true)instead to reduce memory allocation.

This issue is about to provide API for creating indexType according to valueCount and apply it to avro adapter for enum type.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-09-08 01:58:49,16
13255461,[Python][C++] Bad performance of read_csv() with column_types,"Case: Dataset wit 20k columns. Amount of rows can be 0.

{{pyarrow.csv.read_csv('20k_cols.csv')}} works rather fine if no convert_options provided.

Took 150ms.

Now I call {{read_csv()}} with column types mapping that marks 2000 out of these columns as string.

{{pyarrow.csv.read_csv('20k_cols.csv', convert_options=pyarrow.csv.ConvertOptions(column_types=\{'K%d' % i: pyarrow.string() for i in range(2000)}))}}

(K1..K19999 are column names in attached dataset).

My task globally is to read everything as string, avoid any inferring.

This takes several minutes, consumes around 4GB memory.

This doesn't look sane at all.",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-09-07 23:52:43,2
13255398,[Developer] Add command to generate and send e-mail report for a Crossbow run,We also need a simple wrapper to poll a Crossbow job periodically for completion and then send the report once all the tasks have finished. ,pull-request-available,['Developer Tools'],ARROW,New Feature,Major,2019-09-07 01:40:14,3
13255314,[C++] inline errors from external projects' build logs,"Currently when an external project build fails, we get a very uninformative message:

{code}
[88/543] Performing build step for 'flatbuffers_ep'
FAILED: flatbuffers_ep-prefix/src/flatbuffers_ep-stamp/flatbuffers_ep-build flatbuffers_ep-prefix/src/flatbuffers_ep-install/bin/flatc flatbuffers_ep-prefix/src/flatbuffers_ep-install/lib/libflatbuffers.a 
cd /build/cpp/flatbuffers_ep-prefix/src/flatbuffers_ep-build && /usr/bin/cmake -P /build/cpp/flatbuffers_ep-prefix/src/flatbuffers_ep-stamp/flatbuffers_ep-build-DEBUG.cmake && /usr/bin/cmake -E touch /build/cpp/flatbuffers_ep-prefix/src/flatbuffers_ep-stamp/flatbuffers_ep-build
CMake Error at /build/cpp/flatbuffers_ep-prefix/src/flatbuffers_ep-stamp/flatbuffers_ep-build-DEBUG.cmake:16 (message):
  Command failed: 1

   '/usr/bin/cmake' '--build' '.'

  See also

    /build/cpp/flatbuffers_ep-prefix/src/flatbuffers_ep-stamp/flatbuffers_ep-build-*.log
{code}

It would be far more useful if the error were caught and relevant section (or even the entirity) of {{ /build/cpp/flatbuffers_ep-prefix/src/flatbuffers_ep-stamp/flatbuffers_ep-build-*.log}} were output instead. This is doubly the case on CI where accessing those logs is non trivial",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-09-06 16:22:44,6
13255309,[C++] Roll back to jemalloc stable-4 branch until performance issues in 5.2.x addressed,New JIRA for changelog per ongoing thread in ARROW-6417,pull-request-available,['C++'],ARROW,Bug,Major,2019-09-06 15:57:41,14
13255257,[Packaging][Crossbow] Use Azure Pipelines to build linux packages,"We have hit the time limitation of Travis for the Debian builds, se we need to move these builds to another CI provider.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-09-06 12:57:22,3
13255249,[Java][CI] Travis java all-jdks job is broken,"Introduced byARROW-6433, fixing the shade check enabled evaluation of the incorrect body.",pull-request-available,"['Continuous Integration', 'Java']",ARROW,Bug,Major,2019-09-06 12:25:32,13
13255231,[C++] Don't try to dictionary encode dictionary arrays,"With #5077 (or possibly #4949) behavior with dictionary arrays changed, leaving the explicit call to DictionaryEncode() redundant.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-06 11:08:25,6
13255149,[Python] Provide mechanism for python to write out old format,"underlined textI think this needs to be an environment variable, so it can be made to work with old version of the Java library pyspark integration.



[~bryanc]can you check if this captures the requirements?",pull-request-available,"['Format', 'Python']",ARROW,Sub-task,Blocker,2019-09-06 04:39:30,14
13255147,[Format] Clarify dictionary encoding edge cases,"Several recent threads on the mailing list:

1. Edge case for all null columns and interleaved dictionaries

2. Semantics non-delta dictionaries (and relation to the file format).

3. Propose a forward compatible enum so dictionaries can represented as other types besides for a ""flat"" vector.",pull-request-available,"['Documentation', 'Format']",ARROW,Improvement,Major,2019-09-06 04:20:08,15
13255109,[Java] ValueVector#accept may has potential cast exception,"Per discussion [https://github.com/apache/arrow/pull/5195#issuecomment-528425302]

We may use API this way:
{code:java}
RangeEqualsVisitor visitor = new RangeEqualsVisitor(vector1, vector2);
vector3.accept(visitor, range){code}
if vector1/vector2 are say, {{StructVector}}s and vector3 is an {{IntVector}}- things can go bad. we'll use the {{compareBaseFixedWidthVectors()}}and do wrong type-casts for vector1/vector2.",pull-request-available,['Java'],ARROW,Bug,Major,2019-09-05 23:42:39,16
13255052,[C++] Remove unused hashing routines,"The adoption of xxh3 for hashing (in ARROW-6385) probably left around some specialized but unused hashing functions (e.g. CRC-based hashing, perhaps also murmurhash). We should probably remove them if no problem surfaces with xxh3.
",pull-request-available,['C++'],ARROW,Task,Major,2019-09-05 17:10:36,2
13255024,[Developer] Refactor integration/integration_test.py into a proper Python package,"This could also facilitate writing unit tests for the integration tests.

Maybe this could be a part of archery?",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-09-05 14:51:03,14
13255002,[Java] Refactor FixedSizeListVector#splitAndTransfer with slice API,"Currently {{FixedSizeListVector#splitAndTransfer}}actually use {{copyValueSafe}}which has memory copy, we should use slice API instead.

Meanwhile, {{splitAndTransfer}}in all classes should position index check at beginning.",pull-request-available,['Java'],ARROW,Bug,Critical,2019-09-05 13:11:30,16
13254965,[C++][Python] Rename arrow::fs::Selector to FileSelector,In both the C++ implementation and the python binding.,filesystem pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2019-09-05 10:20:40,3
13254921,[C++] Can't build with bundled double-conversion on CentOS 6 x86_64,"https://travis-ci.org/ursa-labs/crossbow/builds/581001313#L8163

{noformat}
-- Installing: /root/rpmbuild/BUILD/apache-arrow-0.14.0.dev451/cpp/build/double-conversion_ep/src/double-conversion_ep/lib64/libdouble-conversion.a
...
make[2]: *** No rule to make target 'double-conversion_ep/src/double-conversion_ep/lib/libdouble-conversion.a', needed by 'release/libarrow.so.15.0.0'.  Stop.
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-05 06:02:16,1
13254913,[Java] Add benchmark and large fake data UT for avro adapter,"To avoid OOM, we have implement iterator API in ARROW-6220.

This issue is about to add tests with a large fake data set (say 6MM rows in JDBC adapter test) and ensures no OOMs occur.

",pull-request-available,['Java'],ARROW,Sub-task,Critical,2019-09-05 04:39:32,16
13254907,[Java] Remove value boxing/unboxing for ApproxEqualsVisitor,"As discussed in https://github.com/apache/arrow/pull/5195#issuecomment-526157961, there are some problems with the current ways of comparing floating point vectors, we solve them in this PR:

1. there are if statements/duplicated members in ApproxEqualsVisitor, making the code redundant and less clear.
2. the comparion of float4 and float8 are based on wrapped objects Float and Double, which may have performance penalty.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-09-05 03:56:50,7
13254884,[C++] CMake build locally fails with MSVC 2015 build generator,"Apparently with my version of CMake (3.15) and Visual Studio, CMAKE_BUILD_TYPE does not get set before TOUPPER is called on it",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-05 01:15:29,14
13254730,[C++] More informative error messages from S3,We should add contextual information about the operation that failed (for example: creating bucket 'XXX').,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-04 14:59:13,2
13254562,[Java] Override ValueVector toString() method,"Currently cpp code {{Array#ToString}}returns the human readable format string like:

[

 1,

 2,

 3

]

But Java {{ValueVector}}did not implement like this way now.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-09-04 04:43:54,16
13254550,"[Format] Add clarifications to Columnar.rst about the contents of ""null"" slots in Varbinary or List arrays",Per mailing list discussion,pull-request-available,['Format'],ARROW,Improvement,Major,2019-09-04 03:05:02,7
13254543,[C++] Use 2x reallocation strategy in arrow::BufferBuilder instead of 1.5x,Change discussed in ARROW-6417 split out into a separate patch,pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-04 02:51:23,14
13254515,[C++] Builds with ARROW_JEMALLOC=ON wait until jemalloc_ep is complete before building any libarrow .cc files,"This has been a frustration for me that my builds wait for the jemalloc_ep build to complete before proceeding to compile any C++ files for libarrow. Since only memory_pool.cc depends on jemalloc_ep, there must be some way to create a dependency between that single C++ file and jemalloc_ep. I spent a few minutes Googling about it but did not come up with an easy answer",pull-request-available,['C++'],ARROW,Improvement,Major,2019-09-03 23:54:36,14
13254506,[OSX][Python][Wheel] Turn off ORC feature in the wheel building scripts,"See https://travis-ci.org/ursa-labs/crossbow/builds/580191793, https://travis-ci.org/ursa-labs/crossbow/builds/580192018, https://travis-ci.org/ursa-labs/crossbow/builds/580192255. Something fails while doing something with thrift, it appears. ",nightly pull-request-available wheel,"['Continuous Integration', 'Packaging']",ARROW,Bug,Blocker,2019-09-03 23:14:35,3
13254501,[CI][Crossbow] Nightly Gandiva jar trusty job fails,https://travis-ci.org/ursa-labs/crossbow/builds/580192384. Error is due to use of {{std::regex}}; replace with RE2.,pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Blocker,2019-09-03 23:00:09,6
13254499,[CI][Crossbow] Nightly conda osx builds fail,"See https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=684, https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=686, https://dev.azure.com/ursa-labs/crossbow/_build/results?buildId=688. Reported failure is

{code}
Traceback (most recent call last):
  File ""/usr/local/miniconda/bin/conda-build"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/cli/main_build.py"", line 445, in main
    execute(sys.argv[1:])
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/cli/main_build.py"", line 436, in execute
    verify=args.verify, variants=args.variants)
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/api.py"", line 209, in build
    notest=notest, need_source_download=need_source_download, variants=variants)
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/build.py"", line 2343, in build_tree
    notest=notest,
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/build.py"", line 1407, in build
    create_build_envs(top_level_pkg, notest)
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/build.py"", line 1261, in create_build_envs
    channel_urls=tuple(m.config.channel_urls))
  File ""/usr/local/miniconda/lib/python3.7/site-packages/conda_build/environ.py"", line 758, in get_install_actions
    raise DependencyNeedsBuildingError(exc, subdir=subdir)
conda_build.exceptions.DependencyNeedsBuildingError: Unsatisfiable dependencies for platform osx-64: {'libcxx==4.0.1=h579ed51_0.conda'}
##[error]Bash exited with code '1'.
{code}",pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Blocker,2019-09-03 22:57:07,3
13254495,[CI][Crossbow] Nightly Centos 6 job fails,"https://travis-ci.org/ursa-labs/crossbow/builds/580192251

Log seems to implicate doubleconversion, which is why I didn't bundle this with the rest in ARROW-6440.",pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Blocker,2019-09-03 22:48:24,1
13254493,"[CI][Crossbow] Nightly ubuntu, debian, and centos package builds fail","See 

* https://travis-ci.org/ursa-labs/crossbow/builds/580192012
* https://travis-ci.org/ursa-labs/crossbow/builds/580192084
* https://travis-ci.org/ursa-labs/crossbow/builds/580192418
* https://travis-ci.org/ursa-labs/crossbow/builds/580192214
* https://travis-ci.org/ursa-labs/crossbow/builds/580192247
* https://travis-ci.org/ursa-labs/crossbow/builds/580192313
* https://travis-ci.org/ursa-labs/crossbow/builds/580192133

The failures look like: 

{code}
   dh_install
dh_install: Cannot find (any matches for) ""usr/bin/plasma_store_server"" (tried in ., debian/tmp)
dh_install: plasma-store-server missing files: usr/bin/plasma_store_server
dh_install: missing files, aborting
make: *** [debian/rules:14: binary] Error 25
dpkg-buildpackage: error: fakeroot debian/rules binary subprocess returned exit status 2
debuild: fatal error at line 1152:
dpkg-buildpackage -rfakeroot -us -uc -ui failed
Failed debuild -us -uc
rake aborted!
{code}",pull-request-available,"['Continuous Integration', 'Packaging']",ARROW,Bug,Blocker,2019-09-03 22:38:28,1
13254490,[R] Add AWS SDK to system dependencies for macOS and Windows,"The Arrow C++ library now has an S3 filesystem implementation (ARROW-453), and in order to take advantage of that from R, we need to add the {{aws-sdk-cpp}} dependency to the macOS and Windows toolchains. 

There is no PKGBUILD for this at https://github.com/msys2/MINGW-packages, but https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=aws-sdk-cpp-git has one. 

For macOS, there already is a formula at https://github.com/Homebrew/homebrew-core/blob/master/Formula/aws-sdk-cpp.rb, maybe that's sufficient?

Once that is in place, we can enable {{ARROW_S3=ON}} in cmake and build with it (https://github.com/apache/arrow/pull/5167/files#diff-b048bf4c1679dce1028fd897a7c43b93R177)

cc [~jeroenooms]",pull-request-available,"['Packaging', 'R']",ARROW,New Feature,Major,2019-09-03 22:15:40,4
13254465,[CI][Crossbow] Nightly dask integration job fails,"See https://circleci.com/gh/ursa-labs/crossbow/2326. Either fix, skip job and create followup Jira to unskip, or delete job.",nightly pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Blocker,2019-09-03 19:34:32,14
13254463,[CI][Crossbow] Nightly HDFS integration job fails,"See https://circleci.com/gh/ursa-labs/crossbow/2322. Either fix, skip job and create followup Jira to unskip, or delete job.

See also ARROW-2248.",nightly pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-09-03 19:32:51,14
13254462,[CI][Crossbow] Nightly java docker job fails,"See https://circleci.com/gh/ursa-labs/crossbow/2335. Either fix, skip job and create followup Jira to unskip, or delete job.",nightly pull-request-available,"['Continuous Integration', 'Java']",ARROW,Bug,Blocker,2019-09-03 19:31:18,13
13254461,[CI][Crossbow] Remove alpine crossbow jobs,"Apparently we don't expect them to pass, so we shouldn't be running them and generating failures that we're supposed to know to ignore.",nightly pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-09-03 19:30:15,3
13254460,[Python] Test suite fails without pandas installed,"See https://circleci.com/gh/ursa-labs/crossbow/2337. 

{code:python}
==================================== ERRORS ====================================
_ ERROR collecting opt/conda/lib/python3.6/site-packages/pyarrow/tests/test_pandas.py _
opt/conda/lib/python3.6/site-packages/pyarrow/tests/test_pandas.py:2894: in <module>
    @pytest.mark.skipif(LooseVersion(pd.__version__) < '0.24.0',
E   NameError: name 'pd' is not defined
{code}

Either fix, skip job and create followup Jira to unskip, or delete job.",nightly pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Blocker,2019-09-03 19:29:07,3
13254459,[CI][Crossbow] Nightly R docker job fails,"See https://circleci.com/gh/ursa-labs/crossbow/2311. Same issue as ARROW-6171. Either fix on 6171, skip job and create followup Jira to unskip, or delete job.",nightly,['Continuous Integration'],ARROW,Bug,Blocker,2019-09-03 19:27:23,13
13254458,[CI][Crossbow] Nightly spark integration job fails,"See https://circleci.com/gh/ursa-labs/crossbow/2310. Either fix, skip job and create followup Jira to unskip, or delete job.",nightly pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-09-03 19:25:17,14
13254457,[CI][Crossbow] Nightly turbodbc job fails,"See https://circleci.com/gh/ursa-labs/crossbow/2313. Either fix, skip job and create followup Jira to unskip, or delete job.",nightly pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-09-03 19:24:06,14
13254392,[FlightRPC] Expose gRPC configuration knobs in Flight,"We should not expose gRPC symbols/APIs publicly, but should still provide a way to configure gRPC options as they may be needed in deployments (for instance, we ran into an issue with gRPC keepalives). In Java, this is fortunately solvable with reflection, but this is impossible in C++/Python.",pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-09-03 13:11:24,0
13254367,[C++][Fuzzing] Fuzzit nightly is broken,"We don't get any new fuzzit uploads anymore, see https://circleci.com/gh/ursa-labs/crossbow/2296 for details. Seems like the binary is not found anymore:

{noformat}
...
+ pushd /build/cpp
/build/cpp /
+ mkdir ./relwithdebinfo/out
+ cp ./relwithdebinfo/arrow-ipc-fuzzing-test ./relwithdebinfo/out/fuzzer
cp: cannot stat './relwithdebinfo/arrow-ipc-fuzzing-test': No such file or directory
Exited with code 1
{noformat}

Looking at https://github.com/ursa-labs/crossbow/branches/all?utf8=%E2%9C%93&query=fuzzit , it seems it is broken as of the 19th of August, and very likely due to [438a140142be423b1b2af2399567a0a8aeba9aa1|https://github.com/apache/arrow/commit/438a140142be423b1b2af2399567a0a8aeba9aa1].",C++ fuzzer pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-09-03 10:40:22,2
13254365,[Python] pyarrow.CompressedOutputStream() never completes with compression='snappy',"On python, the execution of {{CompressedOutputSteam(stream, compression='snappy')}} never completes.

{color:#000000}raw {color}{color:#000000}={color}{color:#000000} pa.BufferOutputStream(){color}
{color:#000000}compressed {color}{color:#000000}={color}{color:#000000} pa.CompressedOutputStream(raw, {color}{color:#a31515}'snappy'{color}{color:#000000}){color}
Note: With 'gzip' this works fine.

",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-03 10:27:59,2
13254320,[Java] Improve the performance of UnionVector when getting underlying vectors,"Getting the underlying vector is a frequent opertation for UnionVector. It relies on this operation to get/set data at each index.

The current implementation is inefficient. In particular, it first gets the minor type at the given index, and then compares it against all possible minor types in a switch statment, until a match is found.

We improve the performance by storing the internal vectors in an array, whose index is the ordinal of the minor type. So given a minor type, its corresponding underlying vector can be obtained in O(1) time.

It should be noted that this technique is also applicable to UnionReader and UnionWriter, and support for UnionReader is already implemented.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-09-03 04:26:28,7
13254267,[R] Remove usage of R CMD config CXXCPP,"From email from BDR at CRAN: 

""R CMD config CXXCPP has been deprecated: it is not used by R itself and 
there are several things wrong with the standard autoconf detection code:

- If CXXCPP is set by the user, it is not tested.  It could be empty, 
which AFAICS none of you allow for.
- The code looks at $CXX -E and /lib/cpp in turn, and tests a system C 
header without consulting CPPFLAGS.  /lib/cpp is unlikely to find C++ 
headers, and we have seen instances where without CPPFLAGS it did not 
find C headers.
- It is the setting for the default C++ compiler, in R-devel C++11 but 
not specified in earlier R (even 3.6.x could be C++98).

It would be better to use $(CXX) -E (or $(CXX11) etc) or test for yourself.

Please change at the next package update.""",pull-request-available,['R'],ARROW,Improvement,Blocker,2019-09-02 15:20:06,4
13254263,[R] Support autogenerating column names,"Following ARROW-6231, the C++ library has a way to create column names. Enable that in R.",pull-request-available,['R'],ARROW,Improvement,Major,2019-09-02 15:02:07,4
13254241,[C++] arrow-flight-test can crash because of port allocation,"I get this error sometimes locally when running the tests in parallel:
{code}
[----------] 11 tests from TestFlightClient
[ RUN      ] TestFlightClient.ListFlights
E0902 15:13:55.996271678   17281 socket_utils_common_posix.cc:201] check for SO_REUSEPORT: {""created"":""@1567430035.996256600"",""description"":""SO_REUSEPORT unavailable on compiling system"",""file"":""../src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":169}
[       OK ] TestFlightClient.ListFlights (17 ms)
[ RUN      ] TestFlightClient.GetFlightInfo
E0902 15:13:56.013065793   17281 server_chttp2.cc:40]        {""created"":""@1567430036.013032600"",""description"":""No address added out of total 1 resolved"",""file"":""../src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":394,""referenced_errors"":[{""created"":""@1567430036.013029044"",""description"":""Unable to configure socket"",""fd"":6,""file"":""../src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":217,""referenced_errors"":[{""created"":""@1567430036.013021880"",""description"":""Address already in use"",""errno"":98,""file"":""../src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":190,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}
../src/arrow/flight/flight_test.cc:271: Failure
Failed
'server->Init(options)' failed with Unknown error: Server did not start properly
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-02 13:16:18,2
13254142,[C++][Parquet] DictEncoderImpl<T>::PutIndicesTyped has bad performance on some systems,"I was doing some benchmarking and noticed that this function showed up as slow due to {{\_\_memmove_avx_unaligned_erms}}. I'm interested to investigate why this is, but for me it's fixed by changing the {{std::vector::reserve}} call to {{std::vector::resize}} and instead assigning elements into {{buffered_indices_}}. I'll add a Python benchmark that illustrates the problem to see if it shows up on other systems",pull-request-available,['C++'],ARROW,Bug,Major,2019-09-01 23:14:00,14
13254082,[C++] jemalloc_ep fails for offline build,"Seems we have some slippage between the dependency download script and ThirdpartyToolchain.cmake

{code}
-- Build files have been written to: /home/wesm/code/arrow/cpp/build
[2/8] Performing download step (verify and extract) for 'jemalloc_ep'
-- jemalloc_ep download command succeeded.  See also /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-download-*.log
[5/8] Performing configure step for 'jemalloc_ep'
FAILED: jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure 
cd /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep && /home/wesm/cpp-toolchain/bin/cmake -P /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-DEBUG.cmake && /home/wesm/cpp-toolchain/bin/cmake -E touch /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure
CMake Error at /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-DEBUG.cmake:49 (message):
  Command failed: No such file or directory

   './configure' 'AR=/usr/bin/ar' 'CC=/usr/bin/clang-7' '--prefix=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist/' '--with-jemalloc-prefix=je_arrow_' '--with-private-namespace=je_arrow_private_' '--without-export' '--disable-cxx' '--disable-libdl' '--disable-initial-exec-tls'

  See also

    /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-*.log


ninja: build stopped: subcommand failed.
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-08-31 19:21:47,14
13254072,[C++] CMake build of arrow libraries fails on Windows,"I am trying to build the python pyarrow extension on Windows 10 using Visual Studio 2015 Build Tools and the current stable CMake.

Following [the instructions|https://github.com/apache/arrow/blob/master/docs/source/developers/python.rst] to the letter, CMake fails with the error:

{{??CMake Error at cmake_modules/SetupCxxFlags.cmake:42 (string):??}}
{{?? string no output variable specified??}}
{{??Call Stack (most recent call first):??}}
{{?? CMakeLists.txt:357 (include)??}}
----
Complete output:

{{(pyarrow-dev) Z:\devel\arrow\cpp\build>cmake -G ""Visual Studio 14 2015 Win64"" ^}}
{{More? -DCMAKE_INSTALL_PREFIX=%ARROW_HOME% ^}}
{{More? -DARROW_CXXFLAGS=""/WX /MP"" ^}}
{{More? -DARROW_GANDIVA=on ^}}
{{More? -DARROW_PARQUET=on ^}}
{{More? -DARROW_PYTHON=on ..}}
{{-- Building using CMake version: 3.15.2}}
{{CMake Error at CMakeLists.txt:30 (string):}}
{{ string no output variable specified}}


{{-- Selecting Windows SDK version to target Windows 10.0.17763.}}
{{-- The C compiler identification is MSVC 19.0.24210.0}}
{{-- The CXX compiler identification is MSVC 19.0.24210.0}}
{{-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe}}
{{-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works}}
{{-- Detecting C compiler ABI info}}
{{-- Detecting C compiler ABI info - done}}
{{-- Detecting C compile features}}
{{-- Detecting C compile features - done}}
{{-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe}}
{{-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe -- works}}
{{-- Detecting CXX compiler ABI info}}
{{-- Detecting CXX compiler ABI info - done}}
{{-- Detecting CXX compile features}}
{{-- Detecting CXX compile features - done}}
{{-- Arrow version: 0.15.0 (full: '0.15.0-SNAPSHOT')}}
{{-- Arrow SO version: 15 (full: 15.0.0)}}
{{-- Found PkgConfig: Z:/Systemdateien/Miniconda3/envs/pyarrow-dev/Library/bin/pkg-config.exe (found version ""0.29.2"")}}
{{-- clang-tidy not found}}
{{-- clang-format not found}}
{{-- infer not found}}
{{-- Found PythonInterp: Z:/Systemdateien/Miniconda3/envs/pyarrow-dev/python.exe (found version ""3.7.3"")}}
{{-- Found cpplint executable at Z:/devel/arrow/cpp/build-support/cpplint.py}}
{{-- Compiler command: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/x86_amd64/cl.exe}}
{{-- Compiler version:}}
{{-- Compiler id: MSVC}}
{{Selected compiler msvc}}
{{-- Performing Test CXX_SUPPORTS_SSE4_2}}
{{-- Performing Test CXX_SUPPORTS_SSE4_2 - Failed}}
{{-- Performing Test CXX_SUPPORTS_ALTIVEC}}
{{-- Performing Test CXX_SUPPORTS_ALTIVEC - Failed}}
{{-- Performing Test CXX_SUPPORTS_ARMCRC}}
{{-- Performing Test CXX_SUPPORTS_ARMCRC - Failed}}
{{-- Performing Test CXX_SUPPORTS_ARMV8_CRC_CRYPTO}}
{{-- Performing Test CXX_SUPPORTS_ARMV8_CRC_CRYPTO - Failed}}
{{CMake Error at cmake_modules/SetupCxxFlags.cmake:42 (string):}}
{{ string no output variable specified}}
{{Call Stack (most recent call first):}}
{{ CMakeLists.txt:357 (include)}}


{{-- Arrow build warning level: CHECKIN}}
{{Configured for build (set with cmake -DCMAKE_BUILD_TYPE=\{release,debug,...})}}
{{CMake Error at cmake_modules/SetupCxxFlags.cmake:438 (message):}}
{{ Unknown build type:}}
{{Call Stack (most recent call first):}}
{{ CMakeLists.txt:357 (include)}}


{{-- Configuring incomplete, errors occurred!}}
{{See also ""Z:/devel/arrow/cpp/build/CMakeFiles/CMakeOutput.log"".}}
{{See also ""Z:/devel/arrow/cpp/build/CMakeFiles/CMakeError.log"".}}

{{(pyarrow-dev) Z:\devel\arrow\cpp\build>}}",build pull-request-available,['C++'],ARROW,Bug,Major,2019-08-31 16:30:17,1
13254039,[C++] Suppress sign-compare warning with g++ 9.2.1,"{noformat}
    ../src/arrow/array/builder_union.cc: In constructor 'arrow::BasicUnionBuilder::BasicUnionBuilder(arrow::MemoryPool*, arrow::UnionMode::type, const std::vector<std::shared_ptr<arrow::ArrayBuilder> >&, const std::shared_ptr<arrow::DataType>&)':
    ../src/arrow/util/logging.h:86:55: error: comparison of integer expressions of different signedness: 'std::vector<std::shared_ptr<arrow::ArrayBuilder> >::size_type' {aka 'long unsigned int'} and 'signed char' [-Werror=sign-compare]
       86 | #define ARROW_CHECK_LT(val1, val2) ARROW_CHECK((val1) < (val2))
          |                                                ~~~~~~~^~~~~~~~
    ../src/arrow/util/macros.h:43:52: note: in definition of macro 'ARROW_PREDICT_TRUE'
       43 | #define ARROW_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
          |                                                    ^
    ../src/arrow/util/logging.h:86:36: note: in expansion of macro 'ARROW_CHECK'
       86 | #define ARROW_CHECK_LT(val1, val2) ARROW_CHECK((val1) < (val2))
          |                                    ^~~~~~~~~~~
    ../src/arrow/util/logging.h:135:19: note: in expansion of macro 'ARROW_CHECK_LT'
      135 | #define DCHECK_LT ARROW_CHECK_LT
          |                   ^~~~~~~~~~~~~~
    ../src/arrow/array/builder_union.cc:79:3: note: in expansion of macro 'DCHECK_LT'
       79 |   DCHECK_LT(type_id_to_children_.size(), std::numeric_limits<int8_t>::max());
          |   ^~~~~~~~~
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-31 04:07:57,1
13254036,[Java] Implement dictionary-encoded subfields for Struct type,"Implement dictionary-encoded subfields for Struct type.

Each child vector will have a dictionary, the dictionary vector is struct type and holds all dictionaries.",pull-request-available,['Java'],ARROW,Sub-task,Major,2019-08-31 03:00:51,16
13253979,[C++] Consolidate ScanOptions and ScanContext,"Currently ScanOptions has two distinct responsibilities: it contains the data selector (and eventually projection schema) for the current scan and it serves as the base class for format specific scan options.

In addition, we have ScanContext which holds the memory pool for the current scan.

I think these classes should be rearranged as follows: ScanOptions will be removed and FileScanOptions will be the abstract base class for format specific scan options. ScanContext will be a concrete struct and contain the data selector, projection schema, a vector of FileScanOptions, and any other shared scan state.",dataset,['C++'],ARROW,Improvement,Minor,2019-08-30 16:14:53,6
13253934,[C++][CI] Fix S3 minio failure,See[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/27065941/job/gwjmr2hudm7693ef],pull-request-available,"['C++', 'Continuous Integration']",ARROW,New Feature,Major,2019-08-30 13:26:50,13
13253921,[C++] Add ResolveNullOptions to Logical kernels,"This would add an enum ResolveNull \{ KLEENE_LOGIC, NULL_PROPAGATE } to define the behavior of merging with AND/OR operators on boolean.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-30 12:26:40,6
13253857,[Java] Support conversions between delta vector and partial sum vector,"What is a delta vector/partial sum vector?

Given an integer vector a with length n, its partial sum vector is another integer vector b with length n + 1, with values defined as:

b(0) = initial sum
b(i ) = a(0) + a(1) + ... + a(i - 1) i = 1, 2, ..., n

Given an integer vector with length n + 1, its delta vector is another integer vector b with length n, with values defined as:

b(i ) = a(i ) - a(i - 1), i = 0, 1, ... , n -1

In this issue, we provide utilities to convert between vector and partial sum vector. It is interesting to note that the two operations corresponding to the discrete integration and differentian.

These conversions have wide applications. For example,

1. The run-length vector proposed by Micah is based on the partial sum vector, while the deduplication functionality is based on delta vector. This issue provides conversions between them.

2. The current VarCharVector/VarBinaryVector implementations are based on partial sum vector. We can transform them to delta vectors before IPC, to reduce network traffic.

3. Converting to delta can be considered as a way for data compression. To further reduce the data volume, the operation can be applied more than once, to further reduce data volume.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-30 08:06:33,7
13253825,"[Python][Flight] list_actions Server RPC is not tested in test_flight.py, nor is return value validated","This server method is implemented and part of the Python server vtable, but it is not tested. If you mistakenly return a ""string"" action type, it will pass silently. We might want to constrain the output to be ActionType or a tuple",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2019-08-30 05:16:59,0
13253822,[Python][Flight] Add built-in methods on FlightServerBase to start server and wait for it to be available,"It seems like this logic could be a part of the library / made general purpose to make it more convenient to spawn servers in Python

https://github.com/apache/arrow/blob/master/python/pyarrow/tests/test_flight.py#L414",pull-request-available,"['FlightRPC', 'Python']",ARROW,Improvement,Major,2019-08-30 05:09:51,3
13253718,[C++][Documentation] Explicit documentation of null slot interpretation,"To my knowledge, there isn't explicit documentation on how null slots in an array should be interpreted. SQL uses Kleene logic, wherein a null is explicitly an unknown rather than a special value. This yields for example `(null AND false) -> false`, since `(x AND false) -> false` for all possible values of x. This is also the behavior of Gandiva's boolean expressions.

By contrast the boolean kernels implement something closer to the behavior of NaN: `(null AND false) -> null`. I think this is simply an error in the boolean kernels but in any case I think explicit documentation should be added to prevent future confusion.

",pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Major,2019-08-29 13:36:34,6
13253703,[C++] Investigate xxh3,"xxh3 is a new hash algorithm by Yann Collet that claims excellent speed on both small/tiny and large keys. It has accelerated paths for x86 SSE2, AVX and ARM NEON. It also has excellent hash quality.
https://fastcompression.blogspot.com/2019/03/presenting-xxh3.html

Perhaps this can replace our current complex strategy involving a custom tiny string hashing implementation, a HW CRC32-based path where available for large strings, and a murmurhash2 fallback.",pull-request-available,"['Benchmarking', 'C++']",ARROW,Task,Major,2019-08-29 12:06:37,2
13253692,[C++] Bump dependencies,This would probably be good before 0.15.0.,pull-request-available,['C++'],ARROW,Improvement,Minor,2019-08-29 11:08:09,2
13253584,[C++] BufferOutputStream::Write is slow for many small writes,{{Write}} calls into {{BufferOutputStream::Reserve}} which does a surprising amount of work. I suggest streamlining the implementation and adding benchmarks for the many-small-writes case,pull-request-available,['C++'],ARROW,Improvement,Critical,2019-08-28 22:31:13,14
13253555,[C++] Do not append any buffers when serializing NullType for IPC,Currently we send a length-0 buffer. It would be better to not include any buffers. ,pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-28 18:14:26,14
13253549,[C++][Dataset] Implement TreeDataSource,The TreeDataSource is required to support partitions pruning of sub-trees.,dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-28 17:13:38,13
13253544,"[Developer] PR merge script has ""master"" target ref hard-coded","If the target ref of a PR is something other than master, we should merge PRs into that branch",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-08-28 16:56:47,14
13253472,[Java] Refactor the code for TimeXXVectors,"This is based on the discussion in[https://lists.apache.org/thread.html/836d3b87ccb6e65e9edf0f220829a29edfa394fc2cd1e0866007d86e@%3Cdev.arrow.apache.org%3E.|https://lists.apache.org/thread.html/836d3b87ccb6e65e9edf0f220829a29edfa394fc2cd1e0866007d86e@%3Cdev.arrow.apache.org%3E,]



The internals of TimeXXVectors are simply IntVector or BigIntVector. There are duplicated code for setting/getting int/long.



We want to refactor the code by:
 # push get/set methods into the base class BaseFixedWidthVector, and make them protected.
 # The APIs in TimeXXVectors references the methods in the base class.



Note that this issue not just reduce redundant code, it also centralizes the logics for getting/setting int/long, making them easy to maintain and change.



If it looks good, later we will make other integer based vectors rely on the base class implementations.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-28 10:29:50,7
13253290,[Python] Support list-of-boolean in Array.to_pandas conversion,"See

{code}
In [4]: paste                                                                                                                                                                                  
a = pa.array(np.array([[True, False], [True, True, True]]))

## -- End pasted text --

In [5]: a                                                                                                                                                                                      
Out[5]: 
<pyarrow.lib.ListArray object at 0x7fc187d3fa40>
[
  [
    true,
    false
  ],
  [
    true,
    true,
    true
  ]
]

In [6]: a.to_pandas()                                                                                                                                                                          
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-6-08d835d60bd1> in <module>
----> 1 a.to_pandas()

~/code/arrow/python/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()
    439             deduplicate_objects=deduplicate_objects)
    440 
--> 441         return self._to_pandas(options, categories=categories,
    442                                ignore_metadata=ignore_metadata)
    443 

~/code/arrow/python/pyarrow/array.pxi in pyarrow.lib.Array._to_pandas()
    815 
    816         with nogil:
--> 817             check_status(ConvertArrayToPandas(c_options, self.sp_array,
    818                                               self, &out))
    819         return wrap_array_output(out)

~/code/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()
     84             raise ArrowKeyError(message)
     85         elif status.IsNotImplemented():
---> 86             raise ArrowNotImplementedError(message)
     87         elif status.IsTypeError():
     88             raise ArrowTypeError(message)

ArrowNotImplementedError: Not implemented type for lists: bool
In ../src/arrow/python/arrow_to_pandas.cc, line 1910, code: VisitTypeInline(*data_->type(), this)
{code}

as reported in https://github.com/apache/arrow/issues/5203",pull-request-available,['Python'],ARROW,Bug,Major,2019-08-27 15:45:33,14
13253271,[C++] Add RecordBatch projection functionality,"define classes RecordBatchProjector (which projects from one schema to another, augmenting with null/constant columns where necessary) and a subtype of RecordBatchIterator which projects each batch yielded by a wrapped iterator.",dataset pull-request-available,['C++'],ARROW,Improvement,Minor,2019-08-27 14:26:43,6
13253169,[Java] Make field vectors final explicitly,"According to the discussion in [https://lists.apache.org/thread.html/836d3b87ccb6e65e9edf0f220829a29edfa394fc2cd1e0866007d86e@%3Cdev.arrow.apache.org%3E,]field vectors should not be extended, so they should be made final explicitly.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-27 02:58:05,7
13253112,[R] Handling unexpected input to time64() et al,"{code:r}
> time64()
Error in Time64__initialize(unit) : 
  argument ""unit"" is missing, with no default
> time64(""ms"")
Error in Time64__initialize(unit) : 
  Not compatible with requested type: [type=character; target=integer].
> time64(1)
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0826 11:13:34.657388 162407872 type.cc:234]  Check failed: unit == TimeUnit::MICRO || unit == TimeUnit::NANO Must be microseconds or nanoseconds
*** Check failure stack trace: ***
Abort trap: 6

> time64(1L)
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0826 11:14:09.445202 251229632 type.cc:234]  Check failed: unit == TimeUnit::MICRO || unit == TimeUnit::NANO Must be microseconds or nanoseconds
*** Check failure stack trace: ***
Abort trap: 6

> time64(""MILLI"")
Error in Time64__initialize(unit) : 
  Not compatible with requested type: [type=character; target=integer].
> time64(TimeUnit$MILLI)
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0826 11:15:12.047847 361547200 type.cc:234]  Check failed: unit == TimeUnit::MICRO || unit == TimeUnit::NANO Must be microseconds or nanoseconds
*** Check failure stack trace: ***
Abort trap: 6
{code}",pull-request-available,['R'],ARROW,Bug,Major,2019-08-26 18:35:57,4
13253111,[R] segfault in Table__from_dots with unexpected schema,"{code:r}
> table(b=1L, schema=c(b = int16()))

 *** caught segfault ***
address 0x7fada725aed0, cause 'memory not mapped'
{code}",pull-request-available,['R'],ARROW,Bug,Major,2019-08-26 18:33:45,4
13253094,[C++] S3: more flexible credential options,"We should perhaps allow passing an optional {{AWSCredentialsProvider}} to {{S3FileSystem::Make}}, all the while keeping an option for a (access key, secret key) pair.

[http://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_auth_1_1_a_w_s_credentials_provider.html]",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-26 17:27:14,2
13253048,[C++] FileSystem::DeleteDir should make it optional to delete the directory itself,"In some situations, it can be desirable to delete the entirety of a directory's contents, but not the directory itself (e.g. when it's a S3 bucket). Perhaps we should add an option for that.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-26 14:16:00,2
13253029,[C++] S3: allow for background writes,"S3 writes can be synchronous: when a part is uploaded, Write() will only return once the server has received the contents and responded ""Ok"".

We may want to add a S3 option to issue writes in the background (e.g. using the S3 client's asynchronous APIs).

Another possibility is to provide a general OutputStream implementation that backgrounds writes (this should be independent of BufferedOutputStream, though, because S3FileSystem already has its own buffering layer).
",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-26 12:53:11,2
13253026,[Java] Avro adapter implement Enum type and nested Record type,"Implement for converting avro {{Enum}}type.

Convert nested avro {{Record}}type to Arrow {{StructVector}}.",pull-request-available,['Java'],ARROW,Sub-task,Major,2019-08-26 12:43:52,16
13253019,[Java] Make range equal visitor reusable,"According to the discussion in [https://github.com/apache/arrow/pull/4993#discussion_r316009165,]we often encountered this scenario: we compare values repeatedly. The comparisons differs only in the parameters (vector to compare, start index, etc).



According to the current API, we have to create a new RangeEqualVisitor object each time the comparison is performed. This leads to non-trivial performance overhead.



To address this problem, we make theRangeEqualVisitor reusable, and allow the client to change parameters of an existing visitor.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-26 12:22:33,7
13252996,[C++] Building without Parquet fails,"Seems like this is a recent regression:
{code}
[214/300] Building CXX object src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o
FAILED: src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o 
/usr/bin/ccache /usr/bin/g++-7  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -DAWS_COMMON_USE_IMPORT_EXPORT -DAWS_EVENT_STREAM_USE_IMPORT_EXPORT -DAWS_SDK_VERSION_MAJOR=1 -DAWS_SDK_VERSION_MINOR=7 -DAWS_SDK_VERSION_PATCH=160 -Isrc -I../src -isystem /home/antoine/miniconda3/envs/pyarrow/include -isystem double-conversion_ep/src/double-conversion_ep/include -isystem ../thirdparty/hadoop/include -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -D_GLIBCXX_USE_CXX11_ABI=1 -D_GLIBCXX_USE_CXX11_ABI=1 -fno-omit-frame-pointer -g -fPIE   -pthread -std=gnu++11 -MD -MT src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o -MF src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o.d -o src/arrow/dataset/CMakeFiles/arrow-dataset-dataset-test.dir/dataset_test.cc.o -c ../src/arrow/dataset/dataset_test.cc
In file included from ../src/parquet/arrow/writer.h:25:0,
                 from ../src/arrow/dataset/test_util.h:27,
                 from ../src/arrow/dataset/dataset_test.cc:20:
../src/parquet/properties.h:30:10: fatal error: parquet/parquet_version.h: Aucun fichier ou dossier de ce type
 #include ""parquet/parquet_version.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-08-26 10:28:43,13
13252908,[Java] Add implementation of DenseUnionVector.,Today only Sparse unions are supported. We should have a dense union implementation vector that conforms to the IPC protocol (the current sparse union vector doesn't do this and there are other JIRAs covering making it compatible).,pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-26 04:05:18,7
13252829,[R] arrow::read_csv_arrow namespace error when package not loaded,"{quote}I am not sure if the arrow::read_csv_arrow() error below is a bug or a feature?



data(""iris"")
 write.csv(iris, ""iris.csv"")
 test <- arrow::read_csv_arrow(""iris.csv"")
 Error in read_delim_arrow(file = ""iris.csv"", delim = "","") :
 could not find function ""read_delim_arrow""
 test <- arrow::read_delim_arrow(""iris.csv"")
 sessionInfo()
 R version 3.6.1 (2019-07-05)
 Platform: x86_64-apple-darwin18.6.0 (64-bit)
 Running under: macOS Mojave 10.14.6
{quote}
Matrix products: default
 BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
 LAPACK: /usr/local/Cellar/openblas/0.3.7/lib/libopenblasp-r0.3.7.dylib

locale:
 [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
 [1] stats graphics grDevices utils datasets
 [6] methods base

loaded via a namespace (and not attached):
 [1] tidyselect_0.2.5 bit_1.1-14 compiler_3.6.1
 [4] magrittr_1.5 assertthat_0.2.1 R6_2.4.0
 [7] tools_3.6.1 fs_1.3.1 glue_1.3.1
 [10] Rcpp_1.0.2 bit64_0.9-7 arrow_0.14.1.1
 [13] rlang_0.4.0 purrr_0.3.2",pull-request-available,['R'],ARROW,Bug,Minor,2019-08-24 20:25:49,4
13252760,[Python] Implement low-level bindings for Dataset,"The following classes should be accessible from Python:
 * class DataSource
 * class DataSourceDiscovery
 * class Dataset
 * class ScanContext, ScanOptions, ScanTask
 * class ScannerBuilder
 * class Scanner

The end result is reading a directory of parquet files as a single stream. One should be able to re-implement[https://github.com/apache/arrow/pull/5720]in python.",dataset pull-request-available,['Python'],ARROW,New Feature,Major,2019-08-23 21:18:09,3
13252759,[R] Implements low-level bindings to Dataset classes,"The following classes should be accessible from R:
 * class DataSource
 * class DataSourceDiscovery
 * class Dataset
 * class ScanContext, ScanOptions, ScanTask
 * class ScannerBuilder
 * class Scanner

The end result is reading a directory of parquet files as a single stream. One should be able to re-implement[https://github.com/apache/arrow/pull/5720]in R.

See also[https://github.com/apache/arrow/pull/5675/files]for another end-to-end example in C++.",dataset pull-request-available,"['C++', 'R']",ARROW,New Feature,Major,2019-08-23 21:17:06,4
13252753,[Python][C++] Rowgroup statistics for pd.NaT array ill defined,"When initialising an array with NaT only values the row group statistic is corrupt returning either random values or raises integer out of bound exceptions.
{code:python}
import io
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

df = pd.DataFrame({""t"": pd.Series([pd.NaT], dtype=""datetime64[ns]"")})
buf = pa.BufferOutputStream()
pq.write_table(pa.Table.from_pandas(df), buf, version=""2.0"")
buf = io.BytesIO(buf.getvalue().to_pybytes())
parquet_file = pq.ParquetFile(buf)
# Asserting behaviour is difficult since it is random and the state is ill defined. 
# After a few iterations an exception is raised.
while True:
    parquet_file.metadata.row_group(0).column(0).statistics.max
{code}
",pull-request-available,['Python'],ARROW,Bug,Minor,2019-08-23 20:40:40,8
13252721,[R] Type function names don't match type names,"I noticed this while working on documentation for ARROW-5505, trying to show how you could pass an explicit schema definition to make a table. For a few types, the name of the type that gets printed (and comes from the C++ library) doesn't match the name of the function you use to specify the type in a schema:
{code:r}
> tab <- to_arrow(data.frame(
+   a = 1:10,
+   b = as.numeric(1:10),
+   c = sample(c(TRUE, FALSE, NA), 10, replace = TRUE),
+   d = letters[1:10],
+   stringsAsFactors = FALSE
+ ))
> tab$schema
arrow::Schema 
a: int32
b: double
c: bool
d: string 
# Alright, let's make that schema
> schema(a = int32(), b = double(), c = bool(), d = string())
Error in bool() : could not find function ""bool""
# Hmm, ok, so bool --> boolean()
> schema(a = int32(), b = double(), c = boolean(), d = string())
Error in string() : could not find function ""string""
# string --> utf8()
> schema(a = int32(), b = double(), c = boolean(), d = utf8())
Error: type does not inherit from class arrow::DataType
# Wha?
> double()
numeric(0)
# Oh. double is a base R function.
> schema(a = int32(), b = float64(), c = boolean(), d = utf8())
arrow::Schema 
a: int32
b: double
c: bool
d: string 
{code}
If you believe this switch statement is correct, these three, along with float and half_float, are the only mismatches:[https://github.com/apache/arrow/blob/master/r/R/R6.R#L81-L109]

{code:r}
> schema(b = float64(), c = boolean(), d = utf8(), e = float32(), f = float16())
arrow::Schema 
b: double
c: bool
d: string
e: float
f: halffloat 
{code}

I canadd aliases (i.e. another function that does the same thing) for bool, string, float, and halffloat, and I can add some magic so that double() (and even integer()) work inside the schema() function. But in looking into the C++ side to confirm where these alternate type names were coming from, I saw some inconsistencies. For example, https://github.com/apache/arrow/blob/master/cpp/src/arrow/type.h#L773-L788 suggests that the StringType should report its name as ""utf8"". But the ToString method here https://github.com/apache/arrow/blob/master/cpp/src/arrow/type.cc#L191 has it report as ""string"". It's unclear why those should report differently.",pull-request-available,['R'],ARROW,Improvement,Major,2019-08-23 17:08:01,4
13252685,[Python] Clarify pyarrow.serialize/deserialize docstrings viz-a-viz relationship with Arrow IPC protocol,Some users have been confused that these functions are equivalent in some way to IPC streams. We should add language explaining in more detail what they do and when to use them,pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-23 14:17:20,14
13252653,[Java] Improve the performance of DictionaryHashTable,"when comparing two entries in the dictionary hash table, it is more efficient to compare the index directly, rather than using Objects.equals, because they are both ints.",pull-request-available,['Java'],ARROW,Improvement,Trivial,2019-08-23 12:33:33,7
13252633,[Java] Improve the dictionary builder API to return the position of the value in the dictionary,"This is an improvement of the {{addValue}}method.

Previously, the method returns a boolean, indicating if the value has been successfully added to the dictionary.

After the change, the method returns an integer, which is the position of the value in the dictionary.

The purpose of this change:
 # the dictionary position contains more information, compared with a boolean indicating if the value is added successfully.
 # this information about the index in the dictionary can be useful, for example, to collect statistics about the dictionary.

With the dictionary position, the information about if a value has been added can be easily determined.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-23 10:27:15,7
13252578,[Java] Incorporate ErrorProne into the java build,[Using https://github.com/google/error-prone|https://github.com/google/error-prone]seems like it would be a good idea to automatically catch more errors.,pull-request-available,"['Continuous Integration', 'Java']",ARROW,Improvement,Major,2019-08-23 04:53:53,16
13252576,[C++] Include missing headers in api.h,I think result.h and array/concatenate.h should be included as they export public symbols.,pull-request-available,['C++'],ARROW,Bug,Minor,2019-08-23 04:42:02,15
13252555,"[Format] Add 4-byte ""stream continuation"" to IPC message format to align Flatbuffers",This is the JIRA corresponding to the mailing list discussion,pull-request-available,['Format'],ARROW,Improvement,Major,2019-08-23 00:23:14,15
13252439,[Python] wrong conversion of DataFrame with boolean values,"From https://github.com/pandas-dev/pandas/issues/28090

{code}
In [19]: df = pd.DataFrame(np.ones((3, 2), dtype=bool), columns=['a', 'b']) 

In [20]: df  
Out[20]: 
      a     b
0  True  True
1  True  True
2  True  True

In [21]: table = pa.table(df) 

In [23]: table.column(0)
Out[23]: 
<pyarrow.lib.ChunkedArray object at 0x7fd08a96e090>
[
  [
    true,
    false,
    false,
  ]
]
{code}

The resulting table has False values while the original DataFrame had only true values. 
It seems this has to do with the fact that it are multiple columns, as with a single column it converts correctly.",pull-request-available,['Python'],ARROW,Bug,Major,2019-08-22 18:11:15,5
13252430,[R] Expand file paths when passing to readers,"All file paths in R are wrapped in {{fs::path_abs()}}, which handles relative paths, but it doesn't expand {{~}}, so this fails:
{code:java}
> df <- read_parquet(""~/Downloads/demofile.parquet"")
 Error in io___MemoryMappedFile__Open(fs::path_abs(path), mode) :
  IOError: Failed to open local file '~/Downloads/demofile.parquet', error: No such file or directory
{code}
This is fixed by using {{fs::path_real()}} instead.

Should this be properly handled in C++ though? cc [~pitrou]",pull-request-available,['R'],ARROW,Bug,Major,2019-08-22 17:02:33,4
13252327,[Python] Ability to create ExtensionBlock on conversion to pandas,"To be able to create a pandas DataFrame in {{to_pandas()}} that holds ExtensionArrays (e.g. towards ARROW-2428 to register a conversion), we first need to add to the {{table_to_blockmanager}} / {{ConvertTableToPandas}} conversion utilities the ability to create an pandas {{ExtensionBlock}} that can hold a pandas {{ExtensionArray}}.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-22 08:53:01,5
13252263,[Integration] Update integration test to use generated binaries to ensure backwards compatibility,Generate stream/file data and check it in to the testing package. Update the integration script to have additional tests that run against the pregenerated artificats.,pull-request-available,['Developer Tools'],ARROW,Sub-task,Blocker,2019-08-22 03:42:25,15
13252259,[Java] Make change to ensure flatbuffer reads are aligned ,See parent bug for details on requirements.,pull-request-available,['Java'],ARROW,Sub-task,Blocker,2019-08-22 03:35:32,16
13252257,[Format] Tracking for ensuring flatbuffer serialized values are aligned in stream/files.,"Overall tracking bug for implementation for IPC/File format proposed by:[https://github.com/apache/arrow/pull/4951/files]



Implementations must support backwards compatibility with the old format (and ideally do memcopies when required to avoid undefined behavior). Having a backwards compatible write mode is optional",pull-request-available,['Format'],ARROW,Improvement,Blocker,2019-08-22 03:33:05,14
13252189,[C++] Declare required Libs.private in arrow.pc package config,"The current arrow.pc package config file produced is deficient and doesn't properly declare static libraries pre-requisities that must be linked in in order to *statically* link in libarrow.a

Currently it just has:

```
 Libs: -L${libdir} -larrow

```

But in cases, e.g. where you enabled snappy, brotli or zlib support in arrow, our toolchains need to see an arrow.pc file something more like:

```
 Libs: -L${libdir} -larrow
 Libs.private: -lsnappy -lboost_system -lz -llz4 -lbrotlidec -lbrotlienc -lbrotlicommon -lzstd

```

If not, we get linkage errors. I'm told the convention is that ifthe .a has an UNDEF, the Requires.private plus the Libs.private should resolve all the undefs. See the Libs.private info in[https://linux.die.net/man/1/pkg-config]



Note, however, asSutou Kouhei pointed out in [https://github.com/apache/arrow/pull/5123#issuecomment-522771452,]the additional Libs.private need to be dynamically generated based on whether functionality like snappy, brotli or zlib is enabled..",pull-request-available,['C++'],ARROW,Bug,Major,2019-08-21 18:05:54,1
13252179,[Java] Make ApproxEqualsVisitor accept DiffFunction to make it more flexible,"Currently {{ApproxEqualsVisitor}}will accept a epsilon for both float and double compare, and the difference calculation is always {{Math.abs}}(f1-f2)

For some cases like {{Validator}}it is not very suitable as:

i. it has different epsilon values for float/double

ii. it difference function is not Math.abs(f1-f2)



To resolve these, make this visitor accept both float/double epsilons and diff functions.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-21 17:08:04,16
13252175,[C++] Write 64-bit integers as strings in JSON integration test files,C++ side of ARROW-1875,pull-request-available,['C++'],ARROW,Sub-task,Major,2019-08-21 16:38:27,6
13252136,[C++] Parquet tests and executables are linked statically,"For some reason, on Linux Parquet tests are now statically linked with {{libparquet}} and {{libarrow}} by default, even though other tests (Arrow, Plasma...) are dynamically-linked.

For example:
{code}
$ ldd build-test/debug/parquet-schema-test 
	linux-vdso.so.1 (0x00007ffd376ad000)
	libgtest_main.so => /home/antoine/miniconda3/envs/pyarrow/lib/libgtest_main.so (0x00007f3affeaf000)
	libgtest.so => /home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so (0x00007f3affde5000)
	libbz2.so.1.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libbz2.so.1.0 (0x00007f3affdd1000)
	liblz4.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/liblz4.so.1 (0x00007f3aff58d000)
	libsnappy.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libsnappy.so.1 (0x00007f3aff384000)
	libz.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libz.so.1 (0x00007f3affdb1000)
	libzstd.so.1.3.7 => /home/antoine/miniconda3/envs/pyarrow/lib/libzstd.so.1.3.7 (0x00007f3affd0a000)
	libboost_filesystem.so.1.67.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libboost_filesystem.so.1.67.0 (0x00007f3aff168000)
	libstdc++.so.6 => /home/antoine/miniconda3/envs/pyarrow/lib/libstdc++.so.6 (0x00007f3afeff4000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3afec56000)
	libgcc_s.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libgcc_s.so.1 (0x00007f3affcbb000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3afe865000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f3afe646000)
	libboost_system.so.1.67.0 => /home/antoine/miniconda3/envs/pyarrow/lib/./libboost_system.so.1.67.0 (0x00007f3afe441000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f3afe239000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f3affc8f000)
{code}

Contrast with e.g.:
{code}
$ ldd build-test/debug/arrow-uri-test 
	linux-vdso.so.1 (0x00007ffe07fb6000)
	libarrow.so.15 => /home/antoine/arrow/dev/cpp/build-test/debug/libarrow.so.15 (0x00007f774f340000)
	libboost_system.so.1.67.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libboost_system.so.1.67.0 (0x00007f774f13b000)
	libgtest_main.so => /home/antoine/miniconda3/envs/pyarrow/lib/libgtest_main.so (0x00007f7751723000)
	libgtest.so => /home/antoine/miniconda3/envs/pyarrow/lib/libgtest.so (0x00007f7751659000)
	libstdc++.so.6 => /home/antoine/miniconda3/envs/pyarrow/lib/libstdc++.so.6 (0x00007f774efc7000)
	libgcc_s.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libgcc_s.so.1 (0x00007f7751645000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f774ebd6000)
	libaws-cpp-sdk-s3.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-s3.so (0x00007f774e990000)
	libaws-cpp-sdk-core.so => /home/antoine/miniconda3/envs/pyarrow/lib/libaws-cpp-sdk-core.so (0x00007f774e893000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f774e68f000)
	liburiparser.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/liburiparser.so.1 (0x00007f77515f2000)
	libbz2.so.1.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libbz2.so.1.0 (0x00007f77515de000)
	liblz4.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/liblz4.so.1 (0x00007f774e46b000)
	libsnappy.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libsnappy.so.1 (0x00007f774e262000)
	libz.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/libz.so.1 (0x00007f77515bc000)
	libzstd.so.1.3.7 => /home/antoine/miniconda3/envs/pyarrow/lib/libzstd.so.1.3.7 (0x00007f774e1bd000)
	libboost_filesystem.so.1.67.0 => /home/antoine/miniconda3/envs/pyarrow/lib/libboost_filesystem.so.1.67.0 (0x00007f774dfa1000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f774dd82000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f774d9e4000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f7751503000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f774d7dc000)
	libcurl.so.4 => /home/antoine/miniconda3/envs/pyarrow/lib/./libcurl.so.4 (0x00007f7751534000)
	libcrypto.so.1.1 => /home/antoine/miniconda3/envs/pyarrow/lib/./libcrypto.so.1.1 (0x00007f774d512000)
	libaws-c-event-stream.so.0unstable => /home/antoine/miniconda3/envs/pyarrow/lib/./libaws-c-event-stream.so.0unstable (0x00007f775152b000)
	libaws-c-common.so.0unstable => /home/antoine/miniconda3/envs/pyarrow/lib/./libaws-c-common.so.0unstable (0x00007f774d4ef000)
	libssh2.so.1 => /home/antoine/miniconda3/envs/pyarrow/lib/././libssh2.so.1 (0x00007f774d4bd000)
	libssl.so.1.1 => /home/antoine/miniconda3/envs/pyarrow/lib/././libssl.so.1.1 (0x00007f774d42d000)
	libgssapi_krb5.so.2 => /home/antoine/miniconda3/envs/pyarrow/lib/././libgssapi_krb5.so.2 (0x00007f774d3de000)
	libkrb5.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libkrb5.so.3 (0x00007f774d302000)
	libk5crypto.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libk5crypto.so.3 (0x00007f774d2e1000)
	libcom_err.so.3 => /home/antoine/miniconda3/envs/pyarrow/lib/././libcom_err.so.3 (0x00007f774d2db000)
	libaws-checksums.so => /home/antoine/miniconda3/envs/pyarrow/lib/././libaws-checksums.so (0x00007f774d2cd000)
	libkrb5support.so.0 => /home/antoine/miniconda3/envs/pyarrow/lib/./././libkrb5support.so.0 (0x00007f774d2be000)
	libresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007f774d0a3000)
{code}

Static linking makes building slower and the test executables larger.",pull-request-available,['C++'],ARROW,Bug,Major,2019-08-21 13:59:12,1
13252121,[Java] Provide RLE vector,"RLE (run length encoding) is a widely used encoding/decoding technique. Compared with other encoding/decoding techniques, it is easier to work with the encoded data.
 
 We want to provide an RLE vector implementation in Arrow. The design details include:
 
 1. RleVector implements ValueVector.
2. the data structure of RleVector includes an inner vector, plus a buffer storing the end indices for runs.
3. we provide random access, with time complexity O(log(n)), so it should not be used frequently.
 4. In the future, we will provide iterators to access the vector in sequence.
 5. RleVector does not support update, but supports appending.
 6. In the future, we will provide encoder/decoder to efficiently transform encoded/decoded vectors.
 ",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-21 12:34:03,7
13252116,[Java] Support stable sort by stable comparators,"Stable sort is desirable in many scenarios. It means equal elements preserve their relative order after sorting.

There are stable sort algorithms. However, in practice, the best sort algorithm is quick sort and quick sort is not stable.

To make the best of both worlds, we support stable sort by stable comparators. It differs from an ordinary comparator in that it breaks ties by comparing the value indices.

With the stable comparator, the quick sort algorithm becomes a stable algorithm.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-21 12:18:36,7
13252023,[Java] Add description to each maven artifact,Note experimental/contrib nature of package and a brief description.,pull-request-available,"['Documentation', 'Java']",ARROW,Improvement,Major,2019-08-21 06:35:04,15
13251852,[Python] atexit: pyarrow.lib.ArrowKeyError: 'No type extension with name arrow.py_extension_type found',"On interrupt, I am frequently seeing the atexit function failing in pyarrow 0.14.1.
{code:java}
 ^CSIGINT/SIGQUIT received...killing workers... 
killing the spooler with pid 22640 
Error in atexit._run_exitfuncs: 
Traceback (most recent call last): 
 File ""/home/alpha/.virtualenvs/wsgi/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs 
 func(*targs, **kargs) 
 File ""pyarrow/types.pxi"", line 1860, in pyarrow.lib._unregister_py_extension_type 
 check_status(UnregisterPyExtensionType()) 
 File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status 
 raise ArrowKeyError(message) 
ArrowKeyError: 'No type extension with name arrow.py_extension_type found' 
Error in sys.exitfunc: 
Traceback (most recent call last): 
 File ""/home/alpha/.virtualenvs/wsgi/lib/python2.7/atexit.py"", line 24, in _run_exitfuncs 
 func(*targs, **kargs) 
 File ""pyarrow/types.pxi"", line 1860, in pyarrow.lib._unregister_py_extension_type 
 File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status 
pyarrow.lib.ArrowKeyError: 'No type extension with name arrow.py_extension_type found' 
spooler (pid: 22640) annihilated 
worker 1 buried after 1 seconds 
goodbye to uWSGI.{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2019-08-20 14:50:47,14
13251849,[C++] Add io::OutputStream::Abort(),"This method would abort the current output stream without trying to flush or commit any pending internal data. This makes sense mostly for buffered streams. For other streams it could simply synonymous to Close().
",pull-request-available,['C++'],ARROW,Wish,Major,2019-08-20 14:25:20,2
13251836,[C++] Simplify FileFormat classes to singletons,"ParquetFileFormat has no state, so passing it around by shared_ptr<FileFormat> is not necessary; we could just keep a single static instance and pass raw pointers.

[~wesmckinn] is there a case where a FileFormat might have state?",dataset,['C++'],ARROW,Improvement,Minor,2019-08-20 13:32:47,6
13251820,[Java] Compare ArrowBufPointers by unsinged integers,"Currently, ArrowBufPointers compare by bytes in lexicographic order. Another way is to compare by unsigned integers (longs, ints, & bytes).

The second way involves additional bit operations for each iteration. However, it can compare 8 bytes at a time. So it is overall faster:



Compare by unsigned integers:

ArrowBufPointerBenchmarks.compareBenchmark avgt 5 65.722  0.381 ns/op



Compare byte-wise:
ArrowBufPointerBenchmarks.compareBenchmark avgt 5 681.372  0.604 ns/op",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-20 12:24:30,7
13251776,[Java] Cleanup JDBC interfaces and eliminate one memcopy for binary/varchar fields,"* If we use direct setting of fields, we can avoid the extra temporary buffer and memcpy by setting bytes directly.
 * We should overwrite existing vectors in consumers before returning results, to avoid the possibility of closing vectors in use (or alternatively make sure we retain the underlying buffers).
 * Try to eliminate some of the state in load() by moving initialization to the constructor.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-20 08:21:36,16
13251681,[C++] Add an option to build with mimalloc,"It's a new allocator, Apache-licensed, by Microsoft. It claims very good performance and is cross-platform (works on Windows and Unix).
https://github.com/microsoft/mimalloc/

There's a detailed set of APIs including aligned allocation and zero-initialized allocation. However, zero-initialized reallocation doesn't seem provided.
https://microsoft.github.io/mimalloc/group__malloc.html#details
",pull-request-available,['C++'],ARROW,Wish,Major,2019-08-19 20:43:27,2
13251611,[C++] CMake ignores ARROW_PARQUET,Passing {{-DARROW_PARQUET=off}} is ignored and Parquet is always built. It shouldn't.,pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2019-08-19 15:00:55,14
13251453,[Java] Add empty() in UnionVector to create instance,Currently complex type vectors all have {{empty}}()API to create instance except {{UnionVector}}.,pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-18 15:33:02,16
13251452,[Java] Implement TypeEqualsVisitor comparing vector type equals considering names and metadata,"Currently when we compare range/vector equals, we first compare vector {{Field}}by its equals method, in this case, its hard to specify whether compare names or metadata.

Implement a {{TypeEqualsVisitor}}will make type comparisons more flexible like cpp implementation dose [https://github.com/apache/arrow/blob/master/cpp/src/arrow/compare.cc#L712]",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-18 15:31:17,16
13251411,[Rust] [DataFusion] Refactor TableProvider to return thread-safe BatchIterator,This refactor is a step towards implementing the new query execution that supports partitions and parallel execution.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Sub-task,Major,2019-08-17 18:29:48,10
13251301,[Python] Add Table.slice method or allow slices in __getitem__,This would improve usability,pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-16 17:28:52,14
13251298,[R] Read parquet files from raw vector,"{{read_parquet}} currently handles a path to a local file or an Arrow input stream. Would it be possible to add support for a raw vector containing the contents of a parquet file?

Apologies if there is already a way to do this. I have tried populating a buffer and passing that as input, butthat is unsupported as well. An example of how to work using an input stream would be useful as well.",pull-request-available,['R'],ARROW,New Feature,Major,2019-08-16 17:21:58,4
13251292,[C++][Parquet] Support reading/writing other Parquet primitive types to DictionaryArray,"As follow up to ARROW-3246, we should support direct read/write of the other Parquet primitive types. Currently only BYTE_ARRAY is implemented as it provides the most performance benefit.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-16 17:12:26,6
13251262,[C++][Fuzzing] Add fuzzer for parquet->arrow read path,The parquet to arrow read path is likely the most commonly used one (esp. by pyarrow) and is a closed step that should allow us to fuzz the reading of untrusted parquet files into memory. This complements the existing arrow ipc fuzzer.,fuzzer pull-request-available,"['C++', 'Continuous Integration', 'Developer Tools']",ARROW,Bug,Major,2019-08-16 14:21:27,2
13251172,[Java] Resolve the ambiguous method overload in RangeEqualsVisitor,"InRangeEqualsVisitor, there are overload methods for both super class and sub class. Thiswill lead to unexpected behavior.

For example, if we callRangeEqualsVisitor#visit(v), where v is a fixed width vector, the method actually called may be visit(ValueVector), which is unexpected.

In general, in the visitor pattern,it is not a good idea to support method overload for both super class and sub-class as parameters.",pull-request-available,['Java'],ARROW,Bug,Blocker,2019-08-16 06:36:51,7
13251160,[Java] Avro adapter implement Array/Map/Fixed type,Support Array/Map/Fixed type in avro adapter.,pull-request-available,['Java'],ARROW,Sub-task,Critical,2019-08-16 05:31:15,16
13251154,[Java] There is no need to consider byte order in ArrowBufHasher,"According to the discussion in [https://github.com/apache/arrow/pull/5063#issuecomment-521276547|https://github.com/apache/arrow/pull/5063#issuecomment-521276547.], Arrow has a mechanism to make sure the data is stored in little-endian, so there is no need to check byte order.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-16 03:42:44,7
13251142,[Python] RecordBatch.from_arrays does not check array types against a passed schema,"Example came from ARROW-6038

{code}
In [4]: pa.RecordBatch.from_arrays([pa.array([])], schema)                                                  
Out[4]: <pyarrow.lib.RecordBatch at 0x7fc36fa18db8>

In [5]: rb = pa.RecordBatch.from_arrays([pa.array([])], schema)                                             

In [6]: rb                                                                                                  
Out[6]: <pyarrow.lib.RecordBatch at 0x7fc37d9c69f8>

In [7]: rb.schema                                                                                           
Out[7]: col: string

In [8]: rb[0]                                                                                               
Out[8]: 
<pyarrow.lib.NullArray object at 0x7fc36fa8ce08>
0 nulls

{code}",beginner pull-request-available,['Python'],ARROW,Bug,Major,2019-08-16 02:35:56,14
13251120,[Developer] Show JIRA issue before merging,"It's useful to confirm whehter the associated JIRA issue is right or not.
    
We couldn't find wrong associated JIRA issue after we merge the pull request https://github.com/apache/arrow/pull/5050 .
",pull-request-available,['Developer Tools'],ARROW,Improvement,Minor,2019-08-16 00:14:39,1
13251104,[Website] Use deploy key on Travis to build and push to asf-site,"ARROW-4473 added CI/CD for the website, but there was some discomfort about having a committer provide a GitHub personal access token to do the pushing of the built site to the asf-site branch. Investigate using GitHub Deploy Keys instead, which are scoped to a single repository, not all public repositories that a user has access to.",pull-request-available,['Website'],ARROW,Improvement,Major,2019-08-15 21:52:34,4
13251103,[C++][CI] Flatbuffers-related failures in CI on macOS,"This seemingly has just started happening randomly today

https://travis-ci.org/apache/arrow/jobs/572381802#L2864",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-08-15 21:34:30,14
13251102,[R] Add macOS build scripts,"CRAN builds binary packages for Windows and macOS. It generally does this by building on its servers and bundling all dependencies in the R package. This has been accomplished by having separate processes for building and hosting system dependencies, and then downloading and bundling those with scripts that get executed at install time (and then create the binary package as a side effect).

ARROW-3758 added the Windows PKGBUILD and related packaging scripts and ran them on our Appveyor. This ticket is to do the same for the macOS scripts.

The purpose of these tickets is to bring the whole build pipeline under our version control and CI so that we can address any C++ build and dependency changes as they arise and not be surprised when it comes time to cut a release. A side benefit is that they also enable us to offer a nightly binary package repository with minimal additional effort.",pull-request-available,['R'],ARROW,Improvement,Major,2019-08-15 21:28:32,4
13251061,"[Python] Expose ""enable_buffered_stream"" option from parquet::ReaderProperties in pyarrow.parquet.read_table",See also PARQUET-1370,pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-15 16:54:35,3
13251046,[Python] Add pyarrow.Array.diff method that exposes arrow::Diff,This would expose the Array diffing functionality in Python to make it easier to see why arrays are unequal,pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-15 15:52:29,14
13251023,[Developer] Add PR merge tool to apache/arrow-site,This will help with creating clean patches and also keeping JIRA clean,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-08-15 13:55:57,14
13251014,[Java] Implement ApproxEqualsVisitor comparing approx for floating point,"Currently we already implemented {{RangeEqualsVisitor/VectorEqualsVisitor}}for comparing range/vector.

And ARROW-6211 is created to make {{ValueVector}}work with generic visitor.

We should also implement {{ApproxEqualsVisitor}}to compare floating point just like cpp does

[https://github.com/apache/arrow/blob/master/cpp/src/arrow/compare.cc]",pull-request-available,['Java'],ARROW,New Feature,Critical,2019-08-15 13:12:28,16
13251012,[Java] Remove useless class ByteArrayWrapper,"This class was introduced into encoding part to compare byte[] values equals.

Since now we compare value/vector equals by new added visitor API by ARROW-6022 instead of comparing {{getObject}}, this class is no use anymore.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-15 13:08:07,16
13251004,[Python] Use FileNotFoundError in HadoopFileSystem.open() in Python 3 ,"When file is absent pyarrow throws
{code:python}
ArrowIOError('HDFS file does not exist: ...')
{code}
which inherits from {{IOError}} and {{pyarrow.lib.ArrowException}}, it would be better if that was {{FileNotFoundError}} a subclass of {{IOError}} for this particular purpose. Also, {{.errno}} property is empty (should be 2) so one needs to match by error message to check for particular error.

*P.S.* There is no  {{FileNotFoundError}} in Python 2, but there is {{.errno}} property there.",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-08-15 11:54:44,2
13250950,[Java] Provide a common interface for float4 and float8 vectors,"We want to provide an interface for floating point vectors (float4 & float8). This interface will make it convenient for many operations on a vector. With this interface, the client code will be greatly simplified, with many branches/switch removed.



The design is similar to BaseIntVector (the interface for all integer vectors). We provide 3 methods for setting & getting floating point values:

 setWithPossibleTruncate

 setSafeWithPossibleTruncate

 getValueAsDouble",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-15 05:46:21,7
13250933,[Website] Add link to R documentation site,"ARROW-6139 added the R documentation at /docs/r/, but we still need to link to it from the website header.",pull-request-available,['Website'],ARROW,Improvement,Minor,2019-08-15 03:02:01,4
13250929,[Java] Provide an interface for numeric vectors,"We want to provide an interface for all vectors with numeric types (small int, float4, float8, etc). This interface will make it convenient for many operations on a vector, like average, sum, variance, etc. With this interface, the client code will be greatly simplified, with many branches/switch removed.



The design is similar to BaseIntVector (the interface for all integer vectors). We provide 3 methods for setting & getting numeric values:

 setWithPossibleRounding

 setSafeWithPossibleRounding

 getValueAsDouble",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-15 02:28:00,7
13250888,[C++] Implement Partition DataSource,This is a DataSource that also has partition metadata. The end goal is to support filtering with a DataSelector/Filter expression. The initial implementation should not deal with PartitionScheme yet.,dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-14 22:21:51,6
13250887,[C++] Implement basic Filter expression classes,This will draft the basic classes for creating boolean expressions that are passed to the DataSources/DataFragments for predicate push-down.,dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-14 22:18:23,6
13250886,[C++] Implements basic Dataset/Scanner/ScannerBuilder,"The goal of this would be to iterate over a Dataset and generate a ""flattened"" stream of RecordBatches from the union of data sources and data fragments. This should not bother with filtering yet.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-14 22:16:09,13
13250885,[Java] Failures on master,"I'm getting builds failing today with errors like

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.2:compile (default-compile) on project arrow-vector: Compilation failure: Compilation failure:
[ERROR] /home/travis/build/apache/arrow/java/vector/src/main/java/org/apache/arrow/vector/complex/ListVector.java:[356,4] error: cannot find symbol
[ERROR] symbol:   variable Preconditions
[ERROR] location: class ListVector
[ERROR] /home/travis/build/apache/arrow/java/vector/src/main/java/org/apache/arrow/vector/complex/NonNullableStructVector.java:[96,4] error: cannot find symbol
[ERROR] symbol:   variable Preconditions
[ERROR] location: class NonNullableStructVector
[ERROR] -> [Help 1]
{code}

see https://travis-ci.org/apache/arrow/jobs/571958044

Is this introduced by a recent patch?",pull-request-available,['Java'],ARROW,Bug,Blocker,2019-08-14 22:13:41,16
13250839,[R] Add option to set CXXFLAGS when compiling R package with $ARROW_R_CXXFLAGS,I want to be able to pass {{-fno-omit-frame-pointer}} with an environment variable,pull-request-available,['R'],ARROW,Improvement,Major,2019-08-14 17:51:46,14
13250794,[Java] ListVector hashCode() is not correct,"Current implement is not correct:
{code:java}
for (int i = start; i < end; i++) {
  hash = 31 * vector.hashCode(i);
}
{code}
Should be something like:
{code:java}
hash = 31 * hash + vector.hashCode(i);{code}",pull-request-available,['Java'],ARROW,Bug,Minor,2019-08-14 13:16:18,16
13250697,[C++] Rename Argsort kernel to SortToIndices,"""Argsort"" is NumPy specific name. Other languages/libraries use
different name:

  * R: order
    * https://cran.r-project.org/doc/manuals/r-release/fullrefman.pdf#Rfn.order

  * MATLAB: sort
    * https://mathworks.com/help/matlab/ref/sort.html
    * ""sort"" returns sorted array and indices to sort array

  * Julia: sortperm
    * https://pkg.julialang.org/docs/julia/THl1k/1.1.1/base/sort.html#Base.sortperm

It's better that we use general name because Arrow C++ isn't a NumPy
compatible library.

""SortToIndices"" means ""sort that returns indices array"".",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-14 05:00:04,1
13250683,[C++][Python] Consider assigning default column names when reading CSV file and header_rows=0,"This is a slight usability rough edge. Assigning default names (like ""f0, f1, ..."") would probably be better since then at least you can see how many columns there are and what is in them. 

{code}
In [10]: parse_options = csv.ParseOptions(delimiter='|', header_rows=0)                                                                                         

In [11]: %time table = csv.read_csv('Performance_2016Q4.txt', parse_options=parse_options)                                                                      
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<timed exec> in <module>

~/miniconda/envs/pyarrow-14-1/lib/python3.7/site-packages/pyarrow/_csv.pyx in pyarrow._csv.read_csv()

~/miniconda/envs/pyarrow-14-1/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: header_rows == 0 needs explicit column names
{code}

In pandas integers are used, so some kind of default string would have to be defined

{code}
In [18]: df = pd.read_csv('Performance_2016Q4.txt', sep='|', header=None, low_memory=False)                                                                     

In [19]: df.columns                                                                                                                                             
Out[19]: 
Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],
           dtype='int64')
{code}",csv pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2019-08-14 03:02:40,2
13250664,[R] Reading in Parquet files are 20x slower than reading fst files in R,"*Problem*

Loading any of the data I mentioned below is 20x slower than the fst format in R.



*How to get the data*

[https://loanperformancedata.fanniemae.com/lppub/index.html]

Register and download any of these. I can't provide the data to you, and I think it's best you register.



!image-2019-08-14-10-04-56-834.png!



*Code*


```r
path = ""data/Performance_2016Q4.txt""

library(data.table)
 library(arrow)

a = data.table::fread(path, header = FALSE)

fst::write_fst(a, ""data/a.fst"")

arrow::write_parquet(a, ""data/a.parquet"")

rm(a); gc()

#read in test
system.time(a <- fst::read_fst(""data/a.fst"")) # 4.61 seconds

rm(a); gc()

read in test
system.time(a <- arrow::read_parquet(""data/a.parquet"") # 99.19 seconds
```",parquet,['R'],ARROW,Improvement,Major,2019-08-14 00:06:00,14
13250632,[C++] Add a DataSource implementation which scans a directory,"DirectoryBasedDataSource should scan a directory (optionally recursively) on construction, yielding FileBasedDataFragments",pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-13 19:02:02,6
13250590,[Python] pyarrow.array() shouldn't coerce np.nan to string,"pa.array() by default regards np.nan as float value and fails on pa.array([np.nan, 'string']). It should also fail on pa.array(['string', np.nan]) instead of coercing it to null value.",pull-request-available,['Python'],ARROW,Bug,Major,2019-08-13 16:01:51,14
13250561,[Website] Update arrow-site/README and any other places to point website contributors in right direction,"I noticed this is out of date

https://github.com/apache/arrow-site/blob/asf-site/README.md

We might want to ask ASF Infra to make the ""master"" branch the default when people land at apache/arrow-site on GitHub",pull-request-available,['Website'],ARROW,Improvement,Major,2019-08-13 13:51:02,4
13250554,[Python] remaining usages of the 'data' attribute (from previous Column) cause warnings,"When writing a file to feather, you get those warnings:

{code}
In [45]: pd.DataFrame({'a': [1, 2, 3]}).to_feather('test.feather')                                                                                                                                                 
/home/joris/scipy/repos/arrow/python/pyarrow/feather.py:62: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute
  if col.data.num_chunks == 1:
/home/joris/scipy/repos/arrow/python/pyarrow/feather.py:97: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute
  self.writer.write_array(name, col.data.chunk(0))
{code}

Since they are coming from pyarrow itself, we can (should) fix them. ",pull-request-available,['Python'],ARROW,Bug,Major,2019-08-13 13:31:34,5
13250415,[Java] Improve the performance of RangeEqualVisitor for comparing variable-width vectors,"Two improvements:
 # Compare the whole range of the data buffer, instead of comparing individual elements.
 # If two elements are of different sizes, there is no need to compare them.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-13 04:40:09,7
13250412,[Java] Add API to avro adapter to limit number of rows returned at a time.,We can either let clients iterate or ideally provide an iterator interface. This is important for large avro data and was also discussed as something readers/adapters should haven.,avro pull-request-available,['Java'],ARROW,Sub-task,Major,2019-08-13 03:52:50,16
13250411,[Java] Add API for JDBC adapter that can convert less then the full result set at a time.,Somehow we should configure number of rows per batch and either let clients iterate or provide an iterator API. Otherwise for large result sets we might run out of memory.,pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-13 03:51:15,16
13250407,[Java] Add UINT type test in integration to avoid potential overflow,"As per discussion [https://github.com/apache/arrow/pull/5002]

For UINT type, when write/read json data in integration test, it extend data type(i.e. Long->BigInteger, Int->Long) to avoid potential overflow.

Like UINT8 the write side and read side code like this:


{code:java}
case UINT8:

 generator.writeNumber(UInt8Vector.getNoOverflow(buffer, index));

 break;{code}

{code:java}
BigInteger value = parser.getBigIntegerValue();

buf.writeLong(value.longValue());
{code}
Should add a test to avoid potential overflow in thedata transfer process.",pull-request-available,['Java'],ARROW,Test,Critical,2019-08-13 03:15:56,16
13250359,[R] Sanitizer errors triggered via R bindings,"When we run the examples of the R package through the sanitizers, several errors show up. These could be related to the segfaults we saw on the macos builder on CRAN.

We use the docker container provided by Winston Chang to test this: https://github.com/wch/r-debug

Steps to reproduce + example outputs at: https://gist.github.com/jeroen/111901c351a4089a9effa90691a1dd81
",pull-request-available,"['C++', 'R']",ARROW,Bug,Critical,2019-08-12 20:21:27,13
13250274,[Java] Support vector rank operation,"Given an unsorted vector, we want to get the index of the ith smallest element in the vector. This function is supported by the rank operation.

We provide an implementationthat gets the index with thedesired rank, without sorting the vector (the vector is left intact), and the implementation takes O( n )time, where n is the vector length.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-12 12:14:43,7
13250217,[Java] Remove dependency on RangeEqualsVisitor from ValueVector interface,"This is a follow-up from[https://github.com/apache/arrow/pull/4933]



public interface VectorVisitor<OUT, IN, EX extends Exception> \{..}



In ValueVector :

public <OUT, IN, EX extends Exception> OUT accept(VectorVisitor<OUT, IN, EX> visitor, IN value) throws EX;

",pull-request-available,['Java'],ARROW,Bug,Major,2019-08-12 08:41:46,16
13250215,[Java] remove equals API from ValueVector,"This is a follow-up from[https://github.com/apache/arrow/pull/4933]

The callers should be fixed to use the RangeEquals API instead.",pull-request-available,['Java'],ARROW,Bug,Major,2019-08-12 08:38:07,16
13250207,[Java] Extract set null method to the base class for fixed width vectors,"Currently, each fixed width vector has the setNull method. All these implementations are identical, so we move them to the base class.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-12 08:05:24,7
13250165,[Java][Docs] Document environment variables/java properties,"Specifically, ""-Dio.netty.tryReflectionSetAccessible=true"" for JVMs >= 9 and BoundsChecking/NullChecking for get.



",pull-request-available,"['Documentation', 'Java']",ARROW,Improvement,Major,2019-08-12 02:44:22,16
13250149,"[Java] Exception in thread ""main"" org.apache.arrow.memory.OutOfMemoryException: Unable to allocate buffer of size 4 due to memory limit. Current allocation: 2147483646","
jdbc query results exceed native heap when using generous -Xmx settings. 

for roughly 800 megabytes of csv/flatfile resultset, arrow is unable to house the contents in RAM long enough to persist to disk, without explicit knowledge beyond unit test sample code.

source:
https://github.com/jnorthrup/jdbc2json/blob/master/src/main/java/com/fnreport/QueryToFeather.kt#L83


{code:java}
Exception in thread ""main"" org.apache.arrow.memory.OutOfMemoryException: Unable to allocate buffer of size 4 due to memory limit. Current allocation: 2147483646
        at org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:307)
        at org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:277)
        at org.apache.arrow.adapter.jdbc.JdbcToArrowUtils.updateVector(JdbcToArrowUtils.java:610)
        at org.apache.arrow.adapter.jdbc.JdbcToArrowUtils.jdbcToFieldVector(JdbcToArrowUtils.java:462)
        at org.apache.arrow.adapter.jdbc.JdbcToArrowUtils.jdbcToArrowVectors(JdbcToArrowUtils.java:396)
        at org.apache.arrow.adapter.jdbc.JdbcToArrow.sqlToArrow(JdbcToArrow.java:225)
        at org.apache.arrow.adapter.jdbc.JdbcToArrow.sqlToArrow(JdbcToArrow.java:187)
        at org.apache.arrow.adapter.jdbc.JdbcToArrow.sqlToArrow(JdbcToArrow.java:156)
        at com.fnreport.QueryToFeather$Companion.go(QueryToFeather.kt:83)
        at com.fnreport.QueryToFeather$Companion$main$1.invokeSuspend(QueryToFeather.kt:95)
        at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)
        at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:241)
        at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:270)
        at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:79)
        at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:54)
        at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)
        at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:36)
        at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)
        at com.fnreport.QueryToFeather$Companion.main(QueryToFeather.kt:93)
        at com.fnreport.QueryToFeather.main(QueryToFeather.kt)
{code}

",jdbc pull-request-available,['Java'],ARROW,Bug,Major,2019-08-11 22:19:57,15
13250102,[Java] Method getBufferSizeFor in BaseRepeatedValueVector/ListVector not correct,"Currently, {{getBufferSizeFor}}in {{BaseRepeatedValueVector}}implemented as below:
{code:java}
if (valueCount == 0) {

 return 0;

}

return ((valueCount + 1) * OFFSET_WIDTH) + vector.getBufferSizeFor(valueCount);
{code}
Here vector.getBufferSizeFor(valueCount)seems not right which should be


{code:java}
int innerVectorValueCount = offsetBuffer.getInt(valueCount * OFFSET_WIDTH);

vector.getBufferSizeFor(innerVectorValueCount)
{code}
ListVector has the same problem.",pull-request-available,['Java'],ARROW,Bug,Critical,2019-08-11 12:13:02,16
13250096,[Java] Avro adapter avoid potential resource leak.,"Currently, avro consumer interface has no close API, which may cause resource leak like {{AvroBytesConsumer#cacheBuffer}}.

To resolve this, make consumer extends {{AutoCloseable}}and create {{CompositeAvroConsumer}}to encompasses consume and close logic.",pull-request-available,['Java'],ARROW,Sub-task,Major,2019-08-11 09:05:07,16
13250063,[C++] CMake fails with file not found error while bundling thrift if python is not installed,"I had this error message while I was trying to reproduce another issue in docker.

To reproduce:

```
FROM debian:buster 
RUN apt-get update 
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y git build-essential cmake 
 
WORKDIR /app 
RUN git clone https://github.com/apache/arrow.git 
WORKDIR /app/arrow/cpp/build 
RUN git checkout 167cea0 # HEAD as of 10-Aug-19
RUN cmake -DARROW_PARQUET=ON -DARROW_DEPENDENCY_SOURCE=BUNDLED .. 
RUN cmake --build . --target thrift_ep -j 8
```

Relevant part of output:
```
Scanning dependencies of target thrift_ep
[ 66%] Creating directories for 'thrift_ep'
[ 66%] Performing download step (verify and extract) for 'thrift_ep'
CMake Error at thrift_ep-stamp/verify-thrift_ep.cmake:11 (message):
 File not found: /thrift/0.12.0/thrift-0.12.0.tar.gz
make[3]: *** [CMakeFiles/thrift_ep.dir/build.make:90: thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-download] Error 1 
make[2]: *** [CMakeFiles/Makefile2:916: CMakeFiles/thrift_ep.dir/all] Error 2
make[1]: *** [CMakeFiles/Makefile2:928: CMakeFiles/thrift_ep.dir/rule] Error 2
make: *** [Makefile:487: thrift_ep] Error 2
```

Installing python fixes the problem, but this isn't directly clear from the error message. The source of issue is that execute_process in get_apache_mirrors macro silently fails and returns empty APACHE_MIRROR value since PYTHON_EXECUTABLE was empty.",pull-request-available,['C++'],ARROW,Bug,Minor,2019-08-10 17:07:38,1
13250060,[Java] Add non-static approach in DictionaryEncoder making it easy to extend and reuse,"As discussed in[https://github.com/apache/arrow/pull/4994].

Current static DictionaryEncoder has some limitation for extension and reuse.

Slightly change the APIs and migrate static method to object based approach.",pull-request-available,['Java'],ARROW,Improvement,Critical,2019-08-10 16:42:12,16
13249890,[C++] fallback to storage type when writing ExtensionType to Parquet,"Writing a table that contains an ExtensionType array to a parquet file is not yet implemented. It currently raises ""ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: extension<arrow.py_extension_type>"" (for a PyExtensionType in this case).

I think minimal support can consist of writing the storage type / array. 

We also might want to save the extension name and metadata in the parquet FileMetadata. 

Later on, this could be potentially be used to restore the extension type when reading. This is related to other issues that need to save the arrow schema (categorical: ARROW-5480, time zones: ARROW-5888). Only in this case, we probably want to store the serialised type in addition to the schema (which only has the extension type's name). ",parquet pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-09 12:47:35,5
13249854,[Packaging][C++] Plasma headers not included for ubuntu-xenial libplasma-dev debian package,"See[https://github.com/kou/arrow/blob/master/dev/tasks/linux-packages/debian.ubuntu-xenial/libplasma-dev.install]

Issue is still present on latest master branch, the debian install script is correct:[https://github.com/kou/arrow/blob/master/dev/tasks/linux-packages/debian/libplasma-dev.install]

The first line is missing from the ubuntu install script causing no headers to be installed when apt-get is used to install libplasma-dev.",debian packaging pull-request-available,"['C++ - Plasma', 'Packaging']",ARROW,Bug,Major,2019-08-09 09:40:09,1
13249826,[Java] Provide hash table based dictionary builder,"This is related ARROW-5862. We provide another type of dictionary builder based on hash table. Compared with a search based dictionary encoder, a hash table based encoder process each new element in O(1) time,but require extra memory space.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-09 06:41:58,7
13249824,[Java] Provide hash table based dictionary encoder,"This is the second part of ARROW-5917. We provide a sort based encoder, as well as a hash table based encoder, to solve the problem with the current dictionary encoder.

In particular, we solve the following problems with the current encoder:
 # There are repeated conversions between Java objects and bytes (e.g. vector.getObject(i)).
 # Unnecessary memory copy (the vector data must be copied to the hash table).
 # The hash table cannot be reused for encoding multiple vectors (other data structure & results cannot be reused either).
 # The output vector should not be created/managed by the encoder (just like in the out-of-place sorter)
 # The hash table requires that the hashCode & equals methods be implemented appropriately, but this is not guaranteed.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-09 06:38:14,7
13249802,[R] Document that you don't have to use tidyselect if you don't want,"I noticed tonight that several functions from the*tidyselect* package are re-exported by*arrow*. Why is this necessary? In my opinion, the*arrow*R package should strive to have as few dependencies as possible and should have no opinion about which parts of the R ecosystem (""tidy"" or otherwise) are used with it.

I think it would be valuable to cut the*tidyselect* re-exports, and to make*feather::read_feather()*'s argument*col_select* take a character vector of column names instead of a ""*tidyselect::vars_select()""* object. I think that would be more natural and would be intuitive for a broader group of R users.

Would you be open to removing*tidyselect* and changing*feather::read_feather()* this way?",pull-request-available,['R'],ARROW,Wish,Minor,2019-08-09 03:21:11,4
13249777,[R] Add note to README about r-arrow conda installation ,"I'm able to successfully installthe C++ and Python libraries fromconda-forge, then successfully install the R packagefrom CRAN ifI use {{--no-test-load}}. But after installation, the R package fails to load because {{dyn.load(""arrow.so"")}} fails. It throws this error when loading:
{code:java}
unable to load shared object '~/R/arrow/libs/arrow.so':
 /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.11' not found (required by ~/.conda/envs/python3.6/lib/libarrow.so.14)
{code}
Do the Arrow C++ libraries actually require GCC 7.1.0 / CXXABI_1.3.11? If not, what might explain this error message? Thanks.",pull-request-available,['R'],ARROW,Bug,Minor,2019-08-08 22:21:22,4
13249767,[R] Only allow R package to install without libarrow on linux,"Seehttps://issues.apache.org/jira/browse/ARROW-6167for backstory. Now that we're on CRAN, we can be less paranoid about build failures getting the package rejected, and we can focus on solidifying the CRAN binary package experience. The macOS binaries for 0.14.1 were built without the C++ library, which we did not expect and cannot reproduce. At this point, it would probably be better to have a failed build than have binariesget made but be useless. Plus, word has it that for macOS binary builds, CRAN will retry if they fail for some reason. It's possible that whatever failed for 0.14.1 was transient, and if the build had failed instead of carried on without libarrow, on retry it may have built successfully.",pull-request-available,['R'],ARROW,Improvement,Major,2019-08-08 20:53:04,4
13249730,[C++] Create InputStream that is an isolated reader of a segment of a RandomAccessFile,"If different threads wants to do buffered reads over different portions of a file (and they are unable to create their own separate file handles), they may clobber each other. I would propose creating an object that keeps the RandomAccessFile internally and implements the InputStream API in a way that is safe from other threads changing the file position",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-08 19:09:40,14
13249707,[Developer] Don't fail in merge script on bad primary author input in multi-author PRs,"I was going on autopilot in a multi-author PR and this happened

{code}
Switched to branch 'PR_TOOL_MERGE_PR_5000_MASTER'
Automatic merge went well; stopped before committing as requested
Author 1: Franois Saint-Jacques <fsaintjacques@gmail.com>
Author 2: Wes McKinney <wesm+git@apache.org>
Enter primary author in the format of ""name <email>"" [Franois Saint-Jacques <fsaintjacques@gmail.com>]: y
fatal: --author '""y""' is not 'Name <email>' and matches no existing author
Command failed: ['git', 'commit', '--no-verify', '--author=""y""', '-m', 'ARROW-6121: [Tools] Improve merge tool ergonomics', '-m', '- merge_arrow_pr.py now accepts the pull-request number as a single optional argument, e.g. `./merge_arrow_pr.py 4921`.\r\n- merge_arrow_pr.py can optionally read a configuration file located in   `~/.config/arrow/merge.conf` which contains options like jira credentials. See the `dev/merge.conf` file as example', '-m', 'Closes #5000 from fsaintjacques/ARROW-6121-merge-ergonomic and squashes the following commits:', '-m', '5298308d7 <Wes McKinney> Handle username/password separately (in case username is set but not password)\n581653735 <Franois Saint-Jacques> Rename merge.conf to merge.conf.sample\n7c51ca8f0 <Franois Saint-Jacques> Add license to config file\n1213946bd <Franois Saint-Jacques> ARROW-6121:  Improve merge tool ergonomics', '-m', 'Lead-authored-by: y\nCo-authored-by: Franois Saint-Jacques <fsaintjacques@gmail.com>\nCo-authored-by: Wes McKinney <wesm+git@apache.org>\nSigned-off-by: Wes McKinney <wesm+git@apache.org>']
With output:
--------------
b''
--------------
Traceback (most recent call last):
  File ""dev/merge_arrow_pr.py"", line 530, in <module>
    if pr.is_merged:
  File ""dev/merge_arrow_pr.py"", line 515, in cli
    PROJECT_NAME = os.environ.get('ARROW_PROJECT_NAME') or 'arrow'
  File ""dev/merge_arrow_pr.py"", line 420, in merge
    '--author=""%s""' % primary_author] +
  File ""dev/merge_arrow_pr.py"", line 89, in run_cmd
    print('--------------')
  File ""dev/merge_arrow_pr.py"", line 81, in run_cmd
    try:
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/subprocess.py"", line 395, in check_output
    **kwargs).stdout
  File ""/home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/subprocess.py"", line 487, in run
    output=stdout, stderr=stderr)
{code}

If the input does not match the expected format, we should loop to request input again rather than failing out (which requires messy manual cleanup of temporary branches)",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-08-08 16:39:28,14
13249690,[C++] Add Array::Validate(),It's a bit weird to have {{ChunkedArray::Validate()}} and {{Table::Validate()}} methods but only a standalone {{ValidateArray}} function for arrays.,easy pull-request-available,['C++'],ARROW,Task,Trivial,2019-08-08 15:32:38,2
13249674,[Java] Fix MapVector#getMinorType and extend AbstractContainerVector addOrGet complex vector API,"i. Currently {{MapVector#getMinorType}}extends {{ListVector}}which returns the wrong {{MinorType}}.

ii. {{AbstractContainerVector}}now only has {{addOrGetList}}, {{addOrGetUnion}}, {{addOrGetStruct}}which not support all complex type like {{MapVector}}and {{FixedSizeListVector}}.",pull-request-available,['Java'],ARROW,Bug,Minor,2019-08-08 14:44:50,16
13249666,[C++] Validate chunks in ChunkedArray::Validate,"If I patch {{Table::Validate()}} to also validate the underlying arrays:
{code:c++}
diff --git a/cpp/src/arrow/table.cc b/cpp/src/arrow/table.cc
index 446010f93..e617470b5 100644
--- a/cpp/src/arrow/table.cc
+++ b/cpp/src/arrow/table.cc
@@ -21,6 +21,7 @@
 #include <cstdlib>
 #include <limits>
 #include <memory>
+#include <sstream>
 #include <utility>
 
 #include ""arrow/array.h""
@@ -184,10 +185,18 @@ Status ChunkedArray::Validate() const {
   }
 
   const auto& type = *chunks_[0]->type();
+  // Make sure chunks all have the same type, and validate them
   for (size_t i = 1; i < chunks_.size(); ++i) {
-    if (!chunks_[i]->type()->Equals(type)) {
+    const Array& chunk = *chunks_[i];
+    if (!chunk.type()->Equals(type)) {
       return Status::Invalid(""In chunk "", i, "" expected type "", type.ToString(),
-                             "" but saw "", chunks_[i]->type()->ToString());
+                             "" but saw "", chunk.type()->ToString());
+    }
+    Status st = ValidateArray(chunk);
+    if (!st.ok()) {
+      std::stringstream ss;
+      ss << ""Chunk "" << i << "": "" << st.message();
+      return st.WithMessage(ss.str());
     }
   }
   return Status::OK();
@@ -343,7 +352,7 @@ class SimpleTable : public Table {
       }
     }
 
-    // Make sure columns are all the same length
+    // Make sure columns are all the same length, and validate them
     for (int i = 0; i < num_columns(); ++i) {
       const ChunkedArray* col = columns_[i].get();
       if (col->length() != num_rows_) {
@@ -351,6 +360,12 @@ class SimpleTable : public Table {
                                "" expected length "", num_rows_, "" but got length "",
                                col->length());
       }
+      Status st = col->Validate();
+      if (!st.ok()) {
+        std::stringstream ss;
+        ss << ""Column "" << i << "": "" << st.message();
+        return st.WithMessage(ss.str());
+      }
     }
     return Status::OK();
   }
{code}

... then {{parquet-arrow-test}} fails and then crashes:
{code}
[...]
[ RUN      ] TestArrowReadWrite.TableWithChunkedColumns
../src/parquet/arrow/arrow-reader-writer-test.cc:347: Failure
Failed
'WriteTable(*table, ::arrow::default_memory_pool(), sink, row_group_size, default_writer_properties(), arrow_properties)' failed with Invalid: Column 0: Chunk 1: Final offset invariant not equal to values length: 210!=733
In ../src/arrow/array.cc, line 1229, code: ValidateListArray(array)
In ../src/parquet/arrow/writer.cc, line 1210, code: table.Validate()
In ../src/parquet/arrow/writer.cc, line 1252, code: writer->WriteTable(table, chunk_size)
../src/parquet/arrow/arrow-reader-writer-test.cc:419: Failure
Expected: WriteTableToBuffer(table, row_group_size, arrow_properties, &buffer) doesn't generate new fatal failures in the current thread.
  Actual: it does.
/home/antoine/arrow/dev/cpp/build-support/run-test.sh : ligne 97 : 28927 Erreur de segmentation  $TEST_EXECUTABLE ""$@"" 2>&1
     28930 Fini                    | $ROOT/build-support/asan_symbolize.py
     28933 Fini                    | ${CXXFILT:-c++filt}
     28936 Fini                    | $ROOT/build-support/stacktrace_addr2line.pl $TEST_EXECUTABLE
     28939 Fini                    | $pipe_cmd 2>&1
     28941 Fini                    | tee $LOGFILE
~/arrow/dev/cpp/build-test/src/parquet

{code}",parquet pull-request-available,['C++'],ARROW,Bug,Major,2019-08-08 13:52:09,14
13249638,[Java] Provide benchmarks to set IntVector with different methods,"

We provide benchmarks toevaluate the performance of setting IntVector in 3 different ways:
 # through a value holder
 # through a writer
 # directly set the value through a set method",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-08 10:55:07,7
13249618,"[R] ""docker-compose run r"" fails","I get the following failure:
{code}
** testing if installed package can be loaded from temporary location
Error: package or namespace load failed for 'arrow' in dyn.load(file, DLLpath = DLLpath, ...):
 unable to load shared object '/usr/local/lib/R/site-library/00LOCK-arrow/00new/arrow/libs/arrow.so':
  /opt/conda/lib/libarrow.so.100: undefined symbol: LZ4F_resetDecompressionContext
Error: loading failed
Execution halted
ERROR: loading failed
* removing '/usr/local/lib/R/site-library/arrow'
{code}
",pull-request-available,"['Developer Tools', 'R']",ARROW,Bug,Critical,2019-08-08 09:23:56,13
13249615,"[R] ""docker-compose build r"" is slow",Apparently it installs and compiles all packages in single-thread mode.,pull-request-available,"['Developer Tools', 'R']",ARROW,Bug,Major,2019-08-08 09:17:34,2
13249545,[R] macOS binary R packages on CRAN don't have arrow_available,"The {{configure}} script in the R package has some [magic|https://github.com/apache/arrow/blob/master/r/configure#L66-L86]that should ensure that on macOS, you're guaranteed a successful library installation even (especially) if you don't have libarrow installed on your system. This magic also is designed so that when CRAN builds a binary package for macOS, the C++ libraries are bundled and ""just work"" when a user installs it, no compilation required.

However, the magic appeared to fail on CRAN this time, as the binaries linked on[https://cran.r-project.org/web/packages/arrow/index.html]were built without libarrow({{arrow::arrow_available()}} returns {{FALSE}}).

I've identified three vectors by which you can get an arrow package installation on macOS in this state:
 # The [check|https://github.com/apache/arrow/blob/master/r/configure#L71]to see if you've already installed {{apache-arrow}} via Homebrew always passes, so if you have Homebrew installed but haven't done {{brew install apache-arrow}}, the script won't do it for you like it looks like it intends. (This is not suspected to be the problem on CRAN because they don't have Homebrew installed.)
 # If the""[autobrew|https://github.com/apache/arrow/blob/master/r/configure#L80-L81]"" installation fails, then the [test on L102|https://github.com/apache/arrow/blob/master/r/configure#L102]will correctly fail. I managed to trigger this (by luck?) on the [R-hub testing service|https://builder.r-hub.io/status/arrow_0.14.1.tar.gz-da083126612b46e28854b95156b87b31#L533].This is possibly what happened on CRAN, though the only [build logs|https://www.r-project.org/nosvn/R.check/r-release-osx-x86_64/arrow-00check.html]we have from CRAN are terse because it believes the build was successful.
 # Someidiosyncrasyin the compiler on the CRAN macOS system such that the autobrew script would successfully download the arrow libraries but the L102 check would error. I've been unable to reproduce this using the [version of clang7 that CRAN provides|https://cran.r-project.org/bin/macosx/tools/].

I have a fix for the first one and will provide workaround documentation for the README and announcement blog post. Unfortunately, I don't know that there's anything we can do about the useless binaries on CRAN at this time, particularly since CRAN is going down for maintenance August 9-18.

cc [~jeroenooms] [~romainfrancois] [~wesmckinn]",pull-request-available,['R'],ARROW,Bug,Critical,2019-08-07 23:27:04,4
13249509,[Integration] Use multiprocessing to run integration tests on multiple CPU cores,The stdout/stderr will have to be captured appropriate so that the console output when run in parallel is still readable,pull-request-available,['Integration'],ARROW,Improvement,Major,2019-08-07 18:44:56,2
13249391,[C++] Implements dataset::ParquetFile and associated Scan structures,"This is first baby step in supporting datasets. The initial implementation will be minimal and trivial, no parallel, no schema adaptation.",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-08-07 15:51:56,13
13249372,[Java] AbstractStructVector#getPrimitiveVectors fails to work with complex child vectors,"Currently in {{AbstractStructVector#getPrimitiveVectors}}, only struct type child vectors will recursively get primitive vectors, other complex type like {{ListVector}}, {{UnionVector}}was treated as primitive type and return directly.

For example,Struct(List(Int), Struct(Int, Varchar)){{getPrimitiveVectors}}should return {{[IntVector, IntVector, VarCharVector]}} instead of [ListVector, IntVector, VarCharVector]",pull-request-available,['Java'],ARROW,Bug,Minor,2019-08-07 14:50:41,16
13249352,[C++] PrettyPrint of arrow::Schema missing identation for first line,"Minor issue, but I noticed when printing a Schema with indentation, like:

{code}
  std::shared_ptr<arrow::Field> field1 = arrow::field(""column1"", arrow::int32());
  std::shared_ptr<arrow::Field> field2 = arrow::field(""column2"", arrow::utf8());

  std::shared_ptr<arrow::Schema> schema = arrow::schema({field1, field2});

  arrow::PrettyPrintOptions options{4};
  arrow::PrettyPrint(*schema, options, &std::cout);
{code}

you get 

{code}
column1: int32
    column2: string
{code}

so not applying the indent for the first line.",beginner pull-request-available,['C++'],ARROW,Bug,Minor,2019-08-07 13:20:15,14
13249326,[Python] possible to create StructArray with type that conflicts with child array's types,"Using the Python interface as example. This creates a {{StructArray}} where the field types don't match the child array types:

{code}
a = pa.array([1, 2, 3], type=pa.int64())
b = pa.array(['a', 'b', 'c'], type=pa.string())
inconsistent_fields = [pa.field('a', pa.int32()), pa.field('b', pa.float64())]

a = pa.StructArray.from_arrays([a, b], fields=inconsistent_fields) 
{code}

The above works fine. I didn't find anything that errors (eg conversion to pandas, slicing), also validation passes, but the type actually has the inconsistent child types:

{code}
In [2]: a
Out[2]: 
<pyarrow.lib.StructArray object at 0x7f450af52eb8>
-- is_valid: all not null
-- child 0 type: int64
  [
    1,
    2,
    3
  ]
-- child 1 type: string
  [
    ""a"",
    ""b"",
    ""c""
  ]

In [3]: a.type
Out[3]: StructType(struct<a: int32, b: double>)

In [4]: a.to_pandas()
Out[4]: 
array([{'a': 1, 'b': 'a'}, {'a': 2, 'b': 'b'}, {'a': 3, 'b': 'c'}],
      dtype=object)

In [5]: a.validate() 
{code}

Shouldn't this be disallowed somehow? (it could be checked in the Python {{from_arrays}} method, but maybe also in {{StructArray::Make}} which already checks for the number of fields vs arrays and a consistent array length). 

Similarly to discussion in ARROW-6132, I would also expect that this the {{ValidateArray}} catches this.",pull-request-available,['Python'],ARROW,Bug,Major,2019-08-07 10:58:20,5
13249322,[Python][C++] UnionArray with invalid data passes validation / leads to segfaults,"From the Python side, you can create an ""invalid"" UnionArray:

{code}
binary = pa.array([b'a', b'b', b'c', b'd'], type='binary') 
int64 = pa.array([1, 2, 3], type='int64') 
types = pa.array([0, 1, 0, 0, 2, 1, 0], type='int8')   # <- value of 2 is out of bound for number of childs
value_offsets = pa.array([0, 0, 2, 1, 1, 2, 3], type='int32')

a = pa.UnionArray.from_dense(types, value_offsets, [binary, int64])
{code}

Eg on conversion to python this leads to a segfault:

{code}
In [7]: a.to_pylist()
Segmentation fault (core dumped)
{code}

On the other hand, doing an explicit validation does not give an error:

{code}
In [8]: a.validate()
{code}

Should the validation raise errors for this case? (the C++ {{ValidateVisitor}} for UnionArray does nothing) 

(so that this can be called from the Python API to avoid creating invalid arrays / segfaults there)
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-08-07 10:30:29,2
13249318,[Java] Support compare semantics for ArrowBufPointer,"Compare two arrow buffer pointers by their content inlexicographic order.

null is smaller and shorter buffer is smaller.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-07 10:21:26,7
13249241,[Java] Extract a super interface for vectors whose elements reside in continuous memory segments,"For vectors whose data elements reside in continuous memory segments,they should implement a common super interface. This will avoid unnecessary code branches.

For now, such vectors include fixed-width vectors and variable-width vectors. In the future, there can be more vectorsincluded.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-07 02:48:05,7
13249219,[C++][Parquet] Write arrow::Array directly into parquet::TypedColumnWriter<T>,This is an initial refactoring task to enable the Arrow write layer to access some of the internal implementation details of {{parquet::TypedColumnWriter<T>}}. See discussion in ARROW-3246,pull-request-available,['C++'],ARROW,Bug,Major,2019-08-06 22:00:19,14
13249097,[Java] UnionVector created by MinorType#getNewVector could not keep field type info properly,"When I worked for other items, I found {{UnionVector}}created by {{VectorSchemaRoot#create(Schema schema, BufferAllocator allocator)}}could not keep field type info properly. For example, if we set metadata in {{Field}}in schema, we could not get it back by {{UnionVector#getField}}.

This is mainly because {{MinorType.Union.getNewVector}} did not pass {{FieldType}}to vector and {{UnionVector#getField}}create a new {{Field}}which cause inconsistent.",pull-request-available,['Java'],ARROW,Bug,Minor,2019-08-06 11:59:18,16
13249000,[Java] Unify the copyFrom and copyFromSafe methods for all vectors,"Some vectors have their own implementations of copyFrom and copyFromSafe methods.

Since we have extracted the copyFrom and copyFromSafe methods to the base interface (see ARROW-6021), wewant all vectors' implementations to override the methods from the super interface.

This will provide a unified way ofcopying data elements.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-08-06 03:01:15,7
13248997,[R] Install instructions on linux could be clearer,"Installing R packages on Linux is almost always from source, which means Arrow needs some system dependencies. The existing help message (from arrow::install_arrow()) is very helpful in pointing that out, but it's still a heavy lift for users who install R packages from source but don't plan to develop Arrow itself.

Here are a couple of things that could make things slightly smoother:
 # I would be very grateful if the install_arrow() message or installation page told me which libraries were essential to make the R package work.
 # install_arrow() refers to a PPA. Previously I've only seen PPAs hosted on launchpad.net, so the bintray URL threw me. Changing it to ""bintray.com PPA"" instead of just ""PPA"" would have caused me less confusion. (Others may differ)
 # A snap package would be easier than installing a new apt address, but I understand that building for snap would be more packaging work and only benefits Ubuntu users.



Thanks for making R bindings, and congratulations on the CRAN release!",documentation pull-request-available,['R'],ARROW,Wish,Minor,2019-08-06 02:46:19,4
13248880,[FlightRPC][Java] Don't double-close response stream,"DoPut in Java double-closes the metadata response stream: if the service implementation sends an error down that channel, the Flight implementation will unconditionally try to complete the stream, violating the gRPC semantics (either an error or a completion may be sent, never both).",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-08-05 13:51:18,0
13248854,[C++] KeyValueMetadata::Equals should not be order-sensitive,"Currently, two KeyValueMetadata instances with the same key/value pairs but in a different order compare unequal.",pull-request-available,['C++'],ARROW,Bug,Major,2019-08-05 10:01:14,2
13248835,[Python] ListArray.from_arrays does not check validity of input arrays,"From https://github.com/apache/arrow/pull/4979#issuecomment-517593918.

When creating a ListArray from offsets and values in python, there is no validation of the offsets that it starts with 0 and ends with the length of the array (but is that required? the docs seem to indicate that: https://github.com/apache/arrow/blob/master/docs/source/format/Layout.rst#list-type (""The first value in the offsets array is 0, and the last element is the length of the values array."").

The array you get ""seems"" ok (the repr), but on conversion to python or flattened arrays, things go wrong:

{code}
In [61]: a = pa.ListArray.from_arrays([1,3,10], np.arange(5)) 

In [62]: a
Out[62]: 
<pyarrow.lib.ListArray object at 0x7fdd9c468678>
[
  [
    1,
    2
  ],
  [
    3,
    4
  ]
]

In [63]: a.flatten()
Out[63]: 
<pyarrow.lib.Int64Array object at 0x7fdd9cbfe9e8>
[
  0,   # <--- includes the 0
  1,
  2,
  3,
  4
]

In [64]: a.to_pylist()
Out[64]: [[1, 2], [3, 4, 1121, 1, 64, 93969433636432, 13]]  # <--includes more elements as garbage
{code}


Calling {{validate}} manually correctly raises:

{code}
In [65]: a.validate()
...
ArrowInvalid: Final offset invariant not equal to values length: 10!=5
{code}

In C++ the main constructors are not safe, and as the caller you need to ensure that the data is correct or call a safe (slower) constructor. But do we want to use the unsafe / fast constructors without validation in Python as default as well? Or should we do a call to {{validate}} here?

A quick search seems to indicate that `pa.Array.from_buffers` does validation, but other `from_arrays` method don't seem to explicitly do this. ",pull-request-available,['Python'],ARROW,Bug,Minor,2019-08-05 08:52:02,5
13248739,[C++] Can't build with g++ 8.3.0 by class-memaccess warning,"This is caused by ARROW-5527.

{noformat}
    src/arrow/util/hashing.h:313:11: error: 'void* memset(void*, int, size_t)' clearing an object of non-trivial type 'struct arrow::internal::HashTable<arrow::internal::ScalarMemoTable<nonstd::sv_lite::basic_string_view<char>, arrow::internal::HashTable>::Payload>::Entry'; use assignment or value-initialization instead [-Werror=class-memaccess]
         memset(entries_, 0, capacity * sizeof(Entry));
         ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    src/arrow/util/hashing.h:197:10: note: 'struct arrow::internal::HashTable<arrow::internal::ScalarMemoTable<nonstd::sv_lite::basic_string_view<char>, arrow::internal::HashTable>::Payload>::Entry' declared here
       struct Entry {
              ^~~~~
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-08-04 00:12:45,1
13248659,[Website] Add favicons and meta tags,"Among the things I noticed recently that should be easy to clean up:
 * We should supply a favicon
 * The <title> is the same for every page and it always says ""Apache Arrow Homepage""
 * There are no opengraph or twitter card meta tags, so there's no link preview
 * The version of bootstrap used is not current and has been flagged as a possible security vulnerability

Much of this could just be fixed by porting to a modern Hugo template, which I'll explore.",pull-request-available,['Website'],ARROW,Improvement,Major,2019-08-02 20:20:39,4
13248657,[C++] IPC stream reader handling of empty streams potentially not robust,"If dictionaries are expected in a stream, but the stream terminates, then ""empty stream"" logic is triggered to suppress errors, see

https://github.com/apache/arrow/blob/master/cpp/src/arrow/ipc/reader.cc#L482

It's probably esoteric but this ""empty stream"" logic will trigger if the stream terminates in the middle of the dictionary messages, which is a legitimate error. So we should only bail out early (concluding that we have an empty stream) if the first dictionary message is null",pull-request-available,['C++'],ARROW,Bug,Major,2019-08-02 20:07:24,14
13248656,[Python] Remove any APIs deprecated prior to 0.14.x,"A number of deprecated APIs, like {{pyarrow.open_stream}}, are still available",pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-02 19:55:40,14
13248647,[Tools] Improve merge tool cli ergonomic,"* Accepts the pull-request number as an optional (first) parameter to the script
* Supports reading the jira username/password from a file",pull-request-available,['Developer Tools'],ARROW,Improvement,Trivial,2019-08-02 18:52:48,13
13248641,[C++][Gandiva] including some headers causes decimal_test to fail,"It seems this is due to precompiled code being contaminated with undesired headers

For example, {{#include <iostream>}} in {{arrow/compare.h}} causes:

{code}
[ RUN      ] TestDecimal.TestCastFunctions
../../src/gandiva/tests/decimal_test.cc:478: Failure
Value of: (array_dec)->Equals(outputs[2], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  0.00,
  0.00,
  0.00,
  0.00
]
../../src/gandiva/tests/decimal_test.cc:481: Failure
Value of: (array_dec)->Equals(outputs[2], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  0.00,
  0.00,
  0.00,
  0.00
]
../../src/gandiva/tests/decimal_test.cc:484: Failure
Value of: (array_dec)->Equals(outputs[3], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  0.00,
  0.00,
  0.00,
  0.00
]
../../src/gandiva/tests/decimal_test.cc:497: Failure
Value of: (array_float64)->Equals(outputs[6], arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1.23,
  1.58,
  -1.23,
  -1.58
] actual array: [
  inf,
  inf,
  -inf,
  -inf
]
[  FAILED  ] TestDecimal.TestCastFunctions (134 ms)
{code}
",pull-request-available,['C++ - Gandiva'],ARROW,Bug,Major,2019-08-02 18:16:29,14
13248559,[Java] Replace google Preconditions with Arrow Preconditions,"Now in java code, most places uses {{org.apache.arrow.util.Preconditions}}, but still some places uses {{com.google.common.base.Preconditions}}.

Remove google Preconditions meanwhile remove duplicated checks.",pull-request-available,['Java'],ARROW,Improvement,Critical,2019-08-02 10:57:09,16
13248555,[Java] Fix the set method of FixedSizeBinaryVector,"For the set method, if the parameter is null, it should clear the validity bit. However, the current implementation throws a NullPointerException.",pull-request-available,['Java'],ARROW,Bug,Minor,2019-08-02 10:34:22,7
13248525,"[Python] support LargeList, LargeString, LargeBinary in conversion to pandas","General python support for those 3 new types has been added: ARROW-6000, ARROW-6084

However, one aspect that is not yet implemented is conversion to pandas (or numpy array):

{code}
In [67]: a = pa.array(['a', 'b', 'c'], pa.large_string()) 

In [68]: a.to_pandas() 
...
ArrowNotImplementedError: large_utf8

In [69]: pa.table({'a': a}).to_pandas()
...
ArrowNotImplementedError: No known equivalent Pandas block for Arrow data of type large_string is known.
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-08-02 07:59:34,5
13248486,[Java] Support vector deduplicate function,"Removeadjacent deduplicated elements from a vector. This function can be used, for example, infinding distinct values, or in compressing the vector data.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-08-02 04:23:38,7
13248478,[Java] Update APIs to support 64-bit address space,"The arrow spec allows for 64 bit address range for buffers (and arrays) we should support this at the API level in Java even if the current Netty backing buffers don't support it.



See comment below. This work item will focus on allowing 64-bit addressing in buffers.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-02 03:44:42,15
13248461,[C++] Appveyor Build_Debug configuration is hanging in C++ unit tests,"Not sure which patch introduced this, but here is one master build where it occurs

https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/26413929/job/sws48m0603ujwya1

The commit before this patch seems to have been OK",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-08-02 01:36:09,2
13248426,[Rust] [DataFusion] Don't allow bare_trait_objects,"Need to remove ""{color:#808080}#![allow(bare_trait_objects)]"" from cargo.toml and fix compiler warnings
{color}",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2019-08-01 20:07:42,10
13248423,[Java] Stop using the maven release plugin,"For reference .. I'm filing this issue to track investigation work around this ..
{code:java}
The biggest problem for the Git commit is our Java package
requires ""apache-arrow-${VERSION}"" tag on
https://github.com/apache/arrow . (Right?)
I think that ""mvm release:perform"" in
dev/release/01-perform.sh does so but I don't know the
details of ""mvm release:perform""...{code}",pull-request-available,"['Developer Tools', 'Java']",ARROW,Improvement,Blocker,2019-08-01 19:55:37,10
13248409,[Testing] Add partitioned CSV file to arrow-testing repo,I need to add a partitioned CSV file to arrow-testing for use in parallel query unit tests in DataFusion,pull-request-available,['Integration'],ARROW,Improvement,Major,2019-08-01 19:00:56,10
13248387,[Rust] [DataFusion] Create physical plan from logical plan,"Once the physical plan is in place and can be executed, I will implement logic to convert the logical plan to a physical plan and remove the legacy code for directly executing a logical plan.",pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2019-08-01 16:56:13,10
13248381,[Rust] Pin to specific Rust nightly release,"Builds are currently non-deterministic because rust-toolchain contains ""nightly"" meaning ""use the latest nightly release of Rust"". This can cause build seemingly random build failure in CI. I propose we modify rust-toolchain to refer to a specific nightly release e.g. ""nightly-2019-07-31"" so that builds are deterministic.

We can update this nightly version when needed (e.g. to pick up new features) as part of the regular PR process.",pull-request-available,['Rust'],ARROW,Improvement,Major,2019-08-01 16:19:17,10
13248363,[Java] Avro adapter implement unions type,"Support convert unions type like [""string""], [""string"", 'int""] and nullable [""string"", ""int"", ""null""]",pull-request-available,['Java'],ARROW,Sub-task,Critical,2019-08-01 15:14:35,16
13248336,[C++] Python subproject ignores ARROW_TEST_LINKAGE,the python subproject links to arrow_python_shared and other shared libraries regardless of ARROW_TEST_LINKAGE https://github.com/apache/arrow/blob/eb5dd50/cpp/src/arrow/python/CMakeLists.txt#L131-L132,pull-request-available,['C++'],ARROW,Bug,Minor,2019-08-01 13:10:15,6
13248292,[Java] reduce branches in algo for first match in VectorRangeSearcher,"This is a follow up Jira for the improvement suggested by [~fsaintjacques] in the PR for

[https://github.com/apache/arrow/pull/4925]

",pull-request-available,['Java'],ARROW,Improvement,Major,2019-08-01 09:20:24,7
13248272,[C++] Python 2.7: arrow_python_test failure,"Please check the attached test log ""arrow-python-test""

The unit test was executed with python 2.7.13 version. I'm not sure this project would support the compatibility.



[ RUN   ] CheckPyError.TestStatus

/Users/OpenSourceProject/arrow_repo/arrow/cpp/src/arrow/python/python-test.cc:94: Failure

Expected equality of these values:

 detail->ToString()

  Which is: ""Python exception: exceptions.TypeError""

 expected_detail

  Which is: ""Python exception: TypeError""

/Users/OpenSourceProject/arrow_repo/arrow/cpp/src/arrow/python/python-test.cc:94: Failure

Expected equality of these values:

 detail->ToString()

  Which is: ""Python exception: exceptions.NotImplementedError""

 expected_detail

  Which is: ""Python exception: NotImplementedError""

[ FAILED ] CheckPyError.TestStatus (0 ms) 

[ RUN   ] CheckPyError.TestStatusNoGIL

/Users/OpenSourceProject/arrow_repo/arrow/cpp/src/arrow/python/python-test.cc:144: Failure

Expected equality of these values:

 st.detail()->ToString()

  Which is: ""Python exception: exceptions.ZeroDivisionError""

 ""Python exception: ZeroDivisionError""

[ FAILED ] CheckPyError.TestStatusNoGIL (0 ms) 

[----------] 2 tests from CheckPyError (0 ms total)



[----------] 1 test from RestorePyError

[ RUN   ] RestorePyError.Basics

/Users/OpenSourceProject/arrow_repo/arrow/cpp/src/arrow/python/python-test.cc:154: Failure

Expected equality of these values:

 st.detail()->ToString()

  Which is: ""Python exception: exceptions.ZeroDivisionError""",pull-request-available test,"['C++', 'Python']",ARROW,Bug,Trivial,2019-08-01 07:27:54,14
13248174,[Rust] [DataFusion] Implement parallel execution for selection,Implement physical plan for selection operator.,pull-request-available,['Rust - DataFusion'],ARROW,Sub-task,Major,2019-07-31 17:46:39,10
13248103,[Java] Refactor Jdbc adapter consume logic,"Jdbc adapter read from {{ResultSet}}looks like:

while (rs.next()) {
 for (int i = 1; i <= columnCount; i++) {
 jdbcToFieldVector(
 rs,
 i,
 rs.getMetaData().getColumnType(i),
 rowCount,
 root.getVector(rsmd.getColumnName(i)),
 config);
 }
 rowCount++;
}

And in {{jdbcToFieldVector}} has lots of switch-case, that is to see, for every single value from ResultSet we have to do lots of analyzing conditions.

I think we could optimize this using consumer/delegate like avro adapter.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-07-31 12:43:04,16
13248102,[Python] create pa.dictionary() type with non-integer indices type crashes,"For example if you mixed the order of the indices and values type:

{code}
In [1]: pa.dictionary(pa.int8(), pa.string())                                                                                                                                                                      
Out[1]: DictionaryType(dictionary<values=string, indices=int8, ordered=0>)

In [2]: pa.dictionary(pa.string(), pa.int8())                                                                                                                                                                      
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0731 14:40:42.748589 26310 type.cc:440]  Check failed: is_integer(index_type->id()) dictionary index type should be signed integer
*** Check failure stack trace: ***
Aborted (core dumped)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-31 12:41:20,5
13248087,[Java] Support compare and search operation for BaseRepeatedValueVector,Support comparison & search for sub-classes ofBaseRepeatedValueVector.,pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-31 11:45:03,7
13248048,[Java] Implement/test UnionFixedSizeListWriter for FixedSizeListVector,"Now we have two list vectors: {{ListVector}}and {{FixedSizeListVector}}.

{{ListVector}} has already implemented UnionListWriter for writing data, however, {{FixedSizeListVector}} doesn't have this yet and seems the only way for users to write data is getting inner vector and set value manually.

Implement a writer for {{FixedSizeListVector}} is useful in some cases.

",pull-request-available,['Java'],ARROW,New Feature,Critical,2019-07-31 08:53:10,16
13247993,[Java] Implement dictionary-encoded subfields for List type,"For example, int type List (valueCount = 5) has data like below:

10, 20

10, 20

30, 40, 50

30, 40, 50

10, 20

could be encoded to:

0, 1

0, 1

2, 3, 4

2, 3, 4

0, 1

with list type dictionary

10, 20, 30, 40, 50

or

10,

20,

30,

40,

50

",pull-request-available,['Java'],ARROW,Sub-task,Minor,2019-07-31 03:33:14,16
13247988,[C++][Parquet] Build logical schema tree mapping Arrow fields to Parquet schema levels,"In several places in cpp/src/parquet/arrow, the {{FromParquetSchema}} function is used to construct fields using a filtered ""view"" of the Parquet schema. This is a hack caused by the lack of some kind of a ""schema tree"" which maps Parquet concepts to Arrow {{Field}} objects. 

One manifestation of this issue is that I was unable to implement dictionary encoded subfields in cases like {{list<string>}}, where you want the inner field to be dictionary-encoded. 

Patch forthcoming",pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-31 02:53:22,14
13247898,[C++] Implement casting Binary <-> LargeBinary,"We should implement bidirectional casts from Binary to LargeBinary and vice-versa. Also including String and LargeString.

In the narrowing direction, the offset width should be checked.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-07-30 17:21:20,2
13247843,[Java] Avoid creating new schema before IPC sending,"If a dictionary is attached to a schema, itmay need to be converted before IPC sending. When this is not the case (which is most likely in practice), there is no need to do the conversion and no need to create a new schema.

We solve the above problem by quickly determining if conversion is required, and if not, weavoid creating a new schema and return the original one immediately.

",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-30 12:17:16,7
13247795,"[Python] Hypothesis test failure, Add StructType::Make that accepts vector of fields","{code:java}
$ python -m pytest --hypothesis-seed=249088922892200171383018406164042644900 --hypothesis --tb=native pyarrow/tests/test_strategies.py 
=========================================================================== test session starts ============================================================================
platform linux -- Python 3.7.3, pytest-5.0.1, py-1.8.0, pluggy-0.12.0
hypothesis profile 'dev' -> max_examples=10, database=DirectoryBasedExampleDatabase('/home/antoine/arrow/dev/python/.hypothesis/examples')
rootdir: /home/antoine/arrow/dev/python, inifile: setup.cfg
plugins: timeout-1.3.3, repeat-0.8.0, hypothesis-3.82.1, forked-1.0.2, xdist-1.28.0
collected 7 items                                                                                                                                                          

pyarrow/tests/test_strategies.py ......F                                                                                                                             [100%]

================================================================================= FAILURES =================================================================================
_______________________________________________________________________________ test_tables ________________________________________________________________________________
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_strategies.py"", line 55, in test_tables
    def test_tables(table):
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/site-packages/hypothesis/core.py"", line 960, in wrapped_test
    raise the_error_hypothesis_found
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/strategies.py"", line 249, in tables
    return pa.Table.from_arrays(children, schema=schema)
  File ""pyarrow/table.pxi"", line 1018, in pyarrow.lib.Table.from_arrays
    return pyarrow_wrap_table(CTable.Make(c_schema, columns))
  File ""pyarrow/public-api.pxi"", line 314, in pyarrow.lib.pyarrow_wrap_table
    check_status(ctable.get().Validate())
  File ""pyarrow/error.pxi"", line 76, in pyarrow.lib.check_status
    raise ArrowInvalid(message)
pyarrow.lib.ArrowInvalid: Column data for field 11 with type struct<: null, : null, : null, : null, : null, : null> is inconsistent with schema struct<: null, : null, : null, : null not null, : null, : null>
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-30 08:07:58,2
13247792,[Python] Large memory test failures,"I get the following errors when running {{pytest --large_memory}}:

{code}
____________________________________________________________________ test_chunked_binary_error_message _____________________________________________________________________
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_feather.py"", line 566, in test_chunked_binary_error_message
    write_feather(df, io.BytesIO())
  File ""/home/antoine/arrow/dev/python/pyarrow/feather.py"", line 182, in write_feather
    writer.write(df)
  File ""/home/antoine/arrow/dev/python/pyarrow/feather.py"", line 96, in write
    check_chunked_overflow(col)
  File ""/home/antoine/arrow/dev/python/pyarrow/feather.py"", line 68, in check_chunked_overflow
    ""lifted in the future"".format(col.name))
AttributeError: 'pyarrow.lib.ChunkedArray' object has no attribute 'name'
_______________________________________________________________ TestConvertStructTypes.test_from_numpy_large _______________________________________________________________
Traceback (most recent call last):
  File ""/home/antoine/arrow/dev/python/pyarrow/tests/test_pandas.py"", line 2010, in test_from_numpy_large
    assert arr.num_chunks == 2
AttributeError: 'pyarrow.lib.StructArray' object has no attribute 'num_chunks'
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-30 07:48:45,14
13247737,[Website] Fix blog post author header,"While working on the R package post, I noticed some oddities with how the author metadata is handled. Links and names are wrong if the author isn't registered in _data/contributors.yml, and thepermalink version of the post works differently from the main feed.",pull-request-available,['Website'],ARROW,Bug,Minor,2019-07-29 21:30:35,4
13247718,"[C++] Reorganize parquet/arrow/reader.cc, remove code duplication, improve readability",The code in parquet/arrow/reader.cc has quite a bit of code duplication and is difficult to follow. In the course of making some other improvements to this code I am going to reorganize and clean things up. PR coming shortly,pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-29 18:03:35,14
13247714,"[FlightRPC] Implement ""half-closed"" semantics for DoPut","Both sides on a DoPut should be able to half-close the stream, indicating they will no longer write. This allows a client to indicate that it is done writing data to the server, while still leaving the stream open so it can read metadata responses until the server finishes. Meanwhile, the server would see that the client has finished and be able to stop blocking on reading client messages.",pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-07-29 17:39:59,0
13247637,[Python] too large memory cost using pyarrow.parquet.read_table with use_threads=True,"I tried to load a parquet file of about 1.8Gb using the following code. It crashed due to out of memory issue.
{code:java}
import pyarrow.parquet as pq
pq.read_table('/tmp/test.parquet'){code}
However, it worked well with use_threads=True as follows
{code:java}
pq.read_table('/tmp/test.parquet', use_threads=False){code}
If pyarrow is downgraded to 0.12.1, there is no such problem.",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-29 11:49:18,6
13247611,[Python][Parquet] Failure when reading Parquet file from S3 with s3fs,"I am reading parquet data from S3 and get ArrowIOError error.

Size of the data:32 part files 90 MB each (3GB approx)

Number of records: Approx 100M

Code Snippet:
{code:java}
from s3fs import S3FileSystem
import pyarrow.parquet as pq

s3 = S3FileSystem()

dataset = pq.ParquetDataset(""s3://location"", filesystem=s3)

df = dataset.read_pandas().to_pandas()
{code}
Stack Trace:
{code:java}
df = dataset.read_pandas().to_pandas()
File ""/root/.local/lib/python3.6/site-packages/pyarrow/parquet.py"", line 1113, in read_pandas
return self.read(use_pandas_metadata=True, **kwargs)
File ""/root/.local/lib/python3.6/site-packages/pyarrow/parquet.py"", line 1085, in read
use_pandas_metadata=use_pandas_metadata)
File ""/root/.local/lib/python3.6/site-packages/pyarrow/parquet.py"", line 583, in read
table = reader.read(**options)
File ""/root/.local/lib/python3.6/site-packages/pyarrow/parquet.py"", line 216, in read
use_threads=use_threads)
File ""pyarrow/_parquet.pyx"", line 1086, in pyarrow._parquet.ParquetReader.read_all
File ""pyarrow/error.pxi"", line 87, in pyarrow.lib.check_status
pyarrow.lib.ArrowIOError: Unexpected end of stream: Page was smaller (197092) than expected (263929)
{code}


*Note: Same code works on relatively smaller dataset (approx < 50M records)*



",parquet pull-request-available,['C++'],ARROW,Bug,Major,2019-07-29 09:31:49,14
13247427,"[C++] Divide up arrow/array.h,cc into files in arrow/array/ similar to builder files","Since these files are getting larger, this would improve codebase navigability. Probably should use the same naming scheme as builder_* e.g. {{arrow/array/array_dict.h}}

I recommend also putting the unit test files related to these in there for better semantic organization. ",pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-26 22:20:27,14
13247414,[Java] Update out-of-date java/flight/README.md,"See example bug report

https://github.com/apache/arrow/issues/4955",pull-request-available,['Java'],ARROW,Bug,Major,2019-07-26 21:00:28,16
13247405,[C++] Support using Array::View from compatible dictionary type to another,"For example, zero-copy conversion from {{dictionary(int32(), binary())}} to {{dictionary(int32(), utf8())}}. The implementation must remember to call {{View}} on {{ArrayData::dictionary}}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-26 19:47:19,14
13247404,[C++] Add ChunkedArray::View which calls to Array::View,"This convenience will help with zero-copy casting from one compatible type to another

I implemented a workaround for this in ARROW-3772",pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-26 19:45:31,14
13247274,[C++] Slice RecordBatch of String array with offset 0 returns whole batch,"We are seeing a very similar bug as inARROW-809, just for a RecordBatch of strings. A slice of a RecordBatch with a string column and offset =0 returns the whole batch instead.


{code:java}
import pandas as pd
import pyarrow as pa
df = pd.DataFrame({ 'b': ['test' for x in range(1000_000)]})
tbl = pa.Table.from_pandas(df)
batch = tbl.to_batches()[0]

batch.slice(0,2).serialize().size
#4000232

batch.slice(1,2).serialize().size
#240
{code}
",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-07-26 10:46:02,14
13247160,[Python] Array equals returns incorrectly if NaNs are in arrays,"{code:python}
import numpy as np
import pyarrow as pa

data = [0, 1, np.nan, None, 4]

arr1 = pa.array(data)
arr2 = pa.array(data)

pa.Array.equals(arr1, arr2)
{code}

Unsure if this is expected behavior, but in Arrow 0.12.1 this returned `True` as compared to `False` in 0.14.1.
",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-25 22:10:03,3
13247151,[C++] Implement alternative DictionaryBuilder that always yields int32 indices,"One problem with the current {{DictionaryBuilder<T>}} in some applications is that, if it is used to produce a series of arrays to form a ChunkedArray, it may yield constituent chunks having different index widths. For example:

{code}
chunk 0: int8 indices
chunk 1: int16 indices
chunk 2: int16 indices
chunk 3: int32 indices
chunk 4: int32 indices
chunk 5: int32 indices
chunk 6: int32 indices
{code}

Obviously this is problematic for these applications. I'm running into this issue in the context of ARROW-3772 where we are looking to decode Parquet data directly to {{DictionaryArray}} without stepping through an intermediate dense decoded stage. 

I'm not sure what to call the class, whether {{DictionaryInt32Builder}} or something similar, but this would be the same API more or less as {{DictionaryBuilder}} but instead use {{Int32Builder}} for the indices rather than {{AdaptiveIntBuilder}}.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-25 20:35:27,14
13247138,[Website] Blog post announcing R package release,"After the R package makes it to CRAN, we should announce it.",pull-request-available,"['R', 'Website']",ARROW,Improvement,Major,2019-07-25 19:25:10,4
13247126,[Java] Dictionary entries are required in IPC streams even when empty,"Java's ArrowReader requires that dictionaries are sent in an IPC stream:[https://github.com/apache/arrow/blob/master/java/vector/src/main/java/org/apache/arrow/vector/ipc/ArrowReader.java#L196-L200].

However, as noted in ARROW-6006, that is not required if the stream contains no values.",pull-request-available,['Java'],ARROW,Bug,Major,2019-07-25 18:02:58,16
13247104,[Python] pyarrow.Table.from_batches produces corrupted table if any of the batches were empty,"When creating a Table from a list/iterator of batches which contains an ""empty"" RecordBatch a Table is produced but attempts to run any pyarrow built-in functions (such as unique()) occasionally result in a Segfault.

The MWE is attached:[^segfault_ex.py]
 # The segfaults happen randomly, around 30% of the time.
 # Commenting out line 10 in the MWE results in no segfaults.
 # The segfault is triggered using the unique() function, but I doubt the behaviour is specific to that function, from what I gather the problem lies in Table creation.

I'm on Windows 10, using Python 3.6 and pyarrow 0.14.0installed through pip (problem also occurs with 0.13.0 from conda-forge).",pull-request-available windows,"['C++', 'Python']",ARROW,Bug,Minor,2019-07-25 16:09:50,2
13247057,[Java] Avro adapter support convert nullable value,"A specific Avro unions type(has two types and one is null type)could convert to a nullable ArrowVector.

For instance, [""null"", ""string""] could represented by a VarcharVector which could has null value.",pull-request-available,['Java'],ARROW,Sub-task,Major,2019-07-25 12:31:14,16
13247006,[C++] CountSetBits doesn't ensure 64-bit aligned accesses,"If the {{data}} pointer isn't 64-bit aligned, then {{CountSetBits}} fails to ensure 64-bit aligned accesses in the 64-bit popcnt loop.",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-25 09:06:21,2
13246959,[Java] Support iterating a vector by ArrowBufPointer,Provide the functionality to traverse a vector (fixed-width vector & variable-width vector) by an iterator. This is convenient for scenarios when accessing vector elements in sequence.,pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-25 05:28:48,7
13246940,[Java] Efficiently compute hash code for ArrowBufPointer,"As ArrowBufHasher is introduced, we can compute the hash code of a continuous region within an ArrowBuf.

We optimize the process to make it efficient to avoid recomputation.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-25 01:53:37,7
13246931,[R] Improve R docs on how to fix library version mismatch,"hi there,

when trying to build the R wrapper using
{code:java}
remotes::install_github(""apache/arrow"", subdir = ""r""){code}
I hit the following error:
{code:java}
Found pkg-config cflags and libs!
 PKG_CFLAGS=-DNDEBUG -DARROW_R_WITH_ARROW
 PKG_LIBS=-larrow -lparquet** libsg++ -std=gnu++11 -I""/usr/share/R/include"" -DNDEBUG -DNDEBUG -DARROW_R_WITH_ARROW -I""/usr/lib/R/site-library/Rcpp/include"" -fvisibility=hidden -fpic -g -O2 -fdebug-prefix-map=/build/r-base-VjHo9C/r-base-3.6.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g -c array.cpp -o array.o
 g++ -std=gnu++11 -I""/usr/share/R/include"" -DNDEBUG -DNDEBUG -DARROW_R_WITH_ARROW -I""/usr/lib/R/site-library/Rcpp/include"" -fvisibility=hidden -fpic -g -O2 -fdebug-prefix-map=/build/r-base-VjHo9C/r-base-3.6.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g -c array_to_vector.cpp -o arrayto_vector.oarrayto_vector.cpp: In function 'Rcpp::List Table_to_dataframe(const std::shared_ptr<arrow::Table>&, bool)':
 array__to_vector.cpp:819:65: error: 'using element_type = class arrow::Column {aka class arrow::Column}' has no member named 'chunks'
 converters[i] = arrow::r::Converter::Make(table->column->chunks());
 ^~~~~~array__to_vector.cpp:820:23: error: 'using element_type = class arrow::Table {aka class arrow::Table}' has no member named 'field'
 names[i] = table->field->name();
 ^~~~~/usr/lib/R/etc/Makeconf:176: recipe for target 'array__to_vector.o' failedmake: *** [array__to_vector.o] Error 1ERROR: compilation failed for package 'arrow'* removing '/home/kchia/R/x86_64-pc-linux-gnu-library/3.6/arrow'Error: Failed to install 'arrow' from GitHub:
 (converted from warning) installation of package '/tmp/RtmpfYJZFa/file33fc6aee0ae6/arrow_0.14.0.9000.tar.gz' had non-zero exit status{code}",pull-request-available,['R'],ARROW,Bug,Major,2019-07-24 23:32:57,4
13246853,[Doc] Add CONTRIBUTING.md,A CONTRIBUTING.md file at the top-level of a repository is automatically picked up by Github and displayed when people open an issue or PR for the first time.,pull-request-available,['Documentation'],ARROW,Task,Minor,2019-07-24 15:39:55,2
13246783,[Java] Provide more hash algorithms ,"Provide more hash algorithms to choose for different scenarios. In particular, we provide the following hash algorithms:
 * Simple hasher: A hasher that calculates the hash code of integers as is, and do not perform any finalization. So the computation is extremely efficient, but the quality of the produced hash code may not be good.

 * Murmur finalizing hasher: Finalize the hash code by the Murmur hashing algorithm. Details of the algorithm can be found in [https://en.wikipedia.org/wiki/MurmurHash]. Murmur hashing is computational expensive, as it involves several integer multiplications. However, the produced hash codes have good quality in the sense that they are uniformly distributed in the universe.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-24 10:35:35,7
13246779,[Java] Support equals API in ValueVector to compare two vectors equal,"In some case, this feature is useful.

In ARROW-1184, {{Dictionary#equals}}not work due to the lack of this API.

Moreover, we already implemented {{equals(int index, ValueVector target, int targetIndex)}}, so this new added API could reuse it.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-24 10:11:21,16
13246743,[Java] Extract copyFrom and copyFromSafe methods to ValueVector interface,"Currently we have copyFrom and copyFromSafe methods in fixed-width and variable-width vectors. Extracting them to the common super interface will make it much more convenient to use them, and avoid unnecessary if-else statements.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-07-24 07:33:34,7
13246724,[Java] Refactor ByteFunctionHelper#hash with new added ArrowBufHasher,"Some logic in these two classes are similar, should replace ByteFunctionHelper#hash logic with ArrowBufHasher since it has murmur hash algorithm which could avoid hash collision.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-24 04:43:33,16
13246594,[FlightRPC] Allow creating Locations with unknown schemes,"Right now Flight clients error if the server hands them a Location with an unknown scheme. Also, you can't construct locations with non-gRPC schemes. Since Flight will want to support other transports, we should allow unknown schemes up until a client is constructed for them. This would also make it possible for a Flight service to reference non-Flight services in FlightInfo.",pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-07-23 14:57:23,0
13246563,[Python] pyarrow get_library_dirs assertion error,"The added code here: [https://github.com/apache/arrow/blob/apache-arrow-0.14.1/python/pyarrow/__init__.py#L257-L265] causes an AssertionError in ubuntu:

{{... line 265, in get_library_dirs}}
 {{assert library_dir.startswith(""-L"")}}
 {{AssertionError}}

I've installed libarrow-dev from the bintray repositories.

Output from pkg-config:

{{pkg-config --debug --libs-only-L arrow}}

{{Package arrow has -L /usr/lib/x86_64-linux-gnu in Libs}}
 {{Removing -L /usr/lib/x86_64-linux-gnu from libs for arrow}}
 {{ pre-remove: arrow}}
 {{post-remove: arrow}}
 {{original: arrow}}
 {{ sorted: arrow}}
 {{adding LIBS_L string """"}}
 {{returning flags string """"}}

Workaround: set the PKG_CONFIG_ALLOW_SYSTEM_LIBS env var.

{{PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 pkg-config --debug --libs-only-L arrow}}

{{Adding 'arrow' to list of known packages}}
 {{Package arrow has -I/usr/include in Cflags}}
 {{Removing -I/usr/include from cflags for arrow}}
 {{Package arrow has -L /usr/lib/x86_64-linux-gnu in Libs}}
 {{ pre-remove: arrow}}
 {{post-remove: arrow}}
 {{original: arrow}}
 {{ sorted: arrow}}
 {{adding LIBS_L string ""-L/usr/lib/x86_64-linux-gnu ""}}
 {{returning flags string ""-L/usr/lib/x86_64-linux-gnu""}}
 {{-L/usr/lib/x86_64-linux-gnu}}

",pull-request-available,['Python'],ARROW,Bug,Minor,2019-07-23 13:05:04,2
13246544,[Python] pyarrow wheel:  `DLL load failed` when importing on windows,"When installing pyarrow 0.14.1 on windows 10 x64 with python 3.7, you get:

>>> import pyarrow
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""C:\Python37\lib\site-packages\pyarrow\__init__.py"", line 49, in <module>
  from pyarrow.lib import cpu_count, set_cpu_count
 ImportError: DLL load failed: The specified module could not be found.

 On 0.14.0 everything works fine.",pull-request-available wheel,"['Packaging', 'Python']",ARROW,Bug,Major,2019-07-23 11:31:35,3
13246542,[Java] Support range searcher,"For a sorted vector, the range searcher finds the first/last occurrence of a particular element.

The search is based on binary search, which takes O(logn) time.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-23 10:55:25,7
13246534,[C++] Fall back on known Apache mirror for Thrift downloads,"AppVeyor builds have started failing with SSL certificate errors on www.apache.org.
See e.g. https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/26166313

Underlying cause is https://github.com/conda-forge/python-feedstock/issues/267
",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2019-07-23 09:59:36,2
13246434,[Release] Don't parallelize the bintray upload script,"It was spawning a lot of docker containers, and resulted fragile uploads.
Patch provided by [~kou] is attached.",pull-request-available,['Packaging'],ARROW,Task,Major,2019-07-22 23:27:01,1
13246418,[C++] Empty IPC streams containing a dictionary are corrupt,"
{code:java}
#include <arrow/api.h>
#include <arrow/ipc/api.h>
#include <arrow/io/api.h>

void check(arrow::Status status) {
    if (!status.ok()) {
        status.Abort();
    }
}

int main() {
    auto type = arrow::dictionary(arrow::int8(), arrow::utf8());
    auto f0 = arrow::field(""f0"", type);
    auto schema = arrow::schema({f0});

    std::shared_ptr<arrow::io::BufferOutputStream> os;
    check(arrow::io::BufferOutputStream::Create(0, arrow::default_memory_pool(), &os));

    std::shared_ptr<arrow::ipc::RecordBatchWriter> writer;
    check(arrow::ipc::RecordBatchStreamWriter::Open(&*os, schema, &writer));
    check(writer->Close());

    std::shared_ptr<arrow::Buffer> buffer;
    check(os->Finish(&buffer));
    arrow::io::BufferReader is(buffer);

    std::shared_ptr<arrow::ipc::RecordBatchReader> reader;
    check(arrow::ipc::RecordBatchStreamReader::Open(&is, &reader));

    std::shared_ptr<arrow::RecordBatch> batch;
    check(reader->ReadNext(&batch));
}
{code}

{noformat}
-- Arrow Fatal Error --
Invalid: Expected message in stream, was null or length 0{noformat}
It seems like this was caused by[https://github.com/apache/arrow/commit/e68ca7f9aed876a1afcad81a417afb87c94ee951], which moved the dictionaryvalues from the DataType to the array itself.

I initially thought I could work around this by writing a zero-length table but that doesn't seem to actually work.

",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-22 21:15:41,14
13246415,[C++] CSV reader ignore_empty_lines option doesn't handle empty lines,"Followup to https://issues.apache.org/jira/browse/ARROW-5747. If{{ignore_empty_lines}} is false and there are empty lines, it fails to parse (again, with {{Invalid: Empty CSV file}}).

Correct behavior should be to fill those empty lines with missing data for all columns.",csv pull-request-available,['C++'],ARROW,Bug,Minor,2019-07-22 20:47:03,2
13246414,[C++] Better input validation and error messaging in CSV reader,"Followup to https://issues.apache.org/jira/browse/ARROW-5747.The error message(s) are not great when you give bad input. For example, if I give too many or too few {{column_names}}, the error I get is {{Invalid: Empty CSV file}}. In fact, that's about the only error message I've seen from the CSV reader, no matter what I've thrown at it.

It would be better iferror messages were more specific so that I as a user might know how to fix my bad input.",csv,"['C++', 'R']",ARROW,Improvement,Major,2019-07-22 20:44:37,2
13246406,[C++][Gandiva] TestCastFunctions does not test int64 casting`,{{outputs[2]}} (corresponds to cast from float32) is checked twice https://github.com/apache/arrow/pull/4817/files#diff-2e911c4dcae01ea2d3ce200892a0179aR478 while {{outputs[1]}} is not checked (corresponds to cast from int64),pull-request-available,['C++ - Gandiva'],ARROW,Bug,Minor,2019-07-22 20:19:03,6
13246360,[C++] Required header files missing when built with -DARROW_DATASET=OFF,"
{noformat}
In file included from /opt/arrow/include/arrow/type_fwd.h:23:0,
                 from /opt/arrow/include/arrow/type.h:29,
                 from /opt/arrow/include/arrow/array.h:32,
                 from /opt/arrow/include/arrow/api.h:23,
                 from src/bindings.cc:1:
/opt/arrow/include/arrow/util/iterator.h:20:10: fatal error: arrow/dataset/visibility.h: No such file or directory
 #include ""arrow/dataset/visibility.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~{noformat}
",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-22 16:18:20,6
13246284,[Java] Open a document to track the API changes,"We need a document to track the API behavior changes, so as not forget about them for the next release.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-22 09:45:33,7
13246242,[Java] Support dictionary encoding for Union type,"Now only Union type is not supported in dictionary encoding.

In the last several weeks, we did some refactor for encoding and now it's time to support Union type.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-22 07:13:20,16
13246238,[Java] Avoid resource leak in flight service,"# In FlightService#doPutCustom, the flight stream must be closed, even if an exception is thrown during the call ofresponseObserver.onError
 # The exception occurred during the call to acceptPut should not be swallowed.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Minor,2019-07-22 06:40:53,7
13246168,[Python] Reading a dictionary column from Parquet results in disproportionate memory usage,"I'm using pyarrow to read a 40MB parquet file.

When reading all of the columns besides the ""body"" columns, the process peaks at 170MB.

Reading only the ""body"" column results in over 6GB of memory used.

I made the file publicly accessible: s3://dhavivresearch/pyarrow/demofile.parquet



",memory parquet,['Python'],ARROW,Bug,Major,2019-07-21 11:44:50,14
13246086,[C++] Array::View fails for string/utf8 as binary,"I encountered this

{code}
-- Arrow Fatal Error --
Invalid: Can't view array of type string as binary: not enough buffers for view type
In ../src/arrow/array.cc, line 1049, code: CheckInputAvailable()
In ../src/arrow/array.cc, line 1100, code: impl.MakeDataView(out_field, &out_data)
{code}

when trying to add a {{BinaryWithRepeats}} function to {{RandomArrayGenerator}}

{code}
  std::shared_ptr<Array> out;
  auto strings = StringWithRepeats(size, unique, min_length, max_length,
                                   null_probability);
  ABORT_NOT_OK(strings->View(binary(), &out));
  return out;
{code}

It looks like utf8 <-> binary view simply aren't tested in array-view-test",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-19 21:55:55,14
13245969,[Java] Avro adapter implement simple Record type ,"1.implement simple Record type witch only contains primitive types

2. add ByteBuffer cache in String/Bytes consumer to reduce creations.",pull-request-available,['Java'],ARROW,Sub-task,Minor,2019-07-19 10:52:57,16
13245917,[Java] Code cleanup for dictionary encoding,"In last few weeks, we did some refactor in dictionary encoding.

Since the new designed hash table for {{DictionaryEncoder}}and {{hashCode}}& {{equals}}API in {{ValueVector}}already checked in, some classed are no use anymore like {{DictionaryEncodingHashTable}}, {{BaseBinaryVector}}and related benchmarks & UT.

Fortunately, these changes are not made into version 0.14, which makes possible to remove them.",pull-request-available,['Java'],ARROW,Improvement,Critical,2019-07-19 04:35:42,16
13245912,[Developer] Do not suggest setting Fix Version for point releases in dev/merge_arrow_pr.py,I think the merge tool should only suggest major releases by default. This will help avoid having resolved issues spuriously set to a patch release Fix Version,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-07-19 03:18:33,14
13245876,[C++] DictionaryBuilder<T> initialization with Array can fail silently,"See

https://github.com/apache/arrow/blob/master/cpp/src/arrow/array/builder_dict.cc#L267

I think it would be better to expose {{InsertValues}} on {{DictionaryBuilder}} and initialize from a known dictionary that way",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-18 21:31:56,2
13245858,[FlightRPC] Expose (de)serialization of protocol types,"It would be nice to be able to serialize/deserialize Flight types (e.g. FlightInfo) to/from the binary representations, in order to interoperate with systems that might want to provide (say) Flight tickets or FlightInfo without using the Flight protocol. For instance, you might have a search server that exposes a REST interface and wants to provide FlightInfo objects for Flight clients, without having to listen on a separate port.",pull-request-available,['FlightRPC'],ARROW,New Feature,Major,2019-07-18 18:58:23,0
13245782,[FlightRPC] [Java] Integration test client doesn't close buffers,"The integration test client doesn't close any of the clients or free any of the buffers it creates.

Trying to do so leads to a leak problem on the dictionary vector case.",pull-request-available,"['FlightRPC', 'Integration', 'Java']",ARROW,Test,Major,2019-07-18 13:21:34,0
13245753,[C++] [Python] Method for read_csv to limit which columns are read?,In pandas there is pd.read_csv(usecols=...) but I can't see a way to do this in pyarrow.,csv pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2019-07-18 11:25:57,2
13245702,[C++] RETURN_IF_ERROR(ctx) should be namespaced,"RETURN_IF_ERROR is a common macro, it shouldn't be exposed in a header file without namespacing to Arrow.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-07-18 08:07:33,15
13245659,[Python][C++] Enable CSV reader to read from concatenated gzip stream,"If two gzipped files are concatenated together, the result is a valid gzip file. However, it appears that pyarrow.csv.read_csv will only read the portion related to the first file.

If the repro script [here|https://gist.github.com/jordansamuels/d69f1c22c58418f5dfa0785b9ecd211e]is run, the output is:

{{$ python repro.py}}
{{pyarrow.csv only reads one row:}}
{{ x}}
{{0 1}}
{{pandas reads two rows:}}
{{ x}}
{{0 1}}
{{1 2}}
{{pyarrow version: 0.14.0}}",pull-request-available,['Python'],ARROW,New Feature,Minor,2019-07-18 03:12:15,2
13245653,[Java] Variable width vectors' get methods should return null when the underlying data is null,"For variable-width vectors (VarCharVector and VarBinaryVector), when the validity bit is not set, it means the underlying data is null, so the get method should return null.

However, the current implementation throws an IllegalStateException whenNULL_CHECKING_ENABLED is set, or returns an empty array when the flag is clear.

Maybe the purpose of this design is to be consistent with fixed-width vectors. However, the scenario is different: fixed-width vectors (e.g. IntVector) throw an IllegalStateException, simply because the primitive types are non-nullable.",behavior-changes pull-request-available,['Java'],ARROW,Bug,Major,2019-07-18 02:21:07,7
13245550,[Website] Blog post introducing Arrow Flight,"I think it's a good time to be bringing more attention to our work over the last 12-14 months on Arrow Flight. 

I would be OK to draft an initial version of the blog post, and I can circulate to others for review / edit / comment. If there are particular benchmarks you would like to see included, contributing code for that would also be helpful. My plan would be to show tcp throughput on localhost, and node-to-node throughput on a local gigabit ethernet network. I think the localhost throughput is important to show that Flight is a tool that you would want to reach for for faster throughput in high performance networking (e.g. 10/40 gigabit)",pull-request-available,['Website'],ARROW,New Feature,Major,2019-07-17 15:35:09,14
13245469,[Java] Provide pointer to Arrow buffer,"Introduce pointer to a memory region within an ArrowBuf.

This pointerwill be used as the basis for calculating the hash code within a vector, and equality determination.

This data structure can be considered as a ""universal value holder"".",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-17 12:14:00,7
13245458,[Java] Remove duplicate Preconditions check in JDBC adapter,Some Preconditions check are duplicate in {{JdbcToArrow#sqlToArrow}},pull-request-available,['Java'],ARROW,Bug,Minor,2019-07-17 11:41:01,16
13245457,[Java] DateUtility#timeZoneList is not correct,"Now {{timeZoneList}} in{{DateUtility}} belongs to Joda time.

Since we have replace Joda time with Java time in ARROW-2015, this should also be changed.

{{TimeStampXXTZVectors}} have a timezone member which seems not used now and its {{getObject}} returns Long(different with that in {{TimeStampXXVectors}} which returns {{LocalDateTime}}), should it return {{LocalDateTime}} with its timezone?

Is it reasonable if we do as follows:
 # removeJoda {{timezoneList}}in {{DateUtility}}
 # add method like{{getLocalDateTimeFromEpochMilli(long epochMillis, String timezone)}} in DateUtility
 # Not sure make {{TimeStampXXTZVectors}} return {{LocalDateTime}}

cc [~emkornfield@gmail.com] [~bryanc] [~siddteotia]",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-17 11:36:23,16
13245418,[Python] Capacity error when converting large UTF32 numpy array to arrow array,"Trying to create a large string array fails with

ArrowCapacityError: Encoded string length exceeds maximum size (2GB)

instead of creatinga chunked array.



A reproducible example:
{code:java}
import uuid
import numpy as np
import pyarrow as pa

li = []
for i in range(100000000):
    li.append(uuid.uuid4().hex)
arr = np.array(li)
parr = pa.array(arr)
{code}
Is it a regression or was it never properly fixed:[https://github.com/apache/arrow/issues/1855]?



",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-17 09:27:38,14
13245364,[R] R Appveyor job does not test changes in the C++ library,"It seems like master is being used 

https://github.com/apache/arrow/blob/master/ci/PKGBUILD#L42

I observed this in 

https://ci.appveyor.com/project/wesm/arrow/builds/26030853/job/7vn8q3l8e24t83jh?fullLog=true

from this PR

https://github.com/apache/arrow/pull/4841 for ARROW-5893",pull-request-available,['R'],ARROW,Bug,Major,2019-07-17 03:05:45,4
13245330,[CI][Python] Do not test manylinux1 wheels in Travis CI,These can be tested via Crossbow either on demand or nightly. Removing these from Travis CI will save 30 minutes of build time resulting in better team productivity,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2019-07-16 21:57:22,14
13245326,[R] Be able to run R-only tests even without C++ library,"I was looking at test output on Linux in preparation for CRAN release and noticed that all tests were being skipped. We should have a way to run tests for code that only is in R and doesn't require linking to the C++ library,not only to protect against possible criticism from CRAN but also to make it easier for community members in the future to contribute R patches even if they struggle to set upa C++ environment.",pull-request-available,['R'],ARROW,Improvement,Minor,2019-07-16 21:04:58,4
13245198,[Python] Link zlib statically in the wheels,Bundling dependencies statically is preferred over bundling as shared libs. ,pull-request-available,['Python'],ARROW,Task,Major,2019-07-16 11:13:11,3
13245019,[Python] Segfault when reading empty table with category as pandas dataframe,"I have two short sample programs which demonstrate the issue:
{code:java}
import pyarrow as pa
import pandas as pd
empty = pd.DataFrame({'foo':[]},dtype='category')
table = pa.Table.from_pandas(empty)
outfile = pa.output_stream('bar')
writer = pa.RecordBatchFileWriter(outfile,table.schema)
writer.write(table)
writer.close()
{code}
{code:java}
import pyarrow as pa
pa.ipc.open_file('bar').read_pandas()
Segmentation fault
{code}

My apologies if this was already reported elsewhere, I searched but could not find an issue which seemed to refer to the same behavior.",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-15 17:09:20,5
13244823,[Rust] [DataFusion] Remove serde_json dependency,"I added a dependency to serde_json early on so that I could serialize logical query plans because I wanted a way to pass them between processes. However, this was just a short term hack and is non-standard. I would like to remove this now.

I am now using gRPC in another project and serializing plans that way based on the Gandiva protobuf def. I will start a discussion on the mailing list in the next 1-2 weeks about pushing some changes into the Arrow repo related to this.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-07-14 14:25:40,10
13244787,[Rust] [DataFusion] Projection push down with aggregate producing incorrect results,"I was testing some queries with the 0.14 release and noticed that the projected schema for a table scan is completely wrong (however the results of the query are not necessarily wrong)


{code:java}
// schema for nyxtaxi csv files
let schema = Schema::new(vec![
    Field::new(""VendorID"", DataType::Utf8, true),
    Field::new(""tpep_pickup_datetime"", DataType::Utf8, true),
    Field::new(""tpep_dropoff_datetime"", DataType::Utf8, true),
    Field::new(""passenger_count"", DataType::Utf8, true),
    Field::new(""trip_distance"", DataType::Float64, true),
    Field::new(""RatecodeID"", DataType::Utf8, true),
    Field::new(""store_and_fwd_flag"", DataType::Utf8, true),
    Field::new(""PULocationID"", DataType::Utf8, true),
    Field::new(""DOLocationID"", DataType::Utf8, true),
    Field::new(""payment_type"", DataType::Utf8, true),
    Field::new(""fare_amount"", DataType::Float64, true),
    Field::new(""extra"", DataType::Float64, true),
    Field::new(""mta_tax"", DataType::Float64, true),
    Field::new(""tip_amount"", DataType::Float64, true),
    Field::new(""tolls_amount"", DataType::Float64, true),
    Field::new(""improvement_surcharge"", DataType::Float64, true),
    Field::new(""total_amount"", DataType::Float64, true),
]);

let mut ctx = ExecutionContext::new();
ctx.register_csv(""tripdata"", ""file.csv"", &schema, true);

let optimized_plan = ctx.create_logical_plan(
    ""SELECT passenger_count, MIN(fare_amount), MAX(fare_amount) \
        FROM tripdata GROUP BY passenger_count"").unwrap();{code}
The projected schema in the table scan has the first two columns from the schema (VendorID and tpetp_pickup_datetime) rather than passenger_count and fare_amount",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Bug,Major,2019-07-13 22:06:16,10
13244756,[Rust] [DataFusion] Table trait should support building complete queries,"DataFusion 0.13 included a preview Table trait, which provides a DataFrame style method of building a logical query plan, but it was not usable for any real-world queries.

I would now like the trait to support building real queries, especially aggregate queries.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2019-07-13 13:59:16,10
13244695,[C++] ArrayBuilders with mutable type are not robustly supported,"(Dense|Sparse)UnionBuilder, DictionaryBuilder, Addaptive(U)IntBuilders and any nested builder which contains one of those may Finish to an array whose type disagrees with what was passed to MakeBuilder. This is not well documented or supported; ListBuilder checks if its child has changed type but StructBuilder does not. Furthermore ListBuilder's check does not catch modifications to a DictionaryBuidler's type and results in an invalid array on Finish: https://github.com/apache/arrow/blob/1bcfbe1/cpp/src/arrow/array-dict-test.cc#L951-L994

Let's add to the ArrayBuilder contract: the type property is null iff that builder's type is indeterminate until Finish() is called. Then all nested builders can check this on their children at construction and bubble type mutability correclty",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-12 20:38:29,6
13244692,[Python] Bundle arrow's LICENSE with the wheels,"Guide to bundle LICENSE files with the wheels: https://wheel.readthedocs.io/en/stable/user_guide.html#including-license-files-in-the-generated-wheel-file

We also need to ensure, that all thirdparty dependencies' license are attached to it, especially because we're statically linking multiple 3rdparty dependencies, and for example uriparser is missing from the LICENSE file.

cc [~wesmckinn]",pull-request-available,['Python'],ARROW,Task,Major,2019-07-12 20:25:50,3
13244646,[FlightRPC] [Python] Flight CI tests are failing,"Flight tests segfault on Travis: [https://travis-ci.org/apache/arrow/jobs/557690959]

The relevant part is:
{noformat}
Fatal Python error: Aborted
Thread 0x00007fcf009fe700 (most recent call first):
  File ""/home/travis/build/apache/arrow/python/pyarrow/tests/test_flight.py"", line 386 in _server_thread
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/threading.py"", line 864 in run
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/threading.py"", line 884 in _bootstrap
Current thread 0x00007fcf1f9fa700 (most recent call first):
  File ""/home/travis/build/apache/arrow/python/pyarrow/tests/test_flight.py"", line 411 in flight_server
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/contextlib.py"", line 99 in __exit__
  File ""/home/travis/build/apache/arrow/python/pyarrow/tests/test_flight.py"", line 670 in test_tls_do_get
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/python.py"", line 165 in pytest_pyfunc_call
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/callers.py"", line 187 in _multicall
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 81 in <lambda>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 87 in _hookexec
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/hooks.py"", line 289 in __call__
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/python.py"", line 1451 in runtest
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 117 in pytest_runtest_call
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/callers.py"", line 187 in _multicall
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 81 in <lambda>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 87 in _hookexec
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/hooks.py"", line 289 in __call__
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 192 in <lambda>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 220 in from_call
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 192 in call_runtest_hook
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 167 in call_and_report
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 87 in runtestprotocol
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/runner.py"", line 72 in pytest_runtest_protocol
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/callers.py"", line 187 in _multicall
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 81 in <lambda>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 87 in _hookexec
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/hooks.py"", line 289 in __call__
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/main.py"", line 278 in pytest_runtestloop
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/callers.py"", line 187 in _multicall
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 81 in <lambda>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 87 in _hookexec
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/hooks.py"", line 289 in __call__
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/main.py"", line 257 in _main
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/main.py"", line 213 in wrap_session
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/main.py"", line 250 in pytest_cmdline_main
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/callers.py"", line 187 in _multicall
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 81 in <lambda>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/manager.py"", line 87 in _hookexec
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pluggy/hooks.py"", line 289 in __call__
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/_pytest/config/__init__.py"", line 74 in main
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/pytest.py"", line 102 in <module>
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/coverage/execfile.py"", line 192 in run_python_file
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/coverage/execfile.py"", line 122 in run_python_module
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/coverage/cmdline.py"", line 627 in do_run
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/coverage/cmdline.py"", line 491 in command_line
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/coverage/cmdline.py"", line 756 in main
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/bin/coverage"", line 10 in <module>
/home/travis/build/apache/arrow/ci/travis_script_python.sh: line 202: 12032 Aborted                 (core dumped) coverage run --append -m pytest $PYARROW_PYTEST_FLAGS pyarrow/tests{noformat}
I haven't been able to reproduce this so far. We may want to just mark the tests as flaky for now.",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2019-07-12 16:17:31,2
13244633,[Java] Define API for ExtensionVector whose data must be serialized prior to being sent via IPC,"As being discussed on the mailing list, a possible use case for ExtensionVector involves having the Arrow buffers contain pointer-type values referring to memory outside of the Arrow memory heap. In IPC, such vectors would need to be serialized to a wholly Arrow-resident form, such as a VarBinaryVector. We do not have an API to allow for this, so this JIRA proposes to add new functions that can indicate to the IPC layer that an ExtensionVector requires additional serialization to a native Arrow type (in such case, the extension type metadata would be discarded)",pull-request-available,['Java'],ARROW,Improvement,Major,2019-07-12 14:30:11,7
13244625,[Java] Test fuzzer inputs,"We are developing a fuzzer-based corpus of malformed IPC inputs

https://github.com/apache/arrow-testing/tree/master/data/arrow-ipc

The Java implementation should also test against these to verify that the correct kind of exception is raised",pull-request-available,['Java'],ARROW,Improvement,Major,2019-07-12 14:16:49,7
13244575,[Java] Support sort & compare for all variable width vectors,All types of variable-width vector can reuse the same comparator for sorting & searching.,pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-12 10:16:16,7
13244551,[Java] Add get to BaseIntVector interface,"1.In the set method should not use long as parameter. It is hardly the case that there are more than 2^32 distinct values in a dictionary. If it really happens, maybe it means we should not have used dictionary in the first place.

2. In addition to the get method, there should also be a set method.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-12 08:31:31,16
13244549,[Java] Redesign the dictionary encoder,"The current dictionary encoder implementation (org.apache.arrow.vector.dictionary.DictionaryEncoder) has heavy performance overhead, which prevents it from being useful in practice:
 # There are repeated conversions between Java objects and bytes (e.g. vector.getObject(i)).
 # Unnecessary memory copy (the vector data must be copied to the hash table).
 # The hash table cannot be reused for encoding multiple vectors (other data structure & results cannot be reused either).
 # The output vector should not be created/managed by the encoder (just like in the out-of-place sorter)
 # The hash table requires that the hashCode & equals methods be implemented appropriately, but this is not guaranteed.

We plan to implement a new one in the algorithm module, and gradually deprecate the current one.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-12 08:25:22,7
13244300,[Java] Make ListVector and MapVector create reader lazily,"Current implementation creates reader eagerly, which may cause unnecessary resource and time. This issue changes thebehavior to lazily create the reader.

This is a follow-up issue for ARROW-5897.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-11 11:54:29,7
13244299,[Python] read_tensor() fails on non-seekable streams,"when reading a tensor from from a compressed pyarrow stream, it fails with
{code:java}
Traceback (most recent call last):
 File ""test.py"", line 10, in <module>
 tensor = pa.read_tensor(in_stream)
 File ""pyarrow/ipc.pxi"", line 470, in pyarrow.lib.read_tensor
 File ""pyarrow/io.pxi"", line 153, in pyarrow.lib.NativeFile.get_random_access_file
 File ""pyarrow/io.pxi"", line 182, in pyarrow.lib.NativeFile._assert_seekable
OSError: only valid on seekable files{code}
example code:
{code:java}
import pyarrow as pa
import numpy as np

a = np.random.random(size = (100,110,3) )

out_stream = pa.output_stream('test.pa', compression='gzip', buffer_size=None)
pa.write_tensor(pa.Tensor.from_numpy(a), out_stream)

in_stream = pa.input_stream('test.pa', compression='gzip', buffer_size=None)
tensor = pa.read_tensor(in_stream)
b = pa.Tensor.to_numpy(tensor){code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-11 11:53:06,14
13244217,[Java] Optimize ByteFunctionHelpers equals & compare logic,"Nowit first compare Long values and then if length < 8 then it compares Byte values.

Add the logic to compare Intvalues when 4 < length < 8.

",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-11 04:28:22,16
13244186,"[CI] Set -DARROW_VERBOSE_THIRDPARTY_BUILD=OFF in builds running in Travis CI, maybe all docker-compose builds by default",This setting should be disabled in general unless we are trying to debug something. It makes logs much more verbose,pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-10 22:29:12,14
13244102,[Java] Implement hash table and equals & hashCode API for dictionary encoding,"As discussed in[https://github.com/apache/arrow/pull/4792]

Implement a hash table to only store hash & index, meanwhile add check equal function in ValueVector API.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-10 12:47:19,16
13244084,[Python][Packaging] Bundle uriparser.dll in windows wheels ,"The windows nightly wheel builds are failing: https://ci.appveyor.com/project/Ursa-Labs/crossbow/builds/25688922 probably caused by 88fcb09, but it's hard to tell because of the error message ""ImportError: DLL load failed: The specified module could not be found."" is not very descriptive.

Theoretically it shouldn't affect the 0.14 release because 88fcb09 was added afterwards.",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-07-10 11:16:03,3
13244041,[Java] Provide functionality to efficiently compute hash code for arbitrary memory segment,"This issue adds a functionality toefficiently compute the hash code fora consecutive memory region. This functionality is important in practical scenarios because it helps:
 * Avoid unnecessary memory copy.

 * Avoid repeated conversions between Java objects & Arrow buffers.

Since the algorithm for calculating hash code has significant performance implications, we need to design an interface so that different algorithms can be easily introduces as plug-ins.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-10 08:20:57,7
13244010,[Java] Remove duplicated logic in MapVector,"Current implementation of MapVector containsmuchlogic duplicate from the super class.We remove the duplication by:
 # Making the default data vector name configurable
 # Extract a method for creating the reader",pull-request-available,['Java'],ARROW,Improvement,Major,2019-07-10 04:18:31,7
13243940,[C++] Remove arrow::Column class from C++ library,"Opening JIRA per ongoing discussion on mailing list.

This class unfortunately touches a lot of places, so I'm going to start by removing it from the C++ and Python libraries to assist with discussion about its fate. ",pull-request-available,"['C++', 'GLib', 'MATLAB', 'Python', 'R', 'Ruby']",ARROW,New Feature,Major,2019-07-09 18:13:12,14
13243907,[Python][C++] Add metadata to store Arrow time zones in Parquet file metadata,"The timezone is not roundtrip safe for timezones other than UTC when storing to parquet. Expected behavior would be that the timezone is properly reconstructed

{code:python}
schema = pa.schema(
    [
        pa.field(""no_tz"", pa.timestamp('us')),
        pa.field(""no_tz"", pa.timestamp('us', tz=""UTC"")),
        pa.field(""no_tz"", pa.timestamp('us', tz=""Europe/Berlin"")),
]
)
buf = pa.BufferOutputStream()
pq.write_metadata(
    schema,
    buf,
    coerce_timestamps=""us""
)

pq_bytes = buf.getvalue().to_pybytes()
reader = pa.BufferReader(pq_bytes)
parquet_file = pq.ParquetFile(reader)
parquet_file.schema.to_arrow_schema()
#Output:
# no_tz: timestamp[us]
# utc: timestamp[us, tz=UTC]
# europe: timestamp[us, tz=UTC]
{code}",parquet pull-request-available,"['C++', 'Python']",ARROW,Improvement,Minor,2019-07-09 15:04:17,14
13243896,[Python][Packaging] Manylinux1/2010 compliance issue with libz,"So we statically link liblz4 in the manylinux1 wheels
{code}
# ldd pyarrow-manylinux1/libarrow.so.14 | grep z
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fc28cef4000)
{code}
but dynamically in the manylinux2010 wheels
{code}
# ldd pyarrow-manylinux2010/libarrow.so.14 | grep z
        liblz4.so.1 => not found  (already deleted to reproduce the issue)
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f56f7440000)
{code}
this what this PR resolves.

What I'm finding strange, that auditwheel seems to bundle libz for manylinux1:
{code}
# ls -lah pyarrow-manylinux1/*z*so.*
-rwxr-xr-x 1 root root 115K Jun 29 00:14 pyarrow-manylinux1/libz-7f57503f.so.1.2.11
{code}
while ldd still uses the system libz:
{code}
# ldd pyarrow-manylinux1/libarrow.so.14 | grep z
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007f91fcf3f000)
{code}
For manylinux2010 we also have liblz4:
{code}
#  ls -lah pyarrow-manylinux2010/*z*so.*
-rwxr-xr-x 1 root root 191K Jun 28 23:38 pyarrow-manylinux2010/liblz4-8cb8bdde.so.1.8.3
-rwxr-xr-x 1 root root 115K Jun 28 23:38 pyarrow-manylinux2010/libz-c69b9943.so.1.2.11
{code}
and ldd similarly tries to load the system libs:
{code}
# ldd pyarrow-manylinux2010/libarrow.so.14 | grep z
        liblz4.so.1 => not found
        libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007fd72764e000)
{code}

Inspecting manylinux1 with `LD_DEBUG=files,libs ldd libarrow.so.14` it seems like to search the right path, but cannot find the hashed version of libz `libz-7f57503f.so.1.2.11`
{code}
       463:     file=libz.so.1 [0];  needed by ./libarrow.so.14 [0]
       463:     find library=libz.so.1 [0]; searching
       463:      search path=/tmp/pyarrow-manylinux1/.          (RPATH from file ./libarrow.so.14)
       463:       trying file=/tmp/pyarrow-manylinux1/./libz.so.1
       463:      search cache=/etc/ld.so.cache
       463:       trying file=/lib/x86_64-linux-gnu/libz.so.1
{code}
There is no `libz.so.1` just `libz-7f57503f.so.1.2.11`.

Similarly for manylinux2010 and libz:
{code}
       470:     file=libz.so.1 [0];  needed by ./libarrow.so.14 [0]
       470:     find library=libz.so.1 [0]; searching
       470:      search path=/tmp/pyarrow-manylinux2010/.               (RPATH from file ./libarrow.so.14)
       470:       trying file=/tmp/pyarrow-manylinux2010/./libz.so.1
       470:      search cache=/etc/ld.so.cache
       470:       trying file=/lib/x86_64-linux-gnu/libz.so.1
{code}
for liblz4 (again, I've deleted the system one):
{code}
       470:     file=liblz4.so.1 [0];  needed by ./libarrow.so.14 [0]
       470:     find library=liblz4.so.1 [0]; searching
       470:      search path=/tmp/pyarrow-manylinux2010/.               (RPATH from file ./libarrow.so.14)
       470:       trying file=/tmp/pyarrow-manylinux2010/./liblz4.so.1
       470:      search cache=/etc/ld.so.cache
       470:      search path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_6$
:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib          (system search path)
{code}
There are no `libz.so.1` nor `liblz4.so.1`, just `libz-c69b9943.so.1.2.11` and `liblz4-8cb8bdde.so.1.8.3`

According to https://www.python.org/dev/peps/pep-0571/ `liblz4` nor `libz` are part of the whitelist, and while these are bundled with the wheel, seemingly cannot be found - perhaps because of the hash in the library name?

I've tried to inspect the wheels with `auditwheel show` with version `2` and `1.10`, both says the following:

{code}
# auditwheel show pyarrow-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl

pyarrow-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl is consistent with
the following platform tag: ""linux_x86_64"".

The wheel references external versioned symbols in these system-
provided shared libraries: libgcc_s.so.1 with versions {'GCC_3.3',
'GCC_3.4', 'GCC_3.0'}, libpthread.so.0 with versions {'GLIBC_2.3.3',
'GLIBC_2.12', 'GLIBC_2.2.5', 'GLIBC_2.3.2'}, libc.so.6 with versions
{'GLIBC_2.4', 'GLIBC_2.6', 'GLIBC_2.2.5', 'GLIBC_2.7', 'GLIBC_2.3.4',
'GLIBC_2.3.2', 'GLIBC_2.3'}, libstdc++.so.6 with versions
{'CXXABI_1.3', 'GLIBCXX_3.4.10', 'GLIBCXX_3.4.9', 'GLIBCXX_3.4.11',
'GLIBCXX_3.4.5', 'GLIBCXX_3.4', 'CXXABI_1.3.2', 'CXXABI_1.3.3'},
librt.so.1 with versions {'GLIBC_2.2.5'}, libm.so.6 with versions
{'GLIBC_2.2.5'}, libdl.so.2 with versions {'GLIBC_2.2.5'}, libz.so.1
with versions {'ZLIB_1.2.0'}

This constrains the platform tag to ""manylinux2010_x86_64"". In order
to achieve a more compatible tag, you would need to recompile a new
wheel from source on a system with earlier versions of these
libraries, such as a recent manylinux image.
{code}

{code}
# auditwheel show pyarrow-0.14.0-cp37-cp37m-manylinux1_x86_64.whl

pyarrow-0.14.0-cp37-cp37m-manylinux1_x86_64.whl is consistent with the
following platform tag: ""linux_x86_64"".

The wheel references external versioned symbols in these system-
provided shared libraries: libgcc_s.so.1 with versions {'GCC_3.4',
'GCC_3.0', 'GCC_3.3'}, libc.so.6 with versions {'GLIBC_2.3',
'GLIBC_2.2.5', 'GLIBC_2.3.4', 'GLIBC_2.4', 'GLIBC_2.3.2'},
libstdc++.so.6 with versions {'CXXABI_1.3', 'GLIBCXX_3.4.5',
'GLIBCXX_3.4'}, librt.so.1 with versions {'GLIBC_2.2.5'}, libm.so.6
with versions {'GLIBC_2.2.5'}, libpthread.so.0 with versions
{'GLIBC_2.3.3', 'GLIBC_2.3.2', 'GLIBC_2.2.5'}, libdl.so.2 with
versions {'GLIBC_2.2.5'}, libz.so.1 with versions {'ZLIB_1.2.0'}

The following external shared libraries are required by the wheel:
{
    ""libc.so.6"": ""/lib/x86_64-linux-gnu/libc-2.24.so"",
    ""libcrypt.so.1"": ""/lib/x86_64-linux-gnu/libcrypt-2.24.so"",
    ""libdl.so.2"": ""/lib/x86_64-linux-gnu/libdl-2.24.so"",
    ""libgcc_s.so.1"": ""/lib/x86_64-linux-gnu/libgcc_s.so.1"",
    ""libm.so.6"": ""/lib/x86_64-linux-gnu/libm-2.24.so"",
    ""libnsl.so.1"": ""/lib/x86_64-linux-gnu/libnsl-2.24.so"",
    ""libpthread.so.0"": ""/lib/x86_64-linux-gnu/libpthread-2.24.so"",
    ""librt.so.1"": ""/lib/x86_64-linux-gnu/librt-2.24.so"",
    ""libstdc++.so.6"": ""/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.22"",
    ""libutil.so.1"": ""/lib/x86_64-linux-gnu/libutil-2.24.so"",
    ""libz.so.1"": ""/lib/x86_64-linux-gnu/libz.so.1.2.8""
}

In order to achieve the tag platform tag ""manylinux2010_x86_64"" the
following shared library dependencies will need to be eliminated:

libz.so.1

In order to achieve the tag platform tag ""manylinux1_x86_64"" the
following shared library dependencies will need to be eliminated:

libz.so.1
{code}

I think there are more todo left with the wheels. IMO the manylinux1 wheels are not compliant because of `libz` and the manylinux2010 wheels are not compliant because of both `libz` and `liblz4` (but incorrectly reported by auditwheel?).

We also need to ensure to run {{auditwheel show}} on the produced wheels in the manylinux-test script https://github.com/apache/arrow/blob/master/dev/tasks/python-wheels/manylinux-test.sh",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-07-09 14:30:47,3
13243870,[Java] Fix the get method of StructVector,"When the data at the specified location is null, there is no need to call the method from super to set the reader

holder.isSet = isSet(index);
super.get(index, holder);",pull-request-available,['Java'],ARROW,Bug,Minor,2019-07-09 10:25:41,7
13243862,[Java] Support dictionary encoding for List and Struct type,"As described in[http://arrow.apache.org/docs/format/Layout.html#dictionary-encoding], List type encoding should be supported.

Now ListVector getObject returns a ArrayList implementation, and its equals and hashCode are already overwritten, so it could be directly supported to be hashMap key in DictionaryEncoder. Since we won't change Dictionary data during encoding/decoding process, use mutable key seems dose't matter.

StructVector is similar to ListVector.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-09 09:50:08,16
13243837,[Java] Provide functionalities to efficiently determine if a validity buffer has completely 1 bits/0 bits,"These utilities can be used to efficiently determine, for example, 
* If all values in a vector are null
* If a vector contains no null
* If a vector contains any valid element
* If a vector contains any invalid element",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-09 07:42:20,7
13243799,[C++] Update arrow parquet writer to use TypedBufferBuilder ,In looking at the code it seems the writer.cc hasn't been updated to use the latest classes.  there is also what appears to be a small performance bug in TypedBufferBuilder,Parquet pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-09 02:44:55,15
13243711,[Python][C++] Parquet reader not forward compatible for timestamps without timezone,"Timestamps without timezone which are written by pyarrow 0.14.0 cannot be read anymore as timestamps by earlier versions.The timestamp is read as an integer when reading in with pyarrow 0.13.0

Looking at the parquet schemas, it seems that the logical type cannot be understood by the older versions, see below.
h4. File generation with pyarrow 0.14.0
{code:java}
import datetime
import pyarrow.parquet as pq
import pandas as pd

df = pd.DataFrame(
    {
        ""datetime64"": pd.Series([""2018-01-01""], dtype=""datetime64[ns]""),
        ""datetime64_ts"": pd.Series(
            [pd.Timestamp(datetime.datetime(2018, 1, 1), tz=""Europe/Berlin"")],
            dtype=""datetime64[ns]"",
        ),
    }
)
pq.write_table(pa.Table.from_pandas(df), ""timezones_pyarrow_14.paquet"")
{code}
h4. Reading with pyarrow 0.13.0
{code:java}
In [1]: import pyarrow.parquet as pq

In [2]: import pyarrow as pa

In [3]: with open(""timezones_pyarrow_14.paquet"", ""rb"") as fd:
   ...:     table = pq.read_pandas(fd)
   ...:

In [4]: table.to_pandas()
Out[4]:
         datetime64             datetime64_ts
0  1514764800000000 2018-01-01 00:00:00+01:00

In [5]: table.to_pandas().dtypes
Out[5]:
datetime64                               int64
datetime64_ts    datetime64[ns, Europe/Berlin]
dtype: object
{code}
h3. Parquet schema as seen by pyarrow versions:

pyarrow 0.13.0 parquet schema
{code:java}
datetime64: INT64
datetime64_ts: INT64 TIMESTAMP_MICROS
{code}
pyarrow 0.14.0 parquet schema
{code:java}
datetime64: INT64 Timestamp(isAdjustedToUTC=false, timeUnit=microseconds)
datetime64_ts: INT64 Timestamp(isAdjustedToUTC=true, timeUnit=microseconds)
{code}",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-07-08 15:17:11,6
13243696,[FlightRPC] Fix auth incompatibilities between Python/Java,"It turns out the blocking-forever issue was a combination of problems in Python and Java. We should simply fix the issues.

---

The Flight Handshake method can be insecure, and currently has a surprising failure mode; we should document these caveats (blocks forever waiting on client/server; insecure depending on deployment configuration)",pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-07-08 14:12:22,0
13243693,[FlightRPC] Test RPC features in integration tests,"We should test not just wire-format compatibility, but feature-compatibility in Flight integration tests. This may mean adding a separate suite of tests to the integration script.

Features that should be tested include:
 * Authentication
 * Error & error code propagation
 * Cancellation
 * Flow control/backpressure",pull-request-available,"['FlightRPC', 'Integration']",ARROW,Test,Major,2019-07-08 14:06:04,0
13243659,[Python] pyarrow 0.14.0 macOS wheels depend on shared libs under /usr/local/opt,"Hello, and congrats on the recent release of Apache Arrow 0.14.0!

This morning I installed pyarrow 0.14.0 on my macOS 10.14.5 system like so:
{code:java}
python3.7 -m venv ~/virtualenv/pyarrow-0.14.0
source ~/virtualenv/pyarrow-0.14.0/bin/activate
pip install --upgrade pip setuptools
pip install pyarrow # installs pyarrow-0.14.0-cp37-cp37m-macosx_10_6_intel.whl

pip freeze --all
# numpy==1.16.4
# pip==19.1.1
# pyarrow==0.14.0
# setuptools==41.0.1
# six==1.12.0
{code}
However I am unable to import pyarrow:
{code:java}
python -c 'import pyarrow'
# Traceback (most recent call last):
# File ""<string>"", line 1, in <module>
# File ""/Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/__init__.py"", line 49, in <module>
# from pyarrow.lib import cpu_count, set_cpu_count
# ImportError: dlopen(/Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/lib.cpython-37m-darwin.so, 2): Library not loaded: /usr/local/opt/openssl/lib/libcrypto.1.0.0.dylib
# Referenced from: /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow.14.dylib
# Reason: image not found
{code}
pyarrow is trying to load a shared library (OpenSSL in this case) from a path under{{/usr/local/opt}} that doesn't exist; perhaps that OpenSSL had been provided by Homebrew as part of your build process? Unfortunately this makes the pyarrow 0.14.0 wheel completely unusable on my system or any system that doesn't have OpenSSL installed in that location. This is a regression from pyarrow 0.13.0 as those wheels ""just worked"".

Additional diagnostic output below. I ran{{otool -L}} on each{{.dylib}} and{{.so}} file in{{/Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow}} and included the output for those with dependencies under{{/usr/local/opt}}:
{code:java}
otool -L /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow.14.dylib
# /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow.14.dylib:
# @rpath/libarrow.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# /usr/local/opt/openssl/lib/libcrypto.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/local/opt/openssl/lib/libssl.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/lib/libz.1.dylib (compatibility version 1.0.0, current version 1.2.8)
# @rpath/libarrow_boost_system.dylib (compatibility version 0.0.0, current version 0.0.0)
# @rpath/libarrow_boost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0)
# @rpath/libarrow_boost_regex.dylib (compatibility version 0.0.0, current version 0.0.0)
# /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.5.0)
# /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.50.2)


otool -L /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow_flight.14.dylib
# /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow_flight.14.dylib:
# @rpath/libarrow_flight.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# @rpath/libarrow.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# /usr/local/opt/openssl/lib/libssl.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/local/opt/openssl/lib/libcrypto.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.5.0)
# /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.50.2)

otool -L /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow_python.14.dylib
# /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libarrow_python.14.dylib:
# @rpath/libarrow_python.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.50.2)
# /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation (compatibility version 150.0.0, current version 1349.64.0)
# @rpath/libarrow_flight.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# @rpath/libarrow.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# /usr/local/opt/openssl/lib/libssl.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/local/opt/openssl/lib/libcrypto.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.5.0)

otool -L /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libgandiva.14.dylib
# /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libgandiva.14.dylib:
# @rpath/libgandiva.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# @rpath/libarrow.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# /usr/local/opt/openssl/lib/libssl.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/local/opt/openssl/lib/libcrypto.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/lib/libz.1.dylib (compatibility version 1.0.0, current version 1.2.8)
# /usr/lib/libncurses.5.4.dylib (compatibility version 5.4.0, current version 5.4.0)
# /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.50.2)
# /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.5.0)

otool -L /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libplasma.14.dylib
# /Users/manselmi/virtualenv/pyarrow-0.14.0/lib/python3.7/site-packages/pyarrow/libplasma.14.dylib:
# @rpath/libplasma.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# @rpath/libarrow.14.dylib (compatibility version 14.0.0, current version 14.0.0)
# /usr/local/opt/openssl/lib/libssl.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/local/opt/openssl/lib/libcrypto.1.0.0.dylib (compatibility version 1.0.0, current version 1.0.0)
# /usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 307.5.0)
# /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.50.2)
{code}",pull-request-available pyarrow wheel,['Python'],ARROW,Bug,Critical,2019-07-08 12:09:25,3
13243655,[Python] Segmentation fault when comparing schema with None,"When comparing a schema with a Python {{None}} I get a segmentation fault.

This is a regression to 0.13.0
{code:java}
In [2]: import pyarrow as pa

In [3]: pa.schema([pa.field(""something"", pa.int64())]).equals(None)
[1]    82085 segmentation fault  ipython
{code}

System information:
System Version: macOS 10.13.6 (17G6030)
Kernel Version: Darwin 17.7.0
Python 3.6.7",pull-request-available,['Python'],ARROW,Bug,Minor,2019-07-08 12:02:10,5
13243537,"[C++] Development compile instructions need to include ""make""","Following the build instructions on

[https://arrow.apache.org/docs/python/development.html]

using condaI additionally needed to install the ""make"" and ""re2"" packages for cmake to succeed. These are such common packages, it probably didn't come up in your tests, but I have a minimal system.

(It's not done with ""make"", but itlookspromising so far.)",documentation pull-request-available,['C++'],ARROW,Bug,Major,2019-07-07 22:54:30,14
13243485,[Python] manylinux2010 wheels have shared library dependency on liblz4,"I am using pyarrow in my project. It works well for version 0.13.0

However, it seems recently there is a release for 0.14.0. After upgrading to the latest, I got this error.
AttributeError: module 'pyarrow' has no attribute 'compat'

Stacktrace:
 2019-07-06 09:08:21 Traceback (most recent call last):
 2019-07-06 09:08:21 File ""/home/jenkins/workspace/CLIENTS_PERF/Tests/ClientsPerf/PythonConnectorPerf/src/PerfTestRunner.py"", line 12, in <module>
 2019-07-06 09:08:21 import snowflake.connector
 2019-07-06 09:08:21 File ""/home/jenkins/workspace/CLIENTS_PERF/Tests/ClientsPerf/PythonConnectorPerf/pythonconnector-perf/lib/python3.5/site-packages/snowflake/connector/__init__.py"", line 21, in <module>
 2019-07-06 09:08:21 from .connection import SnowflakeConnection
 2019-07-06 09:08:21 File ""/home/jenkins/workspace/CLIENTS_PERF/Tests/ClientsPerf/PythonConnectorPerf/pythonconnector-perf/lib/python3.5/site-packages/snowflake/connector/connection.py"", line 42, in <module>
 2019-07-06 09:08:21 from .cursor import SnowflakeCursor, LOG_MAX_QUERY_LENGTH
 2019-07-06 09:08:21 File ""/home/jenkins/workspace/CLIENTS_PERF/Tests/ClientsPerf/PythonConnectorPerf/pythonconnector-perf/lib/python3.5/site-packages/snowflake/connector/cursor.py"", line 35, in <module>
 2019-07-06 09:08:21 from pyarrow.ipc import open_stream
 2019-07-06 09:08:21 File ""/home/jenkins/workspace/CLIENTS_PERF/Tests/ClientsPerf/PythonConnectorPerf/pythonconnector-perf/lib/python3.5/site-packages/pyarrow/__init__.py"", line 47, in <module>
 2019-07-06 09:08:21 import pyarrow.compat as compat

I can provide more detail if requested.",pull-request-available,['Python'],ARROW,Bug,Blocker,2019-07-06 16:44:04,14
13243414,[Release] Helper script for rebasing open pull requests on master,Create a script so we don't have to manually rebase all open pull requests off of master.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-07-05 16:14:03,15
13243406,[Python] simplify cython wrapping of Result,See answer in https://github.com/cython/cython/issues/3018,pull-request-available,['Python'],ARROW,Improvement,Major,2019-07-05 15:22:25,5
13243391,[Python] Segmentation Fault via pytest-runner,"When running {{pytest}}on projects using {{pyarrow==0.14.0}}on Linux, I am getting segmentation faults, but interestingly _only_when run via {{pytest-runner}}(which provides the {{setup.py pytest}} command)

This works (i.e. {{pytest}} directly):
{code:java}
$ pytest

Test session starts (platform: linux, Python 3.7.3, pytest 5.0.0, pytest-sugar 0.9.2)
benchmark: 3.2.2 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/josh/scratch/pyarrow-issue
plugins: sugar-0.9.2, Flask-Dance-2.2.0, env-0.6.2, mock-1.10.4, xdist-1.29.0, requests-mock-1.6.0, forked-1.0.2, dash-1.0.0, cov-2.7.1, html-1.21.1, benchmark-3.2.2, metadata-1.8.0
collecting ...
tests/test_pyarrow.py  100% 

Results (0.09s):
1 passed{code}
However, this does not work, ending in a segmentation fault, even though the tests pass:
{code:java}
$ python setup.py pytest

running pytest
running egg_info
writing pyarrow_issue.egg-info/PKG-INFO
writing dependency_links to pyarrow_issue.egg-info/dependency_links.txt
writing requirements to pyarrow_issue.egg-info/requires.txt
writing top-level names to pyarrow_issue.egg-info/top_level.txt
reading manifest file 'pyarrow_issue.egg-info/SOURCES.txt'
writing manifest file 'pyarrow_issue.egg-info/SOURCES.txt'
running build_ext

Test session starts (platform: linux, Python 3.7.3, pytest 5.0.0, pytest-sugar 0.9.2)
benchmark: 3.2.2 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/josh/scratch/pyarrow-issue
plugins: sugar-0.9.2, Flask-Dance-2.2.0, env-0.6.2, mock-1.10.4, xdist-1.29.0, requests-mock-1.6.0, forked-1.0.2, dash-1.0.0, cov-2.7.1, html-1.21.1, benchmark-3.2.2, metadata-1.8.0
collecting ...
tests/test_pyarrow.py  100% 

Results (0.07s):
1 passed
zsh: segmentation fault (core dumped) python setup.py pytest{code}
backtrace from {{gdb}}
{code:java}
Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
0x00007ffff7c10b58 in ?? () from /usr/lib/libpython3.7m.so.1.0

(gdb) bt
#0 0x00007ffff7c10b58 in ?? () from /usr/lib/libpython3.7m.so.1.0
#1 0x00007ffff7ae46cc in ?? () from /usr/lib/libpython3.7m.so.1.0
#2 0x00007ffff023a6b3 in arrow::py::PyExtensionType::~PyExtensionType() ()
from /home/josh/.virtualenvs/default/lib/python3.7/site-packages/pyarrow/./libarrow_python.so.14
#3 0x00007fffed5e6467 in std::unordered_map<std::string, std::shared_ptr<arrow::ExtensionType>, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::shared_ptr<arrow::ExtensionType> > > >::~unordered_map() ()
from /home/josh/.virtualenvs/default/lib/python3.7/site-packages/pyarrow/./libarrow.so.14
#4 0x00007ffff7de5e70 in __run_exit_handlers () from /usr/lib/libc.so.6
#5 0x00007ffff7de5fae in exit () from /usr/lib/libc.so.6
#6 0x00007ffff7dcfeea in __libc_start_main () from /usr/lib/libc.so.6
#7 0x000055555555505e in _start ()
{code}
I have observed this behaviour on my machine running natively, and also via docker. Also, 0.13.0 does not exhibit this behaviour

",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-05 13:26:38,14
13243335,[Java] Provide dictionary builder,"The dictionary builder servers for the following scenario which is frequently encountered in practice when dictionary encoding is involved: the dictionary values are not known a priori, so they are determined dynamically, as new data arrive continually.

In particular, when a new value arrives, it is tested to check if it is already in the dictionary. If so, it is simply neglected, otherwise, it is added to the dictionary.
 
When all values have been evaluated, the dictionary can be considered complete. So encoding can start afterward.

The code snippet using a dictionary builder should be like this:

{{DictonaryBuilder<IntVector> dictionaryBuilder = ...}}
{{dictionaryBuilder.startBuild();}}
{{...}}
{{dictionaryBuild.addValue(newValue);}}
{{...}}
{{dictionaryBuilder.endBuild();}}",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-05 10:49:34,7
13243271,[Python] Support ExtentionType on conversion to numpy/pandas,"Currently converting a Table of RecordBatch with an ExtensionType array to pandas gives:

{code}
ArrowNotImplementedError: No known equivalent Pandas block for Arrow data of type extension<arrow.py_extension_type> is known.
{code}

And similarly converting the array itself to a python object (to_pandas or to_pylist) gives an ArrowNotImplementedError or a ""KeyError: 28""

Initial support could be to fall back to the storage type.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-07-04 21:51:52,5
13243262,[Python] linking 3rd party cython modules against pyarrow fails since 0.14.0,"Compiling cython modules that link to the pyarrow library, using the recommended approach for getting the appropriate include and link flags has stopped working for PyArrow 0.14.0.



A minimal test case is included in the attachments that demonstrates the problem.",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-04 20:18:08,2
13243258,[Python] Add support for Duration type,"Add support for the Duration type (added in C++: ARROW-835, ARROW-5261)

- add DurationType and DurationArray wrappers
- add inference support for datetime.timedelta / np.timedelta64",pull-request-available,['Python'],ARROW,Improvement,Major,2019-07-04 20:07:50,5
13243256,[Python] Expose compare kernels on Array class,"Expose the compare kernel for comparing with scalar or array (ARROW-3087, ARROW-4990) on the python Array class.

This can implement the {{\_\_eq\_\_}} et al dunder methods on the Array class.
",pull-request-available,['Python'],ARROW,Improvement,Major,2019-07-04 20:02:11,5
13243255,[Python] Expose boolean filter kernel on Array,"Expose the filter kernel (https://issues.apache.org/jira/browse/ARROW-1558) on the python Array class.

Could be done as {{.filter(mask)}} method and/or in {{\_\_getitem\_\_}}.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-07-04 19:56:29,5
13243233,[C++] Compilation of reference benchmarks fails,"{code}
../src/arrow/util/compression-benchmark.cc: In function 'void arrow::util::StreamingDecompression(arrow::Compression::type, const std::vector<unsigned char>&, benchmark::State&)':
../src/arrow/util/compression-benchmark.cc:172:5: error: 'ARROW_CHECK' was not declared in this scope
     ARROW_CHECK(decompressed_size == static_cast<int64_t>(data.size()));
     ^~~~~~~~~~~
../src/arrow/util/compression-benchmark.cc:172:5: note: suggested alternative: 'ARROW_CONCAT'
     ARROW_CHECK(decompressed_size == static_cast<int64_t>(data.size()));
     ^~~~~~~~~~~
     ARROW_CONCAT
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-04 15:19:58,2
13243228,[CI][R] R appveyor job is broken after release,"I forgot to add ci/PKGBUILD to the release script that bumps all of the versions, so the pkgver it has is out of sync with what is in r/DESCRIPTION after the release.

To fix, either update thepkgverin ci/PKGBUILD and add code to the release script and tests to bump its version, or remove the pkgver field entirely and just read it fromr/DESCRIPTION in the pkgver() function, if makepkg allows that. Unclear if onesolution is clearly better than the other.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2019-07-04 14:39:59,4
13243204,[C++] Compiler warnings on mingw-w64,"In mingw64 we see the following warnings:

{code}
[ 54%] Building CXX object src/arrow/CMakeFiles/arrow_static.dir/util/io-util.cc.obj
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/decimal.cc: In static member function 'static arrow::Status arrow::Decimal128::FromString(const string_view&, arrow::Decimal128*, int32_t*, int32_t*)':
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/decimal.cc:313:35: warning: 'dec.arrow::{anonymous}::DecimalComponents::exponent' may be used uninitialized in this function [-Wmaybe-uninitialized]
       *scale = -adjusted_exponent + len - 1;
                ~~~~~~~~~~~~~~~~~~~^~~~
{code} 

{code}
[ 56%] Building CXX object src/arrow/CMakeFiles/arrow_static.dir/util/string_builder.cc.obj
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/io-util.cc: In static member function 'static arrow::Status arrow::internal::TemporaryDir::Make(const string&, std::unique_ptr<arrow::internal::TemporaryDir>*)':
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/io-util.cc:897:3: warning: 'created' may be used uninitialized in this function [-Wmaybe-uninitialized]
   if (!created) {
   ^~
{code}

And on mingw32 we also see these:

{code}
In file included from C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/io/file.cc:25:
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/io/mman.h: In function 'void* mmap(void*, size_t, int, int, int, off_t)':
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/io/mman.h:94:62: warning: right shift count >= width of type [-Wshift-count-overflow]
   const DWORD dwMaxSizeHigh = static_cast<DWORD>((maxSize >> 32) & 0xFFFFFFFFL);
                                                              ^~
{code}

{code}
 54%] Building CXX object src/arrow/CMakeFiles/arrow_static.dir/util/logging.cc.obj
In file included from C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/io-util.cc:63:
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/io/mman.h: In function 'void* mmap(void*, size_t, int, int, int, off_t)':
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/io/mman.h:94:62: warning: right shift count >= width of type [-Wshift-count-overflow]
   const DWORD dwMaxSizeHigh = static_cast<DWORD>((maxSize >> 32) & 0xFFFFFFFFL);
                                                              ^~
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/io-util.cc: In function 'arrow::Status arrow::internal::MemoryMapRemap(void*, size_t, size_t, int, void**)':
C:/msys64/home/mingw-packages/mingw-w64-arrow/src/arrow/cpp/src/arrow/util/io-util.cc:568:55: warning: right shift count >= width of type [-Wshift-count-overflow]
   LONG new_size_high = static_cast<LONG>((new_size >> 32) & 0xFFFFFFFFL);
                                 
{code}",pull-request-available,['C++'],ARROW,Bug,Minor,2019-07-04 12:17:59,2
13243197,[C++] SO versioning schema after release 1.0.0,"Described by [~kou] on the mailing list:

If we may break ABI compatibility each minor version up
release (""Y"" is increased in ""X.Y.Z""), we should include
minor version into SO major version (100, 101 and 102 in the
following examples):

  * 1.0.0 -> libarrow.100.0.0
  * 1.1.0 -> libarrow.101.0.0
  * 1.2.0 -> libarrow.102.0.0

If we don't break ABI compatibility each minor version up
release, we just use the same SO major version (100 in the
following examples) in 1.0.0:

  * 1.0.0 -> libarrow.100.0.0
  * 1.1.0 -> libarrow.100.1.0
  * 1.2.0 -> libarrow.100.2.0",pull-request-available,['C++'],ARROW,Bug,Major,2019-07-04 11:50:51,3
13243171,[Java] Support comparison & sort for more numeric types,"Currently, we only support comparison & sort for 32-bit integers, in this issue, we provide support for more numeric data types:
* byte
* short
* long
* float
* double",pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-04 10:03:36,7
13243155,[Java] Improve the readability and performance of BitVectorHelper#getNullCount,"Improve the implementation by:
1. Count the number of 1 bits by long or int, instead of by byte
2. If the number of value count is a multiple of 8, there is no need to process the last byte separately. This makes the code clearer. ",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-04 08:36:58,7
13243138,[Java] Revise the semantic of lastSet in ListVector,"The {{lastSet}} member in ListVector seems misleading. According to the name, it should refers to the last index that is actually set. However, from the context of the code, it actually means the next index that will be set.

We fix this problem, and make it consistent with the {{lastSet}} in {{BaseVariableWidthVector}}.",behavior-changes pull-request-available,['Java'],ARROW,Improvement,Trivial,2019-07-04 07:33:34,7
13242999,[C++][Flight][OSX] Building 3rdparty grpc cannot find OpenSSL,"Without system grpc installed compiling grpc cannot find openssl:

{code}
Could NOT find OpenSSL, try to set the path to OpenSSL root folder in the
  system variable OPENSSL_ROOT_DIR (missing: OPENSSL_INCLUDE_DIR)
Call Stack (most recent call first):
  /usr/local/Cellar/cmake/3.14.5/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE)
  /usr/local/Cellar/cmake/3.14.5/share/cmake/Modules/FindOpenSSL.cmake:413 (find_package_handle_standard_args)
  cmake/ssl.cmake:45 (find_package)
  CMakeLists.txt:141 (include)
{code}

Passing {{ARROW_CMAKE_OPTION=""-DOPENSSL_ROOT_DIR=$(brew --prefix openssl)""}} doesn't seem to get forwarded to build_grpc
{code}
'/usr/local/Cellar/cmake/3.14.5/bin/cmake' '-DCMAKE_BUILD_TYPE=RELEASE' '-DCMAKE_PREFIX_PATH=';/usr/local;/usr/local;/var/folders/cz/jrwncy5s5cb612sgwscd0z8h0000gn/T/arrow-0.14.0.XXXXX.I2yQzNGd/apache-arrow-0.14.0/cpp/thirdparty/cares_
ep-install;'' '-DgRPC_CARES_PROVIDER=package' '-DgRPC_GFLAGS_PROVIDER=package' '-DgRPC_PROTOBUF_PROVIDER=package' '-DgRPC_SSL_PROVIDER=package' '-DgRPC_ZLIB_PROVIDER=package' '-DCMAKE_CXX_FLAGS= -Qunused-arguments -fcolor-diagnostics -O3
-DNDEBUG -O3 -DNDEBUG -fPIC' '-DCMAKE_C_FLAGS= -Qunused-arguments -O3 -DNDEBUG -O3 -DNDEBUG -fPIC' '-DCMAKE_INSTALL_PREFIX=/var/folders/cz/jrwncy5s5cb612sgwscd0z8h0000gn/T/arrow-0.14.0.XXXXX.I2yQzNGd/apache-arrow-0.14.0/cpp/thirdparty/grp
c_ep-install' '-DCMAKE_INSTALL_LIBDIR=lib' '-DProtobuf_PROTOC_LIBRARY=/usr/local/lib/libprotoc.dylib' '-DBUILD_SHARED_LIBS=OFF' '-GUnix Makefiles' '/var/folders/cz/jrwncy5s5cb612sgwscd0z8h0000gn/T/arrow-0.14.0.XXXXX.I2yQzNGd/apache-arrow-
0.14.0/cpp/build/grpc_ep-prefix/src/grpc_ep'
{code}

Installing grpc with brew helps though.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2019-07-03 13:22:39,1
13242996,[Java][OSX] Flight tests are failing: address already in use,"{code}
Jul 03, 2019 3:09:45 PM io.grpc.netty.NettyServerHandler onStreamError
WARNING: Stream Error
io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 3
        at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:129)
        at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:531)
        at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:183)
        at io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48)
        at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:421)
        at io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251)
        at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160)
        at io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
        at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:118)
        at io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:390)
        at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:450)
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:646)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:581)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)

Jul 03, 2019 3:09:46 PM io.grpc.netty.NettyServerHandler onStreamError
WARNING: Stream Error
io.netty.handler.codec.http2.Http2Exception$StreamException: Received DATA frame for an unknown stream 3
        at io.netty.handler.codec.http2.Http2Exception.streamError(Http2Exception.java:129)
        at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.shouldIgnoreHeadersOrDataFrame(DefaultHttp2ConnectionDecoder.java:531)
        at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder$FrameReadListener.onDataRead(DefaultHttp2ConnectionDecoder.java:183)
        at io.netty.handler.codec.http2.Http2InboundFrameLogger$1.onDataRead(Http2InboundFrameLogger.java:48)
        at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readDataFrame(DefaultHttp2FrameReader.java:421)
        at io.netty.handler.codec.http2.DefaultHttp2FrameReader.processPayloadState(DefaultHttp2FrameReader.java:251)
        at io.netty.handler.codec.http2.DefaultHttp2FrameReader.readFrame(DefaultHttp2FrameReader.java:160)
        at io.netty.handler.codec.http2.Http2InboundFrameLogger.readFrame(Http2InboundFrameLogger.java:41)
        at io.netty.handler.codec.http2.DefaultHttp2ConnectionDecoder.decodeFrame(DefaultHttp2ConnectionDecoder.java:118)
        at io.netty.handler.codec.http2.Http2ConnectionHandler$FrameDecoder.decode(Http2ConnectionHandler.java:390)
        at io.netty.handler.codec.http2.Http2ConnectionHandler.decode(Http2ConnectionHandler.java:450)
        at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:489)
        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:428)
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:646)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:581)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:460)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)

[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.324 s - in org.apache.arrow.flight.auth.TestAuth
[INFO] Running org.apache.arrow.flight.TestFlightClient
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 s - in org.apache.arrow.flight.TestFlightClient
[INFO] Running org.apache.arrow.flight.example.TestExampleServer
[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.001 s - in org.apache.arrow.flight.example.TestExampleServer
[INFO] Running org.apache.arrow.flight.TestServerOptions
[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.044 s <<< FAILURE! - in org.apache.arrow.flight.TestServerOptions
[ERROR] domainSocket(org.apache.arrow.flight.TestServerOptions)  Time elapsed: 0.044 s  <<< ERROR!
java.io.IOException: Failed to bind
        at org.apache.arrow.flight.TestServerOptions.domainSocket(TestServerOptions.java:46)
Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
{code}",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-07-03 13:11:31,0
13242992,[Java] Support Dictionary Encoding for binary type,"Now is not implemented because byte array is not supported to be HashMap key.

One possible way is that wrap them with something to implement equals and hashcode.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-03 12:39:04,16
13242911,[Java] Apply new hash map in DictionaryEncoder,"Follow-up of [ARROW-5814|https://issues.apache.org/jira/browse/ARROW-5814].

Apply new hash map in DictionaryEncoder to make it work.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-07-03 06:22:00,16
13242891,[C++] Factor out status copying code from cast.cc,FUNC_RETURN_NOT_OK reconstructs a status object including line and status information.  This should be replaced with suitable methods from Status instead.   The macro is probably still useful since it returns by setting Status on the context instead of directly returning from a function,pull-request-available,['C++'],ARROW,Improvement,Minor,2019-07-03 03:52:57,2
13242889,[Java] Support search operations for vector data,Support searching for a particular data item in a sorted/unsorted vector.,pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-03 03:41:42,7
13242887,[Release] Migrate and improve binary release verification script,"Requirements:
* parallel downloads
* resumable operation on error downloading
* python with no external dependencies",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-07-03 03:35:56,14
13242870,[C++] Add Protocol Buffers version check,"If we use old Protocol Buffers, bundled gRPC reports build error:

https://lists.apache.org/thread.html/10f8c4d2372638c57c3a956180b2fa3bbd036a27d79eb2eb7b9ffe76@%3Cdev.arrow.apache.org%3E

{noformat}
  /tmp/arrow-0.14.0.dJDu3/apache-arrow-0.14.0/cpp/build/grpc_ep-prefix/src/grpc_ep/src/compiler/php_generator.cc:21:10:
fatal error: google/protobuf/compiler/php/php_generator.h: No such
file or directory
   #include <google/protobuf/compiler/php/php_generator.h>
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  compilation terminated.
  make[5]: *** [CMakeFiles/grpc_plugin_support.dir/src/compiler/php_generator.cc.o]
Error 1
  make[5]: *** Waiting for unfinished jobs....
  /tmp/arrow-0.14.0.dJDu3/apache-arrow-0.14.0/cpp/build/grpc_ep-prefix/src/grpc_ep/src/compiler/ruby_generator.cc:
In function grpc::string grpc_ruby_generator::GetServices(const
FileDescriptor*):
  /tmp/arrow-0.14.0.dJDu3/apache-arrow-0.14.0/cpp/build/grpc_ep-prefix/src/grpc_ep/src/compiler/ruby_generator.cc:165:25:
error: const class google::protobuf::FileOptions has no member named
has_ruby_package; did you mean has_java_package?
       if (file->options().has_ruby_package()) {
                           ^~~~~~~~~~~~~~~~
                           has_java_package
  /tmp/arrow-0.14.0.dJDu3/apache-arrow-0.14.0/cpp/build/grpc_ep-prefix/src/grpc_ep/src/compiler/ruby_generator.cc:166:38:
error: const class google::protobuf::FileOptions has no member named
ruby_package; did you mean java_package?
         package_name = file->options().ruby_package();
                                        ^~~~~~~~~~~~
                                        java_package
  make[5]: *** [CMakeFiles/grpc_plugin_support.dir/src/compiler/ruby_generator.cc.o]
Error 1
  make[4]: *** [CMakeFiles/grpc_plugin_support.dir/all] Error 2
  make[4]: *** Waiting for unfinished jobs....
  make[3]: *** [all] Error 2
{noformat}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-03 01:24:53,1
13242869,[C++] Require c-ares CMake config,Because gRPC requires c-ares' CMake config.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-07-03 01:05:06,1
13242866,[Website] Blog post for 0.14.0 release announcement,As with prior releases,pull-request-available,['Website'],ARROW,New Feature,Major,2019-07-03 00:30:30,14
13242683,[Java] Support compact fixed-width vectors,"In shuffle stage of some applications, FixedWitdhVectors may have very little non-null data.
In this case, directly serialize vectors is not a good choice, generally we can compact the vector make it only holding non-null value and create a BitVector to trace the indices for non-null values so that it could be deserialized properly.",pull-request-available,[],ARROW,New Feature,Minor,2019-07-02 08:36:28,16
13242595,[Release] Remove undefined variable check from verify script,"External shell scripts may refer unbound variable:

{noformat}
/tmp/arrow-0.14.0.yum2X/apache-arrow-0.14.0/test-miniconda/etc/profile.d/conda.sh:
line 55: PS1: unbound variable
{noformat}


https://lists.apache.org/thread.html/ebe8551eed2353b248b19084810ff454942b55470b9cf5837aa6cf79@%3Cdev.arrow.apache.org%3E",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-07-01 20:04:20,1
13242563,[Python] Use pytest marks for Flight test to avoid silently skipping unit tests due to import failures,"The approach used to determine whether or not Flight has been built will fail silently if the extension is built but there is an ImportError caused by linking or other issues 

https://github.com/apache/arrow/blob/master/python/pyarrow/tests/test_flight.py#L35

We should use the same ""auto"" approach as other optional components (see https://github.com/apache/arrow/blob/master/python/pyarrow/tests/conftest.py#L40) with the option for forced opt-in (so that ImportError does not cause silently skipping) so that {{--flight}} will force the tests to run if we expect them to work
",pull-request-available,['Python'],ARROW,Bug,Major,2019-07-01 17:51:51,5
13242551,[Release] Parallel curl does not work reliably in verify-release-candidate-sh,Script can exit early without waiting for curl processes to finish,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-07-01 16:39:38,14
13242495,[Java] Support swap functionality for fixed-width vectors,Support swapping data elements for fixed-width vectors.,pull-request-available,['Java'],ARROW,New Feature,Major,2019-07-01 11:49:15,7
13242459,"[Java] Implement a <Object, int> HashMap for DictionaryEncoder","As a follow-up of [ARROW-5726|https://issues.apache.org/jira/browse/ARROW-5726]. Implement a Map<Object, int> for DictionaryEncoder to reduce boxing/unboxing operations.

Benchmark:
DictionaryEncodeHashMapBenchmarks.testHashMap: avgt  5  31151.345  1661.878 ns/op
DictionaryEncodeHashMapBenchmarks.testDictionaryEncodeHashMap: avgt  5  15549.902  771.647 ns/op",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-01 09:37:28,16
13242411,[Java] Refactor method name and param type in BaseIntVector,"Change to void _setWithPossibleTruncate(int index, long value);_ for better generality.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-07-01 04:16:22,16
13242384,[Rust] Dockerize (add to docker-compose) Rust Travis CI build,https://github.com/apache/arrow/blob/master/.travis.yml#L306,pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Improvement,Major,2019-06-30 19:02:41,10
13242378,[C++] Dockerize C++ with clang 7 Travis CI unit test logic,Convert to docker-compose (or use one of the current Dockerfiles under cpp/),pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-30 18:54:51,13
13242377,"[CI] Dockerize ""lint"" Travis CI job",Run via docker-compose; also enables contributors to lint locally,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-06-30 18:53:26,13
13242325,[Python] pyarrow.csv.read_csv hangs + eats all RAM,"I have quite a sparse dataset in CSV format. A wide table that has several rows but many (32k) columns. Total size ~540K.

When I read the dataset using `pyarrow.csv.read_csv` it hangs, gradually eats all memory and gets killed.

More details on the conditions further. Script to run and all mentioned files are under attachments.

1) `sample_32769_cols.csv` is the dataset that suffers the problem.

2) `sample_32768_cols.csv` is the dataset that DOES NOT suffer and is read in under 400ms on my machine. It's the same dataset without ONE last column. That last column is no different than others and has empty values.

The reason of why exactly this column makes difference between proper execution and hanging failure which looks like some memory leak - no idea.

I have created flame graph for the case (1) to support this issue resolution (`graph.svg`).

",pull-request-available,['Python'],ARROW,Bug,Major,2019-06-29 23:29:07,15
13242309,[Python] Passing zero-dim numpy array to pa.array causes segfault,"```
import pyarrow as pa
import numpy as np

zerod = np.array(0)
result = pa.array(zerod) # <-- segfault
```",pull-request-available,['Python'],ARROW,Bug,Major,2019-06-29 16:41:50,5
13242289,[C++] Small Warning/Linkage cleanups,"* Cleanup some unused warning generated on MacOS, when asserts are turned off
* Change linkage of ubsan.h nullptr replacement from from internal to external (actually build ubsan.cc).  An alternative would be to delete ubsan.cc",pull-request-available,['C++'],ARROW,Improvement,Trivial,2019-06-29 13:12:06,15
13242226,[Archery] Ensure benchmark clone accepts remotes in revision,Found that ursabot would always compare the PR tip commit with itself via https://github.com/apache/arrow/pull/4739#issuecomment-506819250 . This is due to buildbot github behavior of using a git-reset --hard local that changes the `master` rev to this new state. ,pull-request-available,['Developer Tools'],ARROW,Bug,Minor,2019-06-28 19:22:25,13
13242133,[Java] Extract the logic for vector data copying to the super classes,"Currently, each vector has its own {{copyFrom}}method. The implementations for fixed-width vectors are similar, whereas the implementations for the variable-width vectors are similar.

This issue extract such implementations to the base classes, with the following benefits:

1.Less codemakes iteasier to maintain

2. Move the method to the base class makes the method more convenient to use.",pull-request-available,['Java'],ARROW,Improvement,Major,2019-06-28 09:55:18,7
13242106,[C++] BasicDecimal128 is a small object it doesn't always make sense to pass by const ref,"For the builder use case we saw 10% improvement in performance by not using const ref.  
Add benchmarks and look at where it makes sense to avoid passing by ref.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-28 07:41:31,15
13242093,[C++] StructArray : cached boxed fields not thread-safe,"The lazy initialization isn't thread-safe (it relies neither on a lock nor on an atomic).

Perhaps we need a more general ""cached property"" facility to handle these cases.",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-28 06:51:58,2
13242087,[R] Clean up documentation before release,The R package documentation is lackluster. Try to polish the docs for at least the key end-user functions and add enough examples so that CRAN won't use that as an excuse to reject.,pull-request-available,['Documentation'],ARROW,Improvement,Minor,2019-06-28 05:42:01,4
13242086,[GLib][Plasma][CUDA] Plasma::Client#refer_object test is failed,"{noformat}
/home/kou/work/cpp/arrow.kou/c_glib/test/plasma/test-plasma-client.rb:75:in `block (2 levels) in <class:TestPlasmaClient>'
/tmp/local/lib/ruby/gems/2.7.0/gems/gobject-introspection-3.3.6/lib/gobject-introspection/loader.rb:533:in `block in define_method'
/tmp/local/lib/ruby/gems/2.7.0/gems/gobject-introspection-3.3.6/lib/gobject-introspection/loader.rb:616:in `invoke'
/tmp/local/lib/ruby/gems/2.7.0/gems/gobject-introspection-3.3.6/lib/gobject-introspection/loader.rb:616:in `invoke'
Error: test: options: GPU device(TestPlasmaClient::#create):
  Arrow::Error::Io: [plasma][client][refer-object]: IOError: Cuda Driver API call in ../src/arrow/gpu/cuda_context.cc at line 156 failed with code 208: cuIpcOpenMemHandle(&data, *handle, CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS)
  In ../src/arrow/gpu/cuda_context.cc, line 341, code: impl_->OpenIpcBuffer(ipc_handle, &data)
  In ../src/plasma/client.cc, line 586, code: context->OpenIpcBuffer(*object->ipc_handle, &obj_handle->ptr)
{noformat}
",pull-request-available,['GLib'],ARROW,Bug,Major,2019-06-28 05:24:52,1
13242079,[Python] Docker python-nopandas job fails,"{code}
=========================================== ERRORS ============================================
_____ ERROR collecting opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_array.py ______
ImportError while importing test module '/opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_array.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_array.py:35: in <module>
    import pyarrow.tests.strategies as past
opt/conda/lib/python3.7/site-packages/pyarrow/tests/strategies.py:18: in <module>
    import pytz
E   ModuleNotFoundError: No module named 'pytz'
_ ERROR collecting opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_convert_builtin.py _
ImportError while importing test module '/opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_convert_builtin.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_convert_builtin.py:31: in <module>
    import pytz
E   ModuleNotFoundError: No module named 'pytz'
_____ ERROR collecting opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_pandas.py _____
ImportError while importing test module '/opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_pandas.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_pandas.py:29: in <module>
    import hypothesis.extra.pytz as tzst
opt/conda/lib/python3.7/site-packages/hypothesis/extra/pytz.py:34: in <module>
    import pytz
E   ModuleNotFoundError: No module named 'pytz'
___ ERROR collecting opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_strategies.py ___
ImportError while importing test module '/opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_strategies.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_strategies.py:21: in <module>
    import pyarrow.tests.strategies as past
opt/conda/lib/python3.7/site-packages/pyarrow/tests/strategies.py:18: in <module>
    import pytz
E   ModuleNotFoundError: No module named 'pytz'
_____ ERROR collecting opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_types.py ______
ImportError while importing test module '/opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_types.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
opt/conda/lib/python3.7/site-packages/pyarrow/tests/test_types.py:28: in <module>
    import pyarrow.tests.strategies as past
opt/conda/lib/python3.7/site-packages/pyarrow/tests/strategies.py:18: in <module>
    import pytz
E   ModuleNotFoundError: No module named 'pytz'
{code}",pull-request-available,['Python'],ARROW,Bug,Critical,2019-06-28 03:42:46,14
13242062,[Java] org.apache.arrow.flight.TestTls is failed via dev/release/00-prepare.sh,"Details:

{noformat}
[INFO] [INFO] Running org.apache.arrow.flight.TestTls
[INFO] [ERROR] Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.005 s <<< FAILURE! - in org.apache.arrow.flight.TestTls
[INFO] [ERROR] connectTls(org.apache.arrow.flight.TestTls)  Time elapsed: 0.004 s  <<< ERROR!
[INFO] java.lang.RuntimeException: java.io.FileNotFoundException: /home/kou/work/cpp/arrow.pravindra/java/flight/../../testing/data/flight/cert0.pem (No such file or directory)
[INFO] 	at org.apache.arrow.flight.TestTls.lambda$test$3(TestTls.java:105)
[INFO] 	at org.apache.arrow.flight.TestTls.test(TestTls.java:98)
[INFO] 	at org.apache.arrow.flight.TestTls.connectTls(TestTls.java:44)
[INFO] Caused by: java.io.FileNotFoundException: /home/kou/work/cpp/arrow.pravindra/java/flight/../../testing/data/flight/cert0.pem (No such file or directory)
[INFO] 	at org.apache.arrow.flight.TestTls.lambda$test$3(TestTls.java:102)
[INFO] 	at org.apache.arrow.flight.TestTls.test(TestTls.java:98)
[INFO] 	at org.apache.arrow.flight.TestTls.connectTls(TestTls.java:44)
[INFO] 
[INFO] [ERROR] rejectInvalidCert(org.apache.arrow.flight.TestTls)  Time elapsed: 0 s  <<< ERROR!
[INFO] java.lang.Exception: Unexpected exception, expected<io.grpc.StatusRuntimeException> but was<java.lang.RuntimeException>
[INFO] 	at org.apache.arrow.flight.TestTls.lambda$test$3(TestTls.java:105)
[INFO] 	at org.apache.arrow.flight.TestTls.test(TestTls.java:98)
[INFO] 	at org.apache.arrow.flight.TestTls.rejectInvalidCert(TestTls.java:62)
[INFO] Caused by: java.io.FileNotFoundException: /home/kou/work/cpp/arrow.pravindra/java/flight/../../testing/data/flight/cert0.pem (No such file or directory)
[INFO] 	at org.apache.arrow.flight.TestTls.lambda$test$3(TestTls.java:102)
[INFO] 	at org.apache.arrow.flight.TestTls.test(TestTls.java:98)
[INFO] 	at org.apache.arrow.flight.TestTls.rejectInvalidCert(TestTls.java:62)
[INFO] 
[INFO] [ERROR] rejectHostname(org.apache.arrow.flight.TestTls)  Time elapsed: 0.001 s  <<< ERROR!
[INFO] java.lang.Exception: Unexpected exception, expected<io.grpc.StatusRuntimeException> but was<java.lang.RuntimeException>
[INFO] 	at org.apache.arrow.flight.TestTls.lambda$test$3(TestTls.java:105)
[INFO] 	at org.apache.arrow.flight.TestTls.test(TestTls.java:98)
[INFO] 	at org.apache.arrow.flight.TestTls.rejectHostname(TestTls.java:78)
[INFO] Caused by: java.io.FileNotFoundException: /home/kou/work/cpp/arrow.pravindra/java/flight/../../testing/data/flight/cert0.pem (No such file or directory)
[INFO] 	at org.apache.arrow.flight.TestTls.lambda$test$3(TestTls.java:102)
[INFO] 	at org.apache.arrow.flight.TestTls.test(TestTls.java:98)
[INFO] 	at org.apache.arrow.flight.TestTls.rejectHostname(TestTls.java:78)
[INFO] 
[INFO] [INFO] Running org.apache.arrow.flight.TestServerOptions
[INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.114 s - in org.apache.arrow.flight.TestServerOptions
[INFO] [INFO] 
[INFO] [INFO] Results:
[INFO] [INFO] 
[INFO] [ERROR] Errors: 
[INFO] [ERROR]   TestTls.connectTls:44->test:98->lambda$test$3:105 Runtime java.io.FileNotFound...
[INFO] [ERROR]   TestTls.rejectHostname   Unexpected exception, expected<io.grpc.StatusRuntime...
[INFO] [ERROR]   TestTls.rejectInvalidCert   Unexpected exception, expected<io.grpc.StatusRunt...
[INFO] [INFO] 
[INFO] [ERROR] Tests run: 27, Failures: 0, Errors: 3, Skipped: 10
{noformat}

I'm not sure whether this is my environment problem or not.",pull-request-available,['Java'],ARROW,Test,Blocker,2019-06-28 00:45:00,1
13242043,[C++] TestDictionary.Validate test is crashed with release build,"Here is a backtrace:

{noformat}
(gdb) bt
#0  0x00007ffff76b3bba in arrow::DictionaryArray::DictionaryArray(std::shared_ptr<arrow::DataType> const&, std::shared_ptr<arrow::Array> const&, std::shared_ptr<arrow::Array> const&) ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow.so.14
#1  0x00005555557ba6c3 in arrow::TestDictionary_Validate_Test::TestBody() ()
#2  0x00007ffff7fa725a in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#3  0x00007ffff7f9db5a in testing::Test::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#4  0x00007ffff7f9dca8 in testing::TestInfo::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#5  0x00007ffff7f9dd85 in testing::TestCase::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#6  0x00007ffff7f9e29c in testing::internal::UnitTestImpl::RunAllTests() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#7  0x00007ffff7fa776a in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#8  0x00007ffff7f9e3cc in testing::UnitTest::Run() ()
   from /home/kou/work/cpp/arrow.kou/cpp/build/release/libarrow_testing.so.14
#9  0x000055555568eb90 in main ()
{noformat}

It's not occurred with debug build.

Here are CMake options I used:

{noformat}
rm -rf build
mkdir -p build
cd build
CUDA_TOOLKIT_ROOT=/usr \
  cmake .. \
  -G Ninja \
  -DCMAKE_INSTALL_PREFIX=/tmp/local \
  -DCMAKE_BUILD_TYPE=release \
  -DARROW_PYTHON=on \
  -DPythonInterp_FIND_VERSION=on \
  -DPythonInterp_FIND_VERSION_MAJOR=3 \
  -DARROW_PLASMA=on \
  -DARROW_CUDA=on \
  -DARROW_EXTRA_ERROR_CONTEXT=on \
  -DARROW_ORC=on \
  -DARROW_PARQUET=on \
  -DARROW_GANDIVA=on \
  -DARROW_BUILD_TESTS=on \
  -DARROW_FLIGHT=on
{noformat}",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-06-27 21:43:13,14
13242002,[C++] Optimize Take implementation,"There is some question of whether these kernels allocate optimally- for example when Filtering or Taking strings it might be more efficient to pass over the filter/indices twice, first to determine how much character storage will be needed then again into allocated memory: https://github.com/apache/arrow/pull/4531#discussion_r297160457

Additionally, these kernels could probably make good use of scatter/gather SIMD instructions.

Furthermore, Filter's bitmap is currently lazily expanded into the indices of elements to be appended to the output array. It would probably be more efficient to expand to indices in batches, then gather using an index batch.",pull-request-available,['C++'],ARROW,New Feature,Major,2019-06-27 17:23:21,14
13241983,[Python] Stop supporting Python 2.7,"By the end of 2019 many scientific Python projects will stop supporting Python 2 altogether:
https://python3statement.org/

We'll certainly support Python 2 in Arrow 1.0 but we could perhaps drop support in 1.1.",pull-request-available,['Python'],ARROW,Task,Major,2019-06-27 16:11:30,2
13241836,[Java] Improve the performance of ArrowBuf#setZero,"The current implementation involves repeated calls of setLong, setInt & setByte. It is more efficient to directlycall the native function.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-06-27 03:07:56,7
13241829,[Packaging][Python] Python macOS wheels have dynamic dependency on libcares,"I'm afraid while[https://github.com/apache/arrow/pull/4685]fixed the macOS wheels for python 3, but the python 2.7 wheel is still broken (with a different error):
{code:java}
ImportError: dlopen(/Users/pcmoritz/anaconda3/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-darwin.so, 2): Library not loaded: /usr/local/opt/c-ares/lib/libcares.2.dylib

 Referenced from: /Users/pcmoritz/anaconda3/lib/python3.6/site-packages/pyarrow/libarrow_python.14.dylib

 Reason: image not found{code}
I tried the same hack as in[https://github.com/apache/arrow/pull/4685]for libcaresbut it doesn't work (removing the .dylib fails one of the earlier build steps). I think the only way to go forward on this is to compile grpc ourselves. My attempt to do this in[https://github.com/apache/arrow/compare/master...pcmoritz:mac-wheels-py2]fails becauseOpenSSL is not found even though I'm specifying theOPENSSL_ROOT_DIR (see[https://travis-ci.org/pcmoritz/crossbow/builds/550603543]). Let me know if you have any ideas how to fix this!",pull-request-available,[],ARROW,Improvement,Blocker,2019-06-27 01:24:50,14
13241823,[Java] Java compilation failures on master,"Two Flight-related Java patches were merged today and we have compilation failures on master now:

https://travis-ci.org/apache/arrow/jobs/551015006#L956",pull-request-available,['Java'],ARROW,Bug,Blocker,2019-06-27 00:50:52,15
13241798,[C++] Better column name and header support in CSV reader,"While working onARROW-5500, I found a number of issues around the CSV parse options {{header_rows}}:
 * If header_rows is 0, [the reader errors|https://github.com/apache/arrow/blob/8b0318a11bba2aa2cf39bff245ff916a3283d372/cpp/src/arrow/csv/reader.cc#L150]
 * It's not possible to supply your own column names, as [this TODO|https://github.com/apache/arrow/blob/8b0318a11bba2aa2cf39bff245ff916a3283d372/cpp/src/arrow/csv/reader.cc#L149]notes.ARROW-4912 allows renaming columns after reading in, which_maybe_ is enough as long as header_rows == 0 doesn't error, butthen you can't naturally specify column types in the convert options because that takes a map of column name to type.
 * If header_rows is > 1, every cell gets turned into a column name, so if header_rows== 2, you get twice the number of column names as columns. This doesn't error, but it leads to unexpected results.

IMO a better interface would be to have a {{skip_rows}} argument to let you ignore a large header, and a {{column_names}} argument that, if provided, gives the column names. If not provided, the first row after {{skip_rows}} is taken to be the column names. If it were also possible for {{column_names}} to take a {{false}} or {{null}} argument, then we could support the case of autogenerating names when none are provided and there's no header row. Alternatively, we could use a boolean {{header}} argument to govern whether the first (non-skipped) row should be interpreted as column names. (For reference, R's [readr|https://github.com/tidyverse/readr/blob/master/R/read_delim.R#L14-L27]takes TRUE/FALSE/array of strings in one arg; the base [read.csv|https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html]uses separate args for header and col.names. Both have a {{skip}} argument.)

I don't think there's value in trying to be clever about multirow headers and converting those to column names; if there's meaningful information in a tall header, let the user parse it themselves.",csv pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-26 21:02:17,2
13241786,[Website] Move website source out of apache/arrow,"Possibly to apache/arrow-site, which already exists for hosting the static built site.",pull-request-available,['Website'],ARROW,Improvement,Minor,2019-06-26 19:45:04,4
13241739,[C++] Do not error in Table::CombineChunks for BinaryArray types that overflow 2GB limit,Discovered during ARROW-5635 code review,pull-request-available,['C++'],ARROW,Bug,Major,2019-06-26 15:51:44,6
13241737,"[C++] Add CMake option to enable ""large memory"" unit tests","We have a number of unit tests that need to exercise code paths where memory in excess of 2-4GB is allocated. Some of these are marked as {{DISABLED_*}} in googletest which seems to be a recipe for bitrot.

I propose instead to have a CMake option that sets a compiler definition to enable these tests at build time, so that they can be run regularly on machines that have adequate RAM (i.e. not public CI services)",pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-26 15:20:48,6
13241736,[CI] Add daily / weekly Valgrind build,"A daily or weekly Valgrind build on the ursa-labs machines would further check sanity of the C++ code base, though with ASAN and UBSAN builds on Travis-CI we're already well covered.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Wish,Minor,2019-06-26 15:19:34,2
13241723,[CI] Fix docker python build,"python docker image will fail to clean the build directory, installing a previous invocation of `docker-compose run python`. This is not affecting CI that drops the `/build` mount, but only local users.",pull-request-available,['Continuous Integration'],ARROW,Bug,Minor,2019-06-26 14:01:38,13
13241722,[Crossbow][Conda] OSX package builds are failing with missing intrinsics,"Failing builds: https://github.com/ursa-labs/crossbow/branches/all?utf8=%E2%9C%93&query=build-653

Conda 4.7 has introduced a new format https://docs.conda.io/projects/conda/en/latest/release-notes.html#id14
Probably broken packages cause the error, a temporary solution is to pin `conda` to 4.6. 
Upstream issue https://github.com/conda/conda/issues/8825

{code}
In file included from $BUILD_PREFIX/bin/../lib/clang/4.0.1/include/nmmintrin.h:29:
In file included from $BUILD_PREFIX/bin/../lib/clang/4.0.1/include/smmintrin.h:27:
In file included from $BUILD_PREFIX/bin/../lib/clang/4.0.1/include/tmmintrin.h:27:
$BUILD_PREFIX/bin/../lib/clang/4.0.1/include/pmmintrin.h:45:19: error: unknown type name '__m128i'
static __inline__ __m128i __DEFAULT_FN_ATTRS
                  ^
$BUILD_PREFIX/bin/../lib/clang/4.0.1/include/pmmintrin.h:45:27: error: expected unqualified-id
static __inline__ __m128i __DEFAULT_FN_ATTRS
                          ^
$BUILD_PREFIX/bin/../lib/clang/4.0.1/include/pmmintrin.h:31:3: note: expanded from macro '__DEFAULT_FN_ATTRS'
  __attribute__((__always_inline__, __nodebug__, __target__(""sse3"")))
  ^
$BUILD_PREFIX/bin/../lib/clang/4.0.1/include/pmmintrin.h:64:19: error: unknown type name '__m128'
static __inline__ __m128 __DEFAULT_FN_ATTRS
                  ^
$BUILD_PREFIX/bin/../lib/clang/4.0.1/include/pmmintrin.h:64:26: error: expected unqualified-id
static __inline__ __m128 __DEFAULT_FN_ATTRS
{code}",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-06-26 14:01:00,3
13241616,[C++] Appveyor builds failing persistently in thrift_ep build,"See

{code}
72/541] Performing configure step for 'thrift_ep'
FAILED: thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-configure 
cmd.exe /C ""cd /D C:\projects\arrow\cpp\build\thrift_ep-prefix\src\thrift_ep-build && ""C:\Program Files (x86)\CMake\bin\cmake.exe"" -DFLEX_EXECUTABLE=C:/projects/arrow/cpp/build/winflexbison_ep/src/winflexbison_ep-install/win_flex.exe -DBISON_EXECUTABLE=C:/projects/arrow/cpp/build/winflexbison_ep/src/winflexbison_ep-install/win_bison.exe -DZLIB_INCLUDE_DIR= -DWITH_SHARED_LIB=OFF -DWITH_PLUGIN=OFF -DZLIB_LIBRARY= ""-DCMAKE_C_COMPILER=C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe"" -DCMAKE_CXX_COMPILER=C:/Miniconda36-x64/Scripts/clcache.exe -DCMAKE_BUILD_TYPE=RELEASE ""-DCMAKE_C_FLAGS=/DWIN32 /D_WINDOWS /W3  /MD /O2 /Ob2 /DNDEBUG"" ""-DCMAKE_C_FLAGS_RELEASE=/DWIN32 /D_WINDOWS /W3  /MD /O2 /Ob2 /DNDEBUG"" ""-DCMAKE_CXX_FLAGS=/DWIN32 /D_WINDOWS  /GR /EHsc /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING  /MD /Od /UNDEBUG"" ""-DCMAKE_CXX_FLAGS_RELEASE=/DWIN32 /D_WINDOWS  /GR /EHsc /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING  /MD /Od /UNDEBUG"" -DCMAKE_INSTALL_PREFIX=C:/projects/arrow/cpp/build/thrift_ep/src/thrift_ep-install -DCMAKE_INSTALL_RPATH=C:/projects/arrow/cpp/build/thrift_ep/src/thrift_ep-install/lib -DBUILD_SHARED_LIBS=OFF -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF -DBUILD_TUTORIALS=OFF -DWITH_QT4=OFF -DWITH_C_GLIB=OFF -DWITH_JAVA=OFF -DWITH_PYTHON=OFF -DWITH_HASKELL=OFF -DWITH_CPP=ON -DWITH_STATIC_LIB=ON -DWITH_LIBEVENT=OFF -DWITH_MT=OFF -GNinja C:/projects/arrow/cpp/build/thrift_ep-prefix/src/thrift_ep && ""C:\Program Files (x86)\CMake\bin\cmake.exe"" -E touch C:/projects/arrow/cpp/build/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-configure""
-- The C compiler identification is MSVC 19.16.27030.1
-- The CXX compiler identification is MSVC 19.16.27030.1
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: C:/Miniconda36-x64/Scripts/clcache.exe
-- Check for working CXX compiler: C:/Miniconda36-x64/Scripts/clcache.exe -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Parsed Thrift package version: 0.12.0
-- Parsed Thrift version: 0.12.0 (0.2.0)
-- Setting C++11 as the default language level.
-- To specify a different C++ language level, set CMAKE_CXX_STANDARD
CMake Warning (dev) at build/cmake/DefineOptions.cmake:63 (find_package):
  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.
  Run ""cmake --help-policy CMP0074"" for policy details.  Use the cmake_policy
  command to set the policy and suppress this warning.
  Environment variable Boost_ROOT is set to:
    C:\Miniconda36-x64\envs\arrow\Library
  For compatibility, CMake is ignoring the variable.
Call Stack (most recent call first):
  CMakeLists.txt:52 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.
-- Found Boost 1.70.0 at C:/Miniconda36-x64/envs/arrow/Library/lib/cmake/Boost-1.70.0
--   Requested configuration: QUIET
-- Found boost_headers 1.70.0 at C:/Miniconda36-x64/envs/arrow/Library/lib/cmake/boost_headers-1.70.0
-- Boost 1.53 found.
-- libevent NOT found.
-- Could NOT find RUN_HASKELL (missing: RUN_HASKELL) 
-- Could NOT find CABAL (missing: CABAL) 
-- Looking for arpa/inet.h
-- Looking for arpa/inet.h - not found
-- Looking for fcntl.h
-- Looking for fcntl.h - found
-- Looking for getopt.h
-- Looking for getopt.h - not found
-- Looking for inttypes.h
-- Looking for inttypes.h - found
-- Looking for netdb.h
-- Looking for netdb.h - not found
-- Looking for netinet/in.h
-- Looking for netinet/in.h - not found
-- Looking for signal.h
-- Looking for signal.h - found
-- Looking for stdint.h
-- Looking for stdint.h - found
-- Looking for unistd.h
-- Looking for unistd.h - not found
-- Looking for pthread.h
-- Looking for pthread.h - not found
-- Looking for sys/ioctl.h
-- Looking for sys/ioctl.h - not found
-- Looking for sys/param.h
-- Looking for sys/param.h - not found
-- Looking for sys/resource.h
-- Looking for sys/resource.h - not found
-- Looking for sys/socket.h
-- Looking for sys/socket.h - not found
-- Looking for sys/stat.h
-- Looking for sys/stat.h - found
-- Looking for sys/time.h
-- Looking for sys/time.h - not found
-- Looking for sys/un.h
-- Looking for sys/un.h - not found
-- Looking for poll.h
-- Looking for poll.h - not found
-- Looking for sys/poll.h
-- Looking for sys/poll.h - not found
-- Looking for sys/select.h
-- Looking for sys/select.h - not found
-- Looking for sched.h
-- Looking for sched.h - not found
-- Looking for string.h
-- Looking for string.h - found
-- Looking for strings.h
-- Looking for strings.h - not found
-- Looking for gethostbyname
-- Looking for gethostbyname - not found
-- Looking for gethostbyname_r
-- Looking for gethostbyname_r - not found
-- Looking for strerror_r
-- Looking for strerror_r - not found
-- Looking for sched_get_priority_max
-- Looking for sched_get_priority_max - not found
-- Looking for sched_get_priority_min
-- Looking for sched_get_priority_min - not found
-- Performing Test STRERROR_R_CHAR_P
-- Performing Test STRERROR_R_CHAR_P - Failed
-- Looking for pthread.h
-- Looking for pthread.h - not found
-- Found Threads: TRUE  
-- Building without tests
-- Found FLEX: C:/projects/arrow/cpp/build/winflexbison_ep/src/winflexbison_ep-install/win_flex.exe (found version ""2.6.3"") 
-- Found BISON: C:/projects/arrow/cpp/build/winflexbison_ep/src/winflexbison_ep-install/win_bison.exe (found version ""2.7"") 
CMake Error at lib/cpp/CMakeLists.txt:20 (include_directories):
  include_directories given empty-string as include directory.
{code}",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-06-26 01:21:40,2
13241607,[C++] macOS builds failing idiosyncratically on master with warnings from pmmintrin.h,"This just started happening today and doesn't seem to be related to code changes

https://travis-ci.org/apache/arrow/jobs/550459162#L3495",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-06-26 00:14:55,2
13241525,[CI] Turbodbc integration tests are failing ,"Have not investigated yet, build: https://circleci.com/gh/ursa-labs/crossbow/383

cc [~xhochy]",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-06-25 16:14:58,8
13241524,[Python][CI] Selectively skip test cases in the dask integration test,"Have not investigated yet, build: https://circleci.com/gh/ursa-labs/crossbow/387",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2019-06-25 16:14:00,3
13241500,[Python] [CI] Travis-CI failures in test_jvm.py,"See https://travis-ci.org/apache/arrow/jobs/550245616

[~xhochy]",pull-request-available,"['Continuous Integration', 'Java', 'Python']",ARROW,Bug,Blocker,2019-06-25 14:45:50,14
13241494,[Python] [CI] Install pytest-faulthandler before running tests,The `faulthandler` module is able to dump a Python stack trace when the process crashes. This can make some CI failures more palatable.,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2019-06-25 14:27:35,2
13241467,[Java] Implement a common interface for int vectors,"Now in _DictionaryEncoder#encode_ ituse reflection to pull out the set method and then set values.

Set values by reflection is not efficient and code structure is not elegant such as

_Method setter = null;_
_for (Class<?> c : Arrays.asList(int.class, long.class)) {_
 _try {_
 _setter = indices.getClass().getMethod(""setSafe"", int.class, c);_
 _break;_
 _} catch (NoSuchMethodException e) {_
 _// ignore_
 _}_
_}_

Implement a common interface for int vectors to directly get set method and set values seems a good choice.",pull-request-available,['Java'],ARROW,New Feature,Minor,2019-06-25 12:51:23,16
13241458,[Crossbow] Port conda recipes to azure pipelines ,"Conda forge builds stopped working. CF is transitioning toward azure pipelines so port the conda crossbow builds to azure as well, and update the recipes (including gandiva).",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-06-25 11:34:45,3
13241452,[R] [CI] AppVeyor build should use ccache,It looks like ccache is not installed for the R AppVeyor build. [~npr] [~kou],pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2019-06-25 11:09:24,4
13241379,[Java] Support in-place vector sorting,"Support in-place sorting for vectors. An in-place sorter sorts thevector by directly modifying the vector data, so the input and output vectors are the same one.",pull-request-available,['Java'],ARROW,New Feature,Major,2019-06-25 02:56:46,7
13241359,[Python] Support dictionary unification when converting variable dictionaries to pandas,Follow up work to ARROW-5335,pull-request-available,['Python'],ARROW,Improvement,Major,2019-06-24 23:14:23,14
13241353,[Developer] Improve merge PR script to acknowledge co-authors,"The Apache Spark PR merge tool supports lead/co-author acknowledgement that shows up in the GitHub UI, we should try to follow this

https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py

example commit https://github.com/apache/spark/commit/5a7aa6f4df925bf44267f58a8930b93a4e19c4f4",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-06-24 22:43:08,14
13241271,[C++] gandiva-date_time_test failure on Windows,"I get the following failure on Windows:
{code}
Running main() from C:\t\arrow\cpp\build-release\googletest_ep-prefix\src\googletest_ep\googletest\src\gtest_main.cc
[==========] Running 5 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 5 tests from TestProjector
[ RUN      ] TestProjector.TestIsNull
[       OK ] TestProjector.TestIsNull (47 ms)
[ RUN      ] TestProjector.TestDateTime
..\src\gandiva\tests\date_time_test.cc(181): error: Value of: (exp_yy_from_date)->Equals(outputs.at(0), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  2000,
  1999,
  2015,
  2015
] actual array: [
  1970,
  1970,
  1970,
  1970
]
..\src\gandiva\tests\date_time_test.cc(182): error: Value of: (exp_mm_from_date)->Equals(outputs.at(1), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1,
  12,
  6,
  7
] actual array: [
  1,
  1,
  1,
  1
]
..\src\gandiva\tests\date_time_test.cc(183): error: Value of: (exp_mm_from_ts)->Equals(outputs.at(2), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  12,
  1,
  7,
  6
] actual array: [
  1,
  1,
  1,
  1
]
..\src\gandiva\tests\date_time_test.cc(184): error: Value of: (exp_dd_from_ts)->Equals(outputs.at(3), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  31,
  2,
  1,
  29
] actual array: [
  1,
  1,
  1,
  1
]
[  FAILED  ] TestProjector.TestDateTime (62 ms)
[ RUN      ] TestProjector.TestTime
[       OK ] TestProjector.TestTime (31 ms)
[ RUN      ] TestProjector.TestTimestampDiff
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  48996077,
  -48996077,
  0,
  -82800
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  816601,
  -816601,
  0,
  -1380
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  13610,
  -13610,
  0,
  -23
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  567,
  -567,
  0,
  0
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  81,
  -81,
  0,
  0
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  18,
  -18,
  0,
  0
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  6,
  -6,
  0,
  0
] actual array: [
  0,
  0,
  0,
  0
]
..\src\gandiva\tests\date_time_test.cc(327): error: Value of: (exp_output.at(i))->Equals(outputs.at(i), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1,
  -1,
  0,
  0
] actual array: [
  0,
  0,
  0,
  0
]
[  FAILED  ] TestProjector.TestTimestampDiff (204 ms)
[ RUN      ] TestProjector.TestMonthsBetween
..\src\gandiva\tests\date_time_test.cc(389): error: Value of: (exp_output)->Equals(outputs.at(0), arrow::EqualOptions().nans_equal(true))
  Actual: false
Expected: true
expected array: [
  1,
  -1,
  1,
  1
] actual array: [
  0,
  0,
  0,
  0
]
[  FAILED  ] TestProjector.TestMonthsBetween (93 ms)
[----------] 5 tests from TestProjector (437 ms total)

[----------] Global test environment tear-down
[==========] 5 tests from 1 test case ran. (437 ms total)
[  PASSED  ] 2 tests.
[  FAILED  ] 3 tests, listed below:
[  FAILED  ] TestProjector.TestDateTime
[  FAILED  ] TestProjector.TestTimestampDiff
[  FAILED  ] TestProjector.TestMonthsBetween
{code}
",pull-request-available,['C++ - Gandiva'],ARROW,Bug,Major,2019-06-24 14:56:54,2
13241226,[Java] Improve the performance and code structure for ArrowRecordBatch,"Improve the performance ofArrowRecordBatch by reducing the number of divisions.

Improve the code structure ofArrowRecordBatch by removing the useless constructor.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-06-24 10:35:36,7
13241216,[Java] Remove type conversion in getValidityBufferValueCapacity,"Now implementation of getValidityBufferValueCapacity is:

(int) (validityBuffer.capacity() * 8L)

Seems no need to convert it to Long then convert it back to Int, just replace with:

validityBuffer.capacity() * 8

VariableWidthVectorBenchmarks#getValueCapacity shows the performance:

Before:

avgt 55.731  0.160 ns/op

After:

avgt 5 5.124  0.125 ns/op",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-06-24 10:07:42,16
13241198,[Java] Optimize BaseValueVector#computeCombinedBufferSize logic,"Now inBaseValueVector#computeCombinedBufferSize, it computes validity buffer size as follow:

_roundUp8(getValidityBufferSizeFromCount(valueCount))_

which can bebe expanded to

_(((valueCount + 7) >> 3 + 7) / 8) * 8_

Seems there's no need to compute bufferSize first and expression above could be replaced with:

_(valueCount + 63) / 64 * 8_

In this way, performance of_computeCombinedBufferSize_ would be improved. Performance test:

Before:
BaseValueVectorBenchmarks.testC_omputeCombinedBufferSize_ avgt 54083.180  180.363 ns/op

After:

BaseValueVectorBenchmarks.testC_omputeCombinedBufferSize_avgt 5 3808.635 162.347 ns/op

",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-06-24 08:47:04,16
13241164,[C++] Optimize parsing of Decimal128 in CSV,"- Remove multiple string copies in Decimal.FromString()
- Add unsafe append method to Decimal128Builder",pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-24 03:48:45,15
13241142,[R] r/Dockerfile docker-compose build is broken,"See failure log

https://circleci.com/gh/ursa-labs/crossbow/320?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link",pull-request-available,['R'],ARROW,Bug,Major,2019-06-23 21:37:15,14
13241138,[GLib] c_glib/Dockerfile is broken,"There are a couple of issues, I'll open a WIP PR",pull-request-available,['GLib'],ARROW,Bug,Major,2019-06-23 21:17:55,1
13241081,[Python] List of decimals are not supported when converting to pandas,This seems like an oversite.,pull-request-available,['Python'],ARROW,Improvement,Minor,2019-06-23 09:33:20,15
13240966,[C++] Remove remaining uses of ARROW_BOOST_VENDORED,ARROW-5662 deprecatedARROW_BOOST_VENDORED but we still called it in a couple of places. This removes them.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-21 21:27:08,4
13240964,[R] Review R Windows CI build,"Followup toARROW-3758/[https://github.com/apache/arrow/pull/4622]. In that, I leveraged the tools in[https://github.com/r-windows/rtools-backports]to set up CI for Arrow C++ and R on Windows using Appveyor. I was guided mainly by the steps described [here|https://cwiki.apache.org/confluence/display/ARROW/Release+Management+Guide#ReleaseManagementGuide-BuildingWindowspackages]on the Arrow project wiki and iterated until I got a passing build.

Despite getting it to ""work"", I'm certain I've missed some subtleties, and there may be better ways to accomplish this. Some specific questions:
 * I found that I could ignore rtools-backports/ci-library.sh and most of ci-build.sh because it was oriented around building possibly many packages, but there was a block of {{pacman}} stuff I did have to copy here:[https://github.com/apache/arrow/pull/4622/files#diff-f4a8bedb9b0d3fe301a84914916f6d49R22]. I'm not sure how much these are likely to change, but if that's a concern, maybe that setup could be factored out to a separate shell script in rtools-backports, and the arrow CI could {{wget}} and {{source}} it like it does some other resources. That way, our setup here wouldn't diverge.
 * I did not understand what I needed to do with rtools-packages, if anything. It seems that it's not used by R yet, so is it just important to have the PKGBUILD in place there for when is ready? If I wanted to build both rtools-backports and rtools-packages builds in the same job, is the difference only [these environment variables|https://github.com/r-windows/rtools-backports/blob/master/mingw-w64-arrow/PKGBUILD#L48-L52]?
 * The process of taking the appveyor build artifacts, unzipping them, and merging them into the ""rwinlib"" directory layout seemed loose and poorly defined on the wiki, at least as I could tell. I packaged up the process (as I understood it) in a [shell script|https://github.com/apache/arrow/pull/4622/files#diff-c043cda9f4ed847b06efeeacf04634ee], and it produced a zip file that is the right shape (right enough that R could install the arrow R package with it and run tests). Does that script make sense? In particular,
 ** Is there a good way to keep around the other dependencies (double-conversion, boost, thrift) from when the packages are built so that I don't have to re-download them from bintray? I see that they get pulled down at the beginning of each pkgbuild and then removed after, butI don't know where they are put such that I could keep them around and use them later.
 ** Is the {{lib}} directory for other dependencies (e.g. libdouble-conversion.a) and {{lib-4.9.3}} for the arrow and parquet binaries we build, as the wiki says? Or is {{lib}} for the Rtools4.0/gcc8 versions and lib-4.9.3 for the Rtools3.5/gcc4 versions?
 ** libdouble-conversion.a only seems to exist in the rtools-packages Rtools4.0 packages, but that nevertheless works on the R release version. However, if I used the versions of boost and thrift from the Rtools4.0 bintrays, the R package did not build (link) correctly.

To be clear, it is not our intention to fork or otherwise avoid the supported Rtools toolchain that is maintained there; rather, we want to continuously integrate arrow to avoid breaking things and make it easier to submit updates to rtools-backports/packages/rwinlib when there's a new arrow release. We want as much as possible to use the supported tools and workflows and are willing to contribute to enhancing them, though we recognize that our needs (as a big C++ library under heavy active development) are probably not shared by many other projects that use rtools-packages et al.

",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-21 21:15:06,4
13240953,[R] Add snappy to Rtools Windows builds,"Followup toARROW-3758 /[https://github.com/apache/arrow/pull/4622]. The rtools builds of the C++ library for R Windows do not contain Snappy, which is very commonly used with Parquet files.

[https://github.com/r-windows/rtools-backports/pull/6]is an attempt to build snappy with Rtools. Assuming that can be worked out, adding it to {{ci/PKGBUILD}} and setting {{ARROW_WITH_SNAPPY=ON}} there may be enough.

Once this works, we should remove the parquet test skip that PR#4622 added.",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-21 20:28:30,4
13240925,[Python] from_pandas conversion casts values to string inconsistently,"When calling {{pa.Array.from_pandas}} primitive data as input, and casting to string with  ""type=pa.string()"", the resulting pyarrow Array can have inconsistent values. For most input, the result is an empty string, however for some types (int32, int64) the values are '\x01' etc.

{noformat}
In [8]: s = pd.Series([1, 2, 3], dtype=np.uint8)

In [9]: pa.Array.from_pandas(s, type=pa.string())                                                                            
Out[9]: 
<pyarrow.lib.StringArray object at 0x7f90b6091a48>
[
  """",
  """",
  """"
]

In [10]: s = pd.Series([1, 2, 3], dtype=np.uint32)                                                                           

In [11]: pa.Array.from_pandas(s, type=pa.string())                                                                           
Out[11]: 
<pyarrow.lib.StringArray object at 0x7f9097efca48>
[
  """",
  """",
  """"
]
{noformat}

This came from the Spark discussion https://github.com/apache/spark/pull/24930/files#r296187903. Type casting this way in Spark is not supported, but it would be good to get the behavior consistent. Would it be better to raise an UnsupportedOperation error?
",pull-request-available,['Python'],ARROW,Bug,Minor,2019-06-21 17:46:46,5
13240909,[FlightRPC] Wrap gRPC exceptions/statuses,"Instead of requiring users to catch/throw StatusRuntimeException in Flight services/clients, and thereby leaking gRPC details, we should provide our own set of exceptions and status codes. This way, services can provide proper error messages and error codes to clients, which can catch the exception and respond properly.",pull-request-available,"['C++', 'FlightRPC', 'Java']",ARROW,Improvement,Major,2019-06-21 16:22:43,0
13240884,[Rust] datafusion group-by tests depends on result set order,"See https://circleci.com/gh/ursa-labs/crossbow/223?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link

once I properly export ARROW_TEST_DATA and PARQUET_TEST_DATA, I get further failures, e.g.


{code:bash}
running 18 tests
test csv_query_group_by_int_min_max ... FAILED
test csv_query_external_table_count ... ok
test csv_query_count ... ok
test csv_count_star ... ok
test csv_query_avg ... ok
test csv_query_avg_multi_batch ... ok
test csv_query_cast ... ok
test csv_query_group_by_avg ... FAILED
test csv_query_group_by_string_min_max ... FAILED
test csv_query_group_by_int_count ... FAILED
test csv_query_limit ... ok
test csv_query_limit_bigger_than_nbr_of_rows ... ok
test csv_query_limit_with_same_nbr_of_rows ... ok
test csv_query_cast_literal ... ok
test csv_query_limit_zero ... ok
test csv_query_create_external_table ... ok
test csv_query_with_predicate ... ok
test parquet_query ... ok

failures:

---- csv_query_group_by_int_min_max stdout ----
thread 'csv_query_group_by_int_min_max' panicked at 'assertion failed: `(left == right)`
  left: `""4\t0.02182578039211991\t0.9237877978193884\n5\t0.01479305307777301\t0.9723580396501548\n2\t0.16301110515739792\t0.991517828651004\n3\t0.047343434291126085\t0.9293883502480845\n1\t0.05636955101974106\t0.9965400387585364\n""`,
 right: `""4\t0.02182578039211991\t0.9237877978193884\n2\t0.16301110515739792\t0.991517828651004\n5\t0.01479305307777301\t0.9723580396501548\n3\t0.047343434291126085\t0.9293883502480845\n1\t0.05636955101974106\t0.9965400387585364\n""`', datafusion/tests/sql.rs:77:5
note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.

---- csv_query_group_by_avg stdout ----
thread 'csv_query_group_by_avg' panicked at 'assertion failed: `(left == right)`
  left: `""\""a\""\t0.48754517466109415\n\""e\""\t0.48600669271341534\n\""d\""\t0.48855379387549824\n\""c\""\t0.6600456536439784\n\""b\""\t0.41040709263815384\n""`,
 right: `""\""d\""\t0.48855379387549824\n\""c\""\t0.6600456536439784\n\""b\""\t0.41040709263815384\n\""a\""\t0.48754517466109415\n\""e\""\t0.48600669271341534\n""`', datafusion/tests/sql.rs:99:5

---- csv_query_group_by_string_min_max stdout ----
thread 'csv_query_group_by_string_min_max' panicked at 'assertion failed: `(left == right)`
  left: `""\""a\""\t0.02182578039211991\t0.9800193410444061\n\""e\""\t0.01479305307777301\t0.9965400387585364\n\""d\""\t0.061029375346466685\t0.9748360509016578\n\""c\""\t0.0494924465469434\t0.991517828651004\n\""b\""\t0.04893135681998029\t0.9185813970744787\n""`,
 right: `""\""d\""\t0.061029375346466685\t0.9748360509016578\n\""c\""\t0.0494924465469434\t0.991517828651004\n\""b\""\t0.04893135681998029\t0.9185813970744787\n\""a\""\t0.02182578039211991\t0.9800193410444061\n\""e\""\t0.01479305307777301\t0.9965400387585364\n""`', datafusion/tests/sql.rs:187:5

---- csv_query_group_by_int_count stdout ----
thread 'csv_query_group_by_int_count' panicked at 'assertion failed: `(left == right)`
  left: `""\""a\""\t21\n\""e\""\t21\n\""d\""\t18\n\""c\""\t21\n\""b\""\t19\n""`,
 right: `""\""d\""\t18\n\""c\""\t21\n\""b\""\t19\n\""a\""\t21\n\""e\""\t21\n""`', datafusion/tests/sql.rs:175:5
{code}

I suspect that the tests are expecting the group-by results in a fix order. That would be highly dependent on the iterator of the hash table. Note that once I did a rustup update (and docker rmi rustlangrust/nightly), the failures have gone away.",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Minor,2019-06-21 14:49:30,10
13240877,[Python] Drop Python 3.5 from support matrix,"We probably need to maintain Python 3.5 on Linux and macOS for the time being, but we may want to drop it for Windows since conda-forge isn't supporting Python 3.5 anymore, so maintaining wheels for Python 3.5 will come with extra cost",pull-request-available,['Python'],ARROW,Improvement,Blocker,2019-06-21 14:10:29,2
13240876,[R][Lint] Fix hadolint docker linting error,Build error: https://travis-ci.org/apache/arrow/jobs/548603471,pull-request-available,"['Developer Tools', 'R']",ARROW,Bug,Major,2019-06-21 14:07:54,3
13240820,[Python] Missing pandas pytest markers from test_parquet.py,Resulting failures in the nopandas tests: https://circleci.com/gh/ursa-labs/crossbow/123,pull-request-available,['Python'],ARROW,Bug,Major,2019-06-21 09:46:02,3
13240725,[Crossbow] get_apache_mirror.py fails with TLS error on macOS with Python 3.5,"Currently the macOS python 3.5 is failing with
{code:java}
Downloading Apache Thrift from Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 1254, in do_open
    h.request(req.get_method(), req.selector, req.data, headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1107, in request
    self._send_request(method, url, body, headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1152, in _send_request
    self.endheaders(body)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1103, in endheaders
    self._send_output(message_body)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 934, in _send_output
    self.send(msg)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 877, in send
    self.connect()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1261, in connect
    server_hostname=server_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 385, in wrap_socket
    _context=self)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 760, in __init__
    self.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 996, in do_handshake
    self._sslobj.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 641, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:719){code}
I've been looking into this error and will try to push a fix (the openssl version that is used with python 3.5 on macos is too old I think).",pull-request-available,[],ARROW,Improvement,Major,2019-06-20 19:53:30,14
13240724,[Crossbow] manylinux1 wheel building failing,"I tried to set up a crossbow queue (ona0e1fbb9ef51d05a3f28e221cf8c5d4031a50c93), and right now building the manylinux1 wheels seems to be failing because of the arrow flight tests:


{code:java}
_______________________________ test_tls_do_get ________________________________
    def test_tls_do_get():
        """"""Try a simple do_get call over TLS.""""""
        table = simple_ints_table()
>       certs = example_tls_certs()
usr/local/lib/python3.6/site-packages/pyarrow/tests/test_flight.py:563: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
usr/local/lib/python3.6/site-packages/pyarrow/tests/test_flight.py:64: in example_tls_certs
    ""root_cert"": read_flight_resource(""root-ca.pem""),
usr/local/lib/python3.6/site-packages/pyarrow/tests/test_flight.py:48: in read_flight_resource
    root = resource_root()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
    def resource_root():
        """"""Get the path to the test resources directory.""""""
        if not os.environ.get(""ARROW_TEST_DATA""):
>           raise RuntimeError(""Test resources not found; set ""
                               ""ARROW_TEST_DATA to <repo root>/testing"")
E           RuntimeError: Test resources not found; set ARROW_TEST_DATA to <repo root>/testing
usr/local/lib/python3.6/site-packages/pyarrow/tests/test_flight.py:41: RuntimeError{code}
This may have been introduced in [https://github.com/apache/arrow/pull/4594|https://github.com/apache/arrow/pull/4594.]

Any thoughts how we should proceed with this?",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Blocker,2019-06-20 19:46:47,14
13240687,"[Python] Display ""not null"" in Schema.__repr__ for non-nullable fields","Minor usability improvement

{code}
schema = pa.schema([pa.field('a', pa.int64(), nullable=False)])

In [11]: schema                                                                                                                           
Out[11]: a: int64

In [12]: schema[0]                                                                                                                        
Out[12]: pyarrow.Field<a: int64 not null>
{code}

I'd like to see

{code}
In [11]: schema                                                                                                                           
Out[11]: a: int64 not null
{code}

or similar",pull-request-available,['Python'],ARROW,Improvement,Major,2019-06-20 16:18:15,5
13240633,[Python] Underscores in partition (string) values are dropped when reading dataset,"When reading a partitioned dataset, in which the partition column contains string values with underscores, pyarrow seems to be ignoring the underscores in the resulting values.

For example if I write and then read a dataset as follows:
{code:java}
import pyarrow as pa
import pandas as pd

df = pd.DataFrame({
 ""year_week"": [""2019_2"", ""2019_3""],
 ""value"": [1, 2]
})

table = pa.Table.from_pandas(df.head())
pq.write_to_dataset(table, 'test', partition_cols=[""year_week""])

table2 = pq.ParquetDataset('test').read()
{code}
The resulting 'year_week' column in table 2 has lost the underscores:
{code:java}
table2[1] # Gives:

<Column name='year_week' type=DictionaryType(dictionary<values=int64, indices=int32, ordered=0>)>
[

 -- dictionary:
 [
 20192,
 20193
 ]
 -- indices:
 [
 0
 ],

 -- dictionary:
 [
 20192,
 20193
 ]
 -- indices:
 [
 1
 ]
]
{code}
Is this intentional behaviour or is this a bug in arrow?",dataset-parquet-read parquet,['Python'],ARROW,Bug,Major,2019-06-20 11:48:36,5
13240616,[Crossbow] Execute nightly crossbow tests on CircleCI instead of Travis,"The spark integration test has hit the 50mins maximum build time on travis, whereas Circle can run jobs up to 5 hours. All of the nightly tests are run within docker containers so porting travis template to a circleci equivalent template should not be too hard. In order to properly report badges via @ursabot crossbow command, we also need to store which CI service executes a task. So the following changes are required:

- Port https://github.com/apache/arrow/blob/master/dev/tasks/docker-tests/travis.linux.yml to a CircleCI equivalent
- Store which CI is responsible for running a particular task https://github.com/apache/arrow/blob/master/dev/tasks/crossbow.py#L468
- Update ursabot to generate badges based on the property added in the previous step",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-06-20 09:10:50,3
13240551,[JAVA] Provide ability to resync VectorSchemaRoot if types change,"I can't transfer using apache arrow-flihgt. Contains listvector data. The problem description is as follows:
{quote} # I parse an xml file and convert it to an arrow format and finally convert it to a parquet data format. The address of the .xml file data is url [http://www.w3school.com.cn/example/xmle/cd_catalog.xml|http://www.w3school.com.cn/example/xmle/cd_catalog.xml)]
 # I created a schema that uses listvector.
code show as below:
List<FiledVector> list = childrenBuilder.add(ListVector.empty(column.getId().toString(),allocator));
VectorSchemaRoot root = VectorSchemaRoot.of(inVector)
 # Parse the xml file to get the list data in ""cd"". Use api use listvector.
`ListVector listVector = (ListVector) valueVectors;
List<Column> columns = column.getColumns();
Column column1 = columns.get(0);
String name = column1.getId().toString();
UnionListWriter writer = listVector.getWriter();
Writer.allocate();
For (int j = 0; j < column1.getColumns().size();j++) {

writer.setPosition(j);
writer.startList();
Writer.list().startList();
Column column2 = column1.getColumns().get(j);
List<Map<String, String>> lst = (List<Map<String, String>>) ((Map) val).get(name);

For (int k = 0; k < lst.size(); k++) {
Map<String, String> stringStringMap = lst.get(k);
String value = stringStringMap.get(column2.getId().toString());
Switch (column2.getType()) {
Case FLOAT:
Writer.list().float4().writeFloat4(stringConvertFloat(value));
Break;
Case BOOLEAN:
Writer.list().bit().writeBit(stringConvertBoolean(value));
Break;
Case DECIMAL:
Writer.list().decimal().writeDecimal(stringConvertDecimal(value,column2.getScale()));
Break;
Case TIMESTAMP:
Writer.list().dateMilli().writeDateMilli(stringConvertTimestamp(value,column2.format.toString()));
Break;
Case INTEGER:
Case BIGINT:
Writer.list().bigInt().writeBigInt(stringConvertLong(value));
Break;
Case VARCHAR:
VarCharHolder varBinaryHolder = new VarCharHolder();
varBinaryHolder.start = 0;
Byte[] bytes =value.getBytes();
ArrowBuf buffer = listVector.getAllocator().buffer(bytes.length);
varBinaryHolder.buffer = buffer;
buffer.writeBytes(bytes);
varBinaryHolder.end=bytes.length;
Writer.list().varChar().write(varBinaryHolder);
Break;
Default:
Throw new IllegalArgumentException("" error no type !!"");
}
}
Writer.list().endList();
writer.endList();
}`

4.

After the write is complete, I will send to the arrow-flight server. server code :
{quote}
{quote}@Override
public Callable<Flight.PutResult> acceptPut(FlightStream flightStream) {

 return () -> {

 try (VectorSchemaRoot root = flightStream.getRoot()) {

 while (flightStream.next()) {
 VectorSchemaRoot other = null;
 try {
 logger.info("" Receive message ...... size: "" + root.getRowCount());
 other = copyRoot(root);
 ArrowMessage arrowMessage = new ArrowMessage(other, other.getSchema());
 spmc.offer(arrowMessage);
 } catch (Exception e) {

 logger.error(e.getMessage(), e);
 }
 }
 }

 return Flight.PutResult.parseFrom(""ok"".getBytes());
 };

}{quote}
{quote}But the server did not receive any information.!! it is error .{quote}
{quote}client code :{quote}
{quote}root = message.getRoot();
//client.close();
FlightClient.ClientStreamListener listener =
 client.startPut(FlightDescriptor.path(message.getFilename()), root);
listener.putNext();
listener.completed();
client.close();
listener.putNext();
listener.completed();
Flight.PutResult result =
 listener.getResult();
String s = new String(result.toByteArray());
System.out.println(s);{quote}",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Critical,2019-06-20 02:58:56,7
13240515,[Python] Enable Flight wheels on macOS,Follow up to ARROW-3150,pull-request-available,['Python'],ARROW,Improvement,Blocker,2019-06-19 21:59:29,14
13240482,[Python] Table.from_pydict/from_arrays not using types in specified schema correctly ,"Example with {{from_pydict}} (from https://github.com/apache/arrow/pull/4601#issuecomment-503676534):

{code:python}
In [15]: table = pa.Table.from_pydict(
    ...:     {'a': [1, 2, 3], 'b': [3, 4, 5]},
    ...:     schema=pa.schema([('a', pa.int64()), ('c', pa.int32())]))

In [16]: table
Out[16]: 
pyarrow.Table
a: int64
c: int32

In [17]: table.to_pandas()
Out[17]: 
   a  c
0  1  3
1  2  0
2  3  4
{code}

Note that the specified schema has 1) different column names and 2) has a non-default type (int32 vs int64) which leads to corrupted values.

This is partly due to {{Table.from_pydict}} not using the type information in the schema to convert the dictionary items to pyarrow arrays. But then it is also {{Table.from_arrays}} that is not correctly casting the arrays to another dtype if the schema specifies as such.

Additional question for {{Table.pydict}} is whether it actually should override the 'b' key from the dictionary as column 'c' as defined in the schema (this behaviour depends on the order of the dictionary, which is not guaranteed below python 3.6).
",pull-request-available,['Python'],ARROW,Bug,Major,2019-06-19 18:34:58,3
13240474,[C++] ChunkedArray should validate the types of the arrays,"Example from Python, showing that you can currently create a ChunkedArray with incompatible types:

{code:python}
In [8]: a1 = pa.array([1, 2])

In [9]: a2 = pa.array(['a', 'b'])

In [10]: pa.chunked_array([a1, a2])
Out[10]:
<pyarrow.lib.ChunkedArray object at 0x7fca50704d20>
[
  [
    1,
    2
  ],
  [
    ""a"",
    ""b""
  ]
]
{code}

So a {{ChunkedArray::Validate}} can be implemented (and which should probably be called by default upon creation?)",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-06-19 18:09:00,14
13240465,[CI] Fix cpp docker image,"{code:bash}
make -f Makefile.docker run-cpp
...
54/64 Test #79: arrow-dataset-file_test ............***Failed    0.04 sec
Running arrow-dataset-file_test, redirecting output into /build/cpp/build/test-logs/arrow-dataset-file_test.txt (attempt 1/1)
/build/cpp/debug/arrow-dataset-file_test: error while loading shared libraries: libbrotlienc.so.1: cannot open shared object file: No such file or directory
/build/cpp/src/arrow/dataset

      Start 80: arrow-flight-test
55/64 Test #80: arrow-flight-test ..................***Failed    0.04 sec
Running arrow-flight-test, redirecting output into /build/cpp/build/test-logs/arrow-flight-test.txt (attempt 1/1)
/build/cpp/debug/arrow-flight-t
{code}",pull-request-available,[],ARROW,Improvement,Major,2019-06-19 17:17:24,13
13240463,[CI] Fix iwyu docker image,See[https://travis-ci.org/ursa-labs/crossbow/builds/547691665?utm_source=github_status&utm_medium=notification],pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-06-19 17:03:52,13
13240370,[Python] Update manylinux dependency versions,We should bump the versions of upstream libraries we compile in the manylinux build image.,pull-request-available,"['Packaging', 'Python']",ARROW,Task,Major,2019-06-19 08:38:14,2
13240342,[Integration][C++] Create round trip integration test for extension types,With Java and C++ code merged we should verify round-trip of the type.,pull-request-available,"['C++', 'Integration', 'Java']",ARROW,Improvement,Major,2019-06-19 04:40:59,2
13240313,[C++] Build fails on mingw without codecvt,"See comment onhttps://issues.apache.org/jira/browse/ARROW-3758and [chat discussion|https://ursalabs.zulipchat.com/#narrow/stream/180245-dev/topic/Need.20help.20with.20Windows.20compilation/near/168456857]. In the toolchain used to build R packages,{{codecvt}} [isn't found|https://ci.appveyor.com/project/nealrichardson/arrow/builds/25367079#L1177]. This was apparently made into a harder requirement with the filesystem work last month.

We'll need some kind of workaround.[https://stackoverflow.com/questions/15615136/is-codecvt-not-a-std-header]suggests Boost.Locale.",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-18 22:49:07,2
13240303,[Crossbow][Documentation] Move the user guide to the Sphinx documentation,"Move crossbow's already existing README to docs/source/developers/crossbow.rst.
Also answer how to run specific docker tasks with crossbow (like the docker-cpp integration test).",pull-request-available,"['Continuous Integration', 'Documentation']",ARROW,Improvement,Major,2019-06-18 21:48:42,3
13240261,[Flight] Add ability to override hostname checking,"We should add the ability to override hostname checks, so you can connect to localhost over TLS but still verify that the certificate is for some other domain.

Example: when deploying on Kubernetes with headless services, clients connect directly to backend services and do load balancing themselves. Thus all instances of an application must present a certificate for the same hostname. To do health checks in such an environment, you can't connect to the TLS hostname (which may resolve to a different instance); you need to connect to localhost, and override the hostname check.",pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-06-18 18:54:55,0
13240196,[Java] Remove floating point computation from getOffsetBufferValueCapacity,"Some getOffsetBufferValueCapacity methods uses floating point computation to calculate the capacity, which is not necessary.

(int) ((offsetBuffer.capacity() * 1.0) / OFFSET_WIDTH);

It is interesting to note that JIT cannot optimize away the floating point computations:

 !image-2019-06-18-20-30-17-826.png! 

So this has performance penalty:
Before:
VariableWidthVectorBenchmarks.getValueCapacity  avgt    5  6.570  0.004  ns/op

After:
VariableWidthVectorBenchmarks.getValueCapacity  avgt    5  5.787  0.575  ns/op
",pull-request-available,['Java'],ARROW,Improvement,Trivial,2019-06-18 12:31:03,7
13239991,[Python] Enable bz2 in Linux wheels,This would allow e.g. reading bz2-compressed CSV or JSON files.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2019-06-17 16:17:32,2
13239978,[C++] CMake 3.2 build is broken,"# See log

https://gist.github.com/wesm/53147d8d672aa850f49eaeb3c3f24630",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-06-17 15:41:57,13
13239976,[Python][Parquet] Table of nested arrays doesn't round trip,"This is pyarrow 0.13 on Windows.

{code:python}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

def make_table(num_rows):
    typ = pa.list_(pa.field(""item"", pa.float32(), False))
    return pa.Table.from_arrays([
        pa.array([[0] * (i%10) for i in range(0, num_rows)], type=typ),
        pa.array([[0] * ((i+5)%10) for i in range(0, num_rows)], type=typ)
    ], ['a', 'b'])

pq.write_table(make_table(1000000), 'test.parquet')

pq.read_table('test.parquet')
{code}

The last line throws the following exception:


{noformat}
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-4-0f3266afa36c> in <module>
----> 1 pq.read_table('full.parquet')

~\Anaconda3\lib\site-packages\pyarrow\parquet.py in read_table(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, filesystem)
   1150         return fs.read_parquet(path, columns=columns,
   1151                                use_threads=use_threads, metadata=metadata,
-> 1152                                use_pandas_metadata=use_pandas_metadata)
   1153 
   1154     pf = ParquetFile(source, metadata=metadata)

~\Anaconda3\lib\site-packages\pyarrow\filesystem.py in read_parquet(self, path, columns, metadata, schema, use_threads, use_pandas_metadata)
    179                                  filesystem=self)
    180         return dataset.read(columns=columns, use_threads=use_threads,
--> 181                             use_pandas_metadata=use_pandas_metadata)
    182 
    183     def open(self, path, mode='rb'):

~\Anaconda3\lib\site-packages\pyarrow\parquet.py in read(self, columns, use_threads, use_pandas_metadata)
   1012             table = piece.read(columns=columns, use_threads=use_threads,
   1013                                partitions=self.partitions,
-> 1014                                use_pandas_metadata=use_pandas_metadata)
   1015             tables.append(table)
   1016 

~\Anaconda3\lib\site-packages\pyarrow\parquet.py in read(self, columns, use_threads, partitions, open_file_func, file, use_pandas_metadata)
    562             table = reader.read_row_group(self.row_group, **options)
    563         else:
--> 564             table = reader.read(**options)
    565 
    566         if len(self.partition_keys) > 0:

~\Anaconda3\lib\site-packages\pyarrow\parquet.py in read(self, columns, use_threads, use_pandas_metadata)
    212             columns, use_pandas_metadata=use_pandas_metadata)
    213         return self.reader.read_all(column_indices=column_indices,
--> 214                                     use_threads=use_threads)
    215 
    216     def scan_contents(self, columns=None, batch_size=65536):

~\Anaconda3\lib\site-packages\pyarrow\_parquet.pyx in pyarrow._parquet.ParquetReader.read_all()

~\Anaconda3\lib\site-packages\pyarrow\error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Column 1 named b expected length 932066 but got length 932063
{noformat}
",parquet pull-request-available,['Python'],ARROW,Bug,Major,2019-06-17 15:37:46,14
13239975,[C++] Fix Coverity issues,Coverity didn't find really interesting issues but there are a few things worth fixing or working around.,pull-request-available,['C++'],ARROW,Bug,Major,2019-06-17 15:31:09,2
13239841,[C++] -Duriparser_SOURCE=BUNDLED is broken,Due to a typo. Patch incoming,pull-request-available,['C++'],ARROW,Bug,Major,2019-06-17 01:20:55,14
13239839,[CI][GLib] Failed on macOS,"https://travis-ci.org/apache/arrow/jobs/546495122#L3941

{noformat}
arrow-glib/meson.build:225:0: ERROR: gobject-introspection dependency was not found, gir cannot be generated.
{noformat}",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Test,Major,2019-06-17 01:19:06,1
13239752,[C++] get_apache_mirror.py doesn't work with Python 3.5,"{noformat}
% python3 --version
Python 3.5.3
% python3 cpp/build-support/get_apache_mirror.py 
Traceback (most recent call last):
  File ""cpp/build-support/get_apache_mirror.py"", line 31, in <module>
    print(json.loads(suggested_mirror)['preferred'])
  File ""/usr/lib/python3.5/json/__init__.py"", line 312, in loads
    s.__class__.__name__))
TypeError: the JSON object must be str, not 'bytes'
{noformat}

Debian stretch ships Python 3.5 as python3.",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-15 20:45:32,1
13239651,[Python] C++ build failure against Python 2.7 headers,"
See example failure

{code}
FAILED: src/arrow/python/CMakeFiles/arrow_python_objlib.dir/extension_type.cc.o 
/opt/conda/bin/x86_64-conda_cos6-linux-gnu-c++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR="""" -DARROW_PYTHON_EXPORTING -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -Isrc -I../src -isystem /opt/conda/include -isystem jemalloc_ep-prefix/src -isystem ../thirdparty/hadoop/include -isystem /opt/conda/lib/python2.7/site-packages/numpy/core/include -isystem /opt/conda/include/python2.7 -Wno-noexcept-type -fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -g -fPIC   -std=gnu++11 -MD -MT src/arrow/python/CMakeFiles/arrow_python_objlib.dir/extension_type.cc.o -MF src/arrow/python/CMakeFiles/arrow_python_objlib.dir/extension_type.cc.o.d -o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/extension_type.cc.o -c ../src/arrow/python/extension_type.cc
../src/arrow/python/extension_type.cc: In function 'arrow::Status arrow::py::{anonymous}::SerializeExtInstance(PyObject*, std::__cxx11::string*)':
../src/arrow/python/extension_type.cc:36:85: error: ISO C++ forbids converting a string constant to 'char*' [-Werror=write-strings]
   OwnedRef res(PyObject_CallMethod(type_instance, ""__arrow_ext_serialize__"", nullptr));
                                                                                     ^
../src/arrow/python/extension_type.cc: In function 'PyObject* arrow::py::{anonymous}::DeserializeExtInstance(PyObject*, std::shared_ptr<arrow::DataType>, const string&)':
../src/arrow/python/extension_type.cc:65:63: error: ISO C++ forbids converting a string constant to 'char*' [-Werror=write-strings]
                              storage_ref.obj(), data_ref.obj());
                                                               ^
../src/arrow/python/extension_type.cc:65:63: error: ISO C++ forbids converting a string constant to 'char*' [-Werror=write-strings]
cc1plus: all warnings being treated as errors
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-14 21:18:51,14
13239648,[C++] Compilation error due to C++11 string literals on gcc 5.4.0 Ubuntu 16.04,"I'm seeing this locally from {{docker-compose run cpp-ubuntu-xenial}}

{code}
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:543:9: error: missing terminating "" character [-Werror]
   ASSERT_OK(ArrayFromJSON(type, R""delim(
         ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:543:2: error: missing terminating "" character
   ASSERT_OK(ArrayFromJSON(type, R""delim(
  ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:550:1: error: stray '\' in program
 )delim"",
 ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:617:9: error: missing terminating "" character [-Werror]
   ASSERT_OK(ArrayFromJSON(type, R""delim(
         ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:617:2: error: missing terminating "" character
   ASSERT_OK(ArrayFromJSON(type, R""delim(
  ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:622:9: error: stray '\' in program
         [null, ""empty""]
         ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:622:17: error: missing terminating "" character [-Werror]
         [null, ""empty""]
                 ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:622:9: error: missing terminating "" character
         [null, ""empty""]
         ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:630:9: error: stray '\' in program
         [""bootstrapping tautology?"", ""lispy"", null, ""i can see eternity""]
         ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:630:11: error: missing terminating "" character [-Werror]
         [""bootstrapping tautology?"", ""lispy"", null, ""i can see eternity""]
           ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:630:9: error: missing terminating "" character
         [""bootstrapping tautology?"", ""lispy"", null, ""i can see eternity""]
         ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:635:1: error: stray '\' in program
 )delim"",
 ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc: In member function 'virtual void arrow::ipc::internal::json::TestMap_IntegerToInteger_Test::TestBody()':
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:544:1: error: two consecutive '[' shall only introduce an attribute before '[' token
 [
 ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:544:1: error: return-statement with a value, in function returning 'void' [-fpermissive]
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:544:1: error: expected ';' before '[' token
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:545:6: error: expected ']' before '[' token
     [[0, 1], [1, 1], [2, 2], [3, 3], [4, 5], [5, 8]],
      ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc: In member function 'virtual void arrow::ipc::internal::json::TestMap_IntegerMapToStringList_Test::TestBody()':
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:618:1: error: two consecutive '[' shall only introduce an attribute before '[' token
 [
 ^
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:618:1: error: return-statement with a value, in function returning 'void' [-fpermissive]
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:618:1: error: expected ';' before '[' token
/arrow/cpp/src/arrow/ipc/json-simple-test.cc:620:7: error: expected ']' before '[' token
       [
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-14 20:42:58,14
13239533,"[Python] Define extension type API in Python to ""receive"" or ""send"" a foreign extension type","In work in ARROW-840, a static {{arrow.py_extension_type}} name is used. There will be cases where an extension type is coming from another programming language (e.g. Java), so it would be useful to be able to ""plug in"" a Python extension type subclass that will be used to deserialize the extension type coming over the wire. This has some different API requirements since the serialized representation of the type will not have knowledge of Python pickling, etc. ",pull-request-available,['Python'],ARROW,Improvement,Major,2019-06-14 13:22:03,5
13239528,[C++] Set CMP0068 CMake policy to avoid macOS warnings,"These warnings are appearing in the build on macOS

{code}
CMake Warning (dev):
  Policy CMP0068 is not set: RPATH settings on macOS do not affect
  install_name.  Run ""cmake --help-policy CMP0068"" for policy details.  Use
  the cmake_policy command to set the policy and suppress this warning.

  For compatibility with older versions of CMake, the install_name fields for
  the following targets are still affected by RPATH settings:

   arrow_dataset_shared
   arrow_python_shared
   arrow_shared
   arrow_testing_shared
   parquet_shared
   plasma_shared

This warning is for project developers.  Use -Wno-dev to suppress it.
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-14 13:04:20,8
13239478,[Python] pandas.RangeIndex._start/_stop/_step are deprecated,"There are public attributes added RangeIndex.start/stop/step, and the private {{_start/_stop/_step}} are deprecated. See https://github.com/pandas-dev/pandas/pull/26581",pull-request-available,['Python'],ARROW,Bug,Major,2019-06-14 08:29:21,5
13239455,[Python] register pytest markers to avoid warnings,"Currently the python test suite gives warnings like:

{code}
/home/joris/miniconda3/envs/arrow-dev/lib/python3.7/site-packages/_pytest/mark/structures.py:337
  /home/joris/miniconda3/envs/arrow-dev/lib/python3.7/site-packages/_pytest/mark/structures.py:337: PytestUnknownMarkWarning: Unknown pytest.mark.pandas - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html
    PytestUnknownMarkWarning,
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2019-06-14 07:18:56,5
13239397,[R] R package namespace cleanup,Pulling off the cleanup bits from ARROW-5505.,pull-request-available,['R'],ARROW,Improvement,Major,2019-06-13 23:25:17,4
13239341,[Python] Flight tests failing on Python 2.7,See build https://ci.ursalabs.org/#/builders/72/builds/1084,pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Blocker,2019-06-13 17:51:03,14
13239320,[C++] add support for UnionArrays to Take and Filter,"The Take and [Filter|https://github.com/apache/arrow/pull/4366] kernels do not currently support UnionArrays, but this should not be difficult to add",pull-request-available,['C++'],ARROW,New Feature,Minor,2019-06-13 16:11:13,6
13239291,"[R] Run ""no libarrow"" R build in the same CI entry if possible","This build only takes 3min30sec, so I think it's OK to run it in the same CI entry if possible",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-13 14:00:58,4
13239280,[C++] Better support for building UnionArrays,"UnionBuilders (for both sparse and dense mode unions) are not currently supported by MakeBuilder or ArrayFromJSON. This increases friction when working with and testing against union arrays, and support should be added to both. For ArrayFromJSON each entry must be specified with a (type code, value) pair:

{code}
ArrayFromJSON(union_({field(""lint"", list(int32())), field(""str"", utf8())}), R""([
  [0, null],
  [1, ""hello""],
  [0, [1, 2]],
  [1, ""world""]
])"");
{code}

DenseUnionBuilder currently requires the user to explicitly input offsets, but if it were modified to hold pointers to child builders (as ListBuilder, for example) then those offsets could be derived from the lengths of child builders (which is much more user friendly).",pull-request-available,['C++'],ARROW,New Feature,Minor,2019-06-13 13:03:23,6
13239278,[Java] Add more maven style check for Java code,"Add more maven style check for java code, such as unused imports,redundant modifier, etc. In this way, the quality of code will be improved.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-06-13 12:52:40,16
13239262,[Java] Add import for link reference in FieldReader javadoc,Link reference(ValueVector) in FieldReader javadoc has no import.,pull-request-available,['Java'],ARROW,Bug,Trivial,2019-06-13 12:06:16,16
13239237,"[Java] When the isSet of a NullableValueHolder is 0, the buffer field should not be used","For each variable-width vector, like the VarCharVector, it has a set method that uses a NullableValueHolder as the input parameter. When the isSet field is set to 0, it means the value  to set is null, so the buffer field of the NullableValueHolder is invalid, and should not be used. 

For example, the user may set a null value in the VarCharVector with the following code snippet:

NullableVarCharHolder holder = new NullableVarCharHolder();
holder.isSet = 0;
...
varCharVector.set(i, holder);

Please note that in the code above, the holder.buffer is not set, so it is null. According to the VarCharVector#set method, it will set the bytes using holder.buffer even if holder.isSet equals 0. This will lead to an exception.",pull-request-available,['Java'],ARROW,Bug,Minor,2019-06-13 10:28:29,7
13239223,[Java] Provide interfaces and initial implementations for vector sorting,"Data sorting is an important and common feature. For this issue, we provide the basic interfaces for vector sorting. Users can implement customized sorting algorithms by extending our interfaces. In addition, we also give initial sorting implementations for both fixed-width and variable-width vectors. ",pull-request-available,['Java'],ARROW,New Feature,Major,2019-06-13 09:42:44,7
13239179,[Java] shade flatbuffer dependency,"Reported in a [github issue|[https://github.com/apache/arrow/issues/4489]]



After some[discussion|https://github.com/google/flatbuffers/issues/5368]with the Flatbuffers maintainer, it appears that FB generated code is not guaranteed to be compatible with_any other_version of the runtime library other than the exact same version of the flatc used to compile it.

This makes depending on flatbuffers in a library (like arrow) quite risky, as if an app depends on any other version of FB, either directly or transitively, it's likely the versions will clash at some point and you'll see undefined behaviour at runtime.

Shading the dependency looks to me the best way to avoid this.",pull-request-available,['Java'],ARROW,Task,Major,2019-06-13 05:23:33,16
13239130,[C++] Link failure due to googletest shared library on Alpine Linux,"See

{code}
[314/538] Linking CXX shared library debug/libarrow_testing.so.14.0.0
FAILED: debug/libarrow_testing.so.14.0.0 
: && /usr/bin/ccache /usr/bin/g++ -fPIC -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -g   -shared -Wl,-soname,libarrow_testing.so.14 -o debug/libarrow_testing.so.14.0.0 src/arrow/CMakeFiles/arrow_testing_objlib.dir/io/test-common.cc.o src/arrow/CMakeFiles/arrow_testing_objlib.dir/ipc/test-common.cc.o src/arrow/CMakeFiles/arrow_testing_objlib.dir/filesystem/test-util.cc.o src/arrow/CMakeFiles/arrow_testing_objlib.dir/testing/gtest_util.cc.o src/arrow/CMakeFiles/arrow_testing_objlib.dir/testing/random.cc.o  -Wl,-rpath,/build/cpp/debug:/build/cpp/googletest_ep-prefix/src/googletest_ep/lib: debug/libarrow.so.14.0.0 googletest_ep-prefix/src/googletest_ep/lib/libgtestd.so double-conversion_ep/src/double-conversion_ep/lib/libdouble-conversion.a /usr/lib/libcrypto.so brotli_ep/src/brotli_ep-install/lib/libbrotlienc-static.a brotli_ep/src/brotli_ep-install/lib/libbrotlidec-static.a brotli_ep/src/brotli_ep-install/lib/libbrotlicommon-static.a glog_ep-prefix/src/glog_ep/lib/libglog.a -ldl jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -lrt && :
g++: error: googletest_ep-prefix/src/googletest_ep/lib/libgtestd.so: No such file or directory
[327/538] Building CXX object src/arrow/CMakeFiles/arrow-array-test.dir/array-test.cc.o
ninja: build stopped: subcommand failed.
{code}

There is erroneous logic in ThirdpartyToolchain.cmake -- I am fixing. ",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-12 21:48:17,14
13239127,[C++] Flaky thrift_ep tarball downloads,"I have noticed a high incidence of Thrift tarball download flaking out like

{code}
CMake Error at thrift_ep-stamp/thrift_ep-download-DEBUG.cmake:16 (message):
  Command failed: 1
   '/usr/local/cmake-3.12.4/bin/cmake' '-Dmake=' '-Dconfig=' '-P' '/home/travis/build/apache/arrow/cpp-build/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-download-DEBUG-impl.cmake'
  See also
    /home/travis/build/apache/arrow/cpp-build/thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-download-*.log
CMakeFiles/thrift_ep.dir/build.make:89: recipe for target 'thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-download' failed
make[2]: *** [thrift_ep-prefix/src/thrift_ep-stamp/thrift_ep-download] Error 1
CMakeFiles/Makefile2:471: recipe for target 'CMakeFiles/thrift_ep.dir/all' failed
make[1]: *** [CMakeFiles/thrift_ep.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
{code}

Any ideas about how to make this less flaky?",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-12 21:35:18,14
13239067,[C++] arrowConfig.cmake includes uninstalled targets,"I'm building a CMake project against arrow and I'm using:
{code:java}
find_package(arrow 0.13 CONFIG REQUIRED)
{code}
to get the arrow_shared target in scope. This works for me on macOS. I installed apache-arrow with:
{code:java}
brew install apache-arrow{code}
However, when I attempt to build the project in a ubuntu xenial container, I get the following CMake error:
{code:java}
CMake Error at /usr/lib/x86_64-linux-gnu/cmake/arrow/arrowTargets.cmake:151 (message):
The imported target ""arrow_cuda_shared"" references the file


""/usr/lib/x86_64-linux-gnu/libarrow_cuda.so.13.0.0""


but this file does not exist. Possible reasons include:


* The file was deleted, renamed, or moved to another location.


* An install or uninstall procedure did not complete successfully.


* The installation package was faulty and contained


""/usr/lib/x86_64-linux-gnu/cmake/arrow/arrowTargets.cmake""


but not all the files it references.


Call Stack (most recent call first):
/usr/lib/x86_64-linux-gnu/cmake/arrow/arrowConfig.cmake:61 (include)
CMakeLists.txt:15 (find_package)
{code}
I installed arrow with:
{code:java}
curl -sSL ""https://dist.apache.org/repos/dist/dev/arrow/KEYS"" | apt-key add -
echo ""deb [arch=amd64] https://dl.bintray.com/apache/arrow/ubuntu/ xenial main"" | tee -a /etc/apt/sources.list
apt-get update
apt-get install -y libarrow-dev=0.13.0-1
{code}
I can also install libarrow-cuda-dev, but I don't want to because I don't need it.",pull-request-available,['C++'],ARROW,Bug,Minor,2019-06-12 15:04:19,1
13239020,[Python] raise error message when passing invalid filter in parquet reading,"From https://stackoverflow.com/questions/56522977/using-predicates-to-filter-rows-from-pyarrow-parquet-parquetdataset

For example, when specifying a column in the filter which is a normal column and not a key in your partitioned folder hierarchy, the filter gets silently ignored. It would be nice to get an error message for this.  
Reproducible example:

{code:python}
df = pd.DataFrame({'a': [0, 0, 1, 1], 'b': [0, 1, 0, 1], 'c': [1, 2, 3, 4]})
table = pa.Table.from_pandas(df)
pq.write_to_dataset(table, 'test_parquet_row_filters', partition_cols=['a'])
# filter on 'a' (partition column) -> works
pq.read_table('test_parquet_row_filters', filters=[('a', '=', 1)]).to_pandas()
# filter on normal column (in future could do row group filtering) -> silently does nothing
pq.read_table('test_parquet_row_filters', filters=[('b', '=', 1)]).to_pandas()
{code}",dataset-parquet-read parquet pull-request-available,['Python'],ARROW,Bug,Minor,2019-06-12 12:30:15,5
13238940,[C++] import avro C++ code to code base.,"The goal here is to take code as is without compiling it, but flattening it to conform with Arrow's code base standards.  This will give a basis for future PR.",pull-request-available,['C++'],ARROW,Sub-task,Major,2019-06-12 06:13:30,15
13238914,[Python] Document how to use gdb when working on pyarrow,"It may not be obvious to new developers how to set breakpoints in the C++ libraries when driven from Python. The incantation is slightly abstruse, for example

{code}
$ gdb --args env py.test pyarrow/tests/test_array.py -k scalars_mixed_type
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-06-12 02:14:24,14
13238911,[C++] Add uriparser to conda-forge,uriparser is one of the holdouts in our toolchain that is having to be built from source. See also ARROW-5370 ,pull-request-available,"['C++', 'Developer Tools']",ARROW,Improvement,Major,2019-06-12 01:44:35,2
13238908,[Format] Update integration test JSON format documentation,This has slipped behind what is in the integration tests,pull-request-available,"['Format', 'Integration']",ARROW,Improvement,Major,2019-06-12 00:49:03,4
13238896,[C++][Parquet] parquet writer does not handle negative zero correctly,"

I have the following csv file: (Note that {{col_a}}contains a negative zero value.)
{code:java}
col_a,col_b
0.0,0.0
-0.0,0.0{code}
...and process it via:
{code:java}
from pyarrow import csv, parquet
in_csv = 'in.csv'
table = csv.read_csv(in_csv)
parquet.write_to_dataset(table, root_path='./'){code}


The output parquet file is then loaded into S3 and queried via AWS Athena (i.e. PrestoDB / Hive).

Any query that touches {{col_a}}fails with the following error:
{code:java}
HIVE_CANNOT_OPEN_SPLIT: Error opening Hive split {{REDACTED}} (offset=0, length=593): low must be less than or equal to high{code}


As a sanity check, I transformed the csv file to parquet using an AWS Glue Spark Job and I was able to query the output parquet file successfully.

As such, it appears as though the pyarrow writer is producing an invalid parquet file when a columncontains at least one instance of0.0, at least one instance of-0.0, and no other values.

",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-06-11 23:21:28,14
13238860,[C++] Introduce IpcOptions struct object for better API-stability when adding new options,Related to ARROW-2006. There are various IPC-related options like allowing 64-bit lengths that might be better encapsulated in an options struct rather than littered around different public APIs,pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-11 20:04:40,2
13238854,[C++] Support Array::View on arrays with non-zero offsets,Follow up work to initial implementation of {{Array::View}} in ARROW-1774,pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-11 19:37:57,14
13238839,[C++] Investigate performance of VisitBitsUnrolled on different platforms,"Investigate performance of `VisitBitsUnrolled` utility on different platforms, based on [this thread|https://github.com/apache/arrow/pull/4328#discussion_r292515822]",pull-request-available,['C++'],ARROW,Task,Minor,2019-06-11 18:40:34,2
13238836,[Doc] Document JSON reader,"The JSON reader API should be documented at least on the Python side.
See {{docs/source/python/csv.rst}} and {{docs/source/python/api/formats.rst}} for inspiration.",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2019-06-11 18:26:22,2
13238823,[R] Add install_arrow() function to assist the user in obtaining C++ runtime libraries,"FollowingARROW-5488, it will be possible to install the R package without having libarrow installed, but you won't be able to do anything until you do. The error message you get when trying to use the package directs you to call {{install_arrow()}}.

This function will at a minimum give a recommendation of steps to take to install the library. In some cases, we may be able to download and install it for the user.",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-11 17:30:54,4
13238787,[Ruby] red-arrow gem does not compile on ruby:2.5 docker image,"I'm attempting to install the red-arrow gem in a docker container based on the ruby:2.5 image on docker hub. I followed the debian instructions on [https://github.com/red-data-tools/packages.red-data-tools.org] to install libarrow-dev.



I then ran 'gem install red-arrow'

The output is as follows:

Building native extensions. This could take a while...
ERROR:  Error installing red-arrow:
	ERROR: Failed to build gem native extension.

    current directory: /usr/local/bundle/gems/red-arrow-0.13.0/ext/arrow
/usr/local/bin/ruby -I /usr/local/lib/ruby/site_ruby/2.5.0 -r ./siteconf20190611-1782-1gg8bpn.rb extconf.rb
checking --enable-debug-build option... no
checking C++ compiler... g++
checking g++ version... 6.3 (gnu++14)
checking for --enable-debug-build option... no
checking for -Wall option to compiler... yes
checking for -Waggregate-return option to compiler... yes
checking for -Wcast-align option to compiler... yes
checking for -Wextra option to compiler... yes
checking for -Wformat=2 option to compiler... yes
checking for -Winit-self option to compiler... yes
checking for -Wlarger-than-65500 option to compiler... yes
checking for -Wmissing-declarations option to compiler... yes
checking for -Wmissing-format-attribute option to compiler... yes
checking for -Wmissing-include-dirs option to compiler... yes
checking for -Wmissing-noreturn option to compiler... yes
checking for -Wmissing-prototypes option to compiler... yes
checking for -Wnested-externs option to compiler... yes
checking for -Wold-style-definition option to compiler... yes
checking for -Wpacked option to compiler... yes
checking for -Wp,-D_FORTIFY_SOURCE=2 option to compiler... yes
checking for -Wpointer-arith option to compiler... yes
checking for -Wswitch-default option to compiler... yes
checking for -Wswitch-enum option to compiler... yes
checking for -Wundef option to compiler... yes
checking for -Wout-of-line-declaration option to compiler... no
checking for -Wunsafe-loop-optimizations option to compiler... yes
checking for -Wwrite-strings option to compiler... yes
checking for Homebrew... no
checking for arrow... yes
checking for arrow-glib... yes
creating Makefile

current directory: /usr/local/bundle/gems/red-arrow-0.13.0/ext/arrow
make ""DESTDIR="" clean

current directory: /usr/local/bundle/gems/red-arrow-0.13.0/ext/arrow
make ""DESTDIR=""
compiling arrow.cpp
compiling record-batch.cpp
record-batch.cpp: In member function 'VALUE red_arrow::{anonymous}::StructArrayValueConverter::convert(const arrow::StructArray&, int64_t)':
record-batch.cpp:344:40: error: 'const class arrow::StructArray' has no member named 'struct_type'
         const auto struct_type = array.struct_type();
                                        ^~~~~~~~~~~
In file included from /usr/local/include/ruby-2.5.0/ruby/ruby.h:29:0,
                 from /usr/local/include/ruby-2.5.0/ruby.h:33,
                 from /usr/local/bundle/gems/glib2-3.3.6/ext/glib2/rbgobject.h:26,
                 from red-arrow.hpp:33,
                 from record-batch.cpp:20:
/usr/local/include/ruby-2.5.0/ruby/defines.h:105:57: error: void value not ignored as it ought to be
 #define RB_GNUC_EXTENSION_BLOCK(x) __extension__ ({ x; })
                                                         ^
/usr/local/include/ruby-2.5.0/ruby/intern.h:788:35: note: in expansion of macro 'RB_GNUC_EXTENSION_BLOCK'
 #define rb_utf8_str_new(str, len) RB_GNUC_EXTENSION_BLOCK( \
                                   ^~~~~~~~~~~~~~~~~~~~~~~
record-batch.cpp:350:18: note: in expansion of macro 'rb_utf8_str_new'
           key_ = rb_utf8_str_new(field_name.data(), field_name.length());
                  ^~~~~~~~~~~~~~~
record-batch.cpp: In member function 'uint8_t red_arrow::{anonymous}::UnionArrayValueConverter::compute_child_index(const arrow::UnionArray&, arrow::UnionType*, const char*)':
record-batch.cpp:516:66: error: no matching function for call to 'arrow::Status::Invalid(const char [18], const unsigned char&)'
         check_status(Status::Invalid(""Unknown type ID: "", type_id),
                                                                  ^
In file included from /usr/include/arrow/buffer.h:30:0,
                 from /usr/include/arrow/array.h:28,
                 from /usr/include/arrow/api.h:23,
                 from red-arrow.hpp:22,
                 from record-batch.cpp:20:
/usr/include/arrow/status.h:150:17: note: candidate: static arrow::Status arrow::Status::Invalid(const string&)
   static Status Invalid(const std::string& msg) {
                 ^~~~~~~
/usr/include/arrow/status.h:150:17: note:   candidate expects 1 argument, 2 provided
Makefile:234: recipe for target 'record-batch.o' failed
make: *** [record-batch.o] Error 1

make failed, exit code 2

Gem files will remain installed in /usr/local/bundle/gems/red-arrow-0.13.0 for inspection.
Results logged to /usr/local/bundle/extensions/x86_64-linux/2.5.0/red-arrow-0.13.0/gem_make.out




",pull-request-available,['Ruby'],ARROW,Bug,Major,2019-06-11 14:49:17,1
13238586,[Archery] should not return non-zero in `benchmark diff` sub command on regression,"When a regression is detected, but the command ran successfully, it should return zero. Currently it returns the number of regression. This is to play better with ursabot. It should be left to the user to decide what to do with the json data.",pull-request-available,[],ARROW,Improvement,Major,2019-06-10 17:03:51,13
13238287,"[Python] Support binary, utf8, and nested types in Array.from_buffers",See discussion in ARROW-2607,pull-request-available,['Python'],ARROW,Improvement,Major,2019-06-07 19:37:22,14
13238227,[C++] HashTable/MemoTable should use Buffer(s)/Builder(s) for heap data,"The current implementation uses `std::vector` and `std::string` with unbounded size. The refactor would take a memory pool in the constructor for buffer management and would get rid of vectors. This will have the side effect of propagating Status to some calls (notably insert due to Upsize failing to resize).

* MemoTable constructor needs to take a MemoryPool in input
* GetOrInsert must return Status/Result<int32_t>
* MemoTable should use a TypeBufferBuilder instead of std::vector<Payload>
* BinaryMemoTable should use a BinaryBuilder instead of (std::vector<int32_t>, std::string) pair.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-06-07 15:08:36,13
13238223,[Developer] Add more prominent notice to GitHub issue template to direct bug reports to JIRA,"Many people are deleting the issue template without reading it. I will change the template to feature a more prominent notice about filing bug reports in JIRA

Recent examples

* https://github.com/apache/arrow/issues/4489
* https://github.com/apache/arrow/issues/4495",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-06-07 14:50:15,14
13237938,[Python] [Packaging] Use HTTPS consistently for downloading dependencies,"Some download URLs in the manylinux scripts use plain-text protocols:

{code:bash}
python/manylinux1/scripts/build_thrift.sh:wget http://archive.apache.org/dist/thrift/${THRIFT_VERSION}/thrift-${THRIFT_VERSION}.tar.gz
python/manylinux1/scripts/build_python.sh:AUTOCONF_DOWNLOAD_URL=http://ftp.gnu.org/gnu/autoconf
python/manylinux1/scripts/build_python.sh:AUTOMAKE_DOWNLOAD_URL=http://ftp.gnu.org/gnu/automake
python/manylinux1/scripts/build_python.sh:LIBTOOL_DOWNLOAD_URL=http://ftp.gnu.org/gnu/libtool
python/manylinux1/scripts/build_bison.sh:wget http://ftp.gnu.org/gnu/bison/bison-3.0.4.tar.gz
python/manylinux1/scripts/build_re2.sh:curl -sL ""http://github.com/google/re2/archive/${RE2_VERSION}.tar.gz"" -o re2-${RE2_VERSION}.tar.gz
python/manylinux1/scripts/build_llvm.sh:curl -sL http://releases.llvm.org/${LLVM_VERSION}/llvm-${LLVM_VERSION}.src.tar.xz -o llvm-${LLVM_VERSION}.src.tar.xz
python/manylinux1/scripts/build_clang.sh:curl -sL http://releases.llvm.org/${LLVM_VERSION}/cfe-${LLVM_VERSION}.src.tar.xz -o cfe-${LLVM_VERSION}.src.tar.xz
{code}
",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2019-06-06 11:37:58,4
13237937,[Packaging][Documentation] Comments out of date in python/manylinux1/build_arrow.sh,"The script has this comment:
{code:java}
# Usage:
# docker run --rm -v $PWD:/io arrow-base-x86_64 /io/build_arrow.sh
{code}

However, I get:
{code}
Unable to find image 'arrow-base-x86_64:latest' locally
docker: Error response from daemon: pull access denied for arrow-base-x86_64, repository does not exist or may require 'docker login'.
See 'docker run --help'.
{code}",pull-request-available wheel,"['Documentation', 'Packaging', 'Python']",ARROW,Bug,Major,2019-06-06 11:33:58,3
13237936,[Packaging] License check fails with Apache RAT 0.13,"We currently use version 0.12. With 0.13 I get:
{code:java}
NOT APPROVED: js/src/fb/File.ts (xx/js/src/fb/File.ts): false
NOT APPROVED: js/src/fb/Message.ts (xx/js/src/fb/Message.ts): false
NOT APPROVED: js/src/fb/Schema.ts (xx/js/src/fb/Schema.ts): false
NOT APPROVED: js/test/inference/column.ts (xx/js/test/inference/column.ts): false
NOT APPROVED: js/test/inference/nested.ts (xx/js/test/inference/nested.ts): false
NOT APPROVED: js/test/inference/visitor/get.ts (xx/js/test/inference/visitor/get.ts): false
6 unapproved licences. Check rat report: rat.txt
{code}",pull-request-available,['Packaging'],ARROW,Bug,Trivial,2019-06-06 11:19:00,2
13237738,[C++] Header collection CMake logic should only consider filename without directory included,"If ""internal"" is in the directory name then all headers are currently excluded

See report at https://github.com/apache/arrow/issues/4469",pull-request-available,['C++'],ARROW,Bug,Major,2019-06-05 14:42:25,6
13237672,[C++] Printer for uint64 shows wrong values,"From the example in ARROW-5430:

{code}
In [16]: pa.array([14989096668145380166, 15869664087396458664], type=pa.uint64())                                                                                                                                   
Out[16]: 
<pyarrow.lib.UInt64Array object at 0x7ff7c51bdf48>
[
  -3457647405564171450,
  -2577079986313092952
]
{code}

I _think_ the actual conversion is correct, and it's only the printer that is going wrong, as {{to_numpy}} gives the correct values:

{code}
In [17]: pa.array([14989096668145380166, 15869664087396458664], type=pa.uint64()).to_numpy()                                                                                                                        
Out[17]: array([14989096668145380166, 15869664087396458664], dtype=uint64)
{code}",pull-request-available,['C++'],ARROW,Bug,Minor,2019-06-05 08:00:58,2
13237638,[Java] Refactor method name for getstartOffset to use camel case,"The method getstartOffset in classorg.apache.arrow.vector.BaseVariableWidthVector should be refactored to getStartOffset, to comply with the camel case.

Fortunately, this method is not public, so the changes are internal to Arrow.",pull-request-available,['Java'],ARROW,Improvement,Trivial,2019-06-05 04:23:48,7
13237629,[C++] Draft initial public APIs for Datasets project,"The objective of this is to ensure general alignment with the discussion document

https://docs.google.com/document/d/1bVhzifD38qDypnSjtf8exvpP3sSB5x_Kw9m-n66FB2c/edit?usp=sharing

so that an initial working implementation can begin to take place",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2019-06-05 03:16:42,14
13237596,[Packaging] Enable Flight in Conda packages,We should build Conda packages with Flight enabled.,pull-request-available,"['C++', 'Packaging', 'Python']",ARROW,Improvement,Blocker,2019-06-04 22:30:18,0
13237583,"[Format] Feather V2 based on Arrow IPC file format, with compression support","The initial Feather file format isa minimal subset ofthe Arrow IPC format. It has a number of limitations (see[https://wesmckinney.com/blog/feather-arrow-future/]).

We want to retain ""feather"" as the name of the on-disk representation of Arrow memory, so in order to support everything that Arrow supports, we need a ""feather 2.0"" format.

IIUC, defining the file format is ""done"" (dump the memory to disk). Remaining issues include upgrading ""feather"" readers and writers in all languages to support both feather 1.0 and feather 2.0. (e.g.https://issues.apache.org/jira/browse/ARROW-5501)",pull-request-available,['Format'],ARROW,Improvement,Major,2019-06-04 21:27:24,14
13237580,[R] write_parquet(),We can read but not yet write. The C++ library supports this and pyarrow does it.,pull-request-available,['R'],ARROW,Improvement,Major,2019-06-04 21:12:58,8
13237442,[Plasma] [CUDA] Compile error,"I'm starting getting this today:
{code}
../src/plasma/protocol.cc:546:55: error: no matching member function for call to 'CreateVector'
      handles.push_back(fb::CreateCudaHandle(fbb, fbb.CreateVector(handle)));
                                                  ~~~~^~~~~~~~~~~~
/home/antoine/miniconda3/envs/pyarrow/include/flatbuffers/flatbuffers.h:1484:27: note: candidate function not viable: no known conversion from 'std::shared_ptr<arrow::Buffer>' to 'const std::vector<bool>' for 1st argument
  Offset<Vector<uint8_t>> CreateVector(const std::vector<bool> &v) {
                          ^
/home/antoine/miniconda3/envs/pyarrow/include/flatbuffers/flatbuffers.h:1477:42: note: candidate template ignored: could not match 'vector' against 'shared_ptr'
  template<typename T> Offset<Vector<T>> CreateVector(const std::vector<T> &v) {
                                         ^
/home/antoine/miniconda3/envs/pyarrow/include/flatbuffers/flatbuffers.h:1443:42: note: candidate function template not viable: requires 2 arguments, but 1 was provided
  template<typename T> Offset<Vector<T>> CreateVector(const T *v, size_t len) {
                                         ^
/home/antoine/miniconda3/envs/pyarrow/include/flatbuffers/flatbuffers.h:1465:29: note: candidate function template not viable: requires 2 arguments, but 1 was provided
  Offset<Vector<Offset<T>>> CreateVector(const Offset<T> *v, size_t len) {
                            ^
/home/antoine/miniconda3/envs/pyarrow/include/flatbuffers/flatbuffers.h:1501:42: note: candidate function template not viable: requires 2 arguments, but 1 was provided
  template<typename T> Offset<Vector<T>> CreateVector(size_t vector_size,
                                         ^
/home/antoine/miniconda3/envs/pyarrow/include/flatbuffers/flatbuffers.h:1520:21: note: candidate function template not viable: requires 3 arguments, but 1 was provided
  Offset<Vector<T>> CreateVector(size_t vector_size, F f, S *state) {
                    ^
{code}",pull-request-available,"['C++ - Plasma', 'GPU']",ARROW,Bug,Critical,2019-06-04 10:00:24,2
13237361,[R] Stop masking base R functions/rethink namespacing,"The package startup message about masking base functions can be scary. We should avoid masking base functions without a compelling reason (i.e. let's do arrow_array() instead of array(), arrow_table()). The arrow versions do very different things than the base functions; plus, end users shouldnt be dealing directly with Tables and Arrays, so they dont need to figure so prominently in the public API of the package.",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-03 22:57:09,4
13237354,[R] Reorganize read/write file/stream functions,"read_feather and write_feather exist, and there is also write_arrow. But no read_arrow.

Some questions (which go beyond just R): There's talk of a ""feather 2.0"", i.e. ""just"" serializing the IPC format (which IIUC is what write_arrow does). Are we going to continue to call the file format ""Feather"", and possiblycontinue supporting the ""feather 1.0"" format as a subset/special case? Or will ""feather"" mean this limited format and ""arrow"" be the name of the full-featured file?

In terms of this issue, should write_arrow be folded into write_feather and there be an argument for indicating which version to write? Or should the distinction be maintained, and we need to add a read_arrow() function?",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-03 22:37:00,4
13237352,[R] read_csv_arrow() signature should match readr::read_csv(),"So that using it is natural for R users. Internally handle all of the logic needed to map those onto csv_convert_options, csv_read_options, and csv_parse_options. And give a useful error message if a user requests a setting that readr supports but arrow does not.",pull-request-available,['R'],ARROW,Improvement,Major,2019-06-03 22:19:34,4
13237310,[R][CI] Fix relative paths in R codecov.io reporting,"https://issues.apache.org/jira/browse/ARROW-5418added coverage stats for R, but due to an assumption in the coverage runner that the project would be at the top level of the GitHub repository, the `r/` subdirectory was not included, so R coverage stats were put in the wrong place, and detail files (such as[https://codecov.io/gh/apache/arrow/src/master/R/ArrayData.R]) return 404.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Minor,2019-06-03 19:38:45,4
13237295,[Python] Create FileSystem bindings,"Now that we have a C++ filesystem API, it should be usable from Python as well.",filesystem pull-request-available,['Python'],ARROW,Improvement,Major,2019-06-03 17:53:16,3
13237251,[C++] Remove ARROW_BOOST_HEADER_ONLY,That CMake variable isn't exposed as an option and probably doesn't work anymore. All code paths depending on that variable should probably be simplified.,pull-request-available,['C++'],ARROW,Task,Minor,2019-06-03 14:08:27,2
13237181,[CI] [Python] Failure in docs build,"This started failing on Travis-CI with Sphinx 2.1.0:
{code}
Warning, treated as error:
docstring of pyarrow.SerializationContext.register_type:0:Unknown target name: ""type"".
{code}

https://travis-ci.org/apache/arrow/jobs/540490192#L5631",pull-request-available,"['Continuous Integration', 'Documentation', 'Python']",ARROW,Bug,Major,2019-06-03 08:26:21,2
13237164,[Gandiva][Crossbow] OSx builds failing,OSX builds are failing for the last 3 days.,pull-request-available,['Packaging'],ARROW,Task,Major,2019-06-03 07:10:49,14
13237110,[Python] Pandas categorical type doesn't survive a round-trip through parquet,"Writing a string categorical variable to from pandas parquet is read back as string (object dtype). I expected it to be read as category.
The same thing happens if the category is numeric -- a numeric category is read back as int64.

In the code below, I tried out an in-memory arrow Table, which successfully translates categories back to pandas. However, when I write to a parquet file, it's not.

In the scheme of things, this isn't a big deal, but it's a small surprise.


{code:python}
import pandas as pd
import pyarrow as pa


df = pd.DataFrame({'x': pd.Categorical(['a', 'a', 'b', 'b'])})
df.dtypes  # category

# This works:
pa.Table.from_pandas(df).to_pandas().dtypes  # category

df.to_parquet(""categories.parquet"")
# This reads back object, but I expected category
pd.read_parquet(""categories.parquet"").dtypes  # object


# Numeric categories have the same issue:
df_num = pd.DataFrame({'x': pd.Categorical([1, 1, 2, 2])})
df_num.dtypes # category

pa.Table.from_pandas(df_num).to_pandas().dtypes  # category

df_num.to_parquet(""categories_num.parquet"")
# This reads back int64, but I expected category
pd.read_parquet(""categories_num.parquet"").dtypes  # int64
{code}



",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-06-02 16:58:54,14
13236982,[C++] Document required Boost version,"See debugging onhttps://issues.apache.org/jira/browse/ARROW-5470. One possible cause for that error is that the local filesystem patch increased the version of boost that we actually require. The boost version (1.54 vs 1.58) was one difference between failure and success.

Another point of confusion was that CMake reported two different versions of boost at different times.

If we require a minimum version of boost, can we document that better, check for it more accurately in the build scripts, and fail with a useful message if that minimum isn't met? Or something else helpful.

If the actual cause of the failure was something else (e.g. compiler version), we should figure that out too.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-31 22:28:20,14
13236949,[C++] Build failure on googletest_ep on Windows when using Ninja,"I consistently get this error when trying to use Ninja locally:

{code}
-- extracting...
     src='C:/Users/wesmc/code/arrow/cpp/build/googletest_ep-prefix/src/release-1.8.1.tar.gz'
     dst='C:/Users/wesmc/code/arrow/cpp/build/googletest_ep-prefix/src/googletest_ep'
-- extracting... [tar xfz]
-- extracting... [analysis]
-- extracting... [rename]
CMake Error at googletest_ep-stamp/extract-googletest_ep.cmake:51 (file):
  file RENAME failed to rename

    C:/Users/wesmc/code/arrow/cpp/build/googletest_ep-prefix/src/ex-googletest_ep1234/googletest-release-1.8.1

  to

    C:/Users/wesmc/code/arrow/cpp/build/googletest_ep-prefix/src/googletest_ep

  because: Directory not empty



[179/623] Building CXX object src\arrow\CMakeFiles\arrow_static.dir\array\builder_dict.cc.obj
ninja: build stopped: subcommand failed.
{code}

I'm running within cmdr terminal emulator so it's conceivable there's some path modifications that are causing issues.

The CMake invocation is

{code}
cmake -G ""Ninja"" ^          -DCMAKE_BUILD_TYPE=Release ^          -DARROW_BUILD_TESTS=on ^          -DARROW_CXXFLAGS=""/WX /MP"" ^
     -DARROW_FLIGHT=off -DARROW_PARQUET=on -DARROW_GANDIVA=ON -DARROW_VERBOSE_THIRDPARTY_BUILD=on     ..
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2019-05-31 19:05:24,6
13236907,[CI] C++ local filesystem patch breaks Travis R job,"https://issues.apache.org/jira/browse/ARROW-3144changed a C++ API and required downstream bindings to be updated. Romain wasn't immediately available to update R, so we marked the R job on Travis as an ""allowed failure"". That failure looked like this:[https://travis-ci.org/apache/arrow/jobs/538795366#L3711-L3830]The C++ library built fine, but then the R package failed to build because it didn't line up with what's in C++.

Then, the C++ local file system patch (https://issues.apache.org/jira/browse/ARROW-5378) landed. Travis passed,though we were still ignoring the R build, which continued to fail. But, it started failing differently. Here's what the R build failure looks like on that PR, and on master since then:[https://travis-ci.org/apache/arrow/jobs/539207245#L2520-L2640]The C++ library is failing to build, so we're not even getting to the expected R failure.

For reference, the ""C++ & GLib & Ruby w/ gcc 5.4"" build has the most similar setup to the R build, and it's still passing. One difference between the two jobs is that the GLib one has `ARROW_TRAVIS_USE_VENDORED_BOOST=1`, which sounds related to some open R issues, and `boost::filesystem` appears all over the error in the R job.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Blocker,2019-05-31 16:39:40,4
13236885,"[Java] Dockerize Java builds in Travis CI, run multiple JDKs in single entry",The JDK 9 and 11 builds are fast -- 4 minutes each. It would probably be more efficient to run all 3 JDK builds in a single build entry,pull-request-available,['Java'],ARROW,Improvement,Major,2019-05-31 14:59:18,14
13236874,[Crossbow] Support writing submitted job definition yaml to a file,In similar fashion like archery benchmark does. Required to consume the command's output from a buildbot build step.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-05-31 14:36:25,3
13236776,[Java] Add micro-benchmarks for Float8Vector and allocators,"For the past days, we have been involved in some performance related issues. In this process, we have created some performance benchmarks, to help us verify performance results.

Now we want to add such micro-benchmarks to the code base, in the hope that they will be helpful for making performance-related decisions and avoid performance degradation.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-05-31 07:51:05,7
13236719,[C++] Just-released cmake-format 0.5.2 breaks the build,It seems we should always pin the cmake-format version until the developers stop changing the formatting algorithm,pull-request-available,['C++'],ARROW,Bug,Blocker,2019-05-30 23:59:49,14
13236556,[R] Add documentation website (pkgdown),pkgdown ([https://pkgdown.r-lib.org/]) is the standard for R package documentation websites. Build this for arrow and deploy it at https://arrow.apache.org/docs/r.,pull-request-available,"['Documentation', 'R']",ARROW,Improvement,Major,2019-05-30 17:59:05,4
13236551,[Python] TimestampArray.to_pylist() fails with OverflowError: Python int too large to convert to C long,"When I attempt to roundtrip from a list of moderately large (beyond what can be represented in nanosecond precision, but within microsecond precision) datetime objects to pyarrow and back, I get anOverflowError: Python int too large to convert to C long.

pyarrow version:
{noformat}
$ pip freeze | grep pyarrow
pyarrow==0.13.0{noformat}


Reproduction:
{code:java}
import datetime

import pandas
import pyarrow
import pytz


timestamp_rows = [
datetime.datetime(1, 1, 1, 0, 0, 0, tzinfo=pytz.utc),
None,
datetime.datetime(9999, 12, 31, 23, 59, 59, 999999, tzinfo=pytz.utc),
datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=pytz.utc),
]
timestamp_array = pyarrow.array(timestamp_rows, pyarrow.timestamp(""us"", tz=""UTC""))
timestamp_roundtrip = timestamp_array.to_pylist()


# ---------------------------------------------------------------------------
# OverflowError Traceback (most recent call last)
# <ipython-input-25-4a798e917c20> in <module>
# ----> 1 timestamp_roundtrip = timestamp_array.to_pylist()
#
# ~/.pyenv/versions/3.6.4/envs/scratch/lib/python3.6/site-packages/pyarrow/array.pxi in __iter__()
#
# ~/.pyenv/versions/3.6.4/envs/scratch/lib/python3.6/site-packages/pyarrow/scalar.pxi in pyarrow.lib.TimestampValue.as_py()
#
# ~/.pyenv/versions/3.6.4/envs/scratch/lib/python3.6/site-packages/pyarrow/scalar.pxi in pyarrow.lib._datetime_conversion_functions.lambda5()
#
# pandas/_libs/tslibs/timestamps.pyx in pandas._libs.tslibs.timestamps.Timestamp.__new__()
#
# pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.convert_to_tsobject()
#
# OverflowError: Python int too large to convert to C long
{code}
For good measure, I also tested with timezone-naive timestamps with the same error:
{code:java}
naive_rows = [
datetime.datetime(1, 1, 1, 0, 0, 0),
None,
datetime.datetime(9999, 12, 31, 23, 59, 59, 999999),
datetime.datetime(1970, 1, 1, 0, 0, 0),
]
naive_array = pyarrow.array(naive_rows, pyarrow.timestamp(""us"", tz=None))
naive_roundtrip = naive_array.to_pylist()

# ---------------------------------------------------------------------------
# OverflowError Traceback (most recent call last)
# <ipython-input-27-0c32e563d44a> in <module>
# ----> 1 naive_roundtrip = naive_array.to_pylist()
#
# ~/.pyenv/versions/3.6.4/envs/scratch/lib/python3.6/site-packages/pyarrow/array.pxi in __iter__()
#
# ~/.pyenv/versions/3.6.4/envs/scratch/lib/python3.6/site-packages/pyarrow/scalar.pxi in pyarrow.lib.TimestampValue.as_py()
#
# ~/.pyenv/versions/3.6.4/envs/scratch/lib/python3.6/site-packages/pyarrow/scalar.pxi in pyarrow.lib._datetime_conversion_functions.lambda5()
#
# pandas/_libs/tslibs/timestamps.pyx in pandas._libs.tslibs.timestamps.Timestamp.__new__()
#
# pandas/_libs/tslibs/conversion.pyx in pandas._libs.tslibs.conversion.convert_to_tsobject()
#
# OverflowError: Python int too large to convert to C long
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-05-30 17:50:27,14
13236508,[C++] Local filesystem implementation: investigate Windows UNC paths,"Followup to ARROW-5378: Windows paths to networked files (e.g. ""\\server\share\path\file.txt"") and extended-length paths (e.g. ""\\?\c:\some\absolute\path.txt"") should be checked for compatibility with the LocalFileSystem implementation.",pull-request-available,['C++'],ARROW,Task,Major,2019-05-30 14:23:54,2
13236493,[CI] MinGW build failures on AppVeyor,"Apparently the Numpy package is broken. See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/24922425/job/9yoq08uepk5p6dwb

{code}
-- Found PythonLibs: C:/msys64/mingw32/lib/libpython3.7m.dll.a
CMake Error at cmake_modules/FindNumPy.cmake:62 (message):
  NumPy import failure:
  Traceback (most recent call last):
    File ""C:/msys64/mingw32/lib/python3.7/site-packages\numpy\core\__init__.py"", line 40, in <module>
      from . import multiarray
    File ""C:/msys64/mingw32/lib/python3.7/site-packages\numpy\core\multiarray.py"", line 12, in <module>
      from . import overrides
    File ""C:/msys64/mingw32/lib/python3.7/site-packages\numpy\core\overrides.py"", line 6, in <module>
      from numpy.core._multiarray_umath import (
  ImportError: DLL load failed: The specified module could not be found.
  
{code}",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2019-05-30 13:42:17,1
13236482,[CI] [Ruby] CI is failed on AppVeyor,"This happens sometimes.
{code:java}
Error: test: csv.gz(TableTest::#save and .load::path:::format::load: auto detect): Arrow::Error::Io: [csv-reader][read]: IOError: zlib inflate failed: invalid distance too far back
c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.3.6/lib/gobject-introspection/loader.rb:616:in `invoke'
c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.3.6/lib/gobject-introspection/loader.rb:616:in `invoke'
c:/Ruby26-x64/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.3.6/lib/gobject-introspection/loader.rb:533:in `block in define_method'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:158:in `block (2 levels) in load_from_path'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:147:in `block (2 levels) in wrap_input'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:140:in `open_encoding_convert_stream'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:146:in `block in wrap_input'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:125:in `block in open_decompress_input'
C:/projects/arrow/ruby/red-arrow/lib/arrow/block-closable.rb:25:in `open'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:124:in `open_decompress_input'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:145:in `wrap_input'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:157:in `block in load_from_path'
C:/projects/arrow/ruby/red-arrow/lib/arrow/block-closable.rb:25:in `open'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:156:in `load_from_path'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:39:in `load'
C:/projects/arrow/ruby/red-arrow/lib/arrow/csv-loader.rb:26:in `load'
C:/projects/arrow/ruby/red-arrow/lib/arrow/table-loader.rb:158:in `load_as_csv'
C:/projects/arrow/ruby/red-arrow/lib/arrow/table-loader.rb:50:in `load'
C:/projects/arrow/ruby/red-arrow/lib/arrow/table-loader.rb:22:in `load'
C:/projects/arrow/ruby/red-arrow/lib/arrow/table.rb:27:in `load'
C:/projects/arrow/ruby/red-arrow/test/test-table.rb:503:in `block (5 levels) in <class:TableTest>'
===============================================================================

{code}
https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/24813909/job/kkc98r3e4ltxeor3#L2328",pull-request-available,"['Continuous Integration', 'Ruby']",ARROW,Improvement,Major,2019-05-30 13:06:23,1
13236346,"[Website] Clarify what makes a release artifact ""official""","See discussion here:[https://github.com/apache/arrow/pull/4401#discussion_r288348562]

In order to minimize FUD, add some explanation and don't shout ""unofficial"" everywhere.[https://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support?action=show&redirect=Distribution]is one model.",pull-request-available,['Website'],ARROW,Improvement,Minor,2019-05-29 21:50:55,4
13236295,[C++] Implement FindArrowFlight.cmake,"This is required so that downstream CMake users can write:
{code}
find_package(ArrowFlight)
{code}

to find the Flight library paths. In turn this will allow {{PYARROW_BUNDLE_ARROW_CPP}} to work with Arrow Flight.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2019-05-29 17:24:43,2
13236234,[Java] Utilize stream EOS in File format,"We currently do not write EOS at the end of a Message stream inside the File format. As a result, the file cannot be parsed sequentially. This change prepares for other implementations or future referencefeaturesthat parse a File sequentially... i.e. without access to seek().",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-05-29 12:37:41,16
13236225,[Python] Missing pandas pytest marker from parquet tests,So the test suite fails without pandas installed: https://travis-ci.org/ursa-labs/crossbow/builds/538694001,pull-request-available,['Python'],ARROW,Bug,Major,2019-05-29 12:18:36,3
13236209,[Python] expose filters argument in parquet.read_table,"Currently, the {{parquet.read_table}} function can be used both for reading a single file (interface to ParquetFile) as a directory (interface to ParquetDataset). 

ParquetDataset has some extra keywords such as {{filters}} that would be nice to expose through {{read_table}} as well.

Of course one can always use {{ParquetDataset}} if you need its power, but for pandas wrapping pyarrow it is easier to be able to pass through keywords just to {{parquet.read_table}} instead of calling either {{read_table}} or {{ParquetDataset}}. Context: https://github.com/pandas-dev/pandas/issues/26551",parquet pull-request-available,['Python'],ARROW,Improvement,Major,2019-05-29 10:08:33,5
13236117,[Python] Add 'read_at' method to pyarrow.NativeFile,"We'd like to open a file (using HadoopFileSystem) and then read a block of data specified by the offset and the length. We don't want any read ahead to happen.

The libhdfs driver supprots hdfsPread, and it exposes a ReadAt method in the RandomAccessFile interface. However, there is no way to access it from python.

We add a read_at method to pyarrow.NativeFile that would expose this functionality.",pull-request-available,['Python'],ARROW,New Feature,Blocker,2019-05-28 22:10:26,2
13235894,[Java] Provide alternative buffer allocation policy,"The current buffer allocation policy works like this:
 * If the requested buffer size is greater than or equal to the chunk size, the buffer size will be as is.
 * If the requested size is within the chunk size, the buffer size will be rounded to the next power of 2.

This policy can lead to waste of memory in some cases. For example, if we request a buffer of size 10MB, Arrow will round the buffer size to 16 MB. If we only need 10 MB, this will lead to a waste of (16 - 10) / 10 = 60% of memory.

So in this proposal, we provide another policy: the rounded buffer size must be a multiple of some memory unit, like (32 KB). This policy has two benefits:
 # The wasted memory cannot exceed one memory unit (32 KB), which is much smaller than the power-of-two policy.
 # This is the memory allocation policy adopted by some computation engines (e.g. Apache Flink).",pull-request-available,['Java'],ARROW,Improvement,Major,2019-05-28 02:51:54,7
13235865,[Python] RangeIndex serialization change implications,"In 0.13, the conversion of a pandas DataFrame's RangeIndex changed: it is no longer serialized as an actual column in the arrow table, but only saved as metadata (in the pandas metadata) (ARROW-1639).

This change lead to a couple of issues:

- It can sometimes be unpredictable in pandas when you have a RangeIndex and when not. Which means that the resulting schema in arrow can be somewhat unexpected. See ARROW-5104: empty DataFrame has RangeIndex or not depending on how it was created
- The metadata is not always enough (or not updated) to reconstruct it when the table has been modified / subsetted.  
  For example, ARROW-5138: retrieving a single row group from parquet file doesn't restore index properly (since the RangeIndex metadata was for the full table, not this subset)
  And another one, ARROW-5139: empty column selection no longer restores index.

I think we should decide if we either want to try to fix those (or give an option to avoid those issues), or either close those as ""won't fix"".

One idea I had that could potentially alleviate some of those issues:

- Make it possible for the user to still force actual serialization of the index, always, even if it is a RangeIndex.
- To not introduce a new option, we could reuse the {{preserve_index}} keyword: change the default to None (which means the current behaviour), and change {{True}} to mean ""always serialize"" (although this is not fully backwards compatible with 0.13.0 for those users who explicitly specified the keyword).

I am not sure this is worth the added complexity (although I personally like providing the option where the index is simply always serialized as columns, without surprises). But ideally we decide on it for 0.14, to either fix or close the mentioned issues.",pull-request-available,['Python'],ARROW,Improvement,Blocker,2019-05-27 19:13:47,5
13235836,[CI] [C++] Build failure with Google Benchmark,"See e.g. https://travis-ci.org/apache/arrow/jobs/537746871#L3425
{code}
ninja: error: '/home/conda/feedstock_root/build_artifacts/benchmark_1558935667936/_build_env/x86_64-conda_cos6-linux-gnu/sysroot/usr/lib/librt.so', needed by 'debug/arrow-column-benchmark', missing and no known rule to make it
{code}

This happens with the new ""benchmark"" package from conda-forge, so we should downgrade to 1.4.1 for the time being.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2019-05-27 14:56:05,2
13235833,[Packaging][Crossbow] Duplicated key in nightly test configuration,"""docker-python-3.6-alpine"" key is duplicated in dev/tasks/tests.yml",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-05-27 14:31:08,3
13235725,[Java] Implement or remove getCurrentSizeInBytes in VariableWidthVector,NowVariableWidthVector#getCurrentSizeInBytes doesn't seem to have been implemented. We should implement it or just remove it.,pull-request-available,['Java'],ARROW,Improvement,Minor,2019-05-27 06:40:29,16
13235620,[C++] CSV strings_can_be_null option doesn't respect all null_values,"Relates toARROW-5195and[https://github.com/apache/arrow/issues/4184]

I was testing the new *strings_can_be_null* ConvertOption (built from git 184b8deb651c6f6308c0fa2a595f5a40f5da8ce8) in conjunction with the CSV reader and noted that when enabled and an empty string is parsed that it doesn't return NULL despite '' being in the default null_values list ([https://github.com/apache/arrow/blob/f7ef65e5fc367f1f5649dfcea0754e413fcca394/cpp/src/arrow/csv/options.cc#L28)]
{code:java}
options.null_values = {"""", ""#N/A"", ""#N/A N/A"", ""#NA"", ""-1.#IND"", ""-1.#QNAN"",
""-NaN"", ""-nan"", ""1.#IND"", ""1.#QNAN"", ""N/A"", ""NA"",
""NULL"", ""NaN"", ""n/a"", ""nan"", ""null""};
{code}
Given that the *strings_can_be_null*option was added to expose the same NULL processing functionality with respect to strings as*pandas.read_csv,*I believe that it should also be able to handle empty strings.**

In Pandas:
{code:java}
content = b""a,b\n1,null\n2,\n3,test""
df = pd.read_csv(io.BytesIO(content))
print(df)
 a   b
0 1  NaN
1 2  NaN
2 3 test
{code}
In PyArrow:
{code:java}
convert_options = pc.ConvertOptions(strings_can_be_null=True)
table = pc.read_csv(io.BytesIO(content), convert_options=convert_options)
print(table.to_pydict())
OrderedDict([('a', [1, 2, 3]), ('b', [None, '', 'test'])])
{code}


",csv pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2019-05-25 19:29:04,2
13235452,[Website] Add Homebrew to project installation page,cf. https://github.com/apache/arrow/pull/4322#discussion_r287152110,pull-request-available,['Website'],ARROW,Improvement,Minor,2019-05-24 15:59:27,4
13235451,[Release] Release script should update R version everywhere,"See [https://github.com/apache/arrow/pull/4322#discussion_r287151330.]There are probably other places that should be updated (NEWS.md, which doesn't yet exist but needs to).",pull-request-available release,['R'],ARROW,Improvement,Major,2019-05-24 15:55:45,4
13235423,"[C++] Using ""Ninja"" build system generator overrides default Release build type on Windows","Ran into this infuriating issue today. See gist

https://gist.github.com/wesm/c3dd87279ec20b2f2d12665fd264bfef

The cmake invocation that produces this is

{code}
cmake -G ""Ninja"" ^
      -DCMAKE_INSTALL_PREFIX=%ARROW_HOME% ^
      -DARROW_BUILD_TESTS=on ^
      -DARROW_CXXFLAGS=""/WX /MP"" ^
      -DARROW_GANDIVA=on ^
      -DARROW_PARQUET=on ^
      -DARROW_PYTHON=on ^
      ..
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-05-24 14:31:19,6
13235391,[C++] CSV reader doesn't remove BOM,"If aCSV file starts with a byte-order mark,CSV reader doesn't strip itbut instead adds itto the first column name.",csv pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-24 13:30:09,2
13235169,[C++] Integration test Travis CI entry builds many unnecessary targets,Only the IPC and Flight integration test targets are needed to run the tests. It appears that all targets including all unit tests are being built in Travis,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-23 15:29:24,2
13235136,[C++] nonstd::string_view conflicts with std::string_view in c++17,"From GitHub issue https://github.com/apache/arrow/issues/4294

Our vendored string_view header will forward to {{std::string_view}} if that is available. This can produce ABI conflicts and build errors when arrow or applications which use it are built against c++17.

I think it's acceptable to just force usage of nonstd's implementation with {{#define nssv_CONFIG_SELECT_STRING_VIEW nssv_STRING_VIEW_NONSTD}}",pull-request-available,['C++'],ARROW,New Feature,Minor,2019-05-23 13:24:34,6
13235119,[C++] Test failures not propagated in Windows shared builds,"See https://github.com/google/googletest/issues/2261

Try e.g. this change:
{code}
diff --git a/cpp/src/arrow/buffer-test.cc b/cpp/src/arrow/buffer-test.cc
index 9b0530e5c..ce7628f55 100644
--- a/cpp/src/arrow/buffer-test.cc
+++ b/cpp/src/arrow/buffer-test.cc
@@ -35,6 +35,10 @@
 namespace arrow {

 TEST(TestAllocate, Bitmap) {
+  auto buf1 = Buffer::FromString(""a"");
+  auto buf2 = Buffer::FromString(""b"");
+  AssertBufferEqual(*buf1, *buf2);
+
   std::shared_ptr<Buffer> new_buffer;
   ARROW_EXPECT_OK(AllocateBitmap(default_memory_pool(), 100, &new_buffer));
   EXPECT_GE(new_buffer->size(), 13);
{code}

On a Windows shared library build, it outputs this:
{code}
[==========] Running 31 tests from 11 test cases.
[----------] Global test environment set-up.
[----------] 2 tests from TestAllocate
[ RUN      ] TestAllocate.Bitmap
..\src\arrow\testing\gtest_util.cc(120): error: Value of: buffer.Equals(expected
)
  Actual: false
Expected: true
[       OK ] TestAllocate.Bitmap (0 ms)
[ RUN      ] TestAllocate.EmptyBitmap
[       OK ] TestAllocate.EmptyBitmap (0 ms)
[----------] 2 tests from TestAllocate (0 ms total)
{code}
.... and the entire test file is marked passed.",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-05-23 12:14:49,14
13235080,[CI] [C++] Print ccache statistics on Travis-CI,This would allow to know if compilation caching is really in effect.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Wish,Major,2019-05-23 08:39:49,2
13235058,[Rust] [Testing] Add IPC test files to arrow-testing,"We're generating a lot of files for testing, which should ideally live in arrow-testing",pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-05-23 07:10:16,12
13235021,[Python] Flight tests broken by URI changes,"The URI changes merged cleanly but they hadn't been rebased so this is happening

https://travis-ci.org/apache/arrow/jobs/535981561#L5267",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2019-05-23 01:03:29,2
13234971,Test Flight TLS support ,TLS support is not tested in Flight. We need to generate certificates/keys and provide them to the language-specific test runners.,pull-request-available,['FlightRPC'],ARROW,Test,Minor,2019-05-22 18:30:26,0
13234672,[CI] Job time limit exceeded on Travis,"We now frequently hit the 50 minutes job time limit on Travis-CI on the ""Python 2.7 and 3.6 unit tests w/ Valgrind, conda-forge toolchain, coverage"" job.

e.g. https://travis-ci.org/pitrou/arrow/jobs/535373888

Hopefully we can soon ditch Python 2.7, which would allow saving a bit of time.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2019-05-21 17:03:10,2
13234649,[C++] Add an internal temporary directory API,This is needed to easily write tests involving filesystem operations.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-21 15:28:52,2
13234555,Making the rounding behavior of the buffer capacity configurable,"In our scenario, the following code snippet is frequent in our code base:

int requestSize = ...;
 if (requestSize <= allocator.getLimit() - allocator.getAllocatedMemory())

{ ArrowBuf buffer = allocator.buffer(requestSize); }

However, it often causes OutOfMemoryException, due to Arrow's rounding behavior.

For example, we have only 12 MB memory left, and we request a buffer with size 10 MB. Appearantly, there is sufficient memory to meet the request. However, the rounding behavior rounds the request size from 10 MB to 16 MB, and there is no 16 MB memory, so an OutOfMemoryException will be thrown.

We propose two ways to solve this problem:

1. We provide a rounding option as an argument to the BaseAllocator#buffer method. There are two possible values for the rounding option: rounding up and rounding down. In the above scenario, the rounding down option can solve the problem.

2. We add a method to the allocator:

int getRoundedSize(final int size, BaseAllocator.AllocationRoundingOption roundingOption)

This method will give the rounded buffer size, given the initial request size. With this method, the user can freely adjust their request size to avoid OOM.

To make it more convenient to use Arrow, we think both solutionsshould be implemented.

",pull-request-available,['Java'],ARROW,Improvement,Major,2019-05-21 10:17:24,7
13234459,[C++] Fix and enable UBSan for unaligned accesses.,"Currently unaligned access configuration in UBSan has been turned off.  We should introduce a method that safely loads unaligned data, and use it to fix UBSan errors.  Then turn them on for UBSan.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-21 04:39:23,15
13234313,[C++] Make IpcPayload public and add GetPayloadSize,"As discussed in recent mailing list thread

https://lists.apache.org/thread.html/b756209052fecb8c28a5eb37db7aecb82a5f5351fa79a9d86f0dba3e@%3Cuser.arrow.apache.org%3E

The only viable process at the moment for getting an accurate report of stream size is to write a simulated stream using {{MockOutputStream}}. This is suboptimal for a couple of reasons:

* Flatbuffers metadata must be created twice
* Record batch disassembly into IpcPayload must be performed twice

It seems like an interface with a very constrained public API could be provided to deconstruct a sequence of RecordBatches and report the size of the produced IPC stream (based on metadata sizes, and padding), and then this deconstructed set of IPC payloads can be written out to a stream (e.g. using {{FixedSizeBufferWriter}})",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-20 13:23:39,0
13234309,[C++] Compile failure on gcc 5.4.0,"{code}
In file included from ../src/arrow/filesystem/test-util.h:22:0,
                 from ../src/arrow/filesystem/test-util.cc:26:
../src/arrow/filesystem/filesystem.h:58:1: error: type attributes ignored after type is already defined [-Werror=attributes]
 };
 ^
{code}

This is a bug in gcc 5.4.0: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=43407",pull-request-available,['C++'],ARROW,Bug,Major,2019-05-20 13:20:32,2
13234247,[Python] Misleading error message when calling pyarrow.read_record_batch on a complete IPC stream,"{code:python}
>>> batch = pa.RecordBatch.from_arrays([pa.array([b""foo""], type=pa.utf8())], names=['strs'])                                                                   
>>> stream = pa.BufferOutputStream()
>>> writer = pa.RecordBatchStreamWriter(stream, batch.schema)
>>> writer.write_batch(batch)                                                                                                                                  
>>> writer.close()                                                                                                                                             
>>> buf = stream.getvalue()                                                                                                                                    
>>> pa.read_record_batch(buf, batch.schema)                                                                                                                    
Traceback (most recent call last):
  File ""<ipython-input-31-4f168f453f3a>"", line 1, in <module>
    pa.read_record_batch(buf, batch.schema)
  File ""pyarrow/ipc.pxi"", line 583, in pyarrow.lib.read_record_batch
    check_status(ReadRecordBatch(deref(message.message.get()),
  File ""pyarrow/error.pxi"", line 87, in pyarrow.lib.check_status
    raise ArrowIOError(message)
ArrowIOError: Expected IPC message of type schema got record batch

{code}
",beginner pull-request-available,['Python'],ARROW,Bug,Major,2019-05-20 09:50:51,14
13234116,[C++] Detect system uriparser by default,We can use uriparser 0.9.2 or later.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-18 12:37:05,1
13234096,[Rust] Implement Duration and Interval Arrays,"This should ideally includecovering:
 * data types
 * arrays and builders
 * adding to kernels (e.g. including support in cast)",pull-request-available,['Rust'],ARROW,Sub-task,Minor,2019-05-18 06:03:56,12
13234090,[C++][CI] Add UBSan and ASAN into CI,We should be running UBSan and ASAN in CI to detect issues with the C++ build.  ,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-05-18 02:40:21,15
13234012,[C++] Compression round trip test can cause some sanitizers to to fail ,"The compression round trip test, relies on the fact that capacity doesn't decrease even if size does via a resize.  Some sanitizers can detect writes past ""size"" as invalid memory access.",pull-request-available,['C++'],ARROW,Bug,Minor,2019-05-17 17:37:24,15
13234003,[R] Follow DictionaryType/DictionaryArray changes from ARROW-3144,The R build is broken until this work is done. See[https://travis-ci.org/apache/arrow/jobs/533775912#L3021-L3235]for the error.,pull-request-available,['R'],ARROW,Improvement,Blocker,2019-05-17 16:42:18,4
13233976,[Rust] Builds are broken by rustyline on nightly 2019-05-16+,"Rust builds are broken on nightly since 2019-05-16. Please see[https://github.com/kkawakam/rustyline/issues/217]

The issue might need to be fixed on the rustyline crate.",pull-request-available,['Rust - DataFusion'],ARROW,Bug,Critical,2019-05-17 14:32:29,12
13233840,[C++] allow Array to have null buffers when all elements are null,"In the case of all elements of an array being null, no buffers whatsoever *need* to be allocated (similar to NullArray). This is a more extreme case of the optimization which allows the null bitmap buffer to be null if all elements are valid. Currently {{arrow::Array}} requires at least a null bitmap buffer to be allocated (and all bits set to 0).",pull-request-available,['C++'],ARROW,New Feature,Minor,2019-05-16 18:47:50,6
13233820,[Rust] Add support for take kernel functions,"Similar tohttps://issues.apache.org/jira/browse/ARROW-772, a take function would allow us random-access on arrays, which is useful for sorting and (potentially) filtering.

",pull-request-available,['Rust'],ARROW,New Feature,Major,2019-05-16 16:55:41,12
13233816,[Rust] Support filtering on primitive/string lists,"We currently only filter on primitive types, but not on lists and structs. Add the ability to filter on nested array types",pull-request-available,['Rust'],ARROW,Improvement,Major,2019-05-16 16:28:38,12
13233778,[Python/C++] Provide a way to specify the file path in parquet ColumnChunkMetaData,"After ARROW-5258 / https://github.com/apache/arrow/pull/4236 it is now possible to collect the file metadata while writing different files (then how to write those metadata was not yet addressed -> original issue ARROW-1983).

However, currently, the {{file_path}} information in the ColumnChunkMetaData object is not set. This is, I think, expected / correct for the metadata as included within the single file; but for using the metadata in the combined dataset `_metadata`, it needs a file path set.

So if you want to use this metadata for a partitioned dataset, there needs to be a way to specify this file path. 
Ideas I am thinking of currently: either, we could specify a file path to be used when writing, or expose the `set_file_path` method on the Python side so you can create an updated version of the metadata after collecting it.

cc [~pearu] [~mdurant]",parquet pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2019-05-16 13:51:45,14
13233748,[CI] [Java] Gandiva checkstyle failure,"This is failing Travis-CI builds now:
{code}
[WARNING] src/main/java/org/apache/arrow/gandiva/evaluator/Projector.java:[145,3] (javadoc) JavadocMethod: Missing a Javadoc comment.
[WARNING] src/main/java/org/apache/arrow/gandiva/evaluator/DecimalTypeUtil.java:[38,3] (javadoc) JavadocMethod: Missing a Javadoc comment.

[...]

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project arrow-gandiva: You have 2 Checkstyle violations. -> [Help 1]
{code}",pull-request-available,"['C++ - Gandiva', 'Continuous Integration', 'Java']",ARROW,Bug,Blocker,2019-05-16 12:08:34,15
13233638,[C++] Revert changes to qualify duration in vendored date code,"The changes where made to get PR https://github.com/apache/arrow/pull/3644 compiling without using ""duration"" type constructor.  Vendored code changes are bad ... ",pull-request-available,['C++'],ARROW,Task,Major,2019-05-15 23:32:06,15
13233616,[C++] Use ArrayDataVisitor in implementation of dictionary unpacking in compute/kernels/cast.cc,Follow-up to code review from ARROW-3144,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-15 22:15:08,2
13233614,[C++] Consider using Buffer for transpose maps in DictionaryType::Unify instead of std::vector,"In the spirit of ""track all the allocations"", if dictionaries have non-trivial length, we may want to account for this memory more precisely. ",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-15 22:06:24,14
13233612,[Format] Formalize extension type metadata in IPC protocol,"On the heels of ARROW-585 and ARROW-5255, it has been proposed to add reserved keys to {{custom_metadata}} on the {{Field}} Flatbuffer type to pass along the name of a custom/user-defined data type and any serialized data required to reconstruct that data type. 

The objective of this is to enable library users to define new data types that are embedded semantically in pre-defined Arrow data types like Binary or FixedSizeBinary. For example, UUID could be defined as an extension type atop FixedSizeBinary(16) (though UUID may be important enough to formalize as a logical type at some point...)

I will propose language to add to Metadata.rst and start a discussion on the mailing list to possibly call a vote",pull-request-available,['Format'],ARROW,New Feature,Major,2019-05-15 22:03:13,14
13233608,[C++] Add instructions about fixing and testing for -Wdocumentation clang warnings locally,"I found that I had to add {{-Wdocumentation}} to {{ARROW_CXXFLAGS}} to replicate docstring-related failures in CI. I'm not sure why this is being set in Travis CI but not locally (with BUILD_WARNING_LEVEL=CHECKIN), but we should make it consistent and also document that it is a requirement in the C++ development Sphinx documentation",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-15 21:49:59,6
13233604,[C++] Add jemalloc to thirdparty dependency download script,I encountered this issue when doing development on an airlines that I did not obtain jemalloc when running thirdparty/download_dependencies.sh,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-15 21:25:56,14
13233600,[Python] Raise on variable dictionaries when converting to pandas,"Address after ARROW-3144. The current code presumes the dictionary is the same for all chunks. We should check if the dictionary is the same for all chunks, and if not, raise an exception. Dictionary unification will have to occur in a follow up patch",pull-request-available,['Python'],ARROW,Improvement,Blocker,2019-05-15 21:20:32,5
13233599,"[C++] Add ""Type"" to names of arrow::Integer, arrow::FloatingPoint classes for consistency","These intermediate classes used for template metaprogramming (in particular, {{std::is_base_of}}) have inconsistent names with the rest of data types. For clarity, I think we should add ""Type"" to these class names and others like them

Please do after ARROW-3144",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-15 21:18:12,2
13233598,[C++] Fit build option summary into narrower console,"Not urgent, but I noticed that the new build option summary has lines up to 177 characters long from one of the outputs. It would be nice to fit this output into a 80-char-wide or 100-char-wide console for better readability",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-05-15 21:16:48,6
13233592,[R] R package fails to build/install: error in dyn.load(),"Symptoms: R package appears to build but fails to load because it can't find libarrow.14 (with slight name variations per platform). On macOS, I see
{code:java}
# compiling compiling
installing to /Users/enpiar/R/00LOCK-r/00new/arrow/libs
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
Error: package or namespace load failed for arrow in dyn.load(file, DLLpath = DLLpath, ...):
unable to load shared object '/Users/enpiar/R/00LOCK-r/00new/arrow/libs/arrow.so':
dlopen(/Users/enpiar/R/00LOCK-r/00new/arrow/libs/arrow.so, 6): Library not loaded: @rpath/libarrow.14.dylib
Referenced from: /Users/enpiar/R/00LOCK-r/00new/arrow/libs/arrow.so
Reason: image not found
Error: loading failed
Execution halted
ERROR: loading failed
* removing /Users/enpiar/R/arrow{code}
The sparklyr R package has also experienced this on Ubuntu:
{code:java}
** testing if installed package can be loaded from temporary location
Error: package or namespace load failed for 'arrow' in dyn.load(file, DLLpath = DLLpath, ...):
unable to load shared object '/home/travis/R/Library/00LOCK-arrow/00new/arrow/libs/arrow.so':
libarrow.so.14: cannot open shared object file: No such file or directory
{code}
This problem does not occur with the 0.13 release. I can `brew install apache-arrow` and then successfully install the R package; sparklyr on Travis tests 0.11, 0.13, and the latest of master branch of apache/arrow, and only the master branch fails.

According to sparklyr's build history on Travis, the last passing build with master branch of arrow was [April 5|https://travis-ci.org/rstudio/sparklyr/builds/516222934]; the next build was on [April 22|https://travis-ci.org/rstudio/sparklyr/jobs/523326800]and it failed. So something changed in that window, it seems.

There's nothing obvious (to me) in the CMake or make output that would indicate an error in the C++ build.

Interestingly, the R build on arrow's Travis-CI is not failing, so in addition to there apparently being some change thatbroke both [how the R package recommends building|https://github.com/apache/arrow/blob/7a495e7800ed2f184da1d5ffc4efde7d6c4abcac/r/README.Rmd#L38-L43]and [how sparklyr is building|https://github.com/rstudio/sparklyr/blob/master/ci/arrow-build.sh], there appears to be some [combination of flags|https://github.com/apache/arrow/blob/master/ci/travis_before_script_cpp.sh]that does result in a successful build.

Browsing through patches that landed between April 5-22, three seemed like suspects, though I haven't yet tried bisecting the history to find out which commit exactly introduces the relevant change.

*[https://github.com/apache/arrow/commit/f014d76c68a9a977a19f87f7ea77511e4753bb8c]

*[https://github.com/apache/arrow/commit/d9f675328edb1be9fc63a753cf05e3b4e9024707]

*[https://github.com/apache/arrow/commit/76e1bc5dfb9d08e31eddd5cbcc0b1bab934da2c7]

cc [~romainfrancois] [~javierluraschi]

",pull-request-available,['R'],ARROW,Bug,Major,2019-05-15 20:43:09,4
13233574,[Python] [CI] Run Python Flight tests on Travis-CI,Seems like we forgot to enable them.,pull-request-available,"['Continuous Integration', 'FlightRPC', 'Python']",ARROW,Bug,Major,2019-05-15 19:32:40,2
13233566,[R] Add shell scripts to do a full package rebuild and test locally,"The R package development instructions in https://github.com/apache/arrow/blob/master/r/README.Rmd expect that the developer is working in a particular R-console-centric way, perhaps within RStudio or similar. I think we should have scripts that enable development to be performed entirely on the command line. This would probably already exist, except that our Travis-CI setup is relying on some non-Arrow-specific scripts that live outside of this repository. ",release,['R'],ARROW,Improvement,Major,2019-05-15 18:52:53,4
13233522,[Archery][Benchmark] Output properly formatted jsonlines from benchmark diff cli command,{archery benchmark diff --output=diff.jsonl} writes objects without newline separator.,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-05-15 16:12:47,3
13233325,[CI] Enable ccache with MinGW builds,MinGW builds on AppVeyor have become quite slow. We should be able to enable ccache with them and use AppVeyor build caching to speed up some of those builds.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-05-14 21:07:42,1
13233010,[C++] Return more specific invalid Status in Take kernel,"Currently the {{Take}} kernel returns generic Invalid Status for certain cases, that could use more specific error:

- indices of wrong type (eg floats) -> TypeError instead of Invalid?
- out of bounds index -> new IndexError ?

From review in https://github.com/apache/arrow/pull/4281

cc [~bkietz]",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-05-13 15:10:58,6
13232999,[Python] better error message on creating ParquetDataset from empty directory,"Currently, you get when {{path}} is an existing but empty directory:

{code:python}
>>> dataset = pq.ParquetDataset(path)
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-16-346f72ae525e> in <module>
----> 1 dataset = pq.ParquetDataset(path)

~/scipy/repos/arrow/python/pyarrow/parquet.py in __init__(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, memory_map)
    989 
    990         if validate_schema:
--> 991             self.validate_schemas()
    992 
    993         if filters is not None:

~/scipy/repos/arrow/python/pyarrow/parquet.py in validate_schemas(self)
   1025                 self.schema = self.common_metadata.schema
   1026             else:
-> 1027                 self.schema = self.pieces[0].get_metadata().schema
   1028         elif self.schema is None:
   1029             self.schema = self.metadata.schema

IndexError: list index out of range
{code}

That could be a nicer error message. 

Unless we actually want to allow this? (although I am not sure there are good use cases of empty directories to support this, because from an empty directory we cannot get any schema or metadata information?) 
It is only failing when validating the schemas, so with {{validate_schema=False}} it actually returns a ParquetDataset object, just with an empty list for {{pieces}} and no schema. So it would be easy to not error when validating the schemas as well for this empty-directory case.",dataset dataset-parquet-read parquet,['Python'],ARROW,Bug,Minor,2019-05-13 14:24:20,5
13232976,"[Python] Add clarifications to Python ""append"" methods that return new objects","The current docstrings do say that an object is returned but it is not clear in all cases that it is a new object and the original object is left unmodified

see example thread

https://github.com/apache/arrow/issues/4296",pull-request-available,['Python'],ARROW,Improvement,Major,2019-05-13 13:13:02,5
13232798,[Python] parquet documentation outdated on nthreads argument,"[https://arrow.apache.org/docs/python/parquet.html#multithreaded-reads] still mentions {{nthreads}} instead of {{use_threads}}.



From https://github.com/pandas-dev/pandas/issues/26340",pull-request-available,['Python'],ARROW,Bug,Major,2019-05-11 09:43:11,5
13232680,[C++] 0.13 FAILED to build with option -DARROW_NO_DEFAULT_MEMORY_POOL,"I tried to upgrade Apache Arrow to 0.13. But, when building Apache Arrow 0.13 with option{{-DARROW_NO_DEFAULT_MEMORY_POOL}}, I got a lot of failures.

It seems 0.13 assuming default memory pool always available.



My cmake command is:
|{{make .. -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=off -DARROW_USE_GLOG=off -DARROW_WITH_LZ4=off -DARROW_WITH_ZSTD=off -DARROW_WITH_SNAPPY=off -DARROW_WITH_BROTLI=off -DARROW_WITH_ZLIB=off -DARROW_JEMALLOC=off -DARROW_CXXFLAGS=-DARROW_NO_DEFAULT_MEMORY_POOL}}|

I tried to fix the compilation by adding some missing constructors. However, it seems this issue is bigger than I expected. It seems all the builders and appenders have this issue as many classes even don't have a memory pool associated. ",pull-request-available,['C++'],ARROW,Bug,Major,2019-05-10 15:15:25,13
13232496,[Java] Sporadic Flight test failures,"Some of the timeout based tests may fail when CI is a bit loaded, for example:
https://travis-ci.org/apache/arrow/jobs/530352133#L4377

{code}
[ERROR] timeoutFires(org.apache.arrow.flight.TestCallOptions)  Time elapsed: 1.025 s  <<< FAILURE!
java.lang.AssertionError: DEADLINE_EXCEEDED
	at org.apache.arrow.flight.TestCallOptions.lambda$timeoutFires$0(TestCallOptions.java:44)
	at org.apache.arrow.flight.TestCallOptions.test(TestCallOptions.java:70)
	at org.apache.arrow.flight.TestCallOptions.timeoutFires(TestCallOptions.java:37)
{code}",pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-05-09 16:31:50,0
13232438,[CI] setuptools_scm failures,"See https://github.com/apache/arrow/pull/4276

We need to pin setuptools_scm to an earlier version.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Blocker,2019-05-09 10:15:07,2
13232423,[C++] Static libraries are built on AppVeyor,"Building both static and shared libraries on Windows needs to compile all source files twice, making CI slwoer.
Normally, only the shared libraries are needed for testing (except for Parquet, see PARQUET-1420).

",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-05-09 09:37:42,14
13232422,"[Python] Add wrapper for ""take"" kernel on Array ","Expose the {{take}} kernel (for primitive types, ARROW-2102) on the python {{Array}} class. Part of ARROW-2667.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-05-09 09:34:43,5
13232399,[Java] Provide a flag to enable/disable null-checking in vectors' get methods,"For vector classes, the get method first checks if the value at the given index is null. If it is not null, the method goes ahead to retrieve the value. 
For some scenarios, the first check is redundant, because the application code has already checked the null, before calling the get method. This redundant check may have non-trivial performance overheads. 

So we add a flag to enable/disable the null checking, so the user can set the flag according to their own specific scenario. ",pull-request-available,['Java'],ARROW,New Feature,Major,2019-05-09 08:16:46,7
13232311,[C++] Move arrow/util/concatenate.h to arrow/array/,"I think this would be a better location for array/columnar algorithms

Please wait until after ARROW-3144 ",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-08 19:12:39,14
13232310,[Documentation] Enrich the contribution guidelines,Including indications for what Jira fields to use. Plus I see a few other things I'll touch up while I'm in there.,pull-request-available,['Documentation'],ARROW,Improvement,Minor,2019-05-08 19:09:53,4
13232258,[Python] support Structs in Table.from_pandas given a known schema,"ARROW-2073 implemented creating a StructArray from an array of tuples (in addition to from dicts). 
This works in {{pyarrow.array}} (specifying the proper type):

{code}
In [2]: df = pd.DataFrame({'tuples': [(1, 2), (3, 4)]})                                                                                                       

In [3]: struct_type = pa.struct([('a', pa.int64()), ('b', pa.int64())])                                                                                       

In [4]: pa.array(df['tuples'], type=struct_type)                                                                                                              
Out[4]: 
<pyarrow.lib.StructArray object at 0x7f1b02ff6818>
-- is_valid: all not null
-- child 0 type: int64
  [
    1,
    3
  ]
-- child 1 type: int64
  [
    2,
    4
  ]
{code}

But does not yet work when converting a DataFrame to Table while specifying the type in a schema:

{code}
In [5]: pa.Table.from_pandas(df, schema=pa.schema([('tuples', struct_type)]))                                                                                 
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~/scipy/repos/arrow/python/pyarrow/pandas_compat.py in get_logical_type(arrow_type)
     68     try:
---> 69         return logical_type_map[arrow_type.id]
     70     except KeyError:

KeyError: 24

During handling of the above exception, another exception occurred:

NotImplementedError                       Traceback (most recent call last)
<ipython-input-5-c18748f9b954> in <module>
----> 1 pa.Table.from_pandas(df, schema=pa.schema([('tuples', struct_type)]))

~/scipy/repos/arrow/python/pyarrow/table.pxi in pyarrow.lib.Table.from_pandas()

~/scipy/repos/arrow/python/pyarrow/pandas_compat.py in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)
    483     metadata = construct_metadata(df, column_names, index_columns,
    484                                   index_descriptors, preserve_index,
--> 485                                   types)
    486     return all_names, arrays, metadata
    487 

~/scipy/repos/arrow/python/pyarrow/pandas_compat.py in construct_metadata(df, column_names, index_levels, index_descriptors, preserve_index, types)
    207         metadata = get_column_metadata(df[col_name], name=sanitized_name,
    208                                        arrow_type=arrow_type,
--> 209                                        field_name=sanitized_name)
    210         column_metadata.append(metadata)
    211 

~/scipy/repos/arrow/python/pyarrow/pandas_compat.py in get_column_metadata(column, name, arrow_type, field_name)
    149     dict
    150     """"""
--> 151     logical_type = get_logical_type(arrow_type)
    152 
    153     string_dtype, extra_metadata = get_extension_dtype_info(column)

~/scipy/repos/arrow/python/pyarrow/pandas_compat.py in get_logical_type(arrow_type)
     77         elif isinstance(arrow_type, pa.lib.Decimal128Type):
     78             return 'decimal'
---> 79         raise NotImplementedError(str(arrow_type))
     80 
     81 

NotImplementedError: struct<a: int64, b: int64>

{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-05-08 15:52:45,5
13232213,[C++][Plasma] GpuProcessHandle is not released when GPU object deleted,"cpp/CMakeLists.txt
 option(ARROW_CUDA ""Build the Arrow CUDA extensions (requires CUDA toolkit)"" ON)
 option(ARROW_PLASMA ""Build the plasma object store along with Arrow"" ON)

In the plasma client, GpuProcessHandle is never released although GPU object is deleted.
Thus, cuIpcCloseMemHandle is never called.
When I repeatly creat and delete gpu memory, the following error may occur.

IOError: Cuda Driver API call in /home/zilliz/arrow/cpp/src/arrow/gpu/cuda_context.cc at line 155 failed with code 208: cuIpcOpenMemHandle(&data, *handle, CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS)

Note: CUDA_ERROR_ALREADY_MAPPED = 208",pull-request-available,"['C++', 'C++ - Plasma', 'GPU']",ARROW,Bug,Major,2019-05-08 13:00:29,2
13232034,[C++] Write generic filesystem tests,"We need a suite of implementation-agnostic tests for filesystem implementations, to make it easy to validate each implementation against the expected semantics.",pull-request-available,['C++'],ARROW,Task,Major,2019-05-07 14:40:07,2
13231828,[C++] Whitelist benchmarks candidates for regression checks,Rename all benchmarks candidate for regression with the `Regression` prefix.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-06 17:10:06,13
13231736,[Python/CI] Add integration test with kartothek,https://github.com/JDASoftwareGroup/kartothek is a heavy user of Apache Arrow and thus a good indicator whether we have introduced some breakages in {{pyarrow}}. Thus we should run regular integration tests against it as we do with other libraries.,parquet pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2019-05-06 11:59:09,8
13231713,[Java] Allow enabling/disabling boundary checking by environmental variable,"The flag BoundsChecking#BOUNDS_CHECKING_ENABLED determines if boundary checking is enabled/disabled in vector/arrow buffer APIs. 
It has significant performance implications, since boundary checking is a frequent operation.

This issue addresses 2 problems with the flag for boundary checking in Java API:

1. The flag can be determined by an environmental variable: ARROW_ENABLE_UNSAFE_MEMORY_ACCESS, in addition to the system properties. The system properties have higher priority than the environmental variable.

2. There is an old and a new system property for this flag. To disable boundary checking, both the old and new properties must be set to true, which is undesirable:

 !screenshot-1.png! ",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-05-06 07:53:48,7
13231681,[C++] Finish implementation of scalar types for Duration and Interval,Once https://github.com/apache/arrow/pull/3644 is checked in these types should be completed.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-05 22:55:36,15
13231629,[Python][C++] Crash when deserializing from components in a fresh new process,"Trying to deserialize a table from component in a fresh new process crashes with sigsegv:
{noformat}
#1 0x00007fffd5eb93f0 in arrow::py::unwrap_buffer(_object*, std::shared_ptr<arrow::Buffer>*) ()
from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/./libarrow_python.so.13
#2 0x00007fffd5e69260 in arrow::py::GetSerializedFromComponents(int, int, int, _object*, arrow::py::SerializedPyObject*) () from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/./libarrow_python.so.13
#3 0x00007fffd6b1cafe in __pyx_pw_7pyarrow_3lib_18SerializedPyObject_7from_components(_object*, _object*, _object*) () from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
#4 0x00000000004ad919 in PyCFunction_Call ()
#5 0x00007fffd6a88d10 in __Pyx_PyObject_Call(_object*, _object*, _object*) [clone .constprop.1186] ()
from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
#6 0x00007fffd6a41872 in __Pyx__PyObject_CallOneArg(_object*, _object*) ()
from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
#7 0x00007fffd6a89e59 in __Pyx_PyObject_CallOneArg(_object*, _object*) ()
from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
#8 0x00007fffd6ab087f in __pyx_pw_7pyarrow_3lib_165deserialize_components(_object*, _object*, _object*) ()
from /home/yevgeni/uatc/.petastorm3.6/lib/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
#9 0x00000000004adca7 in _PyCFunction_FastCallKeywords ()
#10 0x0000000000545e34 in ?? ()
#11 0x000000000054ac8c in _PyEval_EvalFrameDefault ()
#12 0x0000000000545a51 in ?? ()
#13 0x0000000000546890 in PyEval_EvalCode ()
#14 0x000000000042a9a8 in PyRun_FileExFlags ()
#15 0x000000000042ab8d in PyRun_SimpleFileExFlags ()
#16 0x000000000043e0ba in Py_Main ()
#17 0x0000000000421b04 in main ()
{noformat}
The following snippet can be used to reproduce the issue:
{code:java}
import pickle
import sys

import pandas as pd
import pyarrow as pa

if __name__ == '__main__':
    if sys.argv[1] == 'w':
        df = pd.DataFrame({'int': [1, 2], 'str': ['a', 'b']})
        table = pa.Table.from_pandas(df)
        table_serialized = pa.serialize(table)
        table_serialized_components = table_serialized.to_components()
        with open('/tmp/p.pickle', 'wb') as f:
            pickle.dump(table_serialized_components, f)
        print('/tmp/p.pickle written ok')

    if sys.argv[1] == 'r':
        # UNCOMMENT THE FOLLOWING LINE TO AVOID THE CRASH
        #pa.serialize(0)
        with open('/tmp/p.pickle', 'rb') as f:
            table_serialized_components = pickle.load(f)
        table = pa.deserialize_components(table_serialized_components)
        print(table)

{code}
Then run:
{code:java}
$ python pa_serialization_crashes.py w
/tmp/p.pickle written ok

$ python pa_serialization_crashes.py r
Segmentation fault (core dumped){code}
The crash would not occur if you try to serializeunrelated data beforethe deserialization (see a commented out line in the reproduction instructions)

",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-05-05 05:00:38,2
13231551,"[Website] Update site to use ""official"" Apache Arrow logo, add clearly marked links to logo","See logo at https://docs.google.com/presentation/d/1qmvPpFU7sdm9l6A6LEyI0zIzswGtJW0Sbd_lfHLaXQs/edit#slide=id.g4258234456_0_1

An unofficial logo lacking the ""Apache"" has been making the rounds on the internet, so I think it would be a good idea to update our web properties with the approved logo as discussed on the mailing list

Whoever does this task -- please make sure to compress the PNG asset of the logo prior to checking in to source control",pull-request-available,['Website'],ARROW,Improvement,Major,2019-05-03 22:25:26,4
13231547,[Packaging][deb] Failed to build with LLVM 7.1.0,"https://travis-ci.org/ursa-labs/crossbow/builds/527710714#L6144-L6157

{noformat}
CMake Error at cmake_modules/FindLLVM.cmake:33 (find_package):
  Could not find a configuration file for package ""LLVM"" that is compatible
  with requested version ""7.0"".
  The following configuration files were considered but not accepted:
    /usr/lib/llvm-7/cmake/LLVMConfig.cmake, version: 7.1.0
    /usr/lib/llvm-7/lib/cmake/llvm/LLVMConfig.cmake, version: 7.1.0
    /usr/lib/llvm-7/share/llvm/cmake/LLVMConfig.cmake, version: 7.1.0
    /usr/lib/llvm-3.8/share/llvm/cmake/LLVMConfig.cmake, version: 3.8.1
    /usr/share/llvm-3.8/cmake/LLVMConfig.cmake, version: 3.8.1
Call Stack (most recent call first):
  src/gandiva/CMakeLists.txt:31 (find_package)
{noformat}

Can we use ""7"" instead of ""7.0"" for {{ARROW_LLVM_VERSION}}?",pull-request-available,"['C++ - Gandiva', 'Packaging']",ARROW,Improvement,Major,2019-05-03 22:15:03,1
13231531,[Flight][Java] DoAction does not support result streams,"While Flight defines DoAction as returning a stream of results, the Java APIs only allow returning a single result.",flight pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-05-03 21:00:09,0
13231444,[C++] external Snappy fails on Alpine,"
{code:bash}
FAILED: debug/libarrow.so.14.0.0 
: && /usr/bin/c++ -fPIC -Wno-noexcept-type  -fdiagnostics-color=always -ggdb -O0  -Wall -Wno-conversion -Wno-sign-conversion -Wno-unused-variable -Werror -msse4.2  -g  -Wl,--version-script=/buildbot/amd64-alpine-3_9-cpp/cpp/src/arrow/symbols.map -shared -Wl,-soname,libarrow.so.14 -o debug/libarrow.so.14.0.0 
...
c++: error: snappy_ep/src/snappy_ep-install/lib/libsnappy.a: No such file or directory
{code}
",pull-request-available,['C++'],ARROW,Bug,Major,2019-05-03 13:11:16,13
13231419,[C++] Change variant implementation,"Our vendored variant implementation, [Mapbox variant|https://github.com/mapbox/variant], does not provide the same API as the official [C++17 variant class|https://en.cppreference.com/w/cpp/utility/variant].

We could / should switch to an implementation that follows the C++17 API, such as https://github.com/mpark/variant or https://github.com/martinmoene/variant-lite .",pull-request-available,['C++'],ARROW,Improvement,Major,2019-05-03 09:13:16,2
13230933,[C++][CI] Unpin cmake_format,Once we either fix the cmake files or a newer version of cmake_format (> 0.5.0) that continues to work with existing files we should unpin the version from 0.4.5,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-04-30 15:24:38,15
13230780,[Python] Add option to disable writing statistics to parquet file,"C++  Parquet API exposes an option to disable writing statistics when writing a Parquet file.
It will be useful to expose this API in the Python Arrow API as well.",parquet pull-request-available,['Python'],ARROW,Improvement,Major,2019-04-29 22:10:11,5
13230744,[Python] Improve usability of pyarrow.dictionary function,"{code}
>>> pa.dictionary('int8', ['a', 'b', 'c', 'd'])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Argument 'index_type' has incorrect type (expected pyarrow.lib.DataType, got str)
>>> pa.dictionary(pa.int8(), ['a', 'b', 'c', 'd'])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Argument 'dict_values' has incorrect type (expected pyarrow.lib.Array, got list)
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-04-29 17:54:40,5
13230725,[Python] pandas_version key in pandas metadata no longer populated,"While looking at the pandas metadata, I noticed that the {{pandas_version}} field now is None. I suppose this is due to the recent refactoring of the pandas api compat (https://github.com/apache/arrow/pull/3893). PR coming.",pull-request-available,['Python'],ARROW,Bug,Minor,2019-04-29 15:27:02,5
13230559,[Rust] [DataFusion] Re-implement query execution with an extensible physical query plan,"This story (maybe it should have been an epic with hindsight) is to re-implement query execution in DataFusion using a physical plan that supports partitions and parallel execution.

This will replace the current query execution which happens directly from the logical plan.

The new physical plan is based on traits and is therefore extensible by other projects that use Arrow. For example, another project could add physical plans for distributed compute.

See design doc at [https://docs.google.com/document/d/1ATZGIs8ry_kJeoTgmJjLrg6Ssb5VE7lNzWuz_4p6EWk/edit?usp=sharing] for more info",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,New Feature,Major,2019-04-28 16:08:41,10
13230517,[Java] Improve performance of BaseValueVector#getValidityBufferSizeFromCount,"Now in_BaseValueVector#getValidityBufferSizeFromCount_ and_BitVectorHelper#getValidityBufferSize_, it uses_Math.ceil_to calculate size which is not efficient (lots of unnecessary logic in_StrictMath#floorOrCeil_). Since the valueCount is always not less than 0, we could simply replace _Math.ceil_ with the following code:

_return (valueCount+7) >> 3_;",pull-request-available,['Java'],ARROW,Improvement,Trivial,2019-04-28 03:22:06,16
13230368,[Python] Issues with installing pyarrow for development on MacOS,"gI tried following the [instructions|https://github.com/apache/arrow/blob/master/docs/source/developers/python.rst]for installing pyarrow for developers on macos, and I ran into quite a bit of difficulty. I'm hoping we can improve our documentation and/or tooling to make this a smoother process.

I know we can't anticipate every quirk of everyone's dev environment, but in my case, I was getting set up on a new machine, so this was from a clean slate. I'm also new to contributing to the project, so I'm a ""clean slate"" in that regard too, so my ignorance may be exposing other assumptions in the docs.
 # The instructions recommend using conda, but as this [Stack Overflowquestion|https://stackoverflow.com/questions/55798166/cmake-fails-with-when-attempting-to-compile-simple-test-program]notes, cmake fails. Uwe helpfully suggested installing an older MacOS SDK from [here|https://github.com/phracker/MacOSX-SDKs/releases]. That may work, but I'mpersonally wary to install binaries from an unofficial github account, let alone record that in our docs as an official recommendation. Either way, we should update the docs either to note this necessity or to recommend against installing with conda on macos.
 # After that, I tried to go the Homebrew path. Ultimately this did succeed, but it was rough. It seemed that I had to `brew install` a lot of packages that weren't included in the arrow/python/Brewfile (i.e. try to cmake, see what missing dependency it failed on, `brew install` it, retry `cmake`, and repeat). Among the libs I installed this way weredouble-conversion snappy brotli protobuf gtest rapidjson flatbuffers lz4 zstd c-ares boost. It's not clear how many of these extra dependencies I had to install were because I'd only installed the xcode command-line tools and not the full xcode from the App Store; regardless, the Brewfile should be complete if we want to use it.
 # In searching Jira for the double-conversion issue (the first one I hit), I found [this issue/PR|https://github.com/apache/arrow/pull/4132/files], which added double-conversion to a different Brewfile, in c_glib. So I tried `brew bundle` installing that Brewfile. It would probably be good to have a common Brewfile for the C++ setup, which the python and glib ones could load and then add any other extra dependencies, if necessary. That way, there's one place to add common dependencies.
 # I got close here but still had issues with `BOOST_HOME` not being found, even though I had brew-installed it.From the console output, itappeared that even though I was not using conda and did not have an active conda environment (I'd even done `conda env remove --name pyarrow-dev`), the cmake configuration script detected that conda existed and decided to use conda to resolve dependencies. I tried setting lots of different environment variables to tell cmake not to use conda, but ultimately I was only able to get past this by deleting conda from my system entirely.
 # This let me get to the point of being able to `import pyarrow`. But then running tests failed because the `hypothesis` package was not installed.I see that itis included in requirements-test.txt and setup.py under tests_require, butI followed the installation instructionsand this packagedid not end up in my virtualenv. `pip install hypothesis` resolved it.",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2019-04-26 18:34:41,4
13230246,[Python] index / unknown columns in specified schema in Table.from_pandas,"The {{Table.from_pandas}} method allows to specify a schema (""This can be used to indicate the type of columns if we cannot infer it automatically."").

But, if you also want to specify the type of the index, you get an error:

{code:python}
df = pd.DataFrame({'a': [1, 2, 3], 'b': [0.1, 0.2, 0.3]})
df.index = pd.Index(['a', 'b', 'c'], name='index')

my_schema = pa.schema([('index', pa.string()),
 ('a', pa.int64()),
 ('b', pa.float64()),
 ])

table = pa.Table.from_pandas(df, schema=my_schema)
{code}

gives {{KeyError: 'index'}} (because it tries to look up the ""column names"" from the schema in the dataframe, and thus does not find column 'index').

This also has the consequence that re-using the schema does not work: {{table1 = pa.Table.from_pandas(df1);  table2 = pa.Table.from_pandas(df2, schema=table1.schema)}}

Extra note: also unknown columns in general give this error (column specified in the schema that are not in the dataframe).

At least in pyarrow 0.11, this did not give an error (eg noticed this from the code in example in ARROW-3861). So before, unknown columns in the specified schema were ignored, while now they raise an error. Was this a conscious change?  
So before also specifying the index in the schema ""worked"" in the sense that it didn't raise an error, but it was also ignored, so didn't actually do what you would expect)

Questions:

- I think that we should support specifying the index in the passed {{schema}} ? So that the example above works (although this might be complicated with RangeIndex that is not serialized any more)
- But what to do in general with additional columns in the schema that are not in the DataFrame? Are we fine with keep raising an error as it is now (the error message could be improved then)? Or do we again want to ignore them? (or, it could actually also add them as all nulls to the table)",pull-request-available,['Python'],ARROW,Bug,Minor,2019-04-26 08:33:02,5
13230191,[C++] Build protobuf_ep in parallel when using Ninja,Serial builds of protobuf are quite slow. Currently {{make}} is run without parallelism,pull-request-available,['C++'],ARROW,Improvement,Major,2019-04-25 22:06:18,14
13230183,[Rust] [CI] DataFusion test failure,"Travis-CI Rust jobs have started failing consistently with a DataFusion test failure.
Example here:
https://travis-ci.org/apache/arrow/jobs/524542965

{code}
---- execution::aggregate::tests::test_min_max_sum_count_avg_f64_group_by_uint32 stdout ----
thread 'execution::aggregate::tests::test_min_max_sum_count_avg_f64_group_by_uint32' panicked at 'assertion failed: `(left == right)`
  left: `2`,
 right: `5`', datafusion/src/execution/aggregate.rs:1437:9
note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.
{code}
",pull-request-available,"['Continuous Integration', 'Rust', 'Rust - DataFusion']",ARROW,Bug,Blocker,2019-04-25 21:00:08,10
13230177,[CI] Add Appveyor badge to README,"I was trying to see what was running in appveyor and couldn't find it. Krisztin helped me to find[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow], but it would be nice to add the badge to the README next to the Travis-CI one for a quick link to it (as well as showing off build status).

I was just going to add it myself, but unlike Travis, you can't guess the Appveyor badge URL from the project name because they have a hash in them; only someone with sufficient privileges on the project in Appveyor can get to the settings panel to find the URL.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Trivial,2019-04-25 20:38:17,4
13230106,[C++] Offline dependency downloader misses some libraries,"Not sure yet but maybe this was introduced by https://github.com/apache/arrow/commit/f913d8f0adff71c288a10f6c1b0ad2d1ab3e9e32

{code}
$ thirdparty/download_dependencies.sh /home/wesm/arrow-thirdparty
# Environment variables for offline Arrow build
export ARROW_BOOST_URL=/home/wesm/arrow-thirdparty/boost-1.67.0.tar.gz
export ARROW_BROTLI_URL=/home/wesm/arrow-thirdparty/brotli-v1.0.7.tar.gz
export ARROW_CARES_URL=/home/wesm/arrow-thirdparty/cares-1.15.0.tar.gz
export ARROW_DOUBLE_CONVERSION_URL=/home/wesm/arrow-thirdparty/double-conversion-v3.1.4.tar.gz
export ARROW_FLATBUFFERS_URL=/home/wesm/arrow-thirdparty/flatbuffers-v1.10.0.tar.gz
export ARROW_GBENCHMARK_URL=/home/wesm/arrow-thirdparty/gbenchmark-v1.4.1.tar.gz
export ARROW_GFLAGS_URL=/home/wesm/arrow-thirdparty/gflags-v2.2.0.tar.gz
export ARROW_GLOG_URL=/home/wesm/arrow-thirdparty/glog-v0.3.5.tar.gz
export ARROW_GRPC_URL=/home/wesm/arrow-thirdparty/grpc-v1.20.0.tar.gz
export ARROW_GTEST_URL=/home/wesm/arrow-thirdparty/gtest-1.8.1.tar.gz
export ARROW_LZ4_URL=/home/wesm/arrow-thirdparty/lz4-v1.8.3.tar.gz
export ARROW_ORC_URL=/home/wesm/arrow-thirdparty/orc-1.5.5.tar.gz
export ARROW_PROTOBUF_URL=/home/wesm/arrow-thirdparty/protobuf-v3.7.1.tar.gz
export ARROW_RAPIDJSON_URL=/home/wesm/arrow-thirdparty/rapidjson-2bbd33b33217ff4a73434ebf10cdac41e2ef5e34.tar.gz
export ARROW_RE2_URL=/home/wesm/arrow-thirdparty/re2-2019-04-01.tar.gz
{code}

The 5 dependencies listed after RE2 are not downloaded",pull-request-available,['C++'],ARROW,Bug,Major,2019-04-25 14:28:14,13
13229965,[Format] Missing documentation under `Dictionary encoding` section on MetaData page,"First time throwing up an issue here so let me know if there's anything I missed / more details I can provide.

Just going through the arrow documentation at[https://arrow.apache.org/docs/python/]and I noticed that there's a section that is currently blank.From what I can tell the section[https://arrow.apache.org/docs/format/Metadata.html#dictionary-encoding]currently contains nothing in it. Is that intended? Itwas confusingto see a blank section, but that is just my opinion so it may not be worth changing.

If this is something work fixing / improving, then it's probably worth either filling out that section or simply removing header to avoid future confusion.",documentation,['Documentation'],ARROW,Improvement,Trivial,2019-04-24 20:09:29,14
13229799,[Java] Add performance benchmarks from SQL workloads,"To improve the performance of Arrow implementations. Some performance benchmarks must be setup first.

In this issue, we want to provide some performance benchmarksextracted from our SQL engine, whichis going to be made open source soon. The workloads are obtained by running an open SQL benchmarks TPC-H.",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-04-24 08:10:11,7
13229785,[Python] Inconsistent resulting type during casting in pa.array() when mask is present,"I would expect Int64Array type in all cases below :
{code:java}
>>> pa.array([4, None, 4, None], mask=np.array([False, True, False, True]))                                        
<pyarrow.lib.Int64Array object at 0x91fad3a98> [4, null, 4, null ]

>>> pa.array([4, None, 4, 'rer'], mask=np.array([False, True, False, True]))                                          
<pyarrow.lib.Int64Array object at 0x9201f23b8> [4, null, 4, null ]

>>> pa.array([4, None, 4, 3.], mask=np.array([False, True, False, True]))                                            <pyarrow.lib.DoubleArray object at 0x91fab7638> [  4,  null,  4,  null ]{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-04-24 07:36:15,14
13229756,[Java] add APIs to support vector reuse,"In some scenarios we hope that ValueVector could be reused to reduce creation overhead. This is very common in shuffle stage, it's no need to create ValueVector or realloc buffers every time, suppose that the recordCount of ValueVector and capacity of its buffers is written in stream, when we deserialize it, we can simply judge whether realloc is needed through dataLength.

My proposal is that add APIs in ValueVector to process this logic, otherwise users have to implement by themselves if they want to reuse which is not user-friendly.

If you agree with this, I would like to take this ticket. Thanks",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-04-24 02:36:49,16
13229753,[Java] Add APIs in MessageSerializer to directly serialize/deserialize ArrowBuf,"It seems there no APIs to directly write ArrowBuf to OutputStream or read ArrowBuf from InputStream. These APIs may be helpful when users use Vectors directly instead of RecordBatch, in this case, provide APIs to serialize/deserialize dataBuffer/validityBuffer/offsetBuffer is necessary.

I would like to work on this and make it my first contribution to Arrow. What do you think?",pull-request-available,['Java'],ARROW,Improvement,Minor,2019-04-24 02:03:42,16
13229640,[C++] Improve BufferBuilder performance,"BufferBuilder makes a spurious memset() when extending the buffer size.

We could also tweak the overallocation strategy in Reserve().",pull-request-available,['C++'],ARROW,Improvement,Major,2019-04-23 15:05:44,2
13229553,[Python] Import ABCs from collections is deprecated in Python 3.7,"From running the tests, I see a few deprecation warnings related to that on Python 3, abstract base classes should be imported from `collections.abc` instead of `collections`:

{code:none}
pyarrow/tests/test_array.py:808
 /home/joris/scipy/repos/arrow/python/pyarrow/tests/test_array.py:808: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
 pa.struct([pa.field('a', pa.int64()), pa.field('b', pa.string())]))

pyarrow/tests/test_table.py:18
 /home/joris/scipy/repos/arrow/python/pyarrow/tests/test_table.py:18: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
 from collections import OrderedDict, Iterable

pyarrow/tests/test_feather.py::TestFeatherReader::test_non_string_columns
 /home/joris/scipy/repos/arrow/python/pyarrow/pandas_compat.py:294: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
 elif isinstance(name, collections.Sequence):{code}

Those could be imported depending on python 2/3 in the ``pyarrow.compat`` module.",pull-request-available,['Python'],ARROW,Bug,Major,2019-04-23 09:15:53,5
13229524,[Java] Provide light-weight arrow APIs,"We are trying to incorporate Apache Arrow to Apache Flink runtime. We find Arrow an amazing library, which greatly simplifies the support of columnar data format.

However, for many scenarios, we find the performance unacceptable. Our investigation shows the reason is that, there are too many redundant checks and computations in Arrow API.

For example, the following figures shows that in a single call to Float8Vector.get(int) method (this is one of the most frequently used APIs in Flink computation), there are 20+ method invocations.

!image-2019-04-23-15-19-34-187.png!



There are many other APIs with similar problems. We believe that these checks will makesure of the integrity of the program. However, it also impacts performance severely. For our evaluation, the performance may degrade by two or three orders of magnitude slower, compared to access data on heap memory.

We think at least for some scenarios,we can give the responsibility of integrity check to application owners. If they can be sure all the checks have been passed, we can provide some light-weight APIs and the inherent high performance, to them.

In the light-weight APIs, we only provide minimum checks, or avoid checks at all. The application owner can still develop and debug their code using the original heavy-weight APIs. Once all bugs have been fixed, they can switch to light-weight APIs in their products and enjoy the consequent high performance.

",pull-request-available,['Java'],ARROW,Improvement,Major,2019-04-23 07:31:40,7
13229449,[Python] read_csv ignores null_values on string types,"Let's write a simple CSV with NULL values in a string column:
{quote}with open('foo.csv', 'w') as fobj:
   fobj.write('col1,col2\n1,value\n2,NULL')
 table = csv.read_csv('foo.csv')
 table.column('col2').null_count # => 0
{quote}

 table.column('col2').null_count will be 0, I think it should be 1. Passing in {{ConvertOptions(null_values=[""NULL""])}} doesn't help.



Note that {{pandas.read_csv}} parses these NULLs correctly so I have a workaround available.

But I'd prefer to natively read CSV from pyarrow if possible :)",pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2019-04-22 20:59:05,2
13229301,[Rust] Expose CSV and JSON reader schemas,"It's sometimes convenient to be able to view a datasource's schema without reading the first record batch. This is a proposal to create a `pub fn schema(&self) -> Arc<Schema>` on the various readers that we support.

I think this would also enable schema inference in datafusion",pull-request-available,['Rust'],ARROW,Improvement,Minor,2019-04-21 21:20:40,12
13229032,[Rust] Add temporal builders for StructArray,StructBuilder currently doesn't have builders for temporal arrays.,pull-request-available,['Rust'],ARROW,New Feature,Minor,2019-04-19 11:32:47,12
13229024,[Rust] Ability to flatten StructArray into a RecordBatch,"Add the ability to flatten a schema into a record batch.

StructBuilder and StructArray haveconvenient methods to build multiple arrays.Being able to use these convenient methods and then convert the result to a record batch reduces the amount ofboilerplate when creating Arrow data from sources like databases.",pull-request-available,['Rust'],ARROW,New Feature,Minor,2019-04-19 09:59:46,12
13228785,[CI] MinGW build failures on AppVeyor,"See e.g. https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/23926002

{code}
CMake Error at C:/msys64/mingw32/lib/cmake/Boost-1.70.0/BoostConfig.cmake:95 (find_package):
  Found package configuration file:
    C:/msys64/mingw32/lib/cmake/boost_regex-1.70.0/boost_regex-config.cmake
  but it set boost_regex_FOUND to FALSE so package ""boost_regex"" is
  considered to be NOT FOUND.  Reason given by package:
  No suitable build variant has been found.
Call Stack (most recent call first):
  C:/msys64/mingw32/lib/cmake/Boost-1.70.0/BoostConfig.cmake:124 (boost_find_dependency)
  C:/msys64/mingw32/share/cmake-3.13/Modules/FindBoost.cmake:264 (find_package)
  cmake_modules/ThirdpartyToolchain.cmake:2246 (find_package)
  CMakeLists.txt:544 (include)
{code}",pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-04-18 08:07:33,2
13228689,[Rust] Create Arrow File reader,Initial support for reading the Arrow File format,pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-04-17 19:53:41,12
13228688,[Rust] IPC Support,The overall ticket to keep track of initial IPC support,pull-request-available,['Rust'],ARROW,New Feature,Blocker,2019-04-17 19:51:31,12
13228642,[Python] Allow creating Table from Python dict,"There's already {{Table.to_pydict()}}, we should probably have the reverse {{Table.from_pydict()}} method.",pull-request-available,['Python'],ARROW,Improvement,Major,2019-04-17 16:19:16,2
13228638,[Python] ParquetReader.read_column() doesn't check bounds,"If you call {{ParquetReader.read_column()}} with an invalid column number, it just crashes.",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-04-17 15:59:12,2
13228230,[C++] Use LESS instead of LOWER in compare enum option.,Seehttps://github.com/apache/arrow/pull/3963#discussion_r275596603,pull-request-available,['C++'],ARROW,New Feature,Trivial,2019-04-16 01:56:41,13
13227942,[Python] non-nullable fields are converted to nullable in {{Table.from_pandas}},"In version 0.13.0, the {{Table.from_pandas}} function modifies the input schema by making all non-nullable types nullable.

This can cause problems for example with this code:
{code}
df = pd.DataFrame(list(range(200)), columns=['numcol'])
schema = pa.schema([
 pa.field('numcol', pa.int64(), nullable=False),
])
writer = pq.ParquetWriter(io.BytesIO(), schema, version='2.0')
table = pa.Table.from_pandas(df, schema=schema)
writer.write_table(table)
{code}
Which fails due to the writer schema and the table schema being different.

I believe the direct cause could be [https://github.com/apache/arrow/blob/master/python/pyarrow/table.pxi#L622] where nullable is set to True by default, resulting in the table schema being modified.



Thanks for your valuable work on this library.

Giacomo",pull-request-available,['Python'],ARROW,Bug,Major,2019-04-14 14:27:47,5
13227765,[C++] Upgrade string-view-light to latest,string-view-lite had a typo in one of its macros (fixed in https://github.com/martinmoene/string-view-lite/commit/2f2cce35293b0027056e5449b2c05b5f9c3e89ff).  We should vendor the latest version in the next Arrow release.,pull-request-available,['C++'],ARROW,Bug,Minor,2019-04-12 17:54:09,2
13227728,[Python][Parquet] Statistics for uint64 columns may overflow,"See the attached parquet file, where the statistics max value is smaller than the min value.

You can roundtrip that file through pandas and store it back to provoke the same bug.",parquet,"['C++', 'Python']",ARROW,Bug,Major,2019-04-12 14:59:39,14
13227704,[Python][Documentation] Build docs don't suggest assigning $ARROW_BUILD_TYPE,"[Build documentation|https://arrow.apache.org/docs/developers/python.html] is great. However it does not explicitly suggest assigning a value to `ARROW_BUILD_TYPE` and the error thrown is not obvious:
{code:bash}
...
 [100%] Built target _parquet
  Finished cmake --build for pyarrow
 Bundling includes: include
 error: [Errno 2] No such file or directory: 'include'
{code}
This cost me a couple of hours to debug.

Could we include a note in [build documentation|https://arrow.apache.org/docs/developers/python.html] suggesting devs to run:
{code:bash}
export ARROW_BUILD_TYPE=release
{code}",pull-request-available,"['Developer Tools', 'Documentation', 'Python']",ARROW,Improvement,Minor,2019-04-12 12:42:15,5
13227349,[C++] ABORT_NOT_OK evalutes expression twice,ABORT_NOT_OK in gtest_util.h evaluates the expression twice due to a typo.,pull-request-available,['C++'],ARROW,Bug,Major,2019-04-10 22:46:24,0
13227165,[Packaging][Wheel] Symlink libraries in wheels,"Libraries are copied instead of symlinking in linux and osx wheels, which result quiet big binaries:


This is what the wheel contains before running auditwheel:

{code}
-rwxr-xr-x 1 root root 128K Apr 3 09:02 libarrow_boost_filesystem.so
-rwxr-xr-x 1 root root 128K Apr 3 09:02 libarrow_boost_filesystem.so.1.66.0
-rwxr-xr-x 1 root root 1.2M Apr 3 09:02 libarrow_boost_regex.so
-rwxr-xr-x 1 root root 1.2M Apr 3 09:02 libarrow_boost_regex.so.1.66.0
-rwxr-xr-x 1 root root 30K Apr 3 09:02 libarrow_boost_system.so
-rwxr-xr-x 1 root root 30K Apr 3 09:02 libarrow_boost_system.so.1.66.0
-rwxr-xr-x 1 root root 1.4M Apr 3 09:02 libarrow_python.so
-rwxr-xr-x 1 root root 1.4M Apr 3 09:02 libarrow_python.so.14
-rwxr-xr-x 1 root root 12M Apr 3 09:02 libarrow.so
-rwxr-xr-x 1 root root 12M Apr 3 09:02 libarrow.so.14
-rw-r--r-- 1 root root 6.1M Apr 3 09:02 lib.cpp
-rwxr-xr-x 1 root root 2.4M Apr 3 09:02 [lib.cpython-36m-x86_64-linux-gnu.so|http://lib.cpython-36m-x86_64-linux-gnu.so/]
-rwxr-xr-x 1 root root 55M Apr 3 09:02 libgandiva.so
-rwxr-xr-x 1 root root 55M Apr 3 09:02 libgandiva.so.14
-rwxr-xr-x 1 root root 2.9M Apr 3 09:02 libparquet.so
-rwxr-xr-x 1 root root 2.9M Apr 3 09:02 libparquet.so.14
-rwxr-xr-x 1 root root 309K Apr 3 09:02 libplasma.so
-rwxr-xr-x 1 root root 309K Apr 3 09:02 libplasma.so.14
{code}

After running auditwheel, the repaired wheel contains:

{code}
-rwxr-xr-x 1 root root 128K Apr 3 09:02 libarrow_boost_filesystem.so
-rwxr-xr-x 1 root root 128K Apr 3 09:02 libarrow_boost_filesystem.so.1.66.0
-rwxr-xr-x 1 root root 1.2M Apr 3 09:02 libarrow_boost_regex.so
-rwxr-xr-x 1 root root 1.2M Apr 3 09:02 libarrow_boost_regex.so.1.66.0
-rwxr-xr-x 1 root root 30K Apr 3 09:02 libarrow_boost_system.so
-rwxr-xr-x 1 root root 30K Apr 3 09:02 libarrow_boost_system.so.1.66.0
-rwxr-xr-x 1 root root 1.6M Apr 3 09:55 libarrow_python.so
-rwxr-xr-x 1 root root 1.4M Apr 3 09:02 libarrow_python.so.14
-rwxr-xr-x 1 root root 12M Apr 3 09:55 libarrow.so
-rwxr-xr-x 1 root root 12M Apr 3 09:02 libarrow.so.14
-rw-r--r-- 1 root root 6.1M Apr 3 09:02 lib.cpp
-rwxr-xr-x 1 root root 2.5M Apr 3 09:55 [lib.cpython-36m-x86_64-linux-gnu.so|http://lib.cpython-36m-x86_64-linux-gnu.so/]
-rwxr-xr-x 1 root root 59M Apr 3 09:55 libgandiva.so
-rwxr-xr-x 1 root root 55M Apr 3 09:02 libgandiva.so.14
-rwxr-xr-x 1 root root 3.5M Apr 3 09:55 libparquet.so
-rwxr-xr-x 1 root root 2.9M Apr 3 09:02 libparquet.so.14
-rwxr-xr-x 1 root root 345K Apr 3 09:55 libplasma.so
-rwxr-xr-x 1 root root 309K Apr 3 09:02 libplasma.so.14
{code}

Here is the output of auditwheel [https://travis-ci.org/kszucs/crossbow/builds/514605723#L3340]

They should be symlinks, we have special code for this: https://github.com/apache/arrow/blob/4495305092411e8551c60341e273c8aa3c14b282/python/setup.py#L489-L499 This is probably not going into the wheel as wheels are zip-files and they don't support symlinks by default. So we probably need to pass the `--symlinks` parameter to the wheel code.",wheel,"['Packaging', 'Python']",ARROW,Bug,Major,2019-04-10 12:11:57,14
13227005,[Python] CMake warnings when building,"{code}
-- Running cmake for pyarrow
cmake -DPYTHON_EXECUTABLE=/home/antoine/miniconda3/envs/pyarrow/bin/python  -DPYARROW_BUILD_CUDA=on -DPYARROW_BUILD_FLIGHT=on -DPYARROW_BUILD_PARQUET=on -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_BUILD_ORC=on -DPYARROW_CXXFLAGS=-fdiagnostics-color=always -Wextra -Wunused-result -Wno-unused-parameter -Wno-implicit-fallthrough -Wconversion -D_GLIBCXX_USE_CXX11_ABI=1 -DCMAKE_BUILD_TYPE=debug /home/antoine/arrow/dev/python
CMake Warning (dev) at cmake_modules/BuildUtils.cmake:27:
  Syntax Warning in cmake code at column 14

  Argument not separated from preceding token by whitespace.
Call Stack (most recent call first):
  CMakeLists.txt:87 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.

CMake Warning (dev) at cmake_modules/BuildUtils.cmake:28:
  Syntax Warning in cmake code at column 18

  Argument not separated from preceding token by whitespace.
Call Stack (most recent call first):
  CMakeLists.txt:87 (include)
This warning is for project developers.  Use -Wno-dev to suppress it.

{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2019-04-09 17:24:48,2
13226938,[Packaging][Wheel] Pin LLVM to version 7 in windows builds,"LLVM 8 conda packages has just been released, so it needs to be pinned",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-04-09 12:23:08,3
13226936,[CI] [C++] LLVM-related compile errors,"Travis-CI builds using llvm.org-originated binary packages have started failing:
{code}
[ 76%] Linking CXX executable ../../debug/gandiva-engine_llvm_test
../../debug/libgandiva.so.14.0.0: undefined reference to `typeinfo for llvm::ErrorInfoBase'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
{code}

Apparently this is because LLVM was compiled with RTTI turned off. See example report here:
https://github.com/EOSIO/eos/issues/498
",pull-request-available,"['C++', 'C++ - Gandiva', 'Continuous Integration']",ARROW,Bug,Blocker,2019-04-09 12:17:33,2
13226901,[Dev] Merge script imposes directory name,"The merge script currently fails with a cryptic message if the parent directory isn't named ""arrow"":

{code}
$ ./dev/merge_arrow_pr.py 
ARROW_HOME = /home/antoine/arrow/dev
PROJECT_NAME = dev
Which pull request would you like to merge? (e.g. 34): 4113
url = 'https://api.github.com/repos/apache/dev/pulls/4113'
{'documentation_url': 'https://developer.github.com/v3/pulls/#get-a-single-pull-request',
 'message': 'Not Found'}
Traceback (most recent call last):
  File ""./dev/merge_arrow_pr.py"", line 462, in <module>
    cli()
  File ""./dev/merge_arrow_pr.py"", line 417, in cli
    pr = PullRequest(cmd, github_api, git_remote, jira_con, pr_num)
  File ""./dev/merge_arrow_pr.py"", line 252, in __init__
    self.url = self._pr_data[""url""]
KeyError: 'url'
{code}

PROJECT_HOME is inferred from the current directory name, though it should almost always be ""arrow"".",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-04-09 09:48:51,2
13226823,[C++] Release mode lacks convenience input validation,"I am a new user of the C++ library trying to output data that contains dictionary columns using {{RecordBatchFileWriter}}. Theattached code seg faults, it is able to write a{{Feather}}file but fails when I use the{{RecordBatchFileWriter}}.

I am not sure whether I am using the library correctly, but the produced Feather file loads properly in Julia and has the expected data. I installed Arrow following the [instructions|https://arrow.apache.org/install/] for CentOS.

Here is the stacktrace of the executable compiled with the command 'g++ -g -larrow repro.cpp' :



{{Program terminated with signal SIGSEGV, Segmentation fault.}}
{{#0 0x0000000000000000 in ?? ()}}
{{Missing separate debuginfos, use: debuginfo-install arrow-libs-0.13.0-1.el7.x86_64 boost-filesystem-1.53.0-27.el7.x86_64 boost-regex-1.53.0-27.el7.x86_64 boost-system-1.53.0-27.el7.x86_64 double-conversion-2.0.1-3.el7.x86_64 gflags-2.1.1-6.el7.x86_64 glibc-2.17-260.el7_6.3.x86_64 glog-0.3.3-8.el7.x86_64 libgcc-4.8.5-36.el7.x86_64 libicu-50.1.2-17.el7.x86_64 libstdc++-4.8.5-36.el7.x86_64 libzstd-1.3.8-1.el7.x86_64 lz4-1.7.5-2.el7.x86_64 snappy-1.1.0-3.el7.x86_64 zlib-1.2.7-18.el7.x86_64}}
{{(gdb) bt}}
{{#0 0x0000000000000000 in ?? ()}}
{{#1 0x00007f43fc8870d1 in arrow::ipc::internal::FieldToFlatbufferVisitor::GetResult(arrow::Field const&, flatbuffers::Offset<org::apache::arrow::flatbuf::Field>*) ()}}
{{ from /lib64/libarrow.so.13}}
{{#2 0x00007f43fc880374 in arrow::ipc::internal::FieldToFlatbuffer(flatbuffers::FlatBufferBuilder&, arrow::Field const&, arrow::ipc::DictionaryMemo*, flatbuffers::Offset<org::apache::arrow::flatbuf::Field>*) () from /lib64/libarrow.so.13}}
{{#3 0x00007f43fc880759 in arrow::ipc::internal::SchemaToFlatbuffer(flatbuffers::FlatBufferBuilder&, arrow::Schema const&, arrow::ipc::DictionaryMemo*, flatbuffers::Offset<org::apache::arrow::flatbuf::Schema>*) [clone .constprop.548] () from /lib64/libarrow.so.13}}
{{#4 0x00007f43fc880f7f in arrow::ipc::internal::WriteSchemaMessage(arrow::Schema const&, arrow::ipc::DictionaryMemo*, std::shared_ptr<arrow::Buffer>*) () from /lib64/libarrow.so.13}}
{{#5 0x00007f43fc8986eb in arrow::ipc::RecordBatchStreamWriter::RecordBatchStreamWriterImpl::Start() () from /lib64/libarrow.so.13}}
{{#6 0x00007f43fc898936 in arrow::ipc::RecordBatchFileWriter::RecordBatchFileWriterImpl::Start() () from /lib64/libarrow.so.13}}
{{#7 0x00007f43fc891cfc in arrow::ipc::RecordBatchFileWriter::WriteRecordBatch(arrow::RecordBatch const&, bool) () from /lib64/libarrow.so.13}}
{{#8 0x00000000004022ec in job () at repro.cpp:63}}
{{#9 0x00000000004026e7 in main (argc=1, argv=0x7ffef5892268) at repro.cpp:77}}



Thanks for your help.",pull-request-available,['C++'],ARROW,Wish,Minor,2019-04-09 00:57:41,2
13226806,[Python] ParquetDataset and ParquetPiece not serializable,"Since 0.13.0, parquet instances are no longer serialisable, which means that dask.distributed cannot pass them between processes in order to load parquet in parallel.

Example:
```
>>> import cloudpickle
>>> import pyarrow.parquet as pq
>>> pf = pq.ParquetDataset('nation.impala.parquet')
>>> cloudpickle.dumps(pf)
~/anaconda/envs/py36/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in dumps(obj, protocol)
    893     try:
    894         cp = CloudPickler(file, protocol=protocol)
--> 895         cp.dump(obj)
    896         return file.getvalue()
    897     finally:

~/anaconda/envs/py36/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in dump(self, obj)
    266         self.inject_addons()
    267         try:
--> 268             return Pickler.dump(self, obj)
    269         except RuntimeError as e:
    270             if 'recursion' in e.args[0]:

~/anaconda/envs/py36/lib/python3.6/pickle.py in dump(self, obj)
    407         if self.proto >= 4:
    408             self.framer.start_framing()
--> 409         self.save(obj)
    410         self.write(STOP)
    411         self.framer.end_framing()

~/anaconda/envs/py36/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)
    519
    520         # Save the reduce() output and finally memoize the object
--> 521         self.save_reduce(obj=obj, *rv)
    522
    523     def persistent_id(self, obj):

~/anaconda/envs/py36/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    632
    633         if state is not None:
--> 634             save(state)
    635             write(BUILD)
    636

~/anaconda/envs/py36/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)
    474         f = self.dispatch.get(t)
    475         if f is not None:
--> 476             f(self, obj) # Call unbound method with explicit self
    477             return
    478

~/anaconda/envs/py36/lib/python3.6/pickle.py in save_dict(self, obj)
    819
    820         self.memoize(obj)
--> 821         self._batch_setitems(obj.items())
    822
    823     dispatch[dict] = save_dict

~/anaconda/envs/py36/lib/python3.6/pickle.py in _batch_setitems(self, items)
    845                 for k, v in tmp:
    846                     save(k)
--> 847                     save(v)
    848                 write(SETITEMS)
    849             elif n:

~/anaconda/envs/py36/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)
    494             reduce = getattr(obj, ""__reduce_ex__"", None)
    495             if reduce is not None:
--> 496                 rv = reduce(self.proto)
    497             else:
    498                 reduce = getattr(obj, ""__reduce__"", None)

~/anaconda/envs/py36/lib/python3.6/site-packages/pyarrow/_parquet.cpython-36m-darwin.so in pyarrow._parquet.ParquetSchema.__reduce_cython__()

TypeError: no default __reduce__ due to non-trivial __cinit__
```
The indicated schema instance is also referenced by the ParquetDatasetPiece s.

ref: https://github.com/dask/distributed/issues/2597",parquet pull-request-available,['Python'],ARROW,Bug,Critical,2019-04-08 22:03:22,3
13226767,[CI] Fix conda calls in AppVeyor scripts,"Our {{.bat}} files for AppVeyor CI use ""{{conda}}"" to execute Conda commands, unfortunately those exit the {{.bat}} script immediately after returning. We need to invoke Conda using ""{{call conda}}"" instead. However, doing so exposes a bug in the wheel tests that went unnoticed.

See https://github.com/apache/arrow/pull/4015
",pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-04-08 18:58:38,2
13226696,[Python/C++] Row group retrieval doesn't restore index properly,"When retrieving row groups the index is no longer properly restored to its initial value and is set to an range index starting at zero no matter what. version 0.12.1 restored and int64 index with the correct index values.


{code:python}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
print(pa.__version__)
df = pd.DataFrame(
    {""a"": [1, 2, 3, 4]}
)
print(""total DF"")
print(df.index)
table = pa.Table.from_pandas(df)
buf = pa.BufferOutputStream()
pq.write_table(table, buf, chunk_size=2)
reader = pa.BufferReader(buf.getvalue().to_pybytes())
parquet_file = pq.ParquetFile(reader)
rg = parquet_file.read_row_group(1)

df_restored = rg.to_pandas()
print(""Row group"")
print(df_restored.index)
{code}

Previous behavior
{code:python}
0.12.1
total DF
RangeIndex(start=0, stop=4, step=1)
Row group
Int64Index([2, 3], dtype='int64')
{code}

Behavior now
{code:python}
0.13.0
total DF
RangeIndex(start=0, stop=4, step=1)
Row group
RangeIndex(start=0, stop=2, step=1)
{code}",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2019-04-08 13:25:11,14
13226686,[Flight] Implement authentication APIs,"From the mailing list:
{quote}Proposal 3: Add an interface to define authentication protocols on the
 client and server, using the existing Handshake endpoint and adding a
 protocol-defined, per-call token.
{quote}",pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-04-08 12:12:46,0
13226685,[Flight] Implement call options (timeouts),"From the mailing list:
{quote}Proposal 2: In client/server APIs, add a call options parameter to
 control timeouts and provide access to the identity of the
 authenticated peer (if any).
{quote}",flight pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-04-08 12:12:05,0
13226661,[R][CI] Run nightly tests against multiple R versions,"This requires to fix the docker-compose build of R, which is failing currently:
https://travis-ci.org/kszucs/crossbow/builds/508343597

Reproducible locally with command:
{code}
docker-compose build cpp
docker-compose build r
docker-compose run r
{code}

Then introduce an {{R_VERSION}} build argument to the dockerfile, similarly like
the python docker-compose defines and uses {{PYTHON_VERSION}}, see:
- https://github.com/apache/arrow/blob/master/python/Dockerfile#L21
- https://github.com/apache/arrow/blob/master/docker-compose.yml#L247-L259

Then add to the nightly builds, similarly like python:
- https://github.com/apache/arrow/blob/master/dev/tasks/tests.yml#L29-L31
- https://github.com/apache/arrow/blob/master/dev/tasks/tests.yml#L153-L184

There is already a {{docker-r}} definition, the only difference is to export an 
{{R_VERSION}} environment variable.
",pull-request-available,"['Continuous Integration', 'R']",ARROW,Improvement,Major,2019-04-08 10:04:59,4
13226314,[Python] Cannot roundtrip extreme dates through pyarrow,"You can roundtrip many dates through a pyarrow array:


{noformat}
>>> pa.array([datetime.date(1980, 1, 1)], type=pa.date32())[0]
datetime.date(1980, 1, 1){noformat}


But(on Windows at least), not extreme ones:


{noformat}
>>> pa.array([datetime.date(1960, 1, 1)], type=pa.date32())[0]
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""pyarrow\scalar.pxi"", line 74, in pyarrow.lib.ArrayValue.__repr__
 File ""pyarrow\scalar.pxi"", line 226, in pyarrow.lib.Date32Value.as_py
OSError: [Errno 22] Invalid argument
>>> pa.array([datetime.date(3200, 1, 1)], type=pa.date32())[0]
Traceback (most recent call last):
 File ""<stdin>"", line 1, in <module>
 File ""pyarrow\scalar.pxi"", line 74, in pyarrow.lib.ArrayValue.__repr__
 File ""pyarrow\scalar.pxi"", line 226, in pyarrow.lib.Date32Value.as_py
{noformat}
This is because datetime.utcfromtimestamp and datetime.timestamp fail onthese dates, but it seems we should be able to totally avoid invoking this functionwhen deserializing dates. Ideally we would be able to roundtrip these asdatetimes too, of course, butit's less clear that this will be easy. For some context on this see[https://bugs.python.org/issue29097].

This may be related to ARROW-3176 and ARROW-4746",pull-request-available windows,['Python'],ARROW,Bug,Major,2019-04-05 07:36:59,15
13225954,[C++][Flight] Unit tests in C++ for DoPut,I believe this is only tested via Python and integration tests. We should have corresponding unit tests to the existing DoGet unit tests,pull-request-available,['C++'],ARROW,Improvement,Major,2019-04-03 20:32:00,2
13225745,[C++] Reduce header dependencies,"To tame C++ compile times, we should try to reduce the number of heavy dependencies in our .h files.

Two possible avenues come to mind:
* avoid including `unordered_map` and friends
* avoid including C++ stream libraries (such as `iostream`, `ios`, `sstream`...)

Unfortunately we're currently including `sstream` in `status.h` for some template APIs. We may move those to a separate include file (e.g. `status-builder.h`).",pull-request-available,['C++'],ARROW,Wish,Major,2019-04-03 08:53:42,2
13225735,[Packaging] Avoid bundling static libraries in Windows conda packages,"We're currently bundling static libraries in Windows conda packages. Unfortunately, it causes these to be quite large:
{code:bash}
$ ls -la ./Library/lib
total 507808
drwxrwxr-x 4 antoine antoine      4096 avril  3 10:28 .
drwxrwxr-x 5 antoine antoine      4096 avril  3 10:28 ..
-rw-rw-r-- 1 antoine antoine   1507048 avril  1 20:58 arrow.lib
-rw-rw-r-- 1 antoine antoine     76184 avril  1 20:59 arrow_python.lib
-rw-rw-r-- 1 antoine antoine  61323846 avril  1 21:00 arrow_python_static.lib
-rw-rw-r-- 1 antoine antoine 328090000 avril  1 21:02 arrow_static.lib
drwxrwxr-x 3 antoine antoine      4096 avril  3 10:28 cmake
-rw-rw-r-- 1 antoine antoine    491292 avril  1 21:02 parquet.lib
-rw-rw-r-- 1 antoine antoine 128473780 avril  1 21:03 parquet_static.lib
drwxrwxr-x 2 antoine antoine      4096 avril  3 10:27 pkgconfig
{code}
(see files in https://anaconda.org/conda-forge/arrow-cpp/files )

We should probably only ship dynamic libraries under Windows, as those are reasonably small.
",conda,"['C++', 'Packaging']",ARROW,Wish,Major,2019-04-03 08:29:49,3
13225654,[Flight][C++] Flight DoGet doesn't expose server error message,"If a server sends an error back in DoGet before sending the schema, the Flight client will report only ""no data in Flight stream"", not the actual error message.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2019-04-02 22:22:53,0
13225645,[Packaging] Add APT/Yum verification scripts,We should detect broken APT repository (ARROW-5087) before we release a new version.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-04-02 21:27:08,1
13225586,[Flight] Rename FlightGetInfo message to FlightInfo,Per mailing list discussion,pull-request-available,['FlightRPC'],ARROW,Improvement,Major,2019-04-02 17:17:35,2
13225448,[C++/Python] Writing dictionary encoded columns to parquet is extremely slow when using chunk size,"Currently, there is a workaround for dict encoded columns in place to handle writing dict encoded columns to parquet.

The workaround converts the dict encoded array to its plain version before writing to parquet. This is painfully slow since for every row group the entire array is converted over and over again.

The following example is orders of magnitude slower than the non-dict encoded version:
{code}
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
df = pd.DataFrame({""col"": [""A"", ""B""] * 100000}).astype(""category"")
table = pa.Table.from_pandas(df)
buf = pa.BufferOutputStream()
pq.write_table(
    table,
    buf,
    chunk_size=100,
)
{code}",parquet performance,['C++'],ARROW,Bug,Major,2019-04-02 08:40:16,14
13225411,[C++] Do not set -Werror when using BUILD_WARNING_LEVEL=CHECKIN in release mode,I will also document this issue better in the Sphinx project. See discussion in ARROW-4520,pull-request-available,['C++'],ARROW,Improvement,Major,2019-04-02 02:03:30,14
13225328,[Python] Space leak in  ParquetFile.read_row_group(),"I have a code pattern like this:



reader = pq.ParquetFile(path)

for ix in range(0, reader.num_row_groups):
 table = reader.read_row_group(ix, columns=self._columns)
 # operate on table



But it leaks memory over time, only releasing it when the reader object is collected. Here's a workaround



num_row_groups = pq.ParquetFile(path).num_row_groups

for ix in range(0, num_row_groups):
 table = pq.ParquetFile(path).read_row_group(ix, columns=self._columns)
 # operate on table



This puts an upper bound on memory usage and is what I'd expect from the code. I also put gc.collect() to the end of every loop.



I charted out memory usage for a small benchmark that just copies a file, one row group at a time, converting to pandas and back to arrow on the writer path. Line in black is the first one, using a single reader object. Blue is instantiating a fresh reader in every iteration.",parquet pull-request-available,['Python'],ARROW,Bug,Major,2019-04-01 17:10:50,14
13225311,[Python/C++] Conversion of dict encoded null column fails in parquet writing when using RowGroups,"Conversion of dict encoded null column fails in parquet writing when using RowGroups

{code:python}
import pyarrow.parquet as pq
import pandas as pd
import pyarrow as pa
df = pd.DataFrame({""col"": [None] * 100, ""int"": [1.0] * 100})
df = df.astype({""col"": ""category""})
table = pa.Table.from_pandas(df)
buf = pa.BufferOutputStream()
pq.write_table(
    table,
    buf,
    version=""2.0"",
    chunk_size=10,
)
{code}

fails with 

{{pyarrow.lib.ArrowIOError: Column 2 had 100 while previous column had 10}}",parquet pull-request-available,['C++'],ARROW,Bug,Minor,2019-04-01 15:54:35,14
13225308,[Website] Blog post / release announcement for 0.13.0,Blog post like past releases,pull-request-available,['Website'],ARROW,Improvement,Major,2019-04-01 15:31:56,14
13225307,"[Developer] In merge_arrow_pr.py script, allow user to set a released Fix Version","This is disallowed by the tool right now, I think this should probably be allowed",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-04-01 15:29:49,14
13225301,[Python][Packaging] Reduce size of macOS and manylinux1 wheels,The wheels more than tripled in size from 0.12.0 to 0.13.0. I think this is mostly because of LLVM but we should take a closer look to see if the size can be reduced,pull-request-available wheel,['Python'],ARROW,Improvement,Major,2019-04-01 15:12:49,14
13225271,[C++] Consistently use PATH_SUFFIXES in CMake config,"In trying to set up a build using system libraries installed to non-default paths, CMake doesn't consistently search user-specified paths for libraries.

For instance, FindDoubleConversion.cmake will look only at ${DoubleConversion_ROOT}/libdoubleconversion.so for the shared library, making it impossible to have a directory setup like doubleconversion/lib/*.so + doubleconversion/include. Other Find*.cmake files set PATH_SUFFIXES to also search the lib/ subdirectory; we should do this everywhere.

Additionally, it seems the various Find*.cmake files set PATH_SUFFIXES inconsistently. Some hardcode their own list, others use CMAKE_LIBRARY_ARCHITECTURE.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-04-01 12:52:58,0
13225148,[Documentation] Sphinx is failed by RemovedInSphinx30Warning,"https://travis-ci.org/apache/arrow/jobs/513850506

{noformat}
/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/sphinx/util/docutils.py:311: RemovedInSphinx30Warning: function based directive support is now deprecated. Use class based directive instead.
  RemovedInSphinx30Warning)
Exception occurred:
  File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/breathe/renderer/sphinxrenderer.py"", line 38, in parse_definition
    ast = parser.parse_declaration(""class"")
TypeError: parse_declaration() missing 1 required positional argument: 'directiveType'
The full traceback has been saved in /tmp/sphinx-err-i64grj3x.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
{noformat}",pull-request-available,['Documentation'],ARROW,Improvement,Major,2019-04-01 02:22:05,3
13224856,[Benchmarking] Performs a benchmark run with archery,"Run all regression benchmarks, consume output and re-format according to the format required by dev/benchmarking specification and/or push to upstream database.

This would be implemented as `archery benchmark run`. Provide facility to save/load results as a StaticRunner (such that it can be re-used in comparison without running the benchmark again).",pull-request-available,['C++'],ARROW,New Feature,Major,2019-03-29 15:36:52,13
13224714,[Java] FlightClient should not create a child allocator,"I ran into a problem when testing out Flight using the ExampleFlightServer with InMemoryStore producer.

A client will iterate over endpoints and locations to get the streams, and the example creates a new client for each location. The only way to close the allocator in the FlightClient is to close the FlightClient, which also closes the read channel. If the location is the same for each FlightStream (as is the case for the InMemoryStore), then it seems like grpc will reuse the channel, so closing one read client will shutdown the channel and the remaining FlightStreams cannot be read.

If an allocator was created by the owner of the FlightClient, then the client would not need to close it and this problem would be avoided. I believe other Flight classes do not create child allocators either, so this change would be consistent.",pull-request-available,"['FlightRPC', 'Java']",ARROW,Improvement,Major,2019-03-28 22:47:35,0
13224640,[Packaging] Adjust conda recipes to use ORC conda-forge package on unix systems ,Instead of building orc_ep.,pull-request-available,['Packaging'],ARROW,Task,Major,2019-03-28 16:48:44,3
13224625,[Rust] [DataFusion] Use env var for location of arrow test data,"A small number of tests have hard coded relative path for arrow test data files, meaning that the tests fail in the release tarball.

We should use an ARROW_TEST_DATA env var similar to how we deal with PARQUET_TEST_DATA",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2019-03-28 15:42:47,10
13224558,[GLib][Gandiva] Test failure in release verification script,"I got this:
{code}
     23:   end
     24: 
     25:   def test_value
  => 26:     assert_equal(@value, @node.value)
     27:   end
     28: 
     29:   def test_return_type
/tmp/arrow-0.13.0.NUWtu/apache-arrow-0.13.0/c_glib/test/gandiva/test-string-literal-node.rb:26:in `test_value'
<""Hello""> expected but was
<""\u00106q"">
Failure: test_value(TestGandivaStringLiteralNode)
{code}

When running the verification script:
{code}
./dev/release/verify-release-candidate.sh source 0.13.0 4
{code}

The value displayed above looks like corrupted memory...
",pull-request-available,['GLib'],ARROW,Bug,Major,2019-03-28 10:39:44,1
13224471,[Release][Rust] arrow-testing is missing in verification script,"{noformat}
failures:

---- execution::projection::tests::project_first_column stdout ----
thread 'execution::projection::tests::project_first_column' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5
note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.

---- execution::aggregate::tests::max_f64_group_by_string stdout ----
thread 'execution::aggregate::tests::max_f64_group_by_string' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5

---- execution::aggregate::tests::test_min_max_sum_f64_group_by_uint32 stdout ----
thread 'execution::aggregate::tests::test_min_max_sum_f64_group_by_uint32' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5

---- execution::aggregate::tests::min_f64_group_by_string stdout ----
thread 'execution::aggregate::tests::min_f64_group_by_string' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: ""No such file or directory"" }', src/libcore/result.rs:997:5


failures:
    execution::aggregate::tests::max_f64_group_by_string
    execution::aggregate::tests::min_f64_group_by_string
    execution::aggregate::tests::test_min_max_sum_f64_group_by_uint32
    execution::projection::tests::project_first_column

test result: FAILED. 36 passed; 4 failed; 0 ignored; 0 measured; 0 filtered out

error: test failed, to rerun pass '-p datafusion --lib'
{noformat}",pull-request-available,"['Packaging', 'Rust']",ARROW,Bug,Major,2019-03-28 01:58:28,1
13224440,[Release][Rust] Format error in verification script,"{noformat}
+ cargo fmt --all -- --check
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/arrow/src/csv/writer.rs at line 53:
 //! let batch = RecordBatch::try_new(
 //!     Arc::new(schema),
 //!     vec![Arc::new(c1), Arc::new(c2), Arc::new(c3), Arc::new(c4)],
-//! ).unwrap();
+//! )
+//! .unwrap();
 //!
 //! let file = get_temp_file(""out.csv"", &[]);
 //!
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/datasource/datasource.rs at line 24:
 
 use crate::error::Result;
 
-/// Returned by implementors of `Table#scan`, this `RecordBatchIterator` is wrapped with an `Arc`
-/// and `Mutex` so that it can be shared across threads as it is used.
+/// Returned by implementors of `Table#scan`, this `RecordBatchIterator` is wrapped with
+/// an `Arc` and `Mutex` so that it can be shared across threads as it is used.
 pub type ScanResult = Arc<Mutex<RecordBatchIterator>>;
 
 /// Source table
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/datasource/datasource.rs at line 33:
     /// Get a reference to the schema for this table
     fn schema(&self) -> &Arc<Schema>;
 
-    /// Perform a scan of a table and return a sequence of iterators over the data (one iterator per partition)
+    /// Perform a scan of a table and return a sequence of iterators over the data (one
+    /// iterator per partition)
     fn scan(
         &self,
         projection: &Option<Vec<usize>>,
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/datasource/memory.rs at line 16:
 // under the License.
 
 //! In-memory data source for presenting a Vec<RecordBatch> as a data source that can be
-//! queried by DataFusion. This allows data to be pre-loaded into memory and then repeatedly
-//! queried without incurring additional file I/O overhead.
+//! queried by DataFusion. This allows data to be pre-loaded into memory and then
+//! repeatedly queried without incurring additional file I/O overhead.
 
 use std::sync::{Arc, Mutex};
 
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/context.rs at line 15:
 // specific language governing permissions and limitations
 // under the License.
 
-//! ExecutionContext contains methods for registering data sources and executing SQL queries
+//! ExecutionContext contains methods for registering data sources and executing SQL
+//! queries
 
 use std::cell::RefCell;
 use std::collections::HashMap;
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/context.rs at line 139:
         Ok(plan)
     }
 
-    /// Execute a logical plan and produce a Relation (a schema-aware iterator over a series
-    /// of RecordBatch instances)
+    /// Execute a logical plan and produce a Relation (a schema-aware iterator over a
+    /// series of RecordBatch instances)
     pub fn execute(
         &mut self,
         plan: &LogicalPlan,
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/filter.rs at line 15:
 // specific language governing permissions and limitations
 // under the License.
 
-//! Execution of a filter (predicate) relation. The SQL clause `WHERE expr` represents a filter.
+//! Execution of a filter (predicate) relation. The SQL clause `WHERE expr` represents a
+//! filter.
 
 use std::cell::RefCell;
 use std::rc::Rc;
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/filter.rs at line 32:
 
 /// Implementation of a filter relation
 pub(super) struct FilterRelation {
-    /// The schema for the filter relation. This is always the same as the schema of the input relation.
+    /// The schema for the filter relation. This is always the same as the schema of the
+    /// input relation.
     schema: Arc<Schema>,
     /// Relation that is  being filtered
     input: Rc<RefCell<Relation>>,
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/limit.rs at line 33:
 pub(super) struct LimitRelation {
     /// The relation which the limit is being applied to
     input: Rc<RefCell<Relation>>,
-    /// The schema for the limit relation, which is always the same as the schema of the input relation
+    /// The schema for the limit relation, which is always the same as the schema of the
+    /// input relation
     schema: Arc<Schema>,
     /// The number of rows returned by this relation
     limit: usize,
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/projection.rs at line 15:
 // specific language governing permissions and limitations
 // under the License.
 
-//! Defines the projection relation. A projection determines which columns or expressions are
-//! returned from a query. The SQL statement `SELECT a, b, a+b FROM t1` is an example of a
-//! projection on table `t1` where the expressions `a`, `b`, and `a+b` are the projection
-//! expressions.
+//! Defines the projection relation. A projection determines which columns or expressions
+//! are returned from a query. The SQL statement `SELECT a, b, a+b FROM t1` is an example
+//! of a projection on table `t1` where the expressions `a`, `b`, and `a+b` are the
+//! projection expressions.
 
 use std::cell::RefCell;
 use std::rc::Rc;
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/relation.rs at line 16:
 // under the License.
 
 //! A relation is a representation of a set of tuples. A database table is a
-//! type of relation. During query execution, each operation on a relation (such as projection,
-//! selection, aggregation) results in a new relation.
+//! type of relation. During query execution, each operation on a relation (such as
+//! projection, selection, aggregation) results in a new relation.
 
 use std::sync::{Arc, Mutex};
 
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/optimizer.rs at line 21:
 use crate::logicalplan::LogicalPlan;
 use std::sync::Arc;
 
-/// An optimizer rules performs a transformation on a logical plan to produce an optimized logical plan.
+/// An optimizer rules performs a transformation on a logical plan to produce an optimized
+/// logical plan.
 pub trait OptimizerRule {
     /// Perform optimizations on the plan
     fn optimize(&mut self, plan: &LogicalPlan) -> Result<Arc<LogicalPlan>>;
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/projection_push_down.rs at line 142:
                 schema,
                 ..
             } => {
-                // once we reach the table scan, we can use the accumulated set of column indexes as
-                // the projection in the table scan
+                // once we reach the table scan, we can use the accumulated set of column
+                // indexes as the projection in the table scan
                 let mut projection: Vec<usize> = Vec::with_capacity(accum.len());
                 accum.iter().for_each(|i| projection.push(*i));
 
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/projection_push_down.rs at line 158:
                 }
                 let projected_schema = Schema::new(projected_fields);
 
-                // now that the table scan is returning a different schema we need to create a
-                // mapping from the original column index to the new column index so that we
-                // can rewrite expressions as we walk back up the tree
+                // now that the table scan is returning a different schema we need to
+                // create a mapping from the original column index to the
+                // new column index so that we can rewrite expressions as
+                // we walk back up the tree
 
                 if mapping.len() != 0 {
                     return Err(ExecutionError::InternalError(
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/type_coercion.rs at line 17:
 
 //! The type_coercion optimizer rule ensures that all binary operators are operating on
 //! compatible types by adding explicit cast operations to expressions. For example,
-//! the operation `c_float + c_int` would be rewritten as `c_float + CAST(c_int AS float)`.
-//! This keeps the runtime query execution code much simpler.
+//! the operation `c_float + c_int` would be rewritten as `c_float + CAST(c_int AS
+//! float)`. This keeps the runtime query execution code much simpler.
 
 use std::sync::Arc;
 
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/utils.rs at line 24:
 use crate::error::{ExecutionError, Result};
 use crate::logicalplan::Expr;
 
-/// Recursively walk a list of expression trees, collecting the unique set of column indexes
-/// referenced in the expression
+/// Recursively walk a list of expression trees, collecting the unique set of column
+/// indexes referenced in the expression
 pub fn exprlist_to_column_indices(expr: &Vec<Expr>, accum: &mut HashSet<usize>) {
     expr.iter().for_each(|e| expr_to_column_indices(e, accum));
 }
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/table.rs at line 15:
 // specific language governing permissions and limitations
 // under the License.
 
-//! Table API for building a logical query plan. This is similar to the Table API in Ibis and
-//! the DataFrame API in Apache Spark
+//! Table API for building a logical query plan. This is similar to the Table API in Ibis
+//! and the DataFrame API in Apache Spark
 
 use crate::error::Result;
 use crate::logicalplan::LogicalPlan;
Diff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/tests/sql.rs at line 129:
     assert_eq!(expected, actual);
 }
 
-//TODO Uncomment the following test when ORDER BY is implemented to be able to test ORDER BY + LIMIT
+//TODO Uncomment the following test when ORDER BY is implemented to be able to test ORDER
+// BY + LIMIT
 /*
 #[test]
 fn csv_query_limit_with_order_by() {

{noformat}",pull-request-available,"['Packaging', 'Rust']",ARROW,Bug,Major,2019-03-27 22:01:27,1
13224419,[Release][C++] use bundled gtest and gmock in verify-release-candidate.bat,"bundled GTest is currently required on Windows due to link issues with conda's GTest: https://github.com/apache/arrow/pull/3984 

[verify-release-candidate.bat|https://github.com/apache/arrow/blob/master/dev/release/verify-release-candidate.bat#L71] doesn't include this flag.",pull-request-available,"['C++', 'Packaging']",ARROW,New Feature,Minor,2019-03-27 19:58:30,6
13224359,[Rust] [DataFusion] Refactor aggregate module,"The PR splits out the various aggregate functions into separate files and adds a new `aggregate_array` method to the aggregate function trait, allowing more logic to be delegated to each function.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-27 16:25:53,10
13224325,[C++] Compilation warnings in release mode,Those come up when running the release verification script.,pull-request-available,['C++'],ARROW,Bug,Major,2019-03-27 14:26:12,2
13224323,[Python][C++] Creating list<string> with pyarrow.array can overflow child builder,"I am sorry if this bugs feels rather long and the reproduction data is large, but I was not able to reduce the data even further while still triggering the problem. I was able to trigger this behavior on master and on {{0.11.1}}.

{code:python}
import io
import os.path
import pickle

import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq


def dct_to_table(index_dct):
    labeled_array = pa.array(np.array(list(index_dct.keys())))
    partition_array = pa.array(np.array(list(index_dct.values())))

    return pa.Table.from_arrays(
        [labeled_array, partition_array], names=['a', 'b']
    )


def check_pq_nulls(data):
    fp = io.BytesIO(data)
    pfile = pq.ParquetFile(fp)
    assert pfile.num_row_groups == 1
    md = pfile.metadata.row_group(0)
    col = md.column(1)
    assert col.path_in_schema == 'b.list.item'
    assert col.statistics.null_count == 0  # fails


def roundtrip(table):
    buf = pa.BufferOutputStream()
    pq.write_table(table, buf)

    data = buf.getvalue().to_pybytes()

    # this fails:
    #   check_pq_nulls(data)

    reader = pa.BufferReader(data)
    return pq.read_table(reader)


with open(os.path.join(os.path.dirname(__file__), 'dct.pickle'), 'rb') as fp:
    dct = pickle.load(fp)


# this does NOT help:
#   pa.set_cpu_count(1)
#   import gc; gc.disable()

table = dct_to_table(dct)

# this fixes the issue:
#   table = pa.Table.from_pandas(table.to_pandas())

table2 = roundtrip(table)

assert table.column('b').null_count == 0
assert table2.column('b').null_count == 0  # fails

# if table2 is converted to pandas, you can also observe that some values at the end of column b are `['']` which clearly is not present in the original data
{code}

I would also be thankful for any pointers on where the bug comes from or on who to reduce the test case.",parquet pull-request-available,['Python'],ARROW,Bug,Major,2019-03-27 14:24:15,14
13224209,[Python][Packaging] conda package on non Windows is broken,"https://travis-ci.org/kou/crossbow/builds/511831955

{noformat}
-- Could not find the Gandiva library. Looked for headers in $PREFIX/include, and for libs in $PREFIX/lib
CMake Error at CMakeLists.txt:509 (message):
  Unable to locate Gandiva libraries
{noformat}",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-03-27 05:05:38,1
13224192,[Python][Packaging] wheel for Windows are broken,"[https://ci.appveyor.com/project/kou/crossbow/builds/23383931]


{noformat}
-- Found the Gandiva core library: C:/Miniconda35-x64/envs/arrow/Library/lib/gandiva.lib
CMake Error: File C:/Miniconda35-x64/envs/arrow/Library/lib/gandiva.dll does not exist.
CMake Error at CMakeLists.txt:218 (configure_file):
configure_file Problem configuring file
Call Stack (most recent call first):
CMakeLists.txt:518 (bundle_arrow_lib){noformat}",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-03-27 02:38:37,1
13224158,[C++][Gandiva] Split Gandiva-related conda packages for builds into separate .yml conda env file,These installs are large and should not be required unconditionally in CI and elsewhere,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-03-26 22:23:48,2
13224077,[Rust] [DataFusion] Refactor runtime expression support,Refactor the runtime/compiled expression support to fix tech debt and prepare for implementing COUNT,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-26 16:08:01,10
13224058,"[C++] ""testing"" headers not installed",The {{src/arrow/testing}} headers should be installed along the rest.,pull-request-available,['C++'],ARROW,Bug,Major,2019-03-26 14:58:36,2
13224020,[Release] Add support in the source release script for custom hash,This is a minor feature to help debugging said script on a by overriding the git-archive hash instead of the hash inferred from the release tag.,pull-request-available,['Developer Tools'],ARROW,Improvement,Trivial,2019-03-26 12:10:06,13
13224015,[Release] Fix release script with llvm-7,Source release script fails to compile gandiva because it requires llvm-7 and only llvm-6 is available in the ubuntu18 docker image.,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-03-26 11:46:42,13
13223958,[C++] Cleanup using to std::* in files,"This can also affect parquet.

We inconsistently use std::string, std::vector and std::shared_ptr, this will be an attempt to consistently use std::* instead of do ""use std::vector"".  This is more of suggestion, so if people are opposed to it (or some of the changes).  i'm OK not checking them in.  For now I plan on doing one pull request which will include parquet.",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Bug,Minor,2019-03-26 06:35:28,15
13223847,[C++] Move DCHECK out of sse-utils ,"Some users tried to compile arrow on ppc64, but they face the following error

{code:bash}
In file included from /root/repos/arrow/cpp/src/arrow/json/chunker.h:26:0,
                 from /root/repos/arrow/cpp/src/arrow/json/chunker.cc:18:
/root/repos/arrow/cpp/src/arrow/util/sse-util.h: In function __m128i arrow::SSE4_cmpestrm(__m128i, int, __m128i, int):
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:125:3: error: there are no arguments to DCHECK that depend on a template parameter, so a declaration of DCHECK must be available [-fpermissive]
   DCHECK(false) << ""CPU doesn't support SSE 4.2"";
   ^~~~~~
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:125:3: note: (if you use -fpermissive, G++ will accept your code, but allowing the use of an undeclared name is deprecated)
/root/repos/arrow/cpp/src/arrow/util/sse-util.h: In function int arrow::SSE4_cmpestri(__m128i, int, __m128i, int):
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:131:3: error: there are no arguments to DCHECK that depend on a template parameter, so a declaration of DCHECK must be available [-fpermissive]
   DCHECK(false) << ""CPU doesn't support SSE 4.2"";
   ^~~~~~
/root/repos/arrow/cpp/src/arrow/util/sse-util.h: In function uint32_t arrow::SSE4_crc32_u8(uint32_t, uint8_t):
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:136:3: error: DCHECK was not declared in this scope
   DCHECK(false) << ""SSE support is not enabled"";
   ^~~~~~
/root/repos/arrow/cpp/src/arrow/util/sse-util.h: In function uint32_t arrow::SSE4_crc32_u16(uint32_t, uint16_t):
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:141:3: error: DCHECK was not declared in this scope
   DCHECK(false) << ""SSE support is not enabled"";
   ^~~~~~
/root/repos/arrow/cpp/src/arrow/util/sse-util.h: In function uint32_t arrow::SSE4_crc32_u32(uint32_t, uint32_t):
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:146:3: error: DCHECK was not declared in this scope
   DCHECK(false) << ""SSE support is not enabled"";
   ^~~~~~
/root/repos/arrow/cpp/src/arrow/util/sse-util.h: In function uint32_t arrow::SSE4_crc32_u64(uint32_t, uint64_t):
/root/repos/arrow/cpp/src/arrow/util/sse-util.h:151:3: error: DCHECK was not declared in this scope
   DCHECK(false) << ""SSE support is not enabled"";
{code}

By importing `logging.h` or removing `DCHECK`, they can compile. The fix should be to refactor the SSE detection macro out of this file such that the needing code does not need to import this file and only a header with macro detection.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-03-25 19:08:57,13
13223379,[Python] Fix deprecation warning from setup.py,"I noticed on Python 3.7 today

{code}
Bundling includes: debug/include
setup.py:441: DeprecationWarning: SO is deprecated, use EXT_SUFFIX
  suffix = sysconfig.get_config_var('SO')
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-03-22 15:47:08,4
13223186,[C++] Display summary at the end of CMake configuration,"Some third-party projects like Thrift display a nice and useful summary of the build configuration at the end of the CMake configuration run:
https://ci.appveyor.com/project/pitrou/arrow/build/job/mgi68rvk0u5jf2s4?fullLog=true#L2325

It may be good to have a similar thing in Arrow as well. Bonus points if, for each configuration item, it says which CMake variable can be used to influence it.

Something like:
{code}
-- Build ZSTD support:             ON  [change using ARROW_WITH_ZSTD]
-- Build BZ2 support:              OFF [change using ARROW_WITH_BZ2]
{code}
",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-03-21 20:06:22,6
13223150,[C++] Builds fails to find Ubuntu-packaged re2 library,It seems it's not possible currently to build from a Conda environment but get re2 from the system Ubuntu package (it previously was).,pull-request-available,['C++'],ARROW,Bug,Major,2019-03-21 17:31:40,2
13223065,[Flight][C++] Flight server segfaults when port is in use,"If a Flight server tries to bind to a port in use, it segfaults (as impl_->server_ will be nullptr).",flight pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Minor,2019-03-21 12:54:09,0
13222797,[C++] Set RPATH in correct order for test executables on OSX,"to prevent picking up the already installed libarrow:

{code}
[==========] Running 16 tests from 16 test cases.
[----------] Global test environment set-up.
[----------] 1 test from TestNullScalar
[ RUN      ] TestNullScalar.Basics
[       OK ] TestNullScalar.Basics (0 ms)
[----------] 1 test from TestNullScalar (0 ms total)

[----------] 1 test from TestNumericScalar/0, where TypeParam = arrow::UInt8Type
[ RUN      ] TestNumericScalar/0.Basics
dyld: lazy symbol binding failed: Symbol not found: __ZNK5arrow6Scalar6EqualsERKS0_
  Referenced from: /Users/krisz/Workspace/arrow/python/../cpp/build/debug/arrow-scalar-test
  Expected in: /Users/krisz/.conda/envs/arrow36/lib/libarrow.13.dylib

dyld: Symbol not found: __ZNK5arrow6Scalar6EqualsERKS0_
  Referenced from: /Users/krisz/Workspace/arrow/python/../cpp/build/debug/arrow-scalar-test
  Expected in: /Users/krisz/.conda/envs/arrow36/lib/libarrow.13.dylib
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-20 10:24:27,3
13222734,[Rust] StructArray builder and From<> methods should check that field types match schema,"Similar to how we assert that array data types are equal to their field types, we should do the same for StructArray and StructBuilder where necessary",pull-request-available,['Rust'],ARROW,Improvement,Minor,2019-03-20 05:39:35,12
13222573,[C++][Python] Add GTest_SOURCE=BUNDLED to relevant build docs that use conda-forge toolchain,"The conda-forge gtest packages don't work for me on Windows. In the meantime, it is necessary to use BUNDLED method as we are currently already doing in our Appveyor builds, so we should update the documentation so others don't hit this rough edge

{code}
util-internal-test.obj : error LNK2001: unresolved external symbol ""class testing::internal::Mutex testing::internal::g_gmock_mutex"" (?g_gmock_mutex@internal@testing@@3VMutex@12@A) [C:\Users\wesmc\code\arrow\cp p\build\src\arrow\compute\kernels\arrow-compute-util-internal-test.vcxproj]
util-internal-test.obj : error LNK2001: unresolved external symbol ""class testing::internal::ThreadLocal<class testing::Sequence *> testing::internal::g_gmock_implicit_sequence"" (?g_gmock_implicit_sequence@inte rnal@testing@@3V?$ThreadLocal@PEAVSequence@testing@@@12@A) [C:\Users\wesmc\code\arrow\cpp\build\src\arrow\compute\kernels\arrow-compute-util-internal-test.vcxproj]
C:\Users\wesmc\code\arrow\cpp\build\release\Release\arrow-compute-util-internal-test.exe : fatal error LNK1120: 2 unresolved externals [C:\Users\wesmc\code\arrow\cpp\build\src\arrow\compute\kernels\arrow-comput e-util-internal-test.vcxproj]
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-19 13:09:47,14
13222529,[R] Add crossbow task for r-arrow-feedstock,We also have an R package on conda-forge now: [https://github.com/conda-forge/r-arrow-feedstock]This should be tested using crossbow as we do with the other packages.,pull-request-available,"['Packaging', 'R']",ARROW,Improvement,Major,2019-03-19 09:51:38,8
13222466,[C++] Purely static linking broken,"On the current master,816c10d030842a1a0da4d00f95a5e3749c86a74f (#3965), running


{code:java}
docker-compose build cpp
docker-compose run cpp-static-only{code}
yields
{code:java}
[357/382] Linking CXX executable debug/parquet-encoding-benchmark

FAILED: debug/parquet-encoding-benchmark

: && /opt/conda/bin/ccache /usr/bin/g++ -Wno-noexcept-type -fdiagnostics-color=always -ggdb -O0 -Wall -Wno-conversion -Wno-sign-conversion -Werror -msse4.2 -g -rdynamic src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o -o debug/parquet-encoding-benchmark -Wl,-rpath,/opt/conda/lib /opt/conda/lib/libbenchmark_main.a debug/libparquet.a /opt/conda/lib/libbenchmark.a debug/libarrow.a /opt/conda/lib/libdouble-conversion.a /opt/conda/lib/libbrotlienc.so /opt/conda/lib/libbrotlidec.so /opt/conda/lib/libbrotlicommon.so /opt/conda/lib/libbz2.so /opt/conda/lib/liblz4.so /opt/conda/lib/libsnappy.so.1.1.7 /opt/conda/lib/libz.so /opt/conda/lib/libzstd.so orc_ep-install/lib/liborc.a /opt/conda/lib/libprotobuf.so /opt/conda/lib/libglog.so /opt/conda/lib/libboost_system.so /opt/conda/lib/libboost_filesystem.so jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt /opt/conda/lib/libboost_regex.so /opt/conda/lib/libthrift.so && :

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult::AppendMessage(testing::Message const&)':

/opt/conda/include/gtest/gtest.h:352: undefined reference to `testing::Message::GetString[abi:cxx11]() const'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `parquet::BenchmarkDecodeArrow::InitDataInputs()':

/arrow/cpp/src/parquet/encoding-benchmark.cc:201: undefined reference to `arrow::random::RandomArrayGenerator::StringWithRepeats(long, long, int, int, double)'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `parquet::BM_DictDecodingByteArray::DoEncodeData()':

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::internal::AlwaysTrue()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::internal::AlwaysTrue()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::Message::Message()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::internal::AssertHelper::AssertHelper(testing::TestPartResult::Type, char const*, int, char const*)'

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::internal::AssertHelper::operator=(testing::Message const&) const'

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::internal::AssertHelper::~AssertHelper()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:321: undefined reference to `testing::Message::Message()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:321: undefined reference to `testing::internal::AssertHelper::AssertHelper(testing::TestPartResult::Type, char const*, int, char const*)'

/arrow/cpp/src/parquet/encoding-benchmark.cc:321: undefined reference to `testing::internal::AssertHelper::operator=(testing::Message const&) const'

/arrow/cpp/src/parquet/encoding-benchmark.cc:321: undefined reference to `testing::internal::AssertHelper::~AssertHelper()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:317: undefined reference to `testing::internal::AssertHelper::~AssertHelper()'

/arrow/cpp/src/parquet/encoding-benchmark.cc:321: undefined reference to `testing::internal::AssertHelper::~AssertHelper()'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::internal::scoped_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::reset(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*)':

/opt/conda/include/gtest/internal/gtest-port.h:1215: undefined reference to `testing::internal::IsTrue(bool)'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult testing::internal::CmpHelperNE<parquet::DictEncoder<parquet::DataType<(parquet::Type::type)6> >*, decltype(nullptr)>(char const*, char const*, parquet::DictEncoder<parquet::DataType<(parquet::Type::type)6> >* const&, decltype(nullptr) const&)':

/opt/conda/include/gtest/gtest.h:1573: undefined reference to `testing::AssertionSuccess()'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::internal::scoped_ptr<std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> > >::reset(std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >*)':

/opt/conda/include/gtest/internal/gtest-port.h:1215: undefined reference to `testing::internal::IsTrue(bool)'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult testing::internal::CmpHelperOpFailure<parquet::DictEncoder<parquet::DataType<(parquet::Type::type)6> >*, decltype(nullptr)>(char const*, char const*, parquet::DictEncoder<parquet::DataType<(parquet::Type::type)6> >* const&, decltype(nullptr) const&, char const*)':

/opt/conda/include/gtest/gtest.h:1541: undefined reference to `testing::AssertionFailure()'

/opt/conda/include/gtest/gtest.h:1543: undefined reference to `testing::AssertionResult::AssertionResult(testing::AssertionResult const&)'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult& testing::AssertionResult::operator<< <char [12]>(char const (&) [12])':

/opt/conda/include/gtest/gtest.h:335: undefined reference to `testing::Message::Message()'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult& testing::AssertionResult::operator<< <char const*>(char const* const&)':

/opt/conda/include/gtest/gtest.h:335: undefined reference to `testing::Message::Message()'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult& testing::AssertionResult::operator<< <char [3]>(char const (&) [3])':

/opt/conda/include/gtest/gtest.h:335: undefined reference to `testing::Message::Message()'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult& testing::AssertionResult::operator<< <std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':

/opt/conda/include/gtest/gtest.h:335: undefined reference to `testing::Message::Message()'

src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o: In function `testing::AssertionResult& testing::AssertionResult::operator<< <char [5]>(char const (&) [5])':

/opt/conda/include/gtest/gtest.h:335: undefined reference to `testing::Message::Message()'

collect2: error: ld returned 1 exit status

[366/382] Linking CXX executable debug/parquet-column_writer-test

ninja: build stopped: subcommand failed.{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-19 01:54:40,8
13222452,[Rust] [DataFusion] Implement get_supertype correctly,The current implementation ofget_supertype (used in type coercion logic) is very hacky and should be re-implemented with better unit tests as well.,beginner pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-19 00:31:51,9
13222404,[Python] test failure with Flight enabled,"When building with Flight enabled, I get the following failure:
{code:java}
Traceback (most recent call last):
  File ""/home/antoine/arrow/python/pyarrow/tests/test_array.py"", line 43, in test_total_bytes_allocated
    assert pa.total_allocated_bytes() == 0
AssertionError: assert 256 == 0
 +  where 256 = <built-in function total_allocated_bytes>()
 +    where <built-in function total_allocated_bytes> = pa.total_allocated_bytes
{code}",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2019-03-18 20:06:54,2
13222369,[C++] Equals / ApproxEquals behaviour undefined on FP NaNs,"On floating-point NaN values, the behaviour of Array::Equals and Array::ApproxEquals is currently undefined. It should probably default to something useful (and possibly be overridable using a bool parameter).",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-18 17:05:58,2
13222362,[C++] Turn off cpp benchmarks in cpp docker images,To prevent timeouts on travis: https://travis-ci.org/kszucs/crossbow/builds/507811466,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2019-03-18 16:28:36,3
13222360,[C++] Thirdparty CMake error get_target_property() called with non-existent target LZ4::lz4,"With CMake 3.2 https://travis-ci.org/kszucs/crossbow/builds/507811485

{code}
docker-compose build cpp-cmake32
docker-compose run --rm cpp-cmake32
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-18 16:25:32,8
13222310,[Flight][C++/Python] Remove redundant schema parameter in DoGet,"Now that the Flight implementations are consistent and DoGet streams are self-describing, we should remove the schema parameter to DoGet in C++/Python, as it isn't actually used anywhere. We should also enforce that the first message in the stream is the schema (Java implicitly does this already).",pull-request-available,"['C++', 'FlightRPC', 'Python']",ARROW,Improvement,Minor,2019-03-18 13:09:38,0
13222308,[C++] Support detection of flatbuffers without FlatbuffersConfig.cmake ,This is not always installed:https://github.com/ContinuumIO/anaconda-issues/issues/10731,pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2019-03-18 12:59:02,8
13222306,[Flight] Enable Flight integration tests in Travis,Need a way to mark the dictionary tests as XFAIL,Travis flight pull-request-available travis travis-ci,"['Continuous Integration', 'FlightRPC']",ARROW,Improvement,Major,2019-03-18 12:57:05,0
13222275,[C++]Raise minimal required thrift-cpp to 0.11 in conda environment,This is mainly for installing one the latest Thrift versions using conda and to speed up installation times a bit.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-18 10:35:00,8
13222135,[C++]CMake fails on gRPC ExternalProject,The detection for the {{grpc++}} header location is not working for the ExternalProject as the headers are not installed at configure time.,pull-request-available,['C++'],ARROW,Bug,Major,2019-03-17 10:59:03,8
13222131,[Python] Remove LIBDIR assumptions in Python build,"This is in reference to (4) in [this|http://mail-archives.apache.org/mod_mbox/arrow-dev/201903.mbox/%3C0AF328A1-ED2A-457F-B72D-3B49C8614850%40xhochy.com%3E] mailing list discussion.

Certain sections of setup.py assume a specific location of the C++ libraries. Removing this hard assumption will simplify PyArrow builds significantly. As far as I could tell these assumptions are made in the {{build_ext._run_cmake()}} method (wherever bundling of C++ libraries are handled).
 # The first occurrence is before invoking cmake (see line 237).
 # The second occurrence is when the C++ libraries are moved from their build directory to the Python tree (see line 347). The actual implementation is in the function {{_move_shared_libs_unix(..)}} (see line 468).

Hope this helps.",pull-request-available setup.py,['Python'],ARROW,Improvement,Minor,2019-03-17 10:32:45,1
13222109,[Python] Hypothesis test failures,"I don't think these are being run regularly anywhere (?)

{code}
==================================== FAILURES =====================================
__________________________________ test_pickling __________________________________

    @h.given(
>       past.arrays(
            past.all_types,
            size=st.integers(min_value=0, max_value=10)
        )
    )
    def test_pickling(arr):

pyarrow/tests/test_array.py:822: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:145: in arrays
    type = draw(type)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(150345453957525051493869399118570182513) to this test or run pytest with --hypothesis-seed=150345453957525051493869399118570182513 to reproduce this failure.
__________________________________ test_schemas ___________________________________

    @h.given(past.all_schemas)
>   def test_schemas(schema):

pyarrow/tests/test_strategies.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(275336988088025852485731858331440131646) to this test or run pytest with --hypothesis-seed=275336988088025852485731858331440131646 to reproduce this failure.
___________________________________ test_arrays ___________________________________

    @h.given(past.all_arrays)
>   def test_arrays(array):

pyarrow/tests/test_strategies.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:145: in arrays
    type = draw(type)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(112717218077417468999288153449881179589) to this test or run pytest with --hypothesis-seed=112717218077417468999288153449881179589 to reproduce this failure.
_______________________________ test_chunked_arrays _______________________________

    @h.given(past.all_chunked_arrays)
>   def test_chunked_arrays(chunked_array):

pyarrow/tests/test_strategies.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:206: in chunked_arrays
    type = draw(type)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(249910746870640720519016487591535359287) to this test or run pytest with --hypothesis-seed=249910746870640720519016487591535359287 to reproduce this failure.
_______________________________ test_record_batches _______________________________

    @h.given(past.all_record_batches)
>   def test_record_batches(record_bath):

pyarrow/tests/test_strategies.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:233: in record_batches
    schema = draw(schemas(type, max_fields=max_fields))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(224188657977816444687824754926551268633) to this test or run pytest with --hypothesis-seed=224188657977816444687824754926551268633 to reproduce this failure.
___________________________________ test_tables ___________________________________

    @h.given(past.all_tables)
>   def test_tables(table):

pyarrow/tests/test_strategies.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/tests/strategies.py:249: in tables
    schema = draw(schemas(type, max_fields=max_fields))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:115: in do_draw
    result.append(data.draw(self.element_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in do_draw
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/collections.py:55: in <genexpr>
    return tuple(data.draw(e) for e in self.element_strategies)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:508: in do_draw
    return data.draw(self.element_strategies[i], label=self.branch_labels[i])
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/lazy.py:156: in do_draw
    return data.draw(self.wrapped_strategy)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:233: in draw
    return self.__draw(strategy, label=label)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/internal/conjecture/data.py:242: in __draw
    return strategy.do_draw(self)
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/searchstrategy/strategies.py:570: in do_draw
    result = self.pack(data.draw(self.mapped_strategy))
../../../miniconda/envs/arrow-3.7/lib/python3.7/site-packages/hypothesis/_strategies.py:1200: in <lambda>
    lambda value: target(*value[0], **value[1])
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(11590233605635440545759465827826616216) to this test or run pytest with --hypothesis-seed=11590233605635440545759465827826616216 to reproduce this failure.
__________________________________ test_pickling __________________________________

    @h.given(
>       past.all_types |
        past.all_fields |
        past.all_schemas
    )
    @h.example(
        pa.field(name='', type=pa.null(), metadata={'0': '', '': ''})
    )
    def test_pickling(field):

pyarrow/tests/test_types.py:546: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(29668570043146860011453262620485865691) to this test or run pytest with --hypothesis-seed=29668570043146860011453262620485865691 to reproduce this failure.
__________________________________ test_hashing ___________________________________

    @h.given(
>       st.lists(past.all_types) |
        st.lists(past.all_fields) |
        st.lists(past.all_schemas)
    )
    def test_hashing(items):

pyarrow/tests/test_types.py:559: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/types.pxi:1347: in pyarrow.lib.decimal128
    cpdef DataType decimal128(int precision, int scale=0):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ValueError(""precision should be between 1 and 38"")
E   ValueError: precision should be between 1 and 38

pyarrow/types.pxi:1362: ValueError
----------------------------------- Hypothesis ------------------------------------
You can add @seed(154619181152052597443025012079016131068) to this test or run pytest with --hypothesis-seed=154619181152052597443025012079016131068 to reproduce this failure.
{code}",pull-request-available,['Python'],ARROW,Bug,Blocker,2019-03-16 22:49:58,2
13222096,[Rust] Update top level README to describe current functionality,"Update top level Rust README to reflect new functionality, such as SIMD, cast, date/time, DataFusion, etc",pull-request-available,['Rust'],ARROW,Improvement,Minor,2019-03-16 15:54:15,9
13222095,[Rust] [DataFusion] Update README for 0.13.0 release,"Update the README to reflect the new features in 0.13.0, such as parquet support.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-16 15:53:21,10
13222039,"[C++, Python] Allow specifying column names to CSV reader","Currently I think there is no way to specify custom column names for CSV files. It's possible to specify the full schema of the file, but not just column names.

See the related discussion here:ARROW-3722

The goal of this is to re-use the CSV type-inference but still allow people to specify custom names for the columns. As far as I know, there is currently no way to set column names post-hoc, so we should provide a way to specify them before reading the file.

Related to this, ParseOptions(header_rows=0) is not currently implemented.

Is there any current way to do this or does this need to be implmented?",csv pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2019-03-16 00:50:53,6
13222012,[Rust] [DataFusion] Add support for parquet date/time in int32/64 encoding,The parquet data source does not support reading date/time types that are encoded in INT32/INT64 columns.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-15 22:10:08,10
13222011,[CI] Add docker container to inspect docker context,The larger the docker context the slower the docker build. Adding a small container to inspect the context helps to curate a good {{.dockerignore}}.,pull-request-available,['Continuous Integration'],ARROW,Task,Major,2019-03-15 22:07:27,8
13221980,[C++] Building tests using only static libs not possible,We explicitly depend on shared libs in some tests.,pull-request-available,['C++'],ARROW,Bug,Major,2019-03-15 18:21:41,8
13221962,[C++/Flight] Compilation fails due to unreachable code,"clang on OSX is quite picky, we can simply remove this code.",pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2019-03-15 17:17:51,8
13221902,[C++] Old versions of FindProtobuf.cmake use ALL-CAPS for variables,We only need to handle {{PROTOBUF_PROTOC_LIBRARY}} vs {{Protobuf_PROTOC_LIBRARY}}.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-15 12:28:32,8
13221899,[Rust] [DataFusion] Improve Rustdoc,I plan on spending an hour or two improving the documentation across this crate before we release 0.13.0,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-15 12:18:22,10
13221898,[Rust] [DataFusion] Remove all uses of panic! from tests,Tests should use assert!(false) rather than panic!(),pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-15 12:17:15,10
13221894,[Rust] [DataFusion] Remove all uses of panic! from aggregate.rs,Code cleanup,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-15 12:14:16,10
13221890,[C++] ZLIB include directories not added,This causes a failing centos-7 build.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-15 12:02:57,8
13221867,[C++] Add STATUS messages for Protobuf in CMake,With Protobuf it can easily happen that {{protoc}} and {{libprotobuf}} mismatch. We should have some output about this in CMake to better debug this when users report issues.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-15 10:41:38,8
13221805,[Rust] Inconsistent behaviour with casting sliced primitive array to list array,"[~csun] I was going through the C++ cast implementation to see if I've missed anything, and I noticed that ListCastKernel ([https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/kernels/cast.cc#L665]) doesn't support casting non-zero-offset arrays. So I investigated what happens in Rust ARROW-4865. I found an inconsistency where inheriting the incoming array's offset could lead us to read invalid data.

I tried fixing it, but found that a buffer that I expected to be invalid was being returned as valid, but returning invalid data.

I've currently disabled casting primitive to array where the offset is not zero, and I'd like to wait for ARROW-4853so I can see how sliced lists behave, and fix this inconsistency. That might only happen in 0.14, so I'm fine with that.",pull-request-available,['Rust'],ARROW,Bug,Major,2019-03-15 03:37:13,12
13221792,[Python] read_csv() can't handle decimal128 columns,"h1. Summary

{\{read_csv()}}crashes when givena{{decimal128}}column type in its convert options. The cause is that there's no converter listed [here|https://github.com/apache/arrow/blob/master/cpp/src/arrow/csv/converter.cc#L301-L315].
h1. To Reproduce

1) First, create a CSV file like so and save it somewhere:
{code:java}
Header
123.45
{code}
2) Run the following code on Python 2 or 3:
{code:java}
import pyarrow.csv as pa_csv
import pyarrow as pa
import io

types = {'Header': pa.decimal128(11, 2)}
convert_options = pa_csv.ConvertOptions(column_types=types)
pa_csv.read_csv('/home/dargueta/Desktop/test.csv', convert_options=convert_options)
{code}
read_csv() crashes with the following exception:
{code:java}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/_csv.pyx"", line 397, in pyarrow._csv.read_csv
  File ""pyarrow/error.pxi"", line 89, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: CSV conversion to decimal(11, 2) is not supported

CSV conversion to decimal(11, 2) is not supported
{code}",csv pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-03-15 01:32:24,15
13221776,[Python] bundle_zlib CMake function still uses ARROW_BUILD_TOOLCHAIN,Not sure if our wheels works but: https://github.com/apache/arrow/blob/master/python/CMakeLists.txt#L278,pull-request-available,['Python'],ARROW,Bug,Major,2019-03-14 23:09:07,6
13221775,[Python] python/asv-build.sh is probably broken after CMake refactor,uses {{$ARROW_BUILD_TOOLCHAIN}},pull-request-available,['Python'],ARROW,Bug,Major,2019-03-14 23:07:56,14
13221771,[C++] ARROW_DEPENDENCY_SOURCE=CONDA does not work properly with MSVC,The prefix must have {{\Library}} added to it,pull-request-available,['C++'],ARROW,Improvement,Blocker,2019-03-14 23:00:50,14
13221755,[Plasma] CI failure in test_plasma_list,"https://api.travis-ci.org/v3/job/506259901/log.txt

{noformat}
=================================== FAILURES ===================================
_______________________________ test_plasma_list _______________________________

    @pytest.mark.plasma
    def test_plasma_list():
        import pyarrow.plasma as plasma
    
        with plasma.start_plasma_store(
                plasma_store_memory=DEFAULT_PLASMA_STORE_MEMORY) \
                as (plasma_store_name, p):
            plasma_client = plasma.connect(plasma_store_name)
    
            # Test sizes
            u, _, _ = create_object(plasma_client, 11, metadata_size=7, seal=False)
            l1 = plasma_client.list()
            assert l1[u][""data_size""] == 11
            assert l1[u][""metadata_size""] == 7
    
            # Test ref_count
            v = plasma_client.put(np.zeros(3))
            l2 = plasma_client.list()
            # Ref count has already been released
            assert l2[v][""ref_count""] == 0
            a = plasma_client.get(v)
            l3 = plasma_client.list()
            assert l3[v][""ref_count""] == 1
            del a
    
            # Test state
            w, _, _ = create_object(plasma_client, 3, metadata_size=0, seal=False)
            l4 = plasma_client.list()
            assert l4[w][""state""] == ""created""
            plasma_client.seal(w)
            l5 = plasma_client.list()
            assert l5[w][""state""] == ""sealed""
    
            # Test timestamps
            t1 = time.time()
            x, _, _ = create_object(plasma_client, 3, metadata_size=0, seal=False)
            t2 = time.time()
            l6 = plasma_client.list()
>           assert math.floor(t1) <= l6[x][""create_time""] <= math.ceil(t2)
E           assert 1552568478 <= 1552568477
E            +  where 1552568478 = <built-in function floor>(1552568478.0022461)
E            +    where <built-in function floor> = math.floor

../../pyarrow-test-3.6/lib/python3.6/site-packages/pyarrow/tests/test_plasma.py:1070: AssertionError
----------------------------- Captured stderr call -----------------------------
I0314 13:01:17.901209 19953 store.cc:1093] Allowing the Plasma store to use up to 0.1GB of memory.
I0314 13:01:17.901417 19953 store.cc:1120] Starting object store with directory /dev/shm and huge page support disabled
{noformat}",ci-failure,"['C++ - Plasma', 'Continuous Integration', 'Python']",ARROW,Bug,Major,2019-03-14 20:41:59,2
13221698,[C++] Clarify documentation about how to use external ARROW_PACKAGE_PREFIX while also using CONDA dependency resolution,"I will improve the documentation about this ""hybrid"" use case, where I need to pass

{code}
-DARROW_DEPENDENCY_SOURCE=SYSTEM -DARROW_PACKAGE_PREFIX=$TOOLCHAIN
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-14 15:30:29,14
13221691,[Python] Keep backward compatibility for ParquetDatasetPiece,"See https://github.com/apache/arrow/commit/f2fb02b82b60ba9a90c8bad6e5b11e37fc3ea9d3#r32722497

and 

https://github.com/dask/dask/pull/4587",parquet pull-request-available,['Python'],ARROW,Bug,Blocker,2019-03-14 14:56:35,3
13221673,[Flight][Java] Handle large Flight messages,"Similarly to ARROW-4421, Java/gRPC needs to be configured to allow large messages. The integration tests should also be updated to cover this.",flight pull-request-available,"['FlightRPC', 'Java']",ARROW,Bug,Major,2019-03-14 13:45:18,0
13221669,[C++] Use of gmock fails in compute/kernels/util-internal-test.cc ,"Out of the box build with 

{code}
cmake .. -DARROW_BUILD_TESTS=ON -DARROW_BUILD_BENCHMARKS=ON -DARROW_PARQUET=ON -DARROW_GANDIVA=ON -DARROW_FLIGHT=ON -DARROW_BOOST_VENDORED=ON
{code}

{code}
[ 57%] Building CXX object src/arrow/ipc/CMakeFiles/arrow-ipc-json-simple-test.dir/json-simple-test.cc.o
/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc:152:3: error: reference to non-static member function must be called; did you mean to call it with no arguments?
  ((mock).gmock_out_type).InternalExpectedAt(""/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc"", 152, ""mock"", ""out_type"").WillRepeatedly(Return(boolean()));
  ^~~~~~~~~~~~~~~~~~~~~~~
                         ()
/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc:173:3: error: reference to non-static member function must be called; did you mean to call it with no arguments?
  ((mock).gmock_out_type).InternalExpectedAt(""/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc"", 173, ""mock"", ""out_type"").WillRepeatedly(Return(int32()));
  ^~~~~~~~~~~~~~~~~~~~~~~
                         ()
/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc:192:3: error: reference to non-static member function must be called; did you mean to call it with no arguments?
  ((mock).gmock_out_type).InternalExpectedAt(""/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc"", 192, ""mock"", ""out_type"").WillRepeatedly(Return(boolean()));
  ^~~~~~~~~~~~~~~~~~~~~~~
                         ()
/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc:213:3: error: reference to non-static member function must be called; did you mean to call it with no arguments?
  ((mock).gmock_out_type).InternalExpectedAt(""/home/wesm/code/arrow/cpp/src/arrow/compute/kernels/util-internal-test.cc"", 213, ""mock"", ""out_type"").WillRepeatedly(Return(int32()));
  ^~~~~~~~~~~~~~~~~~~~~~~
                         ()
4 errors generated.
make[2]: *** [src/arrow/compute/kernels/CMakeFiles/arrow-compute-util-internal-test.dir/util-internal-test.cc.o] Error 1
make[1]: *** [src/arrow/compute/kernels/CMakeFiles/arrow-compute-util-internal-test.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-14 13:36:24,6
13221658,[Python] Table.from_pandas() column order not respected,"Starting with 0.12, the order of the columns in {{Table.from_pandas}}is no longer identical to the order of the colums given in the {{columns}} argument. This used to be the case in previous versions.

0.12:
{code:java}
In [1]: pyarrow.Table.from_pandas(pd.DataFrame(dict(a=[1],b=[2])), columns=['b', 'a'])
Out[1]:
pyarrow.Table
a: int64
b: int64{code}
0.11:
{code:java}
In [1]: pyarrow.Table.from_pandas(pd.DataFrame(dict(a=[1],b=[2])), columns=['b', 'a'])
Out[1]:
pyarrow.Table
b: int64
a: int64{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2019-03-14 12:37:33,14
13221642,[C++] zstd ExternalProject failing on Windows,"After [https://github.com/apache/arrow/pull/3885|https://github.com/apache/arrow/pull/3885,]the zstd ExternalProject is failing in the Windows builds, see [https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/23063072/job/bd0gom16atlkddtx]



",ci-failure pull-request-available,"['C++', 'Packaging']",ARROW,Bug,Major,2019-03-14 10:35:59,14
13221601,[Rust] Support casting lists and primitives to lists,This adds support for casting between list arraysand from primitive arrays to single-valuelist arrays,pull-request-available,['Rust'],ARROW,Improvement,Minor,2019-03-14 06:50:32,12
13221429,[Flight][Python] Enable custom FlightDataStream in Python,"We should be able to provide a custom data stream as the result of Flight do_get in Python. In particular, when returning data produced on the fly, or when returning a large Pandas DataFrame, it'd be nice to provide data in chunks as it becomes available, rather than having to copy everything into a Table first.

On the Python side, a FlightDataStream wrapper that accepts RecordBatches from a Python generator should suffice.",flight pull-request-available,"['FlightRPC', 'Python']",ARROW,Improvement,Minor,2019-03-13 14:52:43,0
13221418,[Packaging] Generate default package version based on cpp tags in crossbow.py,It picked up wrong version because of the recent JS release.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-03-13 14:21:00,3
13221411,[Rust] Use Array Slice for limit kernel,"We currently reconstruct an array when taking a limit from it, we can improve performance by using slice from ARROW-3954",pull-request-available,['Rust'],ARROW,Improvement,Minor,2019-03-13 13:45:34,12
13221261,[CI] Integration test failures do not fail the Travis CI build,"See https://github.com/apache/arrow/pull/3871

These changes fail the build, but it is reported as success

The errors can be seen in https://travis-ci.org/apache/arrow/jobs/505028161",pull-request-available,['Continuous Integration'],ARROW,Bug,Blocker,2019-03-13 01:41:19,13
13221231,[C++] Static libparquet not compiled with -DARROW_STATIC on Windows,"When trying to link the R bindings against static libparquet.a + libarrow.a we get a lot of missing arrow symbol warnings from libparquet.a. I think the problem is that libparquet.a was not compiled -DARROW_STATIC, and therefore cannot be linked against libarrow.a.

When arrow cmake is configured with  -DARROW_BUILD_SHARED=OFF I think it should automatically use -DARROW_STATIC when compiling libparquet on Windows?
",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-12 21:27:19,4
13221229,[Python] Add pyarrow.table factory function that dispatches to various ctors based on type of input,"For example, in {{pyarrow.table(df)}} if {{df}} is a {{pandas.DataFrame}}, then table will dispatch to {{pa.Table.from_pandas}}",pull-request-available,['Python'],ARROW,Improvement,Major,2019-03-12 21:23:18,5
13221226,[Java] Update Jackson to 2.9.8,We are looking at removing Jackson from arrow-vector dependencies in ARROW-2501,pull-request-available,['Java'],ARROW,Improvement,Major,2019-03-12 21:14:06,10
13221172,[C++] Persist CMake options in generated CMake config,"(do this after we merged the CMake refactor)



We should persist all options set during the CMake run also in {{arrowConfig.cmake}} so that CMake projects that depend on Arrow also can determine what they are able to provide.",pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2019-03-12 16:31:00,6
13221113,[C++] Support c++filt on a custom path in the run-test.sh script,On conda this is CXXFILT=/opt/conda/bin/x86_64-conda_cos6-linux-gnu-c++filt,pull-request-available,"['C++', 'Developer Tools']",ARROW,Improvement,Major,2019-03-12 12:54:13,3
13221095,"[Python] ""Cannot tell() a compressed stream"" when using RecordBatchStreamWriter","It does not seem like RecordBatchStreamWriter works with compressed streams:

{code:python}
>>> import pyarrow as pa
>>> pa.__version__
'0.12.1'
>>> stream = pa.output_stream('/tmp/a.gz')
>>> batch = pa.RecordBatch.from_arrays([pa.array([1])], ['a'])
>>> writer = pa.RecordBatchStreamWriter(stream, batch.schema)
>>> writer.write(batch)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/ipc.pxi"", line 181, in pyarrow.lib._RecordBatchWriter.write
  File ""pyarrow/ipc.pxi"", line 196, in pyarrow.lib._RecordBatchWriter.write_batch
  File ""pyarrow/error.pxi"", line 89, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: Cannot tell() a compressed stream
{code}

As I understand the documentation, this should be possible, right?",pull-request-available,['Python'],ARROW,Bug,Major,2019-03-12 11:36:13,2
13220898,[C++] CMAKE_AR is not passed to ZSTD thirdparty dependency ,ZSTD_CMAKE_ARGS should utilize https://github.com/apache/arrow/blob/master/cpp/cmake_modules/ThirdpartyToolchain.cmake#L359,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-11 17:46:35,3
13220879,[Python] manylinux1 docker-compose context should be python/manylinux1,Currently it doesn't find the {{scripts}} folder on running {{docker-compose build python-manylinux1}}.,pull-request-available,['Python'],ARROW,Bug,Major,2019-03-11 16:27:02,8
13220872,[C++] Implement benchmark comparison between two git revisions,"It would be a good thing to be able to write

{code}
benchmark-compare-revisions.sh $HASH1 $HASH2
{code}

and to be output a table of benchmark before/after/change for each C++ benchmark, to assess the performance impact of patches",pull-request-available,['C++'],ARROW,New Feature,Major,2019-03-11 15:15:31,13
13220820,[Python] read_csv should accept io.StringIO objects,"It would be nice/convenient if `read_csv` also supported `io.StringIO` objects:



{{In [59]: csv = io.StringIO('''issue_date_utc,variable_name,station_name,station_id,value_date_utc,value}}
{{ ...: 2019-02-26 22:00:00,TEMPERATURE,ARCHERFIELD,040211,2019-02-27 03:00,29.1}}
{{ ...: ''')}}

{{In [60]: pd.read_csv(csv)}}
{{Out[60]: }}
{{ issue_date_utc variable_name ... value_date_utc value}}
{{0 2019-02-26 22:00:00 TEMPERATURE ... 2019-02-27 03:00 29.1}}

{{[1 rows x 6 columns]}}

{{In [61]: pa.csv.read_csv(csv)}}
{{Traceback (most recent call last):}}

{{ File ""<ipython-input-61-d86447415aa3>"", line 1, in <module>}}
{{ pa.csv.read_csv(csv)}}

{{SystemError: <built-in function read_csv> returned NULL without setting an error}}

",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-03-11 12:07:06,2
13220818,[Python] read_csv shouldn't close file handles it doesn't own,"If a file-handle is passed into `read_csv` it is automatically closed:



{{In [47]: csv = io.BytesIO(b'''issue_date_utc,variable_name,station_name,station_id,value_date_utc,value}}
{{ ...: 2019-02-26 22:00:00,TEMPERATURE,ARCHERFIELD,040211,2019-02-27 03:00,29.1}}
{{ ...: ''')}}

{{In [48]: pa.csv.read_csv(csv, convert_options=opts)}}
{{Out[48]: }}
{{pyarrow.Table}}
{{issue_date_utc: timestamp[ns]}}
{{variable_name: string}}
{{station_name: string}}
{{station_id: int64}}
{{value_date_utc: string}}
{{value: double}}

{{In [49]: csv.seek(0)}}
{{Traceback (most recent call last):}}

{{ File ""<ipython-input-50-0644e6e50712>"", line 1, in <module>}}
{{ csv.seek(0)}}

{{ValueError: I/O operation on closed file.}}



This behaviour is in contrast to pandas which leaves the file handle open.

Since the function didn't create the file handle I don't think it should close it.",csv pull-request-available,['Python'],ARROW,Bug,Minor,2019-03-11 11:55:32,14
13220798,[C++/Python] pyarrow.Table.equals segmentation fault on None,Calling {{pyarrow.Table.equals}}with {{None}}causes a segmentation fault; this should be caught.,pull-request-available,"['C++', 'Python']",ARROW,Bug,Trivial,2019-03-11 09:50:54,8
13220684,[Rust] [DataFusion] Add support for * in SQL projection,Currently column names must always be provided and there is no support for SELECT * FROM table,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-10 14:31:55,10
13220564,"[C++] An incorrect dependency leads ""ninja"" to re-evaluate steps unnecessarily on subsequent calls","Not sure about the root cause yet but here are the 5 steps that are re-executing

{code}
$ ninja -v
[1/5] /usr/bin/ccache /usr/bin/g++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist//include -DARROW_NO_DEPRECATED_API -DARROW_PYTHON_EXPORTING -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -Isrc -I../src -isystem /home/wesm/cpp-toolchain/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem jemalloc_ep-prefix/src -isystem ../thirdparty/hadoop/include -isystem orc_ep-install/include -isystem /home/wesm/cpp-toolchain/include/thrift -isystem /home/wesm/miniconda/envs/arrow-3.7/lib/python3.7/site-packages/numpy/core/include -isystem /home/wesm/miniconda/envs/arrow-3.7/include/python3.7m -Wno-noexcept-type  -fdiagnostics-color=always -O3 -DNDEBUG  -Wall -Wno-unused-variable -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG -fPIC   -std=gnu++11 -MD -MT src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o -MF src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o.d -o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o -c ../src/arrow/python/flight.cc
[2/5] : && /usr/bin/ccache /home/wesm/miniconda/envs/arrow-3.7/bin/cmake -E remove release/libarrow_python.a && /usr/bin/ccache /usr/bin/ar qc release/libarrow_python.a  src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/benchmark.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/common.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/config.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/decimal.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/deserialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/helpers.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/inference.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/init.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/io.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_convert.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/python_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/pyarrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/serialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o && /usr/bin/ccache /usr/bin/ranlib release/libarrow_python.a && :
[3/5] : && /usr/bin/ccache /usr/bin/g++ -fPIC -Wno-noexcept-type  -fdiagnostics-color=always -O3 -DNDEBUG  -Wall -Wno-unused-variable -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG   -shared -Wl,-soname,libarrow_python.so.13 -o release/libarrow_python.so.13.0.0 src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/benchmark.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/common.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/config.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/decimal.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/deserialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/helpers.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/inference.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/init.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/io.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_convert.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/numpy_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/python_to_arrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/pyarrow.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/serialize.cc.o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/flight.cc.o  -Wl,-rpath,/home/wesm/code/arrow/cpp/build/release:/home/wesm/cpp-toolchain/lib: -lpthread -ldl -lutil -lrt release/libarrow_flight.so.13.0.0 release/libarrow.so.13.0.0 -ldl jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt /home/wesm/cpp-toolchain/lib/libprotobuf.a /home/wesm/cpp-toolchain/lib/libgrpc++_unsecure.a /home/wesm/cpp-toolchain/lib/libgrpc_unsecure.a /home/wesm/cpp-toolchain/lib/libgpr.a /home/wesm/cpp-toolchain/lib/libaddress_sorting.a /home/wesm/cpp-toolchain/lib/libcares_static.a -Wl,-rpath-link,/home/wesm/cpp-toolchain/lib && :
[4/5] /home/wesm/miniconda/envs/arrow-3.7/bin/cmake -E cmake_symlink_library release/libarrow_python.so.13.0.0  release/libarrow_python.so.13 release/libarrow_python.so && :
[5/5] : && /usr/bin/ccache /usr/bin/g++  -Wno-noexcept-type  -fdiagnostics-color=always -O3 -DNDEBUG  -Wall -Wno-unused-variable -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG  -rdynamic src/arrow/python/CMakeFiles/arrow-python-test.dir/python-test.cc.o  -o release/arrow-python-test  -Wl,-rpath,/home/wesm/code/arrow/cpp/build/release:/home/wesm/miniconda/envs/arrow-3.7/lib:/home/wesm/cpp-toolchain/lib release/libarrow_python_test_main.a release/libarrow_python.so.13.0.0 release/libarrow_testing.so.13.0.0 /home/wesm/miniconda/envs/arrow-3.7/lib/libpython3.7m.so -lpthread -lpthread -ldl -lutil -lrt release/libarrow_flight.so.13.0.0 /home/wesm/cpp-toolchain/lib/libprotobuf.a /home/wesm/cpp-toolchain/lib/libgrpc++_unsecure.a /home/wesm/cpp-toolchain/lib/libgrpc_unsecure.a /home/wesm/cpp-toolchain/lib/libgpr.a /home/wesm/cpp-toolchain/lib/libaddress_sorting.a /home/wesm/cpp-toolchain/lib/libcares_static.a release/libarrow.so.13.0.0 -ldl jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread -lrt /home/wesm/cpp-toolchain/lib/libgtest.a -Wl,-rpath-link,/home/wesm/cpp-toolchain/lib && :
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-03-08 23:16:29,14
13220482,"[Format][C++] Add ""LargeList"" type with 64-bit offsets",Mentioned in https://github.com/apache/arrow/issues/3845,pull-request-available,"['C++', 'Format']",ARROW,Improvement,Minor,2019-03-08 15:17:21,2
13220391,[Rust] Support casting temporal arrays in cast kernels,[ARROW-3882] is too far in the review process to add temporal casts.,pull-request-available,['Rust'],ARROW,New Feature,Major,2019-03-08 05:50:52,12
13220390,[Rust] Write temporal arrays to CSV,"The CSV writer should start supporting writing temporal arrays back to disk.

To be consistent with norms, we should look at what other libraries do for date and time where the resolution is greater than seconds, and potentially deal with the below:
 * Is there optionality to how dates are written, or should it always be DD/MM/YYYY.
 * Should / or - be used?
 * Should time types be written as HH:MM:SS.ms, or 12345ms, 12345us, 12345ns?
 * Should timestamps always be written in the ISO8601 JSONlike format?",pull-request-available,['Rust'],ARROW,New Feature,Major,2019-03-08 05:48:48,12
13220373,[C++] Create/port a StatusOr implementation to be able to return a status or a type,Example from grpc: https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/stubs/statusor.h,pull-request-available,['C++'],ARROW,Improvement,Minor,2019-03-08 03:57:19,15
13220230,[Flight][Python] segfault in simple server implementation,"Python segfaults if you implement a Flight server that returns a data stream but does not keep a reference to the underlying data source (the Table, RecordBatch, etc). The Flight bindings themselves do not keep a reference to the object, so the server will segfault as the memory has been reclaimed.",flight pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2019-03-07 14:43:08,0
13220046,[Python/Packaging] Update manylinux docker image in crossbow task,to {{ARROW - 4778}} see https://github.com/apache/arrow/pull/3823#issuecomment-470129575,pull-request-available,['Python'],ARROW,Task,Major,2019-03-06 22:31:13,3
13220044,[C++] Deprecate and and later remove arrow::io::ReadableFileInterface,See arrow/io/interfaces.h. This is a legacy alias,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-06 22:22:11,3
13220026,[C++] Develop less verbose API for constructing StructArray,See comment at https://github.com/apache/arrow/pull/3579/files#diff-7a1bd8476ae3e687fa8d961059596f06R526,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-06 20:39:25,2
13220024,"[C++] Include ""null"" values (perhaps with an option to toggle on/off) in hash kernel actions",Null is a meaningful value in the context of analytics. We should have the option of considering it distinctly in e.g. {{ValueCounts}} ,pull-request-available,['C++'],ARROW,Improvement,Major,2019-03-06 20:32:23,13
13219911,[C++/Python]Support better parallelisation in manylinux1 base build,Currently we're building some dependencies single-threaded but could build them with much higher parallelisation.,pull-request-available,"['C++', 'Packaging', 'Python']",ARROW,Task,Major,2019-03-06 13:28:29,8
13219803,[C++][CI] Re-enable flaky mingw tests.,There is no {{--exclude-regex}} option for {{ctest}} in {{ci/appveyor-cpp-build-mingw.bat}} when we resolve this.,ci-failure,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-03-06 04:24:44,1
13219802,[C++][CI] Mingw32 builds sometimes timeout,"From [https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22853367/job/e461la5roy7edpsb]



It looks like the thread-pool test. I'm going to disable it for the time being.",ci-failure pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-03-06 04:16:33,15
13219791,[C++] Prototype scalar and array expression types for developing deferred operator algebra,"I am beginning to develop a C++ API for describing analytical expressions on scalars, arrays, and tables in the general style of Ibis (https://docs.ibis-project.org/sql.html) which has been used to successfully model SQL relational algebra as well as a broader variety of analytical operations. This is a large project so I'm starting small",pull-request-available,['C++'],ARROW,New Feature,Major,2019-03-06 02:48:26,14
13219576,[C++] DictionaryBuilder should support bootstrapping from an existing dict type,This would mean adding a new DictionaryBuilder constructor that receives a dictionary type and performs a lazy deep copy if there's any modification. We'll have to investigate how this translate in API ergonomics.,pull-request-available,['C++'],ARROW,Improvement,Minor,2019-03-05 14:42:07,6
13219525,[C++][Parquet] Call Table::Validate when writing a table,"When writing a table to a parquet file that contains both flat arrays of different leng

Reproducer:

{code:python}

import pyarrow as pa
import pyarrow.parquet as pq
import numpy as np

array1 = np.array([0, 1, 2], dtype=np.uint8)
array2 = np.array([[0,1,2], [3, 4, 5]], dtype=np.uint8).T

t1 = pa.uint8()
t2 = pa.list_(pa.uint8())

fields = [
    pa.field('a1', t1),
    pa.field('a2', t2)
]

myschema = pa.schema(fields)

mytable = pa.Table.from_arrays([
    pa.array(array1, type=t1),
    pa.array([array2[:,0], array2[:,1]], type=t2)],
    schema=myschema)

pq.write_table(mytable, 'example.parquet')
{code}

Windows 10 (Python 3.6.4 64-bit, pyarrow 0.11.1) crash code:

{code:bash}
Process finished with exit code -1073741819 (0xC0000005)
{code}


WSL (Python 3.6.5 64-bit, pyarrow 0.12.1) Crash code:

{code:bash}
Segmentation fault (core dumped)
{code}


",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-03-05 11:19:00,13
13219465,[Rust] Improve array limit function where max records > len,"When we have an array of n records, and we want to take a limit that's higher or equat to n, we still iterate through the array values and create a new array.

We could improve this by returning a copy of the array as-is.",pull-request-available,['Rust'],ARROW,Improvement,Trivial,2019-03-05 05:43:41,12
13219449,[C++][CI] arrow-test-array sometimes gets stuck in MinGW build,"Example: https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22804493/job/6mbpslm97p4yj31c#L726

{noformat}
      Start  2: arrow-array-test
{noformat}

isn't finished.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Test,Minor,2019-03-05 02:51:49,1
13219379,[C++] Casting empty boolean array causes segfault,"Reproducer:

{code:python}
import pyarrow as pa

test = pa.array([], type=pa.bool_())
test2 = test.cast(pa.int8())
{code}
",pull-request-available,['C++'],ARROW,Bug,Blocker,2019-03-04 20:03:24,13
13219208,[CI][Java] Flaky TestAuth Flight test,"org.apache.arrow.flight.auth.TestAuth
[ERROR] invalidAuth(org.apache.arrow.flight.auth.TestAuth) Time elapsed: 0.013 s <<< ERROR!
java.io.IOException: Failed to bind
 at org.apache.arrow.flight.auth.TestAuth.setup(TestAuth.java:108)
Caused by: java.net.BindException: Address already in use",pull-request-available,"['Continuous Integration', 'FlightRPC', 'Java']",ARROW,Improvement,Blocker,2019-03-04 05:34:45,15
13219172,[C++]Add pkg-config to conda_env_cpp.yml,"Once the CMake refactor has been merged, we should add {{pkg-config}} to the dependencies as it should be also available for Windows now: https://github.com/conda-forge/pkg-config-feedstock/pull/27 This will simplify some packaging.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Task,Major,2019-03-03 19:00:43,14
13219169,[Rust] RecordBatch::new() should return result instead of panicking,"RecordBatch::new() has some good validation checks, but calls assert_eq instead of returning a Result",pull-request-available,['Rust'],ARROW,Improvement,Major,2019-03-03 18:12:08,12
13219162,[Rust] [DataFusion] GROUP BY performance could be optimized,"The logic to build the group by keys is row-based, performing an array downcast on every single group by value. This could be done in a columnar way instead.



I also wonder if it is possible to avoid converting the result map to an array of map entries.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-03 15:24:15,10
13219154,[C++/Python]PyDataTime_Date wrongly casted to PyDataTime_DateTime,"As mentioned in https://bitbucket.org/pypy/pypy/issues/2842/running-pyarrow-on-pypy-segfaults#comment-50670536, we currently access a {{PyDataTime_Date}} object with a {{PyDataTime_DateTime}} cast in {{PyDateTime_DATE_GET_SECOND}} in our code in two instances. While CPython is able to deal with this wrong usage, PyPy is not able to do so. We should separate the path here into one that deals with dates and another that deals with datetimes.

Reproducible code:
{code:java}
pa.array([datetime.date(2018, 5, 10)], type=pa.date64()){code}",pypy,"['C++', 'Python']",ARROW,Bug,Major,2019-03-03 12:03:05,15
13219118,[C++][Documentation] Document process for replicating static_crt builds on windows,Based on collective wisdom of the mailing list. Give some step by step instructions to getting things to build.,pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Minor,2019-03-02 22:54:37,15
13219117,[CI][C++] Mingw32 builds failing,"[https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf]


C:/projects/arrow/cpp/src/arrow/util/compression_snappy.cc: In member function 'virtual int64_t arrow::util::SnappyCodec::MaxCompressedLen(int64_t, const uint8_t*)':
[385|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L385]C:/projects/arrow/cpp/src/arrow/util/compression_snappy.cc:76:47: error: conversion to 'size_t \{aka unsigned int}' from 'int64_t \{aka long long int}' may alter its value [-Werror=conversion]
[386|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L386] return snappy::MaxCompressedLength(input_len);
[387|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L387] ^
[388|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L388]cc1plus.exe: all warnings being treated as errors
[389|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L389]make[2]: *** [src/arrow/CMakeFiles/arrow_objlib.dir/build.make:89: src/arrow/CMakeFiles/arrow_objlib.dir/util/compression_snappy.cc.obj] Error 1
[390|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L390]make[2]: *** Waiting for unfinished jobs....
[391|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L391]C:/projects/arrow/cpp/src/arrow/util/compression_zstd.cc: In member function 'virtual int64_t arrow::util::ZSTDCodec::MaxCompressedLen(int64_t, const uint8_t*)':
[392|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L392]C:/projects/arrow/cpp/src/arrow/util/compression_zstd.cc:232:38: error: conversion to 'size_t \{aka unsigned int}' from 'int64_t \{aka long long int}' may alter its value [-Werror=conversion]
[393|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L393] return ZSTD_compressBound(input_len);
[394|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L394] ^
[395|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L395]cc1plus.exe: all warnings being treated as errors
[396|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L396]make[2]: *** [src/arrow/CMakeFiles/arrow_objlib.dir/build.make:63: src/arrow/CMakeFiles/arrow_objlib.dir/util/compression_zstd.cc.obj] Error 1
[397|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L397]make[1]: *** [CMakeFiles/Makefile2:1102: src/arrow/CMakeFiles/arrow_objlib.dir/all] Error 2
[398|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L398]make[1]: *** Waiting for unfinished jobs....
[399|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L399]-- googletest_ep build command succeeded. See also C:/projects/arrow/cpp/build/googletest_ep-prefix/src/googletest_ep-stamp/googletest_ep-build-*.log
[400|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L400][ 13%] Performing install step for 'googletest_ep'
[401|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L401]-- googletest_ep install command succeeded. See also C:/projects/arrow/cpp/build/googletest_ep-prefix/src/googletest_ep-stamp/googletest_ep-install-*.log
[402|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L402][ 13%] Completed 'googletest_ep'
[403|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L403][ 13%] Built target googletest_ep
[404|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L404]make: *** [Makefile:141: all] Error 2
[405|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L405]
[406|https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/22762420/job/4xuv4mpnwvmq5byf#L406]",ci-failure pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2019-03-02 22:24:21,15
13219106,[Java] Add documentation to all classes and enable checkstyle for class javadocs,This is likely a big issue. So it might pay to create subtasks for different modules to add javadoc then do one final cleanup for enabling check-style.,pull-request-available,"['Documentation', 'Java']",ARROW,New Feature,Minor,2019-03-02 20:01:44,15
13219105,[Java] Upgrade to JUnit 5 ,Junit 5 has some nice features for unit testing exceptions using Lambdas.,pull-request-available,['Java'],ARROW,New Feature,Minor,2019-03-02 20:00:00,15
13219097,[Rust] [DataFusion] It should be possible to share a logical plan between threads,"I want to be able to compile sql to a logical plan and then share that plan with other threads ( so I can run the same query in parallel on partitions of my input relation).



A/C
 * LogicalPlan uses Arc instead of Rc
 * ExecutionContext has a create_logical_plan method
 * ExecutionContext.sql() is refactored to call create_logical_plan",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-03-02 18:22:21,10
13218907,[Rust] Implement ability to check if two schemas are the same,When creating RecordBatch it would be desirable to ensure that all batches have the same schema and that the schema matches the schema defined for the RecordBatch. We currently have no way to compare two schemas to see if they are equivalent.,newbie pull-request-available,['Rust'],ARROW,Improvement,Major,2019-03-01 15:46:00,12
13218811,[C++] Dictionary tests disabled under MinGW builds,"Follow up to needed for[arrow/pull/3693/files|https://github.com/apache/arrow/pull/3693/files].

Undercpp/src/arrow/CMakeLists.txt, PR disabledarray-dict-test.cc test, by adding:


{code:java}
if(WIN32)
	 add_arrow_test(array-test
			SOURCES
			array-test.cc
			array-binary-test.cc
			array-list-test.cc
			array-struct-test.cc)
else()
	 add_arrow_test(array-test
			SOURCES
			array-test.cc
			array-binary-test.cc
			array-dict-test.cc
			array-list-test.cc
			array-struct-test.cc)
endif(){code}


Which should be reverted and investigated further. The build error that including this test triggers is the following:
{code:java}
/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0xb9a2): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0xcb8a): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0xeef8): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0x10240): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::DictionaryBuilder(std::shared_ptr<arrow::DataType> const&, arrow::MemoryPool*)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0x104fc): undefined reference to `arrow::DictionaryBuilder<arrow::FixedSizeBinaryType>::AppendArray(arrow::Array const&)'
CMakeFiles/arrow-array-test.dir/objects.a(array-dict-test.cc.obj):array-dict-test.cc:(.text+0x108ef): undefined reference to `arrow{code}",pull-request-available,['C++'],ARROW,Test,Major,2019-03-01 05:37:18,1
13218810,[C++] Python not being built nor test under MinGW builds,"Follow up to needed for[arrow/pull/3693/files|https://github.com/apache/arrow/pull/3693/files].

appveyor-cpp-build-mingw.bat has not yet enabled Python tests, need to revert,

-DARROW_PYTHON=OFF

Suggestion was to use,
{code:java}
diff --git a/ci/appveyor-cpp-build-mingw.bat b/ci/appveyor-cpp-build-mingw.bat
index 06e8b7f7..3a853031 100644
--- a/ci/appveyor-cpp-build-mingw.bat
+++ b/ci/appveyor-cpp-build-mingw.bat
@@ -24,6 +24,15 @@ set INSTALL_DIR=%HOMEDRIVE%%HOMEPATH%\install
set PATH=%INSTALL_DIR%\bin;%PATH%
set PKG_CONFIG_PATH=%INSTALL_DIR%\lib\pkgconfig

+for /f ""usebackq"" %%v in (`python3 -c ""import sys; print('.'.join(map(str, sys.version_info[0:2])))""`) do (
+ set PYTHON_VERSION=%%v
+)
+
+set PYTHONHOME=%MINGW_PREFIX%\lib\python%PYTHON_VERSION%
+set PYTHONPATH=%PYTHONHOME%
+set PYTHONPATH=%PYTHONPATH%;%MINGW_PREFIX%\lib\python%PYTHON_VERSION%\lib-dynload
+set PYTHONPATH=%PYTHONPATH%;%MINGW_PREFIX%\lib\python%PYTHON_VERSION%\site-packages
+
{code}
However, this suggestion currently trigger a built error in Travis,
{code:java}
[ 43%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/json-simple.cc.obj
[ 44%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/message.cc.obj
[ 44%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/metadata-internal.cc.obj
[ 45%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/reader.cc.obj
[ 45%] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/ipc/writer.cc.obj
[ 45%] Built target arrow_objlib
make: *** [Makefile:141: all] Error 2
C:\projects\arrow\cpp\build>goto scriptexit{code}
Therefore, additional investigation is needed.",pull-request-available,['C++'],ARROW,Test,Major,2019-03-01 05:37:12,1
13218568,[C++][CI] Clang7 Valgrind complains when not move shared_ptr,https://travis-ci.org/apache/arrow/jobs/499587100,ci-failure pull-request-available,"['C++', 'Continuous Integration']",ARROW,Bug,Blocker,2019-02-28 08:34:09,15
13218491,"[C++][R] New linting script skip files with ""cpp"" extension","As a result, the {{r/lint.sh}} script is not checking the cpp files",pull-request-available,['R'],ARROW,Bug,Major,2019-02-27 23:36:51,14
13218483,[C++] Add multithreaded JSON reader ,"The JSON reader currently only parses from a single, contiguous buffer and only in a single thread. This would be much more useful if it supported multithreaded parsing from a stream, as CSV does",pull-request-available,['C++'],ARROW,New Feature,Major,2019-02-27 23:17:45,6
13218481,[C++] move BitsetStack to bit-util.h,"BitsetStack was written for use in the JSON parser, but it's useful enough that it should be made available in bit-util.h",pull-request-available,['C++'],ARROW,New Feature,Trivial,2019-02-27 23:14:21,6
13218477,[Rust] CSV reader should show line number and error message when failing to parse a line,"We currently throw away the original error and do not report line number, making it very difficult to debug.",newbie pull-request-available,['Rust'],ARROW,Improvement,Major,2019-02-27 22:58:01,10
13218448,[CI][GLib] Plasma test is flaky,"https://travis-ci.org/apache/arrow/jobs/496581538

{noformat}
I0221 17:46:34.225402 20533 store.cc:1093] Allowing the Plasma store to use up to 0.00104858GB of memory.
I0221 17:46:34.227128 20533 store.cc:1120] Starting object store with directory /dev/shm and huge page support disabled
I0221 17:46:34.229485 20533 store.cc:693] Disconnecting client on fd 7
No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.
{noformat}",ci-failure pull-request-available,['GLib'],ARROW,Test,Major,2019-02-27 21:15:24,1
13218441,[C++] Upgrade dependency versions,"At some point we should probably update the versions of the third-party libraries we depend on. There might be useful bug or security fixes there, or performance improvements.",pull-request-available,['C++'],ARROW,Task,Major,2019-02-27 20:23:08,2
13218429,[C++] json parser should not rely on null terminated buffers,"Null terminated buffers are not always trivial to guarantee, for example when parsing mmapped files",pull-request-available,['C++'],ARROW,Bug,Minor,2019-02-27 19:27:49,6
13218418,[C++] Add URI parsing facility,This is a prerequisite for ARROW-4651.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-02-27 18:48:38,2
13218412,Verify release script is over optimist with CUDA detection,"I have a Nvidia GPU without cuda, everytime I run the verification scripts it borks in the middle because ARROW_HAVE_CUDA is evaluated to yes because `nvidia-smi --list-gpus` returns true. This can be a long process if I forget about it.

Would it be better to check for `CUDA_HOME`?",pull-request-available,['Developer Tools'],ARROW,Bug,Minor,2019-02-27 18:30:46,13
13218358,[CI] detect-changes.py is inconsistent,"Some examples of pull-requests with wrong affected files:
  -[pr-3762|https://github.com/apache/arrow/pull/3762/files]shouldn't trigger [javascript|https://travis-ci.org/apache/arrow/jobs/498805479#L217]
  -[pr-3767|https://github.com/apache/arrow/pull/3767/files]shouldn't affect files found in [rust|https://travis-ci.org/apache/arrow/jobs/499122044]and [javascript|https://travis-ci.org/apache/arrow/jobs/499122041#L217]

In [get_travis_commit_range|https://github.com/apache/arrow/blob/master/ci/detect-changes.py#L63-L67] , it references the following [comment|https://github.com/travis-ci/travis-ci/issues/4596#issuecomment-139811122]. If read further down in the[thread|https://github.com/travis-ci/travis-ci/issues/4596#issuecomment-434532772], you'll note that it can go bonkers due to shallowness and commit of branch creation. I'm not sure if this is the issue.",pull-request-available travis-ci,['Continuous Integration'],ARROW,Bug,Major,2019-02-27 14:30:01,2
13218165,[C++][Parquet] 16MB limit on (nested) column chunk prevents tuning row_group_size,"We working on parquet files that involve nested lists. At most they are multi-dimensional lists of simple types (never structs), but i understand, for Parquet, they're still nested columns and involve repetition levels.

Some of these columnshold lists of rather large byte arrays (that dominate the overall size of the row). When we bump the `row_group_size` to above 16MB we see:


{code:java}
File ""pyarrow/_parquet.pyx"", line 700, in pyarrow._parquet.ParquetReader.read_row_group
 File ""pyarrow/error.pxi"", line 89, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs{code}


I conclude it's [this|https://github.com/apache/arrow/blob/master/cpp/src/parquet/arrow/reader.cc#L848] bit complaining:


{code:java}
template <typename ParquetType>
	Status PrimitiveImpl::WrapIntoListArray(Datum* inout_array) {
	if (descr_->max_repetition_level() == 0) {
	  // Flat, no action
	  return Status::OK();
	}
	
	std::shared_ptr<Array> flat_array;
	
	// ARROW-3762(wesm): If inout_array is a chunked array, we reject as this is
	// not yet implemented
	if (inout_array->kind() == Datum::CHUNKED_ARRAY) {
	  if (inout_array->chunked_array()->num_chunks() > 1) {
	    return Status::NotImplemented(
	      ""Nested data conversions not implemented for ""
	      ""chunked array outputs"");{code}


This appears to happeninthe callstack ofColumnReader::ColumnReaderImpl::NextBatch
and it appears to be provoked by [this|https://github.com/apache/arrow/blob/de84293d9c93fe721cd127f1a27acc94fe290f3f/cpp/src/parquet/arrow/record_reader.cc#L604] constant:
{code:java}
template <>    
void TypedRecordReader<ByteArrayType>::InitializeBuilder() {    
  // Maximum of 16MB chunks    
  constexpr int32_t kBinaryChunksize = 1 << 24;    
  DCHECK_EQ(descr_->physical_type(), Type::BYTE_ARRAY);      
  builder_.reset(
    new::arrow::internal::ChunkedBinaryBuilder(kBinaryChunksize, pool_));  }   {code}
Which appears to implythat the column chunk data, if larger thankBinaryChunksize (hardcoded to 16MB), is returned as a Datum::CHUNKED_ARRAY of more than one (16MB) chunks. Which ultimatelly leads to the Status::NotImplemented error.

We have no influence over what data we ingest, we have some influence in how we flatten it and we need to tune our row_group_size to something sensibly larger than 16MB.

We have see no obvious workaround for this and so we need to ask (1) if the above diagnosis appears to correct (2) do people see any sensible workarounds (3) is there an imminent intention to fix this in the Arrow community and if not, how difficult would it be to fix this (in case we can afford helping)",parquet pull-request-available,['C++'],ARROW,Bug,Blocker,2019-02-26 20:11:30,14
13218150,[Python] FlightServerBase.run should exit on Ctrl-C,"Currently, the Python Flight server does run react at all to Ctrl-C (aka SIGINT). It should probably return from the `run()` method after having executed Python signal handlers.",pull-request-available,"['FlightRPC', 'Python']",ARROW,Bug,Major,2019-02-26 18:37:45,2
13218099,[Python] CI failures in test_cython.py,"It seems we're starting to get Travis-CI failures in {{test_cython.py}}. Example here:
{code}
----------------------------- Captured stdout call -----------------------------
Compiling pyarrow_cython_example.pyx because it changed.
[1/1] Cythonizing pyarrow_cython_example.pyx
running build_ext
building 'pyarrow_cython_example' extension
creating build
creating build/temp.linux-x86_64-2.7
gcc -fno-strict-aliasing -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O3 -pipe -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=${PREFIX}=/usr/local/src/conda-prefix -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/travis/build/apache/arrow/pyarrow-test-2.7/lib/python2.7/site-packages/numpy/core/include -I/home/travis/build/apache/arrow/python/pyarrow/include -I/home/travis/build/apache/arrow/pyarrow-test-2.7/include/python2.7 -c pyarrow_cython_example.cpp -o build/temp.linux-x86_64-2.7/pyarrow_cython_example.o -std=c++11
----------------------------- Captured stderr call -----------------------------
gcc: error: unrecognized command line option '-fno-plt'
error: command 'gcc' failed with exit status 1
{code}

(from https://travis-ci.org/apache/arrow/jobs/498722197)

I'm afraid this looks like a conda-forge toolchain issue :-(
",ci-failure pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2019-02-26 13:59:19,2
13218005,[Rust] [DataFusion] Implement parallel query execution using threads,"I am planning on tackling this soon. The basic plan is to change the table scan() method to return multiple scanners (one per partition) so that the query execution can process each partition on a separate thread.

This will involve changing some of the DataFusion APIs to use Arc instead of Rc.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-02-26 05:41:46,10
13217994,[Rust] [DataFusion] Implement in-memory DataSource,"Implement a new in-memory data source so that DataFusion can execute queries against data that is already loaded into memory.



",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-02-26 03:16:04,10
13217913,[Python] Error serializing bool ndarray in py2 and deserializing in py3,"{{np.bool}} is the only dtype I've found that causes this issue. Both empty and non-empty arrays cause it.

The issue only manifests from py2 to py3; staying within the same version succeeds, as does serializing from py3 and deserializing in py2.

This appears to just be due to Python 2 {{str}} being deserialized in Python 3 as {{bytes}}; it should be {{unicode}} on the py2 end to come back as {{str}} in py3. I suppose something in the serialization implementation is writing the dtype (just for bool arrays?) using a {{str}}, but haven't dug into it yet.


{code:bash}
(two)bash-3.2$ python cereal.py
(two)bash-3.2$ cat cereal.py 
# Python 2
import numpy as np
import pyarrow as pa

data = np.array([], dtype=np.dtype('bool'))
buf = pa.serialize(data).to_buffer()

outstream = pa.output_stream(""buffer"")
outstream.write(buf)
outstream.close()

# ...switch to python 3 venv...
(three)bash-3.2$ cat decereal.py 
# Python 3
import numpy as np
import pyarrow as pa

instream = pa.input_stream(""buffer"")
buf = instream.read()

data = pa.deserialize(buf)
print(data)
(three)bash-3.2$ python3 decereal.py 
Traceback (most recent call last):
  File ""decereal.py"", line 10, in <module>
    data = pa.deserialize(buf)
  File ""pyarrow/serialization.pxi"", line 448, in pyarrow.lib.deserialize
  File ""pyarrow/serialization.pxi"", line 411, in pyarrow.lib.deserialize_from
  File ""pyarrow/serialization.pxi"", line 262, in pyarrow.lib.SerializedPyObject.deserialize
  File ""pyarrow/serialization.pxi"", line 175, in pyarrow.lib.SerializationContext._deserialize_callback
TypeError: can only concatenate str (not ""bytes"") to str
{code}
",pull-request-available,['Python'],ARROW,Bug,Minor,2019-02-25 20:49:24,14
13217872,[C++] Implement AssertDatumEquals,Aggregate tests could benefit from this.,pull-request-available,['C++'],ARROW,Improvement,Minor,2019-02-25 16:50:06,13
13217734,[C++]clang-7 matrix entry is build using gcc,"Travis sets the following environment variables:

{code}
$ export CC=""clang-7""
$ export CXX=""clang++-7""
$ export TRAVIS_COMPILER=g++
$ export CXX=g++
$ export CXX_FOR_BUILD=g++
$ export CC=gcc
$ export CC_FOR_BUILD=gcc
$ export PATH=/usr/lib/ccache:$PATH
{code}",pull-request-available travis-ci,"['C++', 'Continuous Integration']",ARROW,Bug,Major,2019-02-25 06:45:40,13
13217681,[Rust] compute::sum performance issue,"I'm running some microbenchmarks over at [https://github.com/andygrove/row-vs-col-rs] to compare performance of arrow to just using Vec<> and I'm finding that Arrow is 18x slower in these benchmarks.

I am quite surprised at this performance difference so am creating this issue and will attempt to optimize the code.",pull-request-available,['Rust'],ARROW,Improvement,Major,2019-02-24 17:22:10,10
13217611,[Java] No Bounds checking on ArrowBuf.slice,"While reviewing some code I realized that there is no bounds checking on ArrowBuf slicing. Example negative test case that should pass but is currently failing can be found here:

[https://gist.github.com/jacques-n/737c26b7016ed29dc710d4aba617340e]

It may be that this doesn't causemore problems because the index checks do exist on memory access but fixing this would make it much easier to understand where a code mistake was made.",pull-request-available,['Java'],ARROW,Bug,Critical,2019-02-23 17:13:55,15
13217481,[C++] DCHECK macro conditions are evaluated in release builds,"{{DCHECK(potentially_expensive())}} will evaluate the argument even in release mode, and is used in several places with the assumption that it will do so (which means removing the guarantee of evaluation causes numerous failures). By contrast, most debug assertion macros elide their arguments entirely ({{<cassert>.assert}}, {{<glog/logging.h>}}) in release mode",pull-request-available,['C++'],ARROW,Bug,Trivial,2019-02-22 16:07:33,14
13217480,[Packaging] Conda-forge build misses gflags on linux,See build: https://travis-ci.org/kszucs/crossbow/builds/496958426,ci-failure pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-02-22 16:04:28,8
13217434,[C++] gflags fails to build due to CMake error,"gflags fails to build as a thirdparty download on linux and cmake 3.10.2. Removing the line `target_compile_definitions(${GFLAGS_LIBRARY} INTERFACE ""GFLAGS_IS_A_DLL=0"")` makes it build without issue.
{code}
CMake Error at cmake_modules/ThirdpartyToolchain.cmake:658 (target_compile_definitions):
Cannot specify compile definitions for imported target ""gflags_static"".
Call Stack (most recent call first):
CMakeLists.txt:506 (include)
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-22 12:55:33,1
13217430,[CI] ubuntu/debian nightlies fail because of missing gandiva files,"The nightly jobs fail with
{code:java}
   dh_install

dh_install: libgandiva13 missing files: usr/lib/*/gandiva/

dh_install: missing files, aborting

debian/rules:14: recipe for target 'binary' failed

make: *** [binary] Error 2

dpkg-buildpackage: error: fakeroot debian/rules binary gave error exit status 2{code}
[~kou] [~kszucs] Is this because we now ship the precompiled code inside of the binary library?",pull-request-available,"['C++', 'Continuous Integration', 'Packaging']",ARROW,Bug,Major,2019-02-22 12:34:24,3
13217398,[Release] gbenchmark should not be needed for verification,"{{gbenchmark}} is built during verification and thus we require a minimal version of CMake 3.6. I would have guessed that we should not require it as we do not need to build the benchmarks during the verification. I guess that a recent fix from [~wesmckinn] may have fixed this, but we should verify this before doing the next release.",pull-request-available,"['C++', 'Packaging']",ARROW,Bug,Major,2019-02-22 09:48:41,13
13217281,[C++] Implicit Flight target dependencies cause compilation failure,"
{code:sh}
In file included from ../src/arrow/flight/internal.h:23:0,
                 from ../src/arrow/python/flight.cc:20:
../src/arrow/flight/protocol-internal.h:22:10: fatal error: arrow/flight/Flight.grpc.pb.h: No such file or directory
 #include ""arrow/flight/Flight.grpc.pb.h""  // IWYU pragma: export
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
{code}
",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2019-02-21 18:46:57,13
13217249,"[Format] Flight Location should be more flexible than a (host, port) pair","The more future-proof solution is probably to define a URI format. gRPC already has something like that, though we might want to define our own format:
https://grpc.io/grpc/cpp/md_doc_naming.html
",pull-request-available,"['FlightRPC', 'Format']",ARROW,Bug,Major,2019-02-21 16:16:47,0
13217219,[C++/CI/R] Add (nightly) job that builds `brew install apache-arrow --HEAD`,"Now that we have an Arrow homebrew formula again and we may want to have it as a simple setup for R Arrow users, we should add a nightlycrossbow task that checks whether this still builds fine.

To implement this, one should write a new travis.yml like [https://github.com/apache/arrow/blob/master/dev/tasks/python-wheels/travis.osx.yml]that calls {{brew install apache-arrow --HEAD}}. This task should then be added to https://github.com/apache/arrow/blob/master/dev/tasks/tests.yml so that it is executed as part of the nightly chain.",nightly pull-request-available travis-ci,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-02-21 14:44:27,4
13217204,[C++/Question] Naming/organizational inconsistencies in cpp codebase,"Even after my eyes are used to the codebase, I still find the namings and/or code organization inconsistent.

h2. File Formats

So arrow already support a couple of file formats, namely parquet, feather, json, csv, orc, but their placement in the codebase is quiet odd:
- parquet: src/parquet
- feather: src/arrow/ipc/feather
- orc: src/arrow/adapters/orc
- csv: src/arrow/csv
- json: src/arrow/json
I might misunderstand the purpose of these sources, but I'd expect them to be organized under the same roof.

h2. Inter-Process-Communication vs. Flight

I'd expect flight's functionality from the ipc names. 

Flight's placement is a bit odd too, because it has its own codename, it should be placed under cpp/src - like parquet, plasma, or gandiva.

",pull-request-available,['C++'],ARROW,Improvement,Major,2019-02-21 13:47:15,2
13217194,[Packaging] dev/release/00-prepare.sh fails for minor version changes,"When the next version is only on the patch level, we don't need to move the debian libraries to a different suffix.",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-02-21 13:02:27,8
13217170,[C++/Packaging] Ship gandiva with the conda-forge packages,"Gandiva is not yet built with the conda packages:
https://github.com/conda-forge/arrow-cpp-feedstock/blob/master/recipe/build.sh",conda-forge pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2019-02-21 11:10:38,3
13217168,[C++/Packaging] Ship Gandiva with OSX and Windows wheels,"Gandiva is only installed via the linux wheels, We should support it on all platforms.",pull-request-available,"['C++ - Gandiva', 'Packaging']",ARROW,Improvement,Major,2019-02-21 11:08:57,3
13217167,[C++/Docker] Build Gandiva in the docker containers,"Install LLVM dependency and enable it:

https://github.com/apache/arrow/pull/3484/files#diff-1f2ebc25efb8f1e6646cbd31ce2f34f4R51",docker,['C++ - Gandiva'],ARROW,Improvement,Major,2019-02-21 11:06:48,8
13217069,[C++] Add compiler diagnostic color when using Ninja,"Due to [ninja-ism|https://github.com/ninja-build/ninja/issues/174], this forces color of errors/warnings.

Very handy for C++.",pull-request-available,['C++'],ARROW,Improvement,Trivial,2019-02-21 03:37:40,13
13216966,[C++] Flight builds complain of -Wstrict-aliasing ,"The compiler is rightly trying to protect us from doing a ""bad thing"", but I think this warning simply needs to be disabled in these compilation units

{code}
/usr/bin/ccache /home/wesm/miniconda/envs/arrow-3.7/bin/x86_64-conda_cos6-linux-gnu-c++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist//include -DARROW_NO_DEPRECATED_API -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -Isrc -I../src -isystem /home/wesm/cpp-toolchain/include -isystem /home/wesm/miniconda/envs/arrow-3.7/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem jemalloc_ep-prefix/src -isystem /usr/local/hadoop/include -isystem /home/wesm/cpp-toolchain/include/thrift -Wno-noexcept-type -fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -O3 -DNDEBUG  -Wall -Wconversion -Wno-sign-conversion -Werror -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG -fPIC   -std=gnu++11 -MD -MT src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/server.cc.o -MF src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/server.cc.o.d -o src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/server.cc.o -c ../src/arrow/flight/server.cc
../src/arrow/flight/server.cc: In member function 'virtual grpc::Status arrow::flight::FlightServiceImpl::DoGet(ServerContext*, const arrow::flight::protocol::Ticket*, ServerWriter<arrow::flight::protocol::FlightData>*)':
../src/arrow/flight/server.cc:192:75: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]
     writer->Write(*reinterpret_cast<const pb::FlightData*>(&schema_payload),
                                                                           ^
../src/arrow/flight/server.cc:199:75: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]
           !writer->Write(*reinterpret_cast<const pb::FlightData*>(&payload),
                                                                           ^
cc1plus: all warnings being treated as errors
[11/113] Building CXX object src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/client.cc.o
FAILED: src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/client.cc.o 
/usr/bin/ccache /home/wesm/miniconda/envs/arrow-3.7/bin/x86_64-conda_cos6-linux-gnu-c++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist//include -DARROW_NO_DEPRECATED_API -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_BZ2 -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -Isrc -I../src -isystem /home/wesm/cpp-toolchain/include -isystem /home/wesm/miniconda/envs/arrow-3.7/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem jemalloc_ep-prefix/src -isystem /usr/local/hadoop/include -isystem /home/wesm/cpp-toolchain/include/thrift -Wno-noexcept-type -fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -O3 -DNDEBUG  -Wall -Wconversion -Wno-sign-conversion -Werror -msse4.2 -fno-omit-frame-pointer -O3 -DNDEBUG -fPIC   -std=gnu++11 -MD -MT src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/client.cc.o -MF src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/client.cc.o.d -o src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/client.cc.o -c ../src/arrow/flight/client.cc
../src/arrow/flight/client.cc: In member function 'virtual arrow::Status arrow::flight::FlightPutWriter::FlightPutWriterImpl::WriteRecordBatch(const arrow::RecordBatch&, bool)':
../src/arrow/flight/client.cc:126:74: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]
     if (!writer_->Write(*reinterpret_cast<const pb::FlightData*>(&payload),
                                                                          ^
../src/arrow/flight/client.cc: In member function 'arrow::Status arrow::flight::FlightClient::FlightClientImpl::DoPut(const arrow::flight::FlightDescriptor&, const std::shared_ptr<arrow::Schema>&, std::unique_ptr<arrow::ipc::RecordBatchWriter>*)':
../src/arrow/flight/client.cc:310:79: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]
     if (!write_stream->Write(*reinterpret_cast<const pb::FlightData*>(&payload),
                                                                               ^
cc1plus: all warnings being treated as errors
[12/113] Performing configure step for 'jemalloc_ep'
-- jemalloc_ep configure command succeeded.  See also /home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep-stamp/jemalloc_ep-configure-*.log
[13/113] Building CXX object src/arrow/flight/CMakeFiles/arrow_flight_objlib.dir/protocol-internal.cc.o
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-20 15:49:56,14
13216965,[Python] Add docker-compose configuration to build and test the project without pandas installed,This will help detect problems where we have unintentionally introduced a hard dependency,pull-request-available,['Python'],ARROW,Improvement,Major,2019-02-20 15:40:43,3
13216910,[Python] Avoid importing Pandas unless necessary,"Importing PyArrow is more than twice slower when Pandas is installed:
{code}
$ time python -c ""import pyarrow""

real	0m0,360s
user	0m0,305s
sys	0m0,037s

$ time python -c ""import sys; sys.modules['pandas'] = None; import pyarrow""

real	0m0,144s
user	0m0,124s
sys	0m0,020s
{code}

We should only import Pandas when necessary, e.g. when asked to ingest or create Pandas data.",pull-request-available,['Python'],ARROW,Improvement,Minor,2019-02-20 12:26:52,14
13216669,[Rust] [DataFusion] Implement type coercion query optimizer rule,"Now that we have a query optimizer, we should re-implement type coercion as an optimizer rule that rewrites expressions with explicit casts where required, so that at runtime we are only comparing like types.

For example, the expression {{float_column < int_column}}would be rewritten as {{float_column < CAST(int_column AS float)}}.

DataFusion already has this logic but the current implementation is somewhat hacky and incomplete. Moving it to the optimizer will allow us to implement this correctly.",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-02-19 15:09:53,10
13216635,[Flight] Add application metadata field to DoGet,"As [proposed on the mailing list|https://lists.apache.org/thread.html/c550264cd60e000d77e10d9d7ac81ea8c49efc37ad447177fa8ee4ee@%3Cdev.arrow.apache.org%3E], we should add a field for application-specific metadata in DoGet payloads and expose this in the APIs. The current APIs are rather RecordBatch-oriented, though.",pull-request-available,"['FlightRPC', 'Format']",ARROW,Improvement,Major,2019-02-19 13:58:10,0
13216632,[Flight] Wrap server busy-wait methods,"Right now in Java, you must manually busy-wait in a loop as the gRPC server's awaitTermination method isn't exposed. Conversely, in C++, you have no choice but to busy-wait as starting the server calls awaitTermination for you. Either Java should also wait on the server, or both Java and C++ should expose an explicit operation to wait on the server.

I would prefer the latter as then the Python bindings could choose to manually busy-wait, which would let Ctrl-C work as normal.",pull-request-available,['FlightRPC'],ARROW,Improvement,Minor,2019-02-19 13:50:09,0
13216588,[C++] Linker errors when building benchmarks,"All C++ benchmarks now fail linking here:
{code}
[10/162] Linking CXX executable release/arrow-io-file-benchmark
FAILED: release/arrow-io-file-benchmark 
: && /usr/bin/ccache /usr/bin/g++-7  -Wno-noexcept-type  -O3 -DNDEBUG  -Wall -msse4.2 -fdiagnostics-color=always -Wextra -Wunused-result -Wno-unused-parameter -Wno-implicit-fallthrough -Wconversion -D_GLIBCXX_USE_CXX11_ABI=1 -fno-omit-frame-pointer -g -O3 -DNDEBUG  -rdynamic src/arrow/io/CMakeFiles/arrow-io-file-benchmark.dir/file-benchmark.cc.o  -o release/arrow-io-file-benchmark  release/libarrow_benchmark_main.a gbenchmark_ep/src/gbenchmark_ep-install/lib/libbenchmark.a -lpthread && :
src/arrow/io/CMakeFiles/arrow-io-file-benchmark.dir/file-benchmark.cc.o: In function `arrow::BenchmarkStreamingWrites(benchmark::State&, std::valarray<long>, arrow::io::OutputStream*, arrow::BackgroundReader*)':
/home/antoine/arrow/cpp/build/../src/arrow/io/file-benchmark.cc:139: undefined reference to `arrow::Status::ToString[abi:cxx11]() const'
/home/antoine/arrow/cpp/build/../src/arrow/io/file-benchmark.cc:63: undefined reference to `arrow::internal::FileWrite(int, unsigned char const*, long)'

[ snip tons of similar errors ]
{code}

My build script:
{code}
ARROW_CXXFLAGS=""$ARROW_CXXFLAGS -fno-omit-frame-pointer -g""

mkdir -p build
pushd build

cmake .. -GNinja \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_INSTALL_PREFIX=$ARROW_HOME \
    -DCMAKE_INSTALL_MESSAGE=LAZY \
    -DARROW_CXXFLAGS=""$ARROW_CXXFLAGS"" \
    -DARROW_BUILD_TESTS=off \
    -DARROW_BUILD_BENCHMARKS=on \
    -DARROW_CUDA=on \
    -DARROW_FLIGHT=on \
    -DARROW_PARQUET=on \
    -DARROW_PLASMA=off \
    -DARROW_PYTHON=on \
    -DARROW_USE_GLOG=off \

nice cmake --build . --target install

popd
{code}",pull-request-available,"['Benchmarking', 'C++']",ARROW,Bug,Critical,2019-02-19 10:41:46,2
13216467,[Docker] Makefile to build dependent docker images,"Docker compose cannot be used to build image hierarchies:
- https://github.com/docker/compose/issues/6093
- https://github.com/docker/compose/issues/6264#issuecomment-429268195",pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-02-18 21:01:35,3
13216447,[C++]Log message in BuildUtils as STATUS,"Without a logging level, they are considered as warning like and will be shown to the user in {{cmake}}after they run a configure.",pull-request-available,['C++'],ARROW,Improvement,Major,2019-02-18 18:29:21,8
13216446,[C++] Add checked_pointer_cast,In the style of {{checked_cast}} but for the equivalent {{dynamic/static_pointer_cast}}.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-02-18 18:13:41,8
13216317,[C++] Rework CMake third-party logic,"Instead of the current approach we are taking with the {{*_HOME}} variables, we should use more CMake features and also give users of Arrow a more high-level control. This is going to be a rather lengthy issue with a lot of subtasks.
 * Let the user decide on the top-level how dependencies should be handled. At the moment I can think ofthe followingmodes:
 ** AUTO: Guess the packaging system we're running in, use this where possible, otherwise build the dependencies through the {{ExternalProject}} logic.
 ** BUNDLED: Don't use any dependencies, build them all through {{ExternalProject}}
 ** SYSTEM: Use CMake's {{find_package}} and {{find_library}} without any custom paths. If packages are on non-default locations, let the user indicate it from the outside using the {{*_ROOT}} variables.
 ** CONDA: Same as SYSTEM but set all {{*_ROOT}} variables to {{ENV\{CONDA_PREFIX\}}}.
 ** BREW: This uses SYSTEM but asks {{brew}} for some dependencies for their installation prefix.
 * prefer dynamic linkage where possible
 * Use {{pkg-config}} and {{*Targets.cmake}} files in projects that publish these
 * Ensure that the necessary integration tests are in place (Fedora, Debian, Ubuntu, Alpine)
 * Integration tests that Arrow's {{*Targets.cmake}} and {{arrow.pc}} work.",pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2019-02-18 08:01:01,8
13216245,[Rust][ [DataFusion] Integrate query optimizer with ExecutionContext,Integrate the new projection push down query optimizer rule with the query engine so that queries only load referenced columns from disk.,pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-02-17 15:42:53,10
13216244,[Python] Master build is broken due to missing licence for .dockerignore,"{code:java}
NOT APPROVED: python/manylinux1/.dockerignore (apache-arrow/python/manylinux1/.dockerignore): false
1 unapproved licences. Check rat report: rat.txt
The command ""$TRAVIS_BUILD_DIR/ci/travis_release_audit.sh"" failed and exited with 1 during .{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-02-17 15:40:21,8
13216216,[Ruby] Arrow::DictionaryArray#[] should returns the item in the indices array,"Arrow::DictionaryArray#[] should returns the item in the indices array. However, the current behavior is error like below:

{{Traceback (most recent call last):}}
 {{    5: from test.rb:4:in `<main>'}}
 {{   4: from test.rb:4:in `new'}}
 {{   3: from /Users/mrkn/src/github.com/apache/arrow/ruby/red-arrow/lib/arrow/dictionary-data-type.rb:103:in `initialize'}}
 {{   2: from /Users/mrkn/.rbenv/versions/2.6.0/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.3.1/lib/gobject-introspection/loader.rb:328:in `block in load_constructor_infos'}}
 {{   1: from /Users/mrkn/.rbenv/versions/2.6.0/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.3.1/lib/gobject-introspection/loader.rb:317:in `block (2 levels) in load_constructor_infos'}}
 {{/Users/mrkn/.rbenv/versions/2.6.0/lib/ruby/gems/2.6.0/gems/gobject-introspection-3.3.1/lib/gobject-introspection/loader.rb:317:in `invoke': *invalid argument Array (expect #<Class:0x00007ff1ad9179a8>) (+ArgumentError+)*}}

test.rb is given below:

{{require 'arrow'}}

{{ary = Arrow::DictionaryArray.new(}}
{{ Arrow::DictionaryDataType.new(:int8, %w[foo bar baz], true),}}
{{ Arrow::Int8Array.new([0, 1, 0, 1, 2, 1, 2, 0])}}
{{)}}

{{ary[0]}}

",pull-request-available,['Ruby'],ARROW,Bug,Major,2019-02-17 08:53:44,1
13216198,[Rust] [DataFusion] Implement DataFrame style API,"We currently can build a logical plan from SQL. We should be able to build the same logical plan from a DataFrame style API as well.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-02-16 23:11:23,10
13216169,[Rust] [DataFusion] Implement projection push down query optimizer rule,"If I run a query like the following:
{code:java}
SELECT MIN(fare_amount), MAX(fare_amount) FROM tripdata{code}
I see this logical plan:
{code:java}
Logical plan: Aggregate: groupBy=[[]], aggr=[[MIN(#10), MAX(#10)]]
 TableScan: tripdata projection=None{code}


This means that every column is being loaded into arrays rather than just the two columns that I care about, resulting in terrible performance.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Major,2019-02-16 17:42:18,10
13216055,Flight C++ DoPut segfaults,"After Wes fixed the undefined behavior, it turns out the implementation of DoPut on the client side is now wrong. It should construct an IpcPayload instead of going through the underlying Protobuf.

Additionally, a previous patch accidentally exposed arrow::ipc::DictionaryMemo under arrow::DictionaryMemo.",flight pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2019-02-15 19:08:54,0
13216011,[C++] Dependency of Flight C++ sources on generated protobuf is not respected,"Seems like we have a race condition somewhere as we frequently run into
{code:java}
[82/273] Building CXX object src/arrow/flight/CMakeFiles/arrow_flight_testing_objlib.dir/test-util.cc.o
FAILED: src/arrow/flight/CMakeFiles/arrow_flight_testing_objlib.dir/test-util.cc.o
/usr/local/bin/ccache /Users/ukorn/miniconda3/envs/pyarrow-dev-2/bin/x86_64-apple-darwin13.4.0-clang++ -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR=/Users/ukorn/Development/arrow-repos-2/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist//include -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_BROTLI -DARROW_WITH_LZ4 -DARROW_WITH_SNAPPY -DARROW_WITH_ZLIB -DARROW_WITH_ZSTD -Isrc -I../src -isystem /Users/ukorn/miniconda3/envs/pyarrow-dev-2/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem jemalloc_ep-prefix/src -isystem ../thirdparty/hadoop/include -isystem /Users/ukorn/miniconda3/envs/pyarrow-dev-2/include/thrift -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -Qunused-arguments -O3 -DNDEBUG -Wall -Wno-unknown-warning-option -msse4.2 -stdlib=libc++ -O3 -DNDEBUG -fPIC -std=gnu++11 -MD -MT src/arrow/flight/CMakeFiles/arrow_flight_testing_objlib.dir/test-util.cc.o -MF src/arrow/flight/CMakeFiles/arrow_flight_testing_objlib.dir/test-util.cc.o.d -o src/arrow/flight/CMakeFiles/arrow_flight_testing_objlib.dir/test-util.cc.o -c ../src/arrow/flight/test-util.cc
In file included from ../src/arrow/flight/test-util.cc:35:
In file included from ../src/arrow/flight/internal.h:29:
../src/arrow/flight/protocol-internal.h:22:10: fatal error: 'arrow/flight/Flight.grpc.pb.h' file not found
#include ""arrow/flight/Flight.grpc.pb.h""
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
[87/273] Building CXX object src/arrow/python/CMakeFiles/arrow_python_objlib.dir/python_to_arrow.cc.o
ninja: build stopped: subcommand failed.
ninja 672,82s user 33,40s system 196% cpu 5:59,62 total{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-15 15:07:07,8
13216005,[Python] Add built wheel to manylinux1 dockerignore.,Currently we add them to the docker context while we don't need them. This shrinks the context for me down to 55k instead of hundreds MiBs.,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-02-15 15:01:49,8
13215952,[C++/Python]Memory corruption on Pandas->Arrow conversion,"When converting DataFrames with numerical columns to Arrow tables we were seeing random segfaults in core Python code. This only happened in environments where we had a high level of parallelisation or slow code execution (e.g. in AddressSanitizer builds).

The reason for these segfaults was that we were incrementing the reference count of the underlying NumPy buffer but were not holding the GIL while changing the reference count.",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-02-15 10:43:35,8
13215882,[C++] gbenchmark_ep is a dependency of unit tests when ARROW_BUILD_BENCHMARKS=ON,"I hit this issue when trying to use clang-7 from conda-forge, and wasn't sure why gbenchmark_ep is getting built when I'm building only a single unit test executable like arrow-array-test

https://github.com/google/benchmark/issues/351",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-15 05:22:08,14
13215810,[C++]Interface link libraries declared on arrow_shared target that are actually non-interface,"We are pulling in {{jemalloc_static}} as an interface linking dependency in the {{arrowTargets.cmake}}. But as it is statically linked inside the shared library, consumers don't need to link against it.",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-14 18:55:20,8
13215798,[Python] Benchmark failures,"I get the following error during running the benchmarks:
{code}
               Traceback (most recent call last):
                 File ""/home/antoine/asv/asv/benchmark.py"", line 1170, in main_run_server
                   main_run(run_args)
                 File ""/home/antoine/asv/asv/benchmark.py"", line 1038, in main_run
                   skip = benchmark.do_setup()
                 File ""/home/antoine/asv/asv/benchmark.py"", line 569, in do_setup
                   result = Benchmark.do_setup(self)
                 File ""/home/antoine/asv/asv/benchmark.py"", line 501, in do_setup
                   setup(*self._current_params)
                 File ""/home/antoine/arrow/python/benchmarks/streaming.py"", line 65, in setup
                   self.source = sink.get_result()
               AttributeError: 'pyarrow.lib.BufferOutputStream' object has no attribute 'get_result'

{code}
",pull-request-available,"['Benchmarking', 'Python']",ARROW,Bug,Major,2019-02-14 18:08:50,2
13215688,[C++] Add version macros to headers,"It would be useful to have compile-time macros in the headers specifying the major/minor/patch versions, so that users can more easily maintain code that can be built with a range of arrow versions.

Other nice-to-haves:
- Maybe a ""combiner"" func that basically spits out the value as an easy to compare integer e.g. 12000 for 0.12.0 or something.
- Git hash",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-02-14 07:15:58,8
13215650,[C++][Flight] Add option to run arrow-flight-benchmark against a perf server running on a different host,Currently the assumption is that both processes are running on localhost. While also interesting (to see how fast things can go taking network IO out of the equation) it is not very realistic. It would be good to both establish a baseline network IO benchmark between two hosts and then see how close a Flight stream can get to that,pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2019-02-14 02:08:03,0
13215574,[Python] pa.decimal128 should validate inputs,"The precision shouldn't be higher than 38, but 39 is happily accepted currently:
{code:python}
>>> ty = pa.decimal128(39, 0)                                                                            
>>> arr = pa.array([2**127], type=ty)                                                                  
>>> arr                                                                                                  
<pyarrow.lib.Decimal128Array object at 0x7f9b89444138>
[
  -170141183460469231731687303715884105728
]
{code}

",pull-request-available,['Python'],ARROW,Bug,Major,2019-02-13 19:17:12,2
13215527,[C++][Flight] Create outgoing composite grpc::ByteBuffer instead of allocating contiguous slice and copying IpcPayload into it,See discussion in https://github.com/apache/arrow/pull/3633,pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2019-02-13 15:36:46,2
13215401,[Python] pyarrow can't read/write filenames with special characters,"When writing or reading files to or from paths that have special characters in them, (e.g., ""#""), pyarrow returns an error:

{code:python}
OSError: Passed non-file path...
{code}

This is a consequence of the following line:
https://github.com/apache/arrow/blob/master/python/pyarrow/filesystem.py#L416

File-paths will be parsed as URIs, which will give strange results for filepaths like: ""bad # actor.parquet"":

ParseResult(scheme='', netloc='', path='/tmp/bad ', params='', query='', fragment='actor.parquet')

This is trivial to reproduce with the following code which uses the `pd.to_parquet` and `pd.read_parquet` interfaces:

{code:python}
import pandas as pd
x = pd.DataFrame({""a"": [1,2,3]})
x.to_parquet(""bad # actor.parquet"")
x.read_parquet(""bad # actor.parquet"")
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2019-02-13 05:34:53,2
13215399,[C++][Flight] Avoid undefined behavior with gRPC memory optimizations,"Because the {{Write}} function and other on {{ServerWriter}} and {{ClientReader}} are declared virtual, some compilers may not behave in the way we want. ",pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2019-02-13 05:29:58,14
13215397,[Rust] Preserve order of JSON inferred schema,"serde_json has the ability to preserve order of JSON records read. This feature might be necessary to ensure that schema inference returns a consistent order of fields each time.

I'd like to add it separately as I'd also need to update JSON tests in datatypes.",pull-request-available,['Rust'],ARROW,Sub-task,Minor,2019-02-13 05:16:14,12
13215298,[C++] LICENSE.txt should be updated.,parquet-cpp/blob/master/LICENSE.txt isnot mentioned there.,pull-request-available,['C++'],ARROW,Task,Blocker,2019-02-12 18:04:59,13
13215272,[Rust] Read nested JSON structs into StructArrays,"_Adding this as a separate task as it's a bit involved._

Add the ability to read in JSON structs that are children of the JSON record being read.
The main concern here is deeply nested structures, which will require a performant and reusable basic JSON reader before dealing with recursion.",pull-request-available,['Rust'],ARROW,Sub-task,Minor,2019-02-12 15:13:02,12
13215213,[Rust] Add basic JSON reader,This is the first step in getting a JSON reader working in Rust,pull-request-available,['Rust'],ARROW,Sub-task,Major,2019-02-12 11:29:54,12
13215064,[C++] Handling of non-aligned slices in Sum kernel,The Sum kernel does not support slices where the offset is not byte-aligned. Other kernels avoid this problem due to BitmapReader usage.,analytics,['C++'],ARROW,Improvement,Major,2019-02-11 17:51:17,13
13214960,[Packaging] Update linux packaging tasks to align with the LLVM 7 migration,"See https://github.com/apache/arrow/pull/3499

Cosmic, Bionic, Xenial and Stretch tasks are failing:
- https://github.com/kszucs/crossbow/branches/all?query=ubuntu
- https://github.com/kszucs/crossbow/branches/all?query=debian
respectively.

We can pin clang-7 on Cosmic and Bionic, but I'm not sure what should be the strategy for Xenial and Stretch. [~kou] Do We need to make exceptions like with Trusty?
",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-02-11 10:58:35,1
13214768,[C++] DCHECK custom messages are unreachable in release,"For release builds, {{DCHECK( x ) << y << z;}} currently expands to

{code}
((void)(x)); 
while (false) ::arrow::util::ArrowLogBase() << y << z;
{code}

This is unreachable which is an error using clang-7",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-09 03:08:07,6
13214524,[Format] remove individual documents in favor of new document once all content is moved,We might want to leave the documents in place and provide links to the new consolidated document in case others are linking to published content.,pull-request-available,['Format'],ARROW,Sub-task,Major,2019-02-08 05:39:14,14
13214517,[C++] Nicer PrettyPrint for date32,"Current:

{code}
In [3]: pa.array([date(2001, 1, 1), None, date(2001, 1, 2)])                                                                                                                                   
Out[3]: 
<pyarrow.lib.Date32Array object at 0x7f033ce9c458>
[
  11323,
  null,
  11324
]
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-02-08 04:31:05,6
13214513,[C++] Reduce the number of unit test executables,"Link times are a significant drag in MSVC builds. They don't affect Linux nearly as much when building with Ninja. I suggest we combine some of the fast-running tests within logical units to see if we can cut down from 106 test executables to 70 or so

{code}
100% tests passed, 0 tests failed out of 107
Label Time Summary:
arrow-tests           =  21.19 sec*proc (48 tests)
arrow_python-tests    =   0.26 sec*proc (1 test)
example               =   0.05 sec*proc (1 test)
gandiva-tests         =  11.65 sec*proc (39 tests)
parquet-tests         =  35.81 sec*proc (18 tests)
unittest              =  68.92 sec*proc (106 tests)
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-02-08 03:18:53,14
13214456,[C++] librt and pthread hacks can cause linking problems,"There are a few places (e.g. {{plasma_store_server}} and {{arrow-stream-to-file}}) where the work I did in https://github.com/apache/arrow/commit/b4278641a6a56c56d2007469b0eb840d52cc007d#diff-6725b893dfc969abac4f4ee39a3a317f is conflicting. 

I got these linking failures on Ubuntu 18.10 using the conda-forge gcc 7.3.0 toolchain

{code}
FAILED: debug/arrow-stream-to-file 
: && /usr/bin/ccache /home/wesm/miniconda/envs/arrow-3.7/bin/x86_64-conda_cos6-linux-gnu-c++  -Wno-noexcept-type -fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe -ggdb -O0  -Wall -Wconversion -Wno-sign-conversion -Werror -msse4.2 -fno-omit-frame-pointer -g  -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags   -rdynamic src/arrow/ipc/CMakeFiles/arrow-stream-to-file.dir/stream-to-file.cc.o  -o debug/arrow-stream-to-file  -Wl,-rpath,/home/wesm/cpp-toolchain/lib debug/libarrow.a /home/wesm/cpp-toolchain/lib/libboost_filesystem.so /home/wesm/cpp-toolchain/lib/libboost_system.so -lpthread -ldl /home/wesm/cpp-toolchain/lib/libdouble-conversion.a /home/wesm/cpp-toolchain/lib/libbrotlidec-static.a /home/wesm/cpp-toolchain/lib/libbrotlienc-static.a /home/wesm/cpp-toolchain/lib/libbrotlicommon-static.a /home/wesm/cpp-toolchain/lib/libbz2.a /home/wesm/cpp-toolchain/lib/liblz4.a /home/wesm/cpp-toolchain/lib/libsnappy.a /home/wesm/cpp-toolchain/lib/libz.so /home/wesm/cpp-toolchain/lib/libzstd.a /home/wesm/cpp-toolchain/lib/libglog.a /home/wesm/cpp-toolchain/lib/libgflags.a /home/wesm/cpp-toolchain/lib/libboost_regex.so -lrt jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a -pthread && :
/home/wesm/miniconda/envs/arrow-3.7/bin/../lib/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/../../../../x86_64-conda_cos6-linux-gnu/bin/ld: jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a(nstime.pic.o): undefined reference to symbol 'clock_gettime@@GLIBC_2.2.5'
/home/wesm/miniconda/envs/arrow-3.7/bin/../lib/gcc/x86_64-conda_cos6-linux-gnu/7.3.0/../../../../x86_64-conda_cos6-linux-gnu/bin/ld: /home/wesm/miniconda/envs/arrow-3.7/bin/../x86_64-conda_cos6-linux-gnu/sysroot/usr/lib/../lib/librt.so: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
{code}

Working on a patch",pull-request-available,"['C++', 'C++ - Plasma']",ARROW,Bug,Major,2019-02-07 20:31:52,14
13214426,[Python][CI] Upgrade to latest flake8 3.7.5 in travis_lint.sh,"We are currently pinned to flake8 3.5. The latest one turns up some additional warnings

{code}
C:\Users\wesmc\code\arrow (master -> origin)
(arrow-dev)  flake8 dev\
dev\merge_arrow_pr.py:60:1: E302 expected 2 blank lines, found 1
dev\merge_arrow_pr.py:64:1: E302 expected 2 blank lines, found 1
dev\merge_arrow_pr.py:99:35: W605 invalid escape sequence '\['
dev\merge_arrow_pr.py:99:39: W605 invalid escape sequence '\]'
dev\merge_arrow_pr.py:99:43: W605 invalid escape sequence '\]'
dev\merge_arrow_pr.py:146:46: W605 invalid escape sequence '\d'
dev\merge_arrow_pr.py:454:5: F841 local variable 'e' is assigned to but never used
dev\release\changelog.py:80:25: W605 invalid escape sequence '\_'
dev\release\changelog.py:81:24: W605 invalid escape sequence '\`'
dev\release\changelog.py:82:24: W605 invalid escape sequence '\*'

C:\Users\wesmc\code\arrow (master -> origin)
(arrow-dev)  flake8 python\
python\pyarrow\filesystem.py:388:16: F632 use ==/!= to compare str, bytes, and int literals
python\pyarrow\filesystem.py:392:18: F632 use ==/!= to compare str, bytes, and int literals
{code}",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2019-02-07 18:02:34,14
13214423,[Plasma] Plasma fails building with CUDA enabled,"When compiling Plasma with CUDA enabled I get the following errors:
{code}
../src/plasma/store.cc: In member function 'uint8_t* plasma::PlasmaStore::AllocateMemory(int, size_t, int*, int64_t*, ptrdiff_t*)':
../src/plasma/store.cc:187:36: error: 'data_size' was not declared in this scope
       DCHECK_OK(context_->Allocate(data_size + metadata_size, &gpu_handle));
                                    ^
{code}

{code}
../src/plasma/store.cc: In member function 'plasma::flatbuf::PlasmaError plasma::PlasmaStore::CreateObject(const ObjectID&, int64_t, int64_t, int, plasma::Client*, plasma::PlasmaObject*)':
../src/plasma/store.cc:236:15: error: 'gpu_handle' was not declared in this scope
     DCHECK_OK(gpu_handle->ExportForIpc(&entry->ipc_handle));
               ^
{code}",pull-request-available,"['C++ - Plasma', 'GPU']",ARROW,Bug,Major,2019-02-07 17:30:47,2
13214324,[CI] CI failing for python Xcode 7.3,"The last couple of PR triggered builds have failed with this :

CMake Error at cmake_modules/FindNumPy.cmake:62 (message):
NumPy import failure:
Traceback (most recent call last):
File ""<string>"", line 1, in <module>
File ""/Users/travis/build/apache/arrow/pyarrow-test-2.7/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>
from . import add_newdocs
File ""/Users/travis/build/apache/arrow/pyarrow-test-2.7/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>
from numpy.lib import add_newdoc
File ""/Users/travis/build/apache/arrow/pyarrow-test-2.7/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>
from .type_check import *
File ""/Users/travis/build/apache/arrow/pyarrow-test-2.7/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>
import numpy.core.numeric as _nx
File ""/Users/travis/build/apache/arrow/pyarrow-test-2.7/lib/python2.7/site-packages/numpy/core/__init__.py"", line 26, in <module>
raise ImportError(msg)

[https://travis-ci.org/apache/arrow/jobs/489917808]",ci-failure pull-request-available,['Python'],ARROW,Bug,Major,2019-02-07 09:07:26,8
13214207,[Rust] [DataFusion] make accumulate_scalar somewhat exhaustive and easier to read,"make accumulate_scalar somewhat exhaustive and easier to read



The current implementation doesn't leverage any of the exhaustiveness checking of matching.This can be made simpler and partially exhaustive.",pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Trivial,2019-02-06 17:23:05,10
13213924,[Website] Fix broken link (author) in DataFusion blog post,I forgot to add contributors.yaml to the original PR,pull-request-available,['Website'],ARROW,Improvement,Minor,2019-02-05 14:38:19,10
13213923,[Website] Add blog archive page,"There's no easy way to get a bulleted list of all blog posts on the Arrow website. See example archive on my personal blog http://wesmckinney.com/archives.html

It would be great to have such a generated archive on our website",pull-request-available,['Website'],ARROW,Improvement,Major,2019-02-05 14:37:57,4
13213904,[Website] Instructions for publishing web site are missing a step,"The instructions for publishing the web site say to run the ""scripts/sync_format_docs.sh"" which copies the top level ""format"" directory to the ""_docs"" directory under site.

Existing files in ""_docs"" have references to markdown files in the ""_docs/format"" directory. For example, the IPC.md contains:
{code:java}
{% include_relative format/IPC.md %}{code}
However my top level format directory does not contain this IPC.md, so I get errors when running jekyll and I have had to create some dummy markdown files as a workaround.

I investigated this a bit and I think there is some prerequisite step that isn't documented that would cause Sphinx to run and generate docs?",pull-request-available,['Website'],ARROW,Improvement,Major,2019-02-05 13:28:11,14
13213892,[Python] Drive letter removed when writing parquet file ,"Hi everyone,
 
 importing this from Github:
 
 I encountered a problem while working with pyarrow: I am working on Windows 10. When I want to save a table using pq.write_table(tab, r'E:\parquetfiles\file1.parquet'), I get the Error ""No such file or directory"".
 After searching a bit, i found out that the drive letter is getting removed while parsing the where string, but I could not find a way solve my problem: I can write the files on my C:\ drive without problems, but I am not able to write a parquet file on another drive than C:.
 Am I doing something wrong or is this just how it works? I would really appreciate any help, because I just cannot fit my files on C: drive.",parquet,['Python'],ARROW,Bug,Blocker,2019-02-05 12:38:50,2
13213817,[Rust] [DataFusion] Post donation clean up tasks,"There are some small cleanup tasks to do now the donation is merged.
 * Rust README needs updating to link to DataFusion and to add information about the new git submodule",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Minor,2019-02-05 00:39:28,10
13213768,[Flight] FlightInfo should use signed integer types for payload size,"The de-facto practice is to use -1 in FlightInfo to indicate that the number of records/size of the payload is unknown, looking at the Java implementation. However, the Protobuf definition uses an unsigned integer type, as does the C++ implementation.",flight pull-request-available,['FlightRPC'],ARROW,Bug,Major,2019-02-04 19:19:13,0
13213732,[Website] Add instructions to do a test-deploy of Arrow website and fix bugs,"This will help with testing and proofing the website.

I have noticed that there are bugs in the website when the baseurl is not a foo.bar.baz, e.g. if you deploy at root foo.bar.baz/test-site many images and links are broken",pull-request-available,['Website'],ARROW,Improvement,Major,2019-02-04 18:04:48,4
13213674,[C++] Pass AR and RANLIB to all external projects,"With the latest updates I have problems linking to gbenchmark on OSX, this fixes it.",pull-request-available,['C++'],ARROW,Bug,Major,2019-02-04 12:43:59,8
13213626,[Python][C++] CI Failing for Python 2.7 and 3.6 with valgrind,"https://travis-ci.org/apache/arrow/jobs/488317992  

The error appears to be some sort of linker or warning as error issue?:

{{cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++
In file included from /home/travis/build/apache/arrow/python/pyarrow/include/arrow/python/platform.h:26:0,
                 from pyarrow_cython_example.cpp:653:
/home/travis/build/apache/arrow/pyarrow-test-2.7/include/python2.7/datetime.h:188:25: warning: PyDateTimeAPI defined but not used [-Wunused-variable]
 static PyDateTime_CAPI *PyDateTimeAPI = NULL;
                         ^
pyarrow_cython_example.cpp:2828:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_array defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_array)(std::shared_ptr< arrow::Array>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2829:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_chunked_array defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_chunked_array)(std::shared_ptr< arrow::ChunkedArray>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2830:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_batch defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_batch)(std::shared_ptr< arrow::RecordBatch>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2831:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_buffer defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_buffer)(std::shared_ptr< arrow::Buffer>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2832:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_column defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_column)(std::shared_ptr< arrow::Column>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2833:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_data_type defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_data_type)(std::shared_ptr< arrow::DataType>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2834:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_field defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_field)(std::shared_ptr< arrow::Field>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2835:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_resizable_buffer defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_resizable_buffer)(std::shared_ptr< arrow::ResizableBuffer>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2836:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_schema defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_schema)(std::shared_ptr< arrow::Schema>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2837:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_table defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_table)(std::shared_ptr< arrow::Table>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2838:20: warning: __pyx_f_7pyarrow_3lib_pyarrow_wrap_tensor defined but not used [-Wunused-variable]
 static PyObject *(*__pyx_f_7pyarrow_3lib_pyarrow_wrap_tensor)(std::shared_ptr< arrow::Tensor>  const &); /*proto*/
                    ^
pyarrow_cython_example.cpp:2840:47: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_batch defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::RecordBatch>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_batch)(PyObject *); /*proto*/
                                               ^
pyarrow_cython_example.cpp:2841:42: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_buffer defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::Buffer>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_buffer)(PyObject *); /*proto*/
                                          ^
pyarrow_cython_example.cpp:2842:42: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_column defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::Column>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_column)(PyObject *); /*proto*/
                                          ^
pyarrow_cython_example.cpp:2843:44: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_data_type defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::DataType>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_data_type)(PyObject *); /*proto*/
                                            ^
pyarrow_cython_example.cpp:2844:41: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_field defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::Field>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_field)(PyObject *); /*proto*/
                                         ^
pyarrow_cython_example.cpp:2845:42: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_schema defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::Schema>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_schema)(PyObject *); /*proto*/
                                          ^
pyarrow_cython_example.cpp:2846:41: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_table defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::Table>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_table)(PyObject *); /*proto*/
                                         ^
pyarrow_cython_example.cpp:2847:42: warning: __pyx_f_7pyarrow_3lib_pyarrow_unwrap_tensor defined but not used [-Wunused-variable]
 static std::shared_ptr< arrow::Tensor>  (*__pyx_f_7pyarrow_3lib_pyarrow_unwrap_tensor)(PyObject *); /*proto*/
                                          ^
/home/travis/build/apache/arrow/pyarrow-test-2.7/compiler_compat/ld: build/temp.linux-x86_64-2.7/pyarrow_cython_example.o: unable to initialize decompress status for section .debug_info
/home/travis/build/apache/arrow/pyarrow-test-2.7/compiler_compat/ld: build/temp.linux-x86_64-2.7/pyarrow_cython_example.o: unable to initialize decompress status for section .debug_info
/home/travis/build/apache/arrow/pyarrow-test-2.7/compiler_compat/ld: build/temp.linux-x86_64-2.7/pyarrow_cython_example.o: unable to initialize decompress status for section .debug_info
/home/travis/build/apache/arrow/pyarrow-test-2.7/compiler_compat/ld: build/temp.linux-x86_64-2.7/pyarrow_cython_example.o: unable to initialize decompress status for section .debug_info
build/temp.linux-x86_64-2.7/pyarrow_cython_example.o: file not recognized: file format not recognized
collect2: error: ld returned 1 exit status
error: command 'g++' failed with exit status 1

}}",ci-failure pull-request-available,"['C++', 'Continuous Integration', 'Python']",ARROW,Bug,Critical,2019-02-04 05:10:02,14
13213594,[Rust] [DataFusion] Add support for Parquet data sources,"As a user, I would like to be able to run SQL queries against Parquet files.

For the initial implementation we can just support primitive types.

",pull-request-available,"['Rust', 'Rust - DataFusion']",ARROW,Improvement,Major,2019-02-03 17:28:55,10
13213587,[Rust] Support read:write of Feather files,"As an Arrow developer/user, I'd like to be able to read and write Feather files.

The current I/O story in Rust isn't great, we don't yet fully support reading and writingbetween Parquet, we can only read CSV but not yet writing. This is an inconvenience (at least for me).

I propose supporting the Feather format in Rust, initially with the following limitations:
 * No date/time support until ARROW-4386(and potentially more work) lands
 * Reading categorical data (from other languages) but not writing them
 * Reading and writing from and to single record batches. We don't yet support slicing of arrays ARROW-3954

If the above are accept(ed|able), we can enhance the Feather support as the dependencies on the above limitations are lifted.

We can also refactor the Feather code as we work on more IPC in Rust.",pull-request-available,['Rust'],ARROW,New Feature,Major,2019-02-03 16:10:32,12
13213528,[C++] Expose bit-util methods for binary boolean operations that don't allocate,The only public APIs in bit-util.h force an allocation into a buffer.,pull-request-available,['C++'],ARROW,Sub-task,Major,2019-02-03 05:54:02,15
13213489,[Testing] Add git submodule for arrow-testing data files,We now have some test data files in arrow-testing that can be shared across Arrow implementations. We need to add this repo as a submodule in the main Arrow repo.,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-02-02 15:17:26,10
13213418,[Python] Cannot create Decimal128 array using integers,"There appears to have been a regression introduced in 0.11.0 such that we can no longer create a {{Decimal128}} array using integers.

To reproduce:
{code:python}
import pyarrow
column = pyarrow.decimal128(16, 4)
array = pyarrow.array([1], column)
{code}

Expected result: Behavior same as 0.10.0 and earlier; a {{Decimal128}} array would be created with no problems.

Actual result: an exception is thrown.

{code}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/array.pxi"", line 175, in pyarrow.lib.array
    return _sequence_to_array(obj, mask, size, type, pool, from_pandas)
  File ""pyarrow/array.pxi"", line 36, in pyarrow.lib._sequence_to_array
    check_status(ConvertPySequence(sequence, mask, options, &out))
  File ""pyarrow/error.pxi"", line 81, in pyarrow.lib.check_status
    raise ArrowInvalid(message)
ArrowInvalid: Could not convert 1 with type int: converting to Decimal128
Could not convert 1 with type int: converting to Decimal128
{code}

The crash doesn't occur if we use a {{decimal.Decimal}} object instead.",pull-request-available,['Python'],ARROW,Bug,Major,2019-02-01 22:30:52,2
13213311,[Rust] Convert File to T: Read + Seek for schema inference,"Arrow-4376allowed us to read csv from a record iterator. We still require a `File` when inferring schemas.

We propose changing from a File to something more generic. See discussion:https://github.com/apache/arrow/pull/3508#issuecomment-457986171",pull-request-available,['Rust'],ARROW,New Feature,Minor,2019-02-01 13:23:48,12
13213254,[JAVA][Flight] Flaky Flight java test,"Pull requests rebeased off of the laster master seem to pass so this is probably a flake:[https://travis-ci.org/apache/arrow/jobs/487275065]

{{[INFO] Running org.apache.arrow.flight.TestBackPressure
[ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 1.396 s <<< FAILURE! - in org.apache.arrow.flight.TestBackPressure
[ERROR] ensureIndependentSteams(org.apache.arrow.flight.TestBackPressure)  Time elapsed: 1.394 s  <<< ERROR!
java.lang.IllegalStateException: 
Memory was leaked by query. Memory leaked: (131072)
Allocator(perf-server) 0/131072/589824/9223372036854775807 (res/actual/peak/limit)
	at org.apache.arrow.flight.TestBackPressure.ensureIndependentSteams(TestBackPressure.java:76)
[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Errors: 
[ERROR]   TestBackPressure.ensureIndependentSteams:76  IllegalState Memory was leaked b...
[INFO] 
[ERROR] Tests run: 13, Failures: 0, Errors: 1, Skipped: 3
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Arrow Java Root POM 0.13.0-SNAPSHOT ......... SUCCESS [  8.590 s]
[INFO] Arrow Format ....................................... SUCCESS [  5.985 s]
[INFO] Arrow Memory ....................................... SUCCESS [ 12.750 s]
[INFO] Arrow Vectors ...................................... SUCCESS [01:13 min]
[INFO] Arrow Tools ........................................ SUCCESS [ 15.462 s]
[INFO] Arrow JDBC Adapter ................................. SUCCESS [ 11.906 s]
[INFO] Arrow Plasma Client ................................ SUCCESS [  3.967 s]
[INFO] Arrow Flight 0.13.0-SNAPSHOT ....................... FAILURE [ 18.237 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 02:30 min
[INFO] Finished at: 2019-02-01T05:15:34Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.20:test (default-test) on project arrow-flight: There are test failures.
[ERROR] 
[ERROR] Please refer to /home/travis/build/apache/arrow/java/flight/target/surefire-reports for the individual test results.
[ERROR] Please refer to dump files (if any exist) [date]-jvmRun[N].dump, [date].dumpstream and [date]-jvmRun[N].dumpstream.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :arrow-flight
}}




",ci-failure pull-request-available,"['FlightRPC', 'Java']",ARROW,Improvement,Major,2019-02-01 07:55:00,13
13213215,[Testing] Add DataFusion test files to arrow-testing repo,Adding DataFusion test CSV and Parquet files to arrow-testing so that all implementations can use them if desired,pull-request-available,['Rust - DataFusion'],ARROW,Improvement,Minor,2019-02-01 01:06:58,10
13212987,[Documentation] Clarify instructions for building documentation,[https://arrow.apache.org/docs/building.html#building-docs]seems to assume some prior setup. It is not entirely clear what that setup is. At the very least we should update it to bridge the gap from instructions for building from sourcehttps://arrow.apache.org/docs/python/development.html#building-the-documentation,pull-request-available,['Documentation'],ARROW,Improvement,Minor,2019-01-31 06:01:17,15
13212898,[Python] Cannot create empty StructArray via pa.StructArray.from_arrays,"{code:python}
In [5]: pa.StructArray.from_arrays([], names=[])
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-d6fa9cf39f31> in <module>
----> 1 pa.StructArray.from_arrays([], names=[])

~/Workspace/arrow/python/pyarrow/array.pxi in pyarrow.lib.StructArray.from_arrays()
   1326         num_arrays = len(arrays)
   1327         if num_arrays == 0:
-> 1328             raise ValueError(""arrays list is empty"")
   1329
   1330         length = len(arrays[0])

ValueError: arrays list is empty
{code}

however

{code:python}
pa.array([], type=pa.struct([]))
{code}

works",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-30 19:42:43,2
13212850,[C++] add unit test for currently unused append method,"TypedBufferBuilder<Arithmetic>::Append(num_copies, value) is currently not tested and is incorrect. Add a test and correct the method.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-01-30 16:06:43,6
13212787,[Python] Manylinux CI builds failing,"Example error build:https://api.travis-ci.org/v3/job/486336662/log.txt



{{+python -c 'import pyarrow; import tensorflow'}}
{{ Traceback (most recent call last):}}
{{ File ""<string>"", line 1, in <module>}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>}}
{{ from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 88, in <module>}}
{{ from tensorflow.python import keras}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py"", line 29, in <module>}}
{{ from tensorflow.python.keras import datasets}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/python/keras/datasets/__init__.py"", line 25, in <module>}}
{{ from tensorflow.python.keras.datasets import imdb}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/python/keras/datasets/imdb.py"", line 25, in <module>}}
{{ from tensorflow.python.keras.preprocessing.sequence import _remove_long_seq}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/python/keras/preprocessing/__init__.py"", line 30, in <module>}}
{{ from tensorflow.python.keras.preprocessing import image}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/tensorflow/python/keras/preprocessing/image.py"", line 23, in <module>}}
{{ from keras_preprocessing import image}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/keras_preprocessing/image/__init__.py"", line 8, in <module>}}
{{ from .dataframe_iterator import DataFrameIterator}}
{{ File ""/home/travis/build/apache/arrow/pyarrow-test-3.6/lib/python3.6/site-packages/keras_preprocessing/image/dataframe_iterator.py"", line 11, in <module>}}
{{ from pandas.api.types import is_numeric_dtype}}
{{ ModuleNotFoundError: No module named 'pandas'}}",ci-failure pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Blocker,2019-01-30 10:53:14,8
13212779,[C++] Update version of vendored gtest to 1.8.1,"conda-forge builds already use 1.8.1



This is a little tricky because library files get renamed on windows with the incremental version bump (debug files become libgmockd.lib).",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-01-30 10:23:38,15
13212666,[Flight][C++] Handle large Flight data messages,"I believe the message payloads are currently limited to 4MB by default, see one developer's discussion here:

https://nanxiao.me/en/message-length-setting-in-grpc/

While it is a good idea to break large messages into smaller ones, we will need to address how to gracefully send larger payloads that may be provided by a user's server implementation. Either we can increase the limit or break up the record batches into smaller chunks in the Flight server base (or both, of course)",pull-request-available,"['C++', 'FlightRPC']",ARROW,Bug,Major,2019-01-29 22:43:17,0
13212659,[INTEGRATION] Make spark integration test pass and test against spark's master branch,As discussed in https://github.com/apache/arrow/pull/3300#discussion_r252026108,pull-request-available,['Integration'],ARROW,Bug,Major,2019-01-29 21:57:03,3
13212489,[C++] Doc build broken,"See https://travis-ci.org/apache/arrow/jobs/485716603#L4746

{code}
/home/travis/build/apache/arrow/cpp/src/arrow/compute/kernel.h:170: error: The following parameters of arrow::compute::UnaryKernel::Call(FunctionContext *ctx, const Datum &input, Datum *out)=0 are not documented:
  parameter 'ctx'
  parameter 'input' (warning treated as error, aborting now)
{code}
",ci-failure pull-request-available,"['C++', 'Continuous Integration', 'Documentation']",ARROW,Bug,Major,2019-01-29 11:00:17,2
13212484,[Doc] Add docker-compose to easily run apache/arrow-site,Eventually all docker related code under https://github.com/apache/arrow/tree/master/dev should be moved to the new docker-compose setup defined in the top-level docker-compose.yml,docker,['Documentation'],ARROW,Bug,Major,2019-01-29 10:45:10,3
13212483,[C++] Stop using cmake COMMAND_EXPAND_LISTS because it breaks package builds for older distros,"COMMAND_EXPAND_LISTS option of add_custom_command is too new on Ubuntu Xenial and Debian stretch. It's available since CMake 3.8: https://cmake.org/cmake/help/v3.8/command/add_custom_command.html
We need to stop using it in cpp/src/gandiva/precompiled/CMakeLists.txt

Also We should pin cmake to version 3.5 in travis builds (xenial ships cmake 3.5)",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-29 10:41:26,3
13212445,[Python] pyarrow.hdfs.connect() failing,"Trying to connect to hdfs using the below snippet. Using {{hadoop-libhdfs}}.
This error appears in {{v0.12.0}}. It doesn't appear in {{v0.11.1}}. (I used the same environment when testing that it still worked on {{v0.11.1}})


{code:java}
In [1]: import pyarrow as pa

In [2]: fs = pa.hdfs.connect()

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-e0007ad7fa95> in <module>()
----> 1 fs = pa.hdfs.connect()

/usr/local/lib64/python2.7/site-packages/pyarrow/hdfs.pyc in connect(host, port, user, kerb_ticket, driver, extra_conf)
    205     fs = HadoopFileSystem(host=host, port=port, user=user,
    206                           kerb_ticket=kerb_ticket, driver=driver,
--> 207                           extra_conf=extra_conf)
    208     return fs

/usr/local/lib64/python2.7/site-packages/pyarrow/hdfs.pyc in __init__(self, host, port, user, kerb_ticket, driver, extra_conf)
     36             _maybe_set_hadoop_classpath()
     37 
---> 38         self._connect(host, port, user, kerb_ticket, driver, extra_conf)
     39 
     40     def __reduce__(self):

/usr/local/lib64/python2.7/site-packages/pyarrow/io-hdfs.pxi in pyarrow.lib.HadoopFileSystem._connect()
     72         if host is not None:
     73             conf.host = tobytes(host)
---> 74         self.host = host
     75 
     76         conf.port = port

TypeError: Expected unicode, got str
{code}",pull-request-available,['Python'],ARROW,Bug,Blocker,2019-01-29 09:03:54,2
13212417,[DOCUMENTATION] Add explicit version numbers to the arrow specification documents.,"Based on conversation on the mailing list it might pay to include version/revision numbers on the specification document. One way is to include the ""release"" version, another might be to only update versioning on changes to the document.",pull-request-available,['Documentation'],ARROW,Improvement,Minor,2019-01-29 06:57:20,3
13212399,[C++] Add missing parameter documentation to UnaryKernel to fix build,Also missing was documentation in util-internal.h,pull-request-available,['C++'],ARROW,Bug,Blocker,2019-01-29 05:34:33,15
13212384,[C++] Fix InvertKernel edge cases,"As noted in CR for https://github.com/apache/arrow/pull/3287, there are some cases where this kernel will probably fail:

* validity bitmap not allocated
* length 0 input with null data buffer",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-29 03:25:02,15
13212332,[CPP/Doc] Remove outdated Parquet documentation,Instructions documented in https://github.com/apache/arrow/blob/master/cpp/doc/Parquet.md are not valid anymore.,pull-request-available,['C++'],ARROW,Task,Trivial,2019-01-28 21:29:13,3
13212296,[C++] ExternalProject_Add does not capture CC/CXX correctly,"The issue is that CC/CXX environment variables are captured on the first invocation of the builder (e.g make or ninja) instead of when CMake is invoked into to build directory. This can lead to compilation errors (notably when compiling with clang in the top directory due to the addition of the `-Qunused-arguments` option).

This leads to an issue where I have a script that prepare the build directory and export CXX within the script. When I jump in the build folder, there's a mismatch between the external gbenchmark (and all deps if conda is not used) compiler and the build.

To reproduce:
# Create a new build directory with clang as compiler, don't build yet
# In a new shell (without the compiler environment variable), go into directory invoke make/ninja",pull-request-available,['C++'],ARROW,Bug,Minor,2019-01-28 18:43:02,13
13212192,[Python] Alpine dockerfile fails to build because pandas requires numpy as build dependency,See failed crossbow task: https://travis-ci.org/kszucs/crossbow/builds/484990267,pull-request-available,['Python'],ARROW,Improvement,Major,2019-01-28 10:10:30,3
13212139,"[C++] Remove usage of ""extern template class"" from NumericArray<T>","We aren't using this to any benefit right now (only the simple constructor is being instantiated once, everything else in inline), and the rules about template visibility and linkage vary a lot between compilers, as evidenced by patches such as https://github.com/apache/arrow/pull/3503

In general I think we should try not to use ""extern template class"" at all in the codebase for the problems that come with it",pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-28 04:24:34,14
13212137,[Python] Add benchmarks for Arrow<>Parquet BYTE_ARRAY serialization (read and write),"This is follow-on work to PARQUET-1508, so we can monitor the performance of this operation over time",parquet pull-request-available,['Python'],ARROW,Improvement,Major,2019-01-28 03:25:08,14
13212107,"[R] Serialize ""labeled"" metadata in Feather files, IPC messages",see https://github.com/apache/arrow/issues/3480,pull-request-available,['R'],ARROW,Improvement,Major,2019-01-27 21:27:31,4
13212104,[R]Installing clang-tools in CI is failing on trusty,"CI fails with
{code:java}
W: The repository 'http://llvm.org/apt/trusty llvm-toolchain-trusty-6.0 Release' does not have a Release file.
W: http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/trusty/Release.gpg: Signature by key 15866BAFD9BCC4F3C1E0DFC7D69548E1C17EAB57 uses weak digest algorithm (SHA1)
W: http://ppa.launchpad.net/kirillshkrogalev/ffmpeg-next/ubuntu/dists/trusty/Release.gpg: Signature by key 1EC6DBC9AA41BD34B32CC5A15C50E96D8EFE5982 uses weak digest algorithm (SHA1)
W: GPG error: https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6B05F25D762E3157
W: The repository 'https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease' is not signed.
W: There is no public key available for the following key IDs:
6B05F25D762E3157  
E: Failed to fetch https://llvm.org/apt/trusty/dists/llvm-toolchain-trusty-6.0/main/binary-amd64/Packages  Protocol http not supported or disabled in libcurl
E: Failed to fetch https://llvm.org/apt/trusty/dists/llvm-toolchain-trusty-6.0/main/binary-i386/Packages  Protocol http not supported or disabled in libcurl
E: Some index files failed to download. They have been ignored, or old ones used instead.
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package clang-6.0
E: Couldn't find any package by glob 'clang-6.0'
E: Couldn't find any package by regex 'clang-6.0'
E: Unable to locate package clang-format-6.0
E: Couldn't find any package by glob 'clang-format-6.0'
E: Couldn't find any package by regex 'clang-format-6.0'
E: Unable to locate package clang-tidy-6.0
E: Couldn't find any package by glob 'clang-tidy-6.0'
E: Couldn't find any package by regex 'clang-tidy-6.0'
The command ""$TRAVIS_BUILD_DIR/ci/travis_install_clang_tools.sh"" failed and exited with 100 during .{code}
Seems like clang-tools are not used in the build, will remove them.",pull-request-available,"['Continuous Integration', 'R']",ARROW,Bug,Major,2019-01-27 20:01:43,8
13212093,[Rust] Implement Date and Time Arrays,"We haveadded date/time types, but have not yet created arrays for these types. See discussion:https://github.com/apache/arrow/pull/3340#issuecomment-452226570",pull-request-available,['Rust'],ARROW,New Feature,Major,2019-01-27 17:23:45,12
13212088,[Python]default_version of a release should not include SNAPSHOT,"The 0.12 release had {{default_version = '0.12.0-SNAPSHOT'}}in the source thus triggering https://github.com/conda-forge/pyarrow-feedstock/issues/69. Instead, we should remove the SNAPSHOT suffix in our release scripts.",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-01-27 16:28:34,1
13212030,[Docker] docker-compose build lint fails,"{code}
$ docker-compose build lint
Building lint
Step 1/4 : FROM arrow:python-3.6
ERROR: Service 'lint' failed to build: pull access denied for arrow, repository does not exist or may require 'docker login'
{code}",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-01-26 19:39:43,14
13211855,[CI] Sphinx dependencies were removed from docs conda environment,"Causes nightly test error: https://travis-ci.org/kszucs/crossbow/builds/484315659

Removed in: [https://github.com/apache/arrow/commit/dc062656d7da7593764cb35fd0c78ba19856a4f3#diff-f257444d60768be1ea6728173a574f36]



",pull-request-available,"['Continuous Integration', 'Documentation']",ARROW,Improvement,Major,2019-01-25 14:38:59,3
13211851,[C++] DictionaryBuilder does not correctly report length and null_count,"In comparison to most builders, they stay constantly at 0.",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-25 14:26:35,8
13211850,[Packaging] Travis fails to deploy conda packages on OSX,See build https://travis-ci.org/kszucs/crossbow/builds/484284006,pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-01-25 14:19:08,3
13211809,[C++] Embed precompiled bitcode in the gandiva library,"We were not running the pyarrow tests after installing the manylinux wheels, which can lead to uncaught issues, like: [https://travis-ci.org/kszucs/crossbow/builds/484284104]

",pull-request-available,['C++ - Gandiva'],ARROW,Improvement,Major,2019-01-25 11:56:25,3
13211801,[Python] Table to pandas conversion fails for list of bool,"When converting a Table with a list of bool to pandas it doesn't work and throws an error. For list of int is working correctly.
{code:java}
 In [94]: x1 = [1,2,3,4]
 In [95]: f1 = pa.field('test', pa.list_(pa.int32()))
 In [96]: array1 = pa.array([x1], type=f1.type)
 In [97]: pa.Table.from_arrays([array1], names=['col1']).to_pandas()
 Out[97]: 
 col1
 0 [1, 2, 3, 4]
 In [98]: x2 = [True, True, False, False]
 In [99]: f2 = pa.field('test', pa.list_(pa.bool_()))
 In [100]: array2 = pa.array([x2], type=f2.type)
 In [101]: pa.Table.from_arrays([array2], names=['col1']).to_pandas()
 ---------------------------------------------------------------------------
 ArrowNotImplementedError Traceback (most recent call last)
 <ipython-input-101-04ab2e0d4c8f> in <module>()
 ----> 1 pa.Table.from_arrays([array2], names=['col1']).to_pandas()
 ~/.virtualenvs/py3-gpu/lib/python3.5/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()
 ~/.virtualenvs/py3-gpu/lib/python3.5/site-packages/pyarrow/table.pxi in pyarrow.lib.Table._to_pandas()
 ~/.virtualenvs/py3-gpu/lib/python3.5/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, categories, ignore_metadata)
 630 
 631 blocks = _table_to_blocks(options, block_table, pa.default_memory_pool(),
 --> 632 categories)
 633 
 634 # Construct the row index
 ~/.virtualenvs/py3-gpu/lib/python3.5/site-packages/pyarrow/pandas_compat.py in _table_to_blocks(options, block_table, memory_pool, categories)
 803 # Convert an arrow table to Block from the internal pandas API
 804 result = pa.lib.table_to_blocks(options, block_table, memory_pool,
 --> 805 categories)
 806 
 807 # Defined above
 ~/.virtualenvs/py3-gpu/lib/python3.5/site-packages/pyarrow/table.pxi in pyarrow.lib.table_to_blocks()
 ~/.virtualenvs/py3-gpu/lib/python3.5/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()
 ArrowNotImplementedError: Not implemented type for list in DataFrameBlock: bool
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-25 11:40:03,8
13211768,Bintray repository signature verification fails,"Installation of the packages using the Bintray repository (in a Ubuntu xenial container) as instructed [here|https://github.com/apache/arrow/blob/master/site/install.md#c-and-glib-c-packages-for-debian-gnulinux-ubuntu-and-centos] fails for me:
{code:java}
W: GPG error: https://dl.bintray.com/apache/arrow/ubuntu xenial Release: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 25BCCA5220D84079 E: The repository 'https://dl.bintray.com/apache/arrow/ubuntu xenial Release' is not signed. {code}
Workaround: https://github.com/apache/arrow/issues/3476#issuecomment-457469820",pull-request-available,['Packaging'],ARROW,Bug,Minor,2019-01-25 09:20:39,1
13211761,[C++] StringDictionaryBuilder segfaults on Finish with only null entries,"Sadly a regression from 0.11, detected during the turbodbc integration.",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-25 08:50:41,8
13211555,[C++] Add CMake format checks,"We should try to standardize the formatting of our CMake files somehow.

The [cmake-format utility|https://github.com/cheshirekow/cmake_format] could help.
",pull-request-available,"['C++', 'Continuous Integration', 'Developer Tools']",ARROW,Improvement,Major,2019-01-24 13:50:40,2
13211542,[C++] Query homebrew for Thrift,Also search for LLVM with homebrew when on OSX and THRIFT_HOME is not set.,pull-request-available,['C++'],ARROW,Bug,Major,2019-01-24 12:52:47,8
13211498,[CI] Add integration (docker) test for turbodbc,We regularly break our API so that {{turbodbc}} needs to make minor changes to support the new Arrow version. We should setup a small integration test to check before a release that {{turbodbc}} can easily upgrade.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2019-01-24 09:27:16,8
13211494,[C++]test-util functions are no longer part of libarrow,I have used these functions in other artifacts like {{turbodbc}}. I would like to have them back as part of libarrow. ,pull-request-available,['C++'],ARROW,Bug,Major,2019-01-24 09:18:17,8
13211446,[C++] Fail to build with static parquet,"Trying to build a static version of arrow with parquet fails as below. Is this configuration possible?



{{ -DARROW_BUILD_STATIC=ON}}
{{ -DARROW_BUILD_TESTS=OFF}}
{{ -DARROW_PYTHON=OFF}}
{{ -DARROW_BOOST_USE_SHARED=OFF}}
{{ -DARROW_WITH_SNAPPY=OFF}}
{{ -DARROW_WITH_ZSTD=OFF}}
{{ -DARROW_WITH_LZ4=OFF}}
{{ -DARROW_JEMALLOC=OFF}}
{{ -DARROW_BUILD_SHARED=OFF}}
{{ -DARROW_BOOST_VENDORED=OFF}}
{{ -DARROW_WITH_ZLIB=OFF}}
{{ -DARROW_WITH_BROTLI=OFF}}
{{ -DARROW_USE_GLOG=OFF}}
{{ -DPTHREAD_LIBRARY=OFF}}
{{ -DARROW_BUILD_UTILITIES=ON}}
{{ -DARROW_TEST_LINKAGE=""static""}}
{{ -DARROW_HDFS=OFF}}
{{ -DARROW_PARQUET=ON}}



...



{{==> cmake . -DCMAKE_C_FLAGS_RELEASE=-DNDEBUG -DCMAKE_CXX_FLAGS_RELEASE=-DNDEBUG}}
{{Last 15 lines from /Users/jeroen/Library/Logs/Homebrew/apache-arrow/01.cmake:}}
{{-- CMAKE_CXX_FLAGS: -Qunused-arguments -O3 -DNDEBUG -Wall -Wno-unknown-warning-option -msse4.2 -maltivec -march=armv8-a+crc -stdlib=libc++}}
{{-- Looking for backtrace}}
{{-- Looking for backtrace - found}}
{{-- backtrace facility detected in default set of libraries}}
{{-- Found Backtrace: /usr/include}}
{{-- Configuring done}}
{{CMake Error at cmake_modules/BuildUtils.cmake:143 (add_dependencies):}}
{{ The dependency target ""arrow_shared"" of target ""parquet_objlib"" does not}}
{{ exist.}}
{{Call Stack (most recent call first):}}
{{ src/parquet/CMakeLists.txt:214 (ADD_ARROW_LIB)}}
{{-- Generating done}}
{{-- Build files have been written to: /tmp/apache-arrow-20190123-44858-1as3l4q/apache-arrow-0.12.0/cpp}}",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-24 03:41:55,14
13211444,[Python] dtype=object arrays cannot be converted to a list-of-list ListArray,"Nested numpy arrays (as the scalar value) cannot be converted to a list-of-list type array:
{code}
arr = np.empty(2, dtype=object)
arr[:] = [np.array([1, 2]), np.array([2, 3])]

pa.array([arr, arr])
{code}
results in
{code:java}
ArrowTypeError: only size-1 arrays can be converted to Python scalars
{code}
Starting from lists of lists works fine:
{code}
lists = [[1, 2], [2, 3]]
pa.array([lists, lists]).type
{code}
{code:none}
ListType(list<item: list<item: int64>>)
{code}
Specifying the type explicitly as {{pa.array([arr, arr], type=pa.list_(pa.list_(pa.int64())))}} does not help.

Due to this, a round-trip is not working, as the list of list type gives back an array of arrays in python:
{code}
In [2]: lists = [[1, 2], [2, 3]] 
   ...: a = pa.array([lists, lists])                                                                                                                                                                                

In [3]: a.to_pandas()                                                                                                                                                                                               
Out[3]: 
array([array([array([1, 2]), array([2, 3])], dtype=object),
       array([array([1, 2]), array([2, 3])], dtype=object)], dtype=object)

In [4]: pa.array(a.to_pandas())                                                                                                                                                                                     
---------------------------------------------------------------------------
ArrowTypeError                            Traceback (most recent call last)
<ipython-input-4-9fee6dc9d0b8> in <module>
----> 1 pa.array(a.to_pandas())

~/scipy/repos/arrow/python/pyarrow/array.pxi in pyarrow.lib.array()

~/scipy/repos/arrow/python/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()

~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowTypeError: only size-1 arrays can be converted to Python scalars
{code}
----
Origingal report:
{code:java}
In [19]: df = pd.DataFrame({'a': [[[1], [2]], [[2], [3]]], 'b': [1, 2]})

In [20]: df.iloc[0].to_dict()
Out[20]: {'a': [[1], [2]], 'b': 1}

In [21]: pa.Table.from_pandas(df).to_pandas().iloc[0].to_dict()
Out[21]: {'a': array([array([1]), array([2])], dtype=object), 'b': 1}

In [24]: np.array(df.iloc[0].to_dict()['a']).shape
Out[24]: (2, 1)

In [25]: pa.Table.from_pandas(df).to_pandas().iloc[0].to_dict()['a'].shape
Out[25]: (2,)
{code}
Adding extra array type is not functioning as expected.



More importantly, this would fail


{code:java}
In [108]: df = pd.DataFrame({'a': [[[1, 2],[2, 3]], [[1,2], [2, 3]]], 'b': [[1, 2],[2, 3]]})

In [109]: df
Out[109]:
a b
0 [[1, 2], [2, 3]] [1, 2]
1 [[1, 2], [2, 3]] [2, 3]

In [110]: pa.Table.from_pandas(pa.Table.from_pandas(df).to_pandas())
---------------------------------------------------------------------------
ArrowTypeError Traceback (most recent call last)
<ipython-input-110-4a09836f807e> in <module>()
----> 1 pa.Table.from_pandas(pa.Table.from_pandas(df).to_pandas())

/Users/pengyu/.pyenv/virtualenvs/starscream/2.7.11/lib/python2.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pandas()
1215 <pyarrow.lib.Table object at 0x7f05d1fb1b40>
1216 """"""
-> 1217 names, arrays, metadata = pdcompat.dataframe_to_arrays(
1218 df,
1219 schema=schema,

/Users/pengyu/.pyenv/virtualenvs/starscream/2.7.11/lib/python2.7/site-packages/pyarrow/pandas_compat.pyc in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)
379 arrays = [convert_column(c, t)
380 for c, t in zip(columns_to_convert,
--> 381 convert_types)]
382 else:
383 from concurrent import futures

/Users/pengyu/.pyenv/virtualenvs/starscream/2.7.11/lib/python2.7/site-packages/pyarrow/pandas_compat.pyc in convert_column(col, ty)
374 e.args += (""Conversion failed for column {0!s} with type {1!s}""
375 .format(col.name, col.dtype),)
--> 376 raise e
377
378 if nthreads == 1:

ArrowTypeError: ('only size-1 arrays can be converted to Python scalars', 'Conversion failed for column a with type object')

{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-24 03:37:26,14
13211441,[C++] Build all benchmarks on Windows without failing,"Some of the benchmarks fail to build on Windows. We don't build them in Appveyor so we might consider changing that, or testing them in a nightly build",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-24 03:13:58,14
13211413,[Python] Run Python Travis CI unit tests on Linux when Java codebase changed,"The Java library is also dependency of the Python tests, but the tests aren't triggered if there is a change to the {{java/}} subtree. This blind spot was introduced when the CI jobs were split apart

https://github.com/apache/arrow/blob/master/.travis.yml#L133",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-24 00:28:57,14
13211411,[C++] Fix compiler warnings with gcc 8.2.0,"I just set up a new machine on Ubuntu 18.10 so I'm getting a few papercuts

{code}
/usr/bin/ccache /usr/bin/c++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_JEMALLOC -DARROW_JEMALLOC_INCLUDE_DIR=/home/wesm/code/arrow/cpp/build/jemalloc_ep-prefix/src/jemalloc_ep/dist//include -DARROW_NO_DEPRECATED_API -DARROW_USE_GLOG -DARROW_USE_SIMD -DARROW_WITH_ZSTD -Isrc -I../src -isystem /home/wesm/cpp-toolchain/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem jemalloc_ep-prefix/src -isystem ../thirdparty/hadoop/include -isystem orc_ep-install/include -isystem /home/wesm/cpp-toolchain/include/thrift -Wno-noexcept-type  -fuse-ld=gold -ggdb -O0  -Wall -Wconversion -Wno-sign-conversion -Werror -msse4.2  -g -fPIE   -std=gnu++11 -MD -MT src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o -MF src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o.d -o src/parquet/CMakeFiles/parquet-encoding-benchmark.dir/encoding-benchmark.cc.o -c ../src/parquet/encoding-benchmark.cc
In file included from ../src/parquet/encoding-internal.h:33,
                 from ../src/parquet/encoding-benchmark.cc:20:
../src/parquet/encoding.h: In instantiation of int parquet::Decoder<DType>::DecodeSpaced(parquet::Decoder<DType>::T*, int, int, const uint8_t*, int64_t) [with DType = parquet::DataType<(parquet::Type::type)6>; parquet::Decoder<DType>::T = parquet::ByteArray; uint8_t = unsigned char; int64_t = long int]:
../src/parquet/encoding.h:110:15:   required from here
../src/parquet/encoding.h:120:11: error: void* memset(void*, int, size_t) clearing an object of non-trivial type parquet::Decoder<parquet::DataType<(parquet::Type::type)6> >::T {aka struct parquet::ByteArray}; use assignment or value-initialization instead [-Werror=class-memaccess]
     memset(buffer + values_read, 0, (num_values - values_read) * sizeof(T));
     ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ../src/parquet/schema.h:31,
                 from ../src/parquet/encoding.h:29,
                 from ../src/parquet/encoding-internal.h:33,
                 from ../src/parquet/encoding-benchmark.cc:20:
../src/parquet/types.h:155:8: note: parquet::Decoder<parquet::DataType<(parquet::Type::type)6> >::T {aka struct parquet::ByteArray} declared here
 struct ByteArray {
        ^~~~~~~~~
cc1plus: all warnings being treated as errors
{code}",pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-24 00:10:57,14
13211372,[C++] Add as complete as possible Ubuntu Trusty / 14.04 build to docker-compose setup,Until we formally stop supporting Trusty it would be useful to be able to verify in Docker that builds work there. I still have an Ubuntu 14.04 machine that I use (and I've been filing bugs that I find on it) but not sure for how much longer,pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-23 19:57:04,14
13211349,[C++] Use TypedBufferBuilder<bool> in BooleanBuilder,Follow up work to ARROW-4031,pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-23 18:40:34,6
13211345,[C++] Update IWYU version in the `lint` dockerfile,"I was trying to cleanup the c++ imports based on the current docker-iwyu suggestions, but it requires to be customized (symbol maps and pragmas) more than it is currently. It'd also help a lot to use the latest IWYU version (see the changelog https://include-what-you-use.org/)",pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-23 18:25:51,14
13211315,"[C++] rewrite cpp/README shorter, with a separate contribution guide","The README.md for the cpp project has grown long and contains a lot of information specific to contributors. Move this information into a separate CONTRIBUTING.md

In particular, draw more attention to the cmake option `-DBUILD_WARNING_LEVEL=CHECKIN` which is used in Travis and includes `-Werror`",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-01-23 15:55:26,14
13211300,[C++] Default BUILD_WARNING_LEVEL to CHECKIN,"Default to checkin instead of production, such that it breaks early for new developers.",build cmake pull-request-available,"['C++', 'Developer Tools']",ARROW,Improvement,Trivial,2019-01-23 15:24:17,13
13211168,[Website] Instructions and scripts for publishing web site appear to be incorrect,"I tried following the instructions for publishing the web site and failed. Issues:

1. The instructions say to clone the asf-site repo but doesn't say where to clone it to (under arrow repo root dir? under arrow/site dir? somewhere else?)

2. The instructions don't say which directory to run the bundle command from. It's probably obvious to some people but not me. I'm assuming it should be from arrow/site

3. The script ""script/sync_format_docs.sh"" seems to be looking in the wrong path and fails.

4. After manually copying the format docs, jekyll fails to serve the content because there is no IPC.md (there are others, just not IPC) .. looks like IPC is a .rst file not an MD?

I'll try and figure this out myself and create a PR to fix ....







",pull-request-available,['Website'],ARROW,Improvement,Major,2019-01-23 03:33:58,10
13211158,[C++] Use FindThreads.cmake to handle -pthread compiler/link options,We are using {{find_library}} to find pthread which may not work for non-system gcc toolchains. The preferred way it seems to handle this is with {{find_package(Threads REQUIRED)}}. I will submit a patch,pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-23 02:44:31,14
13211040,[Python] Array dtype inference incorrect when created from list of mixed numpy scalars,"Minimal reproducer:
{code:python}
import pyarrow as pa
import numpy as np

test_list = [np.dtype('int32').type(10), np.dtype('float32').type(0.5)]
test_array = pa.array(test_list)

# Expected
# test_array
# <pyarrow.lib.DoubleArray object at 0x7f009963bf48>
# [
#   10,
#   0.5
# ]

# Got
# test_array
# <pyarrow.lib.Int32Array object at 0x7f009963bf48>
# [
#   10,
#   0
# ]
{code}
",pull-request-available,['Python'],ARROW,Bug,Minor,2019-01-22 17:49:49,14
13210977,[Packaging] Fix failing OSX clang conda forge builds,Log: https://travis-ci.org/kszucs/crossbow/builds/482871537,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2019-01-22 13:44:52,3
13210859,[Website] Home page of https://arrow.apache.org/ does not mention Go or Rust,Home page of https://arrow.apache.org/ does not mention Go or Rust,pull-request-available,['Website'],ARROW,Improvement,Minor,2019-01-22 00:32:15,10
13210810,"[C++] Lint doesn't work anymore (""[Errno 24] Too many open files"")","{code}
-- Building using CMake version: 3.12.2
-- Arrow version: 0.13.0 (full: '0.13.0-SNAPSHOT')
-- clang-tidy not found
-- clang-format found at /usr/bin/clang-format-6.0
-- infer found at /home/antoine/infer-linux64-v0.15.0/bin/infer
-- Using ccache: /usr/bin/ccache
-- Found cpplint executable at /home/antoine/arrow/cpp/build-support/cpplint.py
-- Configuring done
-- Generating done
-- Build files have been written to: /home/antoine/arrow/cpp/build-lint
Built target lint
Traceback (most recent call last):
  File ""/home/antoine/arrow/cpp/build-support/run_clang_format.py"", line 105, in <module>
    ], stdout=PIPE, stderr=PIPE)
  File ""/home/antoine/arrow/cpp/build-support/lintutils.py"", line 59, in run_parallel
    procs.append(Popen(cmd, **kwargs))
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/subprocess.py"", line 775, in __init__
    restore_signals, start_new_session)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.7/subprocess.py"", line 1412, in _execute_child
    errpipe_read, errpipe_write = os.pipe()
OSError: [Errno 24] Too many open files
CMakeFiles/check-format.dir/build.make:57: recipe for target 'CMakeFiles/check-format' failed
make[3]: *** [CMakeFiles/check-format] Error 1
CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/check-format.dir/all' failed
make[2]: *** [CMakeFiles/check-format.dir/all] Error 2
CMakeFiles/Makefile2:79: recipe for target 'CMakeFiles/check-format.dir/rule' failed
make[1]: *** [CMakeFiles/check-format.dir/rule] Error 2
Makefile:118: recipe for target 'check-format' failed
make: *** [check-format] Error 2
~/arrow/cpp
{code}
",pull-request-available,"['C++', 'Developer Tools']",ARROW,Bug,Major,2019-01-21 17:03:14,6
13210649,[Documentation] Add a docker-compose entry which builds the documentation with CUDA enabled,"This needs to be updated to build with CUDA support (which in turn will require the host machine to have nvidia-docker), among other things",docker pull-request-available,"['Developer Tools', 'Documentation']",ARROW,Bug,Major,2019-01-21 00:26:56,3
13210646,"[C++] FIx doxygen warnings, include doxygen warning checks in CI linting","Doxygen has a bunch of warnings in 0.12 / master

https://gist.github.com/wesm/da6e7ba8cb6aa42875c4f3b98c6a9381

We should endeavor to maintain a clean Doxygen build",pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-20 23:53:58,2
13210633,[Release] Update website and add blog post announcing 0.12.0 release,Working on this,pull-request-available,['Website'],ARROW,New Feature,Major,2019-01-20 21:26:57,14
13210603,[Gandiva/Python] Build LLVM with RTTI in manylinux1 container,We build with RTTI in Arrow and thus need the typeinfo symbols.,pull-request-available,"['C++ - Gandiva', 'Packaging', 'Python']",ARROW,Task,Major,2019-01-20 13:15:42,8
13210412,[Dev] Support selecting features in release scripts,Sometimes not all components can be verified on a system. We should provide some environment variables to exclude them to proceed to the next step.,pull-request-available,"['Developer Tools', 'Packaging']",ARROW,New Feature,Major,2019-01-18 15:32:11,8
13210409,[C++/Gandiva]Support detecting correct LLVM version in Homebrew,We should also search in homebrew for the matching LLVM version for Gandiva on OSX. You can install it via {{brew install llvm@6}}.,pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,New Feature,Major,2019-01-18 15:27:21,8
13210400,[C++] Forward AR and RANLIB to thirdparty builds,"On OSX Mojave, it seems that there are many version of AR present. CMake seems to detect the right one whereas some thirdparty tooling picks up the wrong one.",pull-request-available,['C++'],ARROW,New Feature,Major,2019-01-18 14:55:18,8
13210362,[C++] Ensure minimal bison version on OSX for Thrift,"Thrift currently just uses the first bison it finds but needs actually a newer one. We should look for the minimal version required and fall back explicitly to homebrew and use the newer version if it is available there.

Note: I'll add a fix in our CMake toolchain but will also try to upstream this to Thrift.",pull-request-available,['C++'],ARROW,New Feature,Major,2019-01-18 11:49:53,8
13210149,[CI] Use Ubuntu Xenial (16.04) VMs on Travis-CI,"We're currently using the Trusty VMs on Travis-CI. Besides the fact that Ubuntu Trusty is soon 5 years old, it forces us to use the old pre-C++ ABI migration conda-forge packages. We should migrate all or most jobs to Xenial.",pull-request-available,['Continuous Integration'],ARROW,Task,Major,2019-01-17 12:57:16,2
13210073,[C++][Documentation] It looks like flex and bison are required for parquet,"When trying to build parquet, it initially failed because it couldn't find flex and bison.",pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Trivial,2019-01-17 07:41:38,15
13210057,[C++] Add gmock to toolchain,"Add gmock to the toolchain.



It looks like before this can happen, a gmock feedstock on conda-forge has to be setup so our CI setup can work.",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-01-17 05:22:40,15
13209680,[C++] Add C primitive to Arrow:Type compile time in TypeTraits,"The user would use something like
{code:c++}
...
using ArrowType = CTypeTraits<int64>::ArrowType;
using ArrayType = CTypeTraits<int64>::ArrayType;

auto type = CTypeTraits<int64>::type_singleton();
{code}",pull-request-available,['C++'],ARROW,Improvement,Minor,2019-01-15 15:46:02,13
13209670,[Python/C++][Parquet] Segfault when reading rowgroups with duplicated columns,"When reading a row group using duplicated columns I receive a segfault.
{code:python}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
df = pd.DataFrame({
""col"": [""A"", ""B""]
})

table = pa.Table.from_pandas(df)
buf = pa.BufferOutputStream()

pq.write_table(table, buf)

parquet_file = pq.ParquetFile(buf.getvalue())

parquet_file.read_row_group(0)
parquet_file.read_row_group(0, columns=[""col""])

# boom
parquet_file.read_row_group(0, columns=[""col"", ""col""])

{code}",parquet pull-request-available,['C++'],ARROW,Bug,Minor,2019-01-15 15:26:15,8
13209666,[Python][CI] Disable ORC tests in dask integration test,"https://issues.apache.org/jira/browse/ARROW-3910 changed the default value of to_pandas: to_pandas(date_as_object=True) which breaks dask's ORC tests [https://github.com/dask/dask/blob/e48aca49af9005c938ff4773aa05ca8b20e2e1b1/dask/dataframe/io/orc.py#L19]



cc [~mrocklin]

",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Task,Major,2019-01-15 15:11:51,3
13209551,[C++] Document why DCHECKs are used in kernels,"DCHECKs seem to be used where Status::Invalidmight be considered more appropriate (so programs don't crash). See conversation on[https://github.com/apache/arrow/pull/3287/files]

based on conversation on this Jira and on the CL it seems DCHECKS are in fact desired but we should document appropriate use for them.",pull-request-available,['C++'],ARROW,Bug,Minor,2019-01-15 04:08:16,15
13209537,[Rust] Donate DataFusion,"I am starting work on a PR to donate the DataFusion source code.

If the vote is passed we will be ready to merge. If the vote is not passed we can close this issue and delete the PR.",pull-request-available,['Rust'],ARROW,Improvement,Major,2019-01-15 00:38:26,10
13209470,[Python] test_serialize_deserialize_pandas is failing in multiple build entries,"See
 [https://travis-ci.org/apache/arrow/jobs/479378190#L2427]
 ",ci-failure pull-request-available,['Python'],ARROW,Bug,Blocker,2019-01-14 17:44:47,3
13209386,[Python] Safe cast fails from numpy float64 array with nans to integer,"```

>>> pa.Array.from_pandas(pd.Series([1, None]), type=pa.int32(), safe=True)
 Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/array.pxi"", line 474, in pyarrow.lib.Array.from_pandas
  File ""pyarrow/array.pxi"", line 169, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 69, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 81, in pyarrow.lib.check_status
 pyarrow.lib.ArrowInvalid: Floating point value truncated

```",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-14 11:38:02,3
13209314,[Release] Update release verification script to check binaries on Bintray,The release verification script still expects the binaries to be on the ASF dist server,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-01-14 04:57:05,1
13209306,[Release] Update Windows verification script for 0.12 release,The Windows verification script needs a small fix because the toolchain has acquired new dependencies,pull-request-available,"['C++', 'Developer Tools', 'Python']",ARROW,Bug,Major,2019-01-14 02:24:38,14
13209300,[C++] Gandiva tests fail to compile with Boost in Ubuntu 14.04 apt,"These tests use an API that was not available in the Boost in Ubuntu 14.04; we can change them to use the more compatible API

{code}
/tmp/arrow-0.12.0.BFPHN/apache-arrow-0.12.0/cpp/src/gandiva/lru_cache_test.cc: In member function virtual void gandiva::TestLruCache_TestLruBehavior_Test::TestBody():
/tmp/arrow-0.12.0.BFPHN/apache-arrow-0.12.0/cpp/src/gandiva/lru_cache_test.cc:62:188: error: class boost::optional<std::basic_string<char> > has no member named value
   ASSERT_EQ(cache_.get(TestCacheKey(1)).value(), ""hello"");
                                                                                                                                                                                            ^
/tmp/arrow-0.12.0.BFPHN/apache-arrow-0.12.0/cpp/src/gandiva/lru_cache_test.cc:62:203: error: template argument 1 is invalid
   ASSERT_EQ(cache_.get(TestCacheKey(1)).value(), ""hello"");
                                                                                                                                                                                                           ^
/tmp/arrow-0.12.0.BFPHN/apache-arrow-0.12.0/cpp/src/gandiva/lru_cache_test.cc:62:294: error: class boost::optional<std::basic_string<char> > has no member named value
   ASSERT_EQ(cache_.get(TestCacheKey(1)).value(), ""hello"");
                                                                                                                                                                                                                                                                                                      ^
make[2]: *** [src/gandiva/CMakeFiles/gandiva-lru_cache_test.dir/lru_cache_test.cc.o] Error 1
make[1]: *** [src/gandiva/CMakeFiles/gandiva-lru_cache_test.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-14 00:26:47,14
13209298,[C++] Status error context strings missing lines of code,"The actual code, along with the line breaks, are missing:

{code}
ArrowInvalid: ../src/arrow/python/numpy_to_arrow.cc:803 : ../src/arrow/python/numpy_to_arrow.cc:285 : ../src/arrow/python/numpy_to_arrow.cc:447 : ../src/arrow/python/numpy_to_arrow.cc:349 : ../src/arrow/compute/kernels/cast.cc:1436 : ../src/arrow/compute/kernels/cast.cc:1426 : ../src/arrow/compute/kernels/util-internal.cc:40 : Floating point value truncated
{code}

I am not certain but I would guess this got broken during the Status usability refactoring

cc [~fsaintjacques]",pull-request-available,['C++'],ARROW,Bug,Major,2019-01-13 23:26:56,13
13209297,[C++] Add option to use vendored Boost in verify-release-candidate.sh,"Got burned by this while verifying 0.12 RC2. It is useful to have the option to use the system Boost during verification, but it would also be nice to opt in to using the vendored Boost build",pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-13 22:30:47,14
13209272,[C++][Gandiva] Use approximate comparisons for floating point numbers in gandiva-projector-test,"I experienced a failure due to floating point comparison when running the release verification script for 0.12.0 RC2. 

{code}
[==========] Running 13 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 13 tests from TestProjector
[ RUN      ] TestProjector.TestProjectCache
[       OK ] TestProjector.TestProjectCache (584 ms)
[ RUN      ] TestProjector.TestProjectCacheFieldNames
[       OK ] TestProjector.TestProjectCacheFieldNames (319 ms)
[ RUN      ] TestProjector.TestProjectCacheDouble
[       OK ] TestProjector.TestProjectCacheDouble (304 ms)
[ RUN      ] TestProjector.TestProjectCacheFloat
[       OK ] TestProjector.TestProjectCacheFloat (305 ms)
[ RUN      ] TestProjector.TestIntSumSub
[       OK ] TestProjector.TestIntSumSub (200 ms)
[ RUN      ] TestProjector.TestAllIntTypes
[       OK ] TestProjector.TestAllIntTypes (1945 ms)
[ RUN      ] TestProjector.TestExtendedMath
/tmp/arrow-0.12.0.a2ADf/apache-arrow-0.12.0/cpp/src/gandiva/tests/projector_test.cc:358: Failure
Value of: (expected_cbrt)->Equals(outputs.at(0))
  Actual: false
Expected: true
expected array: [
  2.51984,
  2.15443,
  -2.41014,
  2.02469
] actual array: [
  2.51984,
  2.15443,
  -2.41014,
  2.02469
]
{code}",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Bug,Major,2019-01-13 20:57:49,6
13209221,[Plasma][Python] PlasmaClient.list doesn't work with CUDA enabled Plasma,"{{plasma::ObjectTableEntry}} layout is changed with CUDA enabled build: https://github.com/apache/arrow/blob/master/cpp/src/plasma/common.h#L97-L100
But it's not cared in Cython code: https://github.com/apache/arrow/blob/master/python/pyarrow/_plasma.pyx#L77-L78",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-13 02:38:54,1
13209156,Clarify language around padding/alignment,"The recommendation on padding an alignment reads a little bit strangely, I'm going to take a stab at rewording is slightly.",pull-request-available,['Documentation'],ARROW,Bug,Minor,2019-01-12 07:41:24,15
13209101,[Packaging] Disable crossbow conda OSX clang builds,They are expected to fail. See: https://github.com/apache/arrow/pull/3368,pull-request-available,['Packaging'],ARROW,Task,Major,2019-01-11 21:27:37,3
13209059,[Release] Updating .deb package names in the prepare script failed to run on OSX,"This code segment: [https://github.com/apache/arrow/blob/master/dev/release/00-prepare.sh#L143-L167]



It was complaining about missing files, so something with the pattern is wrong on OSX:

[https://github.com/apache/arrow/blob/master/dev/release/00-prepare.sh#L148]



Additionally the problem can be twofold, depending on the outcome of the RC0. If it passes the VOTE, than We need to run the previous update script, similarly like [~kou] did after 0.11.0 release:

https://github.com/apache/arrow/commit/222632c67464a2bdd4724161c5ae79abf72a8bf7#diff-2294062bc9761f012d9abea73fa86153



",pull-request-available,['Packaging'],ARROW,Task,Major,2019-01-11 17:57:13,1
13209058,[Packaging] Fix RC version conflict between crossbow and rake,"Linux package builds are detecting the release and expecting an RC postfixed version number during submit, like `0.12.0-rc1`. The rest of the crossbow tasks are expecting non-rc version numbers, so currently either one or the other works, but not both.",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-01-11 17:38:45,3
13209055,[Packaging] Fix CMAKE_INSTALL_LIBDIR in release verification script,"Set to

{{-DCMAKE_INSTALL_LIBDIR=lib}}

instead of

{{-DCMAKE_INSTALL_LIBDIR=$ARROW_HOME/lib}}",pull-request-available,['Packaging'],ARROW,Bug,Major,2019-01-11 17:33:33,3
13208897,[Packaging] Create a Dockerfile to build source archive,"[https://github.com/apache/arrow/pull/3375]
It's better that we maintain a Dockerfile than maintaining a shell script for Linux and macOS.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2019-01-11 01:10:12,1
13208833,[C++] Enable building flight against system gRPC,"Right now Flight assumes that gRPC is vendored or that it is installed with CMake. It would be easier to build if it accepted other installations of gRPC, such as ones from Conda (eventually) or system package managers.",flight pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2019-01-10 17:04:23,0
13208795,[Packaging] Set crossbow target explicitly to enable building arbitrary arrow repo,"Like:
{code:java}
python crossbow.py submit -r kszucs/arrow -v 0.12.0 conda-linux-py27

python crossbow.py submit -r apache/arrow -t apache-arrow-0.12.0 -v 0.12.0 -g wheel{code}",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2019-01-10 14:06:52,3
13208687,[Python] Add buffered input and output stream ASV benchmarks with simulated high latency IO,Follow up to ARROW-3126,benchmark pull-request-available,['Python'],ARROW,Improvement,Major,2019-01-10 04:38:01,14
13208643,[Python] Add CUDA API docs,"Since parsing the CUDA API docstrings would require building Arrow with CUDA enabled, it would be better to make it conditional, for example using the [ifconfig|https://www.sphinx-doc.org/en/master/usage/extensions/ifconfig.html] Sphinx extension.",pull-request-available,"['Documentation', 'GPU', 'Python']",ARROW,Improvement,Major,2019-01-09 22:16:11,2
13208633,[Flight] C++ and Java implementations are incompatible,"A C++ client cannot request streams from a Java service, nor can it decode the schema from GetFlightInfo.

Schema: in Java, GetFlightInfo encodes the schema directly via flatbuffers. C++ expects it to be encoded as an IPC message. This isn't a problem in Java as a method exists to decode such schemas, but in C++ the API for reading such a schema isn't really exposed. I'm willing to submit a patch for this, but it's not clear to me which scheme is preferred.

Streams: in Java, DoGet starts with an ArrowMessage containing a schema. C++ does not expect this and segfaults when it tries to decode the message as a record batch. Based on the presentations I've seen, I think C++ is in the wrong here; I have a patch to fix this that I could clean up and submit.",flight pull-request-available,['FlightRPC'],ARROW,Bug,Major,2019-01-09 21:02:18,0
13208555,[Python]Mention boost-cpp directly in the conda meta.yaml for pyarrow,"We seem to also link some boost symbols into pyarrow. This should guarantee through the new `run_exports` pinning mechanism that we only see installs of arrow-cpp and pyarrow with the same `boost-cpp` version.

conda-forge PR: https://github.com/conda-forge/pyarrow-feedstock/pull/63",pull-request-available,"['Packaging', 'Python']",ARROW,Task,Major,2019-01-09 14:31:11,3
13208358,[C++] conda_env_* files cannot be used to create a fresh conda environment on Windows,"See

{code}
 conda create -n arrow-dev python=3.7 --file=ci\conda_env_cpp.yml --file=ci\conda_env_python.yml -c conda-forge
Solving environment: failed                                                                                     
                                                                                                                
PackagesNotFoundError: The following packages are not available from current channels:                          
                                                                                                                
  - rsync                                                                                                       
  - nomkl                                                                                                       
                                                                                                                
Current channels:                                                                                               
                                                                                                                
  - https://conda.anaconda.org/conda-forge/win-64                                                               
  - https://conda.anaconda.org/conda-forge/noarch                                                               
  - https://repo.anaconda.com/pkgs/main/win-64                                                                  
  - https://repo.anaconda.com/pkgs/main/noarch                                                                  
  - https://repo.anaconda.com/pkgs/free/win-64                                                                  
  - https://repo.anaconda.com/pkgs/free/noarch                                                                  
  - https://repo.anaconda.com/pkgs/r/win-64                                                                     
  - https://repo.anaconda.com/pkgs/r/noarch                                                                     
  - https://repo.anaconda.com/pkgs/pro/win-64                                                                   
  - https://repo.anaconda.com/pkgs/pro/noarch                                                                   
  - https://repo.anaconda.com/pkgs/msys2/win-64                                                                 
  - https://repo.anaconda.com/pkgs/msys2/noarch                                                                 
                                                                                                                
To search for alternate channels that may provide the conda package you're                                      
looking for, navigate to                                                                                        
                                                                                                                
    https://anaconda.org                                                                                        
                                                                                                                
and use the search bar at the top of the page.                                                                  
{code}

The Linux/macOS-specific dependencies should be put in conda_env_unix.yml",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2019-01-08 16:53:33,14
13208324,[Format] Metadata.rst does not specify timezone for Timestamp type,"Schema.fbs states that Timestamp type consists of unit and timezone, but Metadata.rst just has unit. This seems like an error.",pull-request-available,['Format'],ARROW,Improvement,Major,2019-01-08 14:09:58,14
13208317,"""./dev/run_docker_compose.sh"" is out of date","The Parquet repo shouldn't be required anymore.

{code:bash}
$ ./dev/run_docker_compose.sh iwyu
Please clone the Parquet repo next to the Arrow repo
{code}

Also, there's another error when trying to run {{docker-compose}} directly:
{code:bash}
$ docker-compose -f arrow/dev/docker-compose.yml build iwyu
ERROR: build path /home/antoine/arrow/dev/dask_integration either does not exist, is not accessible, or is not a valid URL.
{code}",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2019-01-08 13:53:46,13
13208305,[C++] Use same CC and AR for jemalloc as for the main sources,jemalloc currently does its own CC and AR detection and sometimes comes to different conclusions. This let to linking errors for the static library for me.,pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-08 13:04:06,8
13208232,[CI] [Rust] Fix broken cargo coverage,"The Rust {{cargo coverage}} is failing with the following error:

{code}
warning: unused manifest key: package.edition
warning: unused manifest key: package.edition
...
error[E0432]: unresolved import `num`
  --> arrow/src/array_ops.rs:22:5
   |
22 | use num::Zero;
   |     ^^^ did you mean `std::num`?
error[E0432]: unresolved imports `self::csv_crate::StringRecord`, `self::csv_crate::StringRecordsIntoIter`
  --> arrow/src/csv/reader.rs:55:23
   |
55 | use self::csv_crate::{StringRecord, StringRecordsIntoIter};
   |                       ^^^^^^^^^^^^  ^^^^^^^^^^^^^^^^^^^^^ no `StringRecordsIntoIter` in `csv`
   |                       |
   |                       no `StringRecord` in `csv`
error[E0432]: unresolved import `serde_derive`
  --> arrow/src/datatypes.rs:29:5
   |
29 | use serde_derive::{Deserialize, Serialize};
   |     ^^^^^^^^^^^^ maybe a missing `extern crate serde_derive;`?
error[E0432]: unresolved import `serde_json`
  --> arrow/src/datatypes.rs:30:5
   |
30 | use serde_json::{json, Value};
   |     ^^^^^^^^^^ maybe a missing `extern crate serde_json;`?
error[E0432]: unresolved import `libc`
  --> arrow/src/memory.rs:18:5
   |
18 | use libc;
   |     ^^^^ no `libc` in the root
error[E0432]: unresolved import `rand`
  --> arrow/src/util/test_util.rs:18:5
   |
18 | use rand::{thread_rng, Rng};
   |     ^^^^ maybe a missing `extern crate rand;`?
...
{code}

perhaps because it doesn't recognize Rust 2018 edition. ",pull-request-available,['Rust'],ARROW,Test,Major,2019-01-08 07:44:44,9
13208194,[Rust] There should be a README in the top level rust directory,"Due to recent changes, the main README is now in the rust/arrow directory. We need to move the README up one level since it contains information about both the arrow and parquet crates.",pull-request-available,['Rust'],ARROW,Bug,Minor,2019-01-08 03:06:58,10
13208193,[C++] file-benchmark uses <poll.h>,"arrow/io/file-benchmark.cc includes poll.h, which causes the build to fail on Windows",pull-request-available,['C++'],ARROW,Bug,Minor,2019-01-08 02:52:30,2
13208188,[C++] BitmapWriters clobber the first byte when length=0,"When a BitmapWriter or FirstTimeBitmapWriter is constructed with length=0 then Finish() is invoked, the byte at {{start_offset / 8}} is zeroed

The following change causes the test to fail:

{code}
--- a/cpp/src/arrow/util/bit-util-test.cc
+++ b/cpp/src/arrow/util/bit-util-test.cc
@@ -274,6 +274,10 @@ TEST(FirstTimeBitmapWriter, NormalOperation) {
         auto writer = internal::FirstTimeBitmapWriter(bitmap, 10, 3);
         WriteVectorToWriter(writer, {0, 0, 0});
       }
+      {
+        auto writer = internal::FirstTimeBitmapWriter(bitmap, 13, 0);
+        WriteVectorToWriter(writer, {});
+      }
       {
         auto writer = internal::FirstTimeBitmapWriter(bitmap, 13, 3);
         WriteVectorToWriter(writer, {1, 0, 1});
{code}",pull-request-available,['C++'],ARROW,Bug,Minor,2019-01-08 02:40:26,2
13208187,[Rust] Appveyor builds are broken,Changes to Rust directory structure have broken appveyor on all branches - this is trivial to fix and I will push a PR shortly.,pull-request-available,['Rust'],ARROW,Bug,Major,2019-01-08 02:28:48,10
13208092,[Python] TestConvertStructTypes.test_from_numpy_large failing,"This is half of ARROW-4179 (this test should not be running in Travis CI at all). This failure appears to be a regression, and was not caught because we do not regularly run the large_memory tests

[~kszucs] now that we have these large memory DGX machines we should run the {{large_memory}} unit tests at least once a day",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-07 18:19:00,2
13208076,[Python] Tests crashing on all platforms in CI,Since there have been some conda-forge library updates I would guess this is toolchain-related,pull-request-available,['Python'],ARROW,Bug,Blocker,2019-01-07 16:39:20,14
13208060,[C++] Fix TSan and UBSan errors,clang's Thread Sanitizer and Undefined Behaviour Sanitizer report a number of alerts. We should strive to fix or workaround them.,pull-request-available,['C++'],ARROW,Bug,Major,2019-01-07 15:56:08,2
13208055,[C++] Add ThreadPool and TaskGroup microbenchmarks,Quantify CPU cycles associated with task lifecycle,pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-07 15:44:21,2
13207829,[Gandiva] Define logical query plans in protobuf,"I would like the ability to define logical query plans in the Gandiva proto definition.

",pull-request-available,['C++ - Gandiva'],ARROW,Improvement,Major,2019-01-05 14:04:54,10
13207736,[C++] Check for -Wdocumentation issues ,"I fixed some -Wdocumentation issues in ARROW-4157 that showed up on one Linux distribution but not another, both with clang-6.0. Not sure why that is exactly, but it would be good to try to reproduce and see if our CI can be improved to catch these, or in worst case we could do it in one of our docker-compose builds",pull-request-available,['C++'],ARROW,Improvement,Major,2019-01-04 19:38:20,14
13207730,[Dev] Allow maintainers to use a GitHub API token when merging pull requests,I rate limited today on unauthenticated requests for some reason -- the failure mode for dev/merge_arrow_pr.py was pretty bad. I'm making the output more helpful and adding an option to use an API token set via environment variable,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2019-01-04 18:39:28,14
13207718,[C++] -Wdocumentation failures with clang 6.0 on Ubuntu 18.04,I guess these aren't caught on Ubuntu 14.04. Patch incoming,pull-request-available,['C++'],ARROW,Bug,Major,2019-01-04 16:46:16,14
13207676,[C++] xcodebuild failure for cmake generated project,"Using the cmake xcode project generator fails to build using xcodebuild as follows:
{code:java}
$ cmake .. -G Xcode -DARROW_PARQUET=ON -DPARQUET_BUILD_EXECUTABLES=ON -DPARQUET_BUILD_EXAMPLES=ON -DFLATBUFFERS_HOME=/usr/local/Cellar/flatbuffers/1.10.0 -DCMAKE_BUILD_TYPE=Debug -DTHRIFT_HOME=/usr/local/Cellar/thrift/0.11.0 -DARROW_EXTRA_ERROR_CONTEXT=ON -DARROW_BUILD_TESTS=ON -DClangTools_PATH=/usr/local/Cellar/llvm@6/6.0.1_1

....

Libtool xcode-build/src/arrow/arrow.build/Debug/arrow_objlib.build/Objects-normal/libarrow_objlib.a normal x86_64
cd /Users/hhelal/Documents/code/arrow/cpp
export MACOSX_DEPLOYMENT_TARGET=10.14
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool -static -arch_only x86_64 -syslibroot /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk -L/Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_objlib.build/Objects-normal -filelist /Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_objlib.build/Objects-normal/x86_64/arrow_objlib.LinkFileList -o /Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_objlib.build/Objects-normal/libarrow_objlib.a

PhaseScriptExecution CMake\ PostBuild\ Rules xcode-build/src/arrow/arrow.build/Debug/arrow_objlib.build/Script-2604120B03B14AB58C2E586A.sh
cd /Users/hhelal/Documents/code/arrow/cpp
/bin/sh -c /Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_objlib.build/Script-2604120B03B14AB58C2E586A.sh
echo ""Depend check for xcode""
Depend check for xcode
cd /Users/hhelal/Documents/code/arrow/cpp/xcode-build && make -C /Users/hhelal/Documents/code/arrow/cpp/xcode-build -f /Users/hhelal/Documents/code/arrow/cpp/xcode-build/CMakeScripts/XCODE_DEPEND_HELPER.make PostBuild.arrow_objlib.Debug
/bin/rm -f /Users/hhelal/Documents/code/arrow/cpp/xcode-build/debug/Debug/libarrow.dylib
/bin/rm -f /Users/hhelal/Documents/code/arrow/cpp/xcode-build/debug/Debug/libarrow.a

=== BUILD TARGET arrow_shared OF PROJECT arrow WITH THE DEFAULT CONFIGURATION (Debug) ===

Check dependencies

Write auxiliary files
write-file /Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_shared.build/Script-9AFD4DDD88034C5F965570DF.sh
chmod 0755 /Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_shared.build/Script-9AFD4DDD88034C5F965570DF.sh

PhaseScriptExecution CMake\ PostBuild\ Rules xcode-build/src/arrow/arrow.build/Debug/arrow_shared.build/Script-9AFD4DDD88034C5F965570DF.sh
cd /Users/hhelal/Documents/code/arrow/cpp
/bin/sh -c /Users/hhelal/Documents/code/arrow/cpp/xcode-build/src/arrow/arrow.build/Debug/arrow_shared.build/Script-9AFD4DDD88034C5F965570DF.sh
echo ""Creating symlinks""
Creating symlinks
/usr/local/Cellar/cmake/3.12.4/bin/cmake -E cmake_symlink_library /Users/hhelal/Documents/code/arrow/cpp/xcode-build/debug/Debug/libarrow.12.0.0.dylib /Users/hhelal/Documents/code/arrow/cpp/xcode-build/debug/Debug/libarrow.12.dylib /Users/hhelal/Documents/code/arrow/cpp/xcode-build/debug/Debug/libarrow.dylib
CMake Error: cmake_symlink_library: System Error: No such file or directory
CMake Error: cmake_symlink_library: System Error: No such file or directory
make: *** [arrow_shared_buildpart_0] Error 1

** BUILD FAILED **


The following build commands failed:
PhaseScriptExecution CMake\ PostBuild\ Rules xcode-build/src/arrow/arrow.build/Debug/arrow_shared.build/Script-9AFD4DDD88034C5F965570DF.sh
(1 failure)

{code}",pull-request-available,['C++'],ARROW,Wish,Minor,2019-01-04 12:22:25,8
13207602,[Rust] Implement array_ops::sum() for PrimitiveArray<T>,In array_ops we should implement sum() that will return Option<T::Native> for a PrimitiveArray<T>,pull-request-available,['Rust'],ARROW,Improvement,Minor,2019-01-04 03:56:20,10
13207549,[GLib] Add builder_append_value() for consistency,"Because we use builder_append_values() for multiple values.

builder_append() is deprecated.",pull-request-available,['GLib'],ARROW,New Feature,Major,2019-01-03 21:46:27,1
13207548,[GLib] Remove an example to show Torch integration,Because Torch is not in active development.,pull-request-available,['GLib'],ARROW,Improvement,Minor,2019-01-03 21:45:27,1
13207490,[C++] Do not return buffers containing nullptr from internal allocations,"When a 0-byte buffer is allocated, or at the start of a BufferBuilder, the buffer's data pointer can be null. This leads to passing null arguments (with zero sizes) to standard functions such as memset() and memcpy() in many places. UBSAN doesn't like it.

Since a null pointer often means ""failed allocating"" or ""programmer error"", we might want to use a non-null pointer to a static empty piece of data instead.",pull-request-available,['C++'],ARROW,Wish,Major,2019-01-03 15:33:17,2
13207423,[CI/C++] Parquet test misses ZSTD compression codec in CMake 3.2 nightly builds,See build https://travis-ci.org/kszucs/crossbow/builds/474545470,parquet pull-request-available,['C++'],ARROW,Bug,Major,2019-01-03 08:47:08,3
13207422,[CI/Python] Disable ORC on nightly Alpine builds,See CI failure https://travis-ci.org/kszucs/crossbow/builds/474545437,pull-request-available,['Continuous Integration'],ARROW,Task,Major,2019-01-03 08:23:08,3
13207100,[Python] Cast Parquet column statistics to unicode if UTF8 ConvertedType is set,"When writing Pandas data to Parquet format and reading it back again I find that that statistics of text columns are stored as byte arrays rather than as unicode text. 

I'm not sure if this is a bug in Arrow, PyArrow, or just in my understanding of how best to manage statistics.  (I'd be quite happy to learn that it was the latter).

Here is a minimal example

{code:python}
import pandas as pd
df = pd.DataFrame({'x': ['a']})
df.to_parquet('df.parquet')
import pyarrow.parquet as pq
pf = pq.ParquetDataset('df.parquet')
piece = pf.pieces[0]
rg = piece.row_group(0)
md = piece.get_metadata(pq.ParquetFile)
rg = md.row_group(0)
c = rg.column(0)

>>> c
<pyarrow._parquet.ColumnChunkMetaData object at 0x7fd1a377c238>
  file_offset: 63
  file_path: 
  physical_type: BYTE_ARRAY
  num_values: 1
  path_in_schema: x
  is_stats_set: True
  statistics:
    <pyarrow._parquet.RowGroupStatistics object at 0x7fd1a37d4418>
      has_min_max: True
      min: b'a'
      max: b'a'
      null_count: 0
      distinct_count: 0
      num_values: 1
      physical_type: BYTE_ARRAY
  compression: SNAPPY
  encodings: ('PLAIN_DICTIONARY', 'PLAIN', 'RLE')
  has_dictionary_page: True
  dictionary_page_offset: 4
  data_page_offset: 25
  total_compressed_size: 59
  total_uncompressed_size: 55

>>> type(c.statistics.min)
bytes
{code}

My guess is that we would want to store a logical type in the statistics like UNICODE, though I don't have enough experience with Parquet data types to know if this is a good idea or possible.",parquet pull-request-available python,['Python'],ARROW,Bug,Minor,2019-01-01 03:39:05,14
13207095,[Python] setuptools_scm customization does not work for versions above 0.9.0 on Windows,"{code}
C:\Users\wesm\code\arrow\python (ARROW-3910 -> origin)
(pyarrow-dev)  git describe --dirty --tags --long --match 'apache-arrow-[0-9].*'
fatal: No names found, cannot describe anything.

C:\Users\wesm\code\arrow\python (ARROW-3910 -> origin)
(pyarrow-dev)  git describe --dirty --tags --long
apache-arrow-0.11.0-499-gf77c2967-dirty
{code}

It's possible this is a Windows-specific issue",pull-request-available,['Python'],ARROW,Bug,Major,2019-01-01 00:23:01,14
13206971,[Python] Can't reload a pandas dataframe containing a list of datetime.time ,"Ported from https://github.com/apache/arrow/issues/3223

This simple script writes a panda dataframe with a list of datetime.time. However, constructing back the pandas dataframe fails.

I initially realised that when doing a pd.read_parquet('example.parquet'), which fails with the same error.

This is using
pyarrow 0.11.1
pandas 0.23.4

{code}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

times = pd.to_datetime(['09:00', '09:30', '10:00', '10:30', '11:00', '11:30', '12:00']).time
df = pd.DataFrame({'Time': [times]})

table = pa.Table.from_pandas(df)
pq.write_table(table, 'example.parquet')

# works
table2 = pq.read_table('example.parquet')

# fails: ArrowNotImplementedError: Not implemented type for list in DataFrameBlock: time64[us]
df2 = pa.Table.to_pandas(table2)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-12-30 13:03:51,3
13206778,[C++][DOCUMENTATION] Update style guide to reflect some more exceptions,"* It seems we are using doxygen style comments for method/class declaration (""///"")
 * Use the macro NULLPTR([https://arrow.apache.org/docs/cpp/macros_8h.html]",pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Minor,2018-12-28 08:15:10,15
13206651,[C++] Abstract aggregation kernel API,"Related to the particular details of implementing various aggregation types, we should first put a bit of energy into the abstract API for aggregating data in a multi-threaded setting

Aggregators must support both hash/group (e.g. ""group by"" in SQL or data frame libraries) modes and non-group modes. 

Aggregations ideally should also support filter pushdown. For example:

{code}
select $AGG($EXPR)
from $TABLE
where $PREDICATE
{code}

Some systems might materialize the post-predicate / filtered version of {{$EXPR}}, then aggregate that. pandas does this for example. Vectorized performance can be much improved by filtering inside the aggregation kernel. How the predicate true/false values are handled may depend on the implementation details of the kernel (e.g. SUM or MEAN will be a bit different from PRODUCT)",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-27 17:53:19,13
13206632,[C++] Improve linting workflow and documentation for Windows-based developers,"It is challenging to run the clang-format and cpplint checks on Windows out of the box, and it is not documented how to do so. I found that neither the {{format}} nor {{lint}} targets work correctly when building with Visual Studio 2015. We could use Docker, of course, but it would be nice to be able to run these tools natively on Windows",pull-request-available,"['C++', 'Developer Tools']",ARROW,Improvement,Major,2018-12-27 16:20:31,6
13206514,[Python] Define process for testing procedures that check for no macro-level memory leaks,"Some kinds of memory leaks may be difficult to unit test for, and they may not cause valgrind errors necessarily

I had written some ad hoc leak tests in https://github.com/apache/arrow/blob/master/python/scripts/test_leak.py. We have some more of this in ARROW-3324. 

It would be useful to be able to create a sort of ""test suite"" of memory leak checks. They are a bit too intensive to run in CI (since you may have to run something many iterations to see whether it leaks), but we could run them in a nightly build",pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-26 22:37:26,14
13206498,"[Python] Error with ""asv run""","We should write more documentation about common asv workflows. Having not run them in a while, I found some rough edges, like:

{code}
$ asv run -b ToPandasStrings master
 Creating environments
 Discovering benchmarks
 Uninstalling from conda-py3.6-boost-cpp1.67.0-brotli-cmake-cython-flatbuffers1.7.1-libprotobuf-lz4-c-ninja-numpy1.14.5-pandas0.23.3-pip+setuptools_scm-rapidjson-snappy-zstd
 Building 2849f46f <master> for conda-py3.6-boost-cpp1.67.0-brotli-cmake-cython-flatbuffers1.7.1-libprotobuf-lz4-c-ninja-numpy1.14.5-pandas0.23.3-pip+setuptools_scm-rapidjson-snappy-zstd
 Error running /bin/bash /home/wesm/code/arrow/python/.asv/env/3dcfa8f34e2eb2b62de42321b0fd52e3/project/python/asv-build.sh
   STDOUT -------->
   
   STDERR -------->
   /home/wesm/code/arrow/python/.asv/env/3dcfa8f34e2eb2b62de42321b0fd52e3/project/python/asv-build.sh: line 24: conda: command not found
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-12-26 20:22:31,2
13206465,[Python] Clarify in development.rst that virtualenv cannot be used with miniconda/Anaconda,Per mailing list thread on 2018-12-25,pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-26 14:26:06,14
13206400,"[C++][DOCUMENTATION] Add ""python"" to Linux build instructions","make unittest step inthe C++ README.md do not work on fresh ubuntu image without python installed.

{{Error message from the ctest --output-on-failure indicates it is trying to find python:}}

{{
}}{{Running arrow-allocator-test, redirecting output into /home/micahk/arrow/cpp/debug/build/test-logs/arrow-allocator-test.txt (attempt 1/1)}}{{/usr/bin/env: python: No such file or directory}}

",pull-request-available,"['C++', 'Documentation']",ARROW,Bug,Trivial,2018-12-25 23:47:22,15
13206326,[R] Version number patch broke build,"The patch https://github.com/apache/arrow/commit/385c4384eb0dcc384b443f24765c64e9d6d88d28 broke the R build (which is in allowed_failures right now)

{code}
Building with: R CMD build 
0.22s$ R CMD build  .
* checking for file ./DESCRIPTION ... OK
* preparing arrow:
* checking DESCRIPTION meta-information ... ERROR
Malformed package version.
See section 'The DESCRIPTION file' in the 'Writing R Extensions'
manual.
The command ""R CMD build  ."" failed and exited with 1 during .
{code}",pull-request-available,['R'],ARROW,Bug,Major,2018-12-24 22:42:53,1
13206245,[Python] Create time types from Python sequences of integers,"This works for dates, but not times:

{code}
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

    def test_to_pandas_deduplicate_date_time():
        nunique = 100
        repeats = 10
    
        unique_values = list(range(nunique))
    
        cases = [
            # array type, to_pandas options
            ('date32', {'date_as_object': True}),
            ('date64', {'date_as_object': True}),
            ('time32[ms]', {}),
            ('time64[us]', {})
        ]
    
        for array_type, pandas_options in cases:
>           arr = pa.array(unique_values * repeats, type=array_type)

pyarrow/tests/test_convert_pandas.py:2392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
pyarrow/array.pxi:175: in pyarrow.lib.array
    return _sequence_to_array(obj, mask, size, type, pool, from_pandas)
pyarrow/array.pxi:36: in pyarrow.lib._sequence_to_array
    check_status(ConvertPySequence(sequence, mask, options, &out))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   raise ArrowInvalid(message)
E   pyarrow.lib.ArrowInvalid: ../src/arrow/python/python_to_arrow.cc:1012 : ../src/arrow/python/iterators.h:70 : Could not convert 0 with type int: converting to time32
{code}",beginner,['Python'],ARROW,Improvement,Major,2018-12-23 21:33:03,14
13206238,[Packaging] Missing glog dependency from arrow-cpp conda recipe,Follow up for https://github.com/apache/arrow/commit/700bd40afab973d00229a43dff5ce764ed996873,pull-request-available,['Packaging'],ARROW,Bug,Major,2018-12-23 19:24:10,3
13206225,[Python]Use ninja in pyarrow manylinux1 build,This should speed up the built slightly.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2018-12-23 14:42:40,8
13206164,[Python] Tests fail to run because hypothesis update broke its API,See https://hypothesis.readthedocs.io/en/latest/changes.html#v3-84-5,pull-request-available,['Python'],ARROW,Bug,Major,2018-12-22 21:35:06,3
13206057,[Documentation] Add README to docs/ root,This should direct the user to instructions about how to build the documentation,pull-request-available,['Documentation'],ARROW,Improvement,Major,2018-12-21 22:43:34,14
13206016,[C++] Binary identity cast not implemented,"Surfaced by test case in https://github.com/apache/arrow/pull/3239

```
In [1]: import pyarrow as pa

In [2]: pa.array([b'foo'])
Out[2]: 
<pyarrow.lib.BinaryArray object at 0x7f6ed510b9a8>
[
  666F6F
]

In [3]: arr = pa.array([b'foo'])

In [4]: arr.cast(pa.binary())
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-4-d6c6f1159d55> in <module>()
----> 1 arr.cast(pa.binary())

~/miniconda/envs/arrow-dev/lib/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.cast()

~/miniconda/envs/arrow-dev/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: No cast implemented from binary to binary
```",pull-request-available,['C++'],ARROW,Bug,Major,2018-12-21 16:43:22,13
13205878,"[Python] Deprecate pyarrow.open_stream,open_file in favor of pa.ipc.open_stream/open_file","In interest of disambiguating different meanings of ""streams"" and ""files"" I propose that we change these APIs to come out of the {{pyarrow.ipc}} namespace. To help with the transition, we can maintain functions in the top level namespace that create a FutureWarning. Since a lot of people use these functions, we should probably give them 2 major releases to get migrated",pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-21 04:34:41,14
13205846,[C++] Implement optimizations for dictionary unification where dictionaries are prefixes of the unified dictionary,"In the event that the unified dictionary contains other dictionaries as prefixes (e.g. as the result of delta dictionaries), we can avoid memory allocation and index transposition.

See discussion at 

https://github.com/apache/arrow/pull/3165#discussion_r243020982",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-21 00:36:59,2
13205843,[C++] Deprecated method suggests wrong method,ARROW-3545 introduces a typo.,pull-request-available,['C++'],ARROW,Improvement,Minor,2018-12-21 00:06:10,1
13205836,[Rust] Implement common Reader / DataSource trait for CSV and Parquet,"As a developer, I would like to be able to execute queries against Arrow data sources using a common trait.

",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-12-20 23:33:26,10
13205756,[Python] Table.from_batches() fails when passed a schema with metadata,"This seems to be a regression. In 0.10 I used to have this function to set column-level and table-level metadata on an existing Table:
 
{code:python}
def set_metadata(tbl, col_meta={}, tbl_meta={}):
    # Create updated column fields with new metadata
    if col_meta or tbl_meta:
        fields = []
        for col in tbl.itercolumns():
            if col.name in col_meta:
                # Get updated column metadata
                metadata = col.field.metadata or {}
                for k, v in col_meta[col.name].items():
                    metadata[k] = json.dumps(v).encode('utf-8')
                # Update field with updated metadata
                fields.append(col.field.add_metadata(metadata))
            else:
                fields.append(col.field)

        # Get updated table metadata
        tbl_metadata = tbl.schema.metadata
        for k, v in tbl_meta.items():
            tbl_metadata[k] = json.dumps(v).encode('utf-8')

        # Create new schema with updated metadata
        schema = pa.schema(fields, metadata=tbl_metadata)

        # With updated schema build new table (shouldn't copy data?)
        tbl = pa.Table.from_batches(tbl.to_batches(), schema=schema)

    return tbl
{code}

However, in 0.11 this fails with error:

{noformat}
ArrowInvalid: Schema at index 0 was different: 
x: int64
vs
x: int64
...
{noformat}

It works however if I replace from_batches() with from_arrays(), like this:
{code}
tbl = pa.Table.from_arrays(list(tbl.itercolumns()), schema=schema)
{code}

It seems that from_batches() compares the existing batch's schema with the new schema, and upon encountering a difference (in metadata only) fails.

A short test would be this:
{code}
import pandas as pd
import pyarrow as pa

df = pd.DataFrame({'x': [0,1,2]})
tbl = pa.Table.from_pandas(df, preserve_index=False)

field = tbl.schema[0].add_metadata({'test': 'data'})
schema = pa.schema([field])
# tbl2 = pa.Table.from_arrays(list(tbl.itercolumns()), schema=schema)
tbl2 = pa.Table.from_batches(tbl.to_batches(), schema)
tbl2.schema[0].metadata
{code}",pull-request-available regression,"['C++', 'Python']",ARROW,Bug,Major,2018-12-20 15:20:17,3
13205741,[C++] Make CSV nulls configurable,It could be useful to allow the user to configure the set of strings recognized as nulls.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-20 14:31:14,2
13205606,"[GLib] Use ""field"" for struct data type","Because C++ API is changed to use ""field"" by ARROW-3545.
",pull-request-available,['GLib'],ARROW,Improvement,Major,2018-12-20 01:50:53,1
13205492,[C++] Simplify Status and stringstream boilerplate,There's a lot of stringstream repetition when creating a Status.,pull-request-available,['C++'],ARROW,Improvement,Minor,2018-12-19 19:47:14,13
13205453,"[C++] CMake tweaks: allow RelWithDebInfo, improve FindClangTools","SetupCxxFlags.cmake does not list ""RELWITHDEBINFO"" in the [final flag setup|https://github.com/apache/arrow/blob/master/cpp/cmake_modules/SetupCxxFlags.cmake#L363], so cmake will error out if that build config is selected. It's handy for quick debugging without switching your python build etc over to ""DEBUG"".

FindClangTools.cmake could check the version of 'clang-format' (no version suffix) to see if it satisfies a version requirement.",pull-request-available ready-to-commit,['C++'],ARROW,Improvement,Trivial,2018-12-19 18:04:00,6
13205418,[C++] Add machine benchmarks,"I wonder if it may be useful to add machine benchmarks. I have a cache/memory latency benchmark lying around, we could also add e.g. memory bandwidth benchmarks.",pull-request-available,['C++'],ARROW,Wish,Minor,2018-12-19 15:11:21,2
13205411,[CI] Run Travis job where documentation is built when docs/ is changed,"When only changes to the {{docs}} directory are made, most Travis jobs are skipped, even the Python job which (presumably) builds the documentation to check for errors etc.

We should probably have a separate doc building job, or perhaps make it part of the linting job.",pull-request-available,"['Continuous Integration', 'Documentation']",ARROW,Bug,Major,2018-12-19 14:51:10,14
13205306,[Python] schema validation and filters,"Currently[schema validation|https://github.com/apache/arrow/blob/758bd557584107cb336cbc3422744dacd93978af/python/pyarrow/parquet.py#L900]of{{ParquetDataset}}takes place before filtering. This may raise a {{ValueError}}if the schema is different in some dataset pieces, even if these pieces would be subsequently filtered out. I think validation should happen after filtering to prevent such spurious errors:
{noformat}
--- a/pyarrow/parquet.py	
+++ b/pyarrow/parquet.py	
@@ -878,13 +878,13 @@
         if split_row_groups:
             raise NotImplementedError(""split_row_groups not yet implemented"")
 
-        if validate_schema:
-            self.validate_schemas()
-
         if filters is not None:
             filters = _check_filters(filters)
             self._filter(filters)
 
+        if validate_schema:
+            self.validate_schemas()
+
     def validate_schemas(self):
         open_file = self._get_open_file_func()
{noformat}",dataset easyfix parquet pull-request-available,['Python'],ARROW,Bug,Minor,2018-12-19 08:33:00,5
13205222,[Python] Parquet test failures on AppVeyor,"These may be related to the URI handling changes:
https://ci.appveyor.com/project/pitrou/arrow/builds/21100865/job/hcjj8qmamca7pr9u#L2540

[~wesmckinn]",pull-request-available,['Python'],ARROW,Bug,Major,2018-12-18 20:47:26,14
13205199,[C++] ARROW_BOOST_VENDORED doesn't work properly with ninja build,"Not sure if this is a regression, but I found

{code}
$ cmake .. -GNinja -DARROW_BOOST_VENDORED=ON
-- Building using CMake version: 3.12.0
-- Arrow version: 0.12.0 (full: '0.12.0-SNAPSHOT')
clang-tidy not found
clang-format found at /usr/bin/clang-format-6.0
infer not found
-- Found cpplint executable at /home/wesm/code/arrow/cpp/build-support/cpplint.py
-- Compiler command: env LANG=C /usr/bin/c++ -v
-- Compiler version: Using built-in specs.
COLLECT_GCC=/usr/bin/c++
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/4.8/lto-wrapper
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 4.8.5-4ubuntu8~14.04.2' --with-bugurl=file:///usr/share/doc/gcc-4.8/README.Bugs --enable-languages=c,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-4.8 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --with-gxx-include-dir=/usr/include/c++/4.8 --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --enable-gnu-unique-object --disable-libmudflap --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 4.8.5 (Ubuntu 4.8.5-4ubuntu8~14.04.2) 

-- Compiler id: GNU
Selected compiler gcc 4.8.5
Optional gold linker is buggy, using ld linker instead
Configured for DEBUG build (set with cmake -DCMAKE_BUILD_TYPE={release,debug,...})
-- Build Type: DEBUG
-- BOOST_VERSION: 1.67.0
-- BROTLI_VERSION: v0.6.0
-- DOUBLE_CONVERSION_VERSION: v3.1.1
-- FLATBUFFERS_VERSION: 02a7807dd8d26f5668ffbbec0360dc107bbfabd5
-- GBENCHMARK_VERSION: v1.4.1
-- GFLAGS_VERSION: v2.2.0
-- GLOG_VERSION: v0.3.5
-- GRPC_VERSION: v1.14.1
-- GTEST_VERSION: 1.8.0
-- JEMALLOC_VERSION: 17c897976c60b0e6e4f4a365c751027244dada7a
-- LZ4_VERSION: v1.7.5
-- ORC_VERSION: 1.5.1
-- PROTOBUF_VERSION: v3.6.1
-- RAPIDJSON_VERSION: v1.1.0
-- RE2_VERSION: 2018-10-01
-- SNAPPY_VERSION: 1.1.3
-- THRIFT_VERSION: 0.11.0
-- ZLIB_VERSION: 1.2.8
-- ZSTD_VERSION: v1.3.7
-- Found pthread: /usr/lib/x86_64-linux-gnu/libpthread.so
-- Boost include dir: 
-- Boost libraries: 
Added static library dependency boost_system_static: /home/wesm/code/arrow/cpp/boost-test/boost_ep-prefix/src/boost_ep/stage/lib/libboost_system.a
Added static library dependency boost_filesystem_static: /home/wesm/code/arrow/cpp/boost-test/boost_ep-prefix/src/boost_ep/stage/lib/libboost_filesystem.a
Added static library dependency boost_regex_static: /home/wesm/code/arrow/cpp/boost-test/boost_ep-prefix/src/boost_ep/stage/lib/libboost_regex.a
Added static library dependency double-conversion_static: /home/wesm/code/arrow/cpp/boost-test/double-conversion_ep/src/double-conversion_ep/lib/libdouble-conversion.a
-- double-conversion include dir: /home/wesm/code/arrow/cpp/boost-test/double-conversion_ep/src/double-conversion_ep/include
-- double-conversion static library: /home/wesm/code/arrow/cpp/boost-test/double-conversion_ep/src/double-conversion_ep/lib/libdouble-conversion.a
-- RapidJSON include dir: /home/wesm/code/arrow/cpp/boost-test/src/rapidjson_ep/include
-- Flatbuffers include dir: /home/wesm/code/arrow/cpp/boost-test/flatbuffers_ep-prefix/src/flatbuffers_ep-install/include
-- Flatbuffers compiler: /home/wesm/code/arrow/cpp/boost-test/flatbuffers_ep-prefix/src/flatbuffers_ep-install/bin/flatc
Added static library dependency jemalloc_static: /home/wesm/code/arrow/cpp/boost-test/jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc_pic.a
Added shared library dependency jemalloc_shared: /home/wesm/code/arrow/cpp/boost-test/jemalloc_ep-prefix/src/jemalloc_ep/dist//lib/libjemalloc.so
-- Found hdfs.h at: /home/wesm/code/arrow/cpp/thirdparty/hadoop/include/hdfs.h
-- Found the ZLIB shared library: /usr/lib/x86_64-linux-gnu/libz.so
Added shared library dependency zlib_shared: /usr/lib/x86_64-linux-gnu/libz.so
Added static library dependency snappy_static: /home/wesm/code/arrow/cpp/boost-test/snappy_ep/src/snappy_ep-install/lib/libsnappy.a
Added static library dependency brotli_enc_static: /home/wesm/code/arrow/cpp/boost-test/brotli_ep/src/brotli_ep-install/lib/x86_64-linux-gnu/libbrotlienc.a
Added static library dependency brotli_dec_static: /home/wesm/code/arrow/cpp/boost-test/brotli_ep/src/brotli_ep-install/lib/x86_64-linux-gnu/libbrotlidec.a
Added static library dependency brotli_common_static: /home/wesm/code/arrow/cpp/boost-test/brotli_ep/src/brotli_ep-install/lib/x86_64-linux-gnu/libbrotlicommon.a
Added static library dependency lz4_static: /home/wesm/code/arrow/cpp/boost-test/lz4_ep-prefix/src/lz4_ep/lib/liblz4.a
Added static library dependency zstd_static: /home/wesm/code/arrow/cpp/boost-test/zstd_ep-install/lib/libzstd.a
-- GLOG_CMAKE_CXX_FLAGS:  -ggdb -O0 -fPIC -pthread
-- CMAKE_CXX_FLAGS in glog:  -ggdb -O0 -fPIC -pthread
-- Glog version: v0.3.5
-- Glog include dir: /home/wesm/code/arrow/cpp/boost-test/glog_ep-prefix/src/glog_ep/include
-- Glog static library: /home/wesm/code/arrow/cpp/boost-test/glog_ep-prefix/src/glog_ep/lib/libglog.a
Added static library dependency glog_static: /home/wesm/code/arrow/cpp/boost-test/glog_ep-prefix/src/glog_ep/lib/libglog.a
-- CMAKE_C_FLAGS:  -ggdb -O0   -Wall -msse4.2
-- CMAKE_CXX_FLAGS:   -ggdb -O0  -Wall -msse4.2 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/wesm/code/arrow/cpp/boost-test
12:35 ~/code/arrow/cpp/boost-test  (master)$ ninja arrow_shared
ninja: error: 'boost_ep-prefix/src/boost_ep/stage/lib/libboost_system.a', needed by 'debug/libarrow.so.12.0.0', missing and no known rule to make it
{code}

This seems to work fine with using ""make"" ",pull-request-available,['C++'],ARROW,Bug,Major,2018-12-18 18:38:17,14
13205173,[Python] Add tests for casting from binary to utf8,Python-side follow up to ARROW-3387 to ensure that things work as expected in Python,pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-18 16:55:53,14
13205018,[C++] Upgrade to boost-cpp 1.69.0 again,"EDIT: the issue has been present for a large portion of 2018. I found this when merging the macOS C++ builds and changed the build type to Xcode 8.3:

https://travis-ci.org/wesm/arrow/jobs/469297420#L2856

I reported the issue into conda-forge at https://github.com/conda-forge/boost-cpp-feedstock/issues/40

It seems that the Ray project worked around this earlier this year: https://github.com/ray-project/ray/pull/1688",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-18 01:20:25,14
13204988,[Python] Fails to convert pytz.utc with versions 2018.3 and earlier,"pytz.UTC is only a subclass of BaseTzInfo starting from pytz 2018.4 (https://launchpad.net/pytz/+announcement/14962), so with older versions of pytz there are failures in the tests

see: https://github.com/apache/arrow/pull/2975#discussion-diff-239794078L969",pull-request-available,['Python'],ARROW,Bug,Major,2018-12-17 20:52:40,3
13204883,[Python/Integration] HDFS Tests failing with I/O operation on closed file,"To reporoduce run `docker-compose run hdfs-integration`

{code:python}
self = <pyarrow.tests.test_hdfs.TestLibHdfs testMethod=test_cat>

    def test_cat(self):
        path = pjoin(self.tmp_path, 'cat-test')

        data = b'foobarbaz'
        with self.hdfs.open(path, 'wb') as f:
            f.write(data)

>       contents = self.hdfs.cat(path)

opt/conda/lib/python3.6/site-packages/pyarrow/tests/test_hdfs.py:99:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
opt/conda/lib/python3.6/site-packages/pyarrow/filesystem.py:42: in cat
    return f.read()
pyarrow/io.pxi:298: in pyarrow.lib.NativeFile.read
    ???
pyarrow/io.pxi:180: in pyarrow.lib.NativeFile.size
    ???
pyarrow/io.pxi:142: in pyarrow.lib.NativeFile.get_random_access_file
    ???
pyarrow/io.pxi:159: in pyarrow.lib.NativeFile._assert_readable
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: I/O operation on closed file
{code}",hdfs,"['Integration', 'Python']",ARROW,Bug,Major,2018-12-17 12:46:24,14
13204690,[Python] Document use of int96 timestamps and options in Parquet docs,This is not mentioned in the prose docs; it would be helpful for people who are using systems requiring int96 timestamps (e.g. Impala/Redshift Spectrum) to have this,documentation parquet,['Python'],ARROW,Improvement,Major,2018-12-15 22:01:43,14
13204689,[Python/CI] Exercise large memory tests,See comment https://github.com/apache/arrow/pull/3171#issuecomment-447156646,nightly,"['Continuous Integration', 'Python']",ARROW,Task,Major,2018-12-15 22:00:31,3
13204675,[Packaging/Docker] Python tests on alpine miss pytest dependency,"{code:java}
Using /usr/lib/python2.7/site-packages
Searching for numpy==1.15.4
Best match: numpy 1.15.4
Adding numpy 1.15.4 to easy-install.pth file
Using /usr/lib/python2.7/site-packages
Finished processing dependencies for pyarrow==0.11.1.dev385+g9c8ddae1
/
/bin/sh: pytest: not found
The command ""docker-compose run python-alpine"" exited with 127.{code}",pull-request-available,['Packaging'],ARROW,Task,Major,2018-12-15 19:50:05,3
13204669,[Rust] Inconsistent method naming between BinaryArray and PrimitiveArray,PrimitiveArray has value() and BinaryArray has get_value() .. suggest we standardize on one convention,pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-12-15 17:53:15,10
13204667,[CI] Python 2.7 run uses Python 3.6,"I'm not sure when that started to happen, but you can see here and below:
https://travis-ci.org/apache/arrow/jobs/468438196#L2973

... that the Python 2.7 run on Travis-CI actually installs and tests with Python 3.6.
",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2018-12-15 17:37:01,3
13204665,[Python] Update link to 'development.rst' page from Python README.md,"When the Sphinx docs were restructured, the link in the [README|https://github.com/apache/arrow/blob/master/python/README.md]  changed from

[https://github.com/apache/arrow/blob/master/python/doc/source/development.rst]

to

[https://github.com/apache/arrow/blob/master/docs/source/python/development.rst]",pull-request-available,"['Documentation', 'Python']",ARROW,Task,Major,2018-12-15 17:09:25,6
13204660,"[Rust] Add array_ops methods for boolean AND, OR, NOT","We have math and comparison operations and I would now like to add boolean unary and binary operators such as AND, OR, NOT for use in predicates against arrow data.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-12-15 16:18:53,10
13204642,[C++] Make status codes pluggable,"Currently we're defining all Status codes in the Arrow base library, even those pertaining to sub-libraries such as arrow_python, plasma, gandiva, etc. We should try to devise some kind of simple registry system to avoid centralizing those.
",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-15 10:56:12,15
13204597,[Ruby] Interface for FileOutputStream doesn't respect append=True,"It seems that the PR (#1978) that resolved Issue #2018 has not cascaded down through the existing ruby interface.

I've been experimenting with variations of the `write-file.rb` examples, but passing in the append flag as true (`Arrow::FileOutputStream.open(""/tmp/file.arrow"", true)`) still results in overwriting the file, and trying the newer interface using truncate and append flags throws `ArgumentError: wrong number of arguments (3 for 2)`.",pull-request-available,['Ruby'],ARROW,Bug,Major,2018-12-14 23:22:30,1
13204588,[C++] thirdparty/download_dependencies.sh uses tools or options not available in older Linuxes,I found I had to install the {{realpath}} apt package on Ubuntu 14.04. Also {{wget 1.15}} does not have the {{--show-progress}} option,pull-request-available,['C++'],ARROW,Bug,Major,2018-12-14 22:15:18,14
13204559,[C++] Refactor ArrayBuilder bitmap logic into TypedBufferBuilder<bool>,It would be useful to have a specialization of TypedBufferBuilder to simplify building buffers of bits. This could then be utilized by ArrayBuilder (for the null bitmap) and BooleanBuilder (for values),pull-request-available,['C++'],ARROW,Improvement,Minor,2018-12-14 19:11:02,6
13204543,[CI] Use travis_terminate to halt builds when a step fails,"I noticed that Travis CI will soldier onward if a step in its {{script:}} block fails. This wastes build time when there is an error somewhere early on in the testing process

For example, in the main C++ build, if {{travis_script_cpp.sh}} fails, then the subsequent steps will continue.

It seems the way to deal with this is to add {{|| travis_terminate 1}} to lines that can fail

see

https://medium.com/@manjula.cse/how-to-stop-the-execution-of-travis-pipeline-if-script-exits-with-an-error-f0e5a43206bf

I also found this discussion

https://github.com/travis-ci/travis-ci/issues/1066",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-12-14 18:01:33,14
13204532,[C++] Define and document naming convention for internal / private header files not to be installed,"The purpose of this is so that a common {{ARROW_INSTALL_PUBLIC_HEADERS}} can recognize and exclude any file that is non-public from installation.

see discussion on https://github.com/apache/arrow/pull/3172",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-14 17:12:17,14
13204402,[C++] Use separate modular $COMPONENT-test targets for unit tests,"Because some tests depend on libraries from another component (e.g. ""gandiva"" relies on ""arrow""), if we build ""gandiva"" and {{ARROW_BUILD_TESTS=ON}} then we must also built the tests attached to the ""arrow"" target. This can be hacked around using the {{ARROW_TEST_INCLUDE_LABELS}} option, but the best thing would probably be to create separate modular targets for the unit tests:

* ""arrow-tests"" depends on ""arrow""
* ""parquet-tests"" depends on ""parquet"", ""arrow""
* ""gandiva-test"" depends on ""gandiva"", ""arrow""",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-14 04:06:18,14
13204386,[Python] Cython compilation error on cython==0.27.3,"On the latest master, I'm getting the following error:
{code:java}
[ 11%] Compiling Cython CXX source for lib...



Error compiling Cython file:

------------------------------------------------------------

...



  out.init(type)

  return out





cdef object pyarrow_wrap_metadata(

  ^

------------------------------------------------------------



pyarrow/public-api.pxi:95:5: Function signature does not match previous declaration

CMakeFiles/lib_pyx.dir/build.make:57: recipe for target 'CMakeFiles/lib_pyx' failed{code}
With 0.29.0 it is working. This might have been introduced in [https://github.com/apache/arrow/commit/12201841212967c78e31b2d2840b55b1707c4e7b]but I'm not sure.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-14 01:23:11,8
13204350,[Release] Remove source artifacts from dev dist system after release vote passes,"We have accumulated a lot of artifacts in https://dist.apache.org/repos/dist/dev/arrow. This is not scalable. I'm going to remove the old RC's for now, but this should be part of post-release scripts or release manager duties",pull-request-available,['Developer Tools'],ARROW,Improvement,Blocker,2018-12-13 21:05:43,1
13204243,[C++] Fix coverity issues,"I recently got private access to a one-shot Coverity Scan of the codebase. Instead of opening a separate issue for each found buglet, let's fix a number of them at once.",pull-request-available,['C++'],ARROW,Bug,Major,2018-12-13 14:02:53,2
13204205,[C++] Check and update vendored libraries,"Some vendored libraries, like {{util::variant}}, seem old. Also, perhaps we should put them all in a {{vendored/}} directory, to make inspection easier?",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-13 11:16:56,2
13204133,[Gandiva] Refer irhelpers.bc in build directory,"Normally, users remove build directory after they installed.
If users remove build directory, Gandiva doesn't work.

It's better that we install {{irhelpers.bc}} and use it by default. We can use {{irhelpers.bc}} in build directory only when we run tests. I think that test programs use {{Gandiva::Configuration}} that refers {{irhelpers.bc}} in build directory instead of the default {{Gandiva::Configuration}}.",pull-request-available,['C++ - Gandiva'],ARROW,Bug,Major,2018-12-13 01:19:33,1
13204084,[CI] Run Valgrind and C++ code coverage in different bulds,"Currently, we run Valgrind on a coverage-enabled C++ build on Travis-CI. This means the slowness of Valgrind acts as a multiplier of the overhead of outputting coverage information using the instrumentation added by the compiler.

Instead we should probably emit C++ (and Python) coverage information in a different Travis-CI build without Valgrind enabled.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2018-12-12 19:38:21,2
13204058,[C++] Integration test executable failure,"See consistent CI failures:

{code}
-- Creating binary inputs
/home/travis/build/apache/arrow/cpp-build/debug/arrow-json-integration-test --integration --arrow=/tmp/tmpx3363_ef/ffd61e898f66410093235f7b3edc0b8f_struct_example.json_to_arrow --json=/home/travis/build/apache/arrow/integration/data/struct_example.json --mode=JSON_TO_ARROW
-- Validating file
/home/travis/build/apache/arrow/cpp-build/debug/arrow-json-integration-test --integration --arrow=/tmp/tmpx3363_ef/ffd61e898f66410093235f7b3edc0b8f_struct_example.json_to_arrow --json=/home/travis/build/apache/arrow/integration/data/struct_example.json --mode=VALIDATE
-- Validating stream
/home/travis/build/apache/arrow/cpp-build/debug/file-to-stream /tmp/tmpx3363_ef/ffd61e898f66410093235f7b3edc0b8f_struct_example.json_to_arrow > /tmp/tmpx3363_ef/d9968e28006f42ee9df239f50c71231a_struct_example.arrow_to_stream
cat /tmp/tmpx3363_ef/d9968e28006f42ee9df239f50c71231a_struct_example.arrow_to_stream | /home/travis/build/apache/arrow/cpp-build/debug/stream-to-file > /tmp/tmpx3363_ef/eaf061ea34e64a08bf38800d4fe5a9be_struct_example.stream_to_arrow
/home/travis/build/apache/arrow/cpp-build/debug/arrow-json-integration-test --integration --arrow=/tmp/tmpx3363_ef/eaf061ea34e64a08bf38800d4fe5a9be_struct_example.stream_to_arrow --json=/home/travis/build/apache/arrow/integration/data/struct_example.json --mode=VALIDATE
Command failed: /home/travis/build/apache/arrow/cpp-build/debug/arrow-json-integration-test --integration --arrow=/tmp/tmpx3363_ef/eaf061ea34e64a08bf38800d4fe5a9be_struct_example.stream_to_arrow --json=/home/travis/build/apache/arrow/integration/data/struct_example.json --mode=VALIDATE
With output:
--------------
Error message: Invalid: /home/travis/build/apache/arrow/cpp/src/arrow/ipc/json-integration-test.cc:151 code: RecordBatchFileReader::Open(arrow_file.get(), &arrow_reader)
/home/travis/build/apache/arrow/cpp/src/arrow/ipc/reader.cc:624 code: ReadFooter()
File is too small: 0
{code}

https://travis-ci.org/apache/arrow/jobs/467092615#L2567

",ci-failure pull-request-available,['C++'],ARROW,Bug,Major,2018-12-12 17:58:28,14
13204013,Add CODE_OF_CONDUCT.md,"The Apache Software Foundation has a code of conduct that applies to its projects

https://www.apache.org/foundation/policies/conduct.html

We should add a document to the root of the git repository to direct interested individuals to the CoC.",pull-request-available,['Documentation'],ARROW,Improvement,Major,2018-12-12 14:40:55,14
13203943,[GLib] Replace GPU with CUDA,It should be done in ARROW-3209.,pull-request-available,['GLib'],ARROW,Improvement,Major,2018-12-12 09:34:53,1
13203927,[C++][Gandiva] Remove CMake version check,I could build Gandiva with CMake 3.7.2 and LLVM 6.0.0 on Debian stretch. But I disabled Gandiva JNI.,pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Improvement,Major,2018-12-12 08:46:02,1
13203749,[C++] [Doc] Clarify dictionary encoding integer signedness (and width?),"The Arrow spec states that a dictionary-encoded array uses int32 indices. Signed or unsigned? The spec doesn't say.

Also, the C++ implementation supports all kinds of integers as indices (8- to 64-bit, signed and unsigned). I wonder if we should at least mandate a specific signedness.",pull-request-available,"['C++', 'Documentation', 'Format']",ARROW,Improvement,Major,2018-12-11 15:31:04,2
13203659,[CI] Use understandable names in Travis Matrix ,Travis has a new feature to assign labels to the matrix entries making it much easier navigable.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-12-11 09:29:18,8
13203485,[Rust] CSV reader should handle case sensitivity for boolean values,"Excel saves booleans in CSV in upper case, Pandas uses Proper case.

Our CSV reader doesn't recognise (True,False, TRUE, FALSE). I noticed this when making boolean schema inference case insensitive.



I would propose that we convert Boolean strings to lower-case before casting them to Rust's bool type. [~andygrove], what do you think?",pull-request-available,['Rust'],ARROW,Bug,Major,2018-12-10 17:31:47,12
13203467,[C++] Do not build unit tests by default in build system,"This is partly an RFC: given that many people are building the C++ library in order to develop or test other parts of Apache Arrow, it might make sense to opt-in to building the unit tests rather than opting out. There are additional system requirements to build the unit tests, such as a newer version of CMake

[~kou] suggested this originally here

https://github.com/apache/arrow/issues/3140#issuecomment-445694144

In principle I'm in favor of this",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-10 16:30:42,14
13203430,[C++] Write prose documentation,"Now that the C++ docs are in the Sphinx doctree, we should write more comprehensive prose documentation for C++ programmers.
",pull-request-available,"['C++', 'Documentation']",ARROW,Improvement,Major,2018-12-10 13:34:39,2
13203426,[C++] Pass -C option when compiling with ccache to avoid some warnings,"ccache by default will eat comments, some of which are used by gcc to disable certain warnings, like case statements that fall through.

see https://github.com/apache/arrow/issues/3004",pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-10 13:09:26,2
13203408,"[C++] Allow ""binary"" input in simple JSON format",See review comment at https://github.com/apache/arrow/pull/3084#discussion_r240049276,pull-request-available,['C++'],ARROW,Improvement,Major,2018-12-10 11:05:36,2
13203407,[C++] Rename json.h,"This JSON format is mostly used for integration testing, it's not meant for outside consumption. Perhaps rename the header to {{json-integration.h}}?",pull-request-available,['C++'],ARROW,Wish,Major,2018-12-10 11:04:33,2
13203406,[C++] Fix CRTP use in json-simple.cc,See review comment at https://github.com/apache/arrow/pull/3084#discussion_r240049157,pull-request-available,['C++'],ARROW,Bug,Minor,2018-12-10 11:02:41,2
13203310,[Ruby] Homebrew donation solicitation on CLI breaking CI builds,"https://travis-ci.org/apache/arrow/jobs/465677542#L168

The error may be caused by the version of git in the image being too old. I found some threads about this like:

https://github.com/Linuxbrew/brew/issues/820",ci-failure pull-request-available,['Ruby'],ARROW,Bug,Major,2018-12-09 20:22:24,1
13203291,[C++] Update to LLVM and Clang bits to 7.0,"As {{llvmlite}}, the other package in the Python ecosystem moved to LLVM 7, we should follow along to avoid problems when we use it in the same Python environment as Gandiva.

Reference: https://github.com/numba/llvmlite/pull/412",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Improvement,Major,2018-12-09 18:02:21,14
13203286,[Rust] CI build broken because rustfmt not available on nightly toolchain,Posted on /r/rust to ask for help with this. I cannot install or use rustfmt with Rust nightly using the current documentation for rustfmt.,pull-request-available,['Rust'],ARROW,Bug,Major,2018-12-09 17:23:59,10
13203189,[Rust] Time and Timestamp Support,"I'm currently working on schema inference for the csv reader, but I got stuckwhile dealing with timestamps. I had a look at the datatypes.rs, and it looks like timestamps aren't supported in Rust.",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-12-08 06:26:45,10
13203160,[Python] Better error message when user connects to HDFS cluster with wrong port,"I'm trying to connect to HDFS using libhdfs and Kerberos.

I have JAVA_HOME and HADOOP_HOME set and{{pyarrow.hdfs.connect}}sets CLASSPATH correctly.

My connect call looks like:

{{import pyarrow.hdfs}}

{{c = pyarrow.hdfs.connect(host='MYHOST', port=42424,}}

{{            user='ME', kerb_ticket=""/tmp/krb5cc_498970"")}}

This doesn't error but the resulting connection can't do anything. They either error like this:

{{ArrowIOError: HDFS list directory failed, errno: 255 (Unknown error 255)}}

Or swallow errors (e.g.{{exists}}returning{{False}}).

Note that{{connect}}errors if the host is wrong but doesn't error if the port, user, or kerb_ticket are wrong. I have no idea how to debug this, because no useful errors.

Note that I_can_connect using the hdfs Python package. (Of course, that doesn't provide the API I need to read Parquet files.).

Any help would be appreciated greatly.",hdfs,['Python'],ARROW,Bug,Major,2018-12-07 22:42:41,14
13203026,Compat with pandas 0.24 rename of MultiIndex labels -> codes,"Pandas deprecated the `MultiIndex.labels` in favor of `MultiIndex.codes` ([https://github.com/pandas-dev/pandas/pull/23752).] In the pandas parquet/feather tests, we are now seeing warnings about this (and I assume there will be warnings as well in pyarrow tests if running on pandas master).",pull-request-available,['Python'],ARROW,Bug,Major,2018-12-07 12:21:50,5
13203022,"[Rust] Specify edition=""2018"" in Cargo.toml","This is a trivial change to update Cargo.toml to specify the use of Rust 2018 edition. See this blog post for more information:

https://hacks.mozilla.org/2018/12/rust-2018-is-here/",pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-12-07 12:00:03,10
13202718,[CI][GLib] Set timeout to Homebrew,"We can't get Homebrew log when Travis detects hanged Homebrew process and kill the CI job.
We need to detect hanged Homebrew process and kill the Homebrew process to get Homebrew log.",pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Sub-task,Major,2018-12-06 07:57:56,1
13202607,[Python] Build manylinux1 docker image directly in the CI,"Instead of always waiting for {{quay.io}}on PRs, we should built the docker image in the Travis job. We will still pull the current image from {{quay.io}} to profit from the layer caching.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-05 18:15:46,8
13202579,[R] Write vignette for R package,a vignette similar to https://arrow.apache.org/docs/python/index.html,pull-request-available,['R'],ARROW,Improvement,Major,2018-12-05 16:04:20,4
13202438,[Packaging] Stop to refer java/pom.xml to get version information,"https://github.com/apache/arrow/pull/3096#issuecomment-444345068

I want to stop the current version sharing style. (Referring {{java/pom.xml}} from C++, Python, C, Ruby, ....)
It introduces complexity. For example, we generates {{version.rb}} dynamically to create a Ruby package: https://github.com/apache/arrow/blob/master/ruby/red-arrow/version.rb

I think that we can just replace all versions in {{cpp/CMakeLists.txt}}, {{python/setup.py}}, {{c_glib/configure.ac}}, {{ruby/*/lib/*/version.rb}}, {{rust/Cargo.toml}}, ... by {{sed}} in the release process.",pull-request-available,['Packaging'],ARROW,New Feature,Major,2018-12-05 06:17:52,1
13202422,[Rust] Rust nightly build is failing,See recent CI failures such as https://travis-ci.org/apache/arrow/jobs/463656608#L650,ci-failure pull-request-available,['Rust'],ARROW,Bug,Major,2018-12-05 04:31:21,10
13202150,[Python] Segfault reading Parquet files from GNOMAD,"I am getting segfault trying to run a basic program Ubuntu 18.04 VM (AWS). Error also occurs out of box on Mac OS X.

$ sudo snap install --classic google-cloud-sdk
$ gsutil cp gs://gnomad-public/release/2.0.2/vds/exomes/gnomad.exomes.r2.0.2.sites.vds/rdd.parquet/part-r-00000-31fcf9bd-682f-4c20-bbe5-b0bd08699104.snappy.parquet .
$ conda install pyarrow
$ python test.py
Segmentation fault (core dumped)

test.py:

import pyarrow.parquet as pq
path = ""part-r-00000-31fcf9bd-682f-4c20-bbe5-b0bd08699104.snappy.parquet""
pq.read_table(path)

gdb output:

Thread 3 ""python"" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffdf199700 (LWP 13703)]
0x00007fffdfc2a470 in parquet::arrow::StructImpl::GetDefLevels(short const**, unsigned long*) () from /home/ubuntu/miniconda2/lib/python2.7/site-packages/pyarrow/../../../libparquet.so.11

I tested fastparquet, it reads the file just fine.",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2018-12-04 02:14:54,14
13202113,[Python/Documentation]Include Benchmarks.md in Sphinx docs,https://github.com/apache/arrow/pull/2856#issuecomment-443711136,pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2018-12-03 21:51:41,8
13201990,[C++] Random test data generation is slow,It seems a non-negligible amount of time in the test suite is spent in the Mersenne Twister random engine.,pull-request-available,['C++'],ARROW,Improvement,Minor,2018-12-03 13:09:01,2
13201883,[Python] Add option to deduplicate PyBytes / PyString / PyUnicode objects in Table.to_pandas conversion path,"While hashing carries a performance penalty, the memory savings can be huge. See also ARROW-3911 -- we should develop some reusable machinery for conversions that yield Python objects",pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-02 19:16:55,14
13201826,[Python] Add Gandiva bindings to Python wheels,Depends on adding LLVM6 to the build toolchain,pull-request-available,['Python'],ARROW,Improvement,Major,2018-12-02 03:04:13,8
13201779,[CI][GLib] Log Homebrew output,We need more information to fix {{brew update}} problem.,pull-request-available,['GLib'],ARROW,Sub-task,Major,2018-12-01 06:26:40,1
13201487,[Python] Set date_as_object to True in *.to_pandas as default after deduplicating logic implemented,See discussion in ARROW-3899,pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-30 00:44:29,14
13201374,[C++] Break builder.cc into multiple compilation units,"To improve readability I suggest splitting {{builder.cc}} into independent compilation units. Concrete builder classes are generally independent of each other. The only concern is whether inlining some of the base class implementations is important for performance.

This would also make incremental compilation faster when changing one of the concrete classes.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-29 15:47:20,2
13201117,[Rust] CSV reader should return Result<Option<>> not Option<Result<>>,CSV reader should return Result<Option<>> not Option<Result<>>,pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-11-28 14:19:20,10
13200929,[Python] Error reading IPC file with no record batches,When using theRecordBatchFileWriter without actually writing a record batch. The magic byte at the beginning of the file is not written. This causes the exception File is smaller than indicated metadata size when reading that file with the RecordBatchFileReader.,pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-11-27 18:40:05,14
13200876,[C++] Improve adaptive int builder performance,This should also improve dictionary builder performance.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-27 14:56:03,2
13200821,[Python] Creating Array with explicit string type fails on Python 2.7,"Pyarrow arrays of string cannot be created from numpy arrays of string anymore for versions pyarrow>=0.8.0 (this includes pyarrow==0.11.1).

Please find below a quick repro:
{code:python}
import numpy as np
import pyarrow as pa
vec = np.array([""toto"", ""tata""])
pa.array(vec, pa.string())
{code}

Runing this I get the following:

{code:python}
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-4-e753fb3a8193> in <module>()
----> 1 pa.array(vec, pa.string())

/usr/local/lib/python2.7/dist-packages/pyarrow/lib.so in pyarrow.lib.array()

/usr/local/lib/python2.7/dist-packages/pyarrow/lib.so in pyarrow.lib._ndarray_to_array()

/usr/local/lib/python2.7/dist-packages/pyarrow/lib.so in pyarrow.lib.check_status()

ArrowInvalid: 'utf32' codec can't decode bytes in position 0-3: code point not in range(0x110000)
{code}

However, this code snippet was working fine with pyarrow==0.7.1.

Was there any behavior change with string in pyarrow since 0.7.1?
Do you have any workaround for this?

Jacques


",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-11-27 10:37:15,14
13200629,[Python] creating schema with invalid paramaters causes segmanetation fault,"A segmentation fault occurs when executing the following:

{{import pyarrow}}
{{s = pyarrow.schema([None])}}
{{print(s)}}",pull-request-available,['Python'],ARROW,Bug,Major,2018-11-26 17:26:52,2
13200618,[C++] Compilation warnings with gcc 7.3.0,"I sometimes get these warnings in release mode:
{code}
[27/161] Building CXX object src/arrow/CMakeFiles/arrow_objlib.dir/csv/converter.cc.o
In file included from ../src/arrow/csv/converter.cc:24:0:
../src/arrow/builder.h: In member function 'virtual arrow::Status arrow::csv::{anonymous}::TimestampConverter::Convert(const arrow::csv::BlockParser&, int32_t, std::shared_ptr<arrow::Array>*)':
../src/arrow/builder.h:395:5: warning: 'value' may be used uninitialized in this function [-Wmaybe-uninitialized]
     raw_data_[length_++] = val;
     ^~~~~~~~~
../src/arrow/csv/converter.cc:347:18: note: 'value' was declared here
       value_type value;
                  ^~~~~
[93/161] Building CXX object src/parquet/CMakeFiles/parquet_objlib.dir/arrow/writer.cc.o
In file included from ../src/parquet/arrow/writer.cc:18:0:
../src/parquet/arrow/writer.h: In function 'std::shared_ptr<parquet::arrow::ArrowWriterProperties> parquet::arrow::default_arrow_writer_properties()':
../src/parquet/arrow/writer.h:82:40: warning: '<anonymous>.parquet::arrow::ArrowWriterProperties::Builder::coerce_timestamps_unit_' may be used uninitialized in this function [-Wmaybe-uninitialized]
           truncated_timestamps_allowed_));
                                        ^
../src/parquet/arrow/writer.cc: In member function 'arrow::Status parquet::arrow::{anonymous}::ArrowColumnWriter::Write(const arrow::Array&)':
../src/parquet/arrow/writer.cc:278:56: warning: 'num_levels' may be used uninitialized in this function [-Wmaybe-uninitialized]
     RETURN_NOT_OK(this->data_buffer->Resize(num_values * sizeof(T), false));
                                                        ^
../src/parquet/arrow/writer.cc:869:11: note: 'num_levels' was declared here
   int64_t num_levels;
           ^~~~~~~~~~
[94/161] Building CXX object src/parquet/CMakeFiles/parquet_objlib.dir/arrow/reader.cc.o
../src/parquet/arrow/reader.cc: In function 'int64_t parquet::arrow::impala_timestamp_to_nanoseconds(const parquet::Int96&)':
../src/parquet/arrow/reader.cc:79:86: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
   int64_t nanoseconds = *(reinterpret_cast<const int64_t*>(&(impala_timestamp.value)));
                                                                                      ^
{code}
",pull-request-available,['C++'],ARROW,Bug,Minor,2018-11-26 16:57:20,13
13200595,[C++] Additional test cases for ARROW-3831,See [~pitrou] comments here: https://github.com/apache/arrow/pull/3024#pullrequestreview-178218977,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-26 15:01:15,1
13200581,[Rust] Update version to 0.12.0 and update release instructions on wiki,The Rust version of Arrow still has version 0.10.0 in the Cargo.toml ... we need to bump this to 0.12.0 (or 0.12.0-alpha maybe) and update the instructions for releasing Arrow so that this version gets updated when performing a release.,pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-11-26 14:30:44,10
13200578,[Python] Add LLVM6 to manylinux1 base image,"This is necessary to be able to build and bundle libgandiva with the 0.12 release

This (epic!) build definition in Apache Kudu may be useful for building only the pieces that we need for linking the Gandiva libraries, which may help keep the image size minimal

https://github.com/apache/kudu/blob/master/thirdparty/build-definitions.sh#L175",pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-26 14:16:35,8
13200575,[Rust] Update Rust README to reflect new functionality,The Rust README is now very outdated and needs updating before we release 0.12.0,pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-11-26 14:08:12,10
13200574,[Rust] PrimitiveArray<T> should support cast operations,It should be possible to cast PrimitiveArray<i32> to PrimitiveArray<f32> as one example.,pull-request-available,['Rust'],ARROW,Improvement,Major,2018-11-26 14:03:02,12
13200569,[Rust] PrimitiveArray<T> should support simple math operations,"Given two primitive arrays of numeric types, it should be possible to perform simple math operations such as add, subtract, multiply, divide.",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-11-26 13:45:14,10
13200562,[C++] cuda-test failure,"This seems to have started recently. Weirdly, the test passes if I run {{cuda-test}} directly:
{code}
$ ctest -V -R cuda
UpdateCTestConfiguration  from :/home/antoine/arrow/cpp/build-test/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/antoine/arrow/cpp/build-test/DartConfiguration.tcl
Test project /home/antoine/arrow/cpp/build-test
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 16
    Start 16: cuda-test

16: Test command: /home/antoine/arrow/cpp/build-support/run-test.sh ""/home/antoine/arrow/cpp/build-test"" ""test"" ""/home/antoine/arrow/cpp/build-test/debug//cuda-test""
16: Test timeout computed to be: 10000000
16: Running cuda-test, redirecting output into /home/antoine/arrow/cpp/build-test/build/test-logs/cuda-test.txt (attempt 1/1)
16: Running main() from gtest_main.cc
16: [==========] Running 8 tests from 4 test cases.
16: [----------] Global test environment set-up.
16: [----------] 3 tests from TestCudaBuffer
16: [ RUN      ] TestCudaBuffer.Allocate
16: [       OK ] TestCudaBuffer.Allocate (59 ms)
16: [ RUN      ] TestCudaBuffer.CopyFromHost
16: [       OK ] TestCudaBuffer.CopyFromHost (0 ms)
16: [ RUN      ] TestCudaBuffer.FromBuffer
16: [       OK ] TestCudaBuffer.FromBuffer (1 ms)
16: [----------] 3 tests from TestCudaBuffer (60 ms total)
16: 
16: [----------] 3 tests from TestCudaBufferWriter
16: [ RUN      ] TestCudaBufferWriter.UnbufferedWrites
16: [       OK ] TestCudaBufferWriter.UnbufferedWrites (3 ms)
16: [ RUN      ] TestCudaBufferWriter.BufferedWrites
16: [       OK ] TestCudaBufferWriter.BufferedWrites (3 ms)
16: [ RUN      ] TestCudaBufferWriter.EdgeCases
16: [       OK ] TestCudaBufferWriter.EdgeCases (2 ms)
16: [----------] 3 tests from TestCudaBufferWriter (8 ms total)
16: 
16: [----------] 1 test from TestCudaBufferReader
16: [ RUN      ] TestCudaBufferReader.Basics
16: [       OK ] TestCudaBufferReader.Basics (0 ms)
16: [----------] 1 test from TestCudaBufferReader (0 ms total)
16: 
16: [----------] 1 test from TestCudaArrowIpc
16: [ RUN      ] TestCudaArrowIpc.BasicWriteRead
16: ../src/arrow/gpu/cuda-test.cc:331: Failure
16: Failed
16: 'ReadRecordBatch(batch->schema(), device_serialized, default_memory_pool(), &device_batch)' failed with Invalid: Message is length 0
16: [  FAILED  ] TestCudaArrowIpc.BasicWriteRead (5 ms)
16: [----------] 1 test from TestCudaArrowIpc (5 ms total)
16: 
16: [----------] Global test environment tear-down
16: [==========] 8 tests from 4 test cases ran. (73 ms total)
16: [  PASSED  ] 7 tests.
16: [  FAILED  ] 1 test, listed below:
16: [  FAILED  ] TestCudaArrowIpc.BasicWriteRead
16: 
16:  1 FAILED TEST
16:   YOU HAVE 1 DISABLED TEST
16: 
16: ~/arrow/cpp/build-test/src/arrow/gpu
1/1 Test #16: cuda-test ........................***Failed    0.15 sec

0% tests passed, 1 tests failed out of 1

Label Time Summary:
unittest    =   0.15 sec*proc (1 test)

Total Test time (real) =   0.16 sec

The following tests FAILED:
	 16 - cuda-test (Failed)
Errors while running CTest
{code}
",pull-request-available,"['C++', 'GPU']",ARROW,Bug,Major,2018-11-26 13:21:26,2
13200398,[C++] Build shared libraries consistently with -fvisibility=hidden,See https://github.com/apache/arrow/pull/2437,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-25 03:47:17,2
13200370,[C++] Add Peek to InputStream API,"Some InputStream sources support lookahead without advancing the stream, like BufferReader and BufferedInputStream. This is used in Parquet's internal IO APIs, so it would be useful to be able to remove these after a deprecation cycle

https://github.com/apache/arrow/blob/master/cpp/src/parquet/util/memory.h#L375

If an InputStream does not support Peek, the returned {{string_view}} would have zero size. 

Another option is to have {{Peek}} return {{Status}}, so that NotImplemented can be returned. I would prefer to return 0-length if the operation is not supported",pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-24 17:10:14,14
13200369,"[Rust] ""invalid fastbin errors"" since Rust nightly-2018-11-03","Many tests in my DataFusion project started failing since Rust nightly-2018-11-03 with this error.= when calling Arrow to create new arrays.
{code:java}
*** Error in `/home/andy/git/andygrove/datafusion/target/debug/deps/datafusion-fe1bbf92d599090f': invalid fastbin entry (free): 0x00007f5c3c005710 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f5c499aa7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f5c499b337a]
/lib/x86_64-linux-gnu/libc.so.6(+0x82d52)[0x7f5c499b5d52]
/lib/x86_64-linux-gnu/libc.so.6(posix_memalign+0x11d)[0x7f5c499ba71d]
/home/andy/git/andygrove/datafusion/target/debug/deps/datafusion-fe1bbf92d599090f(_ZN5arrow6memory16allocate_aligned17h22616ca11b0b7ea8E+0x38)[0x565548dea148]{code}",pull-request-available,['Rust'],ARROW,Bug,Major,2018-11-24 17:07:07,10
13200367,[Rust] Build against nightly Rust in CI,"In order to allow the parquet-rs code to be contributed, we need to move to using nightly Rust. We should build against both stable and nightly and allow failures against stable.",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-11-24 16:38:05,10
13200305,[GLib] Use travis_retry with brew bundle command,"This has been flaky lately, see

https://travis-ci.org/apache/arrow/jobs/458878912#L1844

It may not make the errors go away, but it might be worth adding retry logic to try a few times before giving up",ci-failure,['GLib'],ARROW,Improvement,Major,2018-11-23 20:47:02,1
13200232,[Python] ParquetDataset().read columns argument always returns partition column,"I just noticed that no matter which columns are specified on load of a dataset, the partition column is always returned. This might lead to strange behaviour, as the resulting dataframe has more than the expected columns:
{code}
import dask as da
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import os
import numpy as np
import shutil

PATH_PYARROW_MANUAL = '/tmp/pyarrow_manual.pa/'

if os.path.exists(PATH_PYARROW_MANUAL):
    shutil.rmtree(PATH_PYARROW_MANUAL)
os.mkdir(PATH_PYARROW_MANUAL)

arrays = np.array([np.array([0, 1, 2]), np.array([3, 4]), np.nan, np.nan])
strings = np.array([np.nan, np.nan, 'a', 'b'])

df = pd.DataFrame([0, 0, 1, 1], columns=['partition_column'])
df.index.name='DPRD_ID'
df['arrays'] = pd.Series(arrays)
df['strings'] = pd.Series(strings)

my_schema = pa.schema([('DPRD_ID', pa.int64()),
                       ('partition_column', pa.int32()),
                       ('arrays', pa.list_(pa.int32())),
                       ('strings', pa.string()),
                       ('new_column', pa.string())])

table = pa.Table.from_pandas(df, schema=my_schema)
pq.write_to_dataset(table, root_path=PATH_PYARROW_MANUAL, partition_cols=['partition_column'])

df_pq = pq.ParquetDataset(PATH_PYARROW_MANUAL).read(columns=['DPRD_ID', 'strings']).to_pandas()
# pd.read_parquet(PATH_PYARROW_MANUAL, columns=['DPRD_ID', 'strings'], engine='pyarrow')
df_pq
{code}
df_pq has column `partition_column`",dataset dataset-parquet-read parquet pull-request-available python,['Python'],ARROW,Bug,Major,2018-11-23 11:44:37,5
13200187,[Gandiva] [C++] Add option to use -static-libstdc++ when building libgandiva_jni.so,"This [commit|https://github.com/apache/arrow/commit/ba2b2ea2301f067cc95306e11546ddb6d402a55c#diff-d5e5df5984ba660e999a7c657039f6af] broke gandiva packaging by removing static linking of std c++, since dremio consumes a fat jar that includes packaged gandiva native libraries we would need to statically link std c++

As suggested in the commit message will re-introduce it as a CMake Flag.",pull-request-available,['C++ - Gandiva'],ARROW,Task,Major,2018-11-23 06:30:47,14
13200103,[Rust] Schema/Field/Datatype should implement serde traits,"JSON serde is already implemented but in a non-standard way which prevents referencing Arrow types from structs that implement the serde Serialization/Deserialization traits.

We should refactor this code so that the Arrow types implement custom serialization. See [https://serde.rs/custom-serialization.html] for more information.

",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-11-22 15:21:17,10
13200094,[C++] Implement string to timestamp cast,Companion work to ARROW-3738,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-22 14:55:32,2
13200086,[C++] used uninitialized warning,"{code}
    In file included from ../src/arrow/util/bit-util-test.cc:35:
    ../src/arrow/util/bit-stream-utils.h: In function 'void arrow::TestZigZag(int32_t)':
    ../src/arrow/util/bit-stream-utils.h:110:11: warning: 'buffer' is used uninitialized in this function [-Wuninitialized]
         memcpy(&buffered_values_, buffer_ + byte_offset_, num_bytes);
         ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
{code}",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-11-22 14:28:28,1
13200052,"[C++] ""make check-format"" is slow",We should probably run {{clang-format}} in parallel.,pull-request-available,['C++'],ARROW,Wish,Major,2018-11-22 11:23:40,2
13199734,[Gandiva] Build on Windows,"I started briefly looking at this. As an initial matter, our FindLLVM.cmake is inadequate. There's one (BSD-licensed) we can pull in from the LLVM-based D compiler: https://github.com/ldc-developers/ldc/blob/master/cmake/Modules/FindLLVM.cmake. If someone can point out another one, I can take a look. ",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Improvement,Major,2018-11-21 02:29:54,14
13199668,[C++] Remove ARROW_USE_SSE and ARROW_SSE3,Those options can be detected programmatically (SSE3 is available on all recent x86-64 CPUs). Instead we may want to add a ARROW_DISABLE_SIMD option if we want to disable all SIMD optimizations at compile-time (e.g. for comparing and testing).,pull-request-available,['C++'],ARROW,Wish,Major,2018-11-20 18:48:58,2
13199642,"[Python] Writing Parquet file from empty table created with Table.from_pandas(..., preserve_index=False) fails","{code:java}
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa


def test_write_empty_preserve_index():

# passes

df = pd.DataFrame()
table = pa.Table.from_pandas(df, preserve_index=True)
pq.write_table(table, 'test1.parquet')
table2 = pq.read_table('test1.parquet')
df2 = table2.to_pandas()
pd.util.testing.assert_frame_equal(df, df2)


def test_write_empty_no_preserve_index():
df = pd.DataFrame()
table = pa.Table.from_pandas(df, preserve_index=False)

# fails here
pq.write_table(table, 'test2.parquet')

table2 = pq.read_table('test2.parquet')
df2 = table2.to_pandas()
pd.util.testing.assert_frame_equal(df, df2){code}


First test passes. Second one fails with this:


{code:java}
___________________________________ test_write_empty_no_preserve_index ___________________________________

def test_write_empty_no_preserve_index():
df = pd.DataFrame()
table = pa.Table.from_pandas(df, preserve_index=False)

# fails here
> pq.write_table(table, 'test2.parquet')

test_empty.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../.conda/envs/pedlenv/lib/python3.6/site-packages/pyarrow/parquet.py:1125: in write_table
writer.write_table(table, row_group_size=row_group_size)
../.conda/envs/pedlenv/lib/python3.6/site-packages/pyarrow/parquet.py:361: in __exit__
self.close()
../.conda/envs/pedlenv/lib/python3.6/site-packages/pyarrow/parquet.py:380: in close
self.writer.close()
pyarrow/_parquet.pyx:916: in pyarrow._parquet.ParquetWriter.close
???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

> ???
E pyarrow.lib.ArrowIOError: Root node did not have children

pyarrow/error.pxi:83: ArrowIOError
{code}


I haven't had a chance to investigate but seems not desired behavior.



",parquet pull-request-available,['Python'],ARROW,Bug,Minor,2018-11-20 16:41:30,14
13199591,[C++] warning: catching polymorphic type by value,"{code}
../src/arrow/compute/compute-test.cc: In member function 'virtual void arrow::compute::TestCast_StringToNumber_Test::TestBody()':
../src/arrow/compute/compute-test.cc:917:17: warning: catching polymorphic type 'class std::runtime_error' by value [-Wcatch-value=]
   } catch (std::runtime_error) {
                 ^~~~~~~~~~~~~
{code}",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-11-20 12:01:26,1
13199381,[Rust] Add ability to infer schema in CSV reader,"A CSV reader is being added in ARROW-3726 and it currently requires an explicit schema to be provided.

It would be nice to have an option where the schema can be inferred automatically.

The user should be able to specify some defaults, such as date/time formats.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-11-19 16:38:49,12
13199378,[Rust] Implement CSV Writer,A CSV reader is being implemented in ARROW-3726 and this ticket is to add the corresponding writer.,pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-11-19 16:36:55,10
13199372,[C++] gflags link errors on Windows,"These errors have been occurring in the last few days

https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/builds/20402981/job/cygaqwbjulgaxcn8",pull-request-available,['C++'],ARROW,Bug,Major,2018-11-19 16:13:39,14
13199367,[C++] Add PREFIX option to ADD_ARROW_BENCHMARK,See option in ADD_ARROW_TEST,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-19 16:02:06,14
13199187,[Doc] Merge Python & C++ and move to top-level,"Merge the C++, Python and Format documentation and move it to the top-level folder.",pull-request-available,['Documentation'],ARROW,Improvement,Major,2018-11-18 18:42:34,8
13199121,[C++] arrow::util::Codec::Decompress() doesn't return decompressed data size,"We can't know decompressed data size when we only have compressed data. The current {{arrow::util::Codec::Decompress()}} doesn't return decompressed data size. So we can't know which data in {{output_buffer}} can be used.

FYI: {{arrow::util::Codec::Compress()}} returns compressed data size.",pull-request-available,['C++'],ARROW,Bug,Major,2018-11-17 22:47:38,1
13199104,[Python] Support protocols to extract Arrow objects from third-party classes,"In the style of NumPy's {{__array__}}, we should be able to ask inputs to {{pa.array}}, {{pa.Table.from_X}}, ... whether they can convert themselves to Arrow objects. This would allow for example to turn objects that hold an Arrow object internally to expose them directly instead of going a conversion path.

My current use case involves Pandas {{ExtensionArray}} instances that internally have Arrow objects and should be reused when we pass the whole {{DataFrame}} to {{pa.Table.from_pandas}}.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-17 18:42:51,5
13199028,[C++] Determine if using ccache caching in Travis CI actually improves build times,In the discussion for https://github.com/apache/arrow/pull/2982 I saw that the cache for master is over 3GB. I'm curious if maintaining such a large cache is inflating our test times,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-16 20:57:53,14
13199023,[Python] The Python README.md does not show how to run the unit test suite,"If I were a new developer to Apache Arrow in Python, it would not be obvious how to invoke the test suite from reading the README.md.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-16 20:41:00,14
13199019,"[R] Document developer workflow for building project, running unit tests in r/README.md","Not being a regular R developer, it's not clear to me how to build and run the test suite if I wanted to contribute to the project",pull-request-available,['R'],ARROW,Improvement,Major,2018-11-16 20:28:58,13
13198888,[Packaging] Update conda variant files to conform with feedstock after compiler migration,"Update https://github.com/apache/arrow/tree/master/dev/tasks/conda-recipes/variants
to https://github.com/conda-forge/arrow-cpp-feedstock/tree/master/.ci_support

A more sophisticated `smithy rerender` like solution would be handy.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-11-16 12:07:51,3
13198699,"[C++/Python] Split C++ and Python unit test Travis CI jobs, run all C++ tests (including Gandiva) together","Our main C++/Python job is bumping up against the 50 minute limit lately, so it is time to do a little bit of reorganization

I suggest that we do this:

* Build and test all C++ code including Gandiva in a single job on Linux and macOS
* Run Python unit tests (but not the C++ tests) on Linux and macOS in a separate job

Code coverage will need to get uploaded in the Linux jobs for both of these, so a little bit of surgery is required",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-11-15 18:28:15,14
13198461,[C++] TestScalarAppendUnsafe is not testing unsafe appends,"In [TestScalarAppendUnsafe|https://github.com/apache/arrow/blob/56e72ba09c3d886c6a5aeb11fb1642af13819f93/cpp/src/arrow/array-test.cc#L1497],AppendNull is called to modify the bitmask which is not unsafe- it reserves at least one more element. UnsafeAppendNull should be used instead.

Since PoolBuffers round their capacities up to multiples of 64 bytes, no buffer is ever in danger of overflowing in this test anyway. More repetitions should be used so that the test will fail if insufficient storage is allocated. More storage should be allocated in the test, too- currently only a single iteration's worth is allocated.",newbie pull-request-available test,['C++'],ARROW,Bug,Minor,2018-11-14 21:44:20,6
13198413,[Python] Segmentation fault when writing empty RecordBatches to Parquet,"h2. Background

I am trying to convert a very sparse dataset to parquet (~3% rows in a range are populated). The file I am working with spans upto ~63M rows. I decided to iterate in batches of 500k rows, 127 batches in total. Each row batch is a {{RecordBatch}}. I create 4 batches at a time, and write to a parquet file incrementally. Something like this:

{code:python}
batches = [..]  # 4 batches
tbl = pa.Table.from_batches(batches)
pqwriter.write_table(tbl, row_group_size=15000)
# same issue with pq.write_table(..)
{code}

I was getting a segmentation fault at the final step, I narrowed it down to a specific iteration. I noticed that iteration had empty batches; specifically, [0, 0, 2876, 14423]. The number of rows for each {{RecordBatch}} for the whole dataset is below:

{code:python}
[14050, 16398, 14080, 14920, 15527, 14288, 15040, 14733, 15345, 15799,
15728, 15942, 14734, 15241, 15721, 15255, 14167, 14009, 13753, 14800,
14554, 14287, 15393, 14766, 16600, 15675, 14072, 13263, 12906, 14167,
14455, 15428, 15129, 16141, 15478, 16257, 14639, 14887, 14919, 15535,
13973, 14334, 13286, 15038, 15951, 17252, 15883, 19903, 16967, 16878,
15845, 12205, 8761, 0, 0, 0, 0, 0, 2876, 14423, 13557, 12723, 14330,
15452, 13551, 12723, 12396, 13531, 13539, 11512, 13175, 13941, 14634,
15515, 14239, 13856, 13873, 14154, 14822, 13543, 14653, 15328, 16171,
15101, 150 55, 15194, 14058, 13706, 14747, 14650, 14694, 15397, 15122,
16055, 16635, 14153, 14665, 14781, 15462, 15426, 16150, 14632, 14532,
15139, 15324, 15279, 16075, 16394, 16834, 15391, 16320, 1650 4, 17248,
15913, 15341, 14754, 16637, 15695, 16642, 18143, 19481, 19072, 15742,
18807, 18789, 14258, 0, 0]
{code}

On excluding the empty {{RecordBatch}}-es, the segfault goes away, but unfortunately I couldn't create a proper minimal example with synthetic data.

h2. Not quite minimal example

The data I am using is from the 1000 Genome project, which has been public for many years, so we can be reasonably sure the data is good. The following steps should help you replicate the issue.

# Download the data file (and index), about 330MB:
{code:bash}
$ wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr20.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz{,.tbi}
{code}
# Install the Cython library {{pysam}}, a thin wrapper around the reference implementation of the VCF file spec. You will need {{zlib}} headers, but that's probably not a problem :)
{code:bash}
$ pip3 install --user pysam
{code}
# Now you can use the attached script to replicate the crash.

h2. Extra information

I have tried attaching gdb, the backtrace when the segfault occurs is shown below (maybe it helps, this is how I realised empty batches could be the reason).

{code}
(gdb) bt
#0  0x00007f3e7676d670 in parquet::TypedColumnWriter<parquet::DataType<(parquet::Type::type)6> >::WriteMiniBatch(long, short const*, short const*, parquet::ByteArray const*) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.11
#1  0x00007f3e76733d1e in arrow::Status parquet::arrow::(anonymous namespace)::ArrowColumnWriter::TypedWriteBatch<parquet::DataType<(parquet::Type::type)6>, arrow::BinaryType>(arrow::Array const&, long, short const*, short const*) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.11
#2  0x00007f3e7673a3d4 in parquet::arrow::(anonymous namespace)::ArrowColumnWriter::Write(arrow::Array const&) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.11
#3  0x00007f3e7673df09 in parquet::arrow::FileWriter::Impl::WriteColumnChunk(std::shared_ptr<arrow::ChunkedArray> const&, long, long) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.11
#4  0x00007f3e7673c74d in parquet::arrow::FileWriter::WriteColumnChunk(std::shared_ptr<arrow::ChunkedArray> const&, long, long) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.11
#5  0x00007f3e7673c8d2 in parquet::arrow::FileWriter::WriteTable(arrow::Table const&, long) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libparquet.so.11
#6  0x00007f3e731e3a51 in __pyx_pw_7pyarrow_8_parquet_13ParquetWriter_5write_table(_object*, _object*, _object*) ()
   from /home/user/miniconda3/lib/python3.6/site-packages/pyarrow/_parquet.cpython-36m-x86_64-linux-gnu.so
{code}",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-11-14 18:08:03,14
13198362,[C++] Signed to unsigned integer cast yields incorrect results when type sizes are the same,"In https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/kernels/cast.cc#L239, an {{int32_t}} is overflows in 

{code}
constexpr in_type kMax = static_cast<in_type>(std::numeric_limits<out_type>::max());
{code}

resulting in safety checks not being correctly performed

{code}
In [2]: arr = pa.array([-1, -2, -3], type='int32')

In [3]: arr.cast('uint32')
Out[3]: 
<pyarrow.lib.UInt32Array object at 0x7f4889457b88>
[
  4294967295,
  4294967294,
  4294967293
]
{code}

This would be expected to error when {{safe=True}}",pull-request-available,['C++'],ARROW,Bug,Major,2018-11-14 15:17:15,13
13198360,"[Python] Enable calling object in Table.to_pandas to ""self-destruct"" for improved memory use","One issue with using {{Table.to_pandas}} is that it results in a memory doubling (at least, more if there are a lot of Python objects created). It would be useful if there was an option to destroy the {{arrow::Column}} references once they've been transferred into the target data frame. This would render the {{pyarrow.Table}} object useless afterward",pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-14 15:13:07,14
13198212,[C++] Use double-conversion conda package in CI toolchain,This is being built from the EP currently,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-14 02:08:26,14
13198158,[R] Incorrect collection of float type,"Repro from `sparklyr`:


{code:java}
library(sparklyr)
library(arrow)

sc <- spark_connect(master = ""local"")
DBI::dbGetQuery(sc, ""SELECT cast(1 as float)""){code}


Actual:
{code:java}
  CAST(1 AS FLOAT)
1       1065353216{code}
Expected:


{code:java}
  CAST(1 AS FLOAT)
1                1{code}
",pull-request-available,['R'],ARROW,Bug,Major,2018-11-13 21:22:18,4
13198152,[C++] Implement BufferedReader for C++,"This will be the reader companion to {{arrow::io::BufferedOutputStream}} and a C++-like version of the {{io.BufferedReader}} class in the Python standard library

https://docs.python.org/3/library/io.html#io.BufferedReader

We already have a partial version of this that's used in the Parquet library

https://github.com/apache/arrow/blob/master/cpp/src/parquet/util/memory.h#L413

In particular we need

* Seek implemented for random access (it will invalidate the buffer)
* Peek method returning {{shared_ptr<Buffer>}}, a zero copy view into buffered memory

This is needed for ARROW-3126",pull-request-available,['C++'],ARROW,New Feature,Major,2018-11-13 20:57:44,14
13198151,[C++] Configure buffer size in arrow::io::BufferedOutputStream,This is hard-coded to 4096 right now. For higher latency file systems it may be desirable to use a larger buffer. See also ARROW-3777 about performance testing for high latency files,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-13 20:44:46,14
13198124,[C++] Don't put implementations in test-util.h,"{{test-util.h}} is included in most (all?) test files, and it's quite long to compile because it includes many other files and recompiles helper functions all the time. Instead we should have only declarations in {{test-util.h}} and put implementations in a separate {{.cc}} file.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-13 18:50:06,14
13198077,"[C++] Implement a mock ""high latency"" filesystem","Some of our tools don't perform well out of the box for filesystems with high latency reads, like cloud blob stores. In such cases, it may be better to use buffered reads with a larger read ahead window. Having a mock filesystem to introduce latency into reads will help with testing / developing APIs for this",pull-request-available,['C++'],ARROW,New Feature,Major,2018-11-13 14:41:47,2
13128844,[C++] Handling Parquet Arrow reads that overflow a BinaryArray capacity,See comment thread in https://stackoverflow.com/questions/48115087/converting-parquetfile-to-pandas-dataframe-with-a-column-with-a-set-of-string-in ,parquet,['C++'],ARROW,Bug,Major,2018-01-05 16:07:15,14
13179812,[C++] Change parquet::arrow::FileReader::ReadRowGroups to read into contiguous arrays,"Instead of creating a chunk per RowGroup, we should read at least for primitive type into a single, pre-allocated Array. This needs some new functionality in the Record reader classes and thus should be done after https://github.com/apache/parquet-cpp/pull/462 is merged.",parquet,['C++'],ARROW,Improvement,Major,2018-08-19 14:27:27,14
13165847,[C++] Read Parquet dictionary encoded ColumnChunks directly into an Arrow DictionaryArray,"Dictionary data is very common in parquet, in the current implementation parquet-cpp decodesdictionary encoded data always before creating aplain arrow array. This process is wasteful sincewe could use arrow's DictionaryArray directly and achieve several benefits:
 # Smaller memory footprint - both in the decoding process and in the resulting arrow table - especially when the dict values are large
 # Better decoding performance - mostly as a result of the first bullet - less memory fetches and less allocations.

I think thosebenefitscouldachievesignificant improvements in runtime.

Mydirection for the implementation is to read the indices (through the DictionaryDecoder, after the RLE decoding) and values separately into 2 arrays and create a DictionaryArrayusing them.

There are some questionsto discuss:
 # Should this be the default behavior for dictionary encoded data
 # Should it be controlled with a parameter in the API
 # What should be the policy in case some of the chunks are dictionary encoded and some are not.

I started implementing this but would like tohear your opinions.",parquet pull-request-available,['C++'],ARROW,Improvement,Major,2018-06-13 14:08:14,14
13175438,[C++] Validate or add option to validate arrow::Table schema in parquet::arrow::FileWriter::WriteTable,Failing to validate will cause a segfault when the passed table does not match the schema used to instantiate the writer. See ARROW-2926 ,parquet pull-request-available,['C++'],ARROW,Improvement,Major,2018-07-29 23:35:05,6
13197820,[C++] Add cast for Null to any type,Casting a column from NullType to any other type is possible as the resulting array will also be all-null but simply with a different type annotation.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-12 14:31:51,2
13197717,[Python] pa.Table.from_pandas doesn't use schema ordering,"Pyarrow is sensitive to the order of the columns upon load of partitioned Files.
With the function {{pa.Table.from_pandas(dataframe, schema=my_schema)}} we can apply a schema to a dataframe. I noticed that the returned {{pa.Table}} object does use the ordering of pandas columns rather than the schema columns. Furthermore it is possible to have columns in the schema but not in the DataFrame (and hence in the resulting pa.Table).

This behaviour requires a lot of fiddling with the pandas Frame in the first place if we like to write compatible partitioned files. Hence I argue that for {{pa.Table.from_pandas}}, and any other comparable function, the schema should be the principal source for the Table structure and not the columns and the ordering in the pandas DataFrame. If I specify a schema I simply expect that the resulting Table actually has this schema.

Here is a little example. If you remove the reordering of df2 everything works fine:
{code:python}
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd
import os
import numpy as np
import shutil

PATH_PYARROW_MANUAL = '/tmp/pyarrow_manual.pa/'

if os.path.exists(PATH_PYARROW_MANUAL):
    shutil.rmtree(PATH_PYARROW_MANUAL)
os.mkdir(PATH_PYARROW_MANUAL)

arrays = np.array([np.array([0, 1, 2]), np.array([3, 4]), np.nan, np.nan])
strings = np.array([np.nan, np.nan, 'a', 'b'])

df = pd.DataFrame([0, 0, 1, 1], columns=['partition_column'])
df.index.name='DPRD_ID'
df['arrays'] = pd.Series(arrays)
df['strings'] = pd.Series(strings)

my_schema = pa.schema([('DPRD_ID', pa.int64()),
                       ('partition_column', pa.int32()),
                       ('arrays', pa.list_(pa.int32())),
                       ('strings', pa.string()),
                       ('new_column', pa.string())])

df1 = df[df.partition_column==0]
df2 = df[df.partition_column==1][['strings', 'partition_column', 'arrays']]


table1 = pa.Table.from_pandas(df1, schema=my_schema)
table2 = pa.Table.from_pandas(df2, schema=my_schema)

pq.write_table(table1, os.path.join(PATH_PYARROW_MANUAL, '1.pa'))
pq.write_table(table2, os.path.join(PATH_PYARROW_MANUAL, '2.pa'))

pd.read_parquet(PATH_PYARROW_MANUAL)
{code}

If 
",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-11-12 08:26:22,3
13141895,[C++] Parquet arrow::Table reads error when overflowing capacity of BinaryArray,"# When reading a parquet file with binary data > 2 GiB, we get an ArrowIOError due to it not creating chunked arrays. Reading each row group individually and then concatenating the tables works, however.


{code:java}
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq


x = pa.array(list('1' * 2**30))

demo = 'demo.parquet'


def scenario():
    t = pa.Table.from_arrays([x], ['x'])
    writer = pq.ParquetWriter(demo, t.schema)
    for i in range(2):
        writer.write_table(t)
    writer.close()

    pf = pq.ParquetFile(demo)

    # pyarrow.lib.ArrowIOError: Arrow error: Invalid: BinaryArray cannot contain more than 2147483646 bytes, have 2147483647
    t2 = pf.read()

    # Works, but note, there are 32 row groups, not 2 as suggested by:
    # https://arrow.apache.org/docs/python/parquet.html#finer-grained-reading-and-writing
    tables = [pf.read_row_group(i) for i in range(pf.num_row_groups)]
    t3 = pa.concat_tables(tables)

scenario()
{code}",parquet pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-03-01 20:07:47,6
13197658,"[R] Build R library on Windows, document build instructions for Windows developers","The status quo for building the R package with the C++ library is a bit special (see procedure [here|https://cwiki.apache.org/confluence/display/ARROW/Release+Management+Guide#ReleaseManagementGuide-UpdatingCRANpackages]) and requires that compilation happen witha specific toolchain for R. There is no guidance on how to get a successful build locally on Windows, and it's not covered by our CI so we are vulnerable to regression.

Goal for this ticket: recreate the environment from [https://github.com/r-windows/rtools-backports]in our Appveyor so that we can build and test R with C++ in continuous integration. Side effect of bringing the {{PKGBUILD}} configuration that the r-windows setup uses into our source control for easier release management. This should also get us closer to, perhaps arriving at, reproducible build steps for local Windows developers.",pull-request-available,['R'],ARROW,Improvement,Major,2018-11-11 19:44:01,4
13197608,"[GLib] Support for CompressedInputStream, CompressedOutputStream","Along with ARROW-3748, this will enable reading compressed CSV files from languages using the GLib bindings, like Ruby",pull-request-available,['GLib'],ARROW,New Feature,Major,2018-11-10 21:12:45,1
13197594,[Packaging] Zstd configure error on linux package builds,"Ubuntu Xenial https://travis-ci.org/kszucs/crossbow/builds/453054759
Ubuntu Bionic https://travis-ci.org/kszucs/crossbow/builds/453054805
Ubuntu Trusty https://travis-ci.org/kszucs/crossbow/builds/453054811
Debian Stretch https://travis-ci.org/kszucs/crossbow/builds/453054727

Perhaps this commit is related: https://github.com/apache/arrow/commit/394b334bba1199bd2d98a158736a6652efce629f

cc [~kou]",pull-request-available,['Packaging'],ARROW,Bug,Major,2018-11-10 18:27:22,1
13197558,[C++] Flip order of data members in arrow::Decimal128,"As discussed in https://github.com/apache/arrow/pull/2845, this will enable a data buffer to be correctly interpreted as {{Decimal128**}}, so memcpy and other operations will work",pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-10 05:27:54,14
13197533,[C++] Calling ArrayBuilder::Resize with length smaller than current appended length results in invalid state,"This was brought up by the Go patch ARROW-3613. If you append some data to a builder, then call {{Resize}} with something smaller than what's reported by {{length()}}, the capacity will be updated, but the length will not. So I think further appends would probably segfault. Either way we should add some tests for this case of ""shrinking"" a builder (which destroys data, but it's permitted by the API ",pull-request-available,['C++'],ARROW,Bug,Major,2018-11-09 23:16:05,13
13197519,[C++] Add CSV conversion option to parse ISO8601-like timestamp strings,See similar functionality in other libraries. I believe pandas has a fast path for iso8601,csv pull-request-available,['C++'],ARROW,New Feature,Major,2018-11-09 22:19:17,2
13197484,[CI/Docker/Python] Support running integration tests on multiple python versions,Currently python-3.6 image is pinned in integration/hdfs/Dockerfile and integration/pandas-master/Dockerfile. It's possible to pass build time argument similarly like the arrow:python-${PYTHON_VERSION} image works.,docker,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2018-11-09 18:52:39,3
13197458,[Python] Proper error handling in _ensure_type,"We have multiple _ensure_type like functions, the in defined in array.pxi bypasses None which causes segfault in the following example:

{code}
pa.array([1, 2, 3]).cast(None)
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-09 16:44:01,8
13197221,[Python] Merging Parquet Files - Pandas Meta in Schema Mismatch,"From:https://stackoverflow.com/questions/53214288/merging-parquet-files-pandas-meta-in-schema-mismatch

I am trying to merge multiple parquet files into one. Their schemas are identical field-wise but my{{ParquetWriter}}is complaining that they are not. After some investigation I found that the pandas meta in the schemas are different, causing this error.

Sample-

{code:python}
import pyarrow.parquet as pq

pq_tables=[]
for file_ in files:
    pq_table = pq.read_table(f'{MESS_DIR}/{file_}')
    pq_tables.append(pq_table)
    if writer is None:
        writer = pq.ParquetWriter(COMPRESSED_FILE, schema=pq_table.schema, use_deprecated_int96_timestamps=True)
    writer.write_table(table=pq_table)
{code}

The error-

{code}
Traceback (most recent call last):
  File ""{PATH_TO}/main.py"", line 68, in lambda_handler
    writer.write_table(table=pq_table)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyarrow/parquet.py"", line 335, in write_table
    raise ValueError(msg)
ValueError: Table schema does not match schema used to create file:
{code}
",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-11-08 19:26:37,3
13197218,"[Python] Document use of pyarrow.foreign_buffer, cuda.foreign_buffer in Sphinx",This could be called out as a major section in http://arrow.apache.org/docs/python/memory.html for better discoverability,pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2018-11-08 19:16:57,2
13197202,[Rust] CSV Reader & Writer,"As an Arrow Rust user, I would like to be able to read and write CSV files, so that I can quickly ingest data into an Arrow format for futher use, and save outputs in CSV.

As there aren't yet many options for working with tabular/df structures in Rust (other than Andy's DataFusion), I'm struggling to motivate for this feature. However, I think building a csv parser into Rust wouldreduce effort forfuture libs (incl DataFusion).",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-11-08 18:28:58,10
13197097,[C++] Allow specifying column types to CSV reader,"I'm not sure how to expose this. The easiest, implementation-wise, would be to allow passing a {{Schema}} (for example inside the {{ConvertOptions}}).

Another possibility is to allow specifying the default types for type inference. For example type inference currently infers integers as {{int64}}, but the user might prefer {{int32}}.

Thoughts?",pull-request-available,['C++'],ARROW,Improvement,Major,2018-11-08 11:33:33,2
13196646,[C++] Don't pass CXX_FLAGS to C_FLAGS,"When C++ specific flags are passed to the C flags, we get problems when C++ specific flags like {{-std=c++14}} are passed. Instead of manually stripping these flags explicitly, we should better separate {{CMAKE_C_FLAGS}}from {{CMAKE_CXX_FLAGS}}.

See also: https://github.com/conda-forge/arrow-cpp-feedstock/pull/64",pull-request-available,['C++'],ARROW,Bug,Major,2018-11-06 18:10:35,8
13196550,[Crossbow][Python] Run nightly tests against pandas master,Follow-up of [https://github.com/apache/arrow/pull/2758] and https://github.com/apache/arrow/pull/2755,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2018-11-06 13:53:11,3
13196518,[Packaging] Nightly CentOS builds are failing,"https://github.com/kszucs/crossbow/branches/all?utf8=%E2%9C%93&query=centos

cc [~kou]",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-11-06 12:30:56,1
13196498,[C++] test failure with zstd 1.3.7,"We may pass a NULL output pointer to {{Codec::Decompress}} when the output size is 0, but recent zstd versions don't like that.

This was already identified (and patched on the side) by one of our packagers:
https://github.com/NixOS/nixpkgs/pull/48928
",pull-request-available,['C++'],ARROW,Bug,Major,2018-11-06 11:41:37,2
13196356,[Python] DataFrame.to_parquet crashes if datetime column has time zones,"On CPython 2.7.15, 3.5.6, 3.6.6, and 3.7.0, creating a Pandas DataFrame with a {{datetime.datetime}} object serializes to Parquet just fine, but crashes with an {{AttributeError}} if you try to use the built-in {{timezone}} objects.

To reproduce, on Python 3:
{code:java}
import datetime as dt
import pandas as pd

df = pd.DataFrame({'foo': [dt.datetime(2018, 1, 1, 1, 23, 45, tzinfo=dt.timezone.utc)]})
df.to_parquet('data.parq')
{code}


On Python 2, create a subclass of {{datetime.tzinfo}}as shown [here|https://docs.python.org/2/library/datetime.html#datetime.tzinfo] and try the same thing.



The following exception results:
{noformat}
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/core/frame.py"", line 1945, in to_parquet
    compression=compression, **kwargs)
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/io/parquet.py"", line 257, in to_parquet
    return impl.write(df, path, compression=compression, **kwargs)
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/io/parquet.py"", line 118, in write
    table = self.api.Table.from_pandas(df)
  File ""pyarrow/table.pxi"", line 1217, in pyarrow.lib.Table.from_pandas
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 381, in dataframe_to_arrays
    convert_types)]
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 380, in <listcomp>
    for c, t in zip(columns_to_convert,
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 370, in convert_column
    return pa.array(col, type=ty, from_pandas=True, safe=safe)
  File ""pyarrow/array.pxi"", line 167, in pyarrow.lib.array
  File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 409, in get_datetimetz_type
    type_ = pa.timestamp(unit, tz)
  File ""pyarrow/types.pxi"", line 1038, in pyarrow.lib.timestamp
  File ""pyarrow/types.pxi"", line 955, in pyarrow.lib.tzinfo_to_string
AttributeError: 'datetime.timezone' object has no attribute 'zone'

'datetime.timezone' object has no attribute 'zone'
{noformat}

 This doesn't happen if you use {{pytz.UTC}} as the timezone object.",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-11-05 21:53:03,3
13196145,[C++] CSV parser should allow ignoring empty lines,"This is a copy/paste of the github issue: https://github.com/apache/arrow/issues/2883



Hi,

I was playing with {{pyarrow.csv}} {{read_csv}} and found a rather strange behavior that I'm not sure is normal.

Parsing will fail if the delimiter of the CSV file is a comma and there's a blank line after the header (see {{basic_with_blank.csv}} example)

Example output:

{{{{Traceback (most recent call last): File ""sorrow.py"", line 14, in <module> table = pa_csv.read_csv(csv) File ""pyarrow/_csv.pyx"", line 198, in pyarrow._csv.read_csv File ""pyarrow/error.pxi"", line 81, in pyarrow.lib.check_status pyarrow.lib.ArrowInvalid: CSV parse error: Expected 2 columns, got 1 }}}}

If I change the CSV delimiter to semicolon, the error disappears and everything is fine!

I'm providing python code and CSV samples which compares with pandas (which does not suffer from this).

Hope this helps, thanks",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-11-04 23:19:51,2
13195693,[Rust] Implement PrimitiveArrayBuilder<T>.push_values,"Follow-up of https://github.com/apache/arrow/pull/2858
See discussion https://github.com/apache/arrow/pull/2858/files#r228808948",pull-request-available,['Rust'],ARROW,Sub-task,Major,2018-11-01 16:19:45,12
13195648,[Python] Support for masked arrays in to/from numpy,"Again, in this PR for vaex: [https://github.com/maartenbreddels/vaex/pull/116]I support masked arrays, it would be nice if this goes into pyarrow. If this approach looks good I could do a PR.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-11-01 12:13:46,5
13195605,[C++]Use FindBacktrace to find execinfo.h support,See https://github.com/apache/arrow/issues/2818 and https://github.com/apache/arrow/issues/2080#issuecomment-434723969,pull-request-available,['C++'],ARROW,Bug,Major,2018-11-01 09:53:48,8
13195570,[Python] Convert big-endian numbers or raise error in pyarrow.array,"I've been playing around getting vaex to support arrow, and it's been going really well, except for some corner cases.

I expect


{code:java}
import numpy as np
import pyarrow as pa
np_array = np.arange(10, dtype='>f8')
pa.array(np_array)

{code}
To give an error, or show proper values, instead I get:

!Screen Shot 2018-11-01 at 09.10.48.png!

",pull-request-available,['Python'],ARROW,Bug,Major,2018-11-01 08:16:47,2
13195496,[C++] Improve CSV parser performance,The CSV parser is currently the bottleneck when reading CSV files. There are a couple ways to make it a bit faster.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-31 21:27:31,2
13195164,[C++] Allow whitespace in numeric CSV fields,"Pandas allows whitespace before and after numbers in CSV files, but Arrow doesn't:
{code:python}
>>> s = b""a,b,c\n12 , 34 , 56\n""
>>> pd.read_csv(io.BytesIO(s))
    a   b   c
0  12  34  56
>>> csv.read_csv(io.BytesIO(s)).to_pandas()
        a        b       c
0  b'12 '  b' 34 '  b' 56'
{code}
",pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-30 17:05:01,2
13195080,[Python] CategoricalIndex is lost after reading back,"When a {{CategoricalIndex}} is written and read back the resulting index is not more categorical.
{code}
df = pd.DataFrame([['a', 'b'], ['c', 'd']], columns=['c1', 'c2'])
df['c1'] = df['c1'].astype('category')
df = df.set_index(['c1'])

table = pa.Table.from_pandas(df)
pq.write_table(table, 'test.parquet')

ref_df = pq.read_pandas('test.parquet').to_pandas()

print(df.index)
# CategoricalIndex(['a', 'c'], categories=['a', 'c'], ordered=False, name='c1', dtype='category')

print(ref_df.index)
# Index(['a', 'c'], dtype='object', name='c1')
{code}
In the metadata the information is correctly contained:
{code:java}
{""name"": ""c1"", ""field_name"": ""c1"", ""p'
            b'andas_type"": ""categorical"", ""numpy_type"": ""int8"", ""metadata"": {""'
            b'num_categories"": 2, ""ordered"": false}
{code}
",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-10-30 10:19:44,14
13195070,[Python] Datetimes from non-DateTimeIndex cannot be deserialized,"Given an index which contains datetimes but is no DateTimeIndex writing the file works but reading back fails.
{code:python}
df = pd.DataFrame(1, index=pd.MultiIndex.from_arrays([[1,2],[3,4]]), columns=[pd.to_datetime(""2018/01/01"")])

# columns index is no DateTimeIndex anymore
df = df.reset_index().set_index(['level_0', 'level_1'])

table = pa.Table.from_pandas(df)
pq.write_table(table, 'test.parquet')

pq.read_pandas('test.parquet').to_pandas()
{code}

results in 
{code}
KeyError                                  Traceback (most recent call last)
~/venv/mpptool/lib/python3.7/site-packages/pyarrow/pandas_compat.py in _pandas_type_to_numpy_type(pandas_type)
    676     try:
--> 677         return _pandas_logical_type_map[pandas_type]
    678     except KeyError:

KeyError: 'datetime'
{code}

The created schema:

{code}
2018-01-01 00:00:00: int64
level_0: int64
level_1: int64
metadata
--------
{b'pandas': b'{""index_columns"": [""level_0"", ""level_1""], ""column_indexes"": [{""n'
            b'ame"": null, ""field_name"": null, ""pandas_type"": ""datetime"", ""nump'
            b'y_type"": ""object"", ""metadata"": null}], ""columns"": [{""name"": ""201'
            b'8-01-01 00:00:00"", ""field_name"": ""2018-01-01 00:00:00"", ""pandas_'
            b'type"": ""int64"", ""numpy_type"": ""int64"", ""metadata"": null}, {""name'
            b'"": ""level_0"", ""field_name"": ""level_0"", ""pandas_type"": ""int64"", ""'
            b'numpy_type"": ""int64"", ""metadata"": null}, {""name"": ""level_1"", ""fi'
            b'eld_name"": ""level_1"", ""pandas_type"": ""int64"", ""numpy_type"": ""int'
            b'64"", ""metadata"": null}], ""pandas_version"": ""0.23.4""}'}
{code}",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-10-30 09:23:46,14
13195064,[Python] Mixed column indexes are read back as strings ,"Consider the following example: 

{code:java}
df = pd.DataFrame(1, index=[pd.to_datetime('2018/01/01')], columns=['a string', pd.to_datetime('2018/01/02')])

table = pa.Table.from_pandas(df)
pq.write_table(table, 'test.parquet')

ref_df = pq.read_pandas('test.parquet').to_pandas()

print(df.columns)
# Index(['a string', 2018-01-02 00:00:00], dtype='object')
print(ref_df.columns)
# Index(['a string', '2018-01-02 00:00:00'], dtype='object')
{code}

The serialized data frame has an index with a string and a datetime field (happened when resetting the index of a formerly datetime only column).
When reading the string back the datetime is converted into a string.

When looking at the schema I find {{""pandas_type"": ""mixed"", ""numpy_ty'
            b'pe"": ""object""}} before serializing and {{""pandas_type"": ""unicode"", ""numpy_'
            b'type"": ""object""}} after reading back. So the schema was aware of the mixed type but did not store the actual types.

The same happens with other types like numbers as well. One can produce interesting situations:

{{pd.DataFrame(1, index=[pd.to_datetime('2018/01/01')], columns=['1', 1])}} can be written but fails to be read back as the index is no more unique with '1' showing up two times.

IIf this is not a bug but expected maybe the user should be somehow warned that information is lost? Like a {{NotImplemented}} exception.",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-10-30 08:44:04,5
13194921,[Python] Add convenience factories to create IO streams,"Currently, creating IO streams requires invoking various constructors with irregular names. It would be nice to expose a higher-level interface.

For example:
{code:python}
def open_reader(source, compression='detect'):
    """"""
    Create an Arrow input stream.

    Parameters
    ----------
    source: str, Path, buffer, file-like object, ...
        The source to open for reading
    compression: str or None
        The compression algorithm to use for on-the-fly decompression.
        If 'detect' and source is a file path, then compression will be
        chosen based on the file extension.
        If None, no compression will be applied.
        Otherwise, a well-known algorithm name must be supplied (e.g. ""gzip"")
    """"""

def open_writer(source, compression='detect'):
    """"""
    Create an Arrow output stream.

    Parameters
    ----------
    source: str, Path, buffer, file-like object, ...
        The source to open for writing
    compression: str or None
        The compression algorithm to use for on-the-fly compression.
        If 'detect' and source is a file path, then compression will be
        chosen based on the file extension.
        If None, no compression will be applied.
        Otherwise, a well-known algorithm name must be supplied (e.g. ""gzip"")
    """"""
{code}

Thoughts?",pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-29 18:09:53,2
13194730,[Rust] Optimize `push_slice` of `BufferBuilder<bool>`,"Current implementation just repeatedly calls `push`, this should be optimized.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-10-29 01:26:44,0
13194686,[C++] Add arrowConfig.cmake generation,This allows simple usage of Arrow in C++ packages using {{find_package(arrow)}} with no additional {{FindArrow.cmake}} in {{cmake_modules}}.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-28 10:55:16,8
13194685,[C++/Python] remove public keyword from Cython api functions,"Based on a conversation with Stefan Behnel, we should be able to change the {{cdef public api}} statements in pyarrow/public-api.pxi to simply {{cdef api}}
",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-10-28 10:51:36,14
13194629,[C++][Python] Move reading from Feather as Table feature to C++ from Python,It's for using the feature from GLib.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Minor,2018-10-27 16:03:14,1
13194605,[C++/Python] Update arrow/python/pyarrow_api.h,"This file should be updated to one generated by Cython 0.29. Also based on a conversation with Stefan Behnel, we should be able to change the {{cdef public api}} statements in pyarrow/public-api.pxi to simply {{cdef api}}",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-10-27 13:34:12,8
13194594,[GLib] cuda.cpp compile error,"Build errors:
- https://travis-ci.org/kszucs/crossbow/builds/446856145
- https://travis-ci.org/kszucs/crossbow/builds/446855716",pull-request-available,['GLib'],ARROW,Bug,Major,2018-10-27 11:36:21,1
13194580,[C#] Add Appveyor build for C#,Test C# library on Windows,pull-request-available,['C#'],ARROW,New Feature,Major,2018-10-27 08:25:05,14
13194374,[Python] Document multithreading options in Sphinx and add to api.rst,I was looking for these functions (like {{cpu_count}}) in api.rst and did not find them,pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-26 10:10:08,2
13194255,[Go] Resize does not correctly update the length,"If you have the following code:
{code:java}
package main

import (
    ""fmt""

    ""github.com/apache/arrow/go/arrow/array""
    ""github.com/apache/arrow/go/arrow/memory""
)

func main() {
    builder := array.NewFloat64Builder(memory.DefaultAllocator)
    fmt.Println(builder.Len(), builder.Cap())
    builder.Reserve(44)
    fmt.Println(builder.Len(), builder.Cap())
    builder.Resize(5)
    fmt.Println(builder.Len(), builder.Cap())
    builder.Reserve(44)
    for i := 0; i < 44; i++ {
        builder.Append(0)
    }
    fmt.Println(builder.Len(), builder.Cap())
    builder.Resize(5)
    fmt.Println(builder.Len(), builder.Cap())
}
{code}
It gives the following output:
{code:java}
0 0
0 64
0 32
44 64
44 32
{code}
For whatever reason, the length is not recorded as 5. I understand why the capacity might not be 5, but it does seem like the length should be set to 5 if the array is resized to a length smaller than its current capacity.",pull-request-available,['Go'],ARROW,Bug,Major,2018-10-25 21:45:40,13
13193884,[C++] Add interface to turn stl_allocator into arrow::MemoryPool,"We already support constructing an {{stl_allocator}}from a {{MemoryPool}}, we should also support the reverse conversion. As the STL allocator does not provide a resize, this will always reallocate, even if there is support for it.",pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-24 14:17:54,8
13193782,[Python] flake8 fails on Crossbow,"It starts failing with flake8 3.6.0 (3.5.0 is fine):

{code}
+python3 -m flake8 --count /home/travis/build/apache/arrow/dev/tasks
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:289:12: W605 invalid escape sequence '\/'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:289:17: W605 invalid escape sequence '\/'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:289:22: W605 invalid escape sequence '\/'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:289:27: W605 invalid escape sequence '\/'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:289:29: W605 invalid escape sequence '\.'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:289:35: W605 invalid escape sequence '\.'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:304:11: W605 invalid escape sequence '\w'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:304:13: W605 invalid escape sequence '\/'
/home/travis/build/apache/arrow/dev/tasks/crossbow.py:304:22: W605 invalid escape sequence '\d'
9
{code}
",pull-request-available,"['Continuous Integration', 'Developer Tools', 'Python']",ARROW,Bug,Major,2018-10-24 08:21:47,14
13193714,[Rust] Release 0.11.0,"We need to update the release version in the Cargo.toml file and then release to crates.io once this has been merged to master.

I will also document the release process as part of the PR.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-10-23 23:40:17,10
13193507,[R] CI builds failing due to GitHub API rate limits,"Could be due to other GitHub issues of late. [~romainfrancois] [~javierluraschi] could you have a look?

https://travis-ci.org/apache/arrow/jobs/445003873#L2325",ci-failure pull-request-available,['R'],ARROW,Bug,Major,2018-10-23 09:35:43,14
13193488,[Python] Get BinaryArray value as zero copy memory view,"If users are embedding other data structures in a {{BinaryArray}}, they cannot access these values in Python without copying

see https://lists.apache.org/thread.html/7c70a1f84a65c0b10cfb84d0b113c85cbed878937087d6676776b465@%3Cdev.arrow.apache.org%3E",pull-request-available,['Python'],ARROW,New Feature,Major,2018-10-23 08:36:15,2
13193263,[Python] Segmentation fault when converting empty table to pandas with categoricals,"{code:java}
import pyarrow as pa


table = pa.Table.from_arrays(arrays=[pa.array([], type=pa.int32())], names=['col'])
table.to_pandas(categories=['col']){code}
This produces a segmentation fault for certain types (e.g, int\{32,64}) while it works for others (e.g. string, binary).",pull-request-available,['Python'],ARROW,Bug,Major,2018-10-22 13:52:04,13
13193108,[Python/Java]Create RecordBatch from VectorSchemaRoot,"Besides the naming differences, {{pyarrow.RecordBatch}} is content-wise the same as a {{org.apache.arrow.vector.VectorSchemaRoot}}. This adds a conversion function to create a {{pyarrow.RecordBatch}} referencing these arrays.",pull-request-available,"['Java', 'Python']",ARROW,New Feature,Major,2018-10-21 14:17:55,8
13193068,[Gandiva][C++] Build error with g++ 8.2.0,"Error message1:
{noformat}
In file included from /home/kou/work/cpp/arrow.kou/cpp/src/gandiva/expr_decomposer.cc:27:
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:46:27: error: 'function' in namespace 'std' does not name a template type
   using maker_type = std::function<Status(const FunctionNode&, FunctionHolderPtr*)>;
                           ^~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:46:22: note: 'std::function' is defined in header '<functional>'; did you forget to '#include <functional>'?
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:30:1:
+#include <functional>
 
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:46:22:
   using maker_type = std::function<Status(const FunctionNode&, FunctionHolderPtr*)>;
                      ^~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:52: error: 'maker_type' was not declared in this scope
   using map_type = std::unordered_map<std::string, maker_type>;
                                                    ^~~~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:52: note: suggested alternative: 'decltype'
   using map_type = std::unordered_map<std::string, maker_type>;
                                                    ^~~~~~~~~~
                                                    decltype
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:62: error: template argument 2 is invalid
   using map_type = std::unordered_map<std::string, maker_type>;
                                                              ^
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:47:62: error: template argument 5 is invalid
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:60:10: error: 'map_type' does not name a type; did you mean 'iswctype'?
   static map_type& makers() {
          ^~~~~~~~
          iswctype
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h: In static member function 'static gandiva::Status gandiva::FunctionHolderRegistry::Make(const string&, const gandiva::FunctionNode&, gandiva::FunctionHolderPtr*)':
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:51:18: error: 'makers' was not declared in this scope
     auto found = makers().find(name);
                  ^~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/function_holder_registry.h:51:18: note: suggested alternative: 'Make'
     auto found = makers().find(name);
                  ^~~~~~
                  Make
{noformat}

Error message2:

{noformat}
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc: In static member function 'static gandiva::NodePtr gandiva::TreeExprBuilder::MakeNull(gandiva::DataTypePtr)':
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:78:70: error: 'float_t' was not declared in this scope
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((float_t)0), true);
                                                                      ^~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:78:70: note: suggested alternative: 'float'
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((float_t)0), true);
                                                                      ^~~~~~~
                                                                      float
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:80:70: error: 'double_t' was not declared in this scope
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((double_t)0), true);
                                                                      ^~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/gandiva/tree_expr_builder.cc:80:70: note: suggested alternative: 'double'
       return std::make_shared<LiteralNode>(data_type, LiteralHolder((double_t)0), true);
                                                                      ^~~~~~~~
                                                                      double
{noformat}",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Bug,Major,2018-10-20 22:41:47,1
13193055,[Release] Address spurious Apache RAT failures in source release script,"Something about the untarring / retarring process is offending Apache RAT and it's reporting a number of unapproved licenses incorrectly

https://gist.github.com/wesm/490dc50c494cc914ae28b4fd897a73eb",pull-request-available,['Developer Tools'],ARROW,Bug,Blocker,2018-10-20 18:26:41,1
13192844,[Python]Pin tensorflow to 1.11.0 in manylinux1 container,Just enough to get {{pyarrow}}in a releasable state.,pull-request-available,['Python'],ARROW,Task,Major,2018-10-19 13:23:50,8
13192658,[Python] Set language_level in Cython sources,"Cython 0.29.0 emits the following warning:
{code}
C:\Miniconda36-x64\envs\arrow\lib\site-packages\Cython\Compiler\Main.py:367: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\projects\arrow\python\pyarrow\_parquet.pxd
{code}

We should probably try to switch it to Python 3.",pull-request-available,['Python'],ARROW,Improvement,Minor,2018-10-18 18:02:56,2
13192653,[CI] Disable optimizations on Windows,"Disabling compiler optimizations, even in release mode, should allow builds to become a bit faster.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Minor,2018-10-18 17:35:58,2
13192531,[C++] Use kUnknownNullCount in NumericArray constructor,"Currently, the default value in the NumericArray constructor for the null_count is 0.

I wonder wether it would be better to usekUnknownNullCount instead? A user could still choose to supply a null_count of 0, or a nullptr as bitmask which would imply a null_count of 0 as well.",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-10-18 12:53:00,6
13192339,[Python] Provide testing setup to verify wheel binaries work in one or more common Linux distributions,"To help catch issues like ARROW-3514: install a candidate wheel in a fresh environment, run Arrow test suite with the installed package",pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-17 18:55:46,3
13192334,[C++/Python] Normalize child/field terminology with StructType,See discussion in https://github.com/apache/arrow/pull/2754,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-10-17 18:34:45,14
13192282,[R] Better support for timestamp format and time zones in R,"See below for original description and reports. In sum, there is a mismatch between how the C++ library and R interpret data without a timezone, and it turns out that we're not passing the timezone to R if it is set in Arrow C++ anyway. 

The [C++ library docs|http://arrow.apache.org/docs/cpp/api/datatype.html#_CPPv4N5arrow13TimestampTypeE] say ""If a timezone-aware field contains a recognized timezone, its values may be localized to that locale upon display; the values of timezone-naive fields must always be displayed as is, with no localization performed on them."" But R's print default, as well as the parsing default, is the current time zone: https://stat.ethz.ch/R-manual/R-devel/library/base/html/strptime.html

The C++ library seems to parse timestamp strings that don't have timezone information as if they are UTC, so when you read timezone-naive timestamps from Arrow and print them in R, they are shifted to be localized to the current timezone. If you print timestamp data from Arrow with {{print(timestamp_var, tz=""GMT"")}} it would look as you expect.

On further inspection, the [arrow-to-vector code for timestamp|https://github.com/apache/arrow/blob/master/r/src/array_to_vector.cpp#L504-L514] doesn't seem to consider time zone information even if it does exist. So we don't have the means currently in R to display timestamp data faithfully, whether or not it is timezone-aware.

Among the tasks here:

* Include the timezone attribute in the POSIXct R vector that gets created from a timestamp Arrow array
* Ensure that timezone-naive data from Arrow is printed in R ""as is"" with no localization 

-----
Original description:

Hello the dream team,

Pasting from[https://github.com/wesm/feather/issues/351]

Thanks for this wonderful package. I was playing with feather and some timestamps and I noticed some dangerous behavior. Maybe it is a bug.

Consider this


{code:java}
import pandas as pd
import feather
import numpy as np
df = pd.DataFrame(
{'string_time_utc' : [pd.to_datetime('2018-02-01 14:00:00.531'), pd.to_datetime('2018-02-01 14:01:00.456'), pd.to_datetime('2018-03-05 14:01:02.200')]}
)
df['timestamp_est'] = pd.to_datetime(df.string_time_utc).dt.tz_localize('UTC').dt.tz_convert('US/Eastern').dt.tz_localize(None)
df
 Out[17]: 
 string_time_utc timestamp_est
 0 2018-02-01 14:00:00.531 2018-02-01 09:00:00.531
 1 2018-02-01 14:01:00.456 2018-02-01 09:01:00.456
 2 2018-03-05 14:01:02.200 2018-03-05 09:01:02.200
{code}
Here I create the corresponding `EST` timestamp of my original timestamps (in `UTC` time).

Now saving the dataframe to `csv` or to `feather` will generate two completely different results.


{code:java}
df.to_csv('P://testing.csv')
df.to_feather('P://testing.feather')
{code}
Switching to R.

Using the good old `csv` gives me something a bit annoying, but expected. R thinks my timezone is `UTC` by default, and wrongly attached this timezone to `timestamp_est`. No big deal, I can always use `with_tz` or even better: import as character and process as timestamp while in R.


{code:java}
> dataframe <- read_csv('P://testing.csv')
 Parsed with column specification:
 cols(
 X1 = col_integer(),
 string_time_utc = col_datetime(format = """"),
 timestamp_est = col_datetime(format = """")
 )
 Warning message:
 Missing column names filled in: 'X1' [1] 
 > 
 > dataframe %>% mutate(mytimezone = tz(timestamp_est))

A tibble: 3 x 4
 X1 string_time_utc timestamp_est 
 <int> <dttm> <dttm> 
 1 0 2018-02-01 14:00:00.530 2018-02-01 09:00:00.530
 2 1 2018-02-01 14:01:00.456 2018-02-01 09:01:00.456
 3 2 2018-03-05 14:01:02.200 2018-03-05 09:01:02.200
 mytimezone
 <chr> 
 1 UTC 
 2 UTC 
 3 UTC  {code}
{code:java}
#Now look at what happens with feather:

 > dataframe <- read_feather('P://testing.feather')
 > 
 > dataframe %>% mutate(mytimezone = tz(timestamp_est))

A tibble: 3 x 3
 string_time_utc timestamp_est mytimezone
 <dttm> <dttm> <chr> 
 1 2018-02-01 09:00:00.531 2018-02-01 04:00:00.531 """" 
 2 2018-02-01 09:01:00.456 2018-02-01 04:01:00.456 """" 
 3 2018-03-05 09:01:02.200 2018-03-05 04:01:02.200 """" {code}
My timestamps have been converted!!! pure insanity. 
 Am I missing something here?

Thanks!!",pull-request-available,['R'],ARROW,Bug,Critical,2018-10-17 15:22:43,4
13192263,[C++] Use unsafe appends when building array from CSV,"When reading a Table from a CSV file, we presize each ArrayBuilder with the right number of elements, so we could unsafe appends. It should make things a small bit faster.",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-10-17 14:26:40,2
13192095,[C++] Fast UTF8 validation functions,"[~lemire] discusses this topic in https://lemire.me/blog/2018/05/16/validating-utf-8-strings-using-as-little-as-0-7-cycles-per-byte/

In Java there is also

https://lemire.me/blog/2018/10/16/validating-utf-8-bytes-java-edition/
",pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-17 01:48:56,2
13191918,[Python] Update zlib library in manylinux1 image,Update to the latest release.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-16 13:19:10,8
13191909,[Python/Documentation] Use sphinx_rtd_theme instead of Bootstrap,"I have got some feedback that the Arrow Python API documentation is a bit confusing as the ToC/Menu is only really visible on the front page. People get confused by the top header. As we are already diverging from the main homepage as this was migrated to a newer bootstrap version anyway, I suggest to change the documentation theme.

As a best practice, I would switch back to the sphinx_rtd_theme as this provides a UX people are used to and happy with. We can customize it if needed later as e.g. dask did: https://github.com/dask/dask-sphinx-theme",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2018-10-16 12:49:45,8
13191897,"[Python] Schema, StructType, StructArray field retrieval by name should raise warning or exception for multiple matches","{code}
ty = pa.struct([
    pa.field('a', pa.int16()),
    pa.field('a', pa.float64())
])
ty['a']
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-16 12:17:53,2
13191896,[Python] Deprecate Schema.field_by_name in favor of __getitem__ ,Similarly like https://github.com/apache/arrow/pull/2754,pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-16 12:14:03,5
13191862,[Ruby] Import Red Parquet,I want to donate Ruby bindings for Apache Parquet GLib. It's developed by me at [https://github.com/red-data-tools/red-parquet] .,pull-request-available,['Ruby'],ARROW,New Feature,Major,2018-10-16 10:01:48,1
13191598,[C++] Implement List Flatten kernel,see also ARROW-45,pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-15 13:48:50,4
13191528,[C++] MinGW 32bit build causes g++ segv,"I'm trying to build MSYS2 packages for 32bit and 64bit: [https://github.com/Alexpux/MINGW-packages/pull/4519]



64bit build works well.



But 32bit build causes g++ segv: [https://dev.azure.com/msys2/mingw/_build/results?buildId=280&view=logs]


{code}
2018-10-11T00:51:09.6355581Z In file included from D:/a/1/s/mingw-w64-arrow/src/apache-arrow-0.11.0/cpp/src/arrow/io/memory.cc:29:0:
2018-10-11T00:51:09.6357210Z D:/a/1/s/mingw-w64-arrow/src/apache-arrow-0.11.0/cpp/src/arrow/util/memory.h: In substitution of 'template<class Function, class ... Args, class Result> std::future<Result> arrow::internal::ThreadPool::Submit(Function&&, Args&& ...) [with Function = <missing>; Args = <missing>; Result = <missing>]':
2018-10-11T00:51:09.6381001Z D:/a/1/s/mingw-w64-arrow/src/apache-arrow-0.11.0/cpp/src/arrow/util/memory.h:63:72:   required from here
2018-10-11T00:51:09.7514314Z D:/a/1/s/mingw-w64-arrow/src/apache-arrow-0.11.0/cpp/src/arrow/util/memory.h:63:72: internal compiler error: Segmentation fault
2018-10-11T00:51:09.7536742Z                                        left + i * chunk_size, chunk_size));
2018-10-11T00:51:09.7537294Z                                                                         ^
2018-10-11T00:51:09.7537623Z 
2018-10-11T00:51:09.7537778Z This application has requested the Runtime to terminate it in an unusual way.
2018-10-11T00:51:09.7537913Z Please contact the application's support team for more information.
2018-10-11T00:51:09.7538417Z 
2018-10-11T00:51:09.7538550Z D:/a/1/s/mingw-w64-arrow/src/apache-arrow-0.11.0/cpp/src/arrow/util/memory.h:63:72: internal compiler error: Aborted
2018-10-11T00:51:09.7538640Z 
2018-10-11T00:51:09.7538764Z This application has requested the Runtime to terminate it in an unusual way.
2018-10-11T00:51:09.7539047Z Please contact the application's support team for more information.
2018-10-11T00:51:09.7539167Z g++.exe: internal compiler error: Aborted (program cc1plus)
2018-10-11T00:51:09.7539287Z Please submit a full bug report,
2018-10-11T00:51:09.7539385Z with preprocessed source if appropriate.
2018-10-11T00:51:09.7539504Z See <https://sourceforge.net/projects/msys2> for instructions.
2018-10-11T00:51:09.7539615Z make[2]: *** [src/arrow/CMakeFiles/arrow_objlib.dir/build.make:414: src/arrow/CMakeFiles/arrow_objlib.dir/io/memory.cc.obj] Error 4
2018-10-11T00:51:09.7539718Z make[2]: *** Waiting for unfinished jobs....
2018-10-11T00:51:10.7253905Z make[1]: *** [CMakeFiles/Makefile2:357: src/arrow/CMakeFiles/arrow_objlib.dir/all] Error 2
2018-10-11T00:51:10.7284933Z make: *** [Makefile:141: all] Error 2
{code}

It'll be a g++ bug but what should we do? Should we stop to use the code with MinGW 32bit build?",pull-request-available,['C++'],ARROW,Bug,Minor,2018-10-15 09:04:09,1
13191477,[Python] zlib deflate exception when writing Parquet file,"The below Python code throws an exception in 0.11.0, but not in 0.10.0.

I was able to reproduce the issue in Amazon Linux, CentOS 7, and Ubuntu 16.04, but not in Windows 7.

The Amazon and CentOS machines are both running zlib 1.2.7, and the Ubuntu machine is using 1.2.8.

Tested with CPython 3.6 in all cases.
{code:python}
import io
import pyarrow
from pyarrow import parquet

tbl = pyarrow.Table.from_arrays([pyarrow.array(['abc', 'def'])], ['some_col'])

f = io.BytesIO()
parquet.write_table(tbl, f, compression='gzip')
{code}

Following is the exception:

{code}
Traceback (most recent call last):
  File ""test_pyarrow.py"", line 8, in <module>
    parquet.write_table(tbl, f, compression='gzip')
  File ""/home/adam/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py"", line 1125, in write_table
    writer.write_table(table, row_group_size=row_group_size)
  File ""/home/adam/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py"", line 376, in write_table
    self.writer.write_table(table, row_group_size=row_group_size)
  File ""pyarrow/_parquet.pyx"", line 934, in pyarrow._parquet.ParquetWriter.write_table
  File ""pyarrow/error.pxi"", line 83, in pyarrow.lib.check_status
pyarrow.lib.ArrowIOError: Arrow error: IOError: zlib deflate failed, output buffer too small
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-10-15 02:23:14,2
13191442,[C++] Inconsistent child accessor naming,"{code}
UnionArray::child
StructArray::field
DataType::child
StructType::child
Schema::field
{code}",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-10-14 10:09:38,15
13191435,[Packaging] Nightly tests for docker-compose images,We need to ensure that the developer containers are working.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-14 08:22:16,3
13190918,[C++] Build jemalloc in parallel,Currently we build the vendored jemalloc with {{make}}. We should use {{make -j}} to build on all available cores.,pull-request-available,['C++'],ARROW,Improvement,Minor,2018-10-11 14:02:51,2
13190868,[CI] Add gandiva to the docker-compose setup,Similarly like the cpp build https://github.com/apache/arrow/blob/master/docker-compose.yml#L33,docker gandiva,['Continuous Integration'],ARROW,Improvement,Major,2018-10-11 10:38:33,8
13190617,[C++]Build with JEMALLOC by default,We already build conda packages and wheels with {{jemalloc}} and we have not had any user complaints about that since a long time. So this is then finally stable and should be used by default.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-10 11:56:41,8
13190522,[Website] Install document for Ubuntu is broken,"[https://lists.apache.org/thread.html/11f0aee1ebde1a011816b84dcd3dca4f7bf14dd397b7531451870f29@%3Cuser.arrow.apache.org%3E]

{quote}
The instructions found here https://arrow.apache.org/install/ don't work.
The /etc/apt/sources.list.d/red-data-tools.list file points to the 'main'
component. The 'main' component only exists for Debian, for Ubuntu it
should be 'universe'.
Seen here: https://packages.red-data-tools.org/ubuntu/dists/bionic/universe
{quote}",pull-request-available,['Website'],ARROW,Bug,Major,2018-10-10 00:03:25,1
13190394,[C++] Int64Builder.Finish(NumericArray<Int64Type>),"I was intuitively thinking that the following code would work:

{{Status s;}}
{{Int64Builder builder;}}
{{s = builder.Append(1);}}
{{s = builder.Append(2);}}

{{std::shared_ptr<NumericArray<Int64Type>> array;}}
{{builder.Finish(&array);}}



However, it does not seem to work, as the finish operation is not overloaded in the Int64 (or the numeric builder).

Would it make sense to add this interface?",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-10-09 15:12:53,6
13190366,[Format] Update Layout.md document to clarify use of 64-bit array lengths,"See https://github.com/apache/arrow/issues/2733. While 64-bit lengths are permitted, it is recommended to limit array sizes to 32-bit length or less. I will update",pull-request-available,['Format'],ARROW,Improvement,Major,2018-10-09 12:51:15,14
13190321,[C++] Row-wise conversion tutorial has fallen out of date,As reported on user@ list,pull-request-available,['C++'],ARROW,Bug,Major,2018-10-09 09:08:10,13
13190121,[Python] Crash when importing tensorflow and pyarrow,We're getting crashes on Travis-CI recently when running the Python test suite with coverage enabled. There is no specific output or traceback unfortunately. Example at https://travis-ci.org/apache/arrow/jobs/438624063#L3571,ci-failure pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Critical,2018-10-08 15:47:53,2
13189866,"[C++] Support CMake 3.2 for ""out of the box"" builds","As reported in the 0.11.0 RC1 release vote, some of our dependencies (like gbenchmark) do not build out of the box with CMake 3.2",pull-request-available,['C++'],ARROW,Bug,Major,2018-10-06 09:08:28,14
13189723,[R] Document mapping of Arrow <-> R types,"Expanded scope to include documenting all types. 

-----
Previous description:

uint8, int16, uint16 -> int32

uint32 -> numeric

int64, uint64 -> ?",pull-request-available,"['Documentation', 'R']",ARROW,New Feature,Major,2018-10-05 13:51:33,4
13189652,[Python] Table.nbytes attribute,"As it says in the title, I think this would be a very handy attribute to have available in Python. You can get it by converting to pandas and using `DataFrame.nbytes` but this is wasteful of both time and memory so it would be good to have this information on the `pyarrow.Table` object itself.

This could be implemented using the [__sizeof__|https://docs.python.org/3/library/sys.html#sys.getsizeof] protocol",pull-request-available,['Python'],ARROW,New Feature,Minor,2018-10-05 07:09:23,5
13189496,"[C++] Use dynamic linking for unit tests, ensure coverage working properly with clang","See experience around this in Apache Kudu
https://github.com/apache/kudu/commit/48799d3fed53c79a5163fd9c4cd9cbae80a9f2d6",pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-04 17:18:25,2
13189448,[Gandiva][C++] Produce fewer test executables,"In ARROW-3254, I am adding the functionality to create test executables from multiple files that use googletest. So we can continue to have relatively small unit test files, but combine unit tests into groups of semantically-related functionality. ",pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Improvement,Major,2018-10-04 14:55:40,14
13189428,[Packaging] Escaped bulletpoints in changelog,See https://github.com/apache/arrow/blob/7940ffe559810fec82cb2fbb0b13f5809cb5fe85/CHANGELOG.md,pull-request-available,['Packaging'],ARROW,Bug,Minor,2018-10-04 13:30:34,3
13189390,"[Gandiva][C++] Configure static linking of libgcc, libstdc++ with LDFLAGS ",This is to create dependency-free binaries for deployment on Linux. Currently this is hard coded but some deployments (e.g. conda) may wish to use the libstdc++ that is available,pull-request-available,"['C++', 'C++ - Gandiva']",ARROW,Bug,Blocker,2018-10-04 10:00:55,14
13189378,[Packaging] Add Apache ORC C++ library to conda-forge,"In the vein of ""toolchain all the things"", it would be useful to be able to obtain the ORC static libraries from a conda package rather than building from source every time",toolchain,['C++'],ARROW,Task,Major,2018-10-04 09:10:31,8
13189355,[GLib] Include Gemfile to archive,It's helpful to setup test environment.,pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-10-04 06:11:28,1
13189354,[Packaging] Add workaround to verify 0.11.0,"tar.gz doesn't include {{c_glib/Gemfile}} that is useful to setup test environment.
It doesn't require to install Arrow GLib.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-04 05:57:58,1
13189332,[Packaging] Add a script to release binaries that use source archive at dist.apache.orgtable bit,In the current release scripts (especially {{dev/release/02-source.sh}}) upload binaries before we upload source archive to dist.apache.org. We need to build and upload binaries after we upload source archive to dist.apache.org to use the source archive at dist.apache.org.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-04 00:10:40,1
13189299,"[C++] Add Windows support, Unix static libs for double-conversion package in conda-forge",I don't think this blocks 0.11 packages but we should build using all toolchain dependencies in 0.12,toolchain,['C++'],ARROW,Task,Major,2018-10-03 20:53:03,2
13189126,[Packaging] Remove RC information from deb/rpm,Because we reuse RC packages as the official release packages when our vote is passed.,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-03 10:36:11,1
13189122,"[C++] Add ""toolchain"" target to ensure that all required toolchain libraries are built","There are cases, such as ARROW-3419, where we need all the external projects to get built",pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-03 10:25:40,14
13189108,"[C++] Fix outstanding include-what-you-use issues in src/arrow, src/parquet codebases","We can address plasma, gandiva in other PRs",pull-request-available,['C++'],ARROW,Bug,Major,2018-10-03 09:34:38,14
13189105,[C++] Update Parquet snapshot version for release,To 1.5.1-SNAPSHOT,pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-03 09:29:51,14
13189092,[Packaging] dev/release/verify-release-cndidate.sh fails Parquet C++ test,"{code}
72: [----------] 5 tests from ReadDecimals/TestArrowReaderAdHocSparkAndHvr
72: [ RUN      ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/0
72: unknown file: Failure
72: C++ exception with description ""Please point the PARQUET_TEST_DATA environment variable to the test data directory"" thrown in the test body.
72: [  FAILED  ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/0, where GetParam() = (""int32_decimal.parquet"", 16-byte object <E0-97 07-64 A9-55 00-00 D0-97 07-64 A9-55 00-00>) (0 ms)
72: [ RUN      ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/1
72: unknown file: Failure
72: C++ exception with description ""Please point the PARQUET_TEST_DATA environment variable to the test data directory"" thrown in the test body.
72: [  FAILED  ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/1, where GetParam() = (""int64_decimal.parquet"", 16-byte object <90-97 07-64 A9-55 00-00 80-97 07-64 A9-55 00-00>) (0 ms)
72: [ RUN      ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/2
72: unknown file: Failure
72: C++ exception with description ""Please point the PARQUET_TEST_DATA environment variable to the test data directory"" thrown in the test body.
72: [  FAILED  ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/2, where GetParam() = (""fixed_length_decimal.parquet"", 16-byte object <D0-94 07-64 A9-55 00-00 C0-94 07-64 A9-55 00-00>) (0 ms)
72: [ RUN      ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/3
72: unknown file: Failure
72: C++ exception with description ""Please point the PARQUET_TEST_DATA environment variable to the test data directory"" thrown in the test body.
72: [  FAILED  ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/3, where GetParam() = (""fixed_length_decimal_legacy.parquet"", 16-byte object <30-80 01-64 A9-55 00-00 20-80 01-64 A9-55 00-00>) (0 ms)
72: [ RUN      ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/4
72: unknown file: Failure
72: C++ exception with description ""Please point the PARQUET_TEST_DATA environment variable to the test data directory"" thrown in the test body.
72: [  FAILED  ] ReadDecimals/TestArrowReaderAdHocSparkAndHvr.ReadDecimals/4, where GetParam() = (""byte_array_decimal.parquet"", 16-byte object <80-CA 02-64 A9-55 00-00 70-CA 02-64 A9-55 00-00>) (0 ms)
72: [----------] 5 tests from ReadDecimals/TestArrowReaderAdHocSparkAndHvr (0 ms total)
{code}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-03 07:47:06,1
13189086,"[Packaging] dev/release/verify-release-cndidate.sh fails in ""conda activate arrow-test""","{code}
+ conda activate arrow-test

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
If your shell is Bash or a Bourne variant, enable conda for the current user with

    $ echo "". /tmp/arrow-0.11.0.a5PLj/apache-arrow-0.11.0/test-miniconda/etc/profile.d/conda.sh"" >> ~/.bashrc

or, for all users, enable conda with

    $ sudo ln -s /tmp/arrow-0.11.0.a5PLj/apache-arrow-0.11.0/test-miniconda/etc/profile.d/conda.sh /etc/profile.d/conda.sh

The options above will permanently enable the 'conda' command, but they do NOT
put conda's base (root) environment on PATH.  To do so, run

    $ conda activate

in your terminal, or to put the base environment on PATH permanently, run

    $ echo ""conda activate"" >> ~/.bashrc

Previous to conda 4.4, the recommended way to activate conda was to modify PATH in
your ~/.bashrc file.  You should manually remove the line that looks like

    export PATH=""/tmp/arrow-0.11.0.a5PLj/apache-arrow-0.11.0/test-miniconda/bin:$PATH""

^^^ The above line should NO LONGER be in your ~/.bashrc file! ^^^
{code}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-03 07:32:35,1
13189071,[Packaging] rat failure in dev/release/02-source.sh,"{code}
NOT APPROVED: cpp/cmake_modules/FindArrow.cmake (apache-arrow-0.11.0/cpp/cmake_modules/FindArrow.cmake): false
NOT APPROVED: python/cmake_modules/FindFlatbuffers.cmake (apache-arrow-0.11.0/python/cmake_modules/FindFlatbuffers.cmake): false
NOT APPROVED: python/cmake_modules/FindLz4.cmake (apache-arrow-0.11.0/python/cmake_modules/FindLz4.cmake): false
NOT APPROVED: python/cmake_modules/FindNumPy.cmake (apache-arrow-0.11.0/python/cmake_modules/FindNumPy.cmake): false
NOT APPROVED: python/cmake_modules/BuildUtils.cmake (apache-arrow-0.11.0/python/cmake_modules/BuildUtils.cmake): false
NOT APPROVED: python/cmake_modules/SnappyCMakeLists.txt (apache-arrow-0.11.0/python/cmake_modules/SnappyCMakeLists.txt): false
NOT APPROVED: python/cmake_modules/FindProtobuf.cmake (apache-arrow-0.11.0/python/cmake_modules/FindProtobuf.cmake): false
NOT APPROVED: python/cmake_modules/FindLLVM.cmake (apache-arrow-0.11.0/python/cmake_modules/FindLLVM.cmake): false
NOT APPROVED: python/cmake_modules/Findjemalloc.cmake (apache-arrow-0.11.0/python/cmake_modules/Findjemalloc.cmake): false
NOT APPROVED: python/cmake_modules/SetupCxxFlags.cmake (apache-arrow-0.11.0/python/cmake_modules/SetupCxxFlags.cmake): false
NOT APPROVED: python/cmake_modules/san-config.cmake (apache-arrow-0.11.0/python/cmake_modules/san-config.cmake): false
NOT APPROVED: python/cmake_modules/FindGLOG.cmake (apache-arrow-0.11.0/python/cmake_modules/FindGLOG.cmake): false
NOT APPROVED: python/cmake_modules/FindRE2.cmake (apache-arrow-0.11.0/python/cmake_modules/FindRE2.cmake): false
NOT APPROVED: python/cmake_modules/FindArrowCuda.cmake (apache-arrow-0.11.0/python/cmake_modules/FindArrowCuda.cmake): false
NOT APPROVED: python/cmake_modules/CompilerInfo.cmake (apache-arrow-0.11.0/python/cmake_modules/CompilerInfo.cmake): false
NOT APPROVED: python/cmake_modules/FindArrow.cmake (apache-arrow-0.11.0/python/cmake_modules/FindArrow.cmake): false
NOT APPROVED: python/cmake_modules/ThirdpartyToolchain.cmake (apache-arrow-0.11.0/python/cmake_modules/ThirdpartyToolchain.cmake): false
NOT APPROVED: python/cmake_modules/FindSnappy.cmake (apache-arrow-0.11.0/python/cmake_modules/FindSnappy.cmake): false
NOT APPROVED: python/cmake_modules/GandivaBuildUtils.cmake (apache-arrow-0.11.0/python/cmake_modules/GandivaBuildUtils.cmake): false
NOT APPROVED: python/cmake_modules/FindZLIB.cmake (apache-arrow-0.11.0/python/cmake_modules/FindZLIB.cmake): false
NOT APPROVED: python/cmake_modules/FindGTest.cmake (apache-arrow-0.11.0/python/cmake_modules/FindGTest.cmake): false
NOT APPROVED: python/cmake_modules/UseCython.cmake (apache-arrow-0.11.0/python/cmake_modules/UseCython.cmake): false
NOT APPROVED: python/cmake_modules/FindZSTD.cmake (apache-arrow-0.11.0/python/cmake_modules/FindZSTD.cmake): false
NOT APPROVED: python/cmake_modules/FindPlasma.cmake (apache-arrow-0.11.0/python/cmake_modules/FindPlasma.cmake): false
NOT APPROVED: python/cmake_modules/FindBrotli.cmake (apache-arrow-0.11.0/python/cmake_modules/FindBrotli.cmake): false
NOT APPROVED: python/cmake_modules/FindPythonLibsNew.cmake (apache-arrow-0.11.0/python/cmake_modules/FindPythonLibsNew.cmake): false
NOT APPROVED: python/cmake_modules/FindThrift.cmake (apache-arrow-0.11.0/python/cmake_modules/FindThrift.cmake): false
NOT APPROVED: python/cmake_modules/FindParquet.cmake (apache-arrow-0.11.0/python/cmake_modules/FindParquet.cmake): false
NOT APPROVED: python/cmake_modules/FindClangTools.cmake (apache-arrow-0.11.0/python/cmake_modules/FindClangTools.cmake): false
NOT APPROVED: python/cmake_modules/FindInferTools.cmake (apache-arrow-0.11.0/python/cmake_modules/FindInferTools.cmake): false
NOT APPROVED: python/cmake_modules/FindGFlags.cmake (apache-arrow-0.11.0/python/cmake_modules/FindGFlags.cmake): false
NOT APPROVED: python/cmake_modules/FindGBenchmark.cmake (apache-arrow-0.11.0/python/cmake_modules/FindGBenchmark.cmake): false
NOT APPROVED: python/cmake_modules/FindGPerf.cmake (apache-arrow-0.11.0/python/cmake_modules/FindGPerf.cmake): false
NOT APPROVED: python/cmake_modules/SnappyConfig.h (apache-arrow-0.11.0/python/cmake_modules/SnappyConfig.h): false
NOT APPROVED: c_glib/doc/arrow-glib/arrow-glib.types (apache-arrow-0.11.0/c_glib/doc/arrow-glib/arrow-glib.types): false
NOT APPROVED: c_glib/doc/arrow-glib/arrow-glib-sections.txt (apache-arrow-0.11.0/c_glib/doc/arrow-glib/arrow-glib-sections.txt): false
NOT APPROVED: c_glib/doc/arrow-glib/arrow-glib-overrides.txt (apache-arrow-0.11.0/c_glib/doc/arrow-glib/arrow-glib-overrides.txt): false
{code}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-10-03 06:16:38,1
13188963,[C++][Dataset] Streaming CSV reader interface for memory-constrainted environments,"CSV reads are currently all-or-nothing. If the results of parsing a CSV file do not fit into memory, this can be a problem. I propose to define a streaming {{RecordBatchReader}} interface so that the record batches produced by reading can be written out immediately to a stream on disk, to be memory mapped later",dataset pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-02 20:09:48,2
13188941,[C++] Add streaming compression interfaces,Currently the compression and decompression methods offered in {{arrow/util/compression.h}} are one-shot. We also need to expose streaming compressor and decompressor interfaces.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-02 18:30:07,2
13188933,[C++] Add option to CSV reader to dictionary encode individual columns or all string / binary columns,"For many datasets, dictionary encoding everything can result in drastically lower memory usage and subsequently better performance in doing analytics

One difficulty of dictionary encoding in multithreaded conversions is that ideally you end up with one dictionary at the end. So you have two options:

* Implement a concurrent hashing scheme -- for low cardinality dictionaries, the overhead associated with mutex contention will not be meaningful, for high cardinality it can be more of a problem

* Hash each chunk separately, then normalize at the end

My guess is that a crude concurrent hash table with a mutex to protect mutations and resizes is going to outperform the latter",csv dataset pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-02 17:48:31,2
13188932,[C++] Add UTF8 conversion modes in CSV reader conversion options,"There should be a few options:

* Assume UTF8, but do not verify (""no seatbelts mode"", for users that have reasonable security about UTF8 and want the maximum performance)

* Full UTF8 verification

* Maybe ASCII-only verification (because ASCII verification is very fast)",csv pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-02 17:44:35,2
13188889,[Python] Document CSV reader,"We should document the Python CSV reader, or at least auto-document the various classes and functions.

Perhaps we should first wait for the API to stabilize.",pull-request-available,"['Documentation', 'Python']",ARROW,Bug,Major,2018-10-02 15:24:09,2
13188884,[C++] Make CSV chunker faster,"Currently the CSV chunker can be the bottleneck in multi-threaded reads (starting from 6 threads, according to my experiments). One way to make it faster is to consider by default that CSV values cannot contain newline characters (overridable via a setting), and then simply search for the last newline character in each block of data.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-02 15:13:49,2
13188855,[Website] Source tarball link missing from install page,This can be seen on http://arrow.apache.org/install/,pull-request-available,['Website'],ARROW,Bug,Major,2018-10-02 13:17:22,3
13188589,[C++/Python]Add docker container for linting,Add a docker container that runs clang-format and flake8 checks.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-10-01 17:03:12,8
13188579,[Python]Support filters in disjunctive normal form in ParquetDataset,This allows us to represent any boolean predicate.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-10-01 16:28:49,8
13188537,[C++] Function to cast binary to string/utf8 with UTF8 validation,This will be useful to have,pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-01 14:38:52,13
13188498,[C++] Implement InputStream for bz2 files,For reading compressed CSV files,csv pull-request-available,['C++'],ARROW,New Feature,Major,2018-10-01 12:02:41,2
13188497,[Python] Support reading CSV files and more from a gzipped file,Requires ARROW-1019,pull-request-available,['Python'],ARROW,New Feature,Major,2018-10-01 12:02:08,2
13188468,[C++] Add double-conversion to cpp/thirdparty/download_dependencies.sh,For offline builds,pull-request-available,['C++'],ARROW,Improvement,Major,2018-10-01 08:55:46,3
13188441,[Rust] Remove memory_pool.rs,A while back we approved a PR to add a custom memory pool but it isn't actually used. Rust has other mechanisms now for specifying custom memory allocators so I think we should remove this unused code.,pull-request-available,['Rust'],ARROW,Improvement,Trivial,2018-10-01 04:17:01,10
13188437,[Python] Dictionary has out-of-bound index when creating DictionaryArray from Pandas with NaN,"Minimal reproducer:

{code:python}
import pandas as pd
import pyarrow as pa

pa.array(pd.Categorical(['a', 'b', 'c'], categories=['a', 'b']))
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-10-01 00:51:48,14
13188414,[C++] Introduce SlicedBuffer class,"The purpose of this class will be to forward certain function calls to the parent buffer, like a request for the device (CPU, GPU, etc.).

As a result of this, we can remove the {{parent_}} member from {{Buffer}} as that member is only there to support slices. ",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-30 18:04:52,14
13188392,[Packaging] Centos 6 build is failing,"See build: https://travis-ci.org/kszucs/crossbow/builds/435068108

cc [~kou]",pull-request-available,['Packaging'],ARROW,Bug,Major,2018-09-30 11:56:55,1
13188352,[Packaging] Wheel builds are failing due to wheel 0.32 release,"Recent update of multibuild: https://github.com/matthew-brett/multibuild/commit/f6128176d90792ba572921c447325130666cb950
Causes build error: https://travis-ci.org/kszucs/crossbow/builds/435068100",pull-request-available,['Packaging'],ARROW,Bug,Major,2018-09-29 19:50:11,3
13188349,[Integration/CI/Python] Add dask integration test to docker-compose setup,Introduced by https://github.com/apache/arrow/pull/2572,pull-request-available,"['Continuous Integration', 'Integration', 'Python']",ARROW,Improvement,Major,2018-09-29 19:32:26,3
13188345,[Doc] Document docker compose setup,Introduced by https://github.com/apache/arrow/pull/2572,pull-request-available,['Documentation'],ARROW,Improvement,Major,2018-09-29 19:25:02,13
13188308,[R] Run cpp/build-support/cpplint.py on C++ source files,This will help with additional code cleanliness,pull-request-available,['R'],ARROW,Improvement,Major,2018-09-29 09:22:48,14
13188307,[GLib] Import Parquet bindings,This is undergoing IP clearance at the moment,pull-request-available,['GLib'],ARROW,New Feature,Major,2018-09-29 09:18:17,1
13188247,[Python] Document parameters of Table.to_pandas method,These are not present in the docstring,pull-request-available,['Python'],ARROW,Improvement,Major,2018-09-28 21:34:34,3
13188123,[Packaging] Build python 3.7 wheels,Follow-up of https://github.com/apache/arrow/pull/2462/files,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-09-28 10:07:41,8
13188122,[Packaging] Fix recently failing wheel builds,Recent crossbow builds for wheel-win and wheel-linux are failing: https://github.com/kszucs/crossbow/branches/all?utf8=%E2%9C%93&query=nightly-56,pull-request-available,['Packaging'],ARROW,Task,Critical,2018-09-28 10:00:35,3
13187904,[Python] test_plasma.py fails (in test_plasma_list),"I routinely get the following failure in {{test_plasma.py}}:
{code}
Traceback (most recent call last):
  File ""/home/antoine/arrow/python/pyarrow/tests/test_plasma.py"", line 825, in test_plasma_list
    assert l3[v][""ref_count""] == 1
AssertionError: assert 0 == 1
-------------------------------- Captured stderr call ---------------------------------
../src/plasma/store.cc:926: Allowing the Plasma store to use up to 0.1GB of memory.
../src/plasma/store.cc:956: Starting object store with directory /dev/shm and huge page support disabled
{code}

I'm not sure whether there's something wrong in my setup (on Ubuntu 18.04, x86-64), or it's a genuine bug.",pull-request-available,"['C++ - Plasma', 'Python']",ARROW,Bug,Major,2018-09-27 12:58:23,2
13187789,[Python] Crash when schema and columns do not match,"I get a segfault when the arrays do not match the schema. Running latest from HEAD ee9b1ba426e2f1f117cde8d8f4ba6fbe3be5674c

{code:python}

sch = pa.schema([
  pa.field('a', pa.int64()),
  pa.field('b', pa.string()),
])
rb = pa.RecordBatch.from_arrays([pa.array([1])], ['a'])
t = pa.Table.from_batches([rb], sch)

{code}


In[6]: t = pa.Table.from_batches([rb], sch)
[mine:11188] *** Process received signal ***
[mine:11188] Signal: Segmentation fault (11)
[mine:11188] Signal code: Address not mapped (1)
[mine:11188] Failing at address: 0x49
[mine:11188] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x128e0)[0x7f89d7a578e0]
[mine:11188] [ 1] /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.10(_ZNK5arrow17SimpleRecordBatch6columnEi+0x48)[0x7f896d651548]
[mine:11188] [ 2] /usr/local/lib/python3.6/dist-packages/pyarrow/libarrow.so.10(_ZN5arrow5Table17FromRecordBatchesERKSt10shared_ptrINS_6SchemaEERKSt6vectorIS1_INS_11RecordBatchEESaIS8_EEPS1_IS0_E+0x48a)[0x7f896d65771a]
[mine:11188] [ 3] /usr/local/lib/python3.6/dist-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x13e98c)[0x7f896dd8a98c]
[mine:11188] [ 4] /usr/bin/python3.6[0x50c3e5]
[mine:11188] [ 5] /usr/bin/python3.6(_PyEval_EvalFrameDefault+0x449)[0x50fad9]
[mine:11188] [ 6] /usr/bin/python3.6[0x50dee7]
[mine:11188] [ 7] /usr/bin/python3.6[0x51b2ea]",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-09-27 03:48:39,3
13187696,[Python] Add ccache to manylinux1 container,Should make the recompilation steps a lot faster.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2018-09-26 18:32:51,8
13187339,[Python] manylinux container confusing,"The layout of the Docker container for manylinux builds is a bit confusing and error-prone. The Arrow source code is present both in {{/arrow}} and {{/io/arrow}}, and it's easy to get the two diverging if you're not careful.",pull-request-available,['Python'],ARROW,Bug,Major,2018-09-25 15:49:26,8
13187305,[Python] Expose stream alignment function in pyarrow.NativeFile,See also ARROW-3319,pull-request-available,['Python'],ARROW,New Feature,Major,2018-09-25 13:54:17,14
13187263,[Python] Support reading Parquet binary/string columns directly as DictionaryArray,"Requires PARQUET-1324 and probably quite a bit of extra work  

Properly implementing this will require dictionary normalization across row groups. When reading a new row group, a fast path that compares the current dictionary with the prior dictionary should be used. This also needs to handle the case where a column chunk ""fell back"" to PLAIN encoding mid-stream",parquet pull-request-available,['Python'],ARROW,Improvement,Major,2018-09-25 11:23:29,14
13187257,[Parquet] Free more internal resources when writing multiple row groups,"See:

* https://github.com/apache/arrow/issues/2614
* https://github.com/apache/arrow/issues/2624

",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-09-25 11:11:02,14
13187046,[CI] Rust job always runs on AppVeyor,"... even if no Rust-affecting files are modified. Looks like the job name was changed but the detection script wasn't updated.

See e.g. [https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/build/1.0.8087/job/oxnuhwld3yb7mvsv]",pull-request-available,"['Continuous Integration', 'Rust']",ARROW,Bug,Major,2018-09-24 14:14:32,2
13187027,[C++] Improve integer parsing performance,"According to the {{number-parsing-benchmark}}, parsing integers from strings currently runs at around ~5M items/sec. (on Ubuntu 18.04). We should be able to do better. This will be important for CSV parsing performance.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-24 12:41:54,2
13187026,[C++] Improve float parsing performance,"According to the {{number-parsing-benchmark}}, parsing integers from strings currently runs at around ~1.6M items/sec. (on Ubuntu 18.04). We should be able to do better. This will be important for CSV parsing performance.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-24 12:41:50,2
13186997,"[GLib] Expose AlignStream methods in InputStream, OutputStream classes","In some cases it may be desireable to write padding in between messages to obtain 8- or 64-byte alignment. After ARROW-3212, the IPC metadata is deterministically-sized, so alignment is up to the user",pull-request-available,['GLib'],ARROW,Improvement,Major,2018-09-24 11:50:37,1
13186989,[C++] Convenience method for reading all batches from an IPC stream or file as arrow::Table,This is being implemented more than once in binding layers,pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-24 10:39:08,14
13186983,"[R] Run clang-format, cpplint checks on R C++ code",Comment to ARROW-3282,pull-request-available,['R'],ARROW,Improvement,Major,2018-09-24 10:28:18,14
13186903,[C++] Enable example arrays to be written with a simplified JSON representation,"In addition to making it easier to generate random data as described in ARROW-2329, I think it would be useful to reduce some of the boilerplate associated with writing down explicit test cases. The benefits of this will be especially pronounced when writing nested arrays. 

Example code that could be improved this way:

https://github.com/apache/arrow/blob/master/cpp/src/arrow/array-test.cc#L3271

Rather than having a ton of hand-written assertions, we could compare with the expected true dataset. Of course, this itself has to be tested endogenously, but I think we can write enough tests for the JSON parser bit to be able to have confidence in tests that are written with it",pull-request-available,['C++'],ARROW,New Feature,Major,2018-09-23 19:37:22,2
13186826,[Website] Update Jekyll and Bootstrap 4,Update to Bootstrap version 4,pull-request-available,['Website'],ARROW,Improvement,Major,2018-09-22 17:40:34,8
13186809,[Release] Update .deb package names in preparation,We need to use libarrow${SO_VERSION} package name for .deb.,pull-request-available,['Packaging'],ARROW,Improvement,Minor,2018-09-22 13:56:41,1
13186714,[C++] Appveyor builds failing,This is my fault for merging https://github.com/apache/arrow/commit/b254d34d156bacac5938da9c16f7bb89882c3a7a without waiting for the build to run. It seems to have broken the MSVC build,pull-request-available,['C++'],ARROW,Bug,Blocker,2018-09-21 20:22:54,14
13186689,[C++] Convenience API for constructing arrow::io::BufferReader from std::string,"See motivating code example:

https://github.com/apache/arrow/commit/db0ef22dd68ae00e11f09da40b6734c1d9770b57#diff-6dc1b0b53e71627dfb98c60b1fd2d45cR39",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-21 18:31:29,14
13186686,[C++] Implement DoPut command for Flight on client and server side  ,This was omitted from ARROW-3146,flight pull-request-available,['C++'],ARROW,New Feature,Major,2018-09-21 18:24:02,0
13186678,"[C++] ""redeclared without dllimport attribute after being referenced with dll linkage"" with MinGW","The following warning is reported by MinGW when we build Arrow GLib with Arrow C++:

{code}
../apache-arrow-0.11.0/c_glib/../cpp/src/arrow/status.h:265:8: warning: 'arrow::Status::Status(arrow::Status&&)' redeclared without dllimport attribute after being referenced with dll linkage
 inline Status::Status(Status&& s) noexcept : state_(s.state_) \{ s.state_ = NULL; }
 ^~~~~~
{code}",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-09-21 17:59:25,1
13186673,[GLib] Add arrow_cpp_build_type and arrow_cpp_build_dir Meson options,It's for building Arrow GLib without installing Arrow C++. GNU Autotools build system already has these options.,pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-09-21 17:48:06,1
13186361,[C++] Allow linking Arrow tests dynamically on Windows,"On Windows, C++ modules are compiled once for each library kind (static, shared). This means we do twice the work on e.g. AppVeyor. We should be able to link the Arrow tests with the Arrow DLL instead, at least on Windows.

Things are a bit more complicated for Parquet because of PARQUET-1420.",pull-request-available,['C++'],ARROW,Bug,Major,2018-09-20 14:06:54,2
13186172,[Python] Add documentation about inspecting Parquet file metadata,"This functionality is available, but not documented in http://arrow.apache.org/python",parquet,['Python'],ARROW,Improvement,Major,2018-09-19 21:30:49,5
13186168,[Packaging] Missing glog dependency from conda-forge recipes,"GLog dependency has introduced via ARROW-3187: [C++] Add support for using glog (Google logging library)  and the latest nightly conda builds are missing it.

See https://github.com/kszucs/crossbow/branches/all?utf8=%E2%9C%93&query=nightly-48-conda

",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-09-19 21:09:20,3
13186076,[Python] Fix warnings in unit test suite,"Observed in recent appveyor build https://ci.appveyor.com/project/pitrou/arrow/build/job/c6b3pslhqag1gnw7

{code}
============================== warnings summary ===============================
<unknown>:1396: DeprecationWarning: invalid escape sequence \(
<unknown>:1401: DeprecationWarning: invalid escape sequence \(
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\formatting.py:32: FutureWarning: array_format is deprecated, use Array.format() instead
  FutureWarning)
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\formatting.py:32: FutureWarning: array_format is deprecated, use Array.format() instead
  FutureWarning)
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\formatting.py:32: FutureWarning: array_format is deprecated, use Array.format() instead
  FutureWarning)
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
source:1396: DeprecationWarning: invalid escape sequence \(
source:1401: DeprecationWarning: invalid escape sequence \(
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: FutureWarning: In the future, NAT != NAT will be True rather than False.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\tests\test_convert_pandas.py:2076: RuntimeWarning: invalid value encountered in sqrt
  _check_series(pd.Series([np.sqrt(-1)] * 3, dtype=object))
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\tests\test_ipc.py:298: FutureWarning: Please use read_next_batch instead of get_next_batch
  reader.get_next_batch()
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: FutureWarning: In the future, NAT != NAT will be True rather than False.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: FutureWarning: In the future, NAT != NAT will be True rather than False.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pandas\core\dtypes\missing.py:431: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if left_value != right_value:
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\tests\test_parquet.py:1526: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the
'datetime.date' is coerced to a datetime. In the future pandas will
not coerce, and 'the values will not compare equal to the
'datetime.date'. To retain the current behavior, convert the
'datetime.date' to a datetime with 'pd.Timestamp'.
  predicate &= df[name] == value
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\tests\test_parquet.py:1974: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  io.BytesIO(expected_string), sep=r'\s{2,}', index_col=None, header=0
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\tests\test_parquet.py:1996: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  sep=r'\s{2,}', index_col=['cut', 'color', 'clarity'], header=0
c:\miniconda36-x64\envs\wheel_test\lib\site-packages\pyarrow\tests\test_parquet.py:2019: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  sep=r'\s{2,}', index_col=['cut', 'color', 'clarity'], header=0
-- Docs: https://docs.pytest.org/en/latest/warnings.html
====== 1003 passed, 93 skipped, 5 xfailed, 32 warnings in 19.77 seconds =======
(wheel_test) C:\projects\arrow>set lastexitcode=0 
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-09-19 14:33:01,3
13186067,[CI] Reduce conda times on AppVeyor,It is frequent on AppVeyor that actual building starts only 10 minutes after the job started. Most of the time before that is spent on {{conda}} operations.,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-09-19 13:58:31,2
13186008,[Python] Create empty table from schema,"When one knows the expected schema for its input data but has no input data for a data pipeline, it is necessary to construct an empty table as a sentinel value to pass through.

This is a small but often useful convenience function.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-09-19 09:30:18,8
13185826,[Python] Implement __getitem__ with integers on pyarrow.Column,This would improve interactive usability,pull-request-available usability,['Python'],ARROW,Improvement,Major,2018-09-18 15:07:14,3
13185825,"[Python] Add ""field"" method to select fields from StructArray",This would improve usability. ,pull-request-available usability,['Python'],ARROW,Improvement,Major,2018-09-18 15:04:08,3
13185769,[CI] Make linting a separate job,"When pushing changes I get routinely frustrated because Travis-CI fails early on a linting failure. It would be nice if linting (Python, C++) was a separate job so as not to disrupt the development workflow so much.",pull-request-available,"['C++', 'Continuous Integration', 'Python']",ARROW,Wish,Major,2018-09-18 10:26:37,2
13185713,[GLib] CI is failued on macOS,"{code}
==> Installing postgis dependency: numpy
==> Downloading https://homebrew.bintray.com/bottles/numpy-1.15.1.sierra.bottle.tar.gz
==> Pouring numpy-1.15.1.sierra.bottle.tar.gz
Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink lib/python2.7/site-packages/numpy/__config__.py
Target /usr/local/lib/python2.7/site-packages/numpy/__config__.py
already exists. You may want to remove it:
 rm '/usr/local/lib/python2.7/site-packages/numpy/__config__.py'

To force the link and overwrite all conflicting files:
 brew link --overwrite numpy

To list all files that would be deleted:
 brew link --overwrite --dry-run numpy
{code}",pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-09-18 05:53:45,1
13185690,[C++] Stop to use IMPORTED_LINK_INTERFACE_LIBRARIES,"Because it's deprecated in CMake 3.2 that is the minimum required
version:

https://cmake.org/cmake/help/v3.2/prop_tgt/IMPORTED_LINK_INTERFACE_LIBRARIES.html

The document says that we should use INTERFACE_LINK_LIBRARIES:

https://cmake.org/cmake/help/v3.2/prop_tgt/INTERFACE_LINK_LIBRARIES.html",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-09-18 01:49:24,1
13185597,[C++] Add option to ADD_ARROW_TEST to compose a test executable from multiple .cc files containing unit tests,"Currently there is a 1-1 correspondence between a .cc file containing unit tests to a test executable. There's good reasons (like readability, code organization) to split up a large test suite among many files. But there are downsides:

* Linking test executables is slow, especially on Windows
* Test executables take up quite a bit of space (the debug/ directory on Linux after a full build is ~1GB)

I suggest enabling ADD_ARROW_TEST to accept a list of files which will be build together into a single test. This will allow us to combine a number of our unit tests and save time and space",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-17 18:09:36,2
13185568,"[C++] Do not hard code the ""v"" part of versions in thirdparty toolchain","When I changed Flatbuffers from ""v1.8.0"" to a git hash, it broke the dependency download script. We should move all the version string to versions.txt rather than having some ""v${FOO_URL}"" in ThirdpartyToolchain.cmake",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-17 16:29:21,3
13185567,[C++] Conversion warnings in cast.cc,"This is with gcc 7.3.0 and {{-Wconversion}}.

{code}
../src/arrow/compute/kernels/cast.cc: In instantiation of void arrow::compute::CastFunctor<O, I, typename std::enable_if<arrow::compute::is_float_truncate<O, I>::value>::type>::operator()(arrow::compute::FunctionContext*, const arrow::compute::CastOptions&, const arrow::ArrayData&, arrow::ArrayData*) [with O = arrow::Int64Type; I = arrow::DoubleType; typename std::enable_if<arrow::compute::is_float_truncate<O, I>::value>::type = void]:
../src/arrow/compute/kernels/cast.cc:1105:1:   required from here
../src/arrow/compute/kernels/cast.cc:291:45: warning: conversion to in_type {aka double} from long int may alter its value [-Wconversion]
           if (ARROW_PREDICT_FALSE(out_value != *in_data)) {
                                   ~~~~~~~~~~^~~~~
../src/arrow/util/macros.h:37:50: note: in definition of macro ARROW_PREDICT_FALSE
 #define ARROW_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                                  ^
../src/arrow/compute/kernels/cast.cc:301:45: warning: conversion to in_type {aka double} from long int may alter its value [-Wconversion]
           if (ARROW_PREDICT_FALSE(out_value != *in_data)) {
                                   ~~~~~~~~~~^~~~~
../src/arrow/util/macros.h:37:50: note: in definition of macro ARROW_PREDICT_FALSE
 #define ARROW_PREDICT_FALSE(x) (__builtin_expect(x, 0))
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-09-17 16:26:42,2
13185533,[Python] Run flake8 on integration_test.py and crossbow.py,"We should keep this code clean, too",pull-request-available,['Python'],ARROW,Improvement,Major,2018-09-17 14:12:45,3
13185512,"[C++] Arrow tests should have label ""arrow""","It would help executing only them, not Parquet unit tests which for some reason are quite a bit longer to run.",pull-request-available,['C++'],ARROW,Wish,Major,2018-09-17 13:01:12,14
13185395,[Python][Parquet] direct reading/writing of pandas categoricals in parquet,"Parquet supports ""dictionary encoding"" of column data in a manner very similar to the concept of Categoricals in pandas. It is natural to use this encoding for a column which originated as a categorical. Conversely, when loading, if the file metadata says that a given column came from a pandas (or arrow) categorical, then we can trust that the whole of the column is dictionary-encoded and load the data directly into a categorical column, rather than expanding the labels upon load and recategorising later.

If the data does not have the pandas metadata, then the guarantee cannot hold, and we cannot assume either that the whole column is dictionary encoded or that the labels are the same throughout. In this case, the current behaviour is fine.



(please forgive that some of this has already been mentioned elsewhere; this is one of the entries in the list at[https://github.com/dask/fastparquet/issues/374]as a feature that is useful in fastparquet)",parquet pull-request-available,['Python'],ARROW,Improvement,Minor,2018-09-17 00:00:18,14
13185394,[Python] Infer index and/or filtering from parquet column statistics,"The metadata included in parquet generally gives the min/max of data for each chunk of each column. This allows early filtering out of whole chunks if they do not meet some criterion, and can greatly reduce reading burden in some circumstances. In Dask, we care about this for setting an index and its ""divisions"" (start/stop values for each data partition) and for directly avoiding including some chunks in the graph of tasks to be processed. Similarly, filtering may be applied on the values of fields defined by the directory partitioning.

Currently, dask using the fastparquet backend is able to infer possible columns to use as an index, perform filtering on that index and do general filtering on any column which has statistical or partitioning information. It would be very helpful to have such facilities via pyarrow also.

This is probably the most important of the requests from Dask.

(please forgive that some of this has already been mentioned elsewhere; this is one of the entries in the list at[https://github.com/dask/fastparquet/issues/374]as a feature that is useful in fastparquet)",dataset dataset-parquet-read parquet,['Python'],ARROW,Improvement,Major,2018-09-16 23:55:28,5
13185372,[C++] Use coarser-grained dispatch to SIMD hash functions,"The way that we dispatch to SSE4 hash functions is a remnant from the Impala codebase, which checks CpuInfo on every iteration in debug builds:

https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/hash-util.h#L43

However, the static {{model_name_}} member is causing some non-determinism related to static member lifetime as reported in ARROW-3241. 

I'm proposing to refactor CpuInfo into a singleton pattern and handle SIMD vs non-SIMD dispatch at a higher level rather than at the lowest level like it is now. This should hopefully make the issue in ARROW-3241 go away",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-16 14:57:07,14
13185303,[C++] Improve random data generation functions,"Our various code for generating random datasets for testing and benchmarks is a bit verbose / awkward.

I suggest defining a nicer API so we can write code like:

{code}
auto arr = random::int32(length, seed);
auto arr = random::varbinary(length, max_value_size, seed);
{code}

Since this is only for test code, we can abort if anything returns error Status to make the API more convenient",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-15 13:05:29,13
13185216,[C++] OutputStream bookkeeping logic when writing IPC file format is incorrect,"The {{position_}} field is initialized as -1, but the file writer class begins writing and aligning the format without updating the position, so this results in an extra byte written. This was reported in https://github.com/apache/arrow/issues/2559. A bit disturbing that this was not caught in unit tests",pull-request-available,['C++'],ARROW,Improvement,Blocker,2018-09-14 18:13:40,14
13185098,[Packaging] Update deb names,We need to update deb package name when we update version because we change lib*.so version for each version up.,pull-request-available,['Packaging'],ARROW,Improvement,Minor,2018-09-14 09:34:35,1
13185096,[C++] Link order is wrong when ARROW_ORC=on and ARROW_PROTOBUF_USE_SHARED=ON,"It's ""-lorc -lprotobuf"" but it should be ""-lorc -lprotobuf"".",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-09-14 09:29:19,1
13184906,[Python] Sphinx's autodoc_default_flags is now deprecated,"Warning, treated as error:
autodoc_default_flags is now deprecated. Please use autodoc_default_options instead.

1.8.0 has just been released, see relevant line in changelog https://github.com/sphinx-doc/sphinx/blob/master/CHANGES#L149",pull-request-available,['Python'],ARROW,Bug,Major,2018-09-13 13:37:22,3
13184863,[Packaging]: Adjust wheel package scripts to account for Parquet codebase migration,The parquet-cpp package is not necessary in these builds after ARROW-3075,pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2018-09-13 11:09:07,3
13184714,[Python] Immutability of bytes is ignored,"Creating a pyarrow.Buffer from Python bytes allows in-place changes of immutable Python strings:
{code:java}
>>> import pyarrow as pa
>>> import numpy as np
>>> a = b'123456'
>>> a[0] = 77 # bytes are immutable, so TypeError is expected
TypeError: 'bytes' object does not support item assignment
>>> b = pa.py_buffer(a)  # but with pyarrow bytes can be changed in-place
>>> arr = np.frombuffer(b, dtype=np.uint8)
>>> arr[0] = 66 # change 'a' in-place, would expect error
>>> a
b'B23456'
>>> hash(a)
-4581532003987476523
>>> arr[0] = 77
>>> a
b'M23456'
>>> hash(a) # hash value stays constant while changing 'a'
-4581532003987476523{code}
{{Notice that numpy.frombuffer respects immutability of bytes:}}
{{}}
{code:java}
>>> arr2 = np.frombuffer(a, dtype=np.uint8)
>>> arr2
array([77, 50, 51, 52, 53, 54], dtype=uint8)
>>> arr2[0] = 88 # expecting error
ValueError: assignment destination is read-only{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-09-12 20:33:50,14
13184699,[Python] NativeFile.write shouldn't accept unicode strings,"Arrow files are binary, but for some reason {{NativeFile.write}} silently converts unicode strings to bytes.
{code:python}
>>> b = io.BytesIO()
>>> b.write(""foo"")
Traceback (most recent call last):
  File ""<ipython-input-6-a7195dbc0372>"", line 1, in <module>
    b.write(""foo"")

TypeError: a bytes-like object is required, not 'str'
>>> f = pa.PythonFile(b)
>>> f.write(""foo"")
>>> b.getvalue()
b'foo'
>>> f.write("""")
>>> b.getvalue()
b'foo\xf0\x9f\x98\x80'
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-09-12 19:12:48,14
13184631,[C++/Python] Pandas object conversion of ListType<DateType> and ListType<TimeType>,See conversation: https://github.com/apache/arrow/pull/2535/files/070e97520e3fbd5beae806b887674eeb64fca822#diff-3eedaa9b40f988a467cb90c10c854203R796,pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-09-12 14:35:57,3
13184198,[C++] Create deterministic IPC metadata,"Currently, the amount of padding bytes written after the IPC metadata header depends on the current position of the {{OutputStream}} passed. So if the message begins on an unaligned (not multiple of 8) offset, then the content of the metadata will be different than if it did. This seems like a leaky abstraction -- aligning the stream should probably be handled separately from writing the IPC protocol.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-11 00:48:02,14
13184141,[C++] Rename libarrow_gpu to libarrow_cuda,I'm proposing to rename this library since we could conceivably have OpenCL bindings in the repository also,pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-10 19:19:15,2
13184140,[C++] Segmentation fault when casting dictionary to numeric with nullptr valid_bitmap ,"Steps to reproduce:
 # Create a partitioned dataset with the following code:

```python

import numpy as np

import pandas as pd

import pyarrow as pa

import pyarrow.parquet as pq

df = pd.DataFrame({ 'one': [-1, 10, 2.5, 100, 1000, 1, 29.2], 'two': [-1, 10, 2, 100, 1000, 1, 11], 'three': [0, 0, 0, 0, 0, 0, 0] })

table = pa.Table.from_pandas(df)

pq.write_to_dataset(table, root_path='/home/yingw787/misc/example_dataset', partition_cols=['one', 'two'])

```
 # Create a Parquet file from a PyArrow Table created from the partitioned Parquet dataset:

```python

import pyarrow.parquet as pq

table = pq.ParquetDataset('/path/to/dataset').read()

pq.write_table(table, '/path/to/example.parquet')

```

EXPECTED:
 * Successful write

GOT:
 * Segmentation fault

Issue reference on GitHub mirror:https://github.com/apache/arrow/issues/2511",parquet pull-request-available,['C++'],ARROW,Bug,Major,2018-09-10 19:18:08,13
13184056,[C++] Building with ARROW_HIVESERVER2=ON with unit tests disabled causes error,"Hello

Activate, support for hive, generate exception in CMake:
{quote}_CMake Error at src/arrow/dbi/hiveserver2/[CMakeLists.txt:116|https://github.com/apache/arrow/blob/a42d4bf1b0cef37849be0b019c34c96bf56a62f9/cpp/src/arrow/dbi/hiveserver2/CMakeLists.txt#L116] (set_property):_
 _set_property could not find TARGET hiveserver2-test. Perhaps it has not_
 _yet been created_
{quote}
{code:java}
RUN cmake \ 
  -DARROW_HIVESERVER2=ON \ 
  -DCMAKE_BUILD_TYPE=Release \ 
  -DARROW_BUILD_TESTS=OFF \
   . {code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-09-10 13:46:30,14
13183929,[R] Enable package to be made available on CRAN,"As of[ARROW-1325|https://issues.apache.org/jira/browse/ARROW-1325],the project contains a minimal working R package that takes advantage of R's built-in support for calling C++ code and uses Rcpp for added support.

Though the exact public interface of the package hasn't been developed yet (so this issue may be running before we walk), one major feature in its development will be release to [CRAN|https://cran.r-project.org/], the official package repository for the R language.

Completing this story would mean:
 * Getting source code to state where [R CMD CHECK|https://www.r-bloggers.com/how-i-learned-to-stop-worrying-and-love-r-cmd-check/]passes with 0 ERRORS, 0 WARNINGS, and 0 NOTES
 * Setting up build process for source and precompiled binary packages (see [data.table|https://github.com/Rdatatable/data.table]and[XGBoost|https://github.com/dmlc/xgboost]as examples of packages that do this)
 * Submission to CRAN and acceptance of the first release



Distribution via CRAN would be a much more natural fit for most R users' workflows than thecurrent build-from-source workflow, and comes with all the other benefits of package managers (e.g. version pegging, easy distribution of platform-specific binaries).",pull-request-available,['R'],ARROW,New Feature,Critical,2018-09-10 04:00:01,4
13183901,[C++] Add support for reading Flight streams with dictionaries,"Some work is needed to handle schemas sent separately from their dictionaries, i.e. ARROW-3144. I'm going to punt on implementing support for this in the initial C++ Flight client",flight pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Major,2018-09-09 17:50:07,2
13183826,[Website] Blog post for 0.11 release,This blog post should also explain the Arrow/Parquet C++ monorepo effort,pull-request-available,['C++'],ARROW,New Feature,Major,2018-09-08 14:44:29,14
13183825,[C++] Add instructions to cpp/README.md about Parquet-only development and Arrow+Parquet,There are two distinct development workflows,parquet pull-request-available,['C++'],ARROW,New Feature,Major,2018-09-08 14:31:23,14
13183824,Enable merge_arrow_py.py script to merge Parquet patches and set fix versions,Follow up to ARROW-3075,pull-request-available,['Developer Tools'],ARROW,New Feature,Major,2018-09-08 14:29:22,14
13183707,"[C++] ""WriteableFile"" is misspelled, should be renamed ""WritableFile"" with deprecation for old name",See e.g. https://docs.oracle.com/javase/7/docs/api/java/nio/channels/WritableByteChannel.html,pull-request-available,['C++'],ARROW,Bug,Blocker,2018-09-07 18:42:04,14
13183633,[Python] Table.from_arrays segfaults if lists and schema are passed,"{code:python}
  data = [
 list(range(5)),
 [-10, -5, 0, 5, 10]
 ]

 schema = pa.schema([
 pa.field('a', pa.uint16()),
 pa.field('b', pa.int64())
 ])

 pa.Table.from_arrays(data, schema=schema)
{code}

Whereas it should raise a `TypeError`",pull-request-available,['Python'],ARROW,Bug,Major,2018-09-07 13:25:39,3
13183551,[GLib] mesonbuild failures in Travis CI,"Something started breaking recently with mesonbuild

{code}
+env CFLAGS=-DARROW_NO_DEPRECATED_API CXXFLAGS=-DARROW_NO_DEPRECATED_API meson build --prefix=/home/travis/build/apache/arrow/c-glib-install-meson -Dgtk_doc=true
Traceback (most recent call last):
  File ""/home/travis/miniconda/bin/meson"", line 26, in <module>
    from mesonbuild import mesonmain
ModuleNotFoundError: No module named 'mesonbuild'
{code}

Perhaps caused by the 8/25 release? https://pypi.org/project/meson/#history",pull-request-available,['GLib'],ARROW,Bug,Major,2018-09-07 04:17:37,1
13183513,"[C++] Add modular build targets, ""all"" target, and require explicit target when invoking make or ninja",This will make it easier to build and install only part of the project,pull-request-available,['C++'],ARROW,New Feature,Major,2018-09-06 22:24:28,14
13183422,[Packaging] Adjust conda package scripts to account for Parquet codebase migration,The parquet-cpp package is not necessary in these builds after ARROW-3075,pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2018-09-06 15:19:42,3
13183421,[C++] Add docker-compose setup to simulate Travis CI run locally,This will permit users to run the C++ CI job locally without waiting for Travis CI workers,pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-06 15:17:47,3
13182947,"[C++] Implement ""readahead spooler"" class for background input buffering",This is closely related to ARROW-501,pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-04 21:19:35,2
13182901,[C++] Break array-test.cc and array.cc into multiple compilation units,"To improve readability I suggest splitting array-test.cc into multiple compilation units (which will still be linked into an executable {{array-test}}). We can do the same thing with array.h/array.cc, while maintaining the {{arrow/array.h}} public header. Some of these could go into {{arrow/columnar}} or {{arrow/impl}}, or something similar. ",pull-request-available,['C++'],ARROW,Improvement,Major,2018-09-04 19:33:48,2
13182882,[CI] Limit clcache cache size,The clcache cache on AppVeyor has a default max size of 1 GB and can reach close to this size (see e.g. https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/build/1.0.7722/job/5gp85w0m5xei0nme#L251). We should limit its size to something more reasonable to lower cache transfer / compression times.,pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2018-09-04 18:06:28,2
13182866,[C++] Consolidate IO interfaces used in arrow/io and parquet-cpp,"With the codebase consolidation, we have the opportunity to remove cruft from the Parquet codebase. I believe it would be simpler and better for the ecosystem to use the Arrow IO interface classes rather than maintaining separate vitual IO interfaces exported from the {{parquet::}} namespace",parquet,['C++'],ARROW,Improvement,Major,2018-09-04 17:17:34,14
13182557,[Python] Cython dependency is missing in non wheel package,"{{pip install --no-binary :all: pyarrow}} requires {{Cython >= 0.27}} but pyarrow doesn't depend on Cython.
",pull-request-available,['Python'],ARROW,Improvement,Minor,2018-09-03 02:57:17,1
13182553,[Python] Enable Flight servers to be implemented in pure Python,"While it will be straightforward to offer a Flight client to Python users, enabling _servers_ to be written _in Python_ will require a glue class to invoke methods on a provided server implementation, coercing to and from various Python objects and Arrow wrapper classes",flight pull-request-available,"['FlightRPC', 'Python']",ARROW,New Feature,Major,2018-09-03 00:23:54,0
13182509,[C++] Improve buffer creation for typed data,"While looking into [https://github.com/apache/arrow/pull/2481,]I noticed this pattern:
{code:java}
const uint8_t* bytes_array = reinterpret_cast<const uint8_t*>(input);
auto buffer = std::make_shared<Buffer>(bytes_array, sizeof(float)*input_length);{code}
It's not the end of the world butseemsa little verbose to me. It would be great to have something like this:
{code:java}
auto buffer = MakeBuffer<float>(input, input_length);{code}
I couldn't find it, does it already exist somewhere? Any thoughts on the API? Potentially specializations to make a buffer out of a std::vector<T> would also be helpful.

",pull-request-available usability,['C++'],ARROW,Improvement,Major,2018-09-01 22:07:10,14
13182438,"[Python][C++] Document how to write _metadata, _common_metadata files with Parquet datasets","This is not mentioned in great detail in 

http://arrow.apache.org/docs/python/parquet.html",dataset parquet pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-08-31 21:59:25,5
13182422,[Packaging] Fix broken nightly package builds introduced with recent cmake changes and orc tests,"See: https://github.com/kszucs/crossbow/branches/all?utf8=%E2%9C%93&query=nightly-32
",pull-request-available,['Packaging'],ARROW,Task,Major,2018-08-31 20:45:21,3
13182391,[C++][Packaging] Use dynamic linking for zlib in conda recipes,See upstream conda-forge issue https://github.com/conda-forge/arrow-cpp-feedstock/issues/60. This will also need to be changed in the crossbow scripts,pull-request-available,"['C++', 'Packaging']",ARROW,Improvement,Major,2018-08-31 16:57:49,14
13182210,[Python] Ship Flight-enabled Python wheels on Linux and Windows,This may involve statically-linking (or bundling where shared libs makes sense) the various required dependencies with {{libarrow_flight.so}} in the manylinux1 wheel build,flight pull-request-available,"['C++', 'FlightRPC']",ARROW,Improvement,Critical,2018-08-30 19:04:14,2
13182209,[C++] Use gRPC (when it exists) from conda-forge for CI builds,"gRPC is not available in conda-forge yet. It has a rather complex dependency chain if you want secure RPCs (e.g. Google is maintaining a fork of OpenSSL, BoringSSL). Some of these dependencies will have to be added to conda-forge in turn.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-30 19:02:44,2
13182148,[C++] MSVC version isn't detected in code page 932,"There is no space between ""Microsoft"" and ""(R)"" in cl.exe output on code page 932.",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-08-30 14:54:08,1
13182044,[C++] Barebones Flight RPC server and client implementations,"Unsecure transport only (SSL support will require a fair bit of toolchain work)

Depends on ARROW-249",pull-request-available,"['C++', 'FlightRPC']",ARROW,New Feature,Major,2018-08-30 05:08:41,14
13182000,"[C++] Move ""dictionary"" member from DictionaryType to ArrayData to allow for changing dictionaries between Array chunks","There are a couple of inter-related issues:

* Cases where a system might send the schema without the dictionaries, and the user wishes to reason about the schema and its types without knowing the dictionary values

* Dictionaries that are changing, e.g. using delta dictionary messages

{{arrow::DictionaryType}} has no ""linkage"" to any external object. I propose adding a ""LinkedDictionaryType"" or something similar (purely a C++ construct), which functionally would be a subclass of {{DictionaryType}}, which would allow a type to be created which will obtain its dictionary later through some kind of ""Dictionary provider"" interface. There is something similar in Java already. This would allow a dictionary to evolve via delta dictionaries, or for a dictionary to be retrieved later e.g. through an RPC or IPC layer",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-29 21:14:55,14
13181945,[C++] CopyBitmap into existing memory,{{CopyBitmap}} currently always allocates a new Buffer for its result. We also want to support the case where we insert into existing memory.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-29 17:12:16,8
13181940,[C++] Fetch all libs from toolchain environment,"When setting ARROW_BUILD_TOOLCHAIN, gtest and orc are currently not taken from the toolchain environment.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-29 16:27:12,2
13181929,[Python] Tensorflow support in pyarrow wheels pins numpy>=1.14,"This was introduced by https://github.com/apache/arrow/pull/2104/files

Two options:

* Don't build with tensorflow support by default
* Increase our minimal support NumPy version to 1.14 overall",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2018-08-29 14:08:56,8
13181869,[Plasma] Plasma fails building with GPU enabled,"{code}
In file included from ../src/plasma/client.h:30:0,
                 from ../src/plasma/client.cc:20:
../src/plasma/common.h:120:19: error: CudaIpcMemHandle was not declared in this scope
   std::shared_ptr<CudaIpcMemHandle> ipc_handle;
                   ^~~~~~~~~~~~~~~~
{code}",pull-request-available,"['C++ - Plasma', 'GPU']",ARROW,Bug,Major,2018-08-29 10:15:46,2
13181694,[C++] Clean up arrow:: public API,"There are a lot of internal classes, types, and functions in the public API docs

http://arrow.apache.org/docs/cpp/namespacearrow.html

it would be more helpful to move some of these to {{internal::}} namespace or otherwise hide them from doxygen",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-28 18:47:47,2
13181653,[C++] Implement n-ary iterator for a collection of chunked arrays with possibly different chunking layouts,This is a common pattern that will result in kernel invocation on chunked arrays,dataframe,['C++'],ARROW,New Feature,Major,2018-08-28 16:29:53,14
13181649,Regenerate 0.10.0 changelog,Some issues were marked with the wrong fix version. We should regenerate the 0.10.0 changelog for historical purposes,pull-request-available,['Documentation'],ARROW,Bug,Major,2018-08-28 16:24:56,14
13181521,[Packaging] Stop to use deprecated BuildRoot and Group in .rpm,"They are deprecated
https://fedoraproject.org/wiki/Packaging:Guidelines says the followings:

{quote}
* The BuildRoot: tag, Group: tag, and %clean section SHOULD NOT be used.
{quote}
",pull-request-available,['Packaging'],ARROW,Improvement,Minor,2018-08-28 06:53:08,1
13181514,[C++] Support system shared zlib,"Debian package recommends not static linking zlib for security reason. If we use zlib as a static library, Debian package lint reports the following error:

{noformat}
E: libarrow10: embedded-library usr/lib/x86_64-linux-gnu/libarrow.so.10.0.0: zlib
{noformat}

embedded-library error detail: https://lintian.debian.org/tags/embedded-library.html",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-08-28 06:31:05,1
13181410,[Python] Update ASV instructions,"The ability to define custom install / build / uninstall commands was added in mainline ASV in https://github.com/airspeed-velocity/asv/pull/699
We don't need to use our own fork / PR anymore.",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-27 16:58:17,2
13181195,[Packaging] Nightly packaging script fails,"Error: https://travis-ci.org/kszucs/crossbow/builds/420569877

Caused by a recent ruamel yaml conda forge update: https://github.com/conda-forge/ruamel_yaml-feedstock/commit/c5263ae9d064a056900ed30ba8aae12cd0dbb74e",pull-request-available,['Packaging'],ARROW,Task,Major,2018-08-26 17:35:13,3
13180523,[C++] Compilation warnings with gcc 7.3.0,"This is happening when building in release mode:
{code}
../src/arrow/python/python_to_arrow.cc: In function 'arrow::Status arrow::py::detail::BuilderAppend(arrow::BinaryBuilder*, PyObject*, bool*)':
../src/arrow/python/python_to_arrow.cc:388:56: warning: 'length' may be used uninitialized in this function [-Wmaybe-uninitialized]
   if (ARROW_PREDICT_FALSE(builder->value_data_length() + length > kBinaryMemoryLimit)) {
                                                        ^
../src/arrow/python/python_to_arrow.cc:385:11: note: 'length' was declared here
   int32_t length;
           ^~~~~~
In file included from ../src/arrow/python/serialize.cc:32:0:
../src/arrow/builder.h: In member function 'arrow::Status arrow::py::SequenceBuilder::Update(int64_t, int8_t*)':
../src/arrow/builder.h:413:5: warning: 'offset32' may be used uninitialized in this function [-Wmaybe-uninitialized]
     raw_data_[length_++] = val;
     ^~~~~~~~~
../src/arrow/python/serialize.cc:90:13: note: 'offset32' was declared here
     int32_t offset32;
             ^~~~~~~~
{code}
",pull-request-available,['C++'],ARROW,Task,Major,2018-08-22 17:20:41,2
13180418,[C++] arrow::PrettyPrint for Table instances,Extend the {{arrow::PrettyPrint}} functionality to also support {{arrow::Table}} instances in addition to {{RecordBatch}}.,beginner pull-request-available,['C++'],ARROW,New Feature,Major,2018-08-22 08:39:21,8
13180417,[C++] arrow::PrettyPrint for Column instances,"Currently, we support {{arrow::ChunkedArray}} instances in {{PrettyPrint}}. We should also support columns. The main addition will be here that will also print the specified field.",beginner,['C++'],ARROW,New Feature,Major,2018-08-22 08:38:09,8
13180242,[CI] C/glib build broken on OS X,"The Travis-CI build fails to find luarocks:
https://travis-ci.org/apache/arrow/jobs/418753219#L2657

{code}
+sudo env PKG_CONFIG_PATH=:/usr/local/opt/libffi/lib/pkgconfig luarocks install lgi
env: luarocks: No such file or directory

The command ""$TRAVIS_BUILD_DIR/ci/travis_before_script_c_glib.sh"" failed and exited with 127 during .
{code}",ci-failure pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Bug,Major,2018-08-21 16:15:25,1
13180240,[C++] Add benchmark for number parsing,Number parsing will become important once we have a CSV reader (or possibly other text-based formats). We should add benchmarks for the internal conversion routines.,pull-request-available,['C++'],ARROW,Wish,Major,2018-08-21 16:00:38,2
13180167,[Python] BufferReader doesn't adhere to the seek protocol,"I have a script that creates a Parquet file and then writes it out to a {{BufferOutputStream}} and then into a {{BufferReader}} with the intention of passing it to a place that takes a file-like object to upload it somewhere else. But the other location relies on being able to seek to the end of the file to figure out how big the file is, e.g.

{code:python}
reader.seek(0, 2)
size = reader.tell()
reader.seek(0)
{code}


But when I do that the following exception is raised:


{code}
pyarrow/io.pxi:209: in pyarrow.lib.NativeFile.seek
???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

> ???
E pyarrow.lib.ArrowIOError: position out of bounds
{code}

I compared it to casting to an {{io.BytesIO}} instead which works:

{code:python}
import io

import pyarrow as pa


def test_arrow_output_stream():
    output = pa.BufferOutputStream()
    output.write(b'hello')

    reader = pa.BufferReader(output.getvalue())

    reader.seek(0, 2)
    assert reader.tell() == 5


def test_python_io_stream():
    output = pa.BufferOutputStream()
    output.write(b'hello')

    buffer = io.BytesIO(output.getvalue().to_pybytes())
    reader = io.BufferedRandom(buffer)

    reader.seek(0, 2)
    assert reader.tell() == 5
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-21 09:14:50,2
13180002,[Python] Allow lighter construction of pa.Schema / pa.StructType,"One shouldn't have to call {{pa.field}} explicitly. See this example:
https://github.com/apache/arrow/pull/2449/files#diff-a01a3e7cbe0d7dd0ec300a725ac0c0c6R148
",pull-request-available,['Python'],ARROW,Wish,Major,2018-08-20 16:38:31,2
13179815,[C++] Add kernels for comparison operations to scalars,"This should implement the comparison operators  {{>=, >, ==, !=, <, <=}} between {{arrow::compute::Datum}} and {{arrow::compute::Scalar}}. 

The result of this kernel will be a boolean type {{arrow::compute::Datum}} where with True/False set according to the outcome of the operation and NA if a row was not valid.

A pre-condition to implement this kernel is to have a working implementation of {{arrow::compute::Scalar}}.",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2018-08-19 14:53:12,13
13179754,[Python] Version in manylinux1 wheel builds is wrong,"Not sure if this is a regression but I noticed:

https://travis-ci.org/apache/arrow/jobs/417498434#L2665",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-18 14:25:47,8
13179701,[Python] Unify Arrow to Python object conversion paths,"Similar to ARROW-2814, we have inconsistent support for converting Arrow nested types back to object sequences. For example, a list of structs fails when calling {{to_pandas}}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-17 23:21:21,3
13179630,"[Website] Add Google Analytics tags to C++, Python API docs",It would be helpful to see which parts of the documentation are seeing traffic,pull-request-available,['Website'],ARROW,Improvement,Major,2018-08-17 16:49:59,14
13179620,[C++] Incorporate apache/parquet-cpp codebase into Arrow C++ codebase and build system,Particular logistics pending discussion and vote https://lists.apache.org/thread.html/53f77f9f1f04b97709a0286db1b73a49b7f1541d8f8b2cb32db5c922@%3Cdev.parquet.apache.org%3E,parquet pull-request-available,['C++'],ARROW,New Feature,Major,2018-08-17 16:05:07,14
13179600,[C++] Use ARROW_RETURN_NOT_OK instead of RETURN_NOT_OK in header files,The {{RETURN_NOT_OK}} macro could conceivably collide with macros in other libraries. It would be better to use the scoped macro in public headers,pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-17 14:40:51,14
13179597,[Release] Host binary artifacts for RCs and releases on ASF Bintray account instead of dist/mirror system,Since the artifacts are large this is a better place for them. ,pull-request-available,['Developer Tools'],ARROW,Improvement,Major,2018-08-17 14:25:17,1
13179596,[Release] Stop using SHA1 checksums per ASF policy,https://www.apache.org/dev/release-distribution#sigs-and-sums,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-08-17 14:24:09,3
13179489,[Python] concat_tables() failing from bad Pandas Metadata,"Looks like the major bug from https://issues.apache.org/jira/browse/ARROW-1941 is back...

After I downgraded from 0.10.0 to 0.9.0, the error disappeared..

{code:python}
new_arrow_table = pa.concat_tables(my_arrow_tables)

 File ""pyarrow/table.pxi"", line 1562, in pyarrow.lib.concat_tables
  File ""pyarrow/error.pxi"", line 81, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Schema at index 2 was different:
{code}

In order to debug this I saved the first 4 arrow tables to 4 parquet files and inspected the parquet files. The parquet schema is identical, but the Pandas Metadata is different.

{code:python}
for i in range(5):
     pq.write_table(my_arrow_tables[i], ""test"" + str(i) + "".parquet"")
{code}

It looks like a column which contains empty strings is getting typed as float64.

{code:python}
>>> test1.schema
HoldingDetail_Id: string
metadata
--------
{b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [
{""name"": ""HoldingDetail_Id"", ""field_name"": ""HoldingDetail_Id"", ""pandas_type"": ""unicode"", ""numpy_type"": ""object"", ""metadata"": null},

>>> test1[0]
<Column name='HoldingDetail_Id' type=DataType(string)>
[
  [
    ""Z4"",
    ""SF"",
    ""J7"",
    ""W6"",
    ""L7"",
    ""Q9"",
    ""NE"",
    ""F7"",


>>> test2.schema
HoldingDetail_Id: string
metadata
--------
{b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [
{""name"": ""HoldingDetail_Id"", ""field_name"": ""HoldingDetail_Id"", ""pandas_type"": ""unicode"", ""numpy_type"": ""float64"", ""metadata"": null},

>>> test2[0]
<Column name='HoldingDetail_Id' type=DataType(string)>
[
  [
    """",
    """",
    """",
    """",
    """",
    """",
    """",
    """",
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-17 01:01:07,3
13179103,[C++] Factor out parsing routines,We have implementations of casting strings to numbers in the {{compute}} directory. Those can be more broadly useful (for example when parsing CSV files). We should therefore centralize them in their own C++ module.,pull-request-available,['C++'],ARROW,Task,Major,2018-08-15 14:53:52,2
13179091,[C++] Streamline namespace array::test,"Currently we have some test helpers that live in the {{arrow::test}} namespace, some in {{arrow}} (or topic subnamespaces such as {{arrow::io}}). I see no reason for the discrepancy.

I propose the simple solution of removing the {{arrow::test}} namespace altogether. If not desirable, then we should make sure we put all helpers in that namespace.",pull-request-available,['C++'],ARROW,Task,Major,2018-08-15 13:55:51,2
13179084,[Python] Feather reads fail with unintuitive error when conversion from pandas yields ChunkedArray,"See report in https://github.com/wesm/feather/issues/321#issuecomment-412884084

Individual string columns with more than 2GB are currently unsupported in the Feather format ",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-15 13:47:27,14
13179029,[INTEGRATION] Fix spark and hdfs dockerfiles,Spark and HDFS integration tests are failing to build.,pull-request-available,['Integration'],ARROW,Task,Major,2018-08-15 09:01:40,3
13178930,[Python] Indicate in NativeFile docstrings methods that are part of the RawIOBase API but not implemented,see https://github.com/apache/arrow/issues/2422,pull-request-available,['Python'],ARROW,Bug,Major,2018-08-14 19:20:38,14
13178863,[Python] Pandas decimal conversion segfault,"This example segfaults when trying to convert a pandas DataFrame with a decimal column and at least one other object column to a pyarrow Table after a round trip through HDF5:
{code:java}
import decimal
import pandas as pd
import pyarrow as pa

data = {'a': {0: 'a'}, 'b': {0: decimal.Decimal('0.0')}}

df = pd.DataFrame.from_dict(data)
df.to_hdf('test.h5', 'test')
df = pd.read_hdf('test.h5', 'test')

table = pa.Table.from_pandas(df)
{code}
This is the gdb backtrace:
{code:java}
#0 0x00007f188a08fc0b in arrow::py::internal::PandasObjectIsNull(_object*) () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/libarrow_python.so.10
#1 0x00007f188a09931c in arrow::py::NumPyConverter::ConvertDecimals() () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/libarrow_python.so.10
#2 0x00007f188a09ef4b in arrow::py::NumPyConverter::ConvertObjectsInfer() () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/libarrow_python.so.10
#3 0x00007f188a09f5db in arrow::py::NumPyConverter::ConvertObjects() () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/libarrow_python.so.10
#4 0x00007f188a09f715 in arrow::py::NumPyConverter::Convert() () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/libarrow_python.so.10
#5 0x00007f188a0a0f5e in arrow::py::NdarrayToArrow(arrow::MemoryPool*, _object*, _object*, bool, std::shared_ptr<arrow::DataType> const&, std::shared_ptr<arrow::ChunkedArray>*) () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/libarrow_python.so.10
#6 0x00007f188ab1a13e in __pyx_pw_7pyarrow_3lib_79array(_object*, _object*, _object*) () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/lib.so
#7 0x00000000004c37ed in PyEval_EvalFrameEx ()
#8 0x00000000004b9ab6 in PyEval_EvalCodeEx ()
#9 0x00000000004c1e6f in PyEval_EvalFrameEx ()
#10 0x00000000004b9ab6 in PyEval_EvalCodeEx ()
#11 0x00000000004d55f3 in ?? ()
#12 0x00007f188aa75eac in __pyx_pw_7pyarrow_3lib_5Table_17from_pandas(_object*, _object*, _object*) () from /home/ashieh/.local/lib/python2.7/site-packages/pyarrow/lib.so
#13 0x00000000004bc3fa in PyEval_EvalFrameEx ()
#14 0x00000000004b9ab6 in PyEval_EvalCodeEx ()
#15 0x00000000004eb30f in ?? ()
#16 0x00000000004e5422 in PyRun_FileExFlags ()
#17 0x00000000004e3cd6 in PyRun_SimpleFileExFlags ()
#18 0x0000000000493ae2 in Py_Main ()
#19 0x00007f18a79c4830 in __libc_start_main (main=0x4934c0 <main>, argc=2, argv=0x7fffcf079508, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffcf0794f8) at ../csu/libc-start.c:291
#20 0x00000000004933e9 in _start ()
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-08-14 14:12:50,14
13178862,[C++] Detect ORC system packages,See https://github.com/apache/arrow/blob/master/cpp/cmake_modules/ThirdpartyToolchain.cmake#L155. After the CMake refactor it is possible to use built ORC packages with {{$ORC_HOME}} but not detected like the other toolchain dependencies,pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-14 14:00:17,14
13178759,[C++] Adopt HiveServer2 client C++ codebase,"I helped develop a small C++/Python library for interacting with databases like Hive and Impala via the HiveServer2 Thrift protocol and making them accessible to Python / pandas:

https://github.com/cloudera/hs2client

Internally this interfaces with HS2's own columnar representation. Arrow is a natural partner for this project, much of which could be discarded. I think Arrow would make as much sense as any place to develop this codebase further. It could be later split off into a new project if a large enough community develops

cc [~twmarshall] [~mjacobs] for thoughts

If we did this, do we need to do a software grant (essentially what I'm proposing is to fork)? Can we just attribute the original Cloudera authors in LICENSE.txt?",pull-request-available,['C++'],ARROW,New Feature,Major,2018-08-14 03:37:14,14
13178613,[C++/Python] ORC reader fails on empty file,"{code}
Traceback (most recent call last):
  File ""/home/antoine/arrow/python/pyarrow/tests/test_orc.py"", line 83, in test_orcfile_empty
    check_example('TestOrcFile.emptyFile')
  File ""/home/antoine/arrow/python/pyarrow/tests/test_orc.py"", line 79, in check_example
    os.path.join(orc_data_dir, '%s.jsn.gz' % name))
  File ""/home/antoine/arrow/python/pyarrow/tests/test_orc.py"", line 62, in check_example_files
    table = orc_file.read()
  File ""/home/antoine/arrow/python/pyarrow/orc.py"", line 149, in read
    return self.reader.read(include_indices=include_indices)
  File ""pyarrow/_orc.pyx"", line 106, in pyarrow._orc.ORCReader.read
    check_status(deref(self.reader).Read(&sp_table))
  File ""pyarrow/error.pxi"", line 81, in pyarrow.lib.check_status
    raise ArrowInvalid(message)
pyarrow.lib.ArrowInvalid: Must pass at least one record batch
{code}

[~jim.crist]",pull-request-available,['C++'],ARROW,Bug,Major,2018-08-13 15:28:33,2
13178603,[C++] cmake downloads and builds ORC even though it's installed,"I have installed orc 1.5.1 from conda-forge, but our cmake build chain still tries to build protobuf and ORC from source (and fails).

{code:bash}
$ ls $CONDA_PREFIX/include/orc/
ColumnPrinter.hh  Common.hh  Exceptions.hh  Int128.hh  MemoryPool.hh  orc-config.hh  OrcFile.hh  Reader.hh  Statistics.hh  Type.hh  Vector.hh  Writer.hh
$ ls -l $CONDA_PREFIX/lib/liborc*
-rw-rw-r-- 2 antoine antoine 1952298 juin  20 17:32 /home/antoine/miniconda3/envs/pyarrow/lib/liborc.a
{code}

[~jim.crist]",pull-request-available,['C++'],ARROW,Bug,Major,2018-08-13 14:19:58,2
13178425,[Python] Remove nullcheck from ipc Message and MessageReader,"Nullcheck is required to prevent segfaults if the extension classes are not (or partially) constructed.
Explicitly disallowing Message() and MessageReader() construction makes nullchecks unnecessary.

See https://github.com/apache/arrow/commit/44bfd0d5a7f4a68b3ab7ce72cd2b68f62a6fec5c for previous work.",pull-request-available,['Python'],ARROW,Task,Major,2018-08-11 11:07:08,3
13178388,[Python] Remove all occurrences of cython's legacy property definition syntax,As well as add missing tests ,pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-10 22:03:14,3
13178175,[Packaging] Source archive can't be extracted by bsdtar on MSYS2,"It's caused by {{python/cmake_modules}}. It's symlink. It seems that bsdtar on MSYS2 can't handle symlink.

Note that GNU tar can extract and {{python/cmake_modules}}'s symlink is resolved.",pull-request-available,['Packaging'],ARROW,Improvement,Minor,2018-08-10 01:52:41,1
13178085,[Python] Clean up NumPy-related C++ headers,"There are 4 different headers. After ARROW-2814, we can probably eliminate numpy_convert.h and combine with numpy_to_arrow.h",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-09 18:03:04,2
13177972,[Python] pkg_resources is slow,"Importing and calling {{pkg_resources}} at pyarrow import time to get the version number is slow (around 200 ms. here, out of 640 ms. total for importing pyarrow).

Instead we could generate a version file, which seems possible using {{setuptools_scm}}'s {{write_to}} parameter: https://github.com/pypa/setuptools_scm/#configuration-parameters
",pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-09 12:31:29,2
13177853,[Python] Trim unneeded work from documentation build in Travis CI,"Some things that are probably not necessary:

* We should build and install the Python library in one place rather than rebuilding it in a new conda environment for the documentation
* Does matplotlib need to be installed? It pulls in a heavy dependency chain including Qt: https://github.com/conda-forge/matplotlib-feedstock/blob/master/recipe/meta.yaml#L48",pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-09 04:44:58,14
13177789,[Plasma] Only run Plasma Python unit tests under valgrind once instead of twice in CI,"We are running these tests multiple times under valgrind. If there is a memory leak or invalid memory access, or is highly likely to be present in both Python 2.7 and 3.x",pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-08 21:05:14,14
13177743,[C++] Replace usages of std::mutex with atomics in memory_pool.cc,I believe it should be possible to make the default memory pool lock-free,pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-08 17:33:51,2
13177720,[C++] Use gold linker in builds if it is available,This will improve linking time in builds,pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-08 16:27:57,14
13177606,[Python] Addition of option to allow empty Parquet row groups,"While our use case is not common, I was able to find one related request from roughly a year ago. Could this be added as a feature?

https://issues.apache.org/jira/browse/PARQUET-1047

*Motivation*

We have an application where each row is associated with one of N contexts, though a minority of contexts may have no associated rows. When encountering the Nth context, we will wish to retrieve all the associated rows. Row groups would provide a natural way to index the data, as the nth context could naturally relate to the nth row group.

Unfortunately, this is not possible at the present time, as pyarrow does not support writing empty row groups. If one writes a pyarrow.Table containing zero rows using pyarrow.parquet.ParquetWriter, it is omitted from the final file, and this distorts the indexing.",parquet pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2018-08-08 07:35:45,14
13177515,[Python] Fix documentation typo for pa.uint8,See http://arrow.apache.org/docs/python/generated/pyarrow.uint8.html#pyarrow.uint8,pull-request-available,['Python'],ARROW,Bug,Major,2018-08-07 20:33:34,2
13177495,"[Website] Fix download links on website for tarballs, checksums",See discussion on the ANNOUNCE thread for 0.10.0,pull-request-available,['Website'],ARROW,Bug,Major,2018-08-07 19:25:24,3
13177477,[Python] Installation crashes with setuptools_scm error,"To reproduce, on Python 3.7.0:
{code:none}
pip3.7 installpyarrow==0.10.0
{code}


The result is a crash:
{code:none}
Collecting pyarrow
Using cached https://files.pythonhosted.org/packages/c0/a0/f7e9dfd8988d94f4952f9b50eb04e14a80fbe39218520725aab53daab57c/pyarrow-0.10.0.tar.gz
Complete output from command python setup.py egg_info:
Traceback (most recent call last):
File ""<string>"", line 1, in <module>
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/setup.py"", line 545, in <module>
url=""https://arrow.apache.org/""
File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/setuptools/__init__.py"", line 131, in setup
return distutils.core.setup(**attrs)
File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/distutils/core.py"", line 108, in setup
_setup_distribution = dist = klass(attrs)
File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/setuptools/dist.py"", line 370, in __init__
k: v for k, v in attrs.items()
File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/distutils/dist.py"", line 292, in __init__
self.finalize_options()
File ""/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/setuptools/dist.py"", line 529, in finalize_options
ep.load()(self, ep.name, value)
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/.eggs/setuptools_scm-3.0.6-py3.7.egg/setuptools_scm/integration.py"", line 23, in version_keyword
dist.metadata.version = get_version(**value)
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/.eggs/setuptools_scm-3.0.6-py3.7.egg/setuptools_scm/__init__.py"", line 135, in get_version
parsed_version = _do_parse(config)
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/.eggs/setuptools_scm-3.0.6-py3.7.egg/setuptools_scm/__init__.py"", line 77, in _do_parse
parse_result = _call_entrypoint_fn(config, config.parse)
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/.eggs/setuptools_scm-3.0.6-py3.7.egg/setuptools_scm/__init__.py"", line 40, in _call_entrypoint_fn
return fn(config.absolute_root)
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/setup.py"", line 498, in parse_version
return version_from_scm(root)
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/.eggs/setuptools_scm-3.0.6-py3.7.egg/setuptools_scm/__init__.py"", line 28, in version_from_scm
return _version_from_entrypoint(root, ""setuptools_scm.parse_scm"")
File ""/private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/.eggs/setuptools_scm-3.0.6-py3.7.egg/setuptools_scm/__init__.py"", line 44, in _version_from_entrypoint
for ep in iter_matching_entrypoints(config.absolute_root, entrypoint):
AttributeError: 'str' object has no attribute 'absolute_root'

----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /private/var/folders/6r/dy0_bd2x2kn735d8kymc4qt00000gn/T/pip-install-w77cbide/pyarrow/

{code}


I suspect this is because {{setuptools_scm}} isn't being used correctly. The function takes one argument, {{root}}, but judging from the code that uses it, it appears to expect a {{setuptools_scm.config.Configuration}} object rather than a file path.

All the documentation says to use {{get_version()}} and the package author doesn't seem to be sure that {{version_from_scm()}} should even be a public function (see [here|https://github.com/pypa/setuptools_scm/blob/master/src/setuptools_scm/__init__.py#L27]). Perhaps going with that would be best.",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-07 18:19:57,2
13177445,[CI] Remove Slack notification,Remove code from ARROW-2682,pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2018-08-07 15:45:06,3
13177433,[Packaging] libarrow-gpu10 deb for Ubuntu 18.04 has broken dependencies,libarrow-gpu10 depends on libcuda1 but libcuda1 doesn't exist on Ubuntu 18.04.,pull-request-available,['Packaging'],ARROW,Bug,Minor,2018-08-07 15:09:01,1
13177217,[Python] Implement better DataType hash function,"{code:python}
>>> x = pa.field('record', pa.struct([pa.field('x', pa.int32(), nullable=False)]))
>>> y = pa.field('record', pa.struct([pa.field('x', pa.int32(), nullable=True)]))
>>> z = pa.field('record', pa.struct([pa.field('x', pa.int32(), nullable=True)]))
>>> x.__hash__()
-9223372036569171727
>>> y.__hash__()
285604054
>>> z.__hash__()
285604076
>>> x.type
StructType(struct<x: int32>)
>>> x.type.__hash__()
429437081997812647
>>> y.type.__hash__()
429437081997812647
>>> x
pyarrow.Field<record: struct<x: int32>>
>>> y
pyarrow.Field<record: struct<x: int32>>

{code}

Expected: 

y.__hash__() should be the same as z.__hash__()

x.type.__hash__() should be different than y.type.__hash__()
",pull-request-available,['Python'],ARROW,Wish,Minor,2018-08-06 20:47:04,3
13177216,[Packaging] Don't modify PATH during rust release verification,"Sadly rustup has a side effect, it modifies .profile or .zprofile.",pull-request-available,['Packaging'],ARROW,Task,Major,2018-08-06 20:38:15,3
13177211,[Python] Do not build unit tests other than python-test in travis_script_python.sh,This is inflating CI runtime. This is a bit of work as we do not currently have granular control over which unit tests get built. We may have to add some logic to tag tests with extra metadata that can be used for conditional compilation (beyond the all-or-nothing {{NO_TESTS}} CMake variable being used now),pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-06 20:28:59,14
13177206,[Python] Do not run ASV benchmarks in every Travis CI build to improve runtimes,I suggest we devise another way to make sure the benchmarks do not bitrot.,pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2018-08-06 20:20:04,14
13177189,[C++] Only include Python C header directories for Python-related compilation units,"This is causing ccache artifacts to not be reusable when switching between Python 2.7 and 3.x

See relevant line https://github.com/apache/arrow/blob/master/cpp/CMakeLists.txt#L721",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-06 18:59:07,14
13177144,[Python] Parquet benchmark failure,"This is a regression on git master:
{code:python}
Traceback (most recent call last):
  File ""/home/antoine/asv/asv/benchmark.py"", line 867, in <module>
    commands[mode](args)
  File ""/home/antoine/asv/asv/benchmark.py"", line 844, in main_run
    result = benchmark.do_run()
  File ""/home/antoine/asv/asv/benchmark.py"", line 398, in do_run
    return self.run(*self._current_params)
  File ""/home/antoine/asv/asv/benchmark.py"", line 473, in run
    samples, number = self.benchmark_timing(timer, repeat, warmup_time, number=number)
  File ""/home/antoine/asv/asv/benchmark.py"", line 520, in benchmark_timing
    timing = timer.timeit(number)
  File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.6/timeit.py"", line 178, in timeit
    timing = self.inner(it, self.timer)
  File ""<timeit-src>"", line 6, in inner
  File ""/home/antoine/asv/asv/benchmark.py"", line 464, in <lambda>
    func = lambda: self.func(*param)
  File ""/home/antoine/arrow/python/benchmarks/parquet.py"", line 54, in time_manifest_creation
    pq.ParquetManifest(self.tmpdir, thread_pool=thread_pool)
TypeError: __init__() got an unexpected keyword argument 'thread_pool'
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-06 15:47:57,2
13177106,[CI] Cut down number of AppVeyor jobs,"AppVeyor builds all jobs serially so it's important not to have too many of them to avoid builds taking too much time and queuing up.

I suggest to remove the following jobs:
- the Release build with Ninja and VS2015; we already have both a Release build with Ninja and VS2017, and a Debug build with Ninja and VS2015
- the two NMake builds: we already exercise the Ninja (cross-platform, fastest) and Visual Studio (standard under Windows) build chains

[~Max Risuhin] you added some of those jobs, do you have any concerns?
",pull-request-available windows,"['C++', 'Continuous Integration']",ARROW,Task,Major,2018-08-06 12:34:18,2
13177014,[GLib] Fail to build with rpath-ed Arrow C++ on macOS,"http://mail-archives.apache.org/mod_mbox/arrow-dev/201808.mbox/%3C1533195722.2696142.1460869648.3991D444%40webmail.messagingengine.com%3E

{quote}
I had the same problem with Glib in GISCAN as mentioned in https://github.com/apache/arrow/pull/2303#issuecomment-406934428
I replaced the make call with `LD_LIBRARY_PATH=$ARROW_HOME/lib make` but did not seem to fix
my problem.
{quote}",pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-08-06 04:14:39,1
13177007,[C++] Remove deprecated APIs in 0.10.0 and below,Reminder to scrub the API before releasing 0.11.0,pull-request-available,['C++'],ARROW,Improvement,Blocker,2018-08-06 02:55:45,14
13177004,[Release] More automated release verification on Windows,"* Create conda environment automatically
* More robust to misc environmental issues",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-08-06 02:38:49,14
13176998,[C++] /EHsc possibly needed for Visual Studio 2015 builds,"I hit this failure when working on verifying 0.10.0; the release verification script uses {{/WX}}. I will remove that; not a blocker I don't think 

{code}
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): error C2220: warning treated as error - no 'object' file generated (compiling source file C:\Users\wesm\code\arrow\cpp\src\arrow\util\compression_ zstd.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): error C2220: warning treated as error - no 'object' file generated (compiling source file C:\Users\wesm\code\arrow\cpp\src\arrow\util\compression_ lz4.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): error C2220: warning treated as error - no 'object' file generated (compiling source file C:\Users\wesm\code\arrow\cpp\src\arrow\builder.cc) [C:\U sers\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): error C2220: warning treated as error - no 'object' file generated (compiling source file C:\Users\wesm\code\arrow\cpp\src\arrow\util\compression_ snappy.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): error C2220: warning treated as error - no 'object' file generated (compiling source file C:\Users\wesm\code\arrow\cpp\src\arrow\util\compression_ brotli.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc (compiling source file C:\Users\wesm\code\arrow\cpp \src\arrow\util\compression_zstd.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc (compiling source file C:\Users\wesm\code\arrow\cpp \src\arrow\util\compression_lz4.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc (compiling source file C:\Users\wesm\code\arrow\cpp \src\arrow\builder.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc (compiling source file C:\Users\wesm\code\arrow\cpp \src\arrow\util\compression_snappy.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\include\xlocale(341): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc (compiling source file C:\Users\wesm\code\arrow\cpp \src\arrow\util\compression_brotli.cc) [C:\Users\wesm\code\arrow\cpp\build\src\arrow\arrow_static.vcxproj]
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-08-06 02:13:50,14
13176922,[Packaging] Verify source release and binary artifacts in different scripts,This would be beneficial for testing releases on slower internet,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-08-04 20:54:04,3
13176920,"The ""--show-progress"" option is only supported in wget 1.16 and higher",I ran into this when running the release verification script on Ubuntu 14.04 Trusty. Example workaround shown in https://stackoverflow.com/a/32491843,pull-request-available,['Packaging'],ARROW,Bug,Major,2018-08-04 20:47:26,14
13176901,[C++] Support scripts / documentation for running clang-tidy on codebase,"Related to ARROW-2952, ARROW-2980",pull-request-available,['C++'],ARROW,Improvement,Major,2018-08-04 15:44:53,6
13176896,[Rust] Travis CI build is failing,See https://travis-ci.org/apache/arrow/jobs/411984476,ci-failure pull-request-available,['Rust'],ARROW,Bug,Blocker,2018-08-04 14:51:59,10
13176798,"[Python] Replace usages of ""source activate"" with ""conda activate"" in CI scripts","This is the ""new style"" environment activation recommended for newer condas",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Bug,Major,2018-08-03 18:56:41,3
13176744,[Python] Give more descriptive names to python_to_arrow.cc/arrow_to_python.cc,The current builtin_convert.cc would be better named as python_to_arrow.cc or something similar. The current ones ought to be named serialize.cc and deserialize.cc or something similar,pull-request-available,['Python'],ARROW,Improvement,Major,2018-08-03 15:56:51,14
13176743,[Python] NumPyConverter::Visit for Binary/String/FixedSizeBinary can overflow,Noted during ARROW-2814 refactor. These functions need to be able to yield chunked output,pull-request-available,['Python'],ARROW,Bug,Major,2018-08-03 15:55:34,14
13176592,[Python] Possible uint64 overflow issues in python_to_arrow.cc,"There are some places, like the {{AppendScalar}} function, where UINT64 or ULONGLONG is being cast to int64 without overflow checking",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-02 23:45:04,2
13176454,[Python] Deadlock during fork-join and use_threads=True,"The following code passes:

{noformat}
import os
import pandas as pd
import pyarrow as pa


df = pd.DataFrame({'x': [1]})
table = pa.Table.from_pandas(df)
df = table.to_pandas(use_threads=False)

pid = os.fork()
if pid != 0:
    os.waitpid(pid, 0)
{noformat}

but the following code will never finish (the {{waitpid}} calls blocks forever, seems that the child process is frozen):

{noformat}
import os
import pandas as pd
import pyarrow as pa


df = pd.DataFrame({'x': [1]})
table = pa.Table.from_pandas(df)
df = table.to_pandas(use_threads=True)

pid = os.fork()
if pid != 0:
    os.waitpid(pid, 0)
{noformat}

(the only difference is {{use_threads=True}})",pull-request-available,['Python'],ARROW,Bug,Major,2018-08-02 15:20:56,2
13176181,[Packaging] Bintray descriptor files are no longer needed,"We're using Bintray for uploading built packages in apache/arrow-dist. We aren't using Bintray in apache/arrow. If we never use Bintray in apache/arrow, we can remove these files.

    .deb: https://bintray.com/kou/apache-arrow-apt/APT
    .rpm: https://bintray.com/kou/apache-arrow-yum/Yum
",pull-request-available,['Packaging'],ARROW,Task,Major,2018-08-01 16:49:52,3
13176165,[C++] Flatbuffers EP fails to compile with GCC 8.1,The flatbuffers version we're using when building it as an EP doesn't compile with GCC 8.1. According to this issue https://github.com/google/flatbuffers/issues/4741 it's fixed in master. We should upgrade the EP version to the earliest version compatible with GCC 8.1,pull-request-available,"['C++', 'Packaging']",ARROW,Task,Major,2018-08-01 15:13:31,14
13175904,[C++] Dockerfile for running include-what-you-use checks,It would be valuable to have a non-nonsense reproducible IWYU report. Every time I want to run this report on a new machine I lose time building the correct version of IWYU and remembering how to correctly run the report,pull-request-available,['C++'],ARROW,Improvement,Major,2018-07-31 17:48:37,14
13175855,[CI] Changes in format/ should cause Appveyor builds to run,Currently they are skipped https://github.com/apache/arrow/blob/master/appveyor.yml#L23,pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-07-31 14:38:11,2
13175850,[C++] Clean up util/bit-util.h,"Currently {{bit-util.h}} has a lot of very similar functions, some possibly unused. It would be nice to do a clean-up pass.",pull-request-available,['C++'],ARROW,Task,Minor,2018-07-31 14:20:25,2
13175836,[Packaging] Generate changelog with crossbow,Basically the port of https://github.com/apache/arrow/blob/master/dev/release/changelog.py,pull-request-available,['Packaging'],ARROW,Sub-task,Major,2018-07-31 13:43:21,3
13175740,[Format] Arrow columnar format docs mentions VectorLayout that does not exist anymore,VectorLayout is used in [format/Metadata.md|https://github.com/apache/arrow/blob/master/format/Metadata.md] that should probably be removed.,documentation pull-request-available,['Documentation'],ARROW,Task,Minor,2018-07-31 05:39:55,14
13175728,[C++] Implement BufferedOutputStream::Flush ,"This is part of the virtual interface:

https://github.com/apache/arrow/blob/master/cpp/src/arrow/io/interfaces.h#L96

The default implementation is no-op",pull-request-available,['C++'],ARROW,Improvement,Major,2018-07-31 03:36:21,2
13175437,[Python] Implement Table.cast for casting from one schema to another (if possible),This is an API convenience that would be very useful. See example use case in ARROW-2926,pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-29 23:24:06,3
13175346,[Python] ParquetWriter segfaults in example where passed schema and table schema do not match,"I get a segmentation fault with simply printing a basic table creation from arrays:


{code:python}
import pyarrow as pa
import pyarrow.parquet as pq

simple_fields = [
 pa.field('POS', pa.uint32()),
 pa.field('desc', pa.string())
]


simple_schema = pa.schema(simple_fields)

simple_from_array = [pa.Array([1]), pa.Array(['bla'])]
simple_table = pa.Table.from_arrays(simple_from_array, ['POS', 'desc'])
print(simple_table)
{code}",parquet pull-request-available,['Python'],ARROW,Bug,Critical,2018-07-28 16:13:30,14
13175087,[Release] Make python command name customizable,"{{python3}} is used for Python 3 on Debian GNU/Linux.
",pull-request-available,['Packaging'],ARROW,Improvement,Minor,2018-07-27 07:54:39,1
13174840,[C++] Improve error message when listing empty HDFS file,"Currently, listing empty HDFS filereturns ""list directory failed"" which doesn't help much for users. An improvement would be like ""list $file_path failed: No such or file directory"", the error message that HDFS command ""hadoop fs -ls"" returns on empty file.",hdfs pull-request-available,['C++'],ARROW,Improvement,Minor,2018-07-26 13:29:17,14
13174650,[Packaging] Remove artifact form ubuntu-trusty build,"apache-arrow_{version}-1.debian.tar.xz

[~kou] please confirm, but ubuntu trusty doesn't build such an archive",pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-25 22:12:11,3
13174422,[Python]Parquet binary statistics that end in '\0' truncate last byte,This is due to an intermediate step treating them as c-strings.,parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-07-25 17:26:09,8
13174257,[Rust] Update version to 0.10.0,Just updating the Cargo.toml version number from 0.1.0 to 0.10.0,pull-request-available,['Rust'],ARROW,Task,Trivial,2018-07-25 03:20:11,10
13173983,[Python] HDFS Docker integration tests leave around files created by root,This is a rough edge for local development. This can be solved by running {{git clean -fdx .}} in the {{python/}} directory after running the tests,pull-request-available,['Python'],ARROW,Bug,Major,2018-07-24 01:43:00,14
13173677,[Glib]Format tests broken due to recent refactor,Missed them while refactoring the C++ code. I'll take care of them,pull-request-available,['GLib'],ARROW,Bug,Major,2018-07-22 17:24:56,8
13173215,[Packaging] Options to build packages from apache source archive,See conversation: https://github.com/apache/arrow/pull/2288,pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-19 15:02:44,1
13173201,[Plasma] Compilation warnings,"Not sure that's recent, but I get the following warnings with clang 6.0.1:
{code}
[37/169] Building CXX object src/plasma/CMakeFiles/plasma_objlib.dir/client.cc.o
../src/plasma/client.cc:283:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
  CudaDeviceManager::GetInstance(&manager_);
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~
../src/plasma/client.cc:411:7: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
      writer.WriteAt(object.data_size, metadata, metadata_size);
      ^~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2 warnings generated.
[53/169] Building CXX object src/plasma/CMakeFiles/plasma_objlib.dir/protocol.cc.o
../src/plasma/protocol.cc:134:5: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
    object->ipc_handle->Serialize(arrow::default_memory_pool(), &handle);
    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/plasma/protocol.cc:174:5: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
    CudaIpcMemHandle::FromBuffer(message->ipc_handle()->handle()->data(),
    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/plasma/protocol.cc:506:7: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
      object.ipc_handle->Serialize(arrow::default_memory_pool(), &handle);
      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/plasma/protocol.cc:541:7: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
      CudaIpcMemHandle::FromBuffer(message->handles()->Get(handle_pos)->handle()->data(),
      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
4 warnings generated.
[63/169] Building CXX object src/plasma/CMakeFiles/plasma_store.dir/store.cc.o
../src/plasma/store.cc:117:3: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
  CudaDeviceManager::GetInstance(&manager_);
  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~
../src/plasma/store.cc:164:5: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
    manager_->GetContext(device_num - 1, &context_);
    ^~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~
../src/plasma/store.cc:194:7: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
      context_->Allocate(data_size + metadata_size, &gpu_handle);
      ^~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/plasma/store.cc:220:5: warning: ignoring return value of function declared with 'warn_unused_result' attribute [-Wunused-result]
    gpu_handle->ExportForIpc(&entry->ipc_handle);
    ^~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~
4 warnings generated.
[125/169] Building CXX object src/plasma/CMakeFiles/client_tests.dir/test/client_tests.cc.o
../src/plasma/test/client_tests.cc:462:35: warning: 'kDeprecatedPlasmaDefaultReleaseDelay' is deprecated: PLASMA_DEFAULT_RELEASE_DELAY is deprecated [-Wdeprecated-declarations]
  int64_t default_delay = plasma::kDeprecatedPlasmaDefaultReleaseDelay;
                                  ^
../src/plasma/client.h:37:16: note: 'kDeprecatedPlasmaDefaultReleaseDelay' has been explicitly marked deprecated here
__attribute__((deprecated(""PLASMA_DEFAULT_RELEASE_DELAY is deprecated"")))
               ^
1 warning generated.
{code}
",pull-request-available,['C++ - Plasma'],ARROW,Bug,Major,2018-07-19 14:09:15,14
13173184,[Website] Add Community tab to website,"This could be similar to http://spark.apache.org/community.html and the ""Community"" tab at the top navigation",pull-request-available,['Website'],ARROW,New Feature,Major,2018-07-19 12:56:36,14
13173085,[Packaging] README.md does not mention setting GitHub API token in user's crossbow repo settings,This environment variable must be configured in the Travis CI UI. I assume it's a similar story for Appveyor,pull-request-available,['Packaging'],ARROW,Bug,Major,2018-07-19 02:18:30,3
13173080,[Packaging] crossbow submit results in duplicate Travis CI build,"I ran

{code}
$ python crossbow.py submit wheel-osx-cp36m
{code}

and two Travis builds were created, see https://travis-ci.org/wesm/crossbow/branches and build-5-wheel-osx-cp36m. Not sure why",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2018-07-19 01:58:46,3
13173078,[Packaging] Crossbow builds can hang if you cloned using SSH,"{code}
$ git clone -b ARROW-2326 git@github.com:wesm/arrow.git arrow
Cloning into 'arrow'...
The authenticity of host 'github.com (192.30.253.112)' can't be established.
RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.
Are you sure you want to continue connecting (yes/no)? 
{code}

The workaround for now is to reclone with https",pull-request-available,['Packaging'],ARROW,Bug,Major,2018-07-19 01:51:00,8
13173057,[Packaging] Don't attempt to download arrow archive in linux builds,With the version increment the rakefile expects there is an already uploaded archive.,pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-18 22:34:21,3
13172763,[Python] Add pytest mark to opt into TensorFlow-related unit tests,"After pulling in ARROW-1744, I hit this error: 

{code}
pyarrow/tests/test_plasma_tf_op.py::test_plasma_tf_op FAILED                   [ 82%]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> captured stdout >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
TensorFlow version: 1.8.0
Compiling Plasma TensorFlow Op...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> captured stderr >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
+++ dirname /home/wesm/code/arrow/python/pyarrow/tensorflow/build.sh
++ cd /home/wesm/code/arrow/python/pyarrow/tensorflow
++ pwd
+ PYARROW_TENSORFLOW_DIR=/home/wesm/code/arrow/python/pyarrow/tensorflow
++ python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_compile_flags()))'
+ TF_CFLAGS='-I/home/wesm/miniconda/envs/arrow-dev/lib/python3.6/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0'
++ python -c 'import tensorflow as tf; print("" "".join(tf.sysconfig.get_link_flags()))'
+ TF_LFLAGS='-L/home/wesm/miniconda/envs/arrow-dev/lib/python3.6/site-packages/tensorflow -ltensorflow_framework'
++ uname
+ '[' Linux == Darwin ']'
+ NDEBUG=-DNDEBUG
++ pkg-config --cflags --libs plasma arrow arrow-python
+ g++ -std=c++11 -g -shared /home/wesm/code/arrow/python/pyarrow/tensorflow/plasma_op.cc -o /home/wesm/code/arrow/python/pyarrow/tensorflow/plasma_op.so -DNDEBUG -I/home/wesm/local/include -I/home/wesm/miniconda/envs/arrow-dev/include/python3.6m -I/home/wesm/local/include -L/home/wesm/local/lib -lplasma -larrow_python -larrow -fPIC -I/home/wesm/miniconda/envs/arrow-dev/lib/python3.6/site-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -L/home/wesm/miniconda/envs/arrow-dev/lib/python3.6/site-packages/tensorflow -ltensorflow_framework -O2
/home/wesm/code/arrow/python/pyarrow/tensorflow/plasma_op.cc:33:10: fatal error: arrow/adapters/tensorflow/convert.h: No such file or directory
 #include ""arrow/adapters/tensorflow/convert.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

use_gpu = False

    @pytest.mark.plasma
    def test_plasma_tf_op(use_gpu=False):
        import pyarrow.plasma as plasma
    
>       plasma.build_plasma_tensorflow_op()

pyarrow/tests/test_plasma_tf_op.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyarrow/plasma.py:56: in build_plasma_tensorflow_op
    subprocess.check_call([""bash"", script_path])
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

popenargs = (['bash', '/home/wesm/code/arrow/python/pyarrow/tensorflow/build.sh'],)
kwargs = {}, retcode = 1
cmd = ['bash', '/home/wesm/code/arrow/python/pyarrow/tensorflow/build.sh']

    def check_call(*popenargs, **kwargs):
        """"""Run command with arguments.  Wait for command to complete.  If
        the exit code was zero then return, otherwise raise
        CalledProcessError.  The CalledProcessError object will have the
        return code in the returncode attribute.
    
        The arguments are the same as for the call function.  Example:
    
        check_call([""ls"", ""-l""])
        """"""
        retcode = call(*popenargs, **kwargs)
        if retcode:
            cmd = kwargs.get(""args"")
            if cmd is None:
                cmd = popenargs[0]
>           raise CalledProcessError(retcode, cmd)
E           subprocess.CalledProcessError: Command '['bash', '/home/wesm/code/arrow/python/pyarrow/tensorflow/build.sh']' returned non-zero exit status 1.

../../../miniconda/envs/arrow-dev/lib/python3.6/subprocess.py:291: CalledProcessError
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
> /home/wesm/miniconda/envs/arrow-dev/lib/python3.6/subprocess.py(291)check_call()
-> raise CalledProcessError(retcode, cmd)
{code}

I misdiagnosed the problem originally (which was my stale Arrow install), but I think it would be good to have a separate pytest mark for TensorFlow-related unit tests since they are also time consuming. The unit test in question now has both the plasma and tensorflow marks",pull-request-available,['Python'],ARROW,Bug,Major,2018-07-17 20:47:28,14
13172747,[Python] Array.to_numpy is invalid for boolean arrays,"This should raise for the time being:

{code}
In [1]: import pyarrow as pa

In [2]: arr = pa.array([True, False, True])

In [3]: arr
Out[3]: 
<pyarrow.lib.BooleanArray object at 0x7f051c73bcc8>
[
  True,
  False,
  True
]

In [4]: arr.to_numpy()
Out[4]: array([ True])
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-07-17 19:41:52,14
13172742,[Python] Add documentation for Array.to_numpy,"See patch for ARROW-564

As part of this we should probably make a ""Using pyarrow with NumPy"" section in the docs (there is one for pandas already)",pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-17 19:30:50,2
13172693,[Packaging] Fix centos-7 build,This is the only failing build: https://travis-ci.org/kszucs/crossbow/builds/404781005,pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-17 15:34:27,1
13172548,[Python] Add context manager APIs to RecordBatch*Writer/Reader classes,This would cause the {{close}} method to be called when the scope exits,pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-17 03:30:20,3
13172542,[Python] Add extra tips about using Parquet to store index-less pandas data,See https://github.com/apache/arrow/pull/2248,pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-17 02:18:55,14
13172520,"[Python][Parquet][C++] Null values in a single partition of Parquet dataset, results in invalid schema on read","{code:python}
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd

from datetime import datetime, timedelta


def generate_data(event_type, event_id, offset=0):
    """"""Generate data.""""""
    now = datetime.utcnow() + timedelta(seconds=offset)
    obj = {
        'event_type': event_type,
        'event_id': event_id,
        'event_date': now.date(),
        'foo': None,
        'bar': u'hello',
    }
    if event_type == 2:
        obj['foo'] = 1
        obj['bar'] = u'world'
    if event_type == 3:
        obj['different'] = u'data'
        obj['bar'] = u'event type 3'
    else:
        obj['different'] = None
    return obj


data = [
    generate_data(1, 1, 1),
    generate_data(1, 1, 3600 * 72),
    generate_data(2, 1, 1),
    generate_data(2, 1, 3600 * 72),
    generate_data(3, 1, 1),
    generate_data(3, 1, 3600 * 72),
]

df = pd.DataFrame.from_records(data, index='event_id')
table = pa.Table.from_pandas(df)

pq.write_to_dataset(table, root_path='/tmp/events', partition_cols=['event_type', 'event_date'])

dataset = pq.ParquetDataset('/tmp/events')
table = dataset.read()
print(table.num_rows)
{code}

Expected output:
{code:python}
6
{code}

Actual:
{code:python}
python example_failure.py
Traceback (most recent call last):
  File ""example_failure.py"", line 43, in <module>
    dataset = pq.ParquetDataset('/tmp/events')
  File ""/Users/sam/.virtualenvs/test-parquet/lib/python2.7/site-packages/pyarrow/parquet.py"", line 745, in __init__
    self.validate_schemas()
  File ""/Users/sam/.virtualenvs/test-parquet/lib/python2.7/site-packages/pyarrow/parquet.py"", line 775, in validate_schemas
    dataset_schema))
ValueError: Schema in partition[event_type=2, event_date=0] /tmp/events/event_type=3/event_date=2018-07-16 00:00:00/be001bf576674d09825539f20e99ebe5.parquet was different.
bar: string
different: string
foo: double
event_id: int64
metadata
--------
{'pandas': '{""pandas_version"": ""0.23.3"", ""index_columns"": [""event_id""], ""columns"": [{""metadata"": null, ""field_name"": ""bar"", ""name"": ""bar"", ""numpy_type"": ""object"", ""pandas_type"": ""unicode""}, {""metadata"": null, ""field_name"": ""different"", ""name"": ""different"", ""numpy_type"": ""object"", ""pandas_type"": ""unicode""}, {""metadata"": null, ""field_name"": ""foo"", ""name"": ""foo"", ""numpy_type"": ""float64"", ""pandas_type"": ""float64""}, {""metadata"": null, ""field_name"": ""event_id"", ""name"": ""event_id"", ""numpy_type"": ""int64"", ""pandas_type"": ""int64""}], ""column_indexes"": [{""metadata"": null, ""field_name"": null, ""name"": null, ""numpy_type"": ""object"", ""pandas_type"": ""bytes""}]}'}

vs

bar: string
different: null
foo: double
event_id: int64
metadata
--------
{'pandas': '{""pandas_version"": ""0.23.3"", ""index_columns"": [""event_id""], ""columns"": [{""metadata"": null, ""field_name"": ""bar"", ""name"": ""bar"", ""numpy_type"": ""object"", ""pandas_type"": ""unicode""}, {""metadata"": null, ""field_name"": ""different"", ""name"": ""different"", ""numpy_type"": ""object"", ""pandas_type"": ""empty""}, {""metadata"": null, ""field_name"": ""foo"", ""name"": ""foo"", ""numpy_type"": ""float64"", ""pandas_type"": ""float64""}, {""metadata"": null, ""field_name"": ""event_id"", ""name"": ""event_id"", ""numpy_type"": ""int64"", ""pandas_type"": ""int64""}], ""column_indexes"": [{""metadata"": null, ""field_name"": null, ""name"": null, ""numpy_type"": ""object"", ""pandas_type"": ""bytes""}]}'}
{code}

Apparently what is happening is that pyarrow is interpreting the schema from each of the partitions individually and the partitions for `event_type=3 / event_date=*`  both have values for the column `different` whereas the other columns do not. The discrepancy causes the `None` values of the other partitions to be labeled as `pandas_type` `empty` instead of `unicode`.",dataset dataset-parquet-read parquet,['Python'],ARROW,Bug,Major,2018-07-16 23:51:08,5
13172517,"[Python] Handle objects exporting the buffer protocol in open_stream, open_file, and RecordBatch*Reader APIs",I have hit this rough edge several times when doing interactive demos. If we can do so safely then this would improve usability,pull-request-available usability,['Python'],ARROW,Improvement,Major,2018-07-16 23:32:32,14
13172254,[C++/Python] Casting float NaN to int should raise an error on safe cast,"Currently the following code works:

{code}
pa.array([np.nan, 1.]).cast(pa.int64())
{code}

It produces:

{code}
<pyarrow.lib.Int64Array object at 0x111ca6778>
[
  -9223372036854775808,
  1
]
{code}

The expected behaviour should be that it raises.",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-07-15 14:09:05,3
13172215,[C++]Update RAT excludes for new install file names,Accidentially merged the previous PR adding them while thinking that the Travis failures was due to the {{manylinux1}} problems.,pull-request-available,['C++'],ARROW,Bug,Major,2018-07-14 20:34:25,8
13172199,[C++/Python]PARQUET_RPATH_ORIGIN=ON missing in manylinux1 build,{{auditwheel}} now also picks up non-Python binaries. Currently the RPATH setting on {{libparquet}} is not working.,pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-07-14 14:02:15,8
13172070,[Packaging] Fix artifact name matching for conda forge packages,"With the recently introduced conda recipe changes and version pinning the artifact names are not known before the build, they contain a hash too.

`arrow-cpp-0.9.1.dev366-py36_0.tar.bz2` => `arrow-cpp-0.9.1.dev366-py36_vc14h12fa3ca_0.tar.bz2`

We need to either remove the pins or use pattern matching in the status and sign commands.",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-07-13 15:42:08,3
13172065,[Packaging] Update nightly build in crossbow as well as the sample configuration,"After `ARROW-2836: [Packaging] Expand build matrices to multiple tasks` gets merged.

https://github.com/apache/arrow/pull/2259",pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-13 15:23:08,3
13172059,[Packaging] Upload additional debian artifacts,"We should add the following files to our artifact list:
{code}
    apache-arrow_{version}-1.debian.tar.xz
    apache-arrow_{version}-1.dsc
    apache-arrow_{version}.orig.tar.gz
{code}",pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-13 15:06:58,3
13172053,[Packaging] Test OSX wheels after build,"Similarly like the manylinux wheels, import optional modules and run pytest.

https://github.com/apache/arrow/blob/master/python/manylinux1/build_arrow.sh#L76",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-07-13 14:50:42,3
13171779,[Python] Speed up null testing with Pandas semantics,The {{PandasObjectIsNull}} helper function can be a significant contributor when converting a Pandas dataframe to Arrow format (e.g. when writing a dataframe to feather format). We can try to speed up the type checks in that function.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-07-12 17:23:25,2
13171696,[Packaging] Expand build matrices to multiple tasks,"Create release tags for each task instead a single one for the job, and create additional tasks per build variant. 
This will resolve the package naming conflicts, but increases the config verbosity (which is fine for now).",pull-request-available,['Packaging'],ARROW,Task,Major,2018-07-12 10:09:12,3
13171688,[C++] ReadAt/WriteAt are inconsistent with moving the files position,"Right now, there is inconsistent behaviour regarding moving the files position pointer after calling ReadAt or WriteAt. For example, the default implementation of ReadAt seeks to the desired offset and calls Read which moves the position pointer. MemoryMappedFile::ReadAt, however, doesn't change the position. WriteableFile::WriteAt seem to move the position in the current implementation, but there is no docstring which prescribes this behaviour.

Antoine suggested that *At methods shouldn't touch the position and it makes more sense, IMHO. The change isn't huge and doesn't seem to break anything internally, but it might break the existing user code.",pull-request-available,['C++'],ARROW,New Feature,Major,2018-07-12 09:40:59,2
13171520,[Python] Column.__repr__ will lock up Jupyter with large datasets,Output should be truncated in the middle,usability,['Python'],ARROW,Improvement,Major,2018-07-11 19:47:13,8
13171519,[Python] Pretty-print schema metadata in Schema.__repr__,"The default formatting is a bit unreadable

{code}
a: string
b: double
c: bool
metadata
--------
{b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [{""name"":'
            b' ""a"", ""field_name"": ""a"", ""pandas_type"": ""unicode"", ""numpy_type"":'
            b' ""object"", ""metadata"": null}, {""name"": ""b"", ""field_name"": ""b"", ""'
            b'pandas_type"": ""float64"", ""numpy_type"": ""float64"", ""metadata"": nu'
            b'll}, {""name"": ""c"", ""field_name"": ""c"", ""pandas_type"": ""bool"", ""nu'
            b'mpy_type"": ""object"", ""metadata"": null}], ""pandas_version"": ""0.23'
            b'.2""}'}
{code}",usability,['Python'],ARROW,Improvement,Major,2018-07-11 19:43:06,14
13171325,[GLib] Add GArrowORCFileReader,It's not a required change for 0.10.0. It's a nice-to-have change.,pull-request-available,['GLib'],ARROW,Improvement,Major,2018-07-11 07:42:46,1
13171287,[C++] LZ4 and Zstd build may be failed in parallel build,"I'm not sure the minimum requirements to reproduce this case.
But at least deb packages build in parallel is failed.",pull-request-available,['C++'],ARROW,Improvement,Minor,2018-07-11 00:11:44,1
13171199,"[C++] Clarification needed between ArrayBuilder::Init(), Resize() and Reserve()",It's still not clear to me why we have three builder methods that seem to do essentially the same thing. This should be clarified somewhere in the docstrings.,pull-request-available,['C++'],ARROW,Wish,Major,2018-07-10 16:37:01,14
13171161,[C++] Need AllocateBuffer / AllocateResizableBuffer variant with default memory pool,It's not very practical that you have to pass the default memory pool explicitly to {{AllocateBuffer}}.,pull-request-available,['C++'],ARROW,Wish,Major,2018-07-10 14:13:53,2
13171066,[C++] Search for flatbuffers in <root>/lib64,Newer versions of flatbuffers install into {{<root>/lib64}}. We also should look there for them.,pull-request-available,['C++'],ARROW,Bug,Major,2018-07-10 05:50:34,8
13170987,[C++] Only zero memory in BooleanBuilder in one place,Follow up work to ARROW-2790,pull-request-available,['C++'],ARROW,Improvement,Major,2018-07-09 22:02:53,14
13170974,[Python] RecordBatch.from_arrays does not validate array lengths are all equal,"Example reproduction as reported on https://github.com/apache/arrow/issues/2098

{code}
import pyarrow as pa
data=[pa.array([1]),pa.array([""tokyo"", ""like"", ""happy""]),pa.array([""derek""])]
batch = pa.RecordBatch.from_arrays(data, ['id', 'tags', 'name'])
{code}",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-07-09 20:57:12,14
13170969,[Python] Better error message when passing SparseDataFrame into Table.from_pandas,"This can be a rough edge for users. Note that pandas sparse support is being considered for deprecation

original issue https://github.com/apache/arrow/issues/1894",pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-09 20:42:17,5
13170967,[C++] Enable libraries to be installed in msys2 on Windows,See GitHub issue for original question https://github.com/apache/arrow/issues/1952,windows,['C++'],ARROW,Improvement,Major,2018-07-09 20:39:13,1
13170917,[CI] Suppress DEBUG logging when building Java library in C++ CI entries,"The Java build is rather verbose with logging output from tests, in addition to the warnings from the checkstyle plugin, e.g.

{code}
08:03:10.299 [main] DEBUG org.apache.arrow.memory.BaseAllocator - closed allocator[ROOT].
08:03:10.334 [main] DEBUG org.apache.arrow.vector.ipc.message.ArrowRecordBatch - Buffer in RecordBatch at 0, length: 2
08:03:10.335 [main] DEBUG org.apache.arrow.vector.ipc.message.ArrowRecordBatch - Buffer in RecordBatch at 8, length: 16
08:03:10.338 [main] DEBUG org.apache.arrow.vector.ipc.WriteChannel - Writing buffer with size: 6
08:03:10.338 [main] DEBUG org.apache.arrow.vector.ipc.WriteChannel - Writing buffer with size: 2
08:03:10.346 [main] DEBUG org.apache.arrow.vector.ipc.WriteChannel - Writing buffer with size: 4
08:03:10.349 [main] DEBUG org.apache.arrow.vector.ipc.WriteChannel - Writing buffer with size: 152
08:03:10.353 [main] DEBUG org.apache.arrow.vector.ipc.WriteChannel - Writing buffer with size: 4
{code}",ci pull-request-available,"['Continuous Integration', 'Java']",ARROW,Improvement,Major,2018-07-09 17:12:49,2
13170818,"[Python] Unify PyObject* sequence conversion paths for built-in sequences, NumPy arrays","Original issue title: ""Struct type inference and conversion works for lists but not NumPy arrays with dtype object""

Example, setup:

{code}
import pandas as pd

s = pd.Series([{'data': {'document_id': None,
  'document_type': None,
  'master_customer_id': None,
  'message': 'User Login Request',
  'policy_id': None,
  'sequence_no': 14,
  'user_name': None},
 'header': {'actor_id': None,
  'actor_type': None,
  'brand_code': 'ES',
  'event_origin': None,
  'event_timestamp': '2018-01-01T18:25:43.511Z',
  'event_type': 'LOGIN',
  'master_customer_id': '14',
  'source': 'CUSTOMER_AUTH_SERVICE',
  'source_id': None,
  'source_version': None},
 'payload_version': '1',
 'status': {'status_code': 100, 'status_message': 'Success'}}])
{code}

This works:

{code}
In [24]: pa.array(list(s))
Out[24]: 
<pyarrow.lib.StructArray object at 0x7f8435b09c28>
[
  {'data': {'document_id': None, 'document_type': None, 'master_customer_id': None, 'message': 'User Login Request', 'policy_id': None, 'sequence_no': 14, 'user_name': None}, 'header': {'actor_id': None, 'actor_type': None, 'brand_code': 'ES', 'event_origin': None, 'event_timestamp': '2018-01-01T18:25:43.511Z', 'event_type': 'LOGIN', 'master_customer_id': '14', 'source': 'CUSTOMER_AUTH_SERVICE', 'source_id': None, 'source_version': None}, 'payload_version': '1', 'status': {'status_code': 100, 'status_message': 'Success'}}
]
{code}

This does not:

{code}
In [23]: pa.array(s)
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-23-eba23a1638b7> in <module>()
----> 1 pa.array(s)

~/code/arrow/python/pyarrow/array.pxi in pyarrow.lib.array()
    175             values, type = pdcompat.get_datetimetz_type(values, obj.dtype,
    176                                                         type)
--> 177             return _ndarray_to_array(values, mask, type, from_pandas, pool)
    178     else:
    179         if mask is not None:

~/code/arrow/python/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()
     75 
     76     with nogil:
---> 77         check_status(NdarrayToArrow(pool, values, mask,
     78                                     use_pandas_null_sentinels,
     79                                     c_type, &chunked_out))

~/code/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()
     79         message = frombytes(status.message())
     80         if status.IsInvalid():
---> 81             raise ArrowInvalid(message)
     82         elif status.IsIOError():
     83             raise ArrowIOError(message)

ArrowInvalid: ../src/arrow/python/numpy_to_arrow.cc:1742 code: converter.Convert()
Error inferring Arrow type for Python object array. Got Python object of type dict but can only handle these types: string, bool, float, int, date, time, decimal, bytearray, list, array
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-07-09 10:26:23,14
13170740,[C++] Strip uninformative lcov output from Travis CI logs,"We have about 650 lines of the type:

{code}
geninfo: WARNING: no data found for /usr/include/c++/4.8/istream
{code}

We should maybe pipe all the lcov output to a file and print any lines that reference things other than /usr/include",pull-request-available,['C++'],ARROW,Improvement,Major,2018-07-08 18:54:16,2
13170658,"[Python] Add unit tests for ProxyMemoryPool, enable new default MemoryPool to be constructed",I could not find unit tests for ProxyMemoryPool,pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-07 22:09:45,2
13170656,[Python] Enable memory-mapping to be toggled in get_reader when reading Parquet files,See relevant discussion in ARROW-2654,parquet pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-07 20:37:13,14
13170644,[Python]Inconsistent handling of np.nan,"Currently we handle {{np.nan}} differently between having a list or a numpy array as an input to {{pa.array()}}:

{code}
>>> pa.array(np.array([1, np.nan]))
<pyarrow.lib.DoubleArray object at 0x11680bea8>
[
  1.0,
  nan
]

>>> pa.array([1., np.nan])
Out[9]:
<pyarrow.lib.DoubleArray object at 0x10bdacbd8>
[
  1.0,
  NA
]
{code}

I would actually think the last one is the correct one. Especially once one casts this to an integer column. There the first one produces a column with INT_MIN and the second one produces a real null.

But, in {{test_array_conversions_no_sentinel_values}}we check that {{np.nan}}does not produce a Null.

Even weirder: 

{code}
>>> df = pd.DataFrame({'a': [1., None]})
>>> df
     a
0  1.0
1  NaN
>>> pa.Table.from_pandas(df).column(0)
<Column name='a' type=DataType(double)>
chunk 0: <pyarrow.lib.DoubleArray object at 0x104bbf958>
[
  1.0,
  NA
]
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-07-07 15:28:44,8
13170580,[C++] Put hashing function into src/arrow/util,"See [https://github.com/apache/arrow/pull/2220]

We should decide what our default go-to hash function should be (maybe murmur3?) and put it intosrc/arrow/util",easytask,['C++'],ARROW,Improvement,Major,2018-07-06 20:47:59,2
13170576,[Docs] Move release management guide to project wiki,I have begun doing this here https://cwiki.apache.org/confluence/display/ARROW/Release+Management+Guide. I think we should remove RELEASE_MANAGEMENT.md and add a note to dev/release/README.md to navigate to the Confluence page,pull-request-available,['Wiki'],ARROW,Improvement,Major,2018-07-06 20:37:32,14
13170457,[Python] Add safe option to Table.from_pandas to avoid unsafe casts,"Ported over from [https://github.com/apache/arrow/issues/2217]


```python
In [8]: import pandas as pd
   ...: import pyarrow as arw

In [9]: df = pd.DataFrame({'A': list('abc'), 'B': np.arange(3)})
   ...: df
Out[9]:
   A  B
0  a  0
1  b  1
2  c  2

In [10]: schema = arw.schema([
    ...:     arw.field('A', arw.string()),
    ...:     arw.field('B', arw.int32()),
    ...: ])

In [11]: tbl = arw.Table.from_pandas(df, preserve_index=False, schema=schema)
    ...: tbl
Out[11]:
pyarrow.Table
A: string
B: int32
metadata
--------
{b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [{""name"":'
            b' ""A"", ""field_name"": ""A"", ""pandas_type"": ""unicode"", ""numpy_type"":'
            b' ""object"", ""metadata"": null}, {""name"": ""B"", ""field_name"": ""B"", ""'
            b'pandas_type"": ""int32"", ""numpy_type"": ""int32"", ""metadata"": null}]'
            b', ""pandas_version"": ""0.23.1""}'}

In [12]: tbl.to_pandas().equals(df)
Out[12]: True
```
...so if the `schema` matches the pandas datatypes all is well - we can roundtrip the DataFrame.

Now, say we have some bad data such that column 'B' is now of type float64. The datatypes of the DataFrame don't match the explicitly supplied `schema` object but rather than raising a `TypeError` the data is silently truncated and the roundtrip DataFrame doesn't match our input DataFame without even a warning raised!
```python
In [13]: df['B'].iloc[0] = 1.23
    ...: df
Out[13]:
   A     B
0  a  1.23
1  b  1.00
2  c  2.00

In [14]: # I would expect/want this to raise a TypeError since the schema doesn't match the pandas datatypes
    ...: tbl = arw.Table.from_pandas(df, preserve_index=False, schema=schema)
    ...: tbl
Out[14]:
pyarrow.Table
A: string
B: int32
metadata
--------
{b'pandas': b'{""index_columns"": [], ""column_indexes"": [], ""columns"": [{""name"":'
            b' ""A"", ""field_name"": ""A"", ""pandas_type"": ""unicode"", ""numpy_type"":'
            b' ""object"", ""metadata"": null}, {""name"": ""B"", ""field_name"": ""B"", ""'
            b'pandas_type"": ""int32"", ""numpy_type"": ""float64"", ""metadata"": null'
            b'}], ""pandas_version"": ""0.23.1""}'}

In [15]: tbl.to_pandas()  # <-- SILENT TRUNCATION!!!
Out[15]:
   A  B
0  a  1
1  b  1
2  c  2

```

To be clear, I would really like `Table.from_pandas` to raise a `TypeError` if the DataFrame types don't match an explicitly supplied schema and would hope this current behaviour would be considered a bug.
",pull-request-available,['Python'],ARROW,Improvement,Major,2018-07-06 10:16:52,3
13170297,"[C++] Simplify symbols.map file, use when building libarrow_python","I did a little work on this in https://github.com/apache/arrow/pull/2096. While that patch was not merged, the changes related to symbol visibility ought to be plucked into a new patch",pull-request-available,['C++'],ARROW,Improvement,Major,2018-07-05 15:24:43,2
13170265,[Python] Run TensorFlow import workaround only on Linux,Follow up to ARROW-2657 https://github.com/apache/arrow/pull/2210,pull-request-available,['Python'],ARROW,Bug,Major,2018-07-05 12:44:49,14
13169857,[Python] Memory Issue passing table from python to c++ via cython,"I wanted to create a simple example of reading a table in Python and pass it to C+, but I'm doing something wrong or there is a memory issue. When the table gets to C+ and I print out column names it also prints out a lot of junk and what looks like pydocs. Let me know if you need any more info. Thanks!

*demo.py*
{code:python}
import numpy
from psy.automl import cyth
import pandas as pd
from absl import app

def main(argv):
  sup = pd.DataFrame({
  'int': [1, 2],
  'str': ['a', 'b']
  })
  table = pa.Table.from_pandas(sup)
  cyth.c_t(table)
{code}

*cyth.pyx*
{code:python}
import pandas as pd
import pyarrow as pa
from pyarrow.lib cimport *

cdef extern from ""cyth.h"" namespace ""psy"":
 void t(shared_ptr[CTable])

def c_t(obj):
 # These print work
 # for i in range(obj.num_columns):
 # print(obj.column(i).name
  cdef shared_ptr[CTable] tbl = pyarrow_unwrap_table(obj)
  t(tbl)
{code}

 *cyth.h*
{code:c++}
#include <iostream>
#include <string>
#include ""arrow/api.h""
#include ""arrow/python/api.h""
#include ""Python.h""

namespace psy {

void t(std::shared_ptr<arrow::Table> pytable) {

// This works
  std::cout << ""NUM"" << pytable->num_columns();

// This prints a lot of garbage
  for(int i = 0; i < pytable->num_columns(); i++) {
  std::cout << pytable->column(i)->name();
  }
 }
}
{code}

",cython pull-request-available,"['Documentation', 'Python']",ARROW,Bug,Major,2018-07-03 16:25:04,2
13169473,[Python] Download boost using curl in manylinux1 image,This is the only artifact where we use {{wget}} which hasnot the necessary level of TLS support to speak with the bintray servers.,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2018-07-02 08:11:52,8
13169317,[C++][Python] Deprecate and rename add_metadata methods,"Deprecate and replace `pyarrow.Field.add_metadata` (and other likely named methods) with replace_metadata, set_metadata or with_metadata. Knowing Spark's immutable API, I would have chosen with_metadata but I guess this is probably not what the average Python user would expect as naming.",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Minor,2018-06-30 09:53:08,3
13168585,"[Python] When installing pyarrow via pip, a debug build is created",I noticed this in the log for https://github.com/apache/arrow/issues/2173. We should probably change the default build type to {{release}},pull-request-available,['Python'],ARROW,Bug,Major,2018-06-27 10:26:09,14
13168284,[C++] ORC ExternalProject needs to declare dependency on vendored protobuf,"When we vendor ORC and Protobuf, they also need to declare their dependencies on each other. Otherwise they are build in parallel and the ORC build fails.",pull-request-available,['C++'],ARROW,Bug,Major,2018-06-26 08:05:43,8
13168279,[Python] Writing to parquet crashes when writing a ListArray of empty lists ,"When writing a ListArray which contains only empty lists to Parquet, Pyarrow crashes. Here is a minimal code snippet which reproduces the crash:
{code:java}
import pyarrow as pa
from pyarrow import parquet as pq

array = pa.array([[]], type=pa.list_(pa.int32()))
table = pa.Table.from_arrays([array], [""A""])
pq.write_table(table, ""tmp.parq""){code}
When the ListArray has at least one non-empty list, the issue disappears.

",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-06-26 07:39:41,2
13168136,[Python] pa.array from np.datetime[D]and type=pa.date64 produces invalid results,"{code}
In [1]: import pyarrow as pa

In [2]: import numpy as np

In [3]: pa.array(np.array(['2015-01-01', '2015-01-01'], dtype='datetime64[D]'))
Out[3]:
<pyarrow.lib.Date32Array object at 0x11571bae8>
[
  datetime.date(2015, 1, 1),
  datetime.date(2015, 1, 1)
]

In [4]: pa.array(np.array(['2015-01-01', '2015-01-01'], dtype='datetime64[D]'), type=pa.date64())
Out[4]:
<pyarrow.lib.Date64Array object at 0x10965d138>
[
  datetime.date(2015, 1, 1),
  datetime.date(1970, 1, 1)
]
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-06-25 19:45:10,8
13168071,[Python] Add address property to Buffer,This would allow getting the start address of the buffer's data.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-25 14:47:57,2
13167868,[Python] Cython api example doesn't work by default on macOS,"The setup.py + example.pyx given in the docs:

[https://arrow.apache.org/docs/python/extending.html#example]

doesn't work on macOS.



The first issue is the error:

*example.cpp:603:10:* *fatal error:* *'unordered_map' file not found*

because (AFAIU) macOS clang doesn't include the required C++11 lib by default.

This can be solved by adding:
{code:java}
os.environ['CFLAGS'] = '-std=c++11 -stdlib=libc++'
{code}
to setup.py



The second issue is that the line
{code:java}
ext.library_dirs.append(pa.get_library_dirs())
{code}
should be
{code:java}
ext.library_dirs.extend(pa.get_library_dirs())
{code}


otherwise this causes a (completely uninformative) typerror during the build because library dirs ends up being a list of list instead of a list of string.

",pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Minor,2018-06-24 11:18:28,3
13167308,[C++] The latest Boost version is wrong,Boost 1.68.0 isn't released yet.,pull-request-available,['C++'],ARROW,Bug,Minor,2018-06-21 05:06:19,1
13167235,[Packaging] Determine whether all the expected artifacts are uploaded,"I can imagine the definition of the expected artifacts as follows:

{code}
conda-win:
    platform: win
    template: conda-recipes/appveyor.yml
    artifacts:
      - arrow-cpp-{version}-py35_vc14_0.tar.bz2
      - pyarrow-{version}-py35_vc14_0.tar.bz2
      - ...
{code}",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-06-20 19:00:49,3
13166986,[Python] ndarray to arrow conversion fails when downcasted from pandas to_numeric,"The following snippet:
{code:java}
import numpy as np
import pandas as pd
import pyarrow as pa

pa.array(pd.to_numeric(pd.Series(np.array([65536,2,3], dtype=np.uint64)), downcast='unsigned'), 
from_pandas=True, type='uint32')
{code}
fails to convert with message:
{noformat}
ArrowNotImplementedError Traceback (most recent call last)
<ipython-input-2-b259c5cb7044> in <module>()
4 
5 pa.array(pd.to_numeric(pd.Series(np.array([65536,2,3], dtype=np.uint64)), downcast='unsigned'), 
----> 6 from_pandas=True, type='uint32')

array.pxi in pyarrow.lib.array()

array.pxi in pyarrow.lib._ndarray_to_array()

error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: Unsupported numpy type 6{noformat}


This is a Windows 64-bit machine, running Python 3.6.5, pyarrow 0.9.0, pandas 0.23.1 and numpy 1.14.5.

Seems to be fine for uint16 or uint8 downcasting. Unfortunately I didn't had the time to dig deeper or try on a Linux machine but it feels like its related to the LLP64 model.",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-06-19 18:03:39,2
13166854,[C++] Link error with Arrow C++ build with -DARROW_ORC=ON on CentOS 7,"Build master with -DARROW_ORC=ON:

{code:shell}
sudo yum install -y epel-release
sudo yum groupinstall -y ""Development Tools""
sudo yum install -y \
  autoconf-archive \
  boost-devel \
  cmake3 \
  git \
  gobject-introspection-devel \
  gtk-doc \
  jemalloc-devel \
  pkg-config \
  tar
git clone https://github.com/apache/arrow.git
mkdir -p arrow/cpp/build
cd arrow/cpp/build
LANG=C cmake3 .. -DCMAKE_BUILD_TYPE=release -DARROW_ORC=ON
make -j4
sudo make install
{code}

Sample program:

{code:cpp}
#include <arrow/api.h>

int main(void) {
  return 0;
}
{code}

Build the sample program:

{noformat}
% g++ -std=c++11 -o sample $(PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig pkg-config --cflags --libs arrow) sample.cpp
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFields(google::protobuf::UnknownFieldSet const&, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::Message::SpaceUsed() const'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::DescriptorPool::FindFileByName(std::string const&) const'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteDouble(int, double, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::BytesUntilTotalBytesLimit() const'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::DescriptorPool::InternalAddGeneratedFile(void const*, int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::StringTypeHandlerBase::New()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::ZeroCopyOutputStream::~ZeroCopyOutputStream()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::Message::CheckTypeAndMergeFrom(google::protobuf::MessageLite const&)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageLite::ParseFromZeroCopyStream(google::protobuf::io::ZeroCopyInputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::ReadRaw(void*, int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageFactory::InternalRegisterGeneratedFile(char const*, void (*)(std::string const&))'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::VarintSize32Fallback(unsigned int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::LogMessage::LogMessage(google::protobuf::LogLevel, char const*, int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::empty_string_'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::StringTypeHandlerBase::Delete(std::string*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::WriteVarint64(unsigned long)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::DescriptorPool::generated_pool()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteEnum(int, int, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteString(int, std::string const&, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(google::protobuf::UnknownFieldSet const&, unsigned char*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::ReadTagFallback()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::OnShutdown(void (*)())'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Swap(google::protobuf::internal::RepeatedPtrFieldBase*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageFactory::generated_factory()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::UnknownFieldSet::AddVarint(int, unsigned long)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::Skip(int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(google::protobuf::UnknownFieldSet const&)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::VarintSize64(unsigned long)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::ReadVarint32Fallback(unsigned int*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteSInt64(int, long, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::ReadString(google::protobuf::io::CodedInputStream*, std::string*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::BytesUntilLimit() const'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageLite::SerializeToString(std::string*) const'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteFixed64(int, unsigned long, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::LogMessage::~LogMessage()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteUInt32(int, unsigned int, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::LogFinisher::operator=(google::protobuf::internal::LogMessage&)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::InitEmptyString()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteSInt32(int, int, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormat::SkipField(google::protobuf::io::CodedInputStream*, unsigned int, google::protobuf::UnknownFieldSet*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::WriteStringWithSizeToArray(std::string const&, unsigned char*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::string const&, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `typeinfo for google::protobuf::io::ZeroCopyOutputStream'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageLite::ParseFromString(std::string const&)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::Message::GetTypeName() const'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::LogMessage::operator<<(char const*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::ReadBytes(google::protobuf::io::CodedInputStream*, std::string*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::UnknownFieldSet::~UnknownFieldSet()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteUInt64(int, unsigned long, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `typeinfo for google::protobuf::Message'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::ReflectionOps::Merge(google::protobuf::Message const&, google::protobuf::Message*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::Message::~Message()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageLite::ParseFromArray(void const*, int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::Message::DiscardUnknownFields()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(google::protobuf::Descriptor const*, google::protobuf::Message const*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::WriteVarint32(unsigned int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::GoogleOnceInitImpl(long*, google::protobuf::Closure*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::UnknownFieldSet::ClearFallback()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::WriteVarint32FallbackToArray(unsigned int, unsigned char*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::UnknownFieldSet::UnknownFieldSet()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(int, google::protobuf::MessageLite const&, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedOutputStream::WriteVarint64ToArray(unsigned long, unsigned char*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::ReadLittleEndian64Fallback(unsigned long*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::FunctionClosure0::~FunctionClosure0()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteBytesMaybeAliased(int, std::string const&, google::protobuf::io::CodedOutputStream*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::Message::InitializationErrorString() const'
/usr/local/lib64/libarrow.so: undefined reference to `typeinfo for google::protobuf::io::ZeroCopyInputStream'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::empty_string_once_init_'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::VerifyVersion(int, int, char const*)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::ZeroCopyInputStream::~ZeroCopyInputStream()'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::PushLimit(int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::PopLimit(int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::GeneratedMessageReflection::GeneratedMessageReflection(google::protobuf::Descriptor const*, google::protobuf::Message const*, int const*, int, int, int, google::protobuf::DescriptorPool const*, google::protobuf::MessageFactory*, int)'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::io::CodedInputStream::ReadVarint64Fallback(unsigned long*)'
/usr/local/lib64/libarrow.so: undefined reference to `vtable for google::protobuf::internal::FunctionClosure0'
/usr/local/lib64/libarrow.so: undefined reference to `google::protobuf::internal::WireFormatLite::WriteBool(int, bool, google::protobuf::io::CodedOutputStream*)'
collect2: error: ld returned 1 exit status
{noformat}
",pull-request-available,['C++'],ARROW,Bug,Minor,2018-06-19 07:05:22,1
13166770,[C++] Clean up cmake CXX_STANDARD and PIC flag setting,"We're using {{-std=c++11}} in afew non-external project places as well as setting {{-fPIC}}. CMake provides the {{CMAKE_CXX_STANDARD}} flag (which we are also using) and the {{CMAKE_POSITION_INDEPENDENT_CODE}} flag for setting these options in a cross platform way (where it matters).

We should use these flags instead of using platform conditional checks to set their values explicitly.",pull-request-available,['C++'],ARROW,Task,Major,2018-06-18 18:53:33,14
13166588,[Python] Make manylinux1 base image independent of Python patch releases,"Currently we have hardcoded some patch release versions of Python. As the image is continuously updated, this breaks from time to time.",pull-request-available,['Python'],ARROW,Bug,Major,2018-06-17 07:59:13,8
13166502,Address apt flakiness with launchpad.net,"We've had some failing builds with errors of this variety:

https://travis-ci.org/apache/arrow/jobs/392830710#L689

I'm not sure the nature of the flakiness, but it would be good if we can make more robust",ci-failure,['Continuous Integration'],ARROW,Bug,Major,2018-06-16 01:41:51,2
13166354,[C++/Python] Variable step size slicing for arrays,"Array slicing should support variable step sizes

The current behavior raises an {{IndexError}}, e.g.
{code}
In [8]: import pyarrow as pa
In [9]: pa.array([1, 2, 3])[::-1]
---------------------------------------------------------------------------
IndexError Traceback (most recent call last)
<ipython-input-9-20da5f9ab67a> in <module>()
----> 1 pa.array([1, 2, 3])[::-1]

array.pxi in pyarrow.lib.Array.__getitem__()

array.pxi in pyarrow.lib._normalize_slice()

IndexError: only slices with step 1 supported
{code}",pull-request-available,"['C++', 'Python']",ARROW,New Feature,Minor,2018-06-15 13:39:36,14
13166160,[Python/C++] Pandas-Arrow doesn't roundtrip when column of lists has empty first element,"Hi, I thought this had been fixed in the past, but this simple use case still breaks:


{code:java}
df = pd.DataFrame(dict(x=[[], [""a""]]))
tbl = pyarrow.Table.from_pandas(df)
print(tbl.schema)
{code}
results in a wrong inferred type of ""list<item: null>"":


{noformat}
x: list<item: null>
  child 0, item: null
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""index_columns"": [""__index_level_0__""], ""column_indexes"": [{""na'
            b'me"": null, ""field_name"": null, ""pandas_type"": ""unicode"", ""numpy_'
            b'type"": ""object"", ""metadata"": {""encoding"": ""UTF-8""}}], ""columns"":'
            b' [{""name"": ""x"", ""field_name"": ""x"", ""pandas_type"": ""list[empty]"",'
            b' ""numpy_type"": ""object"", ""metadata"": null}, {""name"": null, ""fiel'
            b'd_name"": ""__index_level_0__"", ""pandas_type"": ""int64"", ""numpy_typ'
            b'e"": ""int64"", ""metadata"": null}], ""pandas_version"": ""0.22.0""}'}{noformat}
When converting the Table back to pandas all elementsare now None too:


{code:java}
df2 = tbl.to_pandas()
print(df2)

       x

0     [] 
1 [None]
{code}


",pull-request-available,['Python'],ARROW,Bug,Major,2018-06-14 17:46:19,2
13165923,[C++] Internal GetValues function in arrow::compute should check for nullptr,See https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/kernels/util-internal.h#L33 and mailing list discussion,pull-request-available,['C++'],ARROW,Bug,Major,2018-06-13 20:04:34,2
13165907,[C++] Implement Table::Slice methods using Column::Slice,see discussion in ARROW-2358,pull-request-available,['C++'],ARROW,Improvement,Major,2018-06-13 19:02:19,6
13165665,[Python] Examine usages of Invalid and TypeError errors in numpy_to_arrow.cc to see if we are using the right error type in each instance,See discussion in [https://github.com/apache/arrow/pull/2075],pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-12 20:27:03,14
13165489,[Python] Add simple examples to Array.cast docstring,"See discussion in https://github.com/apache/arrow/pull/2129#issuecomment-396460198

The examples should include passing a DataType as a string alias, as well as creating a DataType instance like {{timestamp('ms')}}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-12 04:15:10,8
13165488,[C++/Python] Add Table method that replaces a column with a new supplied column,"This method would produce a new table since table objects are immutable. Example:

{code}
new_column = table.column(name).cast(pa.timestamp('ms'))
new_table = table.set_column(name, new_column)
{code}

See discussion in https://github.com/apache/arrow/pull/2129#issuecomment-396423002",pull-request-available,['Python'],ARROW,New Feature,Major,2018-06-12 04:13:14,8
13165155,[Python]Add test for writing dictionary encoded columns to chunked Parquet files,This would have shown a bug that was in 0.9.0. It is already fixed as part of a different bugfix but we should add a test that ensures that it does not resurface.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-09 20:35:10,8
13165088,[Rust] Travis fails due to formatting diff,"There is a small formatting difference that makes master red: https://travis-ci.org/apache/arrow/jobs/389844420

[~andygrove] can you have a look at this?",ci-failure pull-request-available,['Rust'],ARROW,Bug,Major,2018-06-09 08:19:42,10
13165062,[C++] Plasma does not follow style conventions for variable and function names,See discussion in https://github.com/apache/arrow/pull/2031. We should make a pass through Plasma and rename functions and variables to conform with the rest of the codebase,pull-request-available,['C++'],ARROW,Bug,Major,2018-06-09 00:58:19,14
13165061,[Python] Remove references to timestamps_to_ms argument from documentation,timestamps_to_ms is documented in https://arrow.apache.org/docs/python/pandas.html but does not exist in https://arrow.apache.org/docs/python/generated/pyarrow.Table.html,pull-request-available,['Python'],ARROW,Improvement,Minor,2018-06-09 00:46:33,8
13164994,[Python] Resource Warning (Unclosed File) when using pyarrow.parquet.read_table(),"pyarrow version from python repl:
{noformat}
>>> import pyarrow
>>> pyarrow.__version__
'0.9.0.post1'{noformat}
python interpreter information:
{noformat}
Python 3.6.5 (default, Mar 30 2018, 06:42:10)
[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin{noformat}
arbitrary, potentially relevant system information:
{noformat}
OS                       : macOS High Sierra (10.13.4)
homebrew package         : python: stable 3.6.5 (bottled), devel 3.7.0b4, HEAD
pip version              : pip 10.0.1
pipenv version           : pipenv, version 2018.05.18
pyarrow version (via pip): pyarrow    0.9.0.post1
cython version (via pip) : Cython     0.28.2{noformat}


Issue Description:

I see a ResourceWarning, which doesn't seem to be an error, butseems important enough (a.k.a. annoying enough) that I thought it would be worthasking about. [~xhochy] was nice enough to respond in #general in the arrow slack.

The main problem is as follows:

# with this code in a python unittest:
{noformat}
def test_arrow_from_parquet(self):
table = parquet.read_table(<path as str>){noformat}
I see this warning:
{noformat}
ResourceWarning: unclosed file <_io.BufferedReader name=<path_to_file>{noformat}
# I tried adding the following, per Uwe's request:
{noformat}
warnings.simplefilter(""error""){noformat}
# I then see this information:
{noformat}
test_arrow_from_parquet (tests.datalayer_test.TestFileReader) ... Exception ignored in: <_io.FileIO name=<path_to_file> mode='rb' closefd=True>

ResourceWarning: unclosed file <_io.BufferedReader name=<path_to_file>>{noformat}
# Uwe's thoughts:
{noformat}
That could be a valid error. We dont seem to close the file we open in `ParquetFile.__init__`{noformat}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-06-08 18:48:49,3
13164989,[CI]Notify in Slack about broken builds,"As we want to start using nightlies, we need a better way to notify us of failures. First iteration that at least will notify me is to push failure messages into Slack from Travis.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-06-08 18:06:44,8
13164482,[Python]Expose Parquet ZSTD compression,"It seems like in the python/pyarrow/_parquet.pyx file there's a check for the supported compression scheme:
{code}
cdef int check_compression_name(name) except -1:
    if name.upper() not in ['NONE', 'SNAPPY', 'GZIP', 'LZO', 'BROTLI', 'LZ4']:
        raise ArrowException(""Unsupported compression: "" + name)
    return 0{code}
which does not include ZSTD (Zstandard). From my understanding it should be supported in the underlying c++ library already. Is it possible to add that support?

",pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-06 14:41:02,8
13164130,[Packaging] Start building nightlies,"Travis should trigger daily cron jobs. I'm currently experimenting with submitting direclty from the queue repo's master branch, see https://github.com/kszucs/crossbow/blob/master/.travis.yml",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-06-05 09:04:46,3
13163725,[Python] Build ORC extension in manylinux1 wheels,See discussion in https://github.com/apache/arrow/issues/2100. We just need to set {{export PYARROW_WITH_ORC=1}} in the manylinux1 build,pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-04 01:13:48,8
13163643,"[C++] -Wnull-pointer-arithmetic warning with dlmalloc.c on clang 6.0, Ubuntu 14.04 ","I encountered this failure with {{-Werror}} on a nearly fresh Ubuntu 14.04 install using clang-6 from LLVM apt

{code}
../src/plasma/thirdparty/dlmalloc.c:3504:68: error: arithmetic on a null pointer treated as a cast from integer to pointer is a GNU extension [-Werror,-Wnull-pointer-arithmetic]
      size_t mfree = m->topsize + (((((size_t)(((void*)((char*)(0) + ((sizeof(size_t))<<1)))) & (((size_t)(2 * sizeof(void *))) - ((size_t)1))) == 0)? 0 : ((((size_t)(2 * sizeof(void *))) - ((size_t)(((void*)((char*)(0) + ((sizeof(size_t))<<1)))) & (((size_t)(2 * sizeof(void *))) - ((size_t)1)))) & (((size_t)(2 * sizeof(void *))) - ((size_t)1))))+(((sizeof(struct malloc_segment)) + ((sizeof(size_t))) + (((size_t)(2 * sizeof(void *))) - ((size_t)1))) & ~(((size_t)(2 * sizeof(void *))) - ((size_t)1)))+(((sizeof(mchunk)) + (((size_t)(2 * sizeof(void *))) - ((size_t)1))) & ~(((size_t)(2 * sizeof(void *))) - ((size_t)1))));
{code}

See gist for complete log https://gist.github.com/wesm/61ee3592045bfaf390d5d4557f3c5a22",pull-request-available,['C++'],ARROW,Bug,Major,2018-06-03 00:47:53,14
13163616,[Python/C++]Add index() method to find first occurence of Python scalar,"Python lists have an {{index(x, start, end)}} method to find the first occurence of an element. We should add a method with the same interface supporting Python scalars on the typical triplet {{Array/ChunkedArray/Columns}}.

See also https://docs.python.org/3.6/tutorial/datastructures.html#more-on-lists",Analytics beginner pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-06-02 15:39:33,0
13163614,[Python]Implement __getitem__ / slicing on Buffer,"Subscripting {{pyarrow.Buffer}} instances currently raises, we should implement it in a similar fashion as {{pyarrow.Array.__getitem__}}.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-02 15:16:48,3
13163612,[Python] Make dictionary_encode and unique accesible on Column / ChunkedArray,"At the moment, {{dictionary_encode}} and {{unique}} are only exposed on {{Array}} instances but the backing C++ code allows also to pass in the more higher classes. We should add them to the Cython interface so that they can be called from Python code.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-02 15:14:08,8
13163611,[Python]Add to_pandas / to_numpy to ChunkedArray,Currently we have a {{to_pandas}} method on {{Array}} and {{Column}}instances. The former produces a NumPy array whereas the latter a Pandas Series. We should add also a {{to_pandas}}function to {{ChunkedArray}} that produces a contiguous NumPy array of all the underyling {{pyarrow.Array}} instances.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-06-02 14:54:34,8
13163376,[Python] Experiment with zero-copy pickling,"PEP 574 has an implementation ready and a PyPI-available backport (at [https://pypi.org/project/pickle5/] ). Adding experimental support for it would allow for zero-copy pickling of Arrow arrays, columns, etc.

I think it mainly involves implementing {{reduce_ex}} on the {{Buffer}} class, as described in [https://www.python.org/dev/peps/pep-0574/#producer-api]

In addition, the consumer API added by PEP 574 could be used in Arrow's serialization layer, to avoid or minimize copies when serializing foreign objects.",pull-request-available,['Python'],ARROW,Wish,Major,2018-06-01 08:42:01,2
13163375,[Python] More graceful reading of empty String columns in ParquetDataset,"When currently saving a {{ParquetDataset}} from Pandas, we don't get consistent schemas, even if the source was a single DataFrame. This is due to the fact that in some partitions object columns like string can become empty. Then the resulting Arrow schema will differ. In the central metadata, we will store this column as {{pa.string}} whereas in the partition file with the empty columns, this columns will be stored as {{pa.null}}.

The two schemas are still a valid match in terms of schema evolution and we should respect that in https://github.com/apache/arrow/blob/79a22074e0b059a24c5cd45713f8d085e24f826a/python/pyarrow/parquet.py#L754 Instead of doing a {{pa.Schema.equals}} in https://github.com/apache/arrow/blob/79a22074e0b059a24c5cd45713f8d085e24f826a/python/pyarrow/parquet.py#L778 we should introduce a new method {{pa.Schema.can_evolve_to}} that is more graceful and returns {{True}} if a dataset piece has a null column where the main metadata states a nullable column of any type.",dataset dataset-parquet-read parquet,"['C++', 'Python']",ARROW,Bug,Major,2018-06-01 08:15:06,5
13163275,[C++] Failure with -Werror=conversion on gcc 7.3.0,"Experienced this error building on Ubuntu 18.04 LTS / gcc 7.3.0

{code}
/usr/bin/ccache /usr/bin/g++  -DARROW_EXTRA_ERROR_CONTEXT -DARROW_NO_DEPRECATED_API -isystem /home/wesm/cpp-toolchain/include -isystem gbenchmark_ep/src/gbenchmark_ep-install/include -isystem ../thirdparty/hadoop/include -isystem orc_ep-install/include -Isrc -I../src -isystem /home/wesm/miniconda/envs/arrow-dev/lib/python3.6/site-packages/numpy/core/include -isystem /home/wesm/miniconda/envs/arrow-dev/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=0 -ggdb -O0  -Wall -Wconversion -Wno-sign-conversion -Wno-unknown-warning-option -Werror -std=c++11 -msse3 -Werror -g -fPIC   -std=gnu++11 -MD -MT src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o -MF src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o.d -o src/arrow/python/CMakeFiles/arrow_python_objlib.dir/arrow_to_pandas.cc.o -c ../src/arrow/python/arrow_to_pandas.cc
In file included from ../src/arrow/compute/kernel.h:28:0,
                 from ../src/arrow/compute/api.h:22,
                 from ../src/arrow/python/arrow_to_pandas.cc:46:
../src/arrow/util/variant.h:588:67: error: conversion to unsigned int from long unsigned int may alter its value [-Werror=conversion]
     static const std::size_t data_size = detail::static_max<sizeof(Types)...>::value;
                                                                   ^
../src/arrow/util/variant.h:589:79: error: conversion to unsigned int from long unsigned int may alter its value [-Werror=conversion]
     static const std::size_t data_align = detail::static_max<alignof(Types)...>::value;
                                                                               ^
cc1plus: error: unrecognized command line option -Wno-unknown-warning-option [-Werror]
cc1plus: all warnings being treated as errors
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-05-31 20:39:30,14
13163265,[Python] Error with errno 22 when loading 3.6 GB Parquet file,"I saved a file using pandas to_parquet method, but can't read it back in. Here's the full stack trace:


{code:java}
Traceback (most recent call last):
File ""src/data/CLXP_pull.py"", line 214, in <module>
 main()
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 722, in _call_
 return self.main(*args, **kwargs)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 697, in main
 rv = self.invoke(ctx)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 895, in invoke
 return ctx.invoke(self.callback, **ctx.params)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/click/core.py"", line 535, in invoke
 return callback(*args, **kwargs)
 File ""src/data/CLXP_pull.py"", line 188, in main
 results[fullname] = pd.read_parquet(os.path.join(project_dir, ""data"", ""raw"", fullname+"".parquet""), engine=""pyarrow"")
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pandas/io/parquet.py"", line 257, in read_parquet
 return impl.read(path, columns=columns, **kwargs)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pandas/io/parquet.py"", line 130, in read
 **kwargs).to_pandas()
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pyarrow/parquet.py"", line 939, in read_table
 pf = ParquetFile(source, metadata=metadata)
 File ""/Users/mm51929/projects/2018/03-advisor-recruiting/pyenv/lib/python3.6/site-packages/pyarrow/parquet.py"", line 64, in _init_
 self.reader.open(source, metadata=metadata)
 File ""_parquet.pyx"", line 651, in pyarrow._parquet.ParquetReader.open
 File ""error.pxi"", line 79, in pyarrow.lib.check_status
 pyarrow.lib.ArrowIOError: Arrow error: IOError: [Errno 22] Invalid argument
{code}
Any ideas what could cause this? The file itself is 3.6GB.

I'm runningpandas==0.22.0.",parquet,['Python'],ARROW,Bug,Major,2018-05-31 20:15:11,14
13163171,[C++] Refactor hash table support,"Currently our hash table support is scattered in several places:
 * {{compute/kernels/hash.cc}}
 * {{util/hash.h}} and {{util/hash.cc}}
 * {{builder.cc}} (in the DictionaryBuilder implementation)

Perhaps we should have something like a type-parametered hash table class (perhaps backed by non-owned memory) with several primitives:
 * decide allocation size for a given number of items
 * lookup an item
 * insert an item
 * decide whether resizing is needed
 * resize to a new memory area
 * ...",pull-request-available,['C++'],ARROW,Task,Major,2018-05-31 14:28:30,2
13162944,[C++] Add std::generate()-like function for faster bitmap writing,A {{std::generate}}-like function can benefit from advanced optimizations such as unrolling.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-05-30 16:48:13,2
13162841,[C++/Python] Pandas roundtrip for date objects,"Arrow currently casts date objects to nanosecond precision datetime objects.I'd like to have a way to preserve the type during a roundtrip
{code}
>>> import pandas as pd
>>> import pyarrow as pa
>>> import datetime
>>> pa.date32().to_pandas_dtype()
dtype('<M8[ns]')
>>> df = pd.DataFrame({'date': [datetime.date(2018, 1, 1)]})
>>> df.dtypes
date object
dtype: object
>>> df_rountrip = pa.Table.from_pandas(df).to_pandas()
>>> df_rountrip.dtypes
date    datetime64[ns]
dtype: object
{code}
I'd expect something likethis to work:
{code}
>>> import pandas.testing as pdt
>>> df_rountrip = pa.Table.from_pandas(df).to_pandas(date_as_object=True)
>>> pdt.assert_frame_equal(df_rountrip, df)
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Minor,2018-05-30 09:03:29,3
13162601,[Python] parquet binding fails building on AppVeyor,"This is a new issue (perhaps due to a new Cython version). See e.g. https://ci.appveyor.com/project/pitrou/arrow/build/1.0.509/job/dxdqcdk30kmiy6pd#L4291

Excerpt:

{code}
-- Running cmake --build for pyarrow
C:\Program Files (x86)\CMake\bin\cmake.exe --build . --config release
[1/8] cmd.exe /C ""cd /D C:\projects\arrow\python\build\temp.win-amd64-3.6\Release && C:\Miniconda36-x64\envs\arrow\python.exe -m cython --cplus --working C:/projects/arrow/python --output-file C:/projects/arrow/python/build/temp.win-amd64-3.6/Release/_parquet.cpp C:/projects/arrow/python/pyarrow/_parquet.pyx""
[2/8] cmd.exe /c
[3/8] cmd.exe /C ""cd /D C:\projects\arrow\python\build\temp.win-amd64-3.6\Release && C:\Miniconda36-x64\envs\arrow\python.exe -m cython --cplus --working C:/projects/arrow/python --output-file C:/projects/arrow/python/build/temp.win-amd64-3.6/Release/lib.cpp C:/projects/arrow/python/pyarrow/lib.pyx""
[4/8] cmd.exe /c
[5/8] C:\PROGRA~2\MIB055~1\2017\COMMUN~1\VC\Tools\MSVC\1414~1.264\bin\Hostx64\x64\cl.exe   /TP -DARROW_EXPORTING -D_CRT_SECURE_NO_WARNINGS -D_parquet_EXPORTS -IC:\Miniconda36-x64\envs\arrow\lib\site-packages\numpy\core\include -IC:\Miniconda36-x64\envs\arrow\include -I..\..\..\src -IC:\Miniconda36-x64\envs\arrow\Library\include /bigobj /W3 /wd4800 /DWIN32 /D_WINDOWS  /GR /EHsc /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING  /WX /wd4190 /wd4293 /wd4800 /MD /O2 /Ob2 /DNDEBUG /showIncludes /FoCMakeFiles\_parquet.dir\_parquet.cpp.obj /FdCMakeFiles\_parquet.dir\ /FS -c _parquet.cpp
FAILED: CMakeFiles/_parquet.dir/_parquet.cpp.obj 
C:\PROGRA~2\MIB055~1\2017\COMMUN~1\VC\Tools\MSVC\1414~1.264\bin\Hostx64\x64\cl.exe   /TP -DARROW_EXPORTING -D_CRT_SECURE_NO_WARNINGS -D_parquet_EXPORTS -IC:\Miniconda36-x64\envs\arrow\lib\site-packages\numpy\core\include -IC:\Miniconda36-x64\envs\arrow\include -I..\..\..\src -IC:\Miniconda36-x64\envs\arrow\Library\include /bigobj /W3 /wd4800 /DWIN32 /D_WINDOWS  /GR /EHsc /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING  /WX /wd4190 /wd4293 /wd4800 /MD /O2 /Ob2 /DNDEBUG /showIncludes /FoCMakeFiles\_parquet.dir\_parquet.cpp.obj /FdCMakeFiles\_parquet.dir\ /FS -c _parquet.cpp
Microsoft (R) C/C++ Optimizing Compiler Version 19.14.26428.1 for x64
Copyright (C) Microsoft Corporation.  All rights reserved.
_parquet.cpp(6790): error C2220: warning treated as error - no 'object' file generated
_parquet.cpp(6790): warning C4244: 'argument': conversion from 'int64_t' to 'long', possible loss of data
[6/8] C:\PROGRA~2\MIB055~1\2017\COMMUN~1\VC\Tools\MSVC\1414~1.264\bin\Hostx64\x64\cl.exe   /TP -DARROW_EXPORTING -D_CRT_SECURE_NO_WARNINGS -Dlib_EXPORTS -IC:\Miniconda36-x64\envs\arrow\lib\site-packages\numpy\core\include -IC:\Miniconda36-x64\envs\arrow\include -I..\..\..\src -IC:\Miniconda36-x64\envs\arrow\Library\include /bigobj /W3 /wd4800 /DWIN32 /D_WINDOWS  /GR /EHsc /D_SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING  /WX /wd4190 /wd4293 /wd4800 /MD /O2 /Ob2 /DNDEBUG /showIncludes /FoCMakeFiles\lib.dir\lib.cpp.obj /FdCMakeFiles\lib.dir\ /FS -c lib.cpp
Microsoft (R) C/C++ Optimizing Compiler Version 19.14.26428.1 for x64
Copyright (C) Microsoft Corporation.  All rights reserved.
ninja: build stopped: subcommand failed.
error: command 'C:\\Program Files (x86)\\CMake\\bin\\cmake.exe' failed with exit status 1
(arrow) C:\projects\arrow\python>set lastexitcode=1 
{code}
",pull-request-available windows,['Python'],ARROW,Bug,Major,2018-05-29 14:22:32,2
13162528,[C++] Travis-CI build failure with cpp toolchain enabled,"This is a new failure, perhaps triggered by a conda-forge package update. See example at https://travis-ci.org/apache/arrow/jobs/385002355#L2235

{code}
/usr/bin/ld: /home/travis/build/apache/arrow/cpp-toolchain/lib/libz.a(deflate.o): relocation R_X86_64_32S against `zcalloc' can not be used when making a shared object; recompile with -fPIC
{code}",ci-failure,"['C++', 'Continuous Integration']",ARROW,Bug,Critical,2018-05-29 08:48:25,2
13162525,[Python] Fail building parquet binding on Windows,"For some reason I get the following error. I'm not sure why Thrift is needed here:

{code}
-- Found the Parquet library: C:/Miniconda3/envs/arrow/Library/lib/parquet.lib
-- THRIFT_HOME:
-- Thrift compiler/libraries NOT found:  (THRIFT_INCLUDE_DIR-NOTFOUND, THRIFT_ST
ATIC_LIB-NOTFOUND). Looked in system search paths.
-- Boost version: 1.66.0
-- Found the following Boost libraries:
--   regex
Added static library dependency boost_regex: C:/Miniconda3/envs/arrow/Library/li
b/libboost_regex.lib
Added static library dependency parquet: C:/Miniconda3/envs/arrow/Library/lib/pa
rquet_static.lib
CMake Error at C:/t/arrow/cpp/cmake_modules/BuildUtils.cmake:88 (message):
  No static or shared library provided for thrift
Call Stack (most recent call first):
  CMakeLists.txt:376 (ADD_THIRDPARTY_LIB)

{code}

The {{thrift-cpp}} package from conda-forge is installed.",windows,['Python'],ARROW,Bug,Major,2018-05-29 08:44:05,2
13162387,[C++] Investigate spurious memset() calls,"{{builder.cc}} has TODO statements of the form:

{code:c++}
  // TODO(emkornfield) valgrind complains without this
  memset(data_->mutable_data(), 0, static_cast<size_t>(nbytes));
{code}

Ideally we shouldn't have to zero-initialize a data buffer before writing to it.
",pull-request-available,['C++'],ARROW,Improvement,Major,2018-05-28 12:50:44,2
13162227,[Python] Prevent calling extension class constructors directly,"However docstrings are warning about it, I can instantiate these classes which IMHO should be explicitly disallowed.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-05-26 13:10:51,3
13161953,[C++/Python] Build support and instructions for development on Alpine Linux,"Currently it is not possible (or at least, not documented) how to get the project building out of the box on Alpine",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-05-25 01:51:44,3
13161921,[Go] Add LICENSE additions for Go subproject,The Arrow Go codebase contains code from the Go project. This needs to be mentioned in the main LICENSE.txt,pull-request-available,['Go'],ARROW,Improvement,Blocker,2018-05-24 23:07:24,14
13160865,[Python] Add option (or some equivalent) to toggle memory mapping functionality when using parquet.ParquetFile or other read entry points,"See issue described in https://github.com/apache/arrow/issues/1946. When passing a filename to {{parquet.ParquetFile}}, one cannot control what kind of file reader internally is created (OSFile or MemoryMappedFile)",parquet pull-request-available,['Python'],ARROW,Improvement,Major,2018-05-22 00:29:53,14
13160860,[Python] Random schema and data generator for Arrow conversion and Parquet testing,"See discussion in https://github.com/apache/arrow/issues/2067

Being able to generate random complex schemas and corresponding example data sets will help with exercising edge cases in many different parts of the codebase. One practical example: reading and writing nested data to Parquet format",parquet pull-request-available,['Python'],ARROW,Improvement,Major,2018-05-22 00:09:31,3
13160528,[Rust] Refactor introduced a bug around Arrays of String,"The refactor unintentionally implemented the ArrowPrimitiveType trait for strings. This was not intended. This mistake leaked into one example and the record batch struct.



",pull-request-available,['Rust'],ARROW,Bug,Major,2018-05-19 17:35:30,10
13160526,[CI]Remove 'group: deprecated' in Travis,We should just the latest image if that works fine for us.,pull-request-available,['Continuous Integration'],ARROW,Task,Major,2018-05-19 15:35:43,8
13160522,[Docs]Update the gen_apidocs docker script,"The {{dev/gen_apidocs.sh}} docker-based script is a bit out-of-date compared to the build process we currently have.

Additionally, at the moment it always does a new clean build of the C++, GLib and Cython binaries and this docs generation takes quite long.",pull-request-available,['Documentation'],ARROW,New Feature,Major,2018-05-19 14:26:23,8
13160154,"[Java/Python]Add pyarrow.{Array,Field}.from_jvm / jvm_buffer","As a first iteration of the Java->Python in-memory vector sharing, add a minimal set of functions to access primitive arrays from Java in Python.",pull-request-available,"['Java', 'Python']",ARROW,New Feature,Major,2018-05-17 21:02:14,8
13160009,[Python] MemoryPool bytes_allocated causes seg,"Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.


>>> import pyarrow as pa

>>> mp = pa.MemoryPool()
>>> arr = pa.array([1,2,3], memory_pool=mp)
>>> mp.bytes_allocated()

Segmentation fault (core dumped)

I'll dig into this further, but should bytes_alloacted be returning anything when called like this? Or should it raise NotImplemented?",pull-request-available,['Python'],ARROW,Bug,Minor,2018-05-17 12:25:02,14
13159870,"[Python] TypeError: data type ""mixed-integer"" not understood","Pyarrow 0.9 raises an exception when converting some tables to pandas dataframes. Earlier versions work fine. Repro steps:

{{In [1]: import pandas as pd}}

{{In [2]: import pyarrow as pa}}

{{In [3]: df = pd.DataFrame(\{'foo': [], 123: []})}}

{{In [4]: table = pa.Table.from_pandas(df)}}

{{In [5]: table.to_pandas()}}
{{---------------------------------------------------------------------------}}
{{KeyError Traceback (most recent call last)}}
{{~/envs/cli3/lib/python3.6/site-packages/pyarrow/pandas_compat.py in _pandas_type_to_numpy_type(pandas_type)}}
{{ 666 try:}}
{{--> 667 return _pandas_logical_type_map[pandas_type]}}
{{ 668 except KeyError:}}

{{KeyError: 'mixed-integer'}}

(I ended up with a dataframe with mixed string/integer columns by using pd.read_excel(..., skiprows=[0]) - which skipped the header, and treated the first line of data as column names.)",pull-request-available,['Python'],ARROW,Bug,Major,2018-05-16 22:33:02,3
13159840,[Python] Error reading old Parquet file due to metadata backwards compatibility issue,"Pyarrow 0.8 and 0.9 raises an AssertionError for one of the datasets I have (created using an older version of pyarrow). Repro steps:


{{In [1]: from pyarrow.parquet import ParquetDataset}}

{{In [2]: d = ParquetDataset(['bug.parq'])}}

{{In [3]: t = d.read()}}

{{In [4]: t.to_pandas()}}
{{---------------------------------------------------------------------------}}
{{AssertionError Traceback (most recent call last)}}
{{<ipython-input-4-d17c9e2818f1> in <module>()}}
{{----> 1 t.to_pandas()}}

{{table.pxi in pyarrow.lib.Table.to_pandas()}}

{{~/envs/cli3/lib/python3.6/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, memory_pool, nthreads, categories)}}
{{ 529 # There must be the same number of field names and physical names}}
{{ 530 # (fields in the arrow Table)}}
{{--> 531 assert len(logical_index_names) == len(index_columns_set)}}
{{ 532 }}
{{ 533 # It can never be the case in a released version of pyarrow that}}

{{AssertionError: }}



Here's the file: [https://www.dropbox.com/s/oja3khjsc5tycfh/bug.parq]

(I was not able to attach it here due to a ""missing token"", whatever that means.)",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-05-16 21:42:22,14
13159777,[Python] Segmentation fault when writing empty ListType column to Parquet,"Context Is the following: I am currently dealing with sparse column serialization in parquet. In some cases, many lines are empty I can also have columns containing only empty lists.
However I got a segmentation fault when I try to write in parquet thoses columns filled only with empty lists.

Here is a simple code snipet reproduces the segmentation fault I had:


{noformat}

In [1]: import pyarrow as pa

In [2]: import pyarrow.parquet as pq

In [3]: pa_ar = pa.array([[],[]],pa.list_(pa.int32()))

In [4]: table = pa.Table.from_arrays([pa_ar],[""test""])

In [5]: pq.write_table(
 ...: table=table,
 ...: where=""test.parquet"",
 ...: compression=""snappy"",
 ...: flavor=""spark""
 ...: )
Segmentation fault

{noformat}

May I have it fixed?


Best

Jacques",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-05-16 16:48:20,3
13159735,[Python] test_parquet.py regression with Pandas 0.23.0,"See e.g. https://travis-ci.org/apache/arrow/jobs/379652352#L3124.
",ci-failure pull-request-available,['Python'],ARROW,Bug,Major,2018-05-16 13:26:56,2
13159673,[Python] Unable to write StructArrays with multiple children to parquet,"Although I am able to read StructArray from parquet, I am still unable to write it back from pa.Table to parquet.

I get an ""ArrowInvalid: Nested column branch had multiple children""

Here is a quick example:

{noformat}

In [2]: import pyarrow.parquet as pq

In [3]: table = pq.read_table('test.parquet')

In [4]: table
 Out[4]: 
 pyarrow.Table
 weight: double
 animal_type: string
 animal_interpretation: struct<is_large_animal: bool, is_mammal: bool>
  child 0, is_large_animal: bool
  child 1, is_mammal: bool
 metadata
 --------
 \{'org.apache.spark.sql.parquet.row.metadata': '{""type"":""struct"",""fields"":[{""name"":""weight"",""type"":""double"",""nullable"":true,""metadata"":{}},\{""name"":""animal_type"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""animal_interpretation"",""type"":{""type"":""struct"",""fields"":[\\{""name"":""is_large_animal"",""type"":""boolean"",""nullable"":true,""metadata"":{}},\\\{""name"":""is_mammal"",""type"":""boolean"",""nullable"":true,""metadata"":{}}]},""nullable"":false,""metadata"":{}}]}'}

In [5]: table.schema
 Out[5]: 
 weight: double
 animal_type: string
 animal_interpretation: struct<is_large_animal: bool, is_mammal: bool>
  child 0, is_large_animal: bool
  child 1, is_mammal: bool
 metadata
 --------
 \{'org.apache.spark.sql.parquet.row.metadata': '{""type"":""struct"",""fields"":[{""name"":""weight"",""type"":""double"",""nullable"":true,""metadata"":{}},\{""name"":""animal_type"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""animal_interpretation"",""type"":{""type"":""struct"",""fields"":[\\{""name"":""is_large_animal"",""type"":""boolean"",""nullable"":true,""metadata"":{}},\\\{""name"":""is_mammal"",""type"":""boolean"",""nullable"":true,""metadata"":{}}]},""nullable"":false,""metadata"":{}}]}'}

In [6]: pq.write_table(table,""test_write.parquet"")
 ---------------------------------------------------------------------------
 ArrowInvalid Traceback (most recent call last)
 <ipython-input-6-bd9d7deee437> in <module>()
 ----> 1 pq.write_table(table,""test_write.parquet"")

/usr/local/lib/python2.7/dist-packages/pyarrow/parquet.pyc in write_table(table, where, row_group_size, version, use_dictionary, compression, use_deprecated_int96_timestamps, coerce_timestamps, flavor, **kwargs)
  982 use_deprecated_int96_timestamps=use_int96,
  983 **kwargs) as writer:
 --> 984 writer.write_table(table, row_group_size=row_group_size)
  985 except Exception:
  986 if is_path(where):

/usr/local/lib/python2.7/dist-packages/pyarrow/parquet.pyc in write_table(self, table, row_group_size)
  325 table = _sanitize_table(table, self.schema, self.flavor)
  326 assert self.is_open
 --> 327 self.writer.write_table(table, row_group_size=row_group_size)
  328 
  329 def close(self):

/usr/local/lib/python2.7/dist-packages/pyarrow/_parquet.so in pyarrow._parquet.ParquetWriter.write_table()

/usr/local/lib/python2.7/dist-packages/pyarrow/lib.so in pyarrow.lib.check_status()

ArrowInvalid: Nested column branch had multiple children

{noformat}



I would really appreciate a fix on this.

Best,

Jacques",parquet pull-request-available,['Python'],ARROW,Bug,Major,2018-05-16 10:23:55,15
13158753,[Python] Exclude hidden files when reading Parquet dataset,On Unix systems hidden files are listed because os.walk does not care about hidden files. This especially creates a problem in macOS where .DS_Store files are created automatically.,pull-request-available,['Python'],ARROW,Bug,Minor,2018-05-11 15:23:18,14
13158731,[CI] Collect and publish Python coverage,"Now that our Travis-CI setup is able to collect and publish C++ and Rust coverage, we should do the same for Python and Cython modules in pyarrow.",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2018-05-11 13:45:03,2
13158526,[C++] Improve thread pool size heuristic,We currently use the number of available HW threads as the default thread pool size for CPU-bound tasks. We should also examine well-known environment variables such as {{OMP_NUM_THREADS}}. Some discussion here: https://bugs.python.org/issue32986,pull-request-available,['C++'],ARROW,Improvement,Major,2018-05-10 17:17:42,2
13158524,"[Python] Expose thread pool size setting to Python, and deprecate ""nthreads""","Now that we have a global thread pool, we should:
 * use it in places where we currently require an explicit number of threads (with an additional {{use_threads}} argument to enable parallelism)
 * deprecate the now pointless {{nthreads}} argument
 * expose the thread pool capacity setting in Python",pull-request-available,['Python'],ARROW,Improvement,Major,2018-05-10 17:14:46,2
13158502,[C++/Python] Unit is ignored on comparison of TimestampArrays,"Just ran into this:
{code:java}
ipdb> p py_array
<pyarrow.lib.TimestampArray object at 0x13bcc5278>
[
Timestamp('1970-01-01 00:00:00'),
Timestamp('1970-01-01 00:00:00.000000001'),
Timestamp('1970-01-01 00:00:00.000000002'),
Timestamp('1970-01-01 00:00:00.000000003'),
ipdb> p jvm_array
<pyarrow.lib.TimestampArray object at 0x13bcc52c8>
[
Timestamp('1970-01-01 00:00:00'),
Timestamp('1970-01-01 00:00:01'),
Timestamp('1970-01-01 00:00:02'),
Timestamp('1970-01-01 00:00:03'),
ipdb> py_array.equals(jvm_array)
True{code}",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-05-10 15:28:50,8
13158177,[C++] Upload coverage data to codecov.io,"ARROW-27 (upload coverage data to coveralls.io) has failed moving forward. We can try codecov.io instead, another free code coverage hosting service.",pull-request-available,['C++'],ARROW,Task,Major,2018-05-09 12:17:36,2
13158176,[C++] Crash in cuda-test shutdown with coverage enabled,"If I enable both CUDA and code coverage (using {{-DARROW_GENERATE_COVERAGE=on}}), {{cuda-test}} sometimes crashes at shutdown with the following message:

{code}
*** Error in `./build-test/debug/cuda-test': corrupted size vs. prev_size: 0x0000000001612bb0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fc3d61e47e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x7e9dc)[0x7fc3d61eb9dc]
/lib/x86_64-linux-gnu/libc.so.6(+0x81cde)[0x7fc3d61eecde]
/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7fc3d61f1184]
/home/antoine/arrow/cpp/build-test/debug/libarrow.so.10(+0x9350f3)[0x7fc3d5a510f3]
/lib/x86_64-linux-gnu/libc.so.6(__cxa_finalize+0x9a)[0x7fc3d61a736a]
/home/antoine/arrow/cpp/build-test/debug/libarrow.so.10(+0x3415e3)[0x7fc3d545d5e3]
{code}

(the CUDA tests themselves pass)",pull-request-available,"['C++', 'GPU']",ARROW,Bug,Major,2018-05-09 12:13:59,2
13157919,[Python] Set MACOSX_DEPLOYMENT_TARGET in wheel build,The current `pyarrow` wheels are not usable on older OSX releases due to a problem in the newest Xcode SDK. We need to set {{MACOSX_DEPLOYMENT_TARGET}} to an older OSX release to avoid getting {{Symbol not found: _os_unfair_lock_lock}}.,pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Blocker,2018-05-08 14:58:55,3
13157567,[Python] Arrow fails linking against statically-compiled Python,"See https://issues.apache.org/jira/browse/ARROW-1661?focusedCommentId=16462745&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16462745 : to link statically against {{libpythonXX.a}}, you need to add in some system libraries such as {{libutil}}. Otherwise some symbols end up unresolved.",pull-request-available,['Python'],ARROW,Bug,Major,2018-05-07 11:41:02,2
13156878,[Ruby] Import,"I'm developing Ruby bindings of Apache Arrow at https://github.com/red-data-tools/red-arrow and https://github.com/red-data-tools/red-arrow-gpu .
They should be imported to the Apache Arrow project.",pull-request-available,['Ruby'],ARROW,New Feature,Major,2018-05-03 14:03:50,1
13156517,[Python]Provide pre-commit hooks that check flake8,We should provide pre-commit hooks that users can install (optionally) that check e.g. flake8 and clang-format.,pull-request-available,['Python'],ARROW,Task,Major,2018-05-02 06:42:02,8
13156414,[C++] libarrow.so leaks zlib symbols,"I get the following here:

{code:bash}
$ nm -D -C /home/antoine/miniconda3/envs/pyarrow/lib/libarrow.so.0.0.0 | \grep ' T ' | \grep -v arrow
000000000025bc8c T adler32_z
000000000025c4c9 T crc32_z
00000000002ad638 T _fini
0000000000078ab8 T _init
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-05-01 19:22:13,2
13156402,[CI] Fast finish failing AppVeyor builds,"The main AppVeyor queue is taking very long to schedule jobs, one of the measures to get it better would be to immediately fail a job once a build is broken.",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2018-05-01 18:24:52,8
13156090,[Rust] Add trait bounds for T in Buffer/List,"This is a first step in refactoring the rust API to make better use of traits and generics (and moving away from enums and macros).

This change introduces a trait for the primitive types that are allowed in Buffer and List and removes some use of macros.",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-04-30 11:11:09,10
13155924,[C++] Version shared library files,"We should version installed shared library files (SO under Unix, DLL under Windows) to disambiguate incompatible ABI versions.

CMake provides support for that:
http://pusling.com/blog/?p=352
https://cmake.org/cmake/help/v3.11/prop_tgt/SOVERSION.html",pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-28 18:26:38,2
13155910,[Rust] Refactor Rust API to use traits and generics,"Early on, [~kszucs] and I worked on two different designs for how to represent Arrow arrays in Rust, each with their pros and cons.

Krisztian started out with a generics approach e.g. Array<T> which was great until we tried to implement structs, which can contain mixed types so we ended up using enum to represent arrays, which was great until I got to the list types ... I don't think I can implement nested lists with this approach.

I am reviewing this again now that I am more familiar with Arrow and also my Rust skills have improved greatly since I started working on all of this.

I will be prototyping in a separate repo, and will update thisJira once I have something concrete to share, but I feel it is important to address this before the first official release of the Rust version. Also, if we are going to consider a refactor like this, it is better to do it now while the codebase is tiny.









",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-04-28 16:43:27,10
13155906,[Rust] CI should also build against nightly Rust,"We should build Arrow against Rust nightly, but allow failures.",pull-request-available,['Rust'],ARROW,Improvement,Minor,2018-04-28 16:07:15,3
13155738,[Java] Restore Java unit tests and javadoc test to CI matrix,Accidentally removed in ARROW-2498,pull-request-available,['Java'],ARROW,Bug,Blocker,2018-04-27 16:12:30,10
13155439,[Python] Inferring / converting nested Numpy array is very slow,"Converting a nested Numpy array nested walks over the Numpy data as Python objects, even if the dtype is not ""object"". This makes it pointlessly slow compared to the non-nested case, and even the nested Python list case:

{code:python}
>>> %%timeit data = list(range(10000))
...:pa.array(data)
...:
746 s  8.36 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
>>> %%timeit data = np.arange(10000)
...:pa.array(data)
...:
81.1 s  57.7 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)
>>> %%timeit data = [np.arange(10000)]
...:pa.array(data)
...:
3.39 ms  6.27 s per loop (mean  std. dev. of 7 runs, 100 loops each)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-04-26 15:30:31,2
13155100,[CI] Intermittent npm failures,"We occasionally get the following kind of errors on Travis-CI:
https://travis-ci.org/apache/arrow/jobs/371023429#L1925

{code}
gulp[17318]: ../src/node_file.cc:829:void node::fs::Stat(const v8::FunctionCallbackInfo<v8::Value>&): Assertion `(argc) == (3)' failed.
 1: node::Abort() [gulp]
 2: 0x87b6c5 [gulp]
 3: 0x8b2de2 [gulp]
 4: v8::internal::FunctionCallbackArguments::Call(v8::internal::CallHandlerInfo*) [gulp]
 5: 0xad62fa [gulp]
 6: v8::internal::Builtin_HandleApiCall(int, v8::internal::Object**, v8::internal::Isolate*) [gulp]
 7: 0xad165d0427d
Aborted (core dumped)
npm ERR! code ELIFECYCLE
npm ERR! errno 134
npm ERR! apache-arrow@0.3.0 build: `gulp build`
npm ERR! Exit status 134
npm ERR! 
npm ERR! Failed at the apache-arrow@0.3.0 build script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.
{code}
",ci-failure pull-request-available,"['Continuous Integration', 'JavaScript']",ARROW,Bug,Major,2018-04-25 12:25:35,8
13154922,[C++] Disable MSVC warning C4800,"This warning is practically pointless, and since we treat warnings as errors on Appveyor, it imposes spurious back-and-forths to fix it when it occurs.

https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-3-c4800

{quote}This warning is generated when a value that is not bool is assigned or coerced into type bool. Typically, this message is caused by assigning int variables to bool variables where the int variable contains only values true and false, and could be redeclared as type bool. If you cannot rewrite the expression to use type bool, then you can add ""!=0"" to the expression, which gives the expression type bool. Casting the expression to type bool does not disable the warning, which is by design.

This warning is no longer generated in Visual Studio 2017.
{quote}",build pull-request-available windows,['C++'],ARROW,Wish,Major,2018-04-24 20:01:34,2
13154553,[C++] Add iterator facility for Python sequences,"The idea is to factor out something like the following:
https://github.com/apache/arrow/pull/1935/files#diff-6ea0fcd65b95b76eab9ddfbd7a173725R78

However I'm not sure which idiom or pattern we should choose. [~cpcloud] any idea?",pull-request-available,['C++'],ARROW,Wish,Major,2018-04-23 18:08:59,2
13154528,[Java] Upgrade to JDK 1.8,"I'm trying to use the parquet-arrow module from parquet-mr but I'm runninginto this error because the two projects use different major versions of Java:
{code:java}
 Cause: java.lang.ClassNotFoundException: org.apache.arrow.vector.types.pojo.ArrowType$Struct_{code}
The struct is actually named `Struct` not `Struct_`.

This PR is to track work to upgrade to JDK 1.8

I should note that this is after the recent commit in parquet to upgrade to use arrow-0.8.0.



",pull-request-available,['Java'],ARROW,Sub-task,Major,2018-04-23 16:34:27,10
13154255,[Python]Prevent segfault on accidental call of pyarrow.Array,If you mistype {{pyarrow.Array}} instead of {{pyarrow.array}} you get a segmentation fault on some functions. We should also take care of these segmentation faults but also should prevent the user from calling the constructor again in this fashion.,pull-request-available,['Python'],ARROW,Bug,Major,2018-04-21 20:13:07,8
13154250,[Python]Array.from_buffers does not work for ListArray,"When you get the buffers from a ListArray and feed it back into {{pyarrow.Array.from_buffers}} the code fails with a DCHECK: 

{code}
./src/arrow/array.cc:247 Check failed: (data->buffers.size()) == (2)))
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-04-21 19:19:29,8
13154242,[C++] input stream locking inconsistent,"Reading from the current file pointer is inherently thread-unsafe, since the file pointer may be updated by another thread (either before or during the operation). However, currently, we have:
* {{ReadableFile::Read}} takes a lock
* {{MemoryMappedFile::Read}} doesn't take a lock
* {{BufferReader::Read}} doesn't take a lock

We could always take a lock in {{Read}}. But I don't think there's a pattern where it's useful to call {{Read}} from multiple threads at once (since you're not sure where the file pointer will be exactly when the read starts). So we could as well specify that {{Read}} isn't thread-safe and let people make sure they don't call it from multiple threads.
",pull-request-available,['C++'],ARROW,Bug,Major,2018-04-21 17:18:42,2
13154241,[Plasma] test_plasma.py crashes,"This is new here:

{code}$ py.test   --tb=native pyarrow/tests/test_plasma.py 
===================================================================== test session starts ======================================================================
platform linux -- Python 3.6.5, pytest-3.3.2, py-1.5.2, pluggy-0.6.0
rootdir: /home/antoine/arrow/python, inifile: setup.cfg
plugins: xdist-1.22.0, timeout-1.2.1, repeat-0.4.1, forked-0.2, faulthandler-1.3.1
collected 23 items                                                                                                                                             

pyarrow/tests/test_plasma.py *** Error in `/home/antoine/miniconda3/envs/pyarrow/bin/python': double free or corruption (!prev): 0x0000000001699520 ***
[...]

Current thread 0x00007fe7e8570700 (most recent call first):
  File ""/home/antoine/arrow/python/pyarrow/tests/test_plasma.py"", line 211 in test_connection_failure_raises_exception
[...]
{code}

Here is the C backtrace under gdb:

{code}
#0  0x00007ffff69d0428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54
#1  0x00007ffff69d202a in __GI_abort () at abort.c:89
#2  0x00007ffff6a127ea in __libc_message (do_abort=do_abort@entry=2, fmt=fmt@entry=0x7ffff6b2bed8 ""*** Error in `%s': %s: 0x%s ***\n"")
    at ../sysdeps/posix/libc_fatal.c:175
#3  0x00007ffff6a1b37a in malloc_printerr (ar_ptr=<optimized out>, ptr=<optimized out>, str=0x7ffff6b2c008 ""double free or corruption (!prev)"", action=3)
    at malloc.c:5006
#4  _int_free (av=<optimized out>, p=<optimized out>, have_lock=0) at malloc.c:3867
#5  0x00007ffff6a1f53c in __GI___libc_free (mem=<optimized out>) at malloc.c:2968
#6  0x00007fffbdfcc504 in std::_Sp_counted_ptr<plasma::PlasmaClient*, (__gnu_cxx::_Lock_policy)2>::_M_dispose (this=0x9defb0)
    at /usr/include/c++/4.9/bits/shared_ptr_base.h:373
#7  0x00007fffbdfc903c in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release (this=0x9defb0) at /usr/include/c++/4.9/bits/shared_ptr_base.h:149
#8  0x00007fffbdfc82b9 in std::__shared_count<(__gnu_cxx::_Lock_policy)2>::~__shared_count (this=0x7fffc1214510, __in_chrg=<optimized out>)
    at /usr/include/c++/4.9/bits/shared_ptr_base.h:666
#9  0x00007fffbdfc8276 in std::__shared_ptr<plasma::PlasmaClient, (__gnu_cxx::_Lock_policy)2>::~__shared_ptr (this=0x7fffc1214508, __in_chrg=<optimized out>)
    at /usr/include/c++/4.9/bits/shared_ptr_base.h:914
#10 0x00007fffbdfc8fc4 in std::shared_ptr<plasma::PlasmaClient>::~shared_ptr (this=0x7fffc1214508, __in_chrg=<optimized out>)
    at /usr/include/c++/4.9/bits/shared_ptr.h:93
#11 0x00007fffbdfc8fde in __Pyx_call_destructor<std::shared_ptr<plasma::PlasmaClient> > (x=...)
    at /home/antoine/arrow/python/build/temp.linux-x86_64-3.6/plasma.cxx:281
#12 0x00007fffbdfbc317 in __pyx_tp_dealloc_7pyarrow_6plasma_PlasmaClient (o=0x7fffc12144f0)
    at /home/antoine/arrow/python/build/temp.linux-x86_64-3.6/plasma.cxx:10383
#13 0x00007fffbdfb8986 in __pyx_pf_7pyarrow_6plasma_2connect (__pyx_self=0x0, __pyx_v_store_socket_name=0x7fffbc922c48, 
    __pyx_v_manager_socket_name=0x7ffff7fa0ab0, __pyx_v_release_delay=0, __pyx_v_num_retries=1)
    at /home/antoine/arrow/python/build/temp.linux-x86_64-3.6/plasma.cxx:9147
#14 0x00007fffbdfb7dec in __pyx_pw_7pyarrow_6plasma_3connect (__pyx_self=0x0, __pyx_args=0x7fffbc4d9688, __pyx_kwds=0x0)
    at /home/antoine/arrow/python/build/temp.linux-x86_64-3.6/plasma.cxx:8978
{code}",pull-request-available,"['C++ - Plasma', 'GPU', 'Python']",ARROW,Bug,Major,2018-04-21 17:11:52,2
13154065,[Rust] support nested types,The Rust Array type doesn't seem to support nested types so far. We should implement it.,pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-20 16:25:14,10
13153899,[Rust] Move calls to free() into memory.rs,"We had calls to free memory in a few places, with duplication of effort for Windows support. This PR simply creates a free_aligned() method in memory.rs so we have all memory alloc/free and Windows references in a single file.",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-04-20 02:57:04,10
13153784,[C++] Have a global thread pool,"A few parts of Arrow have started spawning threads to parallelize CPU-bound tasks. They do so by launching a hard-coded number of threads, disregarding machine configuration or the fact that Arrow itself might be used from several threads at once (so hardcoding 8 threads might end up launch N * 8 threads if the user is calling Arrow from N threads at once...).

Instead we probably want a global thread pool policy, with a singleton thread pool for CPU-bound tasks (using, by default, a number of threads equal to the machine's capacity).

See discussion in [https://github.com/apache/arrow/pull/1893]",pull-request-available,['C++'],ARROW,Wish,Major,2018-04-19 17:52:26,2
13153547,[Rust] List assertion error with list of zero length,Building a list of empty lists causes assertion errors when accessing the list slices.,pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-19 00:43:29,10
13153520,[Rust] Assertion when pushing value to Builder/ListBuilder with zero capacity,"If a builder is created with capacity 0 and then data is pushed to the builder, memory reallocation is not triggered and the push fails with an assertion.

The root cause is that memory reallocation calculates the new capacity as 2x the current capacity which doesn't work if the current capacity is zero.

",pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-18 23:02:25,10
13153478,[C++] FileGetSize() should not seek,{{FileGetSize()}} currently seeks to the end of file and reads the current file position. Instead it should simply call {{fstat}} on the file descriptor (or the Windows equivalent).,pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-18 20:47:14,2
13153017,[Rust] Generate code using Flatbuffers,"Generate flatbuffers code for Schema, File, Message, and Tensor and add to project. It would be nice to generate on the fly as part of the build process but this doesn't seem feasible today because:
 * We would have to download and compile Flatbuffers first
 * The Rust implementation of Flatbuffers has a couple minor issues that required manually fixing the generated code",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-17 13:56:17,10
13153016,"[C++] misleading ""append"" flag to FileOutputStream","{{FileOutputStream}} has a constructor option named {{append}}, but all it does is prevent truncation of the file, i.e. it doesn't move the file pointer to the end. And given {{FileOutputStream}} doesn't have a seek method, this option is useless unless you manually seek using the file descriptor.",pull-request-available,['C++'],ARROW,Bug,Major,2018-04-17 13:54:45,2
13152790,[Plasma] plasma_store fails to find libarrow_gpu.so,"After install, I get the following:
{code:bash}
$ which plasma_store
/home/antoine/miniconda3/envs/pyarrow/bin/plasma_store
$ plasma_store
plasma_store: error while loading shared libraries: libarrow_gpu.so.0: cannot open shared object file: No such file or directory
$ ldd `which plasma_store`
	linux-vdso.so.1 =>  (0x00007ffe7bdf0000)
	libarrow_gpu.so.0 => not found
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f5d81676000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5d812ee000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5d80fe5000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5d80dce000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5d80a04000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f5d81893000)
{code}

Note that {{libarrow_gpu.so}} is installed in {{/home/antoine/miniconda3/envs/pyarrow/lib/}}

There are probably two solutions:
* link statically with the Arrow GPU libs (I wonder why this isn't done like it is for the Arrow libs)
* or make the rpath correct",pull-request-available,"['C++ - Plasma', 'GPU']",ARROW,Bug,Major,2018-04-16 18:12:34,2
13152709,[C++] Update flatbuffers to 1.9.0,This will update externalproject and manylinux1 installations of Flatbuffers. The conda-forge update is at https://github.com/conda-forge/flatbuffers-feedstock/pull/9,pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-16 12:55:49,8
13152533,[Python]Build wheels for manylinux2010 tag,"There is now work in progress on an updated manylinux tag based on CentOS6. We should provide wheels for this tag and the old {{manylinux1}} tag for one release and then switch to the new tag in the release afterwards. This should enable us also to raise the minimum compiler requirement to gcc 4.9 (or higher once conda-forge has migrated to a newer compiler).

The relevant PEP is https://www.python.org/dev/peps/pep-0571/",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-04-15 08:51:40,2
13152320,garrow_array_builder_append_values() won't work for large arrays,"I am usinggarrow_array_builder_append_values() to transform a native C array to an Arrow array, without calling arrow_array_builder_append multiple times. When callinggarrow_array_builder_append_values() in array-builder.cpp with following signature:
{code:java}
garrow_array_builder_append_values(GArrowArrayBuilder *builder,
const VALUE *values,
gint64 values_length,
const gboolean *is_valids,
gint64 is_valids_length,
GError **error,
const gchar *context)
{code}
it will fail for large arrays. This is probably happening because the is_valids array is copied to the valid_bytes array (of different type), for which the memory is allocated on the stack, and not on the heap, like shown on the snippet below:
{code:java}
uint8_t valid_bytes[is_valids_length];
for (gint64 i = 0; i < is_valids_length; ++i){ 
  valid_bytes[i] = is_valids[i]; 
}
{code}
A way to avoid this problem would be to allocate memory for the valid_bytes array using malloc() orsomething similar. Is this behavior intended, maybe because no large arrays should be handed over to that function, or it is rather a bug?",pull-request-available,"['C++', 'GLib']",ARROW,Bug,Major,2018-04-13 15:24:27,1
13152237,[Python] Empty chunked array slice crashes,"{code:python}
>>> col = pa.Column.from_array('ints', pa.array([1,2,3]))
>>> col
<pyarrow.lib.Column object at 0x7f65398fff00>
chunk 0: <pyarrow.lib.Int64Array object at 0x7f64fd13ab88>
[
  1,
  2,
  3
]
>>> col.data
<pyarrow.lib.ChunkedArray at 0x7f653986ef00>
>>> col.data[:1]
<pyarrow.lib.ChunkedArray at 0x7f6539884720>
>>> col.data[:0]
Erreur de segmentation (core dumped)
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-04-13 09:54:01,2
13152228,[TEST] Spark integration test fails with permission error,"{{arrow/dev/run_docker_compose.sh spark_integration}}

{code}
Scanning dependencies of target lib
[ 66%] Building CXX object CMakeFiles/lib.dir/lib.cxx.o
[100%] Linking CXX shared module release/lib.so
[100%] Built target lib
-- Finished cmake --build for pyarrow
Bundling includes: release/include
('Moving built C-extension', 'release/lib.so', 'to build path', '/apache-arrow/arrow/python/build/lib.linux-x86_64-2.7/pyarrow/lib.so')
release/_parquet.so
Cython module _parquet failure permitted
release/_orc.so
Cython module _orc failure permitted
release/plasma.so
Cython module plasma failure permitted
running install
error: can't create or remove files in install directory

The following error occurred while trying to add or remove files in the
installation directory:

    [Errno 13] Permission denied: '/home/ubuntu/miniconda/envs/pyarrow-dev/lib/python2.7/site-packages/test-easy-install-1855.write-test'

The installation directory you specified (via --install-dir, --prefix, or
the distutils default setting) was:

    /home/ubuntu/miniconda/envs/pyarrow-dev/lib/python2.7/site-packages/

Perhaps your account does not have write access to this directory?  If the
installation directory is a system-owned directory, you may need to sign in
as the administrator or ""root"" account.  If you do not have administrative
access to this machine, you may wish to choose a different installation
directory, preferably one that is listed in your PYTHONPATH environment
variable.
{code}",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2018-04-13 09:20:31,3
13151949,[Python]Saving to parquet fails for empty lists,"When writing a table to parquet through pandas, if any column includes an empty list, it fails with a segmentation fault.

Minimal example:

{code}
import pyarrow as pa
import pyarrow.parquet as pq
import pandas as pd

def save(rows):
    table1 = pa.Table.from_pandas(pd.DataFrame(rows))
    pq.write_table(table1, 'test-foo.pq')
    table2 = pq.read_table('test-foo.pq')

    print('ROWS:', rows)
    print('TABLE1:', table1.to_pandas(), sep='\n')
    print('TABLE2:', table2.to_pandas(), sep='\n')

save([{'val': ['something']}])
print('---')
save([{'val': []}])  # empty
{code}

Output:

{code}
ROWS: [{'val': ['something']}]
TABLE1:
           val
0  [something]
TABLE2:
           val
0  [something]
---
ROWS: [{'val': []}]
TABLE1:
  val
0  []
[1]    13472 segmentation fault (core dumped)  python3 test.py
{code}

Versions:

{code}
$ pip3 list | grep pyarrow
pyarrow (0.9.0)
$ python3 --version
Python 3.5.2
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-04-12 12:00:51,2
13151618,[C++] Create a device abstraction,"Right now, a plain Buffer doesn't carry information about where it actually lies. That information also cannot be passed around, so you get APIs like {{PlasmaClient}} which take or return device number integers, and have implementations which hardcode operations on CUDA buffers. Also, unsuspecting receivers of a {{Buffer}} pointer may try to act on the underlying memory without knowing whether it's CPU-reachable or not.

Here is a sketch for a proposed Device abstraction:
{code}
class Device {
    enum DeviceKind { KIND_CPU, KIND_CUDA };

    virtual DeviceKind kind() const;
    //MemoryPool* default_memory_pool() const;
    //std::shared_ptr<Buffer> Allocate(...);
};

class CpuDevice : public Device {};

class CudaDevice : public Device {
    int device_num() const;
};

class Buffer {
    virtual DeviceKind device_kind() const;
    virtual std::shared_ptr<Device> device() const;
    virtual bool on_cpu() const {
        return true;
    }

    const uint8_t* cpu_data() const {
        return on_cpu() ? data() : nullptr;
    }
    uint8_t* cpu_mutable_data() {
        return on_cpu() ? mutable_data() : nullptr;
    }

    virtual CopyToCpu(std::shared_ptr<Buffer> dest) const;
    virtual CopyFromCpu(std::shared_ptr<Buffer> src);
};

class CudaBuffer : public Buffer {
    virtual bool on_cpu() const {
        return false;
    }
};

CopyBuffer(std::shared_ptr<Buffer> dest, const std::shared_ptr<Buffer> src);
{code}",pull-request-available,"['C++', 'GPU']",ARROW,Improvement,Major,2018-04-11 11:14:02,2
13151544,[Rust] Add documentation and make some fields private,A first pass at adding rustdoc comments and made some struct fields private and added accessor methods.,pull-request-available,['Rust'],ARROW,Improvement,Trivial,2018-04-11 02:14:39,10
13151413,[Python][C++] Better handle reading empty parquet files,"From [https://github.com/dask/dask/pull/3387#issuecomment-380140003]



Currently pyarrow reads empty parts as float64, even if the underlying columns have other dtypes. This can cause problems for pandas downstream, as certain operations are only valid on certain dtypes, even if the columns are empty.



Copying the comment Uwe over:


bq. {quote}This is the expected behaviour as an empty string column in Pandas is simply an empty column of type object. Sadly object does not tell us much about the type of the column at all. We return numpy.float64 in this case as it's the most efficient type to store nulls in Pandas.{quote}

{quote}This seems unintuitive at best to me. An empty object column in pandas is treated differently in many operations than an empty float64 column (str accessor is available, excluded from numeric operations, etc..). Having an empty file read in as a different dtype than was written could lead to errors in processing code downstream. Would arrow be willing to change this behavior?{quote}

We should probably use another method than `field.type.to_pandas_dtype()` in this case. The column saved in Parquet should be saved with `NA` as type which sadly does not provide enough information. 

We also store the original dtype in the Pandas metadata that is used for the actual DataFrame reconstruction later on. If we would also pick up the metadata when it was written, we should be able to correctly reconstruct the dtype.",dataset dataset-parquet-read parquet,['Python'],ARROW,Improvement,Major,2018-04-10 15:37:14,5
13151401,[Python] Conversion from pandas of empty categorical fails with ArrowInvalid,"The conversion of an empty pandas categorical raises an exception. Before version `0.9.0` this was possible
{code:java}
import pandas as pd
import pyarrow as pa
pa.Table.from_pandas(pd.DataFrame({'cat': pd.Categorical([])})){code}
raises:

{{ArrowInvalid: Dictionary indices must have non-zero length}}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-04-10 14:37:54,8
13151378,[C++] Disambiguate Builder::Append overloads,"See discussion in [https://github.com/apache/arrow/pull/1852#discussion_r179919627]

There are various {{Append()}} overloads in Builder and subclasses, some of which append one value, some of which append multiple values at once.

The API might be clearer and less error-prone if multiple-append variants were named differently, for example {{AppendValues()}}. Especially with the pointer-taking variants, it's probably easy to call the wrong overload by mistake.

The existing methods would have to go through a deprecation cycle.",beginner pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-10 13:28:55,2
13151374,[Rust] Builder<T>::slice_mut assertions are too strict,"The assertions only allow slice up to builder length, rather than up to builder capacity.",pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-10 13:24:50,10
13151368,[Rust] Implement ListBuilder<T>,Implement ListBuilder<T>,pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-10 12:54:42,10
13151237,[Rust] Add Builder.push_slice(&[T]),"When populating a Builder<u8> with Utf8 data it is more efficient to push whole strings as &[u8] rather than one byte at a time.

The same optimization works for all other types too.

",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-04-09 23:48:26,10
13151169,MVP for branch based packaging automation,Described in https://docs.google.com/document/d/1IyhbQpiElxTsI8HbMZ-g9EGPOtcFdtMBzEyDJv48BKc/edit,pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-04-09 19:46:04,3
13151095,[Python]Add API to map Arrow types (including extension types) to pandas ExtensionArray instances for to_pandas conversions,"With the next release of Pandas, it will be possible to define custom column types that back a {{pandas.Series}}. Thus we will not be able to cover all possible column types in the {{to_pandas}}conversion by default as we won't be aware of all extension arrays.

To enable users to create {{ExtensionArray}} instances from Arrow columns in the {{to_pandas}}conversion, we should provide a hook in the {{to_pandas}} call where they can overload the default conversion routines with the ones that produce their {{ExtensionArray}} instances.

This should avoid additional copies in the case where we would nowadays first convert the Arrow column into a default Pandas column (probably of object type) and the user would afterwards convert it to a more efficient {{ExtensionArray}}. This hook here will be especially useful when you build {{ExtensionArrays}} where the storage is backed by Arrow.

The meta-issue that tracks the implementation inside of Pandas is: https://github.com/pandas-dev/pandas/issues/19696",pull-request-available,['Python'],ARROW,Improvement,Major,2018-04-09 15:41:59,5
13151085,[C++] ReadAt implementations suboptimal,"The {{ReadAt}} implementations for at least {{OSFile}} and {{MemoryMappedFile}} take the file lock and seek. They could instead read directly from the given offset, allowing concurrent I/O from multiple threads.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-09 15:15:49,2
13151075,[CI] glib build failure,"The glib build on Travis-CI fails:

[https://travis-ci.org/apache/arrow/jobs/364123364#L6840]

{code}
==> Installing gobject-introspection
==> Downloading https://homebrew.bintray.com/bottles/gobject-introspection-1.56.0_1.sierra.bottle.tar.gz
==> Pouring gobject-introspection-1.56.0_1.sierra.bottle.tar.gz
  /usr/local/Cellar/gobject-introspection/1.56.0_1: 173 files, 9.8MB
Installing gobject-introspection has failed!
{code}",ci-failure pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2018-04-09 14:45:50,1
13151053,[Rust] Array::from missing mapping for u8 type,Macros are used to support Array::from for each primitive type but u8 was missing,pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-09 14:05:45,10
13151047,[Rust] Missing import causing broken build,Recent merges broke the build.,pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-09 13:45:13,10
13151045,[Python] PyArrow datatypes raise ValueError on equality checks against non-PyArrow objects,"Checking a PyArrow datatype object for equality with non-PyArrow datatypes causes a `ValueError` to be raised, rather than either returning a True/False value, or returning[NotImplemented|https://docs.python.org/3/library/constants.html#NotImplemented] if the comparison isn't implemented.

E.g. attempting to call:
{code:java}
import pyarrow
pyarrow.int32() == 'foo'
{code}
results in:
{code:java}
Traceback (most recent call last):
  File ""types.pxi"", line 1221, in pyarrow.lib.type_for_alias
KeyError: 'foo'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""t.py"", line 2, in <module>
    pyarrow.int32() == 'foo'
  File ""types.pxi"", line 90, in pyarrow.lib.DataType.__richcmp__
  File ""types.pxi"", line 113, in pyarrow.lib.DataType.equals
  File ""types.pxi"", line 1223, in pyarrow.lib.type_for_alias
ValueError: No type alias for foo
{code}
The expected outcome for the above would be for the comparison to return `False`, as that's the general behaviour for comparisons between objects of different types (e.g. `1 == 'foo'` or `object() == 12.4` both return `False`).",beginner pull-request-available,['Python'],ARROW,Bug,Minor,2018-04-09 13:37:31,3
13151014,[Rust] Memory is never released,"Another embarrassing bug ... the code was calling the wrong method to release memory and wasn't releasing memory.

I have added some benchmarks for testing performance of creating arrays (and dropping them) and these are working well now after fixing the memory bug.",pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-09 11:50:55,10
13150965,[Site] Website generation depends on local timezone,See discussion at https://github.com/apache/arrow/pull/1853#issuecomment-379670199,pull-request-available,['Website'],ARROW,Bug,Major,2018-04-09 08:38:02,2
13150890,[Rust] List builder fails due to memory not being reserved correctly,I didn't realize that BytesMut.put() doesn't automatically grow the underlying buffer. Therefore the code fails if the data is large than the pre-allocated buffer.,pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-09 04:00:24,10
13150851,[Rust] Review APIs for safety,"The Rust library makes a lot of use of unsafe calls. We should review the API to see if any methods we expose should be marked unsafe or whether we need to add assertions to make APIs safe.

We should also add more unit tests around this.",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-04-08 14:11:55,10
13150644,[C++] Add method to append batches of null-terminated strings to StringBuilder,"We should add a method {{StringBuilder::AppendCStrings(const char** values, const uint8_t* valid_bytes = NULLPTR)}} to the {{StringBuilder}}class to have fast inserts for these strings. See https://github.com/apache/arrow/pull/1845/files for a use case.",pull-request-available,"['C++', 'GLib']",ARROW,Improvement,Major,2018-04-06 16:09:11,1
13150593,[Rust] It should be possible to get a &mut[T] from Builder<T>,"I am currently adding Arrow support to the parquet-rs crate and I found a need to get a mutable slice from a Buffer<T> to pass to the parquet column reader methods.

",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-06 13:40:17,10
13150532,[C++] <functional> is missing in plasma/client.h,"I got the following compile error:

{noformat}
In file included from /home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:20:0:
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:363:32: error: function in namespace std does not name a template type
                     const std::function<std::shared_ptr<Buffer>(
                                ^~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:363:40: error: expected , or ... before < token
                     const std::function<std::shared_ptr<Buffer>(
                                        ^
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:276:8: error: prototype for arrow::Status plasma::PlasmaClient::GetBuffers(const ObjectID*, int64_t, int64_t, const std::function<std::shared_ptr<arrow::Buffer>(const plasma::UniqueID&, const std::shared_ptr<arrow::Buffer>&)>&, plasma::ObjectBuffer*) does not match any in class plasma::PlasmaClient
 Status PlasmaClient::GetBuffers(
        ^~~~~~~~~~~~
In file included from /home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:20:0:
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:362:10: error: candidate is: arrow::Status plasma::PlasmaClient::GetBuffers(const ObjectID*, int64_t, int64_t, int)
   Status GetBuffers(const ObjectID* object_ids, int64_t num_objects, int64_t timeout_ms,
          ^~~~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc: In member function arrow::Status plasma::PlasmaClient::Get(const std::vector<plasma::UniqueID>&, int64_t, std::vector<plasma::ObjectBuffer>*):
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:410:85: error: no matching function for call to plasma::PlasmaClient::GetBuffers(const value_type*, const size_t&, int64_t&, const plasma::PlasmaClient::Get(const std::vector<plasma::UniqueID>&, int64_t, std::vector<plasma::ObjectBuffer>*)::<lambda(const ObjectID&, const std::shared_ptr<arrow::Buffer>&)>&, __gnu_cxx::__alloc_traits<std::allocator<plasma::ObjectBuffer> >::value_type*)
   return GetBuffers(&object_ids[0], num_objects, timeout_ms, wrap_buffer, &(*out)[0]);
                                                                                     ^
In file included from /home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:20:0:
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:362:10: note: candidate: arrow::Status plasma::PlasmaClient::GetBuffers(const ObjectID*, int64_t, int64_t, int)
   Status GetBuffers(const ObjectID* object_ids, int64_t num_objects, int64_t timeout_ms,
          ^~~~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:362:10: note:   candidate expects 4 arguments, 5 provided
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc: In member function arrow::Status plasma::PlasmaClient::Get(const ObjectID*, int64_t, int64_t, plasma::ObjectBuffer*):
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:417:74: error: no matching function for call to plasma::PlasmaClient::GetBuffers(const ObjectID*&, int64_t&, int64_t&, const plasma::PlasmaClient::Get(const ObjectID*, int64_t, int64_t, plasma::ObjectBuffer*)::<lambda(const ObjectID&, const std::shared_ptr<arrow::Buffer>&)>&, plasma::ObjectBuffer*&)
   return GetBuffers(object_ids, num_objects, timeout_ms, wrap_buffer, out);
                                                                          ^
In file included from /home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.cc:20:0:
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:362:10: note: candidate: arrow::Status plasma::PlasmaClient::GetBuffers(const ObjectID*, int64_t, int64_t, int)
   Status GetBuffers(const ObjectID* object_ids, int64_t num_objects, int64_t timeout_ms,
          ^~~~~~~~~~
/home/kou/work/cpp/arrow.kou/cpp/src/plasma/client.h:362:10: note:   candidate expects 4 arguments, 5 provided
{noformat}

I don't know why it's not occurred on Travis CI.",pull-request-available,['C++ - Plasma'],ARROW,Bug,Minor,2018-04-06 07:58:12,1
13150306,"[C++] FixedSizeBinaryBuilder::Append lacks ""const char*"" overload","This implies that calling {{FixedSizeBinaryBuilder::Append}} with a ""const char*"" argument currently instantiates a temporary {{std::string}}.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-05 14:01:06,2
13150265,[C++] Status destructor is expensive,"Let's take the following micro-benchmark (in Python):

{code:bash}
$ python -m timeit -s ""import pyarrow as pa; data = [b'xx' for i in range(10000)]"" ""pa.array(data, type=pa.binary())""
1000 loops, best of 3: 784 usec per loop
{code}

If I replace the Status destructor with a no-op:
{code:c++}
  ~Status() { }
{code}

then the benchmark result becomes:
{code:bash}
$ python -m timeit -s ""import pyarrow as pa; data = [b'xx' for i in range(10000)]"" ""pa.array(data, type=pa.binary())""
1000 loops, best of 3: 561 usec per loop
{code}

This is almost a 30% win. I get similar results on the conversion benchmarks in the benchmark suite.

I'm unsure about the explanation. In the common case, {{delete _state}} should be extremely fast, since the state is NULL. Yet, it seems it adds significant overhead. Perhaps because of exception handling?",pull-request-available,['C++'],ARROW,Improvement,Major,2018-04-05 10:45:03,2
13150243,[Rust] Provide a zero-copy builder for type-safe Buffer<T>,"This PR implements a builder so that buffers can be populated directly in aligned memory (as opposed to being created from Vec<T>).



https://github.com/apache/arrow/pull/1838",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-05 08:57:25,10
13150184,[Python] Correct flake8 errors outside of pyarrow/ directory,"Fix flake8 warnings for files outside of benchmarks directory.



!https://user-images.githubusercontent.com/2118138/38217076-f08a67da-369a-11e8-8166-b3a9ed7d9a60.png!",beginner pull-request-available,['Python'],ARROW,Improvement,Minor,2018-04-04 23:30:27,14
13150070,[Python] pyarrow RecordBatchStreamWriter allows writing batches with different schemas,"A RecordBatchStreamWriter initialised with a given schema will still allow writing RecordBatches that have different schemas. Example:


{code:java}
schema = pa.schema([pa.field('some_field', pa.int64())])
stream = pa.BufferOutputStream()
writer = pa.RecordBatchStreamWriter(stream, schema)

data = [pa.array([1.234])]
batch = pa.RecordBatch.from_arrays(data, ['some_field']) 
# batch does not conform to schema

assert batch.schema != schema

writer.write_batch(batch) # no exception raised
writer.close()
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-04-04 16:36:42,2
13150016,[Python] Segmentation fault from PyArrow when mapping Pandas datetime column to pyarrow.date64,"When trying to call `pyarrow.Table.from_pandas` with a `pandas.DataFrame` and a `pyarrow.Schema` provided, the function call results in a segmentation fault if Pandas `datetime64[ns]` column tries to be converted to a `pyarrow.date64` type.

A minimal example which shows this is:
{code:python}
import pandas as pd
import pyarrow as pa

df = pd.DataFrame({'created': ['2018-05-10T10:24:01']})
df['created'] = pd.to_datetime(df['created'])}}
schema = pa.schema([pa.field('created', pa.date64())])
pa.Table.from_pandas(df, schema=schema)
{code}

Executing the above causes the python interpreter to exit with ""Segmentation fault: 11"".

Attempting to convert into various other datatypes (by specifying different schemas) either succeeds, or raises an exception if the conversion is invalid.",pull-request-available,['Python'],ARROW,Bug,Major,2018-04-04 14:16:16,3
13149995,[C++/Python] CheckPyError() could inspect exception type,"Current {{CheckPyError}} always chooses an ""unknown error"" status. But it could inspect the Python exception and choose, e.g. ""type error"" for a {{TypeError}} exception, etc.

See also ARROW-2389",pull-request-available,"['C++', 'Python']",ARROW,Wish,Major,2018-04-04 13:11:17,2
13149981,[C++] Add StatusCode::OverflowError,"It may be useful to have a {{StatusCode::OverflowError}} return code, to signal that something overflowed allowed limits (e.g. the 2GB limit for string or binary values).",pull-request-available,['C++'],ARROW,Wish,Major,2018-04-04 12:28:02,2
13149884,[C++] Arrow::StringBuilder::Append() uses null_bytes not valid_bytes,Append of other builders use valid_bytes not null_bytes.,pull-request-available,['C++'],ARROW,Improvement,Minor,2018-04-04 03:34:43,1
13149716,[Rust] Implement to_json() for Field and DataType,"Implementing JSON representation of DataType and Field as per Arrow specifications.



",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-03 11:03:28,10
13149661,[C++] Debian packages need to depend on libprotobuf,It seems that we are currently building protobuf using the ExternalProject facility in the debian packages and thus conflict with the system provided protobuf libraries.,pull-request-available,['Packaging'],ARROW,Bug,Major,2018-04-03 05:10:55,1
13149646,[Rust] List<T> was not using memory safely,"During testing, I discovered some bugs in List<T> where it was not using memory safely. These are fixed in the following PR:

https://github.com/apache/arrow/pull/1827",pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-03 03:47:21,10
13149617,[Rust] Buffer<T> should have an Iterator,"It should be possible to obtain an Iterator from a Buffer<T> so that it is easy to access the values.



PR: https://github.com/apache/arrow/pull/1823",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-04-02 23:33:23,10
13149590,[Python] Correct issues in numpy_to_arrow conversion routines,"Following the discussion at [https://github.com/apache/arrow/pull/1689,] there are a few issues with conversion of various types to arrow that are incorrect or could be improved:
 * PyBytes_GET_SIZE is being casted to the wrong type, for example 
{{const int32_t length = static_cast<int32_t>(PyBytes_GET_SIZE(obj));}}

 * Handle the possibility with the statement
{{builder->value_data_length() + length > kBinaryMemoryLimit}}
if length is larger than kBinaryMemoryLimit

 * Look into using common code for binary object conversion to avoid duplication, and allow support for bytes and bytearray objects in other places than numpy_to_arrow.  (possibly put in src/arrow/python/helpers.h)",pull-request-available,['Python'],ARROW,Bug,Major,2018-04-02 21:16:48,2
13149473,[GLib] Travis-CI failures,"See this (empty) PR:
https://github.com/apache/arrow/pull/1822

It results in failures on the GLib builds:
https://travis-ci.org/apache/arrow/builds/361145322

{code}================================================================================
/home/travis/build/apache/arrow/c_glib/test/test-list-array.rb:24:in `test_new'
     21:   def test_new
     22:     value_offsets = Arrow::Buffer.new([0, 2, 5, 5].pack(""l*""))
     23:     data = Arrow::Buffer.new([1, 2, 3, 4, 5].pack(""c*""))
  => 24:     values = Arrow::Int8Array.new(5, data, nil, 0)
     25:     assert_equal(build_list_array(Arrow::Int8DataType.new,
     26:                                   [[1, 2], [3, 4, 5], nil]),
     27:                  Arrow::ListArray.new(3,
/home/travis/build/apache/arrow/c_glib/test/test-list-array.rb:24:in `new'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:328:in `block in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `block (2 levels) in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `invoke'
Error: test_new(TestListArray): ArgumentError: <Arrow::Int8Array#new>: the 2nd argument must not nil: <null_bitmap>
================================================================================
...........................................E
================================================================================
/home/travis/build/apache/arrow/c_glib/test/test-struct-array.rb:41:in `test_new'
     38: 
     39:     data_type = Arrow::StructDataType.new(fields)
     40:     children = [
  => 41:       Arrow::Int8Array.new(2, Arrow::Buffer.new([-29, 2].pack(""C*"")), nil, 0),
     42:       Arrow::BooleanArray.new(2, Arrow::Buffer.new([0b01].pack(""C*"")), nil, 0),
     43:     ]
     44:     assert_equal(struct_array1,
/home/travis/build/apache/arrow/c_glib/test/test-struct-array.rb:41:in `new'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:328:in `block in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `block (2 levels) in load_constructor_infos'
/home/travis/.rvm/gems/ruby-2.4.1/gems/gobject-introspection-3.2.2/lib/gobject-introspection/loader.rb:317:in `invoke'
Error: test_new(TestStructArray): ArgumentError: <Arrow::Int8Array#new>: the 2nd argument must not nil: <null_bitmap>
================================================================================
{code}

",ci-failure pull-request-available,"['Continuous Integration', 'GLib']",ARROW,Bug,Major,2018-04-02 11:36:50,1
13149381,[Rust] Travis should run tests for Rust library,"We need travis to build the Rust library and run tests so that future PRs do not introduce regressions.



PR:https://github.com/apache/arrow/pull/1819",pull-request-available,['Rust'],ARROW,Improvement,Major,2018-04-01 21:29:06,10
13149380,[Rust] Buffer should release memory when dropped,"The initialPR failed to implement Drop for Buffer<T> meaning that memory is never deallocated.

Here is a PR to resolve this:https://github.com/apache/arrow/pull/1821",pull-request-available,['Rust'],ARROW,Bug,Major,2018-04-01 21:25:31,10
13149320,[Rust] Add support for array of List<T>,Add support for List<T> in Array types. Look at Utf8 which wraps List<u8> to see how this works.,pull-request-available,['Rust'],ARROW,New Feature,Major,2018-03-31 19:15:36,10
13149160,Large (>~20 GB) files written to Parquet via PyArrow are corrupted,"When writing large Parquet files (above 10 GB or so) from Pandas to Parquet via the command

{{pq.write_table(my_df, 'table.parquet')}}

The write succeeds, but when the parquet file is loaded, the error message

{{ArrowIOError: Invalid parquet file. Corrupt footer.}}

appears. This same error occurs when the parquet file is written chunkwise as well. When the parquet files are small, say < 5 GB or so (drawn randomly from the same dataset), everything proceeds as normal. I've also tried this with Pandas df.to_parquet(), with the same results.

Update: Looks like any DataFrame with size above ~5GB (on disk) returns the same error.",Parquet bug pandas parquetWriter pull-request-available pyarrow,['Python'],ARROW,Bug,Major,2018-03-30 13:00:03,2
13148952,[Python][C++][Parquet] Support reading Parquet files having a permutation of column order,See discussion in https://github.com/dask/fastparquet/issues/320,dataset dataset-parquet-read parquet,['Python'],ARROW,Improvement,Major,2018-03-29 17:25:21,5
13148573,[Rust] Start native Rust Implementation,"I'm creating this Jira to track work to donate an work-in-progress native Rust implementation of Arrow.

I am actively developing this and relying on it for the memory model of my DataFusion project. I would like to donate the code I have now and start working on it under the Apache Arrow project.

Here is the PR: https://github.com/apache/arrow/pull/1804



",pull-request-available,['Rust'],ARROW,New Feature,Major,2018-03-28 13:42:52,10
13148256,[C++] Type objects produced by DataType factory are not thread safe,"TYPE_FACTORY() macro that produces type shortcuts (boolean(), int32(), utf8() and so on) uses static shared_ptr inside. There are race conditions possible against shared_ptr's reference counter.",pull-request-available,['C++'],ARROW,Task,Minor,2018-03-27 15:51:58,2
13148084,Benchmark PandasObjectIsNull,This is a follow-up to ARROW-2354 ([C++] Make PyDecimal_Check() faster). We should benchmark {{PandasObjectIsNull}} as it gets called in many of our conversion routines in tight loops.,pull-request-available,['C++'],ARROW,Bug,Major,2018-03-26 23:52:28,2
13147974,[C++] PyDecimal_Check() is much too slow,See https://github.com/apache/arrow/issues/1792,pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-03-26 16:41:42,2
13147777,[C++/Python] Test OSX packaging in Travis matrix,"At the moment, we only test the conda based build in Travis but we also ship binary wheels after the release. The process of building them is currently part of the {{arrow-dist}} repository and uses the {{multibuild}}scripts that are used for many other Python packages that also have native code.

The code should be ported to run as a real CI job, i.e. in addition to just packaging the code, we will also need to run the unit tests. Furthermore, once the job is running and green, we also need to look at the runtimes as we already have a quite packed CI matrix and we expect that many steps of the wheel build are just to setup the environment. We should be able to cache them.

Maybe we want to do this as a nightly cron. For a first draft, it will be ok to add it to the full matrix.",beginner,"['C++', 'Python']",ARROW,Improvement,Major,2018-03-25 10:41:09,8
13147627,[Python] Boost shared library bundling is broken for MSVC,See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow-dist/build/1.0.127/job/fjxeaecwidgb6mqr,pull-request-available,['Python'],ARROW,Bug,Major,2018-03-24 00:24:45,14
13147617,[GLib] Remove Go example,"Now, we have Go native implementation.",pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-03-23 22:59:40,1
13147405,[Python] PYARROW_CXXFLAGS doesn't accept multiple options,"Let's say I want to enable multiple warnings. I try:
{code:bash}
PYARROW_CXXFLAGS=""-Wextra -Wconversion"" python setup.py build
{code}

and get the following error:
{code:bash}
[ 22%] Building CXX object CMakeFiles/plasma.dir/plasma.cxx.o
g++-4.9: error: unrecognized command line option '-Wextra -Wconversion'
{code}

For some reason it seems command expansion doesn't work properly. ""{{-Wextra -Wconversion}}"" is passed as a single argument instead of two separate ones...",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-23 09:21:52,2
13147177,[Python] Aware timestamp type fails pickling,"{code:python}
>>> ty = pa.timestamp('ms')
>>> pickle.loads(pickle.dumps(ty))
TimestampType(timestamp[ms])
>>> ty = pa.timestamp('ms', tz='UTC')
>>> pickle.loads(pickle.dumps(ty))
Traceback (most recent call last):
  File ""<ipython-input-15-889e5adab733>"", line 1, in <module>
    pickle.loads(pickle.dumps(ty))
  File ""types.pxi"", line 82, in pyarrow.lib.DataType.__setstate__
  File ""types.pxi"", line 1246, in pyarrow.lib.type_for_alias
ValueError: No type alias for timestamp[ms, tz=utc]

{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-22 15:06:23,2
13147164,[Python] pa.union() mode argument unintuitive,"{code:python}
>>> pa.union([pa.field('x', pa.int16())], 'sparse')
UnionType(union[dense]<x: int16=0>)
>>> pa.union([pa.field('x', pa.int16())], 'dense')
UnionType(union[dense]<x: int16=0>)
>>> pa.union([pa.field('x', pa.int16())], 42)
UnionType(union[dense]<x: int16=0>)
{code}

I would suggest allowing ""sparse"" and ""dense"" strings as aliases to the internal enum values, and disallowing everything else.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-22 14:30:24,2
13147032,[Website] Add blog post about Go codebase donation,Adding blog post from the Arrow PMC,pull-request-available,['Website'],ARROW,Improvement,Major,2018-03-22 02:00:02,14
13146970,[Scripts] Windows release verification script should use boost DSOs instead of static linkage,Fix up shortly,pull-request-available,['Packaging'],ARROW,Task,Major,2018-03-21 20:57:41,14
13146960,[Website] Blog post for 0.9.0 release,Release highlights blog post as in past major releases,pull-request-available,['Website'],ARROW,Improvement,Major,2018-03-21 20:31:11,14
13146901,[Python] boost bundling fails in setup.py,"{code}
[...]
               File ""setup.py"", line 88, in run
                 self._run_cmake()
               File ""setup.py"", line 280, in _run_cmake
                 ""{}_filesystem"".format(self.boost_namespace))
               File ""setup.py"", line 386, in move_shared_libs
                 _move_shared_libs_unix(build_prefix, build_lib, lib_name)
               File ""setup.py"", line 408, in _move_shared_libs_unix
                 ' in ' + build_prefix)
             Exception: Could not find library:libNone_filesystem.so in release
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-21 16:41:17,2
13146649,[Python] Fix indexing implementations,"A number of {{\_\_getitem\_\_}} implementations handle negative or out-of-bounds indices improperly, for example:

{code:python}
>>> a = pa.array([11,12,13])
>>> a[-6]
11
>>> a[-15]
11
>>> a[4]
NA
>>> a[3]
NA
>>> a[1111]
NA
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-03-20 19:37:50,2
13145931,[Python] cannot import pip installed pyarrow on OS X (10.9),"{code:java}
$ pip3 install pyarrow --user
Collecting pyarrow
Using cached pyarrow-0.8.0-cp36-cp36m-macosx_10_6_intel.whl
Requirement already satisfied: six>=1.0.0 in ./Library/Python/3.6/lib/python/site-packages (from pyarrow)
Collecting numpy>=1.10 (from pyarrow)
Using cached numpy-1.14.2-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl
Installing collected packages: numpy, pyarrow
Successfully installed numpy-1.14.2 pyarrow-0.8.0

$ python3
Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyarrow
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/Users/pi/Library/Python/3.6/lib/python/site-packages/pyarrow/__init__.py"", line 32, in <module>
from pyarrow.lib import cpu_count, set_cpu_count
ImportError: dlopen(/Users/pi/Library/Python/3.6/lib/python/site-packages/pyarrow/lib.cpython-36m-darwin.so, 2): Library not loaded: @rpath/libarrow.0.dylib
Referenced from: /Users/pi/Library/Python/3.6/lib/python/site-packages/pyarrow/lib.cpython-36m-darwin.so
Reason: image not found
{code}
I dug into it a bit and found that in older versions of install.rst, Wes mentioned that XCode 6 had trouble with rpath, so not sure if that's what's going on here for me. I'm on 10.9, I know it's really old, so if these wheels can't be made to run on my ancient OS, I just wanted to report this so the wheels uploaded to PyPI can reflect this incompatibility, if that is indeed the case. I might also try some otool / install_name_tool tomfoolery to see if I can get a workaround for myself.

Thank you!",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-03-17 06:16:21,14
13145787,Document requirements to run dev/release/01-perform.sh,"I am unable to run this script on Ubuntu 16.04

{code}
[INFO] [ERROR] Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.8.2:deploy (default-deploy) on project arrow-java-root: Failed to deploy artifacts: Could not transfer artifact org.apache.arrow:arrow-java-root:pom:0.9.0 from/to apache.releases.https (https://repository.apache.org/service/local/staging/deploy/maven2): Failed to transfer file: https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/arrow/arrow-java-root/0.9.0/arrow-java-root-0.9.0.pom. Return code is: 401, ReasonPhrase: Unauthorized. -> [Help 1]
{code}

I'm sure there's an easy fix for this, but the requirements aren't documented in dev/release/README, so other PMC members are likely to also have problems",pull-request-available,['Packaging'],ARROW,Improvement,Major,2018-03-16 17:06:27,14
13145551,[C++] Add buffered output class implementing OutputStream interface,"This purpose of this is to throttle smaller writes to the actual underlying {{OutputStream}} interface, which might be a file or network protocol. ",pull-request-available,['C++'],ARROW,New Feature,Major,2018-03-16 00:41:05,2
13145444,[Python] fix C linkage warning,"When using pyarrow interface from a c++ library one will get the following compiler warning:
{quote}{{warning: 'unwrap_table' has C-linkage specified, but returns user-defined type 'arrow::Status' which is incompatible with C [-Wreturn-type-c-linkage]}}
{{ARROW_EXPORT Status unwrap_table(PyObject* table, std::shared_ptr<Table>* out);}}
{quote}
This is due to a Cython artifact.

",pull-request-available,['Python'],ARROW,Bug,Minor,2018-03-15 17:50:35,2
13145408,[C++] Revert Buffer::mutable_data member to always inline,"If not, linkers must remember to define {{NDEBUG}} depending on whether Arrow was built in release mode or not",pull-request-available,['C++'],ARROW,Bug,Major,2018-03-15 15:11:33,14
13145385,[C++/Python] Add method to flatten a struct array,See ARROW-1886. We want to be able to take a StructArray and flatten it into independent field arrays.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-03-15 14:34:05,2
13145348,[Python] Union array slicing is defective,"{code:python}
>>> a = pa.UnionArray.from_sparse(pa.array([0,1,1], type=pa.int8()), [pa.array([""a"", ""b"", ""c""]), pa.array([2,3,4])])
>>> a
<pyarrow.lib.UnionArray object at 0x7fe9381304a8>
[
  'a',
  3,
  4
]
>>> a[1:]
<pyarrow.lib.UnionArray object at 0x7fe939409598>
[
  2,
  3
]
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-03-15 11:43:27,2
13145274,[GLib] Release builds must define NDEBUG,"Ran into another problem with {{verify-release-candidate.sh 0.9.0 0}} -- the GLib build is not defining NDEBUG -- depending on whether Arrow was built in release or debug mode, some symbols (like {{Buffer::mutable_data}}) may not be inlined

{code}
  CXX      libarrow_glib_la-compute.lo
  CC       enums.lo
  CXXLD    libarrow-glib.la
ar: `u' modifier ignored since `D' is the default (see `U')
  GISCAN   Arrow-1.0.gir
./.libs/libarrow-glib.so: undefined reference to `arrow::Buffer::mutable_data()'
collect2: error: ld returned 1 exit status
linking of temporary binary failed: Command '['/bin/bash', '../libtool', '--mode=link', '--tag=CC', '--silent', 'gcc', '-o', '/tmp/arrow-0.9.0.hlQDV/apache-arrow-0.9.0/c_glib/arrow-glib/tmp-introspect7g38ad/Arrow-1.0', '-export-dynamic', '-g', '-O2', 'tmp-introspect7g38ad/tmp/arrow-0.9.0.hlQDV/apache-arrow-0.9.0/c_glib/arrow-glib/tmp-introspect7g38ad/Arrow-1.0.o', '-L.', 'libarrow-glib.la', '-Wl,--export-dynamic', '-lgmodule-2.0', '-pthread', '-lgio-2.0', '-lgobject-2.0', '-lglib-2.0']' returned non-zero exit status 1
/usr/share/gobject-introspection-1.0/Makefile.introspection:155: recipe for target 'Arrow-1.0.gir' failed
{code}",pull-request-available,['GLib'],ARROW,Bug,Major,2018-03-15 04:48:22,1
13145231,[Python] Struct array slicing defective,"{code:python}
>>> arr = pa.array([(1, 2.0), (3, 4.0), (5, 6.0)], type=pa.struct([pa.field('x', pa.int16()), pa.field('y', pa.float32())]))
>>> arr
<pyarrow.lib.StructArray object at 0x7fdfbe7916d8>
[
  {'x': 1, 'y': 2.0},
  {'x': 3, 'y': 4.0},
  {'x': 5, 'y': 6.0}
]
>>> arr[1:]
<pyarrow.lib.StructArray object at 0x7fdfbe791f48>
[
  {'x': 1, 'y': 2.0},
  {'x': 3, 'y': 4.0}
]
{code}",pull-request-available,"['C++', 'Python']",ARROW,Bug,Major,2018-03-15 01:18:32,2
13145228,Source release scripts fail with Java8,"It's getting harder and harder to install Java7 these days. On a new install of Ubuntu 16.04 I am not even sure how to get Oracle's Java7 installed (though Java8 can be installed through a PPA).

In lieu of fixing all the javadoc problems, it would be great if there was some other workaround to build the release on Java8",pull-request-available,['Packaging'],ARROW,Bug,Major,2018-03-15 01:07:28,1
13145184,[C++] Use std::make_unsigned,"{{arrow/util/bit-util.h}} has a reimplementation of {{boost::make_unsigned}}, but we could simply use {{std::make_unsigned}}, which is C++11.",pull-request-available,['C++'],ARROW,Task,Trivial,2018-03-14 21:17:24,2
13144891,[Python] Unable to read arrow stream containing 0 record batches,"Using java arrow I'm creating an arrow stream, using the stream writer.



Sometimes I don't have anything to serialize, and so Idon't write any record batches. My arrow stream thus consists of just a schema message.
{code:java}
<SCHEMA>
<EOS [optional]: int32>
{code}

I am able to deserialize this arrow stream correctly using the java stream reader, but when reading it with python I instead hit an error
{code}
import pyarrow as pa
# ...
reader = pa.open_stream(stream)
df = reader.read_all().to_pandas()
{code}

produces

{code}
  File ""ipc.pxi"", line 307, in pyarrow.lib._RecordBatchReader.read_all
  File ""error.pxi"", line 77, in pyarrow.lib.check_status
ArrowInvalid: Must pass at least one record batch
{code}

i.e. we're hitting the check in https://github.com/apache/arrow/blob/apache-arrow-0.8.0/cpp/src/arrow/table.cc#L284

The workaround we're currently using is to always ensure we serialize at least one record batch, even if it's empty. However, I think it would be nice to either support a stream without record batches or explicitly disallow this and then match behaviour in java.",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-13 23:44:23,14
13144850,[Python] HDFS test failures,"These weren't caught because we aren't running the HDFS tests in Travis CI

{code}
pyarrow/tests/test_hdfs.py::TestLibHdfs::test_write_to_dataset_no_partitions FAILED
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

self = <pyarrow.tests.test_hdfs.TestLibHdfs testMethod=test_write_to_dataset_no_partitions>

    @test_parquet.parquet
    def test_write_to_dataset_no_partitions(self):
        tmpdir = pjoin(self.tmp_path, 'write-no_partitions-' + guid())
        self.hdfs.mkdir(tmpdir)
        test_parquet._test_write_to_dataset_no_partitions(
>           tmpdir, filesystem=self.hdfs)

pyarrow/tests/test_hdfs.py:367: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyarrow/tests/test_parquet.py:1475: in _test_write_to_dataset_no_partitions
    filesystem=filesystem)
pyarrow/parquet.py:1059: in write_to_dataset
    _mkdir_if_not_exists(fs, root_path)
pyarrow/parquet.py:1006: in _mkdir_if_not_exists
    if fs._isfilestore() and not fs.exists(path):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyarrow.hdfs.HadoopFileSystem object at 0x7f09e87a4c48>

    def _isfilestore(self):
        """"""
            Returns True if this FileSystem is a unix-style file store with
            directories.
            """"""
>       raise NotImplementedError
E       NotImplementedError

pyarrow/filesystem.py:143: NotImplementedError
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
> /home/wesm/code/arrow/python/pyarrow/filesystem.py(143)_isfilestore()
-> raise NotImplementedError
(Pdb) c

pyarrow/tests/test_hdfs.py::TestLibHdfs::test_write_to_dataset_with_partitions FAILED
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

self = <pyarrow.tests.test_hdfs.TestLibHdfs testMethod=test_write_to_dataset_with_partitions>

    @test_parquet.parquet
    def test_write_to_dataset_with_partitions(self):
        tmpdir = pjoin(self.tmp_path, 'write-partitions-' + guid())
        self.hdfs.mkdir(tmpdir)
        test_parquet._test_write_to_dataset_with_partitions(
>           tmpdir, filesystem=self.hdfs)

pyarrow/tests/test_hdfs.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
pyarrow/tests/test_parquet.py:1433: in _test_write_to_dataset_with_partitions
    filesystem=filesystem)
pyarrow/parquet.py:1059: in write_to_dataset
    _mkdir_if_not_exists(fs, root_path)
pyarrow/parquet.py:1006: in _mkdir_if_not_exists
    if fs._isfilestore() and not fs.exists(path):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pyarrow.hdfs.HadoopFileSystem object at 0x7f09e87a4c48>

    def _isfilestore(self):
        """"""
            Returns True if this FileSystem is a unix-style file store with
            directories.
            """"""
>       raise NotImplementedError
E       NotImplementedError

pyarrow/filesystem.py:143: NotImplementedError
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> entering PDB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
> /home/wesm/code/arrow/python/pyarrow/filesystem.py(143)_isfilestore()
-> raise NotImplementedError
(Pdb) c
{code}",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-03-13 20:51:52,14
13144849,[Python] Cython 0.25.2 compilation failure ,"Observed on master branch

{code}
Error compiling Cython file:
------------------------------------------------------------
...
        if hasattr(self, 'as_py'):
            return repr(self.as_py())
        else:
            return super(Scalar, self).__repr__()

    def __eq__(self, other):
   ^
------------------------------------------------------------

/home/wesm/code/arrow/python/pyarrow/scalar.pxi:67:4: Special method __eq__ must be implemented via __richcmp__

Error compiling Cython file:
------------------------------------------------------------
...
        Return true if the tensors contains exactly equal data
        """"""
        self._validate()
        return self.tp.Equals(deref(other.tp))

    def __eq__(self, other):
   ^
------------------------------------------------------------

/home/wesm/code/arrow/python/pyarrow/array.pxi:571:4: Special method __eq__ must be implemented via __richcmp__

Error compiling Cython file:
------------------------------------------------------------
...
        cdef c_bool result = False
        with nogil:
            result = self.buffer.get().Equals(deref(other.buffer.get()))
        return result

    def __eq__(self, other):
   ^
------------------------------------------------------------

/home/wesm/code/arrow/python/pyarrow/io.pxi:675:4: Special method __eq__ must be implemented via __richcmp__
{code}

Upgrading Cython made this go away. We should probably use {{__richcmp__}} though",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-13 20:50:32,2
13144764,[C++] MultipleClients test in io-hdfs-test fails on trunk,"This fails for me locally:

{code}
[ RUN      ] TestHadoopFileSystem/0.MultipleClients
../src/arrow/io/io-hdfs-test.cc:192: Failure
Value of: s.ok()
  Actual: false
Expected: true
{code}",pull-request-available,['C++'],ARROW,Bug,Critical,2018-03-13 15:28:12,14
13144754,[GLib] Run autotools and meson Linux builds in same Travis CI build entry,"Since our CI matrix is going to expand, and these builds are fast (< 5 minutes), I suggest we run these builds in the same job:

https://travis-ci.org/apache/arrow/builds/352848066",pull-request-available,['GLib'],ARROW,Improvement,Major,2018-03-13 14:48:31,1
13144619,[Python] Add source distribution publishing instructions to package / release management documentation,We wish to start publishing source tarballs for Python on PyPI,pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-13 04:59:15,8
13144608,[Python] python/testing/test_hdfs.sh no longer works,"Tried this on a fresh Ubuntu 16.04 install:

{code}
$ ./test_hdfs.sh 
+ docker build -t arrow-hdfs-test -f hdfs/Dockerfile .
Sending build context to Docker daemon  36.86kB
Step 1/6 : FROM cpcloud86/impala:metastore
manifest for cpcloud86/impala:metastore not found
{code}",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-03-13 03:58:56,3
13144414,[Python] Add option to not consider NaN to be null when converting to an integer Arrow type,Follow-on work to ARROW-2135,pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-12 19:04:04,14
13143795,[Python] More consistent / intuitive name for pyarrow.frombuffer,"Now that we have {{pyarrow.foreign_buffer}}, things are a bit odd. We could call {{frombuffer}} something like {{py_buffer}} instead?",pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-09 04:25:04,2
13143784,[C++] README missing instructions for libboost-regex-dev,"After following the instructions in the README, I could not generate a makefile using CMake because of a missing dependency.

The README needs to be updated to include installing libboost-regex-dev.

",pull-request-available,['C++'],ARROW,Improvement,Trivial,2018-03-09 03:49:53,10
13143507,[Python] slicing logic defective,"The slicing logic tends to go too far when normalizing large negative bounds, which leads to results not in line with Python's slicing semantics:
{code}
>>> arr = pa.array([1,2,3,4])
>>> arr[-99:100]
<pyarrow.lib.Int64Array object at 0x7f550813a318>
[
  2,
  3,
  4
]
>>> arr.to_pylist()[-99:100]
[1, 2, 3, 4]
>>> 
>>> 
>>> arr[-6:-5]
<pyarrow.lib.Int64Array object at 0x7f54cd76a908>
[
  3
]
>>> arr.to_pylist()[-6:-5]
[]
{code}
Also note this crash:
{code}
>>> arr[10:13]
/home/antoine/arrow/cpp/src/arrow/array.cc:105 Check failed: (offset) <= (data.length) 
Abandon (core dumped)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-08 10:37:01,2
13143219,"[Python] chunked array not iterable, not indexable",It would be useful to access individual elements of a chunked array either through iteration or indexing.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-07 13:20:21,3
13143215,[Python] Allow subscripting pyarrow.lib.StructValue,"{code:python}
>>> obj
{'x': 42, 'y': True}
>>> type(obj)
pyarrow.lib.StructValue
>>> obj['x']
Traceback (most recent call last):
 File ""<ipython-input-96-93ad3443b3c6>"", line 1, in <module>
 obj['x']
TypeError: 'pyarrow.lib.StructValue' object is not subscriptable
{code}",pull-request-available,['Python'],ARROW,Wish,Major,2018-03-07 13:00:43,3
13143208,[Python] Can't convert Numpy string arrays,"{code:python}
>>> arr = np.array([b'foo', b'bar'], dtype='S3')
>>> pa.array(arr, type=pa.binary(3))
Traceback (most recent call last):
  File ""<ipython-input-40-d52a5aa3b94e>"", line 1, in <module>
    pa.array(arr, type=pa.binary(3))
  File ""array.pxi"", line 177, in pyarrow.lib.array
  File ""array.pxi"", line 77, in pyarrow.lib._ndarray_to_array
  File ""error.pxi"", line 85, in pyarrow.lib.check_status
ArrowNotImplementedError: /home/antoine/arrow/cpp/src/arrow/python/numpy_to_arrow.cc:1661 code: converter.Convert()
NumPyConverter doesn't implement <fixed_size_binary[3]> conversion. 
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-07 12:46:20,3
13143182,[Python] test_plasma error on plasma_store error,"This appears caused by my latest changes:
{code:python}
Traceback (most recent call last):
 File ""/home/antoine/arrow/python/pyarrow/tests/test_plasma.py"", line 192, in setup_method
 plasma_store_name, self.p = self.plasma_store_ctx.__enter__()
 File ""/home/antoine/miniconda3/envs/pyarrow/lib/python3.6/contextlib.py"", line 81, in __enter__
 return next(self.gen)
 File ""/home/antoine/arrow/python/pyarrow/tests/test_plasma.py"", line 168, in start_plasma_store
 err = proc.stderr.read().decode()
AttributeError: 'NoneType' object has no attribute 'read'
{code}",pull-request-available,['Python'],ARROW,Bug,Trivial,2018-03-07 11:16:49,2
13142989,[Python] Create StringArray from buffers,"While we will add a more general-purpose functionality in https://issues.apache.org/jira/browse/ARROW-2281, the interface is more complicate then the constructor that explicitly states all arguments: {{StringArray(int64_t length, const std::shared_ptr<Buffer>& value_offsets, }}

Thus I will also expose this explicit constructor.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-06 21:48:23,8
13142984,[Python]Expose MakeArray to construct arrays from buffers,"To create new arrays from existing buffers in Python, we would need to call into the C++ {{MakeArray}} method. This wouldthen construct the Array and we would only wrap it in Python to have construction support for all Array types.

This would also mean that we need to have a Python representation of {{ArrayData}}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-06 21:42:35,2
13142954,[Python] pyarrow.Array.buffers should also include the offsets,"Currently we only return the buffers but they don't make sense without the offsets for them, esp. the validity bitmap will have a non-zero offset in most cases where the array was sliced.",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-03-06 19:47:24,8
13142893,[Python] Tensor could implement the buffer protocol,"Tensors have an underlying buffer, a data type, shape and strides. It seems like they could implement the Python buffer protocol.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-03-06 15:47:04,2
13142827,[Python] test_plasma spams /tmp,{{test_plasma}} creates a new socket in {{/tmp}} for each test and never cleans up.,pull-request-available,['Python'],ARROW,Bug,Trivial,2018-03-06 11:55:39,2
13142815,[Python] ForeignBuffer doesn't tie Python object lifetime to C++ buffer lifetime,"{{ForeignBuffer}} keeps the reference to the Python base object in the Python wrapper class, not in the C++ buffer instance, meaning if the C++ buffer gets passed around but the Python wrapper gets destroyed, the reference to the original Python base object will be released.",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-06 10:54:47,2
13142804,[Python] Cannot build bdist_wheel for Python,"I am trying current master.

I ran:

{{{{
python setup.py build_ext --build-type=$ARROW_BUILD_TYPE --with-parquet --with-plasma --bundle-arrow-cpp bdist_wheel
}}}}

Output:

{{{{
running build_ext
creating build
creating build/temp.linux-x86_64-3.6
-- Runnning cmake for pyarrow
cmake -DPYTHON_EXECUTABLE=.../Temp/arrow/pyarrow/bin/python  -DPYARROW_BUILD_PARQUET=on -DPYARROW_BOOST_USE_SHARED=on -DPYARROW_BUILD_PLASMA=on -DPYARROW_BUNDLE_ARROW_CPP=ON -DCMAKE_BUILD_TYPE=release .../Temp/arrow/arrow/python
-- The C compiler identification is GNU 7.2.0
-- The CXX compiler identification is GNU 7.2.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
INFOCompiler command: /usr/bin/c++
INFOCompiler version: Using built-in specs.
COLLECT_GCC=/usr/bin/c++
COLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/7/lto-wrapper
OFFLOAD_TARGET_NAMES=nvptx-none
OFFLOAD_TARGET_DEFAULT=1
Target: x86_64-linux-gnu
Configured with: ../src/configure -v --with-pkgversion='Ubuntu 7.2.0-8ubuntu3.2' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --enable-default-pie --with-system-zlib --with-target-system-zlib --enable-objc-gc=auto --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu
Thread model: posix
gcc version 7.2.0 (Ubuntu 7.2.0-8ubuntu3.2) 

INFOCompiler id: GNU
Selected compiler gcc 7.2.0
-- Performing Test CXX_SUPPORTS_SSE3
-- Performing Test CXX_SUPPORTS_SSE3 - Success
-- Performing Test CXX_SUPPORTS_ALTIVEC
-- Performing Test CXX_SUPPORTS_ALTIVEC - Failed
Configured for RELEASE build (set with cmake -DCMAKE_BUILD_TYPE={release,debug,...})
-- Build Type: RELEASE
-- Build output directory: .../Temp/arrow/arrow/python/build/temp.linux-x86_64-3.6/release/
-- Found PythonInterp: .../Temp/arrow/pyarrow/bin/python (found version ""3.6.3"") 
-- Searching for Python libs in .../Temp/arrow/pyarrow/lib64;.../Temp/arrow/pyarrow/lib;/usr/lib/python3.6/config-3.6m-x86_64-linux-gnu
-- Looking for python3.6m
-- Found Python lib /usr/lib/python3.6/config-3.6m-x86_64-linux-gnu/libpython3.6m.so
-- Found PythonLibs: /usr/lib/python3.6/config-3.6m-x86_64-linux-gnu/libpython3.6m.so
-- Found NumPy: version ""1.14.1"" .../Temp/arrow/pyarrow/lib/python3.6/site-packages/numpy/core/include
-- Searching for Python libs in .../Temp/arrow/pyarrow/lib64;.../Temp/arrow/pyarrow/lib;/usr/lib/python3.6/config-3.6m-x86_64-linux-gnu
-- Looking for python3.6m
-- Found Python lib /usr/lib/python3.6/config-3.6m-x86_64-linux-gnu/libpython3.6m.so
-- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") 
-- Checking for module 'arrow'
--   Found arrow, version 0.9.0-SNAPSHOT
-- Arrow ABI version: 0.0.0
-- Arrow SO version: 0
-- Found the Arrow core library: .../Temp/arrow/dist/lib/libarrow.so
-- Found the Arrow Python library: .../Temp/arrow/dist/lib/libarrow_python.so
-- Boost version: 1.63.0
-- Found the following Boost libraries:
--   system
--   filesystem
--   regex
Added shared library dependency arrow: .../Temp/arrow/dist/lib/libarrow.so
Added shared library dependency arrow_python: .../Temp/arrow/dist/lib/libarrow_python.so
-- Found the Parquet library: .../Temp/arrow/dist/lib/libparquet.so
Added shared library dependency parquet: .../Temp/arrow/dist/lib/libparquet.so
-- Checking for module 'plasma'
--   Found plasma, version 
-- Plasma ABI version: 0.0.0
-- Plasma SO version: 0
-- Found the Plasma core library: .../Temp/arrow/dist/lib/libplasma.so
-- Found Plasma executable: .../Temp/arrow/dist/bin/plasma_store
Added shared library dependency libplasma: .../Temp/arrow/dist/lib/libplasma.so
-- Configuring done
-- Generating done
-- Build files have been written to: .../Temp/arrow/arrow/python/build/temp.linux-x86_64-3.6
-- Finished cmake for pyarrow
-- Running cmake --build for pyarrow
make
Scanning dependencies of target lib_pyx
[ 11%] Compiling Cython CXX source for lib...
[ 11%] Built target lib_pyx
Scanning dependencies of target lib
[ 22%] Building CXX object CMakeFiles/lib.dir/lib.cxx.o
[ 33%] Linking CXX shared module release/lib.cpython-36m-x86_64-linux-gnu.so
[ 33%] Built target lib
Scanning dependencies of target _parquet_pyx
[ 44%] Compiling Cython CXX source for _parquet...
[ 44%] Built target _parquet_pyx
Scanning dependencies of target _parquet
[ 55%] Building CXX object CMakeFiles/_parquet.dir/_parquet.cxx.o
[ 66%] Linking CXX shared module release/_parquet.cpython-36m-x86_64-linux-gnu.so
[ 66%] Built target _parquet
Scanning dependencies of target plasma_pyx
[ 77%] Compiling Cython CXX source for plasma...
[ 77%] Built target plasma_pyx
Scanning dependencies of target plasma
[ 88%] Building CXX object CMakeFiles/plasma.dir/plasma.cxx.o
[100%] Linking CXX shared module release/plasma.cpython-36m-x86_64-linux-gnu.so
[100%] Built target plasma
-- Finished cmake --build for pyarrow
.../Temp/arrow/arrow/python/build/lib.linux-x86_64-3.6/pyarrow
['release/libarrow.so.0.0.0', 'release/libarrow.so.0', 'release/libarrow.so']
release/libarrow.so.0.0.0
['release/libarrow_python.so.0.0.0', 'release/libarrow_python.so.0', 'release/libarrow_python.so']
release/libarrow_python.so.0.0.0
['release/libplasma.so.0.0.0', 'release/libplasma.so.0', 'release/libplasma.so']
release/libplasma.so.0.0.0
['release/libparquet.so.1.3.2', 'release/libparquet.so.1', 'release/libparquet.so']
release/libparquet.so.1.3.2
}}}}

And after that it crashes because it cannot find {{arrow_boost_filesystem}} in {{build_prefix}} in {{_move_shared_libs_unix}} call which makes {{libs}} variable there empty.

If I do:

{{{{
$ find . -name *.so
./build/lib.linux-x86_64-3.6/pyarrow/libparquet.so
./build/lib.linux-x86_64-3.6/pyarrow/libplasma.so
./build/lib.linux-x86_64-3.6/pyarrow/libarrow.so
./build/lib.linux-x86_64-3.6/pyarrow/libarrow_python.so
./build/temp.linux-x86_64-3.6/release/_parquet.cpython-36m-x86_64-linux-gnu.so
./build/temp.linux-x86_64-3.6/release/libparquet.so
./build/temp.linux-x86_64-3.6/release/libboost_filesystem.so
./build/temp.linux-x86_64-3.6/release/libplasma.so
./build/temp.linux-x86_64-3.6/release/libarrow.so
./build/temp.linux-x86_64-3.6/release/libboost_system.so
./build/temp.linux-x86_64-3.6/release/lib.cpython-36m-x86_64-linux-gnu.so
./build/temp.linux-x86_64-3.6/release/libboost_regex.so
./build/temp.linux-x86_64-3.6/release/libarrow_python.so
./build/temp.linux-x86_64-3.6/release/plasma.cpython-36m-x86_64-linux-gnu.so
./pyarrow/_parquet.cpython-36m-x86_64-linux-gnu.so
./pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
./pyarrow/plasma.cpython-36m-x86_64-linux-gnu.so
}}}}

So it seems the issue is that the filename is {{libboost_filesystem.so}}, but it is trying to move {{libarrow_boost_filesystem.so}}?

Changing lines in {{setup.py}} to:

{{{{
                    move_shared_libs(build_prefix, build_lib, ""boost_filesystem"")
                    move_shared_libs(build_prefix, build_lib, ""boost_system"")
                    move_shared_libs(build_prefix, build_lib, ""boost_regex"")
}}}}

seems to make it build.",pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2018-03-06 10:02:27,8
13142723,Remove MD5 checksums from release process,The ASF has changed its release policy for signatures and checksums to contraindicate the use of MD5 checksums: http://www.apache.org/dev/release-distribution#sigs-and-sums. We should remove this from our various release scripts prior to the 0.9.0 release,pull-request-available,['Developer Tools'],ARROW,Bug,Major,2018-03-06 01:28:26,14
13142641,[Python] test_cython.py fails if pyarrow is not in import path (e.g. with inplace builds),"see 

{code}
$ py.test pyarrow/tests/test_cython.py 
===================================== test session starts =====================================
platform linux -- Python 3.6.4, pytest-3.4.1, py-1.5.2, pluggy-0.6.0
rootdir: /home/wesm/code/arrow/python, inifile: setup.cfg
collected 1 item                                                                              

pyarrow/tests/test_cython.py F                                                          [100%]

========================================== FAILURES ===========================================
_______________________________________ test_cython_api _______________________________________

tmpdir = local('/tmp/pytest-of-wesm/pytest-3/test_cython_api0')

    @pytest.mark.skipif(
        'ARROW_HOME' not in os.environ,
        reason='ARROW_HOME environment variable not defined')
    def test_cython_api(tmpdir):
        """"""
        Basic test for the Cython API.
        """"""
        pytest.importorskip('Cython')
    
        ld_path_default = os.path.join(os.environ['ARROW_HOME'], 'lib')
    
        test_ld_path = os.environ.get('PYARROW_TEST_LD_PATH', ld_path_default)
    
        with tmpdir.as_cwd():
            # Set up temporary workspace
            pyx_file = 'pyarrow_cython_example.pyx'
            shutil.copyfile(os.path.join(here, pyx_file),
                            os.path.join(str(tmpdir), pyx_file))
            # Create setup.py file
            if os.name == 'posix':
                compiler_opts = ['-std=c++11']
            else:
                compiler_opts = []
            setup_code = setup_template.format(pyx_file=pyx_file,
                                               compiler_opts=compiler_opts,
                                               test_ld_path=test_ld_path)
            with open('setup.py', 'w') as f:
                f.write(setup_code)
    
            # Compile extension module
            subprocess.check_call([sys.executable, 'setup.py',
>                                  'build_ext', '--inplace'])

pyarrow/tests/test_cython.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

popenargs = (['/home/wesm/miniconda/envs/arrow-dev/bin/python', 'setup.py', 'build_ext', '--inplace'],)
kwargs = {}, retcode = 1
cmd = ['/home/wesm/miniconda/envs/arrow-dev/bin/python', 'setup.py', 'build_ext', '--inplace']

    def check_call(*popenargs, **kwargs):
        """"""Run command with arguments.  Wait for command to complete.  If
        the exit code was zero then return, otherwise raise
        CalledProcessError.  The CalledProcessError object will have the
        return code in the returncode attribute.
    
        The arguments are the same as for the call function.  Example:
    
        check_call([""ls"", ""-l""])
        """"""
        retcode = call(*popenargs, **kwargs)
        if retcode:
            cmd = kwargs.get(""args"")
            if cmd is None:
                cmd = popenargs[0]
>           raise CalledProcessError(retcode, cmd)
E           subprocess.CalledProcessError: Command '['/home/wesm/miniconda/envs/arrow-dev/bin/python', 'setup.py', 'build_ext', '--inplace']' returned non-zero exit status 1.

../../../miniconda/envs/arrow-dev/lib/python3.6/subprocess.py:291: CalledProcessError
------------------------------------ Captured stderr call -------------------------------------
Traceback (most recent call last):
  File ""setup.py"", line 7, in <module>
    import pyarrow as pa
ModuleNotFoundError: No module named 'pyarrow'
================================== 1 failed in 0.23 seconds ===================================
{code}

I encountered this bit of brittleness in a fresh install where I had not run {{setup.py develop}} nor {{setup.py install}} on my local pyarrow dev area",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-05 19:46:55,14
13142420,[C++] Appveyor builds failing on master,See https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/build/1.0.5563,pull-request-available,['C++'],ARROW,Bug,Blocker,2018-03-05 04:15:27,8
13142404,[Developer][Integration] Serialize schema- and field-level custom metadata in integration test JSON format,I don't believe we are doing this at present. We should validate that each implementation properly handles the incoming metadata from other Arrow emitters,pull-request-available,"['C++', 'Integration']",ARROW,Bug,Major,2018-03-04 21:47:53,6
13142396,[Python] Local in-place dev versions picking up JS tags,"I thought we had fixed this bug, but it's back:

{code}
$ ipython
Python 3.5.2 | packaged by conda-forge | (default, Jul 26 2016, 01:32:08) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: pa.__version__
Out[1]: '0.3.1.dev52+g8b1c8118'
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-04 20:01:30,8
13142391,[Python]Support __eq__ on scalar values,Support a generic {{__eq__}} method the {{ArrayValue}} class. We might want to specialise it in the future in C++ to avoid some copies but as a first attempt delegate the comparison to the Python types.,pull-request-available,['Python'],ARROW,New Feature,Major,2018-03-04 19:08:16,8
13142362,"[Python]Create buffer from address, size and base","Given a memoryaddress and a size, we should be able to construct an Arrow buffer from this. The additional base object will be used to hold a reference to the underlying, original buffer so that it does not go out of scope before the Arrow buffer.",pull-request-available,['Python'],ARROW,New Feature,Major,2018-03-04 14:53:46,8
13142288,[Java/Python]in-process vector sharing from Java to Python,"Currently we seem to use in all applications of Arrow the IPC capabilities to move data between a Java process and a Python process. While this is 0-serialization, it is not zero-copy. By taking the address and offset, we can already create Python buffers from Java buffers: https://github.com/apache/arrow/pull/1693. This is still a very low-level interface and we should provide the user with:

* A guide on how to load Apache Arrow java libraries in Python (either through a fat-jar that was shipped with Arrow or how he should integrate it into its Java packaging)
* {{pyarrow.Array.from_jvm}}, {{pyarrow.RecordBatch.from_jvm}},  functions that take the respective Java objects and emit Python objects. These Python objects should also ensure that the underlying memory regions are kept alive as long as the Python objects exist.

This issue can also be used as a tracker for the various sub-tasks that will need to be done to complete this rather large milestone.",beginner,"['Java', 'Python']",ARROW,New Feature,Major,2018-03-03 17:08:46,8
13142106,[Python] Revert static linkage of parquet-cpp in manylinux1 wheel,"Although we are not in a theoretical way the authoritative source of parquet-cpp with the pyarrow manylinux1 wheel, in practical way we actually are this and statically linking parquet-cpp can introduce some problems that dynamically linking it does not (e.g. duplicate unloading of the library if you include it in a Python wheel and in the process that creates the Python interpreter).",pull-request-available,['Python'],ARROW,Bug,Major,2018-03-02 14:00:51,8
13141935,[C++] Slicing NullArray should not cause the null count on the internal data to be unknown,see https://github.com/apache/arrow/blob/master/cpp/src/arrow/array.cc#L101,pull-request-available,['C++'],ARROW,Bug,Major,2018-03-01 23:16:31,14
13141820,[C++] Update build docs for Windows,We should update the C++ build docs for Windows to recommend use of Ninja and clcache for faster builds.,pull-request-available,"['C++', 'Documentation']",ARROW,Task,Minor,2018-03-01 15:17:32,2
13141769,[C++] Detect clcache in cmake configuration,By default Windows builds should use clcache if installed.,pull-request-available,['C++'],ARROW,Improvement,Minor,2018-03-01 11:54:22,2
13141496,[CI] Use clcache on AppVeyor,There's a ccache equivalent for Windows named clcache that could be used to speed up AppVeyor builds: https://github.com/frerich/clcache/,CI pull-request-available windows,['Continuous Integration'],ARROW,Improvement,Major,2018-02-28 10:41:49,2
13141297,[Python]JS version number is sometimes picked up,"Depending on the installed git version, command line arguments are parsed differently. Thus in some cases we pick up the non-JS tag, in other cases simply the latest one.",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-27 17:22:46,8
13141269,[Python] Table.from_pandas does not create chunked_arrays.,"When creating a large enough array, pyarrow raises an exception:
{code:java}
import numpy as np
import pandas as pd
import pyarrow as pa

x = list('1' * 2**31)
y = pd.DataFrame({'x': x})
t = pa.Table.from_pandas(y)
# ArrowInvalid: BinaryArrow cannot contain more than 2147483646 bytes, have 2147483647{code}
The array should be chunked for the user. As is, data frames with >2 GiB inbinary data willstruggle to get into arrow.",pull-request-available,['Python'],ARROW,Bug,Blocker,2018-02-27 16:02:50,14
13141074,[C++] Get rid of boost regex usage,We're using {{boost::regex}} to parse decimal strings for {{decimal128}} types. We should use {{libre2}} instead.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-02-26 23:33:10,2
13141012,Change default fix version in merge tool to be the next mainline release version,"Right now, the default fix version is coming up as ""JS-0.4.0"". It would be nice if this could be changed to be 0.9.0 since we have multiple release lines",pull-request-available,['Developer Tools'],ARROW,Bug,Major,2018-02-26 19:16:31,14
13140991,[Python] PythonFile should infer mode when not given,"The following is clearly not optimal:

{code:python}
>>> f = open('README.md', 'r')
>>> pa.PythonFile(f).mode
'wb'
{code}
",pull-request-available,['Python'],ARROW,Improvement,Minor,2018-02-26 17:28:10,2
13140803,[C++/Python]Build Protobuf in base manylinux 1 docker image,This should cut down the build times of the {{manylinux1}} CI matrix entry.,pull-request-available,"['Packaging', 'Python']",ARROW,Bug,Major,2018-02-25 19:42:30,8
13140797,[C++]TestBuffer_ResizeOOM has a memory leak with jemalloc,"Current travis failure:
{code:java}
5: ==21958== 128 bytes in 1 blocks are definitely lost in loss record 1 of 2
5: ==21958==    at 0x6228F9: je_arrow_mallocx (jemalloc.c:2292)
5: ==21958==    by 0x5AA05E: arrow::(anonymous namespace)::AllocateAligned(long, unsigned char**) (memory_pool.cc:58)
5: ==21958==    by 0x5AA7CF: arrow::DefaultMemoryPool::Allocate(long, unsigned char**) (memory_pool.cc:96)
5: ==21958==    by 0x5A04F1: arrow::PoolBuffer::Reserve(long) (buffer.cc:93)
5: ==21958==    by 0x5A07E3: arrow::PoolBuffer::Resize(long, bool) (buffer.cc:104)
5: ==21958==    by 0x571417: arrow::TestBuffer_ResizeOOM_Test::TestBody() (buffer-test.cc:119)
5: ==21958==    by 0x60FE27: void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (gtest.cc:2402)
5: ==21958==    by 0x609E29: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (gtest.cc:2438)
5: ==21958==    by 0x5EF880: testing::Test::Run() (gtest.cc:2475)
5: ==21958==    by 0x5F0127: testing::TestInfo::Run() (gtest.cc:2656)
5: ==21958==    by 0x5F07EB: testing::TestCase::Run() (gtest.cc:2774)
5: ==21958==    by 0x5F71BB: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:4649){code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-02-25 17:56:10,8
13140796,[Python]Partition columns are not correctly loaded in schema of ParquetDataset,Currently the partition columns are not included in the schema of a ParquetDataset. We correctly write them out in the {{_common_metadata}} file but we fail to load this file correctly.,pull-request-available,['Python'],ARROW,Bug,Major,2018-02-25 17:17:47,8
13140488,[C++] Build fails with TLS error on parquet-cpp clone,"{code:java}
Cloning into 'parquet-cpp'...
error: error:1407742E:SSL routines:SSL23_GET_SERVER_HELLO:tlsv1 alert protocol version while accessing https://github.com/apache/parquet-cpp.git/info/refs?service=git-upload-pack
fatal: HTTP request failed
The command '/bin/sh -c git clone https://github.com/apache/parquet-cpp.git' returned a non-zero code: 128{code}",pull-request-available,"['C++', 'Continuous Integration']",ARROW,Improvement,Major,2018-02-23 12:37:44,8
13140249,[Python] Docstring for parquet.read_table is misleading or incorrect,"See https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L872

One should be able to pass a Python file object directly. The docstring suggests otherwise",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-22 16:58:13,14
13140211,"Document ""undefined symbol"" issue and workaround",See [https://github.com/apache/arrow/issues/1612],pull-request-available,['Documentation'],ARROW,Task,Trivial,2018-02-22 14:42:36,2
13139982,[Plasma] Segfault when retrieving RecordBatch from plasma store,"It can be reproduced with the following script:

{code:python}
import pyarrow as pa
import pyarrow.plasma as plasma

def retrieve1():
    client = plasma.connect('test', """", 0)

    key = ""keynumber1keynumber1""
    pid = plasma.ObjectID(bytearray(key,'UTF-8'))

    [buff] = client .get_buffers([pid])
    batch = pa.RecordBatchStreamReader(buff).read_next_batch()

    print(batch)
    print(batch.schema)
    print(batch[0])

    return batch

client = plasma.connect('test', """", 0)

test1 = [1, 12, 23, 3, 21, 34]
test1 = pa.array(test1, pa.int32())

batch = pa.RecordBatch.from_arrays([test1], ['FIELD1'])

key = ""keynumber1keynumber1""
pid = plasma.ObjectID(bytearray(key,'UTF-8'))
sink = pa.MockOutputStream()
stream_writer = pa.RecordBatchStreamWriter(sink, batch.schema)
stream_writer.write_batch(batch)
stream_writer.close()

bff = client.create(pid, sink.size())

stream = pa.FixedSizeBufferWriter(bff)
writer = pa.RecordBatchStreamWriter(stream, batch.schema)
writer.write_batch(batch)
client.seal(pid)

batch = retrieve1()
print(batch)
print(batch.schema)
print(batch[0])
{code}


Preliminary backtrace:



{code}

CESS (code=1, address=0x111138158)

 frame #0: 0x000000010e6457fc lib.so`__pyx_pw_7pyarrow_3lib_10Int32Value_1as_py(_object*, _object*) + 28

lib.so`__pyx_pw_7pyarrow_3lib_10Int32Value_1as_py:

->0x10e6457fc <+28>: movslq (%rdx,%rcx,4), %rdi

 0x10e645800 <+32>: callq0x10e698170      ; symbol stub for: PyInt_FromLong

 0x10e645805 <+37>: testq%rax, %rax

 0x10e645808 <+40>: je 0x10e64580c      ; <+44>

(lldb)bt
 * thread #1: tid = 0xf1378e, 0x000000010e6457fc lib.so`__pyx_pw_7pyarrow_3lib_10Int32Value_1as_py(_object*, _object*) + 28, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x111138158)

* frame #0: 0x000000010e6457fc lib.so`__pyx_pw_7pyarrow_3lib_10Int32Value_1as_py(_object*, _object*) + 28

 frame #1: 0x000000010e5ccd35 lib.so`__Pyx_PyObject_CallNoArg(_object*) + 133

 frame #2: 0x000000010e613b25 lib.so`__pyx_pw_7pyarrow_3lib_10ArrayValue_3__repr__(_object*) + 933

 frame #3: 0x000000010c2f83bc libpython2.7.dylib`PyObject_Repr + 60

 frame #4: 0x000000010c35f651 libpython2.7.dylib`PyEval_EvalFrameEx + 22305

{code}",pull-request-available,"['C++ - Plasma', 'Python']",ARROW,Bug,Major,2018-02-21 19:11:27,2
13139892,[Plasma] plasma_store has runtime dependency on Boost shared libraries when ARROW_BOOST_USE_SHARED=on,"I'm not sure why, but when I run the pyarrow test suite (for example {{py.test pyarrow/tests/test_plasma.py}}), plasma_store forks endlessly:

{code:bash}
$ ps fuwww
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
[...]
antoine  27869 12.0  0.4 863208 68976 pts/7    S    13:41   0:01 /home/antoine/miniconda3/envs/pyarrow/bin/python /home/antoine/arrow/python/pyarrow/plasma_store -s /tmp/plasma_store40209423 -m 100000000
antoine  27885 13.0  0.4 863076 68560 pts/7    S    13:41   0:01  \_ /home/antoine/miniconda3/envs/pyarrow/bin/python /home/antoine/arrow/python/pyarrow/plasma_store -s /tmp/plasma_store40209423 -m 100000000
antoine  27901 12.1  0.4 863076 68320 pts/7    S    13:41   0:01      \_ /home/antoine/miniconda3/envs/pyarrow/bin/python /home/antoine/arrow/python/pyarrow/plasma_store -s /tmp/plasma_store40209423 -m 100000000
antoine  27920 13.6  0.4 863208 68868 pts/7    S    13:41   0:01          \_ /home/antoine/miniconda3/envs/pyarrow/bin/python /home/antoine/arrow/python/pyarrow/plasma_store -s /tmp/plasma_store40209423 -m 100000000
[etc.]
{code}
",pull-request-available,['C++ - Plasma'],ARROW,Bug,Major,2018-02-21 12:44:32,2
13139805,Commits to master should run all builds in CI matrix,"After ARROW-2083, we are only running builds related to changed components with each patch in Travis CI and Appveyor. 

The problem with this is that when we merge patches to master, our Travis CI configuration (implemented by ASF infra to help alleviate clogged up build queues) is set up to cancel in-progress builds whenever a new commit is merged.

So basically we could have in our timeline:

* Patch merged affecting C++, Python
* Patch merged affecting Java
* Patch merged affecting JS

So when the Java patch is merged, any in-progress C++/Python builds will be cancelled. And if the JS patch comes in, the Java builds would be immediately cancelled.

In light of this I believe on master branch we should always run all of the builds unconditionally",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2018-02-21 01:41:54,2
13139761,[C++]Only use specific version of jemalloc,"As we want to avoid conflicts with system copies of jemalloc that are either outdated or are incompatible (known bugs, known shortcomings, or don't compile)with older compilers, we want to only use our own prefixed version of jemalloc.",pull-request-available,['C++'],ARROW,Improvement,Major,2018-02-20 20:52:19,8
13139555,Remove CI directives from squashed commit messages,"In our PR squash tool, we are potentially picking up CI directives like {{[skip appveyor]}} from intermediate commits. We should regex these away and instead use directives in the PR title if we wish the commit to master to behave in a certain way",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-02-20 02:11:02,14
13139539,[Python] ASV benchmark setup does not account for C++ library changing,"See https://github.com/apache/arrow/blob/master/python/README-benchmarks.md

Perhaps we could create a helper script that will run all the currently-defined benchmarks for a specific commit, and ensure that we are running against pristine, up-to-date release builds of Arrow (and any other dependencies, like parquet-cpp) at that commit? 

cc [~pitrou]",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-19 23:05:39,2
13139522,[C++] arrow/util/io-util.h missing from libarrow-dev,"{{arrow/util/io-util.h}} is missing from the {{libarow-dev}} package (ubuntu/trusty): 
{code:java}
> ls -1 /usr/include/arrow/util/
bit-stream-utils.h
bit-util.h
bpacking.h
compiler-util.h
compression.h
compression_brotli.h
compression_lz4.h
compression_snappy.h
compression_zlib.h
compression_zstd.h
cpu-info.h
decimal.h
hash-util.h
hash.h
key_value_metadata.h
logging.h
macros.h
parallel.h
rle-encoding.h
sse-util.h
stl.h
type_traits.h
variant
variant.h
visibility.h
{code}

{code:java}
> apt-cache show libarrow-dev
Package: libarrow-dev
Architecture: amd64
Version: 0.8.0-2
Multi-Arch: same
Priority: optional
Section: libdevel
Source: apache-arrow
Maintainer: Kouhei Sutou <kou@clear-code.com>
Installed-Size: 5696
Depends: libarrow0 (= 0.8.0-2)
Filename: pool/trusty/universe/a/apache-arrow/libarrow-dev_0.8.0-2_amd64.deb
Size: 602716
MD5sum: de5f2bfafd90ff29e4b192f4e5d26605
SHA1: e3d9146b30f07c07b62f8bdf9f779d0ee5d05a75
SHA256: 30a89b2ac6845998f22434e660b1a7c9d91dc8b2ba947e1f4333b3cf74c69982
SHA512: 99f511bee6645a68708848a58b4eba669a2ec8c45fb411c56ed2c920d3ff34552c77821eff7e428c886d16e450bdd25cc4e67597972f77a4255f302a56d1eac8
Homepage: https://arrow.apache.org/
Description: Apache Arrow is a data processing library for analysis
 .
 This package provides header files.
Description-md5: e4855d5dbadacb872bf8c4ca67f624e3
{code}
",pull-request-available,['C++'],ARROW,Bug,Minor,2018-02-19 21:04:01,14
13139470,[Python] arrow_ep build is triggering during parquet-cpp build in Travis CI,see e.g. https://travis-ci.org/apache/arrow/jobs/342781531#L5546. This may be related to upstream changes in Parquet,pull-request-available,['Python'],ARROW,Bug,Major,2018-02-19 15:53:17,14
13139411,[Python] NumPyBuffer destructor should hold the GIL,"Failure to hold the GIL can lead to crashes, depending on presence of several threads or whatever the object allocator needs to do.",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-19 12:09:15,2
13139410,[Python] Incorrect conversion from Numpy array when stride % itemsize != 0,"In the example below, the input array has a stride that's not a multiple of the itemsize:

{code:python}
>>> data = np.array([(42, True), (43, False)],
...:                dtype=[('x', np.int32), ('y', np.bool_)])
...:                
...:                                        
>>> data['x']
array([42, 43], dtype=int32)
>>> pa.array(data['x'], type=pa.int32())
<pyarrow.lib.Int32Array object at 0x7ff60a8415e8>
[
  42,
  11009
]
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-19 12:05:21,2
13139409,[Python] OwnedRef is fragile,"Some uses of OwnedRef can implicitly invoke its (default) copy constructor, which will lead to extraneous decrefs.",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-19 12:00:12,2
13139310,[Python] construct_metadata fails on reading files where no index was preserved,The current master will then just return an empty DataFrame.,pull-request-available,['Python'],ARROW,Bug,Major,2018-02-18 17:14:09,8
13139250,[C++] Build toolchain builds with jemalloc,We have fixed all known problems in the jemalloc 4.x branch and should be able to gradually reactivate it in our builds to get its performance boost.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-02-17 16:33:35,8
13138795,"Install apt dependencies separate from built-in Travis commands, retry on flakiness","This would also allow us to run the detect changes script earlier than installing apt dependencies, so unnecessary builds will terminate faster",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-02-15 15:44:31,2
13138520,[CI] Isolate Sphinx dependencies,"In the Travis Python test script, we always install the documentation dependencies. We should only install them when building the docs, since they are not trivial and may take time fetching.",pull-request-available,['Continuous Integration'],ARROW,Task,Trivial,2018-02-14 15:32:15,2
13138493,[Python] pa.frombuffer(bytearray) returns immutable Buffer,"I'd expect it to return a mutable buffer:
{code:python}
>>> pa.frombuffer(bytearray(10)).is_mutable
False
{code}",pull-request-available,['Python'],ARROW,Improvement,Minor,2018-02-14 14:00:33,2
13138491,[Python] __eq__ unimplemented on Buffer,"Having to call {{equals()}} is un-Pythonic:
{code:python}
>>> pa.frombuffer(b'foo') == pa.frombuffer(b'foo')
False
>>> pa.frombuffer(b'foo').equals(pa.frombuffer(b'foo'))
True
{code}

Same for many other pyarrow types, incidently.",pull-request-available,['Python'],ARROW,Improvement,Minor,2018-02-14 13:44:12,2
13138182,[Python] Error when converting from list of uint64 arrays,"{code:python}
>>> pa.array(np.uint64([0,1,2]), type=pa.uint64())
<pyarrow.lib.UInt64Array object at 0x7f0e6c1eab88>
[
  0,
  1,
  2
]
>>> pa.array([np.uint64([0,1,2])], type=pa.list_(pa.uint64()))
Traceback (most recent call last):
  File ""<ipython-input-70-1b103d3e4574>"", line 1, in <module>
    pa.array([np.uint64([0,1,2])], type=pa.list_(pa.uint64()))
  File ""array.pxi"", line 181, in pyarrow.lib.array
  File ""array.pxi"", line 36, in pyarrow.lib._sequence_to_array
  File ""error.pxi"", line 98, in pyarrow.lib.check_status
ArrowException: Unknown error: /home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:979 code: AppendPySequence(seq, size, real_type, builder.get())
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:402 code: static_cast<Derived*>(this)->AppendSingle(ref.obj())
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:402 code: static_cast<Derived*>(this)->AppendSingle(ref.obj())
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:542 code: CheckPyError()
an integer is required
{code}

More simply, it also fails with a sequence of Numpy uint64 scalars:
{code:python}
>>> pa.array([np.uint64(1)], type=pa.uint64())
Traceback (most recent call last):
  File ""<ipython-input-71-679cb6c34137>"", line 1, in <module>
    pa.array([np.uint64(1)], type=pa.uint64())
  File ""array.pxi"", line 181, in pyarrow.lib.array
  File ""array.pxi"", line 36, in pyarrow.lib._sequence_to_array
  File ""error.pxi"", line 98, in pyarrow.lib.check_status
ArrowException: Unknown error: /home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:979 code: AppendPySequence(seq, size, real_type, builder.get())
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:402 code: static_cast<Derived*>(this)->AppendSingle(ref.obj())
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:542 code: CheckPyError()
an integer is required
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-02-13 14:13:17,2
13138178,[Python] array equality defaults to identity,"I'm not sure this is deliberate, but it doesn't look very desirable to me:
{code}
>>> pa.array([1,2,3], type=pa.int32()) == pa.array([1,2,3], type=pa.int32())
False
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-02-13 13:48:36,14
13138170,[Python] reorganize test_convert_pandas.py,{{test_convert_pandas.py}} is getting painful to navigate through. We should reorganize the tests in various classes / categories.,pull-request-available,['Python'],ARROW,Task,Trivial,2018-02-13 13:29:37,2
13138161,[Python] Type inference doesn't work on lists of Numpy arrays,"{code:python}
>>> arr = np.int16([2, 3, 4])
>>> pa.array(arr)
<pyarrow.lib.Int16Array object at 0x7f939f30a0e8>
[
 2,
 3,
 4
]
>>> pa.array([arr])
Traceback (most recent call last):
 File ""<ipython-input-6-254285212203>"", line 1, in <module>
 pa.array([arr])
 File ""array.pxi"", line 181, in pyarrow.lib.array
 File ""array.pxi"", line 26, in pyarrow.lib._sequence_to_array
 File ""error.pxi"", line 77, in pyarrow.lib.check_status
ArrowInvalid: /home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:964 code: InferArrowType(seq, &real_type)
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:321 code: seq_visitor.Visit(obj)
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:195 code: VisitElem(ref, level)
Error inferring Arrow data type for collection of Python objects. Got Python object of type ndarray but can only handle these types: bool, float, integer, date, datetime, bytes, unicode
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-13 12:51:51,8
13137935,[Python]Provide a manylinux1 wheel for cp27m,"Currently we only provide it forcp27mu, we should also build them forcp27m",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-12 18:47:26,8
13137931,[Python] Conversion from Numpy struct array unimplemented,"{code:python}
>>> arr = np.array([(1.5,)], dtype=np.dtype([('x', np.float32)]))
>>> arr
array([(1.5,)], dtype=[('x', '<f4')])
>>> arr[0]
(1.5,)
>>> arr['x']
array([1.5], dtype=float32)
>>> arr['x'][0]
1.5
>>> pa.array(arr, type=pa.struct([pa.field('x', pa.float32())]))
Traceback (most recent call last):
 File ""<ipython-input-18-27a52820b7d8>"", line 1, in <module>
 pa.array(arr, type=pa.struct([pa.field('x', pa.float32())]))
 File ""array.pxi"", line 177, in pyarrow.lib.array
 File ""error.pxi"", line 77, in pyarrow.lib.check_status
 File ""error.pxi"", line 85, in pyarrow.lib.check_status
ArrowNotImplementedError: /home/antoine/arrow/cpp/src/arrow/python/numpy_to_arrow.cc:1585 code: converter.Convert()
NumPyConverter doesn't implement <struct<x: float>> conversion.
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-12 18:34:14,2
13137926,[Python] Conversion from Numpy float16 array unimplemented,"{code}
>>> arr = np.array([1.5], dtype=np.float16)
>>> pa.array(arr, type=pa.float16())
Traceback (most recent call last):
  File ""<ipython-input-6-e432e6663efb>"", line 1, in <module>
    pa.array(arr)
  File ""array.pxi"", line 177, in pyarrow.lib.array
  File ""array.pxi"", line 84, in pyarrow.lib._ndarray_to_array
  File ""public-api.pxi"", line 158, in pyarrow.lib.pyarrow_wrap_array
KeyError: 10
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-12 18:27:10,2
13137895,[C++] Have FatalLog abort instead of exiting,"Not sure this is desirable, since {{util/logging.h}} was taken from glog, but the various debug checks current {{std::exit(1)}} on failure. This is a clean exit (though with an error code) and therefore doesn't trigger the usual debugging tools such as gdb or Python's faulthandler. By replacing it with something like {{std::abort()}} the exit would be recognized as a process crash.



Thoughts?",pull-request-available,['C++'],ARROW,Improvement,Trivial,2018-02-12 16:06:06,2
13137868,[Python] Non-nullable schema fields not checked in conversions from pandas,"If you provide a schema with{{nullable=False}}butpass a{{DataFrame}}which in fact has nullsit appears theschema is ignored? I would expect an error here.
{code}
import pyarrow as pa
import pandas as pd

df = pd.DataFrame({""a"":[1.2, 2.1, pd.np.NaN]})
schema = pa.schema([pa.field(""a"", pa.float64(), nullable=False)])
table = pa.Table.from_pandas(df, schema=schema)
table[0]

<pyarrow.lib.Column object at 0x7f213bf2fb70>
chunk 0: <pyarrow.lib.DoubleArray object at 0x7f213bf20ea8>
[
  1.2,
  2.1,
  NA
]
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-12 14:04:36,14
13137866,[Python] NaN values silently casted to int64 when passing explicit schema for conversion in Table.from_pandas,"If you create a{{Table}}from a{{DataFrame}}of ints with a NaN value the NaN is improperly cast. Since pandas casts these to floats, when converted to a table the NaN is interpreted as an integer. This seemslike a bug since a known limitation in pandas (the inability to have null valued integers data) is taking precedence over arrow's functionality to store these as an IntArray with nulls.


{code}
import pyarrow as pa
import pandas as pd

df = pd.DataFrame({""a"":[1, 2, pd.np.NaN]})
schema = pa.schema([pa.field(""a"", pa.int64(), nullable=True)])
table = pa.Table.from_pandas(df, schema=schema)
table[0]


<pyarrow.lib.Column object at 0x7f2151d19c90>
chunk 0: <pyarrow.lib.Int64Array object at 0x7f213bf356d8>
[
  1,
  2,
  -9223372036854775808
]{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-12 14:01:31,2
13137858,[CI] Make Travis commit inspection more robust,See [https://github.com/apache/arrow/pull/1586#issuecomment-364857558],pull-request-available,['Continuous Integration'],ARROW,Task,Minor,2018-02-12 13:13:08,2
13137745,[Python]Segmentation fault on conversion of empty nested arrays to Pandas,"After resolving https://issues.apache.org/jira/browse/ARROW-2129,we still cannot convert empty nested arrays to Pandas. It fails with the following backtrace:
{code:java}
(gdb) bt
#0 PyErr_Restore (type=type@entry=0x0, value=value@entry=0x0, traceback=traceback@entry=0x0) at Python/errors.c:42
#1 0x00007ffff7a1c48b in PyErr_Clear () at Python/errors.c:355
#2 0x00007ffff7959320 in PyDict_GetItem (op=0x7fffe831df78, key=key@entry=0x7fffec8873f0) at Objects/dictobject.c:1417
#3 0x00007ffff0278e89 in _buffer_clear_info (arr=0x7fffe37f52b0)
from /home/uwe/miniconda3/envs/pyarrow-dev/lib/python3.6/site-packages/numpy/core/multiarray.cpython-36m-x86_64-linux-gnu.so
#4 _array_dealloc_buffer_info (self=self@entry=0x7fffe37f52b0) at numpy/core/src/multiarray/buffer.c:776
#5 0x00007ffff022b0de in array_dealloc (self=0x7fffe37f52b0)
from /home/uwe/miniconda3/envs/pyarrow-dev/lib/python3.6/site-packages/numpy/core/multiarray.cpython-36m-x86_64-linux-gnu.so
#6 0x00007ffff172af72 in arrow::py::OwnedRef::reset (obj=0x0, this=0x7fffffff3760) at ../src/arrow/python/common.h:75
#7 arrow::py::OwnedRef::reset (this=0x7fffffff3760) at ../src/arrow/python/common.h:79
#8 0x00007ffff174220e in arrow::py::OwnedRef::~OwnedRef (this=0x7fffffff3760, __in_chrg=<optimized out>) at ../src/arrow/python/common.h:72
#9 arrow::py::ConvertListsLike<arrow::Int64Type> (options=..., col=std::shared_ptr (count 2, weak 0) 0x10b0ca0, out_values=<optimized out>,
out_values@entry=0xf793d0) at ../src/arrow/python/arrow_to_pandas.cc:522
#10 0x00007ffff173d833 in arrow::py::ArrowDeserializer::Visit (this=this@entry=0x7fffffff3aa0, type=...) at ../src/arrow/python/arrow_to_pandas.cc:1708
#11 0x00007ffff173dc0b in arrow::VisitTypeInline<arrow::py::ArrowDeserializer> (type=..., visitor=visitor@entry=0x7fffffff3aa0)
at ../src/arrow/visitor_inline.h:59
#12 0x00007ffff1728063 in arrow::py::ArrowDeserializer::Convert (out=0x7fffffff3ba8, this=0x7fffffff3aa0) at ../src/arrow/python/arrow_to_pandas.cc:1744
#13 arrow::py::ConvertColumnToPandas (options=..., options@entry=..., col=std::shared_ptr (count 2, weak 0) 0x10b0ca0, py_ref=py_ref@entry=0x7fffe37d4ea8,
out=out@entry=0x7fffffff3ba8) at ../src/arrow/python/arrow_to_pandas.cc:1769
#14 0x00007ffff17282e6 in arrow::py::ConvertArrayToPandas (options=..., arr=std::shared_ptr (count 2, weak 0) 0xfe9960, py_ref=0x7fffe37d4ea8,
out=0x7fffffff3ba8) at ../src/arrow/python/arrow_to_pandas.cc:1763
#15 0x00007ffff211b594 in __pyx_pw_7pyarrow_3lib_5Array_26to_pandas(_object*, _object*, _object*) ()
from /home/uwe/Development/arrow-repos-1/arrow/python/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so
#16 0x00007ffff7967ded in _PyCFunction_FastCallDict (func_obj=func_obj@entry=0x7ffff62921b0, args=args@entry=0x11d2bd8, nargs=<optimized out>,
kwargs=kwargs@entry=0x0) at Objects/methodobject.c:231{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-11 21:30:08,2
13137744,[Doc] Add links / mentions of Plasma store to main README,"This should be listed as separate from, but noted as a part of, the C++ implementation",pull-request-available,['Website'],ARROW,Improvement,Major,2018-02-11 21:28:43,14
13137742,[Python] Serialization test fails on Windows when library has been built in place / not installed,"I am not sure why this doesn't come up in Appveyor:

{code}
================================== FAILURES ===================================
________________ test_deserialize_buffer_in_different_process _________________

    def test_deserialize_buffer_in_different_process():
        import tempfile
        import subprocess

        f = tempfile.NamedTemporaryFile(delete=False)
        b = pa.serialize(pa.frombuffer(b'hello')).to_buffer()
        f.write(b.to_pybytes())
        f.close()

        dir_path = os.path.dirname(os.path.realpath(__file__))
        python_file = os.path.join(dir_path, 'deserialize_buffer.py')
>       subprocess.check_call([sys.executable, python_file, f.name])

pyarrow\tests\test_serialization.py:596:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

popenargs = (['C:\\Miniconda3\\envs\\pyarrow-dev\\python.exe', 'C:\\Users\\wesm\\code\\arrow\\python\\pyarrow\\tests\\deserialize_buffer.py', 'C:\\Users\\wesm\\AppData\\Local\\Temp\\tmp1gi__att'],)
kwargs = {}, retcode = 1
cmd = ['C:\\Miniconda3\\envs\\pyarrow-dev\\python.exe', 'C:\\Users\\wesm\\code\\arrow\\python\\pyarrow\\tests\\deserialize_buffer.py', 'C:\\Users\\wesm\\AppData\\Local\\Temp\\tmp1gi__att']

    def check_call(*popenargs, **kwargs):
        """"""Run command with arguments.  Wait for command to complete.  If
        the exit code was zero then return, otherwise raise
        CalledProcessError.  The CalledProcessError object will have the
        return code in the returncode attribute.

        The arguments are the same as for the call function.  Example:

        check_call([""ls"", ""-l""])
        """"""
        retcode = call(*popenargs, **kwargs)
        if retcode:
            cmd = kwargs.get(""args"")
            if cmd is None:
                cmd = popenargs[0]
>           raise CalledProcessError(retcode, cmd)
E           subprocess.CalledProcessError: Command '['C:\\Miniconda3\\envs\\pyarrow-dev\\python.exe', 'C:\\Users\\wesm\\code\\arrow\\python\\pyarrow\\tests\\deserialize_buffer.py', 'C:\\Users\\wesm\\AppData\\Local\\Temp\\tmp1gi__att']' returned non-zero exit status 1.

C:\Miniconda3\envs\pyarrow-dev\lib\subprocess.py:291: CalledProcessError
---------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File ""C:\Users\wesm\code\arrow\python\pyarrow\tests\deserialize_buffer.py"", line 22, in <module>
    import pyarrow as pa
ModuleNotFoundError: No module named 'pyarrow'
=============== 1 failed, 15 passed, 4 skipped in 0.40 seconds ================
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-11 21:07:54,14
13137727,[Python]Segmentation fault on conversion of empty array to Pandas,Converting an empty {{pyarrow.Array}} to a Pandas series causes a segmentation fault.,pull-request-available,['Python'],ARROW,Bug,Major,2018-02-11 17:57:02,8
13137722,[Python] Cannot serialize array of empty lists,"This currently failing:
{code:java}
data = pd.Series([[], [], []])
arr = pa.array(data)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
array.pxi:181: in pyarrow.lib.array
???
array.pxi:26: in pyarrow.lib._sequence_to_array
???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

> ???
E pyarrow.lib.ArrowTypeError: Unable to determine data type

{code}
The code in {{SeqVisitor::GetType}}suggests that we don't want to support thus but I would have expected that the above should result in {{List<Null>}}.",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-11 17:03:43,8
13137617,[Python] ArrowInvalid raised if the first item of a nested list of numpy arrays is empty,"See example below:
{noformat}
In [1]: import numpy as np

In [2]: import pandas as pd

In [3]: import pyarrow as pa

In [4]: num_lists = [[2,3,4], [3,6,7,8], [], [2]]

In [5]: series = pd.Series([np.array(s, dtype=float) for s in num_lists])

In [6]: pa.array(series)
Out[6]: 
<pyarrow.lib.ListArray object at 0x7f0db8ad1688>
[
  [2.0,
   3.0,
   4.0],
  [3.0,
   6.0,
   7.0,
   8.0],
  [],
  [2.0]
]

In [7]: num_lists.append([])

In [8]: series = pd.Series([np.array(s, dtype=float) for s in num_lists])

In [9]: pa.array(series)
Out[9]: 
<pyarrow.lib.ListArray object at 0x7f0db8ad1e58>
[
  [2.0,
   3.0,
   4.0],
  [3.0,
   6.0,
   7.0,
   8.0],
  [],
  [2.0],
  []
]

In [10]: num_lists.insert(0, [])

In [11]: series = pd.Series([np.array(s, dtype=float) for s in num_lists])

In [12]: pa.array(series)
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-99-fc3a903278e6> in <module>()
----> 1 pa.array(series)

array.pxi in pyarrow.lib.array()

array.pxi in pyarrow.lib._ndarray_to_array()

error.pxi in pyarrow.lib.check_status()

ArrowInvalid: trying to convert NumPy type object but got float64
{noformat}",pull-request-available,['Python'],ARROW,Bug,Major,2018-02-10 11:09:40,8
13137294,[C++][Java] Handle Arrow stream with zero record batch,"It looks like currently many places of the code assume that there needs tobe at least one record batch for streaming format. Is zero-recordbatch not supported by design?

e.g. [https://github.com/apache/arrow/blob/master/java/tools/src/main/java/org/apache/arrow/tools/StreamToFile.java#L45]
{code:none}
  public static void convert(InputStream in, OutputStream out) throws IOException {
    BufferAllocator allocator = new RootAllocator(Integer.MAX_VALUE);
    try (ArrowStreamReader reader = new ArrowStreamReader(in, allocator)) {
      VectorSchemaRoot root = reader.getVectorSchemaRoot();
      // load the first batch before instantiating the writer so that we have any dictionaries
      if (!reader.loadNextBatch()) {
        throw new IOException(""Unable to read first record batch"");
      }
      ...
{code}
Pyarrow-0.8.0 does not load 0-recordbatch stream either. It would throw an exception originated from [https://github.com/apache/arrow/blob/a95465b8ce7a32feeaae3e13d0a64102ffa590d9/cpp/src/arrow/table.cc#L309:]
{code:none}
Status Table::FromRecordBatches(const std::vector<std::shared_ptr<RecordBatch>>& batches,
                                std::shared_ptr<Table>* table) {
  if (batches.size() == 0) {
    return Status::Invalid(""Must pass at least one record batch"");
  }
  ...{code}",pull-request-available,"['C++', 'Java']",ARROW,Bug,Major,2018-02-08 23:18:28,14
13137279,[Python] Improve error message when calling parquet.read_table on an empty file,Currently it raises an exception about memory mapping failing,pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-08 22:24:47,14
13137266,[C++] Pin clang to version 5.0,Let's do this after the next release.,pull-request-available,['C++'],ARROW,Improvement,Major,2018-02-08 21:39:53,14
13136908,[C++] Linting could be faster,Currently {{make lint}} style-checks C++ files sequentially (by calling {{cpplint}}). We could instead style-check those files in parallel.,pull-request-available,['C++'],ARROW,Improvement,Trivial,2018-02-07 17:26:48,2
13136895,[Python] Only require pytest-runner on test commands,"We only require it for tests, otherwise we should not depend on it.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-07 16:34:11,8
13136782,[Python] Update instructions for ASV,"Now that PR [https://github.com/airspeed-velocity/asv/pull/611] has been merged, we don't need to advertise our fork anymore.",pull-request-available,['Python'],ARROW,Task,Trivial,2018-02-07 08:42:42,2
13136714,[Python] pyarrow.array can't take a pandas Series of python datetime objects.,"{{>import pyarrow}}
 > from datetime import datetime
 > import pandas
 > dt = pandas.Series([datetime(2017, 12, 1), datetime(2017, 12, 3), datetime(2017, 12, 15)], dtype=object)
 > pyarrow.array(dt, from_pandas=True)

Raises following:

---------------------------------------------------------------------------
 ArrowInvalid Traceback (most recent call last)
 <ipython-input-8-0d49f7fc5c49> in <module>()
 ----> 1 pyarrow.array(dt, from_pandas=True)

array.pxi in pyarrow.lib.array()

array.pxi in pyarrow.lib._ndarray_to_array()

error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Error inferring Arrow type for Python object array. Got Python object of type datetime but can only handle these types: string, bool, float, int, date, time, decimal, list, array

As far as I can tell, the issue seems to be the call to PyDate_CheckExact here (instead of using PyDate_Check):

[https://github.com/apache/arrow/blob/3098c1411930259070efb571fb350304b18ddc70/cpp/src/arrow/python/numpy_to_arrow.cc#L1005]",pull-request-available,['Python'],ARROW,Bug,Minor,2018-02-07 00:51:13,8
13136681,[C++] Implement take kernel functions - nested array value type,Should support nested array value types.,pull-request-available,['C++'],ARROW,Sub-task,Major,2018-02-06 21:58:49,6
13136678,[C++] Implement take kernel functions - primitive value type,Should implement the basic functionality of take kernel and support primitive value types.,pull-request-available,['C++'],ARROW,Sub-task,Major,2018-02-06 21:56:50,6
13136612,[Python] Drop Python 3.4 support,"conda-forge has already dropped it, Pandas dropped it in 0.21, we should also think of dropping support for it.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-06 17:04:00,2
13136605,[Python] Support DictionaryArray::FromArrays in Python bindings,Follow up work from ARROW-1757.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-06 16:18:35,14
13136419,[Python] Suppress valgrind stdout/stderr in Travis CI builds when there are no errors,"See https://travis-ci.org/apache/arrow/jobs/337777265#L7858. It might be nice to have an environment variable so that this can be toggled on or off, for debugging purposes. See also ARROW-1380",pull-request-available,"['Continuous Integration', 'Python']",ARROW,Improvement,Major,2018-02-05 23:19:36,2
13136417,[C++] Suppress ORC EP build logging by default,See build logs: https://travis-ci.org/apache/arrow/jobs/337777265#L9569. This logging should be made equivalent to other EP builds (see e.g. the protobuf build preceding ORC),pull-request-available,['C++'],ARROW,Improvement,Major,2018-02-05 23:13:39,2
13136415,[Python] Use toolchain libraries and PROTOBUF_HOME for protocol buffers,"This is being built from source in Travis CI at the moment; using a toolchain build could help with build times

Speaking of which, libprotobuf could use some TLC in conda-forge -- I ran out of bandwidth to do this myself: https://github.com/conda-forge/staged-recipes/pull/3087. [~Max Risuhin] do you have time to look into adding a C++-only conda-forge package?

cc [~jim.crist]",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-05 23:11:06,14
13136414,[Python] Possibly do not test pytorch serialization in Travis CI,"I am not sure it is worth downloading ~400MB in binaries

{code}
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    libgcc-5.2.0               |                0         1.1 MB  defaults
    pillow-5.0.0               |           py27_0         958 KB  conda-forge
    libtiff-4.0.9              |                0         511 KB  conda-forge
    libtorch-0.1.12            |          nomkl_0         1.7 MB  defaults
    olefile-0.44               |           py27_0          50 KB  conda-forge
    torchvision-0.1.9          |   py27hdb88a65_1          86 KB  soumith
    openblas-0.2.19            |                2        14.1 MB  conda-forge
    numpy-1.13.1               |py27_blas_openblas_200         8.4 MB  conda-forge
    pytorch-0.2.0              |py27ha262b23_4cu75       312.2 MB  soumith
    mkl-2017.0.3               |                0       129.5 MB  defaults
    ------------------------------------------------------------
                                           Total:       468.6 MB
{code}

Follow up from ARROW-2071 https://github.com/apache/arrow/pull/1561",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-05 22:57:42,14
13136144,[GLib] Rename to GARROW_TYPE_BOOLEAN for consistency,"Array name and data type name use ""boolean"" not ""bool"".

GArrowType only uses ""bool"".",pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-02-04 15:04:30,1
13136141,[GLib] Add GArrowNumericArray,It's useful to determine whether an array is a numeric array or not.,pull-request-available,['GLib'],ARROW,Improvement,Minor,2018-02-04 14:42:56,1
13136128,[Python]Binaries of 3rdparty are not stripped in manylinux1 base image,"CMake pip package: [https://github.com/scikit-build/cmake-python-distributions/issues/32]

Pandas pip package: [https://github.com/pandas-dev/pandas/issues/19531]

NumPy pip package:https://github.com/numpy/numpy/issues/10519",pull-request-available,"['Packaging', 'Python']",ARROW,Improvement,Major,2018-02-04 11:34:29,8
13135828,Support skipping builds,"While appveyor supports a [skip appveyor] you cannot skip only travis. What is the feeling about adding e.g. [https://github.com/travis-ci/travis-ci/issues/5032#issuecomment-273626567]to our build. We could also do some simple kind of change detection that we don't build the C++/Python parts and only Java and the integration tests if there was a change in the PR that only affects Java.

I think it might be worthwhile to spend a bit on that to get a bit of load of the CI infrastructure.",pull-request-available,['Continuous Integration'],ARROW,Improvement,Major,2018-02-02 18:10:04,2
13135523,[Python] Document on how to use Storefact & Arrow to read Parquet from S3/Azure/...,"We're using this happily in production, also withcolumn projection down to the storage layer. Others should also benefit from this.",dataset-parquet-read parquet,['Python'],ARROW,Improvement,Major,2018-02-01 18:30:53,8
13135465,[Python] Allow type inference for struct arrays,"Support inferring a struct type in a {{pa.array}} call, if a sequence of dicts (or dict of sequences?= is given. Of course, this could mean that the wrong field order may be inferred, though on Python 3.6+ dicts retain ordering until the first deletion.",pull-request-available,['Python'],ARROW,Improvement,Minor,2018-02-01 15:41:58,2
13135463,[Python] Create StructArray from sequence of tuples given a known data type,"Following ARROW-1705, we should support calling {{pa.array}} with a sequence of tuples, presuming a struct type is passed for the {{type}} parameter.

We also probably want to disallow mixed inputs, e.g. a sequence of both dicts and tuples. The user should use only one idiom at a time.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-02-01 15:37:57,2
13135338,[Python] decimal128.byte_width crashes,"{code:bash}
$ python -c ""import pyarrow as pa; ty = pa.decimal128(20, 7); print(ty.byte_width)""
Erreur de segmentation (core dumped)
{code}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-02-01 07:48:26,2
13135232,[Python] Reduce runtime of builds in Travis CI,"For some reason, recently each Python build has been taking about 15 minutes to run. I speculate this is due to VM thrashing caused by reduced resources on the Travis CI workers, related to the problem I fixed in ARROW-2062.

We should experiment, but it seems like perhaps this can be fixed either by:

* Reducing the size of the Plasma store on Travis CI
* Disabling valgrind in Plasma tests

The slowness could be caused by something else, though, so we should investigate (and have pytest report slow tests in the logs)",pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-31 21:27:05,2
13135150,[Python] chdir logic in setup.py buggy,"In some conditions, {{setup.py}} may change the current directory and omit restoring the previous one, which can fail some operations.",pull-request-available,['Python'],ARROW,Bug,Minor,2018-01-31 17:34:19,2
13135134,[Python] Document that Plasma is not (yet) supported on Windows,See discussion in https://github.com/apache/arrow/issues/1531,pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-31 16:46:20,14
13135132,[Python] Expose Array's buffers to Python users,This amounts to converting {{arr->data()->buffers}} to a list of {{pyarrow.Buffer}} objects,pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-31 16:12:05,2
13134894,[C++] Stalled builds in test_serialization.py in Travis CI,"We've been seeing consistent stalled builds of this nature in Travis CI in the last 24-48 hours:

https://travis-ci.org/apache/arrow/jobs/335320563",ci-failure pull-request-available,['C++'],ARROW,Bug,Blocker,2018-01-30 21:40:15,14
13134890,[C++] Run ASAN builds in Travis CI,"[Address Sanitizer|https://github.com/google/sanitizers/wiki/AddressSanitizer] might be a better alternative to valgrind in clang or gcc builds. As part of this, we should also document how users can run their own local ASAN builds",pull-request-available,['C++'],ARROW,Improvement,Major,2018-01-30 21:10:01,2
13134849,[Python] Configure size of data pages in pyarrow.parquet.write_table,"It would be useful to be able to set the size of data pages (within Parquet column chunks) from Python. The current default is set to 1MiB at https://github.com/apache/parquet-cpp/blob/0875e43010af485e1c0b506d77d7e0edc80c66cc/src/parquet/properties.h#L81. It might be useful in some situations to lower this for more granular access.

We should provide this value as a parameter to {{pyarrow.parquet.write_table}}.",beginner parquet pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-30 18:40:28,14
13134774,Compilation warnings,"I suppose this may vary depending on the compiler, but I get the following warnings with gcc 4.9:
{code}
/home/antoine/arrow/cpp/src/plasma/fling.cc: In function int send_fd(int, int):
/home/antoine/arrow/cpp/src/plasma/fling.cc:46:50: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
   *reinterpret_cast<int*>(CMSG_DATA(header)) = fd;
                                                  ^
/home/antoine/arrow/cpp/src/arrow/python/io.cc: In member function virtual arrow::Status arrow::py::PyReadableFile::Read(int64_t, std::shared_ptr<arrow::Buffer>*):
/home/antoine/arrow/cpp/src/arrow/python/io.cc:153:60: warning: bytes_obj may be used uninitialized in this function [-Wmaybe-uninitialized]
   Py_DECREF(bytes_obj);
                                                            ^
/home/antoine/arrow/cpp/src/arrow/python/io.cc: In member function virtual arrow::Status arrow::py::PyReadableFile::Read(int64_t, int64_t*, void*):
/home/antoine/arrow/cpp/src/arrow/python/io.cc:141:60: warning: bytes_obj may be used uninitialized in this function [-Wmaybe-uninitialized]
   Py_DECREF(bytes_obj);
                                                            ^
/home/antoine/arrow/cpp/src/arrow/python/io.cc: In member function virtual arrow::Status arrow::py::PyReadableFile::GetSize(int64_t*):
/home/antoine/arrow/cpp/src/arrow/python/io.cc:187:20: warning: file_size may be used uninitialized in this function [-Wmaybe-uninitialized]
   *size = file_size;
                    ^
/home/antoine/arrow/cpp/src/arrow/python/io.cc:46:65: warning: current_position may be used uninitialized in this function [-Wmaybe-uninitialized]
                              const_cast<char*>(argspec), args...);
                                                                 ^
/home/antoine/arrow/cpp/src/arrow/python/io.cc:175:11: note: current_position was declared here
   int64_t current_position;
           ^
/home/antoine/arrow/cpp/src/arrow/ipc/json-internal.cc: In function arrow::Status arrow::ipc::internal::json::GetField(const Value&, const arrow::ipc::DictionaryMemo*, std::shared_ptr<arrow::Field>*):
/home/antoine/arrow/cpp/src/arrow/ipc/json-internal.cc:876:81: warning: dictionary_id may be used uninitialized in this function [-Wmaybe-uninitialized]
     RETURN_NOT_OK(dictionary_memo->GetDictionary(dictionary_id, &dictionary));
                                                                                 ^
/home/antoine/arrow/cpp/src/arrow/ipc/json-internal.cc: In function arrow::Status arrow::ipc::internal::json::ReadSchema(const Value&, arrow::MemoryPool*, std::shared_ptr<arrow::Schema>*):
/home/antoine/arrow/cpp/src/arrow/ipc/json-internal.cc:1354:80: warning: dictionary_id may be used uninitialized in this function [-Wmaybe-uninitialized]
     RETURN_NOT_OK(dictionary_memo->AddDictionary(dictionary_id, dictionary));
                                                                                ^
/home/antoine/arrow/cpp/src/arrow/ipc/json-internal.cc:1349:13: note: dictionary_id was declared here
     int64_t dictionary_id;
             ^
In file included from /home/antoine/arrow/cpp/src/arrow/api.h:25:0,
                 from /home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:29:
/home/antoine/arrow/cpp/src/arrow/builder.h: In member function arrow::Status arrow::py::TimestampConverter::AppendItem(const arrow::py::OwnedRef&):
/home/antoine/arrow/cpp/src/arrow/builder.h:284:5: warning: t may be used uninitialized in this function [-Wmaybe-uninitialized]
     raw_data_[length_++] = val;
     ^
/home/antoine/arrow/cpp/src/arrow/python/builtin_convert.cc:576:13: note: t was declared here
     int64_t t;
             ^

{code}
",build pull-request-available,['C++'],ARROW,Task,Trivial,2018-01-30 14:40:27,2
13134522,Unify OwnedRef and ScopedRef,"Currently {{OwnedRef}} and {{ScopedRef}} have similar semantics with small differences. Furtheremore, the naming distinction isn't obvious.

I propose to unify them as a single {{OwnedRef}} class with the following characteristics:
- doesn't take the GIL automatically
- has a {{release()}} method that decrefs the pointer (and sets the internal copy to NULL) before returning it
- has a {{detach()}} method that returns the pointer (and sets the internal copy to NULL) without decrefing it

For the rare situations where an {{OwnedRef}} may be destroyed with the GIL released, a {{OwnedRefNoGIL}} derived class would also be proposed (the naming scheme follows Cython here).

Opinions / comments?",pull-request-available,['Python'],ARROW,Task,Trivial,2018-01-29 18:22:27,2
13134307,[Python/C++] Upate Thrift pin to 0.11,"All the updates on the Parquet side are done, thus we need to unpin Thrift again to get a green master.",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2018-01-28 16:42:34,8
13134251,[Python] Add support for PEP519 - pathlib and similar objects,"Currently `pyarrow` doesn't seem to support reading from `pathlib.Path` or similar objects. [PEP519|https://www.python.org/dev/peps/pep-0519/] introduced `__fspath__` which could be used to transform any `Path` like object to a string.

[Pandas|https://github.com/pandas-dev/pandas/blob/a9d8e04ab68f688f899b4164bfa1ac868c9c1c64/pandas/io/common.py#L120-L160] has a sample implementation, though I think a simpler implementation of it could be used.


{code:java}
import pathlib
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

df = pd.DataFrame({
 'Foo': ['A', 'A', 'B', 'B', 'C'],
 'Bar': ['A1', 'A2', 'B2', 'D3', ''],
})

test_dir = pathlib.Path(__file__).parent / 'test'
test_dir.mkdir(parents=True, exist_ok=True)
table = pa.Table.from_pandas(df)
path = test_dir / 'file1.parquet'

# Doesn't work
pq.write_table(table, path)

# Works
pq.write_table(table, str(path))
{code}


[https://github.com/apache/arrow/issues/1522]



",pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-27 22:31:26,2
13133930,[Python] Deserialized Numpy array must keep ref to underlying tensor,"pyarrow.deserialize works fine, however.

{code:python}
Python 2.7.12 (default, Nov 20 2017, 18:23:56)
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pyarrow as pa, numpy as np
>>> with open('test.pyarrow', 'w') as f:
...     f.write(pa.serialize(np.arange(10, dtype=np.int32)).to_buffer().to_pybytes())
...
>>> pa.read_serialized(pa.OSFile('test.pyarrow')).deserialize()
array([54846320, 0, 45484448, 0, 4, 5, 6, 7, 8, 9], dtype=int32)
>>> pa.deserialize(pa.frombuffer(open('test.pyarrow').read()))
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-26 05:21:31,2
13133929,[Python] pyarrow.Buffer().to_pybytes() segfaults,"{{Python 2.7.12 (default, Nov 20 2017, 18:23:56)}}
{{[GCC 5.4.0 20160609] on linux2}}
{{Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.}}
{{>>> import pyarrow}}
{{>>> pyarrow.__version__}}
{{'0.8.0'}}
{{>>> pyarrow.Buffer().to_pybytes()}}
{{Segmentation fault (core dumped)}}",pull-request-available,['Python'],ARROW,Bug,Minor,2018-01-26 05:17:45,2
13133728,[C++] Update vendored cpplint.py to a Py3-compatible one,See https://github.com/cpplint/cpplint (BSD 3-clause),pull-request-available,['C++'],ARROW,Improvement,Major,2018-01-25 16:06:10,2
13133651,pa.array() doesn't work with iterators,"pa.array handles iterables fine, but not iterators if size isn't passed:
{code:java}
>>> arr = pa.array(range(5))
>>> arr
<pyarrow.lib.Int64Array object at 0x7f4652a05318>
[
  0,
  1,
  2,
  3,
  4
]
>>> arr = pa.array(iter(range(5)))
>>> arr
<pyarrow.lib.NullArray object at 0x7f4633c1d638>
[
 NA,
 NA,
 NA,
 NA,
 NA
]
{code}

This is because InferArrowSize() first exhausts the iterator.",pull-request-available,['Python'],ARROW,Bug,Minor,2018-01-25 10:52:33,2
13133543,[C++] ORC ep installs on each call to ninja build (even if no work to do),"On each call to {{ninja}}, the ORC EP reinstalls


{code:java}
$ ninja
[2/48] Performing configure step for 'orc_ep'
-- Build type: DEBUG
-- compiler Clang version 4.0.1
-- SNAPPY_HOME: /home/wesm/cpp-toolchain
-- Found the Snappy header: /home/wesm/cpp-toolchain/include/snappy.h
-- Found the Snappy library: /home/wesm/cpp-toolchain/lib/libsnappy.a
-- ZLIB_HOME: /home/wesm/cpp-toolchain
-- Found the ZLIB header: /home/wesm/cpp-toolchain/include/zlib.h
-- Found the ZLIB library: /usr/lib/x86_64-linux-gnu/libz.a
-- LZ4_HOME: /home/wesm/cpp-toolchain
-- Found the LZ4 header: /home/wesm/cpp-toolchain/include/lz4.h
-- Found the LZ4 library: /home/wesm/cpp-toolchain/lib/liblz4.a
-- PROTOBUF_HOME: /home/wesm/code/arrow/cpp/thirdparty/protobuf_ep-install
-- Found the Protobuf headers: /home/wesm/code/arrow/cpp/thirdparty/protobuf_ep-install/include
-- Found the Protobuf library: /usr/lib/x86_64-linux-gnu/libprotobuf.a
-- Found the Protoc library: /usr/lib/x86_64-linux-gnu/libprotoc.a
-- Found the Protoc executable: /home/wesm/code/arrow/cpp/thirdparty/protobuf_ep-install/bin/protoc
-- Configuring done
-- Generating done
-- Build files have been written to: /home/wesm/code/arrow/cpp/build/orc_ep-prefix/src/orc_ep-build
[3/48] Performing build step for 'orc_ep'
ninja: no work to do.
[4/5] Performing install step for 'orc_ep'
[0/1] Install the project...
-- Install configuration: ""DEBUG""
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/./LICENSE
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/./NOTICE
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/orc-config.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Reader.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Type.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Exceptions.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Statistics.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/ColumnPrinter.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/OrcFile.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Int128.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Vector.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Writer.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/Common.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/include/orc/MemoryPool.hh
-- Up-to-date: /home/wesm/code/arrow/cpp/build/orc_ep-install/lib/liborc.a
[5/5] Completed 'orc_ep'
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2018-01-25 00:10:09,8
13133497,[C++] ipc::Message::SerializeTo does not pad the message body,"I just want to note this here as a follow-up to ARROW-1860. I think that padding is the correct behavior, but I wasn't sure enough to make the fix there",pull-request-available,['C++'],ARROW,Bug,Major,2018-01-24 21:35:39,14
13133459,[Python] Cast all timestamp resolutions to INT96 use_deprecated_int96_timestamps=True,"When writing to a Parquet file, if `use_deprecated_int96_timestamps` is True, timestamps are only written as 96-bit integers if the timestamp has nanosecond resolution. This is a problem because Amazon Redshift timestamps only have microsecond resolution but require them to be stored in 96-bit format in Parquet files.

I'd expect theuse_deprecated_int96_timestamps flag to cause _all_ timestamps to be written as 96 bits, regardless of resolution. If this is a deliberate design decision, it'd be immensely helpful if it were explicitly documented as part of the argument.



To reproduce:



1.Create a table with atimestamp having microsecond or millisecond resolution, and save it to a Parquet file. Be sure to set `use_deprecated_int96_timestamps` to True.


{code:java}
import datetime
import pyarrow
from pyarrow import parquet

schema = pyarrow.schema([
    pyarrow.field('last_updated', pyarrow.timestamp('us')),
])

data = [
    pyarrow.array([datetime.datetime.now()], pyarrow.timestamp('us')),
]

table = pyarrow.Table.from_arrays(data, ['last_updated'])

with open('test_file.parquet', 'wb') as fdesc:
    parquet.write_table(table, fdesc,
                        use_deprecated_int96_timestamps=True)

{code}


2.Inspect the file.I used parquet-tools:


{noformat}
dak@tux ~ $ parquet-tools meta test_file.parquet
file:     file:/Users/dak/test_file.parquet

creator:   parquet-cpp version 1.3.2-SNAPSHOT



file schema: schema

--------------------------------------------------------------------------------

last_updated: OPTIONAL INT64 O:TIMESTAMP_MICROS R:0 D:1



row group 1: RC:1 TS:76 OFFSET:4

--------------------------------------------------------------------------------

last_updated: INT64 SNAPPY DO:4 FPO:28 SZ:76/72/0.95 VC:1 ENC:PLAIN,PLAIN_DICTIONARY,RLE{noformat}
",c++ parquet pull-request-available redshift timestamps,['Python'],ARROW,Bug,Major,2018-01-24 18:41:48,13
13133266,[C++] Test opening IPC stream reader or file reader on an empty InputStream,This was reported to segfault in ARROW-1589,pull-request-available,['C++'],ARROW,Bug,Major,2018-01-24 03:53:41,14
13133265,[Format] Add custom metadata field specific to a RecordBatch message,"While we can have schema- and field-level custom metadata, we cannot send metadata at the record batch level. This could include things like statistics (although statistics isn't a great example, because this might be something we want to eventually standardize), but other things too

See message definitions in https://github.com/apache/arrow/blob/master/format/Message.fbs",pull-request-available,['Format'],ARROW,Improvement,Major,2018-01-24 03:43:55,15
13132907,Array initialization with  large (>2**31-1) uint64 values fails,"Trying to create pyarrow arrays with large values causes a 'too big to convert' error:

Python 3.5.1 (default, Jun 6 2016, 14:30:01)
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.1.0 -- An enhanced Interactive Python.
? -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help -> Python's own help system.
object? -> Details about 'object', use 'object??' for extra details.

In [1]: import pyarrow as pa

In [2]: pa.array([2**63-1], type=pa.uint64())
Out[2]:
<pyarrow.lib.UInt64Array object at 0x1050e73b8>
[
 9223372036854775807
]

In [3]: pa.array([2**63], type=pa.uint64())
---------------------------------------------------------------------------
ArrowException Traceback (most recent call last)
<ipython-input-3-0d103b16c512> in <module>()
----> 1 pa.array([2**63], type=pa.uint64())

/Users/roddis/.pyenv/versions/3.5.1/envs/g3.5.1/lib/python3.5/site-packages/pyarrow/array.pxi in pyarrow.lib.array (/Users/travis/build/BryanCutler/arrow-dist/arrow/python/build/temp.macosx-10.6-intel-3.5/lib.cxx:29283)()

/Users/roddis/.pyenv/versions/3.5.1/envs/g3.5.1/lib/python3.5/site-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array (/Users/travis/build/BryanCutler/arrow-dist/arrow/python/build/temp.macosx-10.6-intel-3.5/lib.cxx:27945)()

/Users/roddis/.pyenv/versions/3.5.1/envs/g3.5.1/lib/python3.5/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status (/Users/travis/build/BryanCutler/arrow-dist/arrow/python/build/temp.macosx-10.6-intel-3.5/lib.cxx:9068)()

ArrowException: Unknown error: int too big to conver",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-23 01:58:30,14
13132043,[C++] Compiler warnings with CHECKIN warning level in ORC adapter,"I am encountering this locally with clang 4.0 and BUILD_WARNING_LEVEL=CHECKIN. We should investigate why these aren't showing up in our CI builds

{code}
../src/arrow/adapters/orc/adapter.cc:138:38: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long') to 'int32_t' (aka 'int') [-Werror,-Wshorten-64-to-32]
      *out = fixed_size_binary(type->getMaximumLength());
             ~~~~~~~~~~~~~~~~~ ~~~~~~^~~~~~~~~~~~~~~~~~
../src/arrow/adapters/orc/adapter.cc:151:30: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long') to 'int32_t' (aka 'int') [-Werror,-Wshorten-64-to-32]
        *out = decimal(type->getPrecision(), type->getScale());
               ~~~~~~~ ~~~~~~^~~~~~~~~~~~~~
../src/arrow/adapters/orc/adapter.cc:151:52: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long') to 'int32_t' (aka 'int') [-Werror,-Wshorten-64-to-32]
        *out = decimal(type->getPrecision(), type->getScale());
               ~~~~~~~                       ~~~~~~^~~~~~~~~~
../src/arrow/adapters/orc/adapter.cc:176:24: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]
      int size = type->getSubtypeCount();
          ~~~~   ~~~~~~^~~~~~~~~~~~~~~~~
../src/arrow/adapters/orc/adapter.cc:188:24: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]
      int size = type->getSubtypeCount();
          ~~~~   ~~~~~~^~~~~~~~~~~~~~~~~
../src/arrow/adapters/orc/adapter.cc:262:21: error: implicit conversion loses integer precision: 'uint64_t' (aka 'unsigned long') to 'int' [-Werror,-Wshorten-64-to-32]
    int size = type.getSubtypeCount();
        ~~~~   ~~~~~^~~~~~~~~~~~~~~~~
../src/arrow/adapters/orc/adapter.cc:452:18: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    for (int i = offset; i < length + offset; i++) {
             ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:477:18: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    for (int i = offset; i < length + offset; i++) {
             ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:543:26: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    int start = builder->length();
        ~~~~~   ~~~~~~~~~^~~~~~~~
../src/arrow/adapters/orc/adapter.cc:572:26: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    int start = builder->length();
        ~~~~~   ~~~~~~~~~^~~~~~~~
../src/arrow/adapters/orc/adapter.cc:599:18: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    for (int i = offset; i < length + offset; i++) {
             ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:615:18: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    for (int i = offset; i < length + offset; i++) {
             ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:632:20: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
      for (int i = offset; i < length + offset; i++) {
               ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:642:20: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
      for (int i = offset; i < length + offset; i++) {
               ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:519:26: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    int start = builder->length();
        ~~~~~   ~~~~~~~~~^~~~~~~~
../src/arrow/adapters/orc/adapter.cc:389:16: note: in instantiation of function template specialization 'arrow::adapters::orc::ORCFileReader::Impl::AppendNumericBatchCast<arrow::NumericBuilder<arrow::Int32Type>, int, orc::LongVectorBatch, long>' requested here
        return AppendNumericBatchCast<Int32Builder, int32_t, liborc::LongVectorBatch,
               ^
../src/arrow/adapters/orc/adapter.cc:519:26: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    int start = builder->length();
        ~~~~~   ~~~~~~~~~^~~~~~~~
../src/arrow/adapters/orc/adapter.cc:392:16: note: in instantiation of function template specialization 'arrow::adapters::orc::ORCFileReader::Impl::AppendNumericBatchCast<arrow::NumericBuilder<arrow::Int16Type>, short, orc::LongVectorBatch, long>' requested here
        return AppendNumericBatchCast<Int16Builder, int16_t, liborc::LongVectorBatch,
               ^
../src/arrow/adapters/orc/adapter.cc:519:26: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    int start = builder->length();
        ~~~~~   ~~~~~~~~~^~~~~~~~
../src/arrow/adapters/orc/adapter.cc:395:16: note: in instantiation of function template specialization 'arrow::adapters::orc::ORCFileReader::Impl::AppendNumericBatchCast<arrow::NumericBuilder<arrow::Int8Type>, signed char, orc::LongVectorBatch, long>' requested here
        return AppendNumericBatchCast<Int8Builder, int8_t, liborc::LongVectorBatch,
               ^
../src/arrow/adapters/orc/adapter.cc:519:26: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    int start = builder->length();
        ~~~~~   ~~~~~~~~~^~~~~~~~
../src/arrow/adapters/orc/adapter.cc:401:16: note: in instantiation of function template specialization 'arrow::adapters::orc::ORCFileReader::Impl::AppendNumericBatchCast<arrow::NumericBuilder<arrow::FloatType>, float, orc::DoubleVectorBatch, double>' requested here
        return AppendNumericBatchCast<FloatBuilder, float, liborc::DoubleVectorBatch,
               ^
../src/arrow/adapters/orc/adapter.cc:599:18: error: implicit conversion loses integer precision: 'int64_t' (aka 'long') to 'int' [-Werror,-Wshorten-64-to-32]
    for (int i = offset; i < length + offset; i++) {
             ~   ^~~~~~
../src/arrow/adapters/orc/adapter.cc:407:16: note: in instantiation of function template specialization 'arrow::adapters::orc::ORCFileReader::Impl::AppendBinaryBatch<arrow::StringBuilder>' requested here
        return AppendBinaryBatch<StringBuilder>(batch, offset, length, builder);
               ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
{code}",ORC pull-request-available,['C++'],ARROW,Bug,Major,2018-01-18 23:10:50,14
13131747,[Python] Sequence converter for float32 not implemented,"See bug report in [https://github.com/apache/arrow/issues/1431,]example
{code:java}
import pyarrow as pa
l = [[1.2, 3.4], [9.0, 42.0]]
pa.array(l, type=pa.list_(pa.float32())){code}
",pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-18 01:17:49,14
13131727,[Python] pyflakes warnings on Cython files not failing build,"I see the following flakes in master:
{code:java}
pyarrow/plasma.pyx:251:80: E501 line too long (82 > 79 characters)
pyarrow/plasma.pyx:305:80: E501 line too long (96 > 79 characters)
pyarrow/_orc.pyx:53:46: E127 continuation line over-indented for visual indent
pyarrow/_orc.pyx:72:49: E703 statement ends with a semicolon
pyarrow/_orc.pyx:75:52: E703 statement ends with a semicolon
pyarrow/_orc.pyx:88:80: E501 line too long (85 > 79 characters)
pyarrow/_orc.pyx:92:80: E501 line too long (94 > 79 characters)
pyarrow/_orc.pxd:32:80: E501 line too long (87 > 79 characters)
pyarrow/_orc.pxd:43:80: E501 line too long (90 > 79 characters)
9{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-17 23:48:19,14
13131683,[C++] Add shrink_to_fit option in BufferBuilder::Resize,See discussion inhttps://github.com/apache/arrow/pull/1481#discussion_r162157558,pull-request-available,['C++'],ARROW,Improvement,Major,2018-01-17 20:01:09,14
13131644,[Python] Do not use deprecated kwarg in pandas.core.internals.make_block,see bug report in [https://github.com/apache/arrow/issues/1484],pull-request-available,['Python'],ARROW,Bug,Major,2018-01-17 17:43:17,14
13131102,[Python] Table.from_pandas crashes when data frame is empty,"Loading an empty CSV file, and then attempting to create a PyArrow Table from it makes the application crash. The followingcode should be able to reproduce theissue:
{code}
import numpy as np
import pandas as pd
import pyarrow as pa

FIELDS = ['id', 'name']
NUMPY_TYPES = {
    'id': np.int64,
    'name': np.unicode
}
PYARROW_SCHEMA = pa.schema([
    pa.field('id', pa.int64()),
    pa.field('name', pa.string())
])

file = open('input.csv', 'w')
file.close()

df = pd.read_csv(
    'input.csv',
    header=None,
    names=FIELDS,
    dtype=NUMPY_TYPES,
    engine='c',
)

pa.Table.from_pandas(df, schema=PYARROW_SCHEMA)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-15 22:46:12,2
13130964,[Python] pyarrow.read_serialized cannot read concatenated records,"The following code
{quote}import pyarrow as pa

f = pa.OSFile('arrow_test', 'w')
 pa.serialize_to(12, f)
 pa.serialize_to(23, f)
 f.close()

f = pa.OSFile('arrow_test', 'r')
 print(pa.read_serialized(f).deserialize())
 print(pa.read_serialized(f).deserialize())
 f.close()
{quote}
gives the following result:
{quote}$ python pyarrow_test.py
 First: 12
 Traceback (most recent call last):
 File ""pyarrow_test.py"", line 10, in <module>
 print('Second: {}'.format(pa.read_serialized(f).deserialize()))
 File ""pyarrow/serialization.pxi"", line 347, in pyarrow.lib.read_serialized (/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:79159)
 File ""pyarrow/error.pxi"", line 77, in pyarrow.lib.check_status (/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:8270)
 pyarrow.lib.ArrowInvalid: Expected schema message in stream, was null or length 0
{quote}
I would have expected read_serialized to sucessfully read the second value.",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-15 08:58:08,14
13130737,[Python] Test against Pandas master,"We have seen recently a lot of breakage with Pandas master. This is an annoyance to our users and should already break in our builds instead of their chains. There is no need to add another entry to matrix, just in one of them to re-run the tests with the Pandas master after they ran successfully.",pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-13 11:40:14,3
13130678,[Python] Add function for determining implied Arrow schema from pandas.DataFrame,"Currently the only option is to use {{Table/Array.from_pandas}} which does significant unnecessary work and allocates memory. If only the schema is of interest, then we could do less work and not allocate memory.

We should provide the user a function {{pyarrow.Schema.from_pandas}} which takes a DataFrame as an input and returns the respective Arrow schema. The functionality for determing the schema is already available in the Python code, it is at moment just very tightly bound to the conversion infrastructure.",beginner pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-12 23:03:44,3
13130356,[GLib] Docker-based documentation build is broken,"I'm putting up a patch that got me to here

{code}
  DOC   Building HTML
../arrow-glib-docs.xml:25: warning: failed to load external entity ""../xml/gtkdocentities.ent""
  %gtkdocentities;
                  ^
Entity: line 1: 
 %gtkdocentities; 
                 ^
../arrow-glib-docs.xml:29: parser error : Entity 'package_name' not defined
    <title>&package_name; Reference Manual</title>
                         ^
../arrow-glib-docs.xml:31: parser error : Entity 'package_string' not defined
      for &package_string;.
                          ^
warning: failed to load external entity ""../xml/basic-array.xml""
../arrow-glib-docs.xml:43: element include: XInclude error : could not load ../xml/basic-array.xml, and no fallback was found
warning: failed to load external entity ""../xml/composite-array.xml""
../arrow-glib-docs.xml:44: element include: XInclude error : could not load ../xml/composite-array.xml, and no fallback was found
{code}

",pull-request-available,['GLib'],ARROW,Bug,Major,2018-01-11 23:25:14,14
13129962,[Python] Return parquet statistics min/max as values instead of strings,"Currently `min` and `max` column statistics are returned as formatted strings of the _physical type_. This makes using them in python a bit tricky, as the strings need to be parsed as the proper _logical type_. Observe:


{code}
In [20]: import pandas as pd

In [21]: df = pd.DataFrame({'a': [1, 2, 3],
    ...:                    'b': ['a', 'b', 'c'],
    ...:                    'c': [pd.Timestamp('1991-01-01')]*3})
    ...:

In [22]: df.to_parquet('temp.parquet', engine='pyarrow')

In [23]: from pyarrow import parquet as pq

In [24]: f = pq.ParquetFile('temp.parquet')

In [25]: rg = f.metadata.row_group(0)

In [26]: rg.column(0).statistics.min  # string instead of integer
Out[26]: '1'

In [27]: rg.column(1).statistics.min  # weird space added after value due to formatter
Out[27]: 'a '

In [28]: rg.column(2).statistics.min  # formatted as physical type (int) instead of logical (datetime)
Out[28]: '662688000000'
{code}

Since the type information is known, it should be possible to convert these to arrow values instead of strings.",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-10 20:40:28,14
13129338,"[Website] Add more visible link to ""Powered By"" page to front page, simplify Powered By","There can be a fine line between what is an open source project and what is a company and what is a product or organization, so to keep things simple I have consolidated things into a single list without passing any judgments. I have also added a link to the Powered By page that is visible without digging into any menus on the front page",pull-request-available,['Website'],ARROW,Improvement,Major,2018-01-08 20:22:40,14
13128572,[Python] Unit testing setup for ORC files,"ORC does not have a production ready C++ writer yet, so we will need to figure out another way to generate test data files to probe all of the corners of our ORC reader",pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-04 18:57:28,2
13128517,[C++] Support JAVA_HOME paths in HDFS libjvm loading that include the jre directory,See discussion in https://github.com/apache/arrow/issues/1437,pull-request-available,['C++'],ARROW,Bug,Major,2018-01-04 15:23:00,14
13128260,[C++/Python] Create Array from sequence of numpy.datetime64,Currently we only support {{datetime.datetime}} and {{datetime.date}}but {{numpy.datetime64}} also occurs quite often in the numpy/pandas-related world.,pull-request-available,['Python'],ARROW,Improvement,Major,2018-01-03 15:52:36,3
13128044,[Python] Writing Parquet file with flavor='spark' loses pandas schema metadata,"You can see the issue in the {{_sanitize_schema}} method

https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L201

see https://github.com/apache/arrow/issues/1452",pull-request-available,['Python'],ARROW,Bug,Major,2018-01-02 18:38:04,14
13127579,[Python] Add metadata accessor to pyarrow.Field,Depends on ARROW-906 for this data to survive IPC roundtrip,pull-request-available,['Python'],ARROW,Improvement,Major,2017-12-28 20:34:33,14
13127267,[Python/C++] Add option to Array.from_pandas and pyarrow.array to perform unsafe casts,Per mailing list thread,pull-request-available,['Python'],ARROW,Improvement,Major,2017-12-26 22:24:54,3
13125768,Download page must not link to snapshots / nightly builds,"Nightly builds / snapshots which are not formal releases must not be linked from the main download page.

Such builds have not been voted on and should only be used by project developers who should be made aware that the code is without any guarantees.

Nightly builds are not formal ASF releases, and must not be promoted to the general public.

See [1] second para. The second sentence states:

""Do not include any links on the project website that might encourage non-developers to download and use nightly builds, snapshots, release candidates, or any other similar package.""

[1] http://www.apache.org/dev/release.html#what",pull-request-available,['Website'],ARROW,Bug,Major,2017-12-19 01:20:34,14
13125574,[GLib] Build failure with --with-arrow-cpp-build-dir and GPU enabled Arrow C++,--with-arrow-cpp-build-dir is used in building deb packages.,pull-request-available,['GLib'],ARROW,Bug,Minor,2017-12-18 09:57:15,1
13125442,[C++] w4996 warning due to std::tr1 failing builds on Visual Studio 2017,"See for example. [~Max Risuhin] do you know what is the most appropriate fix (besides silencing the deprecation warning)?

{code}
C:\projects\arrow\cpp\build\googletest_ep-prefix\src\googletest_ep\googletest\include\gtest/internal/gtest-port.h(996): warning C4996: 'std::tr1': warning STL4002: The non-Standard std::tr1 namespace and TR1-only machinery are deprecated and will be REMOVED. You can define _SILENCE_TR1_NAMESPACE_DEPRECATION_WARNING to acknowledge that you have received this warning.
{code}",pull-request-available,['C++'],ARROW,New Feature,Major,2017-12-16 23:39:22,14
13125304,[C++] Move various Arrow testing utility code from Parquet to Arrow codebase,see https://github.com/apache/parquet-cpp/pull/426 and comments within,pull-request-available,['C++'],ARROW,Improvement,Major,2017-12-15 18:53:58,14
13125303,[C++] Add benchmarks comparing performance of internal::BitmapReader/Writer with naive approaches,The performance may also vary across platforms/compilers. This would be helpful to know how much they help,pull-request-available,['C++'],ARROW,Improvement,Major,2017-12-15 18:45:23,2
13124978,[GLib] Add garrow_timestamp_data_type_get_unit(),"If 0.8.0 RC2 is dropped, I hope that 0.8.0 includes this.
If RC2 has no problem, I hope that 0.9.0 includes this. ",pull-request-available,['GLib'],ARROW,New Feature,Minor,2017-12-14 15:32:35,1
13124498,[GLib] Must set GI_TYPELIB_PATH in verify-release-candidate.sh,This is a new necessary step when validating a release candidate,pull-request-available,['GLib'],ARROW,Bug,Major,2017-12-12 21:51:42,14
13124357,[C++] make -j may fail to build with -DARROW_GPU=on,"Here is a error log:

{code}
[100%] Built target gflags_nothreads_static
Install the project...
-- Install configuration: ""RELEASE""
-- Installing: /tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/build/gflags_ep-prefix/src/gflags_ep/lib/cmake/gflags/gflags-config.cmake
-- Installing: /tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/build/gflags_ep-prefix/src/gflags_ep/lib/cmake/gflags/gflags-config-version.cmake
-- Installing: /tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/build/gflags_ep-prefix/src/gflags_ep/lib/cmake/gflags/gflags-targets.cmake
-- Installing: /tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/build/gflags_ep-prefix/src/gflags_ep/lib/cmake/gflags/gflags-targets-release.cmake
-- Installing: /tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/build/gflags_ep-prefix/src/gflags_ep/bin/gflags_completions.sh
-- Installing: /tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/build/gflags_ep-prefix/src/gflags_ep/lib/pkgconfig/gflags.pc
-- Installing: /home/kou/.cmake/packages/gflags/fb801def37c922433975cbfefb3aa08d
[ 26%] Completed 'gflags_ep'
[ 55%] Building C object CMakeFiles/brotlienc.dir/enc/literal_cost.c.o
[ 26%] Built target gflags_ep
Scanning dependencies of target arrow_gpu_objlib
[ 26%] Building CXX object src/arrow/gpu/CMakeFiles/arrow_gpu_objlib.dir/cuda_arrow_ipc.cc.o
[ 59%] Building C object CMakeFiles/brotlienc.dir/enc/memory.c.o
/tmp/arrow-0.8.0.GLyu7/apache-arrow-0.8.0/cpp/src/arrow/gpu/cuda_arrow_ipc.cc:26:10: fatal error: arrow/ipc/Message_generated.h: No such file or directory
 #include ""arrow/ipc/Message_generated.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
src/arrow/gpu/CMakeFiles/arrow_gpu_objlib.dir/build.make:62: recipe for target 'src/arrow/gpu/CMakeFiles/arrow_gpu_objlib.dir/cuda_arrow_ipc.cc.o' failed
make[2]: *** [src/arrow/gpu/CMakeFiles/arrow_gpu_objlib.dir/cuda_arrow_ipc.cc.o] Error 1
CMakeFiles/Makefile2:2108: recipe for target 'src/arrow/gpu/CMakeFiles/arrow_gpu_objlib.dir/all' failed
make[1]: *** [src/arrow/gpu/CMakeFiles/arrow_gpu_objlib.dir/all] Error 2
make[1]: *** Waiting for unfinished jobs....
{code}",pull-request-available,['C++'],ARROW,Bug,Minor,2017-12-12 12:58:10,1
13124167,[Website] Add org affiliations to committers.html,see e.g. http://hadoop.apache.org/who.html,pull-request-available,['Website'],ARROW,Bug,Major,2017-12-11 16:47:49,14
13123773,[Python] Creating a pyarrow.Array with timestamp of different unit is not casted,"This is similar to ARROW-1680 but slightly different in that an error is not raised but the unit will still remain unchanged only when using a timezone

{noformat}
In [47]: us_with_tz = pa.timestamp('us', tz='America/New_York')

In [48]: s = pd.Series([val])

In [49]: s_nyc = s.dt.tz_localize('tzlocal()').dt.tz_convert('America/New_York')

In [50]: arr = pa.Array.from_pandas(s_nyc, type=us_with_tz)

In [51]: arr.type
Out[51]: TimestampType(timestamp[ns, tz=America/New_York])

In [52]: arr2 = pa.Array.from_pandas(s, type=pa.timestamp('us'))

In [53]: arr2.type
Out[53]: TimestampType(timestamp[us])
{noformat}

There is an easy workaround to apply the cast after creating the pyarrow.Array, which seems to work fine

{noformat}
In [54]: arr = pa.Array.from_pandas(s_nyc).cast(us_with_tz, safe=False)

In [55]: arr.type
Out[55]: TimestampType(timestamp[us, tz=America/New_York])
{noformat}",pull-request-available,['Python'],ARROW,Bug,Major,2017-12-08 19:02:29,14
13123553,[Python] Add more functions for checking exact types in pyarrow.types,"In https://github.com/apache/arrow/blob/master/python/pyarrow/types.py, we can check {{pyarrow.is_date}} but not whether something is date32 or date64. See discussion in https://github.com/apache/spark/pull/19884#discussion_r155626249",pull-request-available,['Python'],ARROW,Improvement,Major,2017-12-07 21:36:13,14
13123548,[C++] Deprecate PrimitiveArray::raw_values,"I ran into an odd issue where, even though I was casting to {{arrow::PrimitiveArray}}, it picked up the {{raw_values}} method from a subclass of {{PrimitiveArray}} (which includes a slice offset)

{code}
(gdb) p reinterpret_cast<const int64_t*>(reinterpret_cast<const PrimitiveArray&>(arr).raw_values())[0]
$9 = 25
(gdb) p reinterpret_cast<const int64_t*>(reinterpret_cast<const PrimitiveArray&>(arr).raw_values_)[0]
$10 = 10
(gdb) p arr.offset()
$11 = 15
{code}

I think the {{raw_values}} method in PrimitiveArray should be deprecated and removed, since it is dangerous to use as it does not include a slice offset, if any",pull-request-available,['C++'],ARROW,Bug,Major,2017-12-07 20:44:05,14
13123539,[Python]Remove mkdir race condition from write_to_dataset ,"If two processes create the same directory tree, one of them might see that a directory does not exist but before the actual call to {{mkdir}} is done, the second process already created the directory. In this case the former process will raise an exception.",pull-request-available,['Python'],ARROW,Improvement,Major,2017-12-07 20:13:39,8
13123510,[C++] Add kernel functions for determining value range (maximum and minimum) of integer arrays,"These functions can be useful internally for determining when a ""small range"" alternative to a hash table can be used for integer arrays. The maximum and minimum is determined in a single scan.

We already have infrastructure for aggregate kernels, so this would be an easy addition.",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2017-12-07 17:21:35,5
13123305,[C++] Do not allocate memory for primitive outputs in CastKernel::Call implementation,"This is some refactoring / tidying. Unless an output of cast has a non-determinate size (e.g. is Binary or something else), the {{CastKernel::Call}} implementation should assume that it is writing into pre-allocated memory. The corresponding memory allocation can be lifted into the {{arrow::compute::Cast}} API",pull-request-available,['C++'],ARROW,Improvement,Major,2017-12-06 21:44:13,14
13123252,[Python] test_primitive_serialization fails on Python 2.7.3,"{{test_primitive_serialization}}fails on with the following error: Python 2.7.3

{code}
str = <memory at 0x44dff28>
 
     def loads(str):
 >       file = StringIO(str)
E       TypeError: expected read buffer, memoryview found
{code}

More context:

{code}
     def test_primitive_serialization(large_memory_map):
         with pa.memory_map(large_memory_map, mode=""r+"") as mmap:
             for obj in PRIMITIVE_OBJECTS:
                 serialization_roundtrip(obj, mmap)
>               serialization_roundtrip(obj, mmap, pa.pandas_serialization_context)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2017-12-06 17:28:52,14
13123214,[Python]Unknown list item type: binary,"While strings are supported as list elements in the Arrow<->Pandas conversion, binary (bytes in Py3, str in Py2) are not supported.",pull-request-available,['Python'],ARROW,Bug,Major,2017-12-06 15:11:35,8
13123186,[Python] NaT date32 values are only converted to nulls if from_pandas is used,"{code}
np.array([None, date(2017, 4, 4)], dtype='datetime64[D]')
pa.array(expected, from_pandas=True) -> [null, 2017-4-4]
pa.array(expected, from_pandas=True) -> [1970-1-1, 2017-4-4]
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2017-12-06 13:02:24,14
13123156,[Python] Masking for date32 arrays not working,"{code}
arr = np.array([date(2017, 4, 3), date(2017, 4, 4)], dtype='datetime64[D]')
mask = [True, False]
t32 = pa.date32()
result = pa.array(arr, mask=np.array(mask))
expected = pa.array(np.array([None, date(2017, 4, 4)], dtype='datetime64[D]'))
{code}

The above code produces an array that has no null values.",pull-request-available,['Python'],ARROW,Bug,Major,2017-12-06 10:37:46,8
13123121,[Python] --exclude is not available in older git versions,The workaround implemented yesterday for the version determination uses Git's {{--exclude}} option which is only available on very recent versions of Git. Instead we should use a {{--match}} expression that only matches the non-JS tags.,pull-request-available,['Python'],ARROW,Bug,Major,2017-12-06 08:30:23,8
13122654,"[Python] Add function to ""flatten"" structs within tables","See discussion in https://issues.apache.org/jira/browse/ARROW-1873

When a user has a struct column, it may be more efficient to flatten the struct into multiple columns of the form {{struct_name.field_name}} for each field in the struct. Then when you call {{to_pandas}}, Python dictionaries do not have to be created, and the conversion will be much more efficient",pull-request-available,['Python'],ARROW,Improvement,Major,2017-12-04 19:22:30,2
13122600,[C++] Make JsonReader/JsonWriter classes internal APIs,"These are exposed in the public API in {{arrow::ipc}}, and could possibly mislead users: http://arrow.apache.org/docs/cpp/namespacearrow_1_1ipc.html",pull-request-available,['C++'],ARROW,Improvement,Major,2017-12-04 15:09:22,14
13122572,[Python] BUG: Table.to_pandas metadata checking fails if columns are not present,"Found this bug in the example in the pandas documentation (http://pandas-docs.github.io/pandas-docs-travis/io.html#parquet), which does:


{code}
df = pd.DataFrame({'a': list('abc'),
                   'b': list(range(1, 4)),
                   'c': np.arange(3, 6).astype('u1'),
                   'd': np.arange(4.0, 7.0, dtype='float64'),
                   'e': [True, False, True],
                   'f': pd.date_range('20130101', periods=3),
                   'g': pd.date_range('20130101', periods=3, tz='US/Eastern')})

df.to_parquet('example_pa.parquet', engine='pyarrow')

pd.read_parquet('example_pa.parquet', engine='pyarrow', columns=['a', 'b'])
{code}


and this raises in the last line reading a subset of columns:

{code}
...
/home/joris/miniconda3/envs/dev/lib/python3.5/site-packages/pyarrow/pandas_compat.py in _add_any_metadata(table, pandas_metadata)
    357     for i, col_meta in enumerate(pandas_metadata['columns']):
    358         if col_meta['pandas_type'] == 'datetimetz':
--> 359             col = table[i]
    360             converted = col.to_pandas()
    361             tz = col_meta['metadata']['timezone']

table.pxi in pyarrow.lib.Table.__getitem__()

table.pxi in pyarrow.lib.Table.column()

IndexError: Table column index 6 is out of range
{code}


This is due to checking the `pandas_metadata` for all columns (and in this case trying to deal with a datetime tz column), while in practice not all columns are present in this case ('mismatch' between pandas metadata and actual schema). 

A smaller example without parquet:

{code}
In [38]: df = pd.DataFrame({'a': [1, 2, 3], 'b': pd.date_range(""2017-01-01"", periods=3, tz='Europe/Brussels')})

In [39]: table = pyarrow.Table.from_pandas(df)

In [40]: table
Out[40]: 
pyarrow.Table
a: int64
b: timestamp[ns, tz=Europe/Brussels]
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""columns"": [{""pandas_type"": ""int64"", ""metadata"": null, ""numpy_t'
            b'ype"": ""int64"", ""name"": ""a""}, {""pandas_type"": ""datetimetz"", ""meta'
            b'data"": {""timezone"": ""Europe/Brussels""}, ""numpy_type"": ""datetime6'
            b'4[ns, Europe/Brussels]"", ""name"": ""b""}, {""pandas_type"": ""int64"", '
            b'""metadata"": null, ""numpy_type"": ""int64"", ""name"": ""__index_level_'
            b'0__""}], ""index_columns"": [""__index_level_0__""], ""pandas_version""'
            b': ""0.22.0.dev0+277.gd61f411""}'}

In [41]: table.to_pandas()
Out[41]: 
   a                         b
0  1 2017-01-01 00:00:00+01:00
1  2 2017-01-02 00:00:00+01:00
2  3 2017-01-03 00:00:00+01:00

In [44]: table_without_tz = table.remove_column(1)

In [45]: table_without_tz
Out[45]: 
pyarrow.Table
a: int64
__index_level_0__: int64
metadata
--------
{b'pandas': b'{""columns"": [{""pandas_type"": ""int64"", ""metadata"": null, ""numpy_t'
            b'ype"": ""int64"", ""name"": ""a""}, {""pandas_type"": ""datetimetz"", ""meta'
            b'data"": {""timezone"": ""Europe/Brussels""}, ""numpy_type"": ""datetime6'
            b'4[ns, Europe/Brussels]"", ""name"": ""b""}, {""pandas_type"": ""int64"", '
            b'""metadata"": null, ""numpy_type"": ""int64"", ""name"": ""__index_level_'
            b'0__""}], ""index_columns"": [""__index_level_0__""], ""pandas_version""'
            b': ""0.22.0.dev0+277.gd61f411""}'}

In [46]: table_without_tz.to_pandas()          # <------ wrong output !
Out[46]: 
                                     a
1970-01-01 01:00:00+01:00            1
1970-01-01 01:00:00.000000001+01:00  2
1970-01-01 01:00:00.000000002+01:00  3

In [47]: table_without_tz2 = table_without_tz.remove_column(1)

In [48]: table_without_tz2
Out[48]: 
pyarrow.Table
a: int64
metadata
--------
{b'pandas': b'{""columns"": [{""pandas_type"": ""int64"", ""metadata"": null, ""numpy_t'
            b'ype"": ""int64"", ""name"": ""a""}, {""pandas_type"": ""datetimetz"", ""meta'
            b'data"": {""timezone"": ""Europe/Brussels""}, ""numpy_type"": ""datetime6'
            b'4[ns, Europe/Brussels]"", ""name"": ""b""}, {""pandas_type"": ""int64"", '
            b'""metadata"": null, ""numpy_type"": ""int64"", ""name"": ""__index_level_'
            b'0__""}], ""index_columns"": [""__index_level_0__""], ""pandas_version""'
            b': ""0.22.0.dev0+277.gd61f411""}'}

In [49]: table_without_tz2.to_pandas()     # <------ error !
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-49-c82f33476c6b> in <module>()
----> 1 table_without_tz2.to_pandas()

table.pxi in pyarrow.lib.Table.to_pandas()

/home/joris/miniconda3/envs/dev/lib/python3.5/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, memory_pool, nthreads)
    289         pandas_metadata = json.loads(metadata[b'pandas'].decode('utf8'))
    290         index_columns = pandas_metadata['index_columns']
--> 291         table = _add_any_metadata(table, pandas_metadata)
    292 
    293     block_table = table

/home/joris/miniconda3/envs/dev/lib/python3.5/site-packages/pyarrow/pandas_compat.py in _add_any_metadata(table, pandas_metadata)
    357     for i, col_meta in enumerate(pandas_metadata['columns']):
    358         if col_meta['pandas_type'] == 'datetimetz':
--> 359             col = table[i]
    360             converted = col.to_pandas()
    361             tz = col_meta['metadata']['timezone']

table.pxi in pyarrow.lib.Table.__getitem__()

table.pxi in pyarrow.lib.Table.column()

IndexError: Table column index 1 is out of range
{code}

The reason is that `_add_any_metadata` does not check if the column it is processing (currently only datetime tz columns need such processing) is actually present in the schema.

Working on a fix, will submit a PR.
",pull-request-available,['Python'],ARROW,Bug,Major,2017-12-04 13:07:22,5
13122531,[C++] Reintroduce DictionaryBuilder,We need the {{DictionaryBuilder}} to incrementally build Arrow Arrays of {{DictionaryType}}. The kernels only support en-bloc conversions of Arrays which yields a higher memory usage.,pull-request-available,['C++'],ARROW,Bug,Critical,2017-12-04 09:52:57,8
13122518,[Python] setuptools_scm picks up JS version tags,Building wheels from the current master will end up in {{pyarrow-0.2.1.dev15+g3b438bc-cp36-cp36m-manylinux1_x86_64.whl}},pull-request-available,['Python'],ARROW,Bug,Major,2017-12-04 09:13:08,8
13122259,[Java] Write 64-bit ints as strings in integration test JSON files,"Javascript can't handle 64-bit integers natively, so writing them as strings in the JSON would make implementing the integration tests a lot simpler.",pull-request-available,"['Integration', 'JavaScript']",ARROW,Task,Minor,2017-12-01 20:13:16,16
13121802,[Python] Segmentation fault when loading total 2GB of parquet files,"We are trying to load 100 parquet files, and each of them is around 20MB. Before we port [ARROW-1830] into our pyarrow distribution, we use {{glob}} to list all the files, and then load them as pandas dataframe through pyarrow. 

The schema of the parquet files is like 
{code:java}
root
 |-- dateint: integer (nullable = true)
 |-- profileid: long (nullable = true)
 |-- time: long (nullable = true)
 |-- label: double (nullable = true)
 |-- weight: double (nullable = true)
 |-- features: array (nullable = true)
 |    |-- element: double (containsNull = true)
{code}

If we only load couple of them, it works without any issue. However, when loading 100 of them, we got segmentation fault as the following. FYI, if we flatten {{features: array[double]}} into top level, the file sizes are around the same, and work fine too. 

Is there anything we can try to eliminate this issue? Thanks.

{code}
>>> import glob
>>> files = glob.glob(""/home/dbt/data/*"")
>>> data = pq.ParquetDataset(files).read().to_pandas()
[New Thread 0x7fffe8f84700 (LWP 23769)]
[New Thread 0x7fffe3b93700 (LWP 23770)]
[New Thread 0x7fffe3392700 (LWP 23771)]
[New Thread 0x7fffe2b91700 (LWP 23772)]
[Thread 0x7fffe2b91700 (LWP 23772) exited]
[Thread 0x7fffe3b93700 (LWP 23770) exited]

Thread 4 ""python"" received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffe3392700 (LWP 23771)]
0x00007ffff270fc94 in arrow::Status arrow::VisitTypeInline<arrow::py::ArrowDeserializer>(arrow::DataType const&, arrow::py::ArrowDeserializer*) ()
   from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
(gdb) backtrace
#0  0x00007ffff270fc94 in arrow::Status arrow::VisitTypeInline<arrow::py::ArrowDeserializer>(arrow::DataType const&, arrow::py::ArrowDeserializer*) ()
   from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
#1  0x00007ffff2700b5a in arrow::py::ConvertColumnToPandas(arrow::py::PandasOptions, std::shared_ptr<arrow::Column> const&, _object*, _object**) ()
   from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
#2  0x00007ffff2714985 in arrow::Status arrow::py::ConvertListsLike<arrow::DoubleType>(arrow::py::PandasOptions, std::shared_ptr<arrow::Column> const&, _object**) () from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
#3  0x00007ffff2716b92 in arrow::py::ObjectBlock::Write(std::shared_ptr<arrow::Column> const&, long, long) ()
   from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
#4  0x00007ffff270a489 in arrow::py::DataFrameBlockCreator::WriteTableToBlocks(int)::{lambda(int)#1}::operator()(int) const ()
   from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
#5  0x00007ffff270a67c in std::thread::_Impl<std::_Bind_simple<arrow::Status arrow::ParallelFor<arrow::py::DataFrameBlockCreator::WriteTableToBlocks(int)::{lambda(int)#1}&>(int, int, arrow::py::DataFrameBlockCreator::WriteTableToBlocks(int)::{lambda(int)#1}&)::{lambda()#1} ()> >::_M_run() ()
   from /home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/../../../libarrow_python.so.0
#6  0x00007ffff1e30c5c in std::execute_native_thread_routine_compat (__p=<optimized out>)
    at /opt/conda/conda-bld/compilers_linux-64_1505664199673/work/.build/src/gcc-7.2.0/libstdc++-v3/src/c++11/thread.cc:110
#7  0x00007ffff7bc16ba in start_thread (arg=0x7fffe3392700) at pthread_create.c:333
#8  0x00007ffff78f73dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2017-11-30 01:51:31,14
13121280,[C++] Adding a column to an empty Table fails,"If we remove a column from a table and add new column (probably a casted version of it), it is rejected as the Table {{num_rows}} was reset to 0:

{code}
Invalid: Added column's length must match table's length. Expected length 0 but got length 1
{code}",pull-request-available,['C++'],ARROW,Bug,Major,2017-11-28 12:27:11,14
13120798,"[Python] Fix up ASV setup, add developer instructions for writing new benchmarks and running benchmark suite locally",We need to start writing more microbenchmarks as we go to prevent unintentional performance regressions (this has been a constant thorn in my side for years: http://wesmckinney.com/blog/introducing-vbench-new-code-performance-analysis-and-monitoring-tool/). ,pull-request-available,['Python'],ARROW,Improvement,Major,2017-11-26 17:52:48,2
13120792,"[C++] Add data structure to ""stage"" a sequence of IPC messages from in-memory data","Currently, when you need to pre-allocate space for a record batch or a stream (schema + dictionaries + record batches), you must make multiple passes over the data structures of interest (and use e.g. {{MockOutputStream}} to compute the size of the output buffer). It would be useful to make a single pass to ""prepare"" the IPC payload for both sizing and writing to prevent having to make multiple passes",flight pull-request-available,['C++'],ARROW,New Feature,Major,2017-11-26 16:43:59,14
13120741,[Python] Add switch for boost linkage with static parquet in wheels,"Currently we will always link {{boost_regex}} statically, in some build situation, shared linkage is preferred.",pull-request-available,['Python'],ARROW,New Feature,Major,2017-11-25 19:21:34,8
13120735,[Python] Auto-detect Parquet ABI version when using PARQUET_HOME,"Current we hardcode 1.0.0 in the case that {{PARQUET_HOME}} is set, we should try to detect the correct version by either globbing on {{libparquet.1.*.so/dylib}} or by install a {{CMakeConfig}} for {{parquet-cpp}}",pull-request-available,['Python'],ARROW,Bug,Major,2017-11-25 17:41:37,8
13120677,[Python] Improve performance of serializing object dtype ndarrays,"I haven't looked carefully at the hot path for this, but I would expect these statements to have roughly the same performance (offloading the ndarray serialization to pickle)

{code}
In [1]: import pickle

In [2]: import numpy as np

In [3]: import pyarrow as pa
a
In [4]: arr = np.array(['foo', 'bar', None] * 100000, dtype=object)

In [5]: timeit serialized = pa.serialize(arr).to_buffer()
10 loops, best of 3: 27.1 ms per loop

In [6]: timeit pickled = pickle.dumps(arr)
100 loops, best of 3: 6.03 ms per loop
{code}

[~robertnishihara] [~pcmoritz] I encountered this while working on ARROW-1783, but it can likely be resolved independently",pull-request-available,['Python'],ARROW,Improvement,Major,2017-11-24 20:15:12,14
13120503,[C++] Use const void* in Writable::Write instead of const uint8_t*,"I noticed that {{Writable}} is also misspelled. This would spare the user many usages of {{reinterpret_cast<const uint8_t*>(...)}}, and also will not cause any explicit API breakage (except where users have overridden the virtual function)",pull-request-available,['C++'],ARROW,Improvement,Major,2017-11-23 15:24:04,14
13120291,[Python]Expose Decimal128Type,"In the course of the renaming, we forgot to update the Python code to the new {{Decimal128Type}}, thus master is currently failing.",pull-request-available,['Python'],ARROW,Bug,Major,2017-11-22 18:00:12,8
13120287,[C++] Basic benchmark suite for hash kernels,"* Integers, small cardinality and large cardinality
* Short strings, small/large cardinality
* Long strings, small/large cardinality

These benchmarks will enable us to refactor without fear, and to experiment with faster hash functions",pull-request-available,['C++'],ARROW,New Feature,Major,2017-11-22 17:51:08,14
13119532,[C++] Use compute::Datum uniformly for input argument to kernels,This is some API tidying after ARROW-1559. Some kernel APIs are still using {{ArrayData}} for the input argument,pull-request-available,['C++'],ARROW,Improvement,Major,2017-11-19 22:37:37,14
13119531,[Java] Unable to read unsigned integers outside signed range for bit width in integration tests,"I believe this was introduced recently (perhaps in the refactors), but there was a problem where the integration tests weren't being properly run that hid the error from us

see https://github.com/apache/arrow/pull/1294#issuecomment-345553066",columnar-format-1.0 pull-request-available,['Java'],ARROW,Bug,Major,2017-11-19 21:58:03,15
13119507,[C++] Create Arrow schema from std::tuple types,"Given an `std::tuple` type, create an Arrow schema instance. ",pull-request-available,['C++'],ARROW,Improvement,Minor,2017-11-19 18:08:10,8
13119493,[Doc]Build documentation in separate build folders,Use non-default folders for building Arrow and Parquet C++ for the documentation in the docker image. Currently they infer with the default build folder a typical developer would chose and thus interfere with local development.,pull-request-available,['Documentation'],ARROW,Improvement,Major,2017-11-19 15:30:19,8
13119387,[Python] Error when loading all the files in a dictionary,"I can read one parquet file, but when I tried to read all the parquet files in a folder, I got an error.

{code:java}
>>> data = pq.ParquetDataset('./aaa/part-00000-d8268e3a-4e65-41a3-a43e-01e0bf68ee86')
>>> data = pq.ParquetDataset('./aaa/')
Ignoring path: ./aaa//part-00000-d8268e3a-4e65-41a3-a43e-01e0bf68ee86
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/pyarrow/parquet.py"", line 638, in __init__
    self.validate_schemas()
  File ""/usr/local/lib/python2.7/site-packages/pyarrow/parquet.py"", line 647, in validate_schemas
    self.schema = self.pieces[0].get_metadata(open_file).schema
IndexError: list index out of range
>>> 
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2017-11-18 01:50:28,14
13118965,[C++] Implement hash kernel specialization for BooleanType,Follow up to ARROW-1559,pull-request-available,['C++'],ARROW,Improvement,Major,2017-11-16 16:24:23,14
13118133,[C++] Make RecordBatch interface virtual to permit record batches that lazy-materialize columns,"This should be looked at soon to prevent having to define a different virtual interface for record batches. There are places where we are using the record batch constructor directly, and in some third party code (like MapD), so this might be good to get done for 0.8.0",pull-request-available,['C++'],ARROW,Improvement,Major,2017-11-14 03:07:53,14
13117616,[Python] RowGroup filtering on file level,We can build upon the API defined in {{fastparquet}} for defining RowGroup filters: https://github.com/dask/fastparquet/blob/master/fastparquet/api.py#L296-L300 and translate them into the C++ enums we will define in https://issues.apache.org/jira/browse/PARQUET-1158 . This should enable us to provide the user with a simple predicate pushdown API that we can extend in the background from RowGroup to Page level later on.,dataset dataset-parquet-read parquet pull-request-available,['C++'],ARROW,Improvement,Major,2017-11-10 16:28:52,5
13117436,Integration tests generate date[DAY] values outside of reasonable range,"The integration tests are generating random int32 values, but for systems that use millisecond-based date objects (like JavaScript), converting to millisecond date will cause an overflow in a lot of cases. We should generate values that are within a reasonable year range so that overflows when converting to milliseconds do not occur",pull-request-available,['Python'],ARROW,Bug,Major,2017-11-10 00:13:37,14
13117387,[Format] List expected on-wire buffer layouts for each kind of Arrow physical type in specification,"see ARROW-1693, ARROW-1785",columnar-format-1.0,['Format'],ARROW,Improvement,Major,2017-11-09 20:11:08,14
13117385,[Format/C++/Java] Remove VectorLayout metadata from Flatbuffers metadata,"Based on the discussions on the mailing list, we should remove the VectorLayout metadata from the Flatbuffers spec for 0.8.0 since we are already breaking the metadata. We should instead make the buffer layouts an immutable part of the format specification so there is no ambiguity about what buffers accompany each type of field in an Arrow payload

see also ARROW-1693",pull-request-available,"['C++', 'Format', 'Java']",ARROW,Improvement,Major,2017-11-09 20:10:19,14
13117346,[Python] Read and write pandas.DataFrame in pyarrow.serialize by decomposing the BlockManager rather than coercing to Arrow format,"See discussion in https://github.com/dask/distributed/pull/931

This will permit zero-copy reads for DataFrames not containing Python objects. In the event of an {{ObjectBlock}} these arrays will not be worse than pickle to reconstruct on the receiving side",pull-request-available,['Python'],ARROW,New Feature,Major,2017-11-09 17:35:21,14
13117344,[Python] Convert SerializedPyObject to/from sequence of component buffers with minimal memory allocation / copying,"See discussion on Dask org:

https://github.com/dask/distributed/pull/931

It would be valuable for downstream users to compute the serialized payload as a sequence of memoryview-compatible objects without having to allocate new memory on write. This means that the component tensor messages must have their metadata and bodies in separate buffers. This will require a bit of work internally reassemble the object from a collection of {{pyarrow.Buffer}} objects

see also ARROW-1509",pull-request-available,['Python'],ARROW,New Feature,Major,2017-11-09 17:33:23,14
13117343,"[Python] Expose compressors as pyarrow.compress, pyarrow.decompress","These should release the GIL, and serve as an alternative to the various compressor wrapper libraries out there. They should have the ability to work with {{pyarrow.Buffer}} or {{PyBytes}} as the user prefers",pull-request-available,['Python'],ARROW,New Feature,Major,2017-11-09 17:27:08,14
13117077,[CI] OSX Builds on Travis-CI time out often,"It looks like this command:

{code}
brew bundle --file=cpp/Brewfile
{code}

takes quite awhile to run. In this build (https://travis-ci.org/cpcloud/arrow/jobs/299219939) it took about 16 minutes to run.

Is there something we can do to make this faster or use something else to install those packages?

cc [~kou] [~xhochy]",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2017-11-08 19:50:30,8
13116820,"[Python] Link parquet-cpp statically, privately in manylinux1 wheels","We currently link parquet-cpp dynamically in the {{manylinux1}}wheels. This also makes us the authority on the distribution of {{parquet-cpp}} inside of the wheel-based ecosystem. Instead of doing this, we should statically, privately link {{parquet-cpp}} inside of the wheels.",pull-request-available,['Python'],ARROW,Bug,Blocker,2017-11-07 22:25:41,8
13116804,[C++] Add static ctor ArrayData::Make for nicer syntax in places,"Because of the fickleness of {{std::make_shared}}, we are having to store a vector of buffers in an lvalue rather than passing the buffers in the {{make_shared}} function call as an initializer list. This makes things a bit awkward, and could be made possibly better with a factory function",pull-request-available,['C++'],ARROW,New Feature,Major,2017-11-07 21:30:12,14
13116699,[C++[ arrow::gpu::CudaContext::bytes_allocated() isn't defined,"arrow/gpu/cuda_context.h declares arrow::gpu::CudaContext::bytes_allocated() but it's not defined.
Should it be removed or defined?

CudaContext::CudaContextImple::bytes_allocated() exists. So it's easy to define it.",pull-request-available,['C++'],ARROW,Bug,Minor,2017-11-07 14:57:43,1
13116527,"[C++] Add ""view"" function to create zero-copy views for compatible types, if supported","Similar to NumPy's {{ndarray.view}}, but with the restriction that the input and output types have the same physical Arrow memory layout. This might be as simple as adding a ""zero copy only"" option to the existing {{Cast}} kernel",pull-request-available,['C++'],ARROW,New Feature,Major,2017-11-06 21:54:18,2
13115763,[Doc] Use dependencies from conda in C++ docker build,Currently dependencies are built from scratch mostly in the C++ build for the documentation instead of using the conda-installed ones. Setting {{ARROW_BUILD_TOOLCHAIN=$CONDA_PREFIX}}should improve build times.,pull-request-available,['Documentation'],ARROW,Improvement,Trivial,2017-11-02 22:01:42,8
13115693,[Python] Add -c conda-forge for Windows dev installation instructions,Otherwise the wrong gflags binary is pulled.,pull-request-available,['Python'],ARROW,Bug,Trivial,2017-11-02 17:57:04,8
13115586,[Python] DataType should be hashable,"We can then use the DataType objects as keys in dictionary for example. xref https://github.com/ibis-project/ibis/pull/1194#discussion_r148493472

{code}
In [1]: import pyarrow as pa

In [2]: pa.__version__
Out[2]: '0.7.1'

In [3]: pa.int8()
Out[3]: DataType(int8)

In [4]: hash(pa.int8())
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-bca0e6e2f6af> in <module>()
----> 1 hash(pa.int8())

TypeError: unhashable type: 'pyarrow.lib.DataType'
{code}",pull-request-available,['Python'],ARROW,Improvement,Major,2017-11-02 10:39:42,14
13115552,[C++] unittest failure for language environment,"After clone the project and run ""make unittest"" ,the ""io-file-test"" has error:  

locale::facet::_S_create_c_locale name not valid 

need to add LC_ALL=""en_US.UTF-8"" to pass this test",pull-request-available,['C++'],ARROW,Bug,Trivial,2017-11-02 07:14:17,14
13112813,[C++] Don't export symbols of statically linked libraries,Currently we export the symbols of the underlying compression libraries like Brotli. These are private dependencies of Arrow and should not pollute the linking namespace.,pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2017-10-28 13:26:56,8
13112618,C++: clang-format is not detected correct on OSX anymore,Paths changed slightly in recent homebrew builds. We need to adjust our script to call the correct executable again.,pull-request-available,['C++'],ARROW,Bug,Major,2017-10-27 14:59:50,8
13112420,"[C++] Comparison function for DictionaryArray to determine if indices are ""compatible""","For example, if one array's dictionary is larger than the other, but the overlapping beginning portion is the same, then the respective dictionary indices correspond to the same values. Therefore, in analytics, one may choose to drop the smaller dictionary in favor of the larger dictionary, and this need not incur any computational overhead (beyond comparing the dictionary prefixes -- there may be some way to engineer ""dictionary lineage"" to make this comparison even cheaper)",pull-request-available,['C++'],ARROW,New Feature,Major,2017-10-26 22:37:35,6
13112309,[Python] Fix usages of assertRaises causing broken build,There were conflicting changes from PRs merged -- one of the test classes was made not a subclass of unittest.TestCase,pull-request-available,['Python'],ARROW,Bug,Major,2017-10-26 15:13:46,14
13112280,[C++] Cast kernels cannot write into sliced output array,"If the offset is non-zero, the results will be written into the wrong location:

https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/cast.cc#L218",pull-request-available,['C++'],ARROW,Bug,Major,2017-10-26 13:36:16,14
13112076,[Python] RecordBatch.from_pandas fails on DataFrame with no columns when preserve_index=False,"I believe this should have well-defined behavior and not raise an error:

{code}
In [5]: pa.RecordBatch.from_pandas(pd.DataFrame({}), preserve_index=False)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-4dda72b47dbd> in <module>()
----> 1 pa.RecordBatch.from_pandas(pd.DataFrame({}), preserve_index=False)

~/code/arrow/python/pyarrow/table.pxi in pyarrow.lib.RecordBatch.from_pandas (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:39957)()
    586             df, schema, preserve_index, nthreads=nthreads
    587         )
--> 588         return cls.from_arrays(arrays, names, metadata)
    589 
    590     @staticmethod

~/code/arrow/python/pyarrow/table.pxi in pyarrow.lib.RecordBatch.from_arrays (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:40130)()
    615 
    616         if not number_of_arrays:
--> 617             raise ValueError('Record batch cannot contain no arrays (for now)')
    618 
    619         num_rows = len(arrays[0])

ValueError: Record batch cannot contain no arrays (for now)
{code}",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-25 19:09:01,14
13112072,[Python] Provide for selecting a subset of columns to convert in RecordBatch/Table.from_pandas,"Currently it's all-or-nothing, and to do the subsetting in pandas incurs a data copy. This would enable columns (by name or index) to be selected out without additional data copying. We should add a {{columns=}} argument to the the {{from_pandas}} calls and do the subsetting when we dispatch the individual arrays for conversion to Arrow.

cc [~cpcloud] [~jreback]",beginner pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-25 19:00:55,2
13112010,[C++] Run clang-format checks in Travis CI,I think it's reasonable to expect contributors to run clang-format on their code. This may lead to a higher number of failed builds but will eliminate noise diffs in unrelated patches,pull-request-available,['C++'],ARROW,Improvement,Major,2017-10-25 16:21:18,14
13111749,[C++] Add linting script to look for C++/CLI issues,"This includes:

* Using {{nullptr}} in header files (we must instead use an appropriate macro to use {{__nullptr}} when the host compiler is C++/CLI)
* Including {{<mutex>}} in a public header (e.g. header files without ""impl"" or ""internal"" in their name)",pull-request-available,['C++'],ARROW,Improvement,Major,2017-10-24 16:46:38,14
13111559,[Python] Implement casts from timestamp to date32/date64 and support in Array.from_pandas,"When calling {{Array.from_pandas}} with a pandas.Series of dates and specifying the desired pyarrow type, an error occurs.  If the type is not specified then {{from_pandas}} will interpret the data as a timestamp type.

{code}
import pandas as pd
import pyarrow as pa
import datetime

arr = pa.array([datetime.date(2017, 10, 23)])
c = pa.Column.from_array(""d"", arr)

s = c.to_pandas()
print(s)
# 0   2017-10-23
# Name: d, dtype: datetime64[ns]

result = pa.Array.from_pandas(s, type=pa.date32())
print(result)
""""""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/array.pxi"", line 295, in pyarrow.lib.Array.__repr__ (/home/bryan/git/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:26221)
  File ""/home/bryan/.local/lib/python2.7/site-packages/pyarrow-0.7.2.dev21+ng028f2cd-py2.7-linux-x86_64.egg/pyarrow/formatting.py"", line 28, in array_format
    values.append(value_format(x, 0))
  File ""/home/bryan/.local/lib/python2.7/site-packages/pyarrow-0.7.2.dev21+ng028f2cd-py2.7-linux-x86_64.egg/pyarrow/formatting.py"", line 49, in value_format
    return repr(x)
  File ""pyarrow/scalar.pxi"", line 63, in pyarrow.lib.ArrayValue.__repr__ (/home/bryan/git/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:19535)
  File ""pyarrow/scalar.pxi"", line 137, in pyarrow.lib.Date32Value.as_py (/home/bryan/git/arrow/python/build/temp.linux-x86_64-2.7/lib.cxx:20368)
ValueError: year is out of range
""""""
{code}",pull-request-available,['Python'],ARROW,New Feature,Major,2017-10-23 22:55:21,14
13111508,"[Python] Implement pickling for Column, ChunkedArray, RecordBatch, Table","At the moment the types {{pyarrow.Column/ChunkedArray/RecordBatch/Table}} cannot be pickled. Although it may not be the fastest way to transport them from one process to another, it is a very comfortable one. We should implement a {{__reduce__()}} for all of them.",beginner pull-request-available,['Python'],ARROW,New Feature,Major,2017-10-23 18:54:22,2
13111272,[Python] flake8 checks still not failing builds,"I'm showing flake8 warnings in master, I'm not sure why these are still trickling through",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-23 02:38:41,14
13111173,[Python] StructArray.from_arrays should handle sequences that are coercible to arrays,Currently the arrays passed must be `pyarrow.Array` objects already.,pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-21 16:34:39,14
13111172,[Python] Create StructArray from sequence of dicts given a known data type,See https://github.com/apache/arrow/issues/1217,pull-request-available,['Python'],ARROW,New Feature,Major,2017-10-21 16:31:16,2
13111171,[GLib] Go example in test suite is broken,"see e.g. https://travis-ci.org/apache/arrow/jobs/290802235

{code}
~/build/apache/arrow/c_glib/example/go ~/build/apache/arrow/c_glib ~/build/apache/arrow
$GOPATH/bin/gir-generator	\
	-o $GOPATH/src/gir/arrow-1.0	\
	-config arrow-1.0/config.json	\
	arrow-1.0/arrow.go.in
go build -o read-batch read-batch.go
# gir/glib-2.0
/home/travis/gopath/src/gir/glib-2.0/glib.go:166: cannot convert (*(*[999999]*C.uint8_t)(unsafe.Pointer(ret1)))[i0] (type *C.uint8_t) to type uint8
/home/travis/gopath/src/gir/glib-2.0/glib.go:166: cannot use uint8((*(*[999999]*C.uint8_t)(unsafe.Pointer(ret1)))[i0]) (type uint8) as type *uint8 in assignment
make: *** [read-batch] Error 2
{code}",ci-failure pull-request-available,['GLib'],ARROW,Bug,Major,2017-10-21 16:00:04,1
13111170,[C++] Vendor exact version of jemalloc we depend on,"Since we are likely going to be using a patched jemalloc, we probably should not support using jemalloc with any other version, or relying on system packages. jemalloc would therefore always be built together with Arrow if {{ARROW_JEMALLOC}} is on

For this reason I believe we should vendor the code at the pinned commit as with Redis and other projects: https://github.com/antirez/redis/tree/unstable/deps",pull-request-available,['C++'],ARROW,Improvement,Major,2017-10-21 15:56:33,8
13111069,[GitHub] Add ISSUE_TEMPLATE.md,We're starting to get more issues opened on GitHub. It would be good to make these users aware that bugs and feature requests need to end up on JIRA,pull-request-available,['Documentation'],ARROW,New Feature,Major,2017-10-20 18:45:18,14
13111044,[C++] Add codec benchmarks,"This will also help users validate in release builds that the compression libraries have been built with the appropriate optimization levels, etc.",pull-request-available,['C++'],ARROW,New Feature,Major,2017-10-20 17:28:02,2
13110339,"Documentation generation script creates ""apidocs"" directory under site/java",I encountered this when running the doc build. The contents of apidocs should be rsync'd with the java directory in git,pull-request-available,['Java'],ARROW,Bug,Major,2017-10-18 16:02:05,14
13110304,[Python] Simplify user API for reading nested Parquet columns,It is not necessarily intuitive for users to have to specify the complete path to the leaf in the Parquet schema. See https://github.com/apache/arrow/issues/1207,pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-18 14:30:31,14
13110293,"[Python] Restore ""TimestampType"" to pyarrow namespace",It seems this was removed and maybe should not have been (being used in patches like https://github.com/apache/spark/pull/18664/files),pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-18 13:31:32,14
13110291,[Python] Add documentation / example for reading a directory of Parquet files on S3,Opened based on comment https://github.com/apache/arrow/pull/916#issuecomment-337563492,dataset dataset-parquet-read filesystem parquet pull-request-available,"['Documentation', 'Python']",ARROW,Improvement,Major,2017-10-18 13:19:28,13
13110290,[Python] Error writing with nulls in lists,"Created from https://github.com/apache/arrow/issues/1208

Hi,
Not sure if this is related or the same as ARROW-1584, but I can't seem to find a way to handle arrays of lists which occasionally consist of empty lists only.

To reproduce:

{code}
na = [] # None, [""""]

arrays = {
    'c1': pa.array([[""test""], na, na], type=pa.list_(pa.string())),
    'c2': pa.array([na, na, na], type=pa.list_(pa.string())),
}

rb = pa.RecordBatch.from_arrays(list(arrays.values()), list(arrays.keys()))
df = rb.to_pandas()

pa.serialize_pandas(df)
# > ArrowNotImplementedError: Unable to convert type: null

tbl = pa.Table.from_pandas(df)
sink = pa.BufferOutputStream()
writer = pa.RecordBatchFileWriter(sink, tbl.schema)
writer.write_table(tbl)
# > ArrowNotImplementedError: Unable to convert type: null
{code}

In my use case I'm processing data in batches where individual fields contain lists of strings. Some of the batches may, however, contain empty lists only. And there doesn't seem to be any representation in Arrow at the moment to deal with this situation.

Also, since I'm serializing the batches into a single file/stream, their schemas need to be consistent, which is why I tried explicitly specifying the type of the array as list_(string). The only workaround I've found is to replace empty lists with [""""], but that implies lots of unnecessary glue code on the client side. Is there a better workaround until this is fixed in an official conda release?",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-18 13:15:58,14
13109587,[C++] Correctly truncate oversized validity bitmaps when writing Feather format,"An extra {{0}} appears in the beginning when serializing and deserializing an array with more than {{128}} values and at least one {{NULL}} value using {{Feather}}. Once the extra {{0}} is inserted a value is trimmed at the end.

Here is the C++ code to write such an array:

{code:java}
#include <iostream>
#include <arrow/api.h>
#include <arrow/io/file.h>
#include <arrow/ipc/feather.h>
#include <arrow/pretty_print.h>

int main() {
  // 1. Build Array
  arrow::DoubleBuilder builder;
  for (int i = 0; i < 129; i++)
      if (i == 0)
          builder.AppendNull();
      else
          builder.Append(i);

  std::shared_ptr<arrow::Array> array;
  builder.Finish(&array);

  arrow::PrettyPrint(*array, 0, &std::cout);
  std::cout << std::endl;

  // 2. Write to Feather file
  std::shared_ptr<arrow::io::FileOutputStream> stream;
  arrow::io::FileOutputStream::Open(""out.f"", false, &stream);

  std::unique_ptr<arrow::ipc::feather::TableWriter> writer;
  arrow::ipc::feather::TableWriter::Open(stream, &writer);

  writer->SetNumRows(129);
  writer->Append(""id"", *array);

  writer->Finalize();
  stream->Close();

  return 0;
}
{code}

The output of running this code is:

{code:java}
# g++-4.9 -std=c++11 example.cpp -larrow && ./a.out
[null, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]
{code}

The array is deserialized in Python and looks like this:
 
{code:java}
>>> pandas.read_feather('out.f')
        id
0      NaN
1      0.0
2      1.0
3      2.0
4      3.0
5      4.0
6      5.0
7      6.0
8      7.0
9      8.0
10     9.0
11    10.0
12    11.0
13    12.0
14    13.0
15    14.0
16    15.0
17    16.0
18    17.0
19    18.0
20    19.0
21    20.0
22    21.0
23    22.0
24    23.0
25    24.0
26    25.0
27    26.0
28    27.0
29    28.0
..     ...
99    98.0
100   99.0
101  100.0
102  101.0
103  102.0
104  103.0
105  104.0
106  105.0
107  106.0
108  107.0
109  108.0
110  109.0
111  110.0
112  111.0
113  112.0
114  113.0
115  114.0
116  115.0
117  116.0
118  117.0
119  118.0
120  119.0
121  120.0
122  121.0
123  122.0
124  123.0
125  124.0
126  125.0
127  126.0
128  127.0

[129 rows x 1 columns]
{code}

Notice the {{0.0}} value on index {{1}}. The value should have been {{1.0}}. Also, the last value is {{127.0}} instead of {{128.0}}.
",pull-request-available,['C++'],ARROW,Bug,Major,2017-10-16 04:21:49,14
13109566,[Python] Use RecordBatch.from_pandas in FeatherWriter.write,"In addition to making the implementation simpler, we will also benefit from multithreaded conversions, so faster write speeds",pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-16 01:08:51,14
13109343,[C++] Change arrow::MakeArray to not return Status,It should not be possible for this function to fail. We can do a DCHECK internally of any internal Status values for testing purposes,pull-request-available,['C++'],ARROW,Improvement,Major,2017-10-13 19:22:26,14
13108963,[GLib] Support Meson,It's a base work for ARROW-1642. ,pull-request-available,['GLib'],ARROW,New Feature,Major,2017-10-12 15:56:00,1
13108117,[Python] Python 3.7 support,"Things to do for Python 3.7 (mostly depends on downstream):
||Task||Done||
|Pandas release (0.23.2)|(/)|
|manylinux1 container with Python 3.7|(/)|
|conda-forge python update (optional, can also use Anaconda version):https://github.com/conda-forge/python-feedstock/issues/177|(/)|
|conda-forge dependencies are built for Python 3.7 (and/or upstream manylinux1 wheels), see https://github.com/conda-forge/conda-forge-enhancement-proposals/pull/10 |(x)|

See discussion in [https://github.com/apache/arrow/issues/1125]",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-09 22:20:36,8
13107862,[Python] Out of bounds dictionary indices causes segfault after converting to pandas,"Minimal reproduction:

{code}
import numpy as np
import pandas as pd
import pyarrow as pa
 
num = 100
arr = pa.DictionaryArray.from_arrays(
    np.arange(0, num),
    np.array(['a'], np.object),
    np.zeros(num, np.bool),
    True)

print(arr.to_pandas())
{code}

At no time in the Arrow codebase do we validate that the dictionary indices are in bounds. It seems that pandas is overly trusting of the validity of the indices. So we should add a method someplace to validate that the dictionary non-null indices are not out of bounds (perhaps in {{CategoricalBlock::WriteIndices}}).

As an aside: there may be other times when doing analytics on categorical data that external data will have out of bounds index values. We should plan for these and decide whether to raise an exception or treat them as null",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-08 19:04:29,14
13107401,[Python] pa.DataType cannot be pickled,"In [26]: t
Out[26]: DataType(int64)

In [25]: pickle.dumps(t)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-25-f90063f6658b> in <module>()
----> 1 pickle.dumps(t)

/home/icexelloss/miniconda3/envs/spark-dev/lib/python3.5/site-packages/pyarrow/lib.cpython-35m-x86_64-linux-gnu.so in pyarrow.lib.DataType.__reduce_cython__()

TypeError: no default __reduce__ due to non-trivial __cinit__

This is discovered when trying to send a pa.DataType along with a udf in pyspark. The workaround is to send pyspark DataType and convert to pa.DataType. It would be nice to able to pickle pa.DataType.",pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-05 23:40:36,14
13107216,C++: Add cast from Dictionary[NullType] to NullType,Cleanup after https://issues.apache.org/jira/browse/PARQUET-1121,pull-request-available,['C++'],ARROW,New Feature,Major,2017-10-05 12:41:04,8
13107117,[Python] pyarrow.array cannot handle NumPy scalar types,"Example repro

{code}
In [1]: import pyarrow as pa
impo
In [2]: import numpy as np

In [3]: pa.array([np.random.randint(0, 10, size=5), None])
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-3-b364fa5d75aa> in <module>()
----> 1 pa.array([np.random.randint(0, 10, size=5), None])

/home/wesm/code/arrow/python/pyarrow/array.pxi in pyarrow.lib.array (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:24892)()
    171         if mask is not None:
    172             raise ValueError(""Masks only supported with ndarray-like inputs"")
--> 173         return _sequence_to_array(obj, size, type, pool)
    174 
    175 

/home/wesm/code/arrow/python/pyarrow/array.pxi in pyarrow.lib._sequence_to_array (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:23496)()
     23     if type is None:
     24         with nogil:
---> 25             check_status(ConvertPySequence(sequence, pool, &out))
     26     else:
     27         if size is None:

/home/wesm/code/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:7876)()
     75         message = frombytes(status.message())
     76         if status.IsInvalid():
---> 77             raise ArrowInvalid(message)
     78         elif status.IsIOError():
     79             raise ArrowIOError(message)

ArrowInvalid: /home/wesm/code/arrow/cpp/src/arrow/python/builtin_convert.cc:740 code: InferArrowTypeAndSize(obj, &size, &type)
/home/wesm/code/arrow/cpp/src/arrow/python/builtin_convert.cc:319 code: InferArrowType(obj, out_type)
/home/wesm/code/arrow/cpp/src/arrow/python/builtin_convert.cc:299 code: seq_visitor.Visit(obj)
/home/wesm/code/arrow/cpp/src/arrow/python/builtin_convert.cc:180 code: VisitElem(ref, level)
Error inferring Arrow data type for collection of Python objects. Got Python object of type ndarray but can only handle these types: bool, float, integer, date, datetime, bytes, unicode
{code}

If these inner values are converted to Python built-in int types then it works fine",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-05 02:33:28,8
13107102,[C++][Parquet] Read and write nested Parquet data with a mix of struct and list nesting levels,"We have many nested parquet files generated from Apache Spark for ranking problems, and we would like to load them in python for other programs to consume. 

The schema looks like 
{code:java}
root
 |-- profile_id: long (nullable = true)
 |-- country_iso_code: string (nullable = true)
 |-- items: array (nullable = false)
 |    |-- element: struct (containsNull = false)
 |    |    |-- show_title_id: integer (nullable = true)
 |    |    |-- duration: double (nullable = true)
{code}

And when I tried to load it with nightly build pyarrow on Oct 4, 2017, I got the following error.
{code:python}
Python 3.6.2 |Anaconda, Inc.| (default, Sep 30 2017, 18:42:57) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import numpy as np
>>> import pandas as pd
>>> import pyarrow as pa
>>> import pyarrow.parquet as pq
>>> table2 = pq.read_table('part-00000')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/parquet.py"", line 823, in read_table
    use_pandas_metadata=use_pandas_metadata)
  File ""/home/dbt/miniconda3/lib/python3.6/site-packages/pyarrow/parquet.py"", line 119, in read
    nthreads=nthreads)
  File ""_parquet.pyx"", line 466, in pyarrow._parquet.ParquetReader.read_all
  File ""error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: lists with structs are not supported.
{code}

I somehow get the impression that after https://issues.apache.org/jira/browse/PARQUET-911 is merged, we should be able to load the nested parquet in pyarrow. 

Any insight about this? 

Thanks.",parquet pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2017-10-05 00:43:40,15
13106966,[C++] Do not include <mutex> in public headers,This is a part of ARROW-1134,pull-request-available,['C++'],ARROW,Improvement,Major,2017-10-04 15:10:54,14
13106862,Resolve OpenSSL issues in Travis CI,"Following error occurs:

error: downloading
  'https://github.com/google/snappy/releases/download/1.1.3/snappy-1.1.3.tar.gz'
  failed
    status_code: 1
    status_string: ""Unsupported protocol""
    log: Protocol ""https"" not supported or disabled in libcurl
  Closing connection -1

",pull-request-available,['Continuous Integration'],ARROW,Bug,Major,2017-10-04 05:48:10,14
13106808,[Format] Integration tests for null type,"This was not implemented on the C++ side, and came up in ARROW-1584. Realistically arrays may be of null type, and we should be able to message these correctly",columnar-format-1.0 pull-request-available,"['C++', 'Integration', 'Java']",ARROW,New Feature,Blocker,2017-10-03 21:28:10,14
13106726,Add release management guide for PMCs,"The Arrow release process has grown increasingly complex and time consuming, and there are many manual steps that a release manager must undertake before, during, and after a release vote. I will write a document to list all of these. Some of them desperately need to be more automated (like updating the online API documentation and website). Binary packaging is still not as automated as it could be.",pull-request-available,['Documentation'],ARROW,Improvement,Major,2017-10-03 16:09:44,14
13106609,[Website] Updates for 0.7.1 release,Since this is a bugfix release I'm going to skip the blog post portion and merely update the website changelog and front matter,pull-request-available,['Website'],ARROW,Improvement,Major,2017-10-03 03:07:54,14
13106485,"[Python] numpy ""unicode"" arrays not understood","

{code}
import numpy as np
pa.StringArray.from_pandas(np.empty(1, np.unicode))
{code}

Throws:

{noformat}
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-68-f9bc946f2c0a> in <module>()
      1 import numpy as np
----> 2 pa.StringArray.from_pandas(np.empty(1, np.unicode))

array.pxi in pyarrow.lib.Array.from_pandas()

error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: Unsupported numpy type 19
{noformat}

np.object arrays work, though...",pull-request-available,['Python'],ARROW,Bug,Major,2017-10-02 17:17:13,14
13106412,[Python] Permit categorical conversions in Table.to_pandas on a per-column basis,Currently this is all or nothing,pull-request-available,['Python'],ARROW,Improvement,Major,2017-10-02 13:59:44,8
13106255,[C++] Fix problematic code paths identified by infer tool,I'm making a pass over the output from ARROW-1626,pull-request-available,['C++'],ARROW,Bug,Major,2017-09-30 20:37:31,14
13106136,"[C++] Follow up fixes / tweaks to compiler warnings for Plasma / LLVM 4.0, add to readme",Not sure why these failures weren't caught in CI,pull-request-available,['C++'],ARROW,Bug,Major,2017-09-29 20:49:07,14
13105565,Python: Download Boost in manylinux1 build from bintray,"Sourceforge often fails, so use the alternative source. See also https://github.com/ray-project/ray/pull/1019 for this.",pull-request-available,['Python'],ARROW,Bug,Major,2017-09-27 20:54:19,8
13104736,Python: Windows wheels don't include .lib files.,These files are needed to link against {{pyarrow}} natively. Surfaced in https://ci.appveyor.com/project/MathMagique/turbodbc/build/master%20-%20%23297/job/wg9gxnfol9ynkmao / https://github.com/blue-yonder/turbodbc/pull/125,pull-request-available,['Python'],ARROW,Bug,Major,2017-09-25 11:18:20,14
13104591,[C++] Add BinaryArray method to get a value as a std::string,"e.g.

{{std::string val = arr.GetString(i);}}

This would improve usability for libraries that work with collections of std::string",pull-request-available,['C++'],ARROW,Improvement,Major,2017-09-24 16:22:46,14
13104551,[C++] READ_NEXT_BITSET reads one byte past the last byte on last iteration,"(int) byte_offset_valid_bits = 131072
(int) bit_offset_valid_bits = 0
(uint8_t) bitset_valid_bits = '?'
(int) i = 1048575

(lldb) print *num_values
(int64_t) $0 = 1048576

(lldb) print valid_bits[131072]
error: Couldn't apply expression side effects : Couldn't dematerialize a result variable: couldn't read its memory
(lldb) print valid_bits[131071]
(uint8_t) $6 = '?'

1048576 / 8 = 131072 (number of values in the array)

Last readable offset is 131071.

In the last iteration, READ_NEXT_BITSET reads one byte past the last byte of the valid_bits array.

When this allocation happens at the end of a block, a crash occurs.

These macros are used in loops in several places in Arrow and Parquet.

parquet-cpp/src/parquet/arrow/writer.cc:

Status GenerateLevels(const Array& array, const std::shared_ptr<Field>& field,
                        int64_t* values_offset, ::arrow::Type::type* values_type,
                        int64_t* num_values, int64_t* num_levels,
                        std::shared_ptr<Buffer>* def_levels,
                        std::shared_ptr<Buffer>* rep_levels,
                        std::shared_ptr<Array>* values_array) {
[....]
          const uint8_t* valid_bits = array.null_bitmap_data();
          INIT_BITSET(valid_bits, static_cast<int>(array.offset()));
          for (int i = 0; i < array.length(); i++) {
            if (bitset_valid_bits & (1 << bit_offset_valid_bits)) {
              def_levels_ptr[i] = 1;
            } else {
              def_levels_ptr[i] = 0;
            }
            READ_NEXT_BITSET(valid_bits);  <-- crashes here on last iteration
          }

arrow/util/bitutil.h:

#define INIT_BITSET(valid_bits_vector, valid_bits_index)        \
  int byte_offset_##valid_bits_vector = (valid_bits_index) / 8; \
  int bit_offset_##valid_bits_vector = (valid_bits_index) % 8;  \
  uint8_t bitset_##valid_bits_vector = valid_bits_vector[byte_offset_##valid_bits_vector];

#define READ_NEXT_BITSET(valid_bits_vector)                                          \
  bit_offset_##valid_bits_vector++;                                                  \
  if (bit_offset_##valid_bits_vector == 8) {                                         \
    bit_offset_##valid_bits_vector = 0;                                              \
    byte_offset_##valid_bits_vector++;                                               \
    bitset_##valid_bits_vector = valid_bits_vector[byte_offset_##valid_bits_vector]; \
  }

A quick fix is to allocate one more byte for null_bitmap_ in ArrayBuilder::Init and ArrayBuilder::Resize in arrow/cpp/src/arrow/builder.cc. 

The capacity of null_bitmap() is changed by this fix.

The following tests FAILED:
	  2 - array-test (Failed)
	 14 - feather-test (Failed)

A more extensive fix would require changing how INIT_BITSET and READ_NEXT_BITSET operate where they are used in Parquet and Arrow.


To reproduce this problem:

1) Download the CSV file.

Source: https://catalog.data.gov/dataset?res_format=CSV

State Drug Utilization Data 2016
https://data.medicaid.gov/api/views/3v6v-qk5s/rows.csv?accessType=DOWNLOAD

2) Run FileConvert (see https://github.com/renesugar/FileConvert)

./bin/FileConvert -i ./State_Drug_Utilization_Data_2016.csv -o ./State_Drug_Utilization_Data_2016.parquet 
",crash pull-request-available,['C++'],ARROW,Bug,Major,2017-09-24 05:13:54,14
13104544,[C++] Zero-copy Buffer constructor from std::string,"There are instances where we may process an Arrow stream stored in an {{std::string}}, and there's a bit of typing to construct a {{io::BufferReader}} from an {{std::string}}, so we can make this simpler",pull-request-available,['C++'],ARROW,Improvement,Major,2017-09-23 21:58:28,14
13104357,[C++][Parquet] Unable to read Parquet files with list inside struct,"Is PyArrow currently unable to read in Parquet files with a vector as a column? For example, the schema of such a file is below:

{{<pyarrow._parquet.ParquetSchema object at 0x7f2d42493c88>
mbc: FLOAT
deltae: FLOAT
labels: FLOAT
features.type: INT32 INT_8
features.size: INT32
features.indices.list.element: INT32
features.values.list.element: DOUBLE}}

Using either pq.read_table() or pq.ParquetDataset('/path/to/parquet').read() yields the following error: ArrowNotImplementedError: Currently only nesting with Lists is supported.

From the error I assume that this may be implemented in further releases?
",parquet,"['C++', 'Python']",ARROW,Bug,Major,2017-09-22 17:59:24,15
13104025,[Python] Fix package dependency issues causing build failures,"We are installing package requirements for the Python build in two steps, and the second step is causing conda to downgrade NumPy, resulting in an ABI conflict and broken build. I'm not sure why this suddenly started happening, but installing the packages all at once and pinning the NumPy version should fix it

https://travis-ci.org/apache/arrow/jobs/278202858#L9106",ci-failure pull-request-available,['Python'],ARROW,Bug,Major,2017-09-21 17:15:19,14
13104011,[PYTHON] serialize_pandas should pass through the preserve_index keyword,"I'm doing some benchmarking of Arrow serialization for dask.distributed to serialize dataframes.

Overall things look good compared to the current implementation (using pickle). The biggest difference was pickle's ability to use pandas' RangeIndex to avoid serializing the entire Index of values when possible.

I suspect that a ""range type"" isn't in scope for arrow, but in the meantime applications using Arrow could detect the `RangeIndex`, and pass {{ pyarrow.serialize_pandas(df, preserve_index=False) }} ",pull-request-available,['Python'],ARROW,Improvement,Minor,2017-09-21 16:37:41,14
13103903,C++: Xcode 9 is not correctly detected,See https://github.com/ray-project/ray/issues/1000,pull-request-available,['C++'],ARROW,Bug,Major,2017-09-21 11:52:58,8
13103748,[PYTHON] serialize_pandas on empty dataframe,"This code

{code:python}
import pandas as pd
import pyarrow as pa

pa.serialize_pandas(pd.DataFrame())
{code}

Raises

{code}
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-71-ad21add45f0d> in <module>()
----> 1 pa.serialize_pandas(pd.DataFrame())

~/Envs/dask-dev/lib/python3.6/site-packages/pyarrow/ipc.py in serialize_pandas(df)
    158     sink = pa.BufferOutputStream()
    159     writer = pa.RecordBatchStreamWriter(sink, batch.schema)
--> 160     writer.write_batch(batch)
    161     writer.close()
    162     return sink.get_result()

pyarrow/ipc.pxi in pyarrow.lib._RecordBatchWriter.write_batch (/Users/travis/build/apache/arrow-dist/arrow/python/build/temp.macosx-10.6-intel-3.6/lib.cxx:59238)()

pyarrow/error.pxi in pyarrow.lib.check_status (/Users/travis/build/apache/arrow-dist/arrow/python/build/temp.macosx-10.6-intel-3.6/lib.cxx:8113)()

ArrowNotImplementedError: Unable to convert type: null

{code}

Presumably {{pa.deserialize_pandas}} will need a fix as well.",pull-request-available python serialization,['Python'],ARROW,Bug,Minor,2017-09-20 21:59:00,14
13103674,[Python] Set up + document nightly conda builds for macOS,It's already been great to be able to test the nightlies on Linux in conda; it would be great to be able to do the same on macOS,nightly,['Python'],ARROW,Improvement,Major,2017-09-20 17:44:42,3
13103585,[C++/Python] Run lint checks in Travis CI to fail for linting issues as early as possible,"The lint checks are run relatively late in the CI process, and a build may fail after holding a worker for ~20 minutes or more. These could fail much sooner and free up build slaves",pull-request-available,"['C++', 'Python']",ARROW,Improvement,Major,2017-09-20 12:25:06,14
13103473,[Python] Add pyarrow.column factory function,This would internally call {{Column.from_array}} as appropriate,pull-request-available,['Python'],ARROW,New Feature,Major,2017-09-20 01:01:27,14
13103438,[C++] Implement stateful kernel function that uses DictionaryBuilder to compute dictionary indices,An operator utilizing this kernel may need some way to indicate to multithreaded schedulers that it cannot be parallelized on chunked arrays (unless we implement a concurrent hash table),Analytics,['C++'],ARROW,New Feature,Major,2017-09-19 23:06:02,14
13103437,"[C++] Implement ""value counts"" kernels for tabulating value frequencies","This is related to ""match"", ""isin"", and ""unique"" since hashing is generally required",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2017-09-19 23:02:36,15
13103428,[C++] Define API for creating a kernel instance from function of scalar input and output with a particular signature,"This could include an {{std::function}} instance (but these cannot be inlined by the C++ compiler), but should also permit use with inline-able functions or functors",Analytics,['C++'],ARROW,New Feature,Major,2017-09-19 22:40:50,14
13103408,[C++] Implement logical unary and binary kernels for boolean arrays,"And, or, not (negate), xor",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2017-09-19 22:09:21,8
13103394,[C++] Implement boolean selection kernels,"Select values where a boolean selection array is true. As a default, if any values in the selection are null, then values in the output array should be null. 

The null behaviour does not need to be toggable, if the user wants to select nothing in the case of null, then it is necessary to call selection_array.fillna(false) first.",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2017-09-19 21:49:13,6
13103288,[C++] Incorporate AssertArraysEqual function from PARQUET-1100 patch,see discussion in https://github.com/apache/parquet-cpp/pull/398,pull-request-available,['C++'],ARROW,Improvement,Major,2017-09-19 16:38:35,14
13103190,[Python] Document that pip wheels depend on MSVC14 runtime,"I just tried pyarrow on Windows 10, and it fails to import for me:

{code}
>>> import pyarrow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python36\lib\site-packages\pyarrow\__init__.py"", line 32, in <module>
    from pyarrow.lib import cpu_count, set_cpu_count
ImportError: DLL load failed: The specified module could not be found.
{code}

Not sure which DLL is failing, but I do see some DLLs in the pyarrow folder:

{code}
C:\Users\dima\Documents>dir ""C:\Program Files\Python36\lib\site-packages\pyarrow\""
 Volume in drive C has no label.
 Volume Serial Number is 4CE9-CC3C

 Directory of C:\Program Files\Python36\lib\site-packages\pyarrow

09/19/2017  01:14 AM    <DIR>          .
09/19/2017  01:14 AM    <DIR>          ..
09/19/2017  01:14 AM         2,382,336 arrow.dll
09/19/2017  01:14 AM           604,160 arrow_python.dll
09/19/2017  01:14 AM             3,402 compat.py
...
{code}

However, I cannot open them using ctypes.cdll. I wonder if some dependency is missing?

{code}
>>> open('C:\\Program Files\\Python36\\Lib\\site-packages\\pyarrow\\parquet.dll', 'rb')
<_io.BufferedReader name='C:\\Program Files\\Python36\\Lib\\site-packages\\pyarrow\\parquet.dll'>
>>>
>>> cdll.LoadLibrary('C:\\Program Files\\Python36\\Lib\\site-packages\\pyarrow\\parquet.dll')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python36\lib\ctypes\__init__.py"", line 426, in LoadLibrary
    return self._dlltype(name)
  File ""C:\Program Files\Python36\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found
{code}
",pull-request-available,['Python'],ARROW,Bug,Major,2017-09-19 08:50:41,14
13102978,[Python] Fix flaky test on Windows ,See failure in https://ci.appveyor.com/project/ApacheSoftwareFoundation/arrow/build/1.0.3408/job/6b0xfl341f1rqjyl,pull-request-available,['Python'],ARROW,Bug,Major,2017-09-18 14:53:53,14
13102848,[GLib] Support build append in builder,It improves performance.,pull-request-available,['GLib'],ARROW,New Feature,Major,2017-09-17 23:45:50,1
13102816,[GLib] Support GLib 2.40 again,"Ubuntu 14.04 ships GLib 2.40. If we support Ubuntu 14.04, it's better that we support GLib 2.40.",pull-request-available,['GLib'],ARROW,Improvement,Minor,2017-09-17 11:43:58,1
13102527,[C++] row_wise_conversion example doesn't correspond to ListBuilder constructor arguments,"In the row_wise_conversion code example we find:

{code}
    std::shared_ptr<DoubleBuilder> components_values_builder =
        std::make_shared<DoubleBuilder>(arrow::default_memory_pool());
    arrow::ListBuilder components_builder(arrow::default_memory_pool(),
        components_values_builder);
{code}

This generates some compile time errors since the second argument of the ListBuilder constructor expects a unique pointer.

However, I guess the example shows the correct case, where we want to still be able to use the DoubleBuilder after constructing the ListBuilder, so I probably shouldn't std::move it to the constructor as a unique_ptr.

I'm not sure how to fix this as I don't know which functionality is desired.",pull-request-available,['C++'],ARROW,Bug,Trivial,2017-09-15 10:54:37,14
13102454,[C++] Windows release verification script should not modify conda environment,I will submit a patch to fix,pull-request-available,['C++'],ARROW,Bug,Major,2017-09-15 02:52:27,14
13102450,[C++] Race condition with arrow_gpu ,"The Arrow GPU/CUDA build has a race condition with the generated Flatbuffers files. We must add a dependency so that the Flatbuffers files are compiled before the relevant arrow_gpu units are compiled.

Encountered this when testing 0.7.0 RC0. Because arrow_gpu is alpha I don't think this should block",pull-request-available,['C++'],ARROW,Bug,Major,2017-09-15 02:31:04,14
13102443,[C++] Fix valgrind warnings in cuda-test if possible,Running cuda-test with {{-DARROW_TEST_MEMCHECK=on}} fails with RC0. Not a big issue for 0.7.0 but may be worth investigating,pull-request-available,['C++'],ARROW,Bug,Major,2017-09-15 01:30:39,14
13102108,[C++] Support building with full path install_name on macOS,"The current libarrow.dylib uses @rpath/libarrow.0.dylib for install_name. It works well when we can set DYLD_LIBRARY_PATH environment variable or libarrow.dylib is installed into the standard path such as /usr/local/lib/.

There are some cases that we can't set DYLD_LIBRARY_PATH. For example, we can't set DYLD_LIBRARY_PATH when we use libarrow.0.dylib via a shell script. Because the recent macOS doesn't inherit DYLD_LIBRARY_PATH for security reason. It's caused as System Integration Protection (SIP). We need to use libarrow.0.dylib via a shell script when we build Arrow GLib's *.gir files. It means that we need to install Arrow C++ into the standard path for building Arrow GLib on macOS. If we install Arrow C++ into the non-standard path such as ~/local/, we can't build Arrow GLib.

If we use full path for libarrow.dylib's install_name, we don't need to set DYLD_LIBRARY_PATH. It means that we can build Arrow GLib with Arrow C++ installed into the non-standard path such as ~/local/.",pull-request-available,['C++'],ARROW,Improvement,Minor,2017-09-14 00:03:52,1
13100918,[GitHub] Add CONTRIBUTING.md and ISSUE_TEMPLATE.md,We have enabled Issues on GitHub as a lighter weight way for users to engage with the Arrow developers. We should add the appropriate GitHub markdown documents so that users are directed to take the appropriate actions should they wish to contribute a patch or file a feature request in JIRA,pull-request-available,['Documentation'],ARROW,Improvement,Major,2017-09-08 20:53:36,14
13100663,[C++] Type casting function kernel suite,Tracking issue to collect related casting issues,Analytics,['C++'],ARROW,New Feature,Major,2017-09-08 02:32:42,14
13100646,[C++] Implement ArrayBuilder::Finish in terms of internal::ArrayData,"In {{arrow::compute}} we will frequently be working with unboxed internal array data, so it would be useful for the builders to be able to return the internal data container. This may also simplify some code in arrow/builder.cc",pull-request-available,['C++'],ARROW,Improvement,Major,2017-09-08 01:11:45,14
13100621,[C++] Decide if arrow::RecordBatch needs to be copyable,"Since this data structure is immutable, it is unclear if there are any benefits to being copyable",pull-request-available,['C++'],ARROW,New Feature,Major,2017-09-07 23:03:05,14
13099488,[Python] Run s3fs unit tests in Travis CI,"We'll need to set up an S3 bucket to write to with credentials that cannot compromise anyone's AWS account. I've been testing locally with a user that I set up but I wouldn't be comfortable checking in these credentials, even in encrypted form, without more scrutiny",filesystem pull-request-available,['Python'],ARROW,Improvement,Major,2017-09-03 21:53:50,3
13099483,[Python] More informative error message when attempting to write an unsupported Arrow type to Parquet format,See https://github.com/pandas-dev/pandas/issues/17102#issuecomment-326746184,pull-request-available,['Python'],ARROW,Improvement,Major,2017-09-03 20:37:07,14
13098092,[Python] Document semantic differences between Spark timestamps and Arrow timestamps,"The way that Spark treats non-timezone-aware timestamps as session local can be problematic when using pyarrow which may view the data coming from toPandas() as time zone naive (but with fields as though it were UTC, not session local). We should document carefully how to properly handle the data coming from Spark to avoid problems.

cc [~bryanc] [~holdenkarau]",pull-request-available,['Python'],ARROW,Improvement,Major,2017-08-28 21:23:16,15
13097194,"[Format] Use for ""page"" attribute in Buffer in metadata","This attribute is currently unused in any Arrow implementation. I think the original idea is that the ""page"" might indicate a particular shared memory page, so that a record batch could be spread across multiple memory regions.

The downside of this unused attribute is that Buffer metadata takes 24 bytes instead of 16 due to padding. ",pull-request-available,['Format'],ARROW,Bug,Major,2017-08-24 02:19:15,14
13095216,"[Website] Add ""Powered By"" page to the website",See http://spark.apache.org/powered-by.html as an example. It would be useful to collect a list of projects which are using Arrow in some form,pull-request-available,['Website'],ARROW,Improvement,Major,2017-08-17 03:16:38,8
13093555,"[Python] Conversion from nested NumPy arrays fails on integers other than int64, float32","The inferred types are the largest ones, and then later conversion fails on any arrays with smaller types because only exact conversions are implemented thus far",pull-request-available,['Python'],ARROW,Bug,Major,2017-08-09 18:09:10,14
13092244,"[C++] Define ""virtual table"" interface","The idea is that a virtual table may reference Arrow data that is not yet available in memory. The implementation will define the semantics of how columns are loaded into memory. 

A virtual column interface will need to accompany this. For example:

{code:language=c++}
std::shared_ptr<VirtualTable> vtable = ...;
std::shared_ptr<VirtualColumn> vcolumn = vtable->column(i);
std::shared_ptr<Column> = vcolumn->Materialize();
std::shared_ptr<Table> = vtable->Materialize();
{code}",dataframe,['C++'],ARROW,New Feature,Major,2017-08-03 16:54:57,14
13091928,[C++] Support ARROW_BOOST_VENDORED on Windows / MSVC,Follow up to ARROW-1303,pull-request-available windows,['C++'],ARROW,Improvement,Major,2017-08-02 15:47:54,1
13090217,[C++] Implement Fixed Size List type,"At the moment, we only support lists with a variable size per entry. In some cases, each entry of a list column will have the same number of elements. In this case, we can use a more effective data structure as well as do certain optimisations on the operations of this type. To implement this type:

* Describe the memory structure of it in Layout.md
* Add the type to the enums in the C++ code
* Add FixedSizeListArray, FixedSizeListType and FixedSizeListBuilder classes to the C++ library",beginner pull-request-available,['C++'],ARROW,New Feature,Major,2017-07-26 14:38:32,6
13089371,[Python] Define API for user type checking of array types,"We have some subclasses of {{pyarrow.lib.DataType}}, but we haven't been designing with the intent of writing {{isinstance(arr.type, pyarrow.TimestampType)}}. We should think about the public API for such type-checking or other type of schema validation. ",pull-request-available,['Python'],ARROW,New Feature,Major,2017-07-24 00:11:22,14
13087472,[C++] Improve / correct doxygen function documentation in arrow::ipc,In looking over headers I spotted some things in ipc/reader.h and ipc/writer.h that could be improved. All functions and classes should also have {{\brief}} annotations to improve the appearance of the generated documentation,pull-request-available,['C++'],ARROW,Improvement,Major,2017-07-17 04:06:42,14
13086908,[Python] Enable s3fs to be used with ParquetDataset and reader/writer functions,"Pyarrow dataset function can't read from s3 using s3fs as the filesystem. Is  there a way we can add the support for read from s3 based on partitioned files ?

I am trying to address the problem mentioned in the stackoverflow link :
https://stackoverflow.com/questions/45082832/how-to-read-partitioned-parquet-files-from-s3-using-pyarrow-in-python",pull-request-available,['Python'],ARROW,Improvement,Minor,2017-07-13 14:00:36,14
13086623,[C++] Implement converter between Arrow record batches and Avro records,This would be useful for streaming systems that need to consume or produce Avro in C/C++,pull-request-available,['C++'],ARROW,New Feature,Major,2017-07-12 16:05:02,15
13086387,[C++] Implement Map logical type,A map is implemented as a list of structs with fields key and value. We should separately discuss whether this merits an addition to the Arrow metadata ,pull-request-available,['C++'],ARROW,New Feature,Major,2017-07-11 21:46:38,6
13084609,[Java] Dictionary.equals is not working correctly,The {{Dictionary.equals}} method does not return True when the dictionaries are equal.  This is because {{equals}} is not implemented for FieldVector and so that comparison defaults to comparing the two objects only and not the vector data.,pull-request-available,['Java'],ARROW,Bug,Major,2017-07-04 18:07:59,16
13084092,[Python] Create alternative to Table.from_pandas that yields a list of RecordBatch objects with a given chunk size,Will provide a fix for ARROW-1167,pull-request-available,['Python'],ARROW,Improvement,Major,2017-07-02 22:23:45,14
13084022,[GLib] Investigate root cause of ListArray glib test failure,See https://travis-ci.org/apache/arrow/jobs/248738468 and comments in https://github.com/apache/arrow/pull/790,ci-failure,['GLib'],ARROW,Bug,Major,2017-07-01 22:11:23,1
13083732,[C++] Segmentation faults on Fedora 24 with pyarrow-manylinux1 and self-compiled turbodbc,"Original issue: https://github.com/blue-yonder/turbodbc/issues/102

When using the {{pyarrow}}{{manylinux1}}Wheels to build Turbodbc on Fedora 24, the {{turbodbc_arrow}}unittests segfault. The main environment attribute here is that the compiler version used for building Turbodbc is newer than the one used for Arrow.",pull-request-available,['C++'],ARROW,Bug,Major,2017-06-30 09:59:02,8
13081467,[C++] Allow C++/CLI projects to build with Arrow,"Currently, the inclusion of <mutex> in some of Arrow's C++ headers prevents C++/CLI code from building against it.

From a C++/CLI project:

#include <arrow/io/file.h>
...

""#error directive: <mutex> is not supported when compiling with /clr or /clr:pure.""

This could be patched by optionally relying on Boost's mutex/lock_guard instead of std, or not exposing the #include <mutex> publically.",pull-request-available,['C++'],ARROW,Improvement,Minor,2017-06-21 15:55:09,14
13079225,[C++] Create Record Batch Builder class as a reusable and efficient way to transpose row-by-row data to columns,"When dealing with IO interfaces that yield one record at a time, a common task will be appending each element to a sequence. ",pull-request-available,['C++'],ARROW,New Feature,Major,2017-06-12 14:35:18,14
13077044,[Python] add get_include to expose directory containing header files,"{{numpy.get_include}} streamlines extension development:

{code}
 import numpy as np
        ...
        Extension('extension_name', ...
                include_dirs=[np.get_include()])
{code}

It would be nice if pyarrow had the same thing",pull-request-available,['Python'],ARROW,Improvement,Minor,2017-06-03 02:47:41,14
13072201,[Python] Add ASV benchmarks for streaming columnar deserialization,We need to carefully monitor the performance of critical operations like streaming format to pandas wall clock time a la http://wesmckinney.com/blog/arrow-streaming-columnar/,pull-request-available,['Python'],ARROW,Improvement,Major,2017-05-15 20:34:17,2
13071780,[Python] Add documentation about using pyarrow from other Cython and C++ projects,"Follow up work to ARROW-819, ARROW-714",pull-request-available,['Python'],ARROW,Improvement,Major,2017-05-13 19:45:52,2
13071629,[C++] Implement input stream and output stream with Gzip codec,"After incorporating the compression code and toolchain from parquet-cpp, we should be able to add a codec layer for on-the-fly compression and decompression",csv pull-request-available,['C++'],ARROW,New Feature,Major,2017-05-12 18:43:38,2
13071628,"[C++] Add option to create FileOutputStream, ReadableFile from OS file descriptor",Currently we require a file path. It should also be possible to initialize from a file descriptor,pull-request-available,['C++'],ARROW,New Feature,Major,2017-05-12 18:41:26,2
13069847,[Website] Add FAQ page about project,"As some suggested initial topics for the FAQ:

* How Apache Arrow is related to Apache Parquet (the difference between a ""storage format"" and an ""in-memory format"" causes confusion)
* How is Arrow similar to / different from Flatbuffers and Cap'n Proto
* How Arrow uses Flatbuffers (I have had people incorrectly state to me things like ""Arrow is just Flatbuffers under the hood"")

Any other ideas?",pull-request-available,['Website'],ARROW,New Feature,Major,2017-05-08 01:04:06,4
13069845,[C++/Python] Implement Array.isvalid/notnull/isnull as scalar functions,"For arrays with nulls, this amounts to returning the validity bitmap. Without nulls, an array of all 1 bits must be constructed. For isnull, the bits must be flipped (in this case, the un-set part of the new bitmap must stay 0, though).",dataframe pull-request-available,['C++'],ARROW,Improvement,Major,2017-05-08 00:55:10,6
13068564,Support integration testing on Python 2.7,Currently {{integration_test.py}} is Python 3 only,pull-request-available,"['Integration', 'Python']",ARROW,Improvement,Major,2017-05-02 21:44:32,14
13067829,[Python] Account for multiarch systems in development.rst,Some systems will install libraries in lib64,pull-request-available,['Python'],ARROW,Improvement,Major,2017-04-28 22:07:36,14
13067616,libjemalloc.so.2: cannot open shared object file: ,">>> import pyarrow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/default/src/venv/lib/python2.7/site-packages/pyarrow-0.2.1.dev244+g14bec24-py2.7-linux-x86_64.egg/pyarrow/__init__.py"", line 28, in <module>
    import pyarrow._config
ImportError: libjemalloc.so.2: cannot open shared object file: No such file or directory

$LD_LIBRARY_PATH has libarrow_jemalloc.a along with other libraries including libarrow.so,  libparquet.so, libparquet_arrow.so. Pyarrow was built using ----with-jemalloc and parquet-cpp was cmake-d with 
-DPARQUET_ARROW=ON  

Also, noticed that arrow/python documentation has been cleaned up with the installation instructions having the coda approach only .Is this the only supported way going forward ?
",pyarrow,['C++'],ARROW,Bug,Major,2017-04-28 07:15:46,8
13067146,[C++] Serialize Field metadata to IPC metadata,Follow up work to ARROW-898,pull-request-available,['C++'],ARROW,Improvement,Major,2017-04-26 19:04:24,3
13067033,[C++] Build C++ project including thirdparty dependencies from local tarballs,"Could there be a flag or similar to make offline building of the project easier (or maybe that is in place and I don't see it)?  

Something that perhaps in CMakeLists.txt that has the option for the URL to be a file:// path? 

This would help me in that I could download all relevant tarballs and have the same build happen offline or online.",pull-request-available,['C++'],ARROW,Wish,Minor,2017-04-26 11:33:23,14
13066524,[C++] Implement arrow::PrettyPrint for ChunkedArray,"At the moment {{repr}} only outputs the default {{""<pyarrow.lib.ChunkedArray at 0x7ff48e1d9c30>""}} whereas the underlying {{pyarrow.Array}} class has an informative output.",beginner pull-request-available,['Python'],ARROW,Improvement,Major,2017-04-24 23:40:02,8
13064470,"[C++] Implement Schema unification, merging unequal but equivalent schemas","Some Parquet datasets may contain schemas with mixed REQUIRED/OPTIONAL repetition types. While such schemas aren't strictly equal, we will need to consider them equivalent on the read path",dataset parquet pull-request-available,['C++'],ARROW,New Feature,Major,2017-04-17 20:00:10,13
13064310,[Python] Handle more kinds of null sentinel objects from pandas 0.x,Follow-on work to ARROW-707. See https://github.com/pandas-dev/pandas/blob/master/pandas/_libs/lib.pyx#L193 and discussion in https://github.com/apache/arrow/pull/554,pull-request-available,['Python'],ARROW,Improvement,Major,2017-04-17 01:22:26,14
13064305,[Python] Provide Python API for creating user-defined data types that can survive Arrow IPC,"The user will provide:

* Data type subclass that can indicate the physical storage type
* ""get state"" and ""set state"" functions for serializing custom metadata to bytes
* An optional function for ""boxing"" scalar values from the physical array storage

Internally, this will build on an analogous C++ API for defining user data types",pull-request-available,['Python'],ARROW,New Feature,Major,2017-04-16 23:38:46,2
13064302,[Python] Efficient construction of arrays from non-pandas 1D NumPy arrays,This is follow on work to ARROW-825,pull-request-available,['Python'],ARROW,New Feature,Major,2017-04-16 22:58:15,14
13064248,[Format] Add Timedelta type to describe time intervals,"xref https://github.com/apache/arrow/pull/551 and https://github.com/apache/arrow/pull/551#issuecomment-294325969

this will allow round-tripping of pandas ``Timedelta`` and numpy ``timedelt64[ns]`` types. The will have a similar TimeUnit to TimestampType (s, us, ms, ns). Possible impl include making this pure 64-bit.

",columnar-format-1.0 pull-request-available,['Format'],ARROW,Improvement,Minor,2017-04-16 00:42:02,15
13061965,[C++] Implement take kernel functions,"Among other things, this can be used to convert from DictionaryArray back to dense array. This is equivalent to {{ndarray.take}} in NumPy",Analytics columnar-format-1.0,['C++'],ARROW,New Feature,Major,2017-04-06 03:41:29,6
13061285,[C++] Adopt FileSystem abstraction,"See, e.g. in TensorFlow: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system.h",filesystem pull-request-available,['C++'],ARROW,Improvement,Major,2017-04-03 22:58:10,2
13060901,[Format] Add LargeBinary and LargeString types,"These are string and binary types that use 64-bit offsets. Java will not need to implement these types for the time being, but they are needed when representing very large datasets in C++",pull-request-available,"['C++', 'Format']",ARROW,New Feature,Major,2017-04-01 13:56:01,2
13058300,"[C++] Build JSON ""scanner"" for reading record batches from line-delimited JSON files",Umbrella issue for using RapidJSON to parse JSON files to Arrow record batches,pull-request-available,['C++'],ARROW,New Feature,Major,2017-03-22 16:42:46,6
13056705,[C++] Do not build/run io-hdfs-test if ARROW_HDFS=off,This is a rough edge for folks doing C++ development,pull-request-available,['C++'],ARROW,Bug,Major,2017-03-16 15:48:40,14
13048263,[C++] Provide iterator access to primitive elements inside a Column/ChunkedArray,"Given a ChunkedArray, an Arrow user must currently iterate over all its chunks and then cast them to their types to extract the primitive memory regions to access the values. A convenient way to access the underlying values would be to offer a function that takes a ChunkedArray and returns a C++ iterator over all elements.

While this may not be the most performant way to access the underlying data, it should have sufficient performance and adds a convenience layer for new users.",beginner newbie,['C++'],ARROW,Improvement,Major,2017-03-04 08:23:00,6
13048132,Some logical types not supported when loading Parquet,"When I try to read a parquet file with some logical types in it, pyarrow says they are not supported:

{code}
table = pq.read_table('t.parquet')
---------------------------------------------------------------------------
ArrowException                            Traceback (most recent call last)
<ipython-input-14-b7190e66bcb5> in <module>()
----> 1 table = pq.read_table('parquet/t')

/opt/conda/lib/python3.5/site-packages/pyarrow/parquet.py in read_table(source, columns, nthreads, metadata)
    113 
    114     pf = ParquetFile(source, metadata=metadata)
--> 115     return pf.read(columns=columns, nthreads=nthreads)
    116 
    117 

/opt/conda/lib/python3.5/site-packages/pyarrow/parquet.py in read(self, nrows, columns, nthreads)
     78 
     79         return self.reader.read(column_indices=column_indices,
---> 80                                 nthreads=nthreads)
     81 
     82 

/opt/conda/lib/python3.5/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.ParquetReader.read (/feedstock_root/build_artefacts/pyarrow_1488133203047/work/arrow-f6924ad83bc95741f003830892ad4815ca3b70fd/python/build/temp.linux-x86_64-3.5/_parquet.cxx:7706)()

/opt/conda/lib/python3.5/site-packages/pyarrow/error.pyx in pyarrow.error.check_status (/feedstock_root/build_artefacts/pyarrow_1488133203047/work/arrow-f6924ad83bc95741f003830892ad4815ca3b70fd/python/build/temp.linux-x86_64-3.5/error.cxx:1197)()

ArrowException: NotImplemented: Unhandled logical type for int32
{code}

This is the schema of the parquet file (see attached):

{code}
optional group root {
  optional int64 instant (TIMESTAMP_MILLIS);
  optional int32 time (TIME_MILLIS);
  optional double a-double;
  optional int64 another-int;
  optional binary a-string (UTF8);
  optional group list (LIST) {
    repeated group list {
      optional int64 element;
    }
  }
  optional boolean a-boolean;
  optional group a-group {
    optional boolean bool;
    optional int64 another;
  }
  optional int64 an-int;
  optional int32 a-date (DATE);
}
{code}

I assume this is because not pyarrow doesn't support loading all the parquet logical types yet. Is there someplace I can look (even if it's not documented, just in the codebase), where I can find what types are supported currently and which are not?
 ",parquet,['Python'],ARROW,Bug,Major,2017-03-03 18:41:50,14
13046637,Add JIRA fix version to merge tool,Like parquet-mr's tool. This will make releases less painful,pull-request-available,['Python'],ARROW,New Feature,Major,2017-02-27 13:07:46,14
13046512,[C++] Define public API for user-defined data types,"This will include:

* Implementing a subclass of DataType
* A ""fallback"" mechanism for receivers that do not understand our custom metadata
* Implementing a serializer interface for custom metadata (to be send and received in an IPC setting)",pull-request-available,['C++'],ARROW,New Feature,Major,2017-02-26 22:40:32,14
13042885,Script to easily verify release in all languages,"Having a script as in {{parquet-cpp}} that downloads the tarball, verifies the signature and builds/tests in all 3 three languages would be very helpful for future releaes.",pull-request-available,['Developer Tools'],ARROW,Wish,Major,2017-02-14 09:54:24,14
13042653,[C++] Implement functions to conform unequal dictionaries amongst multiple Arrow arrays,"We may wish to either

* Conform the dictionary indices to reference a common dictionary
* Concatenate indices into a new array with a common dictionary

This is related to in-memory dictionary encoding, as you start with a partially-built dictionary and then add entries as you observe new ones in other dictionaries, all the while ""rebasing"" indices to consistently reference the same dictionary at the end",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2017-02-13 19:22:27,2
13042299,[C++] Add function to concatenate like-typed arrays,"A la 

{{Status arrow::Concatenate(const std::vector<std::shared_ptr<Array>>& arrays, MemoryPool* pool, std::shared_ptr<Array>* out)}}",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2017-02-11 15:56:18,6
13038660,C++: Make Status::OK method constexpr,We call this method very often and probably have a small overhead as it isn't {{constexpr}}. We also cannot simply declare it {{constexpr}} as {{arrow::Status}}is not a literal type yet.,beginner newbie,['C++'],ARROW,New Feature,Major,2017-01-28 17:55:37,14
13038659,[C++] Verbose Array::Equals,In failing unit tests I often wished {{Array::Equals}} would tell me where they aren't equal. This would save a lot of time in debugging.,pull-request-available,['C++'],ARROW,New Feature,Major,2017-01-28 17:52:18,6
13038474,Building pyarrow with parquet,"I'm so close to getting pyarrow installed but I'm getting this error when building extensions:

{code}
python setup.py build_ext --with-parquet --build-type=release install

...

Using ld linker
-- Searching for Python libs in /opt/conda/lib64;/opt/conda/lib;/opt/conda/lib/python3.5/config-3.5m
-- Looking for python3.5m
-- Found Python lib /opt/conda/lib/libpython3.5m.so
-- Searching for Python libs in /opt/conda/lib64;/opt/conda/lib;/opt/conda/lib/python3.5/config-3.5m
-- Looking for python3.5m
-- Found Python lib /opt/conda/lib/libpython3.5m.so
-- Found the Parquet library: /usr/local/lib/libparquet.so
-- Found the Arrow core library: /usr/local/lib/libarrow.so
-- Found the Arrow IO library: /usr/local/lib/libarrow_io.so
-- Found the Arrow IPC library: /usr/local/lib/libarrow_ipc.so
-- Added shared library dependency arrow: /usr/local/lib/libarrow.so
-- Added shared library dependency arrow_io: /usr/local/lib/libarrow_io.so
-- Added shared library dependency arrow_ipc: /usr/local/lib/libarrow_ipc.so
-- Added shared library dependency python: /opt/conda/lib/libpython3.5m.so
CMake Error at CMakeLists.txt:434 (message):
  Unable to locate Parquet libraries

-- Configuring incomplete, errors occurred!
See also ""/tmp/arrow/python/build/temp.linux-x86_64-3.5/CMakeFiles/CMakeOutput.log"".
See also ""/tmp/arrow/python/build/temp.linux-x86_64-3.5/CMakeFiles/CMakeError.log"".
error: command 'cmake' failed with exit status 1
{code}

I'm not sure how to proceed since it finds the parquet library prior to raising an error that it cant find it.   Any suggestions or is there other information I need to provide?",beginner build,['Python'],ARROW,Bug,Trivial,2017-01-27 16:06:31,8
13036848,[C++/Python] Construct List container from offsets and values subarrays,This is the inverse operation from flattening a list type into its child values (dropping the offsets),pull-request-available,"['C++', 'Python']",ARROW,New Feature,Major,2017-01-22 04:58:35,14
13031560,[C++] Add filesystem implementation for Amazon S3,The BSD-licensed C++ code in SFrame (https://github.com/turi-code/SFrame/tree/master/oss_src/fileio) may provide some inspiration. ,filesystem pull-request-available,['C++'],ARROW,New Feature,Major,2017-01-02 23:27:59,2
13029481,Segfaults and encoding issues in Python Parquet reads,"I've conda installed pyarrow and am trying to read data from the parquet-compatibility project.  I haven't explicitly built parquet-cpp or anything and may or may not have old versions lying around, so please take this issue with some salt:

{code:python}
In [1]: import pyarrow.parquet

In [2]: t = pyarrow.parquet.read_table('nation.plain.parquet')
---------------------------------------------------------------------------
ArrowException                            Traceback (most recent call last)
<ipython-input-2-5d966681a384> in <module>()
----> 1 t = pyarrow.parquet.read_table('nation.plain.parquet')

/home/mrocklin/Software/anaconda/lib/python3.5/site-packages/pyarrow/parquet.pyx in pyarrow.parquet.read_table (/feedstock_root/build_artefacts/work/arrow-79344b335849c2eb43954b0751018051814019d6/python/build/temp.linux-x86_64-3.5/parquet.cxx:2783)()

/home/mrocklin/Software/anaconda/lib/python3.5/site-packages/pyarrow/parquet.pyx in pyarrow.parquet.ParquetReader.read_all (/feedstock_root/build_artefacts/work/arrow-79344b335849c2eb43954b0751018051814019d6/python/build/temp.linux-x86_64-3.5/parquet.cxx:2200)()

/home/mrocklin/Software/anaconda/lib/python3.5/site-packages/pyarrow/error.pyx in pyarrow.error.check_status (/feedstock_root/build_artefacts/work/arrow-79344b335849c2eb43954b0751018051814019d6/python/build/temp.linux-x86_64-3.5/error.cxx:1185)()

ArrowException: NotImplemented: list<: uint8>
{code}

Additionally I tried to read data from a Python file-like object pointing to data on S3.  Let me know if you'd prefer a separate issue.

{code:python}
In [1]: import s3fs

In [2]: fs = s3fs.S3FileSystem()

In [3]: f = fs.open('dask-data/nyc-taxi/2015/parquet/part.0.parquet')

In [4]: f.read(100)
Out[4]: b'PAR1\x15\x00\x15\x90\xc4\xa2\x12\x15\x90\xc4\xa2\x12,\x15\xc2\xa8\xa4\x02\x15\x00\x15\x06\x15\x08\x00\x00\x00\x80\xbf\xe7\x8b\x0b\x05\x00\x00\x80\xbf\xe7\x8b\x0b\x05\x00\x00\x80\xbf\xe7\x8b\x0b\x05\x00@\xc2\xce\xe7\x8b\x0b\x05\x00\xc0F\xed\xe7\x8b\x0b\x05\x00\xc0F\xed\xe7\x8b\x0b\x05\x00\x00\x89\xfc\xe7\x8b\x0b\x05\x00@\xcb\x0b\xe8\x8b\x0b\x05\x00\x80\r\x1b\xe8\x8b\x0b'

In [5]: import pyarrow.parquet

In [6]: t = pyarrow.parquet.read_table(f)
Segmentation fault (core dumped)
{code}

Here is a more reproducible version:

{code:python}
In [1]: with open('nation.plain.parquet', 'rb') as f:
   ...:     data = f.read()
   ...:     

In [2]: from io import BytesIO

In [3]: f = BytesIO(data)

In [4]: f.seek(0)
Out[4]: 0

In [5]: import pyarrow.parquet

In [6]: t = pyarrow.parquet.read_table(f)
Segmentation fault (core dumped)
{code}

I was however pleased with round-trip functionality within this project, which was very pleasant.",parquet python,['Python'],ARROW,Bug,Minor,2016-12-20 15:20:40,14
13027307,C++: Add Equals implementation to compare ChunkedArrays,The {{arrow::ChunkedArray}}class is missing a method to compare it to other {{arrow::ChunkedArray}}instances. There should be one implemented in the way the {{Equals}} implementation is done in the subclasses of {{arrow::Array}} and is needed to provide a way to compare {{arrow::Column}}instances.,beginner newbie,['C++'],ARROW,New Feature,Major,2016-12-11 21:16:28,8
13027305,C++: Add Equals implementation to compare Columns,The {{arrow::Column}}class is missing a method to compare it to other {{arrow::Column}}instances. There should be one implemented in the way the {{Equals}} implementation is done in the subclasses of {{arrow::Array}} and is needed to provide a way to compare {{arrow::Table}}instances.,beginner newbie,['C++'],ARROW,New Feature,Major,2016-12-11 21:11:37,8
13027287,C++: Add Equals implementation to compare Tables,The {{arrow::Table}}class is missing a method to compare it to other {{arrow::Table}}instances. There should be one implemented in the way the {{Equals}} implementation is done in the subclasses of {{arrow::Array}}.,beginner newbie,['C++'],ARROW,New Feature,Major,2016-12-11 17:11:09,8
13026801,[Format] Handling of buffer padding in the IPC metadata,"See discussion in ARROW-399. Do we include padding bytes in the metadata or set the actual used bytes? In the latter case, the padding would be a part of the format (any buffers continue to be expected to be 64-byte padded, to permit AVX512 instructions)",pull-request-available,['Format'],ARROW,New Feature,Major,2016-12-08 22:05:53,14
13021221,Python: Use setuptools_scm/setuptools_scm_git_archive to provide the version number,"Instead of relying on a hardcoded string, it would be better to infer the version number from the SCM. This would get rid of the custom code in the release script (which I'm not sure if it is actually working) and also provide better (i.e. increasing) dev-versions. ",newbie,['Python'],ARROW,Improvement,Major,2016-11-16 18:05:53,8
13020923,Python: Add support for conversion of Pandas.Categorical,"At the moment conversion from {{pandas.Categorical}}columns fails with {{ArrowException: Invalid: only handle 1-dimensional arrays}}. As a better alternative, we should provide one of the following solutions:

 * Convert the categorical column to a string (Pandas type {{object}}) column, then use the conversion routines for strings. Add some metadata to the Arrow column that it was initially a Pandas string column so that in the case of a roundtrip, it will be a categorical column again.
 * Implement the conversion of the column to a dictionary-encoded Arrow column. This is the preferred solution but may be more complicated to implement as certain requirements have not yet been implemented.",newbie,['Python'],ARROW,New Feature,Minor,2016-11-15 21:01:56,14
13019154,Python: Pandas conversion from `datetime.date` columns,It seems to be a common practice to store some columns as Python {{datetime.date}} to avoid issues with far future/past dates in Pandas. We can natively store this data in Arrow as well in Parquet but there is no conversion available yet. A simple way could be to provide a path pandas.Series -> numpy.ndarray(datetime64[D]) -> arrow.Array(DATE)),newbie,['Python'],ARROW,New Feature,Major,2016-11-08 09:49:37,8
13016266,[Format] Interval(DAY_TIME) has no unit,"Interval(DATE_TIME) assumes milliseconds.
we should have a time unit like timestamp.",columnar-format-1.0 pull-request-available,['Format'],ARROW,Bug,Major,2016-10-28 20:48:05,15
13013676,Instructions for building with conda,"According to [this comment|https://issues.apache.org/jira/browse/ARROW-230?focusedCommentId=15588846&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15588846], Arrow 0.1.0 for Python can be installed with Conda. {{arrow-cpp}} is a dependency of the Python version, and I can install {{arrow-cpp}} locally with

{noformat}
conda install --channel conda-forge parquet-cpp numpy pandas pytest
cd apache-arrow-0.1.0/cpp
conda-build conda.recipe --channel conda-forge
conda install -c conda-forge --use-local arrow-cpp
cd ../python
{noformat}

but I can't build and locally install the {{conda.recipe}} in the Python directory because conda keeps trying to get the {{arrow-cpp}} on {{conda-forge}}, rather than the one in the 0.1.0 release. Those versions are incompatible due to a changed API:

{noformat}
[ 24%] Building CXX object CMakeFiles/pyarrow.dir/src/pyarrow/adapters/builtin.cc.o
/usr/bin/c++   -Dpyarrow_EXPORTS -isystem /opt/miniconda2/conda-bld/conda.recipe_1476908391204/_b_env_placehold_placehold_/lib/python2.7/site-packages/numpy/core/include -isystem /opt/miniconda2/conda-bld/conda.recipe_1476908391204/_b_env_placehold_placehold_/include/python2.7 -isystem /opt/apache-arrow-0.1.0/python/src -isystem /opt/miniconda2/conda-bld/conda.recipe_1476908391204/_b_env_placehold_placehold_/include  -std=c++11 -Wall -ggdb -O0 -g -fPIC   -fPIC -o CMakeFiles/pyarrow.dir/src/pyarrow/adapters/builtin.cc.o -c /opt/apache-arrow-0.1.0/python/src/pyarrow/adapters/builtin.cc
/opt/apache-arrow-0.1.0/python/src/pyarrow/adapters/builtin.cc: In function 'pyarrow::Status pyarrow::ConvertPySequence(PyObject*, std::shared_ptr<arrow::Array>*)':
/opt/apache-arrow-0.1.0/python/src/pyarrow/adapters/builtin.cc:434:26: error: no matching function for call to 'arrow::ArrayBuilder::Finish()'
   *out = builder->Finish();
                          ^
/opt/apache-arrow-0.1.0/python/src/pyarrow/adapters/builtin.cc:434:26: note: candidate is:
In file included from /opt/miniconda2/conda-bld/conda.recipe_1476908391204/_b_env_placehold_placehold_/include/arrow/api.h:24:0,
                 from /opt/apache-arrow-0.1.0/python/src/pyarrow/adapters/builtin.cc:23:
/opt/miniconda2/conda-bld/conda.recipe_1476908391204/_b_env_placehold_placehold_/include/arrow/builder.h:96:18: note: virtual arrow::Status arrow::ArrayBuilder::Finish(std::shared_ptr<arrow::Array>*)
   virtual Status Finish(std::shared_ptr<Array>* out) = 0;
                  ^
/opt/miniconda2/conda-bld/conda.recipe_1476908391204/_b_env_placehold_placehold_/include/arrow/builder.h:96:18: note:   candidate expects 1 argument, 0 provided
make[2]: *** [CMakeFiles/pyarrow.dir/src/pyarrow/adapters/builtin.cc.o] Error 1
make[2]: Leaving directory `/opt/apache-arrow-0.1.0/python/build/temp.linux-x86_64-2.7'
make[1]: *** [CMakeFiles/pyarrow.dir/all] Error 2
make[1]: Leaving directory `/opt/apache-arrow-0.1.0/python/build/temp.linux-x86_64-2.7'
make: *** [all] Error 2
error: command 'make' failed with exit status 2
{noformat}

If I do {{conda-build --channel local --channel conda-forge --override-channels}}, it can't find some of the packages I've installed. If I don't {{--override-channels}}, it tries to use {{arrow-cpp 0.1.post-1}} from {{conda-forge}} as the dependency and I get the compilation error above.

Note: my {{conda list}} is

{noformat}
# packages in environment at /opt/miniconda2:
#
conda-build               2.0.6                    py27_0
blas                      1.1                    openblas    conda-forge
conda                     4.1.12                   py27_0    conda-forge
conda-env                 2.5.2                    py27_0    conda-forge
numpy                     1.11.2          py27_blas_openblas_200  [blas_openblas]  conda-forge
openblas                  0.2.18                        5    conda-forge
pandas                    0.19.0              np111py27_0    conda-forge
parquet-cpp               0.1.pre                       3    conda-forge
pytest                    3.0.3                    py27_0    conda-forge
thrift-cpp                0.9.3                         3    conda-forge
enum34                    1.1.6                    py27_0
filelock                  2.0.6                    py27_0
jinja2                    2.8                      py27_1
libgfortran               3.0.0                         1
arrow-cpp                 0.1                           0    local
markupsafe                0.23                     py27_2
mkl                       11.3.3                        0
openssl                   1.0.2h                        1
patchelf                  0.9                           0
pip                       8.1.2                    py27_0
pkginfo                   1.3.2                    py27_0
py                        1.4.31                   py27_0
pycosat                   0.6.1                    py27_1
pycrypto                  2.6.1                    py27_4
python                    2.7.12                        1
python-dateutil           2.5.3                    py27_0
pytz                      2016.7                   py27_0
pyyaml                    3.11                     py27_4
readline                  6.2                           2
requests                  2.10.0                   py27_0
ruamel_yaml               0.11.14                  py27_0
setuptools                23.0.0                   py27_0
six                       1.10.0                   py27_0
sqlite                    3.13.0                        0
tk                        8.5.18                        0
wheel                     0.29.0                   py27_0
yaml                      0.1.6                         0
zlib                      1.2.8                         3
{noformat}

I'm pretty sure the problem here is something I don't know about conda (I started using it this morning), but I can't figure out how to install this package out-of-the-box.
",build,['Python'],ARROW,Improvement,Major,2016-10-19 21:57:59,14
13011081,[Python] Timeline for dropping Python 2.7 support,"I'd be in favor of dropping 2.7 support altogether from pyarrow, [~xhochy] what do you think?",pull-request-available,['Python'],ARROW,Improvement,Major,2016-10-10 15:48:39,2
13006642,[Format] Add body buffer compression option to IPC message protocol using LZ4 or ZSTD,"It may be useful if data is to be sent over the wire to compress the data buffers themselves as their being written in the file layout.

I would propose that we keep this extremely simple with a global buffer compression setting in the file Footer. Probably only two compressors worth supporting out of the box would be zlib (higher compression ratios) and lz4 (better performance).

What does everyone think?",pull-request-available,['Format'],ARROW,New Feature,Major,2016-09-21 20:29:10,14
12982521,[C++] libhdfs: feedback to help determining cause of failure in opening file path,"In ARROW-222, there is no meaningful feedback other than ""it failed"". ",pull-request-available,['C++'],ARROW,Bug,Major,2016-06-23 23:57:17,14
12980147,[C++] Build conda artifacts in a build environment with better cross-linux ABI compatibility,"Operating system: Linux

Steps to reproduce:
{noformat}
conda create -y -n test_pyarrow python=2.7
source activate test_pyarrow
conda install -y -c apache/label/dev pyarrow
python -c 'import pyarrow.parquet'
{noformat}

Fails with
{noformat}
*** Error in `python': free(): invalid pointer: 0x00007f275a09bbc0 ***
Aborted
{noformat}

Full backtrace from running python under GDB:
{noformat}
#0  0x00007ffff6d39cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff6d3d0d8 in __GI_abort () at abort.c:89
#2  0x00007ffff6d76394 in __libc_message (do_abort=do_abort@entry=1, fmt=fmt@entry=0x7ffff6e84b28 ""*** Error in `%s': %s: 0x%s ***\n"") at ../sysdeps/posix/libc_fatal.c:175
#3  0x00007ffff6d8266e in malloc_printerr (ptr=<optimized out>, str=0x7ffff6e80c19 ""free(): invalid pointer"", action=1) at malloc.c:4996
#4  _int_free (av=<optimized out>, p=<optimized out>, have_lock=0) at malloc.c:3840
#5  0x00007fffec08f640 in _M_dispose (__a=..., this=<optimized out>) at /usr/include/c++/4.9/bits/basic_string.h:249
#6  ~basic_string (this=0x7fffffffd1a0, __in_chrg=<optimized out>) at /usr/include/c++/4.9/bits/basic_string.h:547
#7  __gnu_cxx::new_allocator<arrow::Field>::construct<arrow::Field<char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> > (this=<optimized out>, __p=0x7163d0)
    at /usr/include/c++/4.9/ext/new_allocator.h:120
#8  0x00007fffec0854a6 in _S_construct<arrow::Field, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__p=0x7163d0, __a=...) at /usr/include/c++/4.9/bits/alloc_traits.h:253
#9  construct<arrow::Field, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__p=0x7163d0, __a=...) at /usr/include/c++/4.9/bits/alloc_traits.h:399
#10 _Sp_counted_ptr_inplace<char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__a=..., this=0x7163c0) at /usr/include/c++/4.9/bits/shared_ptr_base.h:515
#11 construct<std::_Sp_counted_ptr_inplace<arrow::Field, std::allocator<arrow::Field>, (__gnu_cxx::_Lock_policy)2>, std::allocator<arrow::Field> const, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__p=0x7163c0, this=<synthetic pointer>) at /usr/include/c++/4.9/ext/new_allocator.h:120
#12 _S_construct<std::_Sp_counted_ptr_inplace<arrow::Field, std::allocator<arrow::Field>, (__gnu_cxx::_Lock_policy)2>, std::allocator<arrow::Field> const, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__p=<optimized out>, __a=<synthetic pointer>) at /usr/include/c++/4.9/bits/alloc_traits.h:253
#13 construct<std::_Sp_counted_ptr_inplace<arrow::Field, std::allocator<arrow::Field>, (__gnu_cxx::_Lock_policy)2>, std::allocator<arrow::Field> const, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__p=<optimized out>, __a=<synthetic pointer>) at /usr/include/c++/4.9/bits/alloc_traits.h:399
#14 __shared_count<arrow::Field, std::allocator<arrow::Field>, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__a=..., this=0x7fffffffd1f8) at /usr/include/c++/4.9/bits/shared_ptr_base.h:619
#15 __shared_ptr<std::allocator<arrow::Field>, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__a=..., __tag=..., this=0x7fffffffd1f0) at /usr/include/c++/4.9/bits/shared_ptr_base.h:1090
#16 shared_ptr<std::allocator<arrow::Field>, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__a=..., __tag=..., this=0x7fffffffd1f0) at /usr/include/c++/4.9/bits/shared_ptr.h:316
#17 allocate_shared<arrow::Field, std::allocator<arrow::Field>, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> (__a=...) at /usr/include/c++/4.9/bits/shared_ptr.h:588
#18 make_shared<arrow::Field, char const (&) [1], std::shared_ptr<arrow::UInt8Type> const&> () at /usr/include/c++/4.9/bits/shared_ptr.h:604
#19 __static_initialization_and_destruction_0 (__initialize_p=1, __priority=65535) at /home/travis/build/apache/arrow/cpp/src/arrow/parquet/schema.cc:50
#20 _GLOBAL__sub_I_schema.cc(void) () at /home/travis/build/apache/arrow/cpp/src/arrow/parquet/schema.cc:306
#21 0x00007ffff7dea13a in call_init (l=<optimized out>, argc=argc@entry=3, argv=argv@entry=0x7fffffffe0d8, env=env@entry=0x749b80) at dl-init.c:78
#22 0x00007ffff7dea223 in call_init (env=<optimized out>, argv=<optimized out>, argc=<optimized out>, l=<optimized out>) at dl-init.c:36
#23 _dl_init (main_map=main_map@entry=0x9f5660, argc=3, argv=0x7fffffffe0d8, env=0x749b80) at dl-init.c:126
#24 0x00007ffff7deec70 in dl_open_worker (a=a@entry=0x7fffffffd4b8) at dl-open.c:577
#25 0x00007ffff7de9ff4 in _dl_catch_error (objname=objname@entry=0x7fffffffd4a8, errstring=errstring@entry=0x7fffffffd4b0, mallocedp=mallocedp@entry=0x7fffffffd4a0, 
    operate=operate@entry=0x7ffff7dee9a0 <dl_open_worker>, args=args@entry=0x7fffffffd4b8) at dl-error.c:187
#26 0x00007ffff7dee3bb in _dl_open (file=0x71f590 ""/home/lomereiter/miniconda2/envs/test_pyarrow/lib/python2.7/site-packages/pyarrow/parquet.so"", mode=-2147483646, caller_dlopen=<optimized out>, nsid=-2, 
    argc=3, argv=0x7fffffffe0d8, env=0x749b80) at dl-open.c:661
#27 0x00007ffff75d202b in dlopen_doit (a=a@entry=0x7fffffffd6d0) at dlopen.c:66
#28 0x00007ffff7de9ff4 in _dl_catch_error (objname=0x625d90, errstring=0x625d98, mallocedp=0x625d88, operate=0x7ffff75d1fd0 <dlopen_doit>, args=0x7fffffffd6d0) at dl-error.c:187
#29 0x00007ffff75d262d in _dlerror_run (operate=operate@entry=0x7ffff75d1fd0 <dlopen_doit>, args=args@entry=0x7fffffffd6d0) at dlerror.c:163
#30 0x00007ffff75d20c1 in __dlopen (file=<optimized out>, mode=<optimized out>) at dlopen.c:87
#31 0x00007ffff7b22dde in _PyImport_GetDynLoadFunc (fqname=<optimized out>, shortname=<optimized out>, 
    pathname=0x71f590 ""/home/lomereiter/miniconda2/envs/test_pyarrow/lib/python2.7/site-packages/pyarrow/parquet.so"", fp=0x704920) at Python/dynload_shlib.c:130
#32 0x00007ffff7b078d8 in _PyImport_LoadDynamicModule (name=0x6b6fe0 ""pyarrow.parquet"", pathname=0x71f590 ""/home/lomereiter/miniconda2/envs/test_pyarrow/lib/python2.7/site-packages/pyarrow/parquet.so"", 
    fp=0x704920) at ./Python/importdl.c:42
#33 0x00007ffff7b05f81 in import_submodule (mod=0x7ffff7e82b08, subname=0x6b6fe8 ""parquet"", fullname=0x6b6fe0 ""pyarrow.parquet"") at Python/import.c:2704
#34 0x00007ffff7b061f4 in load_next (mod=0x7ffff7e82b08, altmod=0x7ffff7e82b08, p_name=<optimized out>, buf=0x6b6fe0 ""pyarrow.parquet"", p_buflen=0x7fffffffdb00) at Python/import.c:2519
#35 0x00007ffff7b06860 in import_module_level (level=<optimized out>, fromlist=0x7ffff7da3cd0 <_Py_NoneStruct>, locals=<optimized out>, globals=<optimized out>, name=0x0) at Python/import.c:2236
#36 PyImport_ImportModuleLevel (name=<optimized out>, globals=<optimized out>, locals=<optimized out>, fromlist=0x7ffff7da3cd0 <_Py_NoneStruct>, level=<optimized out>) at Python/import.c:2292
#37 0x00007ffff7ae614f in builtin___import__ (self=<optimized out>, args=<optimized out>, kwds=<optimized out>) at Python/bltinmodule.c:49
#38 0x00007ffff7a3cd23 in PyObject_Call (func=0x7ffff7fb6fc8, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2546
#39 0x00007ffff7ae6633 in PyEval_CallObjectWithKeywords (func=0x7ffff7fb6fc8, arg=0x7ffff7e9ff70, kw=<optimized out>) at Python/ceval.c:4219
#40 0x00007ffff7aeb29e in PyEval_EvalFrameEx (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2622
---Type <return> to continue, or q <return> to quit---
#41 0x00007ffff7af0a2e in PyEval_EvalCodeEx (co=0x7ffff7eb6e30, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=0, kws=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0)
    at Python/ceval.c:3582
#42 0x00007ffff7af0b42 in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at Python/ceval.c:669
#43 0x00007ffff7b0f5cc in run_mod (arena=0x641360, flags=<optimized out>, locals=0x7ffff7f58168, globals=0x7ffff7f58168, filename=0x7ffff7b3f7ed ""<string>"", mod=<optimized out>) at Python/pythonrun.c:1370
#44 PyRun_StringFlags (str=0x601010 ""import pyarrow.parquet\n"", start=257, globals=0x7ffff7f58168, locals=0x7ffff7f58168, flags=<optimized out>) at Python/pythonrun.c:1333
#45 0x00007ffff7b108f0 in PyRun_SimpleStringFlags (command=0x601010 ""import pyarrow.parquet\n"", flags=0x7fffffffdfb0) at Python/pythonrun.c:974
#46 0x00007ffff7b26457 in Py_Main (argc=<optimized out>, argv=<optimized out>) at Modules/main.c:589
#47 0x00007ffff6d24ec5 in __libc_start_main (main=0x400710 <main>, argc=3, argv=0x7fffffffe0d8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe0c8)
    at libc-start.c:287
#48 0x0000000000400649 in _start ()
{noformat}",python,"['C++', 'Python']",ARROW,Bug,Major,2016-06-17 09:41:10,14
12971400,builds failing on master branch with apt-get error,"
Logs from: https://travis-ci.org/apache/arrow/jobs/131207432
0.50s$ sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install clang-format-3.7 clang-tidy-3.7 gcc-4.9 g++-4.9 gdb ccache cmake valgrind
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package g++-4.9
E: Couldn't find any package by regex 'g++-4.9'
apt-get.diagnostics
apt-get install failed
$ cat ~/apt-get-update.log
Get:1 http://downloads-distro.mongodb.org dist Release.gpg [490 B]
Hit http://us.archive.ubuntu.com precise Release.gpg
Get:2 http://us.archive.ubuntu.com precise-updates Release.gpg [198 B]
Get:3 http://downloads-distro.mongodb.org dist Release [2,040 B]
Get:4 http://us.archive.ubuntu.com precise-backports Release.gpg [198 B]
Hit http://us.archive.ubuntu.com precise Release
Get:5 http://downloads-distro.mongodb.org dist/10gen amd64 Packages [30.9 kB]
Get:6 http://us.archive.ubuntu.com precise-updates Release [55.4 kB]
Hit http://ppa.launchpad.net precise Release.gpg
Get:7 http://security.ubuntu.com precise-security Release.gpg [198 B]
Get:8 http://downloads-distro.mongodb.org dist/10gen i386 Packages [30.5 kB]
Get:9 http://us.archive.ubuntu.com precise-backports Release [55.5 kB]
Hit http://ppa.launchpad.net precise Release.gpg
Get:10 http://security.ubuntu.com precise-security Release [55.5 kB]
Hit http://us.archive.ubuntu.com precise/main Sources
Ign http://downloads-distro.mongodb.org dist/10gen TranslationIndex
Hit http://us.archive.ubuntu.com precise/universe Sources
Get:11 http://ppa.launchpad.net precise Release.gpg [316 B]
Hit http://us.archive.ubuntu.com precise/multiverse Sources
Hit http://us.archive.ubuntu.com precise/main amd64 Packages
Hit http://us.archive.ubuntu.com precise/universe amd64 Packages
Get:12 http://ppa.launchpad.net precise Release.gpg [316 B]
Hit http://us.archive.ubuntu.com precise/multiverse amd64 Packages
Hit http://us.archive.ubuntu.com precise/main i386 Packages
Hit http://us.archive.ubuntu.com precise/universe i386 Packages
Hit http://ppa.launchpad.net precise Release.gpg
Hit http://us.archive.ubuntu.com precise/multiverse i386 Packages
Get:13 http://security.ubuntu.com precise-security/main Sources [142 kB]
Hit http://us.archive.ubuntu.com precise/main TranslationIndex
Get:14 http://ppa.launchpad.net precise Release.gpg [316 B]
Hit http://us.archive.ubuntu.com precise/multiverse TranslationIndex
Hit http://us.archive.ubuntu.com precise/universe TranslationIndex
Hit http://ppa.launchpad.net precise Release
Get:15 http://us.archive.ubuntu.com precise-updates/main Sources [496 kB]
Get:16 http://security.ubuntu.com precise-security/universe Sources [48.5 kB]
Hit http://ppa.launchpad.net precise Release
Get:17 http://us.archive.ubuntu.com precise-updates/universe Sources [127 kB]
Get:18 http://security.ubuntu.com precise-security/multiverse Sources [2,721 B]
Get:19 http://us.archive.ubuntu.com precise-updates/multiverse Sources [10.2 kB]
Get:20 http://us.archive.ubuntu.com precise-updates/main amd64 Packages [989 kB]
Get:21 http://ppa.launchpad.net precise Release [12.9 kB]
Get:22 http://security.ubuntu.com precise-security/main amd64 Packages [607 kB]
Get:23 http://us.archive.ubuntu.com precise-updates/universe amd64 Packages [276 kB]
Get:24 http://us.archive.ubuntu.com precise-updates/multiverse amd64 Packages [16.9 kB]
Get:25 http://ppa.launchpad.net precise Release [12.9 kB]
Get:26 http://us.archive.ubuntu.com precise-updates/main i386 Packages [1,051 kB]
Get:27 http://us.archive.ubuntu.com precise-updates/universe i386 Packages [286 kB]
Hit http://ppa.launchpad.net precise Release
Get:28 http://us.archive.ubuntu.com precise-updates/multiverse i386 Packages [17.1 kB]
Get:29 http://us.archive.ubuntu.com precise-updates/main TranslationIndex [208 B]
Get:30 http://us.archive.ubuntu.com precise-updates/multiverse TranslationIndex [202 B]
Get:31 http://ppa.launchpad.net precise Release [13.0 kB]
Get:32 http://us.archive.ubuntu.com precise-updates/universe TranslationIndex [205 B]
Get:33 http://security.ubuntu.com precise-security/universe amd64 Packages [132 kB]
Get:34 http://us.archive.ubuntu.com precise-backports/main Sources [5,922 B]
Hit http://ppa.launchpad.net precise/main amd64 Packages
Get:35 http://security.ubuntu.com precise-security/multiverse amd64 Packages [3,156 B]
Get:36 http://us.archive.ubuntu.com precise-backports/restricted Sources [28 B]
Get:37 http://us.archive.ubuntu.com precise-backports/universe Sources [43.5 kB]
Hit http://ppa.launchpad.net precise/main i386 Packages
Get:38 http://us.archive.ubuntu.com precise-backports/multiverse Sources [5,750 B]
Get:39 http://us.archive.ubuntu.com precise-backports/main amd64 Packages [6,477 B]
Get:40 http://security.ubuntu.com precise-security/main i386 Packages [663 kB]
Get:41 http://us.archive.ubuntu.com precise-backports/restricted amd64 Packages [28 B]
Hit http://ppa.launchpad.net precise/main TranslationIndex
Get:42 http://us.archive.ubuntu.com precise-backports/universe amd64 Packages [45.8 kB]
Get:43 http://us.archive.ubuntu.com precise-backports/multiverse amd64 Packages [5,419 B]
Get:44 http://security.ubuntu.com precise-security/universe i386 Packages [140 kB]
Get:45 http://us.archive.ubuntu.com precise-backports/main i386 Packages [6,478 B]
Get:46 http://us.archive.ubuntu.com precise-backports/restricted i386 Packages [28 B]
Get:47 http://us.archive.ubuntu.com precise-backports/universe i386 Packages [45.6 kB]
Get:48 http://security.ubuntu.com precise-security/multiverse i386 Packages [3,333 B]
Get:49 http://us.archive.ubuntu.com precise-backports/multiverse i386 Packages [5,413 B]
Get:50 http://us.archive.ubuntu.com precise-backports/main TranslationIndex [202 B]
Hit http://ppa.launchpad.net precise/main amd64 Packages
Get:51 http://us.archive.ubuntu.com precise-backports/multiverse TranslationIndex [202 B]
Get:52 http://security.ubuntu.com precise-security/main TranslationIndex [208 B]
Get:53 http://us.archive.ubuntu.com precise-backports/restricted TranslationIndex [193 B]
Get:54 http://us.archive.ubuntu.com precise-backports/universe TranslationIndex [205 B]
Hit http://ppa.launchpad.net precise/main i386 Packages
Get:55 http://security.ubuntu.com precise-security/multiverse TranslationIndex [199 B]
Hit http://us.archive.ubuntu.com precise/main Translation-en
Hit http://ppa.launchpad.net precise/main TranslationIndex
Hit http://us.archive.ubuntu.com precise/multiverse Translation-en
Hit http://us.archive.ubuntu.com precise/universe Translation-en
Get:56 http://security.ubuntu.com precise-security/universe TranslationIndex [205 B]
Get:57 http://us.archive.ubuntu.com precise-updates/main Translation-en [415 kB]
Get:58 http://ppa.launchpad.net precise/main amd64 Packages [682 B]
Get:59 http://us.archive.ubuntu.com precise-updates/multiverse Translation-en [9,806 B]
Get:60 http://security.ubuntu.com precise-security/main Translation-en [253 kB]
Get:61 http://us.archive.ubuntu.com precise-updates/universe Translation-en [165 kB]
Get:62 http://ppa.launchpad.net precise/main i386 Packages [685 B]
Get:63 http://us.archive.ubuntu.com precise-backports/main Translation-en [5,737 B]
Get:64 http://us.archive.ubuntu.com precise-backports/multiverse Translation-en [4,852 B]
Get:65 http://us.archive.ubuntu.com precise-backports/restricted Translation-en [28 B]
Get:66 http://us.archive.ubuntu.com precise-backports/universe Translation-en [35.6 kB]
Get:67 http://ppa.launchpad.net precise/main TranslationIndex [196 B]
Get:68 http://ppa.launchpad.net precise/main amd64 Packages [1,398 B]
Get:69 http://security.ubuntu.com precise-security/multiverse Translation-en [1,698 B]
Get:70 http://ppa.launchpad.net precise/main i386 Packages [943 B]
Get:71 http://security.ubuntu.com precise-security/universe Translation-en [84.5 kB]
Get:72 http://ppa.launchpad.net precise/main TranslationIndex [199 B]
Hit http://ppa.launchpad.net precise/main amd64 Packages
Hit http://ppa.launchpad.net precise/main i386 Packages
Hit http://ppa.launchpad.net precise/main TranslationIndex
Get:73 http://ppa.launchpad.net precise/main amd64 Packages [3,405 B]
Get:74 http://ppa.launchpad.net precise/main i386 Packages [3,405 B]
Get:75 http://ppa.launchpad.net precise/main TranslationIndex [199 B]
Hit http://ppa.launchpad.net precise/main Translation-en
Hit http://ppa.launchpad.net precise/main Translation-en
Get:76 http://ppa.launchpad.net precise/main Translation-en [464 B]
Get:77 http://ppa.launchpad.net precise/main Translation-en [735 B]
Hit http://ppa.launchpad.net precise/main Translation-en
Get:78 http://ppa.launchpad.net precise/main Translation-en [1,556 B]
Get:79 http://apt.postgresql.org precise-pgdg Release.gpg [819 B]
Get:80 http://apt.postgresql.org precise-pgdg Release [40.1 kB]
Get:81 http://apt.postgresql.org precise-pgdg/main amd64 Packages [64.6 kB]
Err http://downloads-distro.mongodb.org dist/10gen Translation-en_US
  Could not connect to downloads-distro.mongodb.org:80 (23.23.96.15), connection timed out
Err http://downloads-distro.mongodb.org dist/10gen Translation-en
  Unable to connect to downloads-distro.mongodb.org:http:
Get:82 http://apt.postgresql.org precise-pgdg/main i386 Packages [64.6 kB]
Ign http://apt.postgresql.org precise-pgdg/main TranslationIndex
Ign http://apt.postgresql.org precise-pgdg/main Translation-en_US
Ign http://apt.postgresql.org precise-pgdg/main Translation-en
Fetched 6,613 kB in 12s (544 kB/s)
Get:1 http://llvm.org llvm-toolchain-precise-3.7 Release.gpg [819 B]
Get:2 http://llvm.org llvm-toolchain-precise-3.7 Release [3,355 B]
Hit http://downloads-distro.mongodb.org dist Release.gpg
Hit http://us.archive.ubuntu.com precise Release.gpg
Get:3 http://llvm.org llvm-toolchain-precise-3.7/main amd64 Packages [6,616 B]
Get:4 http://llvm.org llvm-toolchain-precise-3.7/main i386 Packages [6,635 B]
Hit http://us.archive.ubuntu.com precise-updates Release.gpg
Ign http://llvm.org llvm-toolchain-precise-3.7/main TranslationIndex
Hit http://downloads-distro.mongodb.org dist Release
Hit http://us.archive.ubuntu.com precise-backports Release.gpg
Hit http://us.archive.ubuntu.com precise Release
Hit http://downloads-distro.mongodb.org dist/10gen amd64 Packages
Hit http://us.archive.ubuntu.com precise-updates Release
Hit http://security.ubuntu.com precise-security Release.gpg
Hit http://apt.postgresql.org precise-pgdg Release.gpg
Hit http://us.archive.ubuntu.com precise-backports Release
Ign http://llvm.org llvm-toolchain-precise-3.7/main Translation-en_US
Hit http://downloads-distro.mongodb.org dist/10gen i386 Packages
Ign http://llvm.org llvm-toolchain-precise-3.7/main Translation-en
Hit http://us.archive.ubuntu.com precise/main Sources
Hit http://us.archive.ubuntu.com precise/universe Sources
Ign http://downloads-distro.mongodb.org dist/10gen TranslationIndex
Hit http://security.ubuntu.com precise-security Release
Hit http://us.archive.ubuntu.com precise/multiverse Sources
Hit http://us.archive.ubuntu.com precise/main amd64 Packages
Hit http://us.archive.ubuntu.com precise/universe amd64 Packages
Hit http://security.ubuntu.com precise-security/main Sources
Hit http://us.archive.ubuntu.com precise/multiverse amd64 Packages
Hit http://apt.postgresql.org precise-pgdg Release
Hit http://us.archive.ubuntu.com precise/main i386 Packages
Hit http://us.archive.ubuntu.com precise/universe i386 Packages
Hit http://us.archive.ubuntu.com precise/multiverse i386 Packages
Hit http://security.ubuntu.com precise-security/universe Sources
Hit http://us.archive.ubuntu.com precise/main TranslationIndex
Hit http://security.ubuntu.com precise-security/multiverse Sources
Hit http://us.archive.ubuntu.com precise/multiverse TranslationIndex
Hit http://apt.postgresql.org precise-pgdg/main amd64 Packages
Hit http://us.archive.ubuntu.com precise/universe TranslationIndex
Hit http://us.archive.ubuntu.com precise-updates/main Sources
Hit http://security.ubuntu.com precise-security/main amd64 Packages
Hit http://us.archive.ubuntu.com precise-updates/universe Sources
Hit http://us.archive.ubuntu.com precise-updates/multiverse Sources
Hit http://us.archive.ubuntu.com precise-updates/main amd64 Packages
Ign http://downloads-distro.mongodb.org dist/10gen Translation-en_US
Hit http://security.ubuntu.com precise-security/universe amd64 Packages
Hit http://us.archive.ubuntu.com precise-updates/universe amd64 Packages
Hit http://us.archive.ubuntu.com precise-updates/multiverse amd64 Packages
Ign http://downloads-distro.mongodb.org dist/10gen Translation-en
Hit http://apt.postgresql.org precise-pgdg/main i386 Packages
Hit http://us.archive.ubuntu.com precise-updates/main i386 Packages
Hit http://security.ubuntu.com precise-security/multiverse amd64 Packages
Hit http://us.archive.ubuntu.com precise-updates/universe i386 Packages
Hit http://us.archive.ubuntu.com precise-updates/multiverse i386 Packages
Hit http://us.archive.ubuntu.com precise-updates/main TranslationIndex
Hit http://security.ubuntu.com precise-security/main i386 Packages
Hit http://us.archive.ubuntu.com precise-updates/multiverse TranslationIndex
Hit http://us.archive.ubuntu.com precise-updates/universe TranslationIndex
Hit http://us.archive.ubuntu.com precise-backports/main Sources
Ign http://apt.postgresql.org precise-pgdg/main TranslationIndex
Hit http://security.ubuntu.com precise-security/universe i386 Packages
Hit http://us.archive.ubuntu.com precise-backports/restricted Sources
Hit http://security.ubuntu.com precise-security/multiverse i386 Packages
Hit http://us.archive.ubuntu.com precise-backports/universe Sources
Hit http://us.archive.ubuntu.com precise-backports/multiverse Sources
Hit http://us.archive.ubuntu.com precise-backports/main amd64 Packages
Hit http://us.archive.ubuntu.com precise-backports/restricted amd64 Packages
Hit http://security.ubuntu.com precise-security/main TranslationIndex
Hit http://us.archive.ubuntu.com precise-backports/universe amd64 Packages
Hit http://us.archive.ubuntu.com precise-backports/multiverse amd64 Packages
Hit http://us.archive.ubuntu.com precise-backports/main i386 Packages
Hit http://security.ubuntu.com precise-security/multiverse TranslationIndex
Hit http://us.archive.ubuntu.com precise-backports/restricted i386 Packages
Hit http://us.archive.ubuntu.com precise-backports/universe i386 Packages
Hit http://us.archive.ubuntu.com precise-backports/multiverse i386 Packages
Hit http://security.ubuntu.com precise-security/universe TranslationIndex
Hit http://us.archive.ubuntu.com precise-backports/main TranslationIndex
Hit http://us.archive.ubuntu.com precise-backports/multiverse TranslationIndex
Hit http://us.archive.ubuntu.com precise-backports/restricted TranslationIndex
Hit http://security.ubuntu.com precise-security/main Translation-en
Hit http://us.archive.ubuntu.com precise-backports/universe TranslationIndex
Hit http://us.archive.ubuntu.com precise/main Translation-en
Hit http://us.archive.ubuntu.com precise/multiverse Translation-en
Hit http://us.archive.ubuntu.com precise/universe Translation-en
Hit http://us.archive.ubuntu.com precise-updates/main Translation-en
Hit http://us.archive.ubuntu.com precise-updates/multiverse Translation-en
Hit http://security.ubuntu.com precise-security/multiverse Translation-en
Hit http://us.archive.ubuntu.com precise-updates/universe Translation-en
Hit http://us.archive.ubuntu.com precise-backports/main Translation-en
Hit http://us.archive.ubuntu.com precise-backports/multiverse Translation-en
Hit http://security.ubuntu.com precise-security/universe Translation-en
Hit http://us.archive.ubuntu.com precise-backports/restricted Translation-en
Hit http://us.archive.ubuntu.com precise-backports/universe Translation-en
Ign http://apt.postgresql.org precise-pgdg/main Translation-en_US
Ign http://apt.postgresql.org precise-pgdg/main Translation-en
Err http://ppa.launchpad.net precise Release.gpg
  Could not connect to ppa.launchpad.net:80 (91.189.95.83), connection timed out
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Err http://ppa.launchpad.net precise Release.gpg
  Unable to connect to ppa.launchpad.net:http:
Fetched 17.4 kB in 10s (1,740 B/s)
Reading package lists...
W: Failed to fetch http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/precise/Release.gpg  Could not connect to ppa.launchpad.net:80 (91.189.95.83), connection timed out
W: Failed to fetch http://ppa.launchpad.net/git-core/v1.8/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Failed to fetch http://ppa.launchpad.net/kalakris/cmake/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Failed to fetch http://ppa.launchpad.net/rwky/redis/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Failed to fetch http://ppa.launchpad.net/travis-ci/zero-mq/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Failed to fetch http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Failed to fetch http://ppa.launchpad.net/ubuntugis/ppa/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Failed to fetch http://ppa.launchpad.net/webupd8team/java/ubuntu/dists/precise/Release.gpg  Unable to connect to ppa.launchpad.net:http:
W: Some index files failed to download. They have been ignored, or old ones used instead.
The command ""sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install clang-format-3.7 clang-tidy-3.7 gcc-4.9 g++-4.9 gdb ccache cmake valgrind"" failed and exited with 100 during .",ci-failure,['Continuous Integration'],ARROW,Bug,Blocker,2016-05-20 04:58:06,15
12956060,Python: API documentation via sphinx-apidoc,"For developers using Arrow via Python, we should provide an automatically generated API documentation via sphinx(-apidoc).",Python,['Python'],ARROW,Task,Major,2016-04-05 06:25:47,8
12954256,Apache Arrow cpp code does not support power architecture,"Apache Arrow cpp code does not support power architecture.

While building ""Apache Arrow"" for cpp for power architecture, it throws below error: ""c++: error: unrecognized command line option -msse3""
Unable to port it for power.",build patch,['C++'],ARROW,New Feature,Major,2016-03-29 10:18:29,8
12948730,[Java] Method can return the value bigger than long MAX_VALUE,"Method org.apache.drill.exec.util.DecimalUtility.adjustScaleMultiply(long input, int factor) can return the value bigger than long max value.
For example by comparison two decimal18 values 9223372036854775807 and 0.001. To adjust first value scale this method should return 9223372036854775807 * 1000 - bigger than long max value.

Class DecimalUtility.java will be a part of org.apache.arrow after renaming described in [DRILL-4455 Depend on Apache Arrow for Vector and Memory| https://issues.apache.org/jira/browse/DRILL-4455]",adjustScale arrow decimal pull-request-available,['Java'],ARROW,Bug,Major,2016-03-10 10:11:42,16
12947906,[C++] Consider adding a scalar type object model,"Just did this on the Python side. In later analytics routines, passing in scalar values (example: Array + Scalar) requires some kind of container. Some systems, like the R language, solve this problem with length-1 arrays, but we should do some analysis of use cases and figure out what will work best for Arrow.",Analytics pull-request-available,['C++'],ARROW,New Feature,Major,2016-03-08 05:17:00,14
12946274,C++: Upload gcov coverage data to coveralls.io,I recently did this for Parquet (https://github.com/apache/parquet-cpp/commit/9cab887f2337ae6205cbd7ea908bcb749c5c9343); it will be nice to have the coverage report available to assist with unit testing.,pull-request-available,['C++'],ARROW,New Feature,Major,2016-03-02 19:58:42,14
12946267,[C++] Implement delimited file scanner / CSV reader,"Like Parquet and binary file formats, text files will be an important data medium for converting to and from in-memory Arrow data. 

pandas has some (Apache-compatible) business logic we can learn from here (as one of the gold-standard CSV readers in production use)

https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.h
https://github.com/pydata/pandas/blob/master/pandas/parser.pyx

While very fast, this this should be largely written from scratch to target the Arrow memory layout, but we can reuse certain aspects like the tokenizer DFA (which originally came from the Python interpreter csv module implementation)

https://github.com/pydata/pandas/blob/master/pandas/src/parser/tokenizer.c#L713",csv pull-request-available,['C++'],ARROW,New Feature,Major,2016-03-02 19:40:16,2
