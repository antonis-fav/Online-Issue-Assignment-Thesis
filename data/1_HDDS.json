[
   {
      "_id": "13391832",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-07-26 08:29:14",
      "description": "_kubernetes_ tests wait for cluster startup, checking some conditions with retry.  In worst case all conditions are checked 100 times with 3 seconds delay, so the test may take 15 minutes to fail.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Reduce retry in Kubernetes test"
   },
   {
      "_id": "13391786",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335609",
            "id": "12335609",
            "name": "kubernetes"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-07-26 05:18:12",
      "description": "_kubernetes_ check is failing in CI at {{ozone}}.  It is probably related to persistent volumes.  I propose to disable the failing test temporarily, until it can be fixed.\r\n\r\nLast successful run: https://github.com/apache/ozone/runs/3137129344\r\nFirst failed run: https://github.com/apache/ozone/runs/3140001398\r\n\r\nSCM init container fails to start due to:\r\n\r\n{code}\r\nUnable to create directory /data/metadata specified in configuration setting ozone.metadata.dirs\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable failing kubernetes test"
   },
   {
      "_id": "13391581",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335609",
            "id": "12335609",
            "name": "kubernetes"
         }
      ],
      "created": "2021-07-23 14:14:25",
      "description": "{{kubernetes.sh}} installs {{flekszible}} for Linux, even on other platforms.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Install OS-specific flekszible"
   },
   {
      "_id": "13391111",
      "assignee": "elek",
      "components": [],
      "created": "2021-07-21 10:53:14",
      "description": "Please see: https://github.com/apache/ozone/pull/2449",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump jetty version to 9.4.41.v20210516"
   },
   {
      "_id": "13390913",
      "assignee": "elek",
      "components": [],
      "created": "2021-07-20 11:35:39",
      "description": "Please see: https://github.com/apache/ozone/pull/2443",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump Apache Ratis version to 2.1"
   },
   {
      "_id": "13390903",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-07-20 10:52:55",
      "description": "TestPeriodicVolumeChecker requires at least 130 seconds due to two long sleeps.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid long sleep in TestPeriodicVolumeChecker"
   },
   {
      "_id": "13390689",
      "assignee": "elek",
      "components": [],
      "created": "2021-07-19 12:06:29",
      "description": "Please see: https://github.com/apache/ozone/pull/2434",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Multi-raft style placement with permutations for offline data generator"
   },
   {
      "_id": "13389978",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2021-07-16 07:25:11",
      "description": "S3 head uses OM API lookup key which refreshes pipeline info by contacting SCM.\r\n\r\nFor S3 head we donot require any pipeline info, we need very basic details like length, type, etag and last modification time. \r\n\r\nBy removing pipeline info which is not required for HEAD object, HEAD API performance can be improved.\r\n\r\nThis is identified during looking up graphs from [~kerneltime] testing",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid refresh pipeline for S3 headObject"
   },
   {
      "_id": "13389655",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-07-14 15:41:43",
      "description": "{noformat}\r\n[main] ERROR ozone.HddsDatanodeService: HttpServer failed to start.\r\n...\r\nCaused by: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/datanode.keytab\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Datanode HTTP server fails to start in ozonesecure due to wrong keytab name"
   },
   {
      "_id": "13389649",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-07-14 14:55:59",
      "description": "Currently, with multi raft feature ON, a data node can participate in multiple pipelines. But, same set of dns can be part of multiple pipelines . This leads to 2 problems:\r\n\r\n1) In case of let's say 5 datanodes, as and when 3 datanodes register initially with multi-raft feature ON, those 3 datanodes will pair among themselves to create two pipelines thereby satisfying the default pipeline limit of 2 per dn. Next datanodes even after registering cannot take part in writes bcoz it needs a 6th datanode to come up to form the nest tri-set , thereby remain unutilized.\r\n\r\n2) With same set of Datanodes being part of multiple plpelines, if one datanode fails, the source for container re-replication will be only the other set of 2 nodes and can beocme a bottleneck.\r\n\r\n3) This uneven distribution of pipelines among dns will lead to load distribution getting uneven as well.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disallow same set of DNs to be part of multiple pipelines"
   },
   {
      "_id": "13389417",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-07-13 15:25:57",
      "description": "Hadoop 3.3 support was recently added (HDDS-3292) in Ozone.  We should run Hadoop/MapReduce acceptance tests for this version, too.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add acceptance test for Hadoop 3.3"
   },
   {
      "_id": "13389400",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         }
      ],
      "created": "2021-07-13 14:05:32",
      "description": "Follow-up of HDDS-5384: check usages of {{OmKeyLocationInfoGroup.getLocationList()}} and {{getLocationListCount()}}, and replace with cheaper calls if possible.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Reduce usage of OmKeyLocationInfoGroup.getLocationList()"
   },
   {
      "_id": "13388403",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-07-08 13:27:42",
      "description": "Some calls to {{Preconditions.check...}} eagerly format the error message, resulting in unnecessary allocations.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid eager string formatting in preconditions"
   },
   {
      "_id": "13388000",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-07-06 18:56:16",
      "description": "Failure first noticed in this CI run on upgrade branch: https://github.com/apache/ozone/runs/2969789739\r\n\r\nWas able to reproduce failure on master after about 100 runs of the test locally.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent test failure in TestSCMPipelineManager#testPipelineReload"
   },
   {
      "_id": "13387882",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2021-07-06 09:28:40",
      "description": "{{ozone insight logs -v datanode.dispatcher}} prints binary data ({{buffers: \"4w7u1Zp8@{\"}})\r\n\r\n{noformat}\r\n[TRACE|org.apache.hadoop.ozone.container.common.impl.HddsDispatcher|OzoneProtocolMessageDispatcher] [service=DatanodeClient] [type=ReadChunk] request is processed. Response:\r\ncmdType: ReadChunk\r\ntraceID: \"\"\r\nresult: SUCCESS\r\nreadChunk {\r\n  blockID {\r\n    containerID: 1\r\n    localID: 107544261427200009\r\n    blockCommitSequenceId: 34\r\n  }\r\n  chunkData {\r\n    chunkName: \"107544261427200009_chunk_1\"\r\n    offset: 0\r\n    len: 10\r\n    checksumData {\r\n      type: CRC32\r\n      bytesPerChecksum: 1048576\r\n      checksums: \"354d261247\"\r\n    }\r\n  }\r\n  dataBuffers {\r\n    buffers: \"4w7u1Zp8@{\"\r\n    buffers: \"<redacted>\"\r\n  }\r\n}\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Data buffers incorrectly filtered for Ozone Insight"
   },
   {
      "_id": "13386148",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2021-06-28 03:36:14",
      "description": "HDDS-5243 was a patch for omitting unnecessary key locations for clients on reading. But the same warning of large response size observed in our cluster for putting data. The patch can also be ported for putting data, as long as until object versioning is supported.\r\n\r\nMy hypothesis is: The large message was originally, and possibly maybe due to this warning and sudden connection close from client side on reading large message in Hadoop IPC layer, from Ozone Manager - which causes hopeless 15 retries from RetryInvocationHandler. The retries create another entry in OpenKeyTable but they never moved to KeyTable because the key never gets commited.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Return latest version of key location for client on createKey/createFile"
   },
   {
      "_id": "13386025",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2021-06-26 19:52:08",
      "description": "{{OzoneClientProducer}} catches any {{Throwable}} while creating the Ozone client and logs it at debug level.  I think this should be avoided for OOME at least, possibly other {{Error}} types.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid catching Error while creating Ozone client"
   },
   {
      "_id": "13385599",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-06-24 11:04:29",
      "description": "steps taken :\r\nRunning ozone fault injection tests which involve activating/deactivating pipelines.\r\nIn one of the tests, while activating pipeline, SCM terminated.SCM log:\r\n{code:java}\r\n2021-06-23 23:57:03,166 WARN org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline PipelineID=661b906a-fd6b-4c14-b6b5-8c0a12c67ff7 is not found in the pipeline Map. Pipeline may have been deleted already.\r\n2021-06-23 23:57:03,168 ERROR org.apache.ratis.statemachine.StateMachine: Terminating with exit status 1: null\r\njava.lang.IllegalMonitorStateException\r\n\tat java.base/java.util.concurrent.locks.ReentrantReadWriteLock$Sync.tryRelease(ReentrantReadWriteLock.java:372)\r\n\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1302)\r\n\tat java.base/java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.unlock(ReentrantReadWriteLock.java:1147)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.updatePipelineState(PipelineStateManagerV2Impl.java:270)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor101.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:168)\r\n\tat org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:139)\r\n\tat org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1690)\r\n\tat org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:234)\r\n\tat org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:179)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n2021-06-23 23:57:03,174 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG: \r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM terminated with exit status 1: null"
   },
   {
      "_id": "13384801",
      "assignee": "elek",
      "components": [],
      "created": "2021-06-21 07:57:02",
      "description": "It seems only a few key classes are required from Apache Hadoop to be reused. The goal here is investigating the build process and check how they can be re-used. \r\n\r\nOne option is just copy the files with some minor updates (e.g. using ConfigurationSource interface instead of o.a.hadoop.Configuration) which would help to reduce dependencies.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "EC: Adopt EC related utility from Hadoop source repository"
   },
   {
      "_id": "13384800",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2021-06-21 07:55:10",
      "description": "_Acceptance (secure)_ check is frequently failing, usually at S3 tests.  The root cause is that datanodes are shut down due to too many \"bad\" volumes.\r\n\r\n{noformat:title=S3 Gateway log}\r\nINTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks\r\n{noformat}\r\n\r\n{noformat:title=SCM log}\r\nPipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0.\r\n{noformat}\r\n\r\n{noformat:title=Datanode log}\r\ndatanode_2  | 2021-06-19 13:26:08,010 [Periodic HDDS volume checker] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds\r\ndatanode_2  | 2021-06-19 13:36:08,013 [Periodic HDDS volume checker] WARN volume.StorageVolumeChecker: checkAllVolumes timed out after 600000 ms\r\ndatanode_2  | 2021-06-19 13:36:08,014 [Periodic HDDS volume checker] WARN volume.MutableVolumeSet: checkAllVolumes got 1 failed volumes - [/data/hdds/hdds]\r\ndatanode_2  | 2021-06-19 13:36:08,016 [Periodic HDDS volume checker] INFO volume.MutableVolumeSet: Moving Volume : /data/hdds/hdds to failed Volumes\r\ndatanode_2  | 2021-06-19 13:36:08,016 [Periodic HDDS volume checker] ERROR statemachine.DatanodeStateMachine: DatanodeStateMachine Shutdown due to too many bad volumes, check hdds.datanode.failed.data.volumes.tolerated and hdds.datanode.failed.metadata.volumes.tolerated\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Datanode shutdown due to too many bad volumes in CI"
   },
   {
      "_id": "13384781",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-06-21 06:20:16",
      "description": "When client tries to figure out leader, when it contacts the first node if it is not a leader, we see this kind of exception in the log.\r\n\r\n\r\n{code:java}\r\n2021-06-02 07:08:31,760 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9961, call Call#0 Retry#0 org.apache.hadoop.hdds.protocol.SCMSecurityProtocol.submitRequest from 172.27.161.194:45505\r\norg.apache.hadoop.hdds.ratis.ServerNotLeaderException: Server:38e91e94-b4fe-4307-b3b0-f8c1e7e2d4d7 is not the leader. Suggested leader is Server:quasar-vudtrs-8.quasar-vudtrs.root.hwx.site:9961.\r\n\tat org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)\r\n\tat org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:191)\r\n\tat org.apache.hadoop.hdds.scm.protocol.SCMSecurityProtocolServerSideTranslatorPB.submitRequest(SCMSecurityProtocolServerSideTranslatorPB.java:92)\r\n\tat org.apache.hadoop.hdds.protocol.proto.SCMSecurityProtocolProtos$SCMSecurityProtocolService$2.callBlockingMethod(SCMSecurityProtocolProtos.java:15124)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:533)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Suppress logging of ServerNotLeaderException "
   },
   {
      "_id": "13384220",
      "assignee": "xyao",
      "components": [],
      "created": "2021-06-16 18:56:30",
      "description": "This is intended for OM/SCM and server only.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move SCMUpdateProtocol to hdds interface-server package"
   },
   {
      "_id": "13384151",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-06-16 11:48:12",
      "description": "{code:java}\r\n\u00a02021-06-16 03:02:14,478 ERROR org.apache.ratis.statemachine.StateMachine: Terminating with exit status 1: PipelineID=fe572097-8689-42ae-83aa-ba86439c5a0e not found\r\norg.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=fe572097-8689-42ae-83aa-ba86439c5a0e not found\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:121)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.getPipeline(PipelineStateManagerV2Impl.java:125)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManagerV2Impl.updatePipelineState(PipelineStateManagerV2Impl.java:250)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:168)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:139)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1690)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:234)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:179)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:834)\r\n2021-06-16 03:02:14,701 INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM-HA] SCM start failed with PipelineNotFoundException"
   },
   {
      "_id": "13384111",
      "assignee": "elek",
      "components": [],
      "created": "2021-06-16 08:53:24",
      "description": "Administrators/vendors may require restricting available replication configs. For example, to disable STANDALONE replication or restrict certain EC scheme.\r\n\r\nThis patch creates a simple validator which can enforce this validation rule.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allow to restrict available ReplicationConfig "
   },
   {
      "_id": "13384022",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2021-06-15 22:32:14",
      "description": "Steps:\r\n # place a file under ozone from a secure cluster with kerberos and tls, say under ozone at: /s3v/blog-test1/1.txt\r\n # do a hdfs dfs cat of the file:\r\n\r\nhdfs dfs -cat\u00a0ofs://ozone1/s3v/blog-test/1.txt\r\n{code}\r\n21/06/11 2100 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties\r\n 21/06/11 2100 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n 21/06/11 2100 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started\r\n Jun 11, 2021 901 PM\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider\r\n INFO: Java 9 ALPN API unavailable (this may be normal)\r\n Jun 11, 2021 901 PM\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider\r\n INFO: netty-tcnative unavailable (this may be normal)\r\n java.lang.IllegalArgumentException: Failed to load any of the given libraries: [netty_tcnative_linux_x86_64_fedora, netty_tcnative_linux_x86_64, netty_tcnative_x86_64, netty_tcnative]\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:104)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:592)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:136)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.defaultSslProvider(GrpcSslContexts.java:225)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.configure(GrpcSslContexts.java:145)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.forClient(GrpcSslContexts.java:94)\r\n at org.apache.hadoop.hdds.scm.XceiverClientGrpc.connectToDatanode(XceiverClientGrpc.java:181)\r\n :\r\n :\r\n Jun 11, 2021 901 PM\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts defaultSslProvider\r\n INFO: Jetty ALPN unavailable (this may be normal)\r\n java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN\r\n at java.lang.Class.forName0(Native Method)\r\n at java.lang.Class.forName(Class.java:348)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.JettyTlsUtil.isJettyAlpnConfigured(JettyTlsUtil.java:64)\r\n at\u00a0[org.apache.ratis.thirdparty.io|http://org.apache.ratis.thirdparty.io/].grpc.netty.GrpcSslContexts.findJdkProvider(GrpcSslContexts.java:249)\r\n :\r\n :\r\n cat: Exception getting XceiverClient:\u00a0[org.apache.hadoop.ozone.shaded.com|http://org.apache.hadoop.ozone.shaded.com/].google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Could not find TLS ALPN provider; no working netty-tcnative, Conscrypt, or Jetty NPN/ALPN available\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "java.lang.ClassNotFoundException: org/eclipse/jetty/alpn/ALPN"
   },
   {
      "_id": "13383918",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         }
      ],
      "created": "2021-06-15 10:44:38",
      "description": "    // We don't take a lock in this path, since we walk the\r\n    // underlying table using an iterator. That automatically creates a\r\n    // snapshot of the data, so we don't need these locks at a higher level\r\n    // when we iterate.\r\n\r\nWe have above snippet in listKeys, in listStatus also we use underlying table iterator, when iterating, we might skip the lock",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "ScaleTest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid usage of lock in listStatus"
   },
   {
      "_id": "13383881",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-06-15 07:50:04",
      "description": "If {{BackgroundPipelineCreatorV2}} tries to trigger one-shot run while pipeline creation is in progress (before entering {{wait()}}), then its {{notifyAll()}} call is lost.  This can result in unnecessary wait, as it will only check if it needs to run after the wait is over.\r\n\r\nI think this may contribute to tests timing out waiting for SCM safemode exit.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Pipeline creator may miss one-shot run"
   },
   {
      "_id": "13383845",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-06-15 05:01:06",
      "description": "Cache key for integration tests is incomplete, hash of pom.xml files is missing.\r\n\r\n{code:title=https://github.com/apache/ozone/runs/2818911609#step:4:1}\r\nRun actions/cache@v2\r\n  with:\r\n    path: ~/.m2/repository\r\n    key: maven-repo--8-\r\n    restore-keys: maven-repo--8\r\n  maven-repo-\r\n  maven-repo-\r\n...\r\nCache restored from key: maven-repo--8-\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Wrong cache key for integration tests"
   },
   {
      "_id": "13383783",
      "assignee": "xyao",
      "components": [],
      "created": "2021-06-14 18:42:55",
      "description": "This can help test verification of block token usage.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Adding debug log for block token verification"
   },
   {
      "_id": "13383732",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-06-14 13:32:56",
      "description": "HTML report files are missing from acceptance test results:\r\n\r\n{code}\r\ncp: cannot stat '/mnt/ozone/hadoop-ozone/dev-support/checks/../../../target/acceptance/log.html': No such file or directory\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "HTML report missing from acceptance results"
   },
   {
      "_id": "13383699",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-06-14 10:00:38",
      "description": "The container report handler is single thread in the SCM Event queue and the process is synchronized per datanode as well. Need to explore if it can be made multithreaded.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "ScaleTest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Container report processing is single threaded"
   },
   {
      "_id": "13383398",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2021-06-11 11:24:54",
      "description": "Jaeger tracing is broken for key creation requests.\r\n\r\n{code:title=steps}\r\ncd hadoop-ozone/dist/target/ozone-*/compose/ozone\r\nexport COMPOSE_FILE=docker-compose.yaml:monitoring.yaml\r\n# console 1\r\nOZONE_REPLICATION_FACTOR=3 ./run.sh\r\n# console 2\r\ndocker-compose exec -T scm ozone freon ockg -n1 -t1\r\n{code}\r\n\r\n{noformat:title=log}\r\nom_1          | java.lang.NoSuchMethodException: Method not found: allocateBlock\r\nom_1          | \tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:65)\r\nom_1          | \tat com.sun.proxy.$Proxy36.allocateBlock(Unknown Source)\r\nom_1          | \tat org.apache.hadoop.ozone.om.request.key.OMKeyRequest.allocateBlock(OMKeyRequest.java:130)\r\nom_1          | \tat org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.preExecute(OMKeyCreateRequest.java:151)\r\nom_1          | \tat org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:139)\r\nom_1          | \tat org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)\r\nom_1          | \tat org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Method not found: allocateBlock - when tracing is enabled"
   },
   {
      "_id": "13383350",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-06-11 08:19:55",
      "description": "        LOG.info(\"bharat starting from sm\");",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Remove unncessary log added durig HDDS-5263"
   },
   {
      "_id": "13383275",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2021-06-10 21:11:37",
      "description": "Follow up from HDDS-5244. The method is no longer needed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove getRequestType method from OM request classes."
   },
   {
      "_id": "13382839",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-06-09 05:13:02",
      "description": "Avoid unncessary logging in SCM followe during report processing.\r\n{code:java}\r\nscm3_1      | 2021-06-09 05:10:34,386 [EventQueue-PipelineReportForPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.StateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl@74c0ee39, cost 799.6us\r\nscm3_1      | 2021-06-09 05:10:34,388 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {\r\nscm3_1      |   id: \"c499d751-b5c7-4ddf-b3ab-b71c75128fca\"\r\nscm3_1      |   uuid128 {\r\nscm3_1      |     mostSigBits: -4280153224896885281\r\nscm3_1      |     leastSigBits: -5500101187051810870\r\nscm3_1      |   }\r\nscm3_1      | }\r\nscm3_1      | isLeader: true\r\nscm3_1      | bytesWritten: 0\r\nscm3_1      |  from dn=74c73b20-b5c0-408d-ae95-2687d1835546{ip: 192.168.0.12, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}.\r\nscm3_1      | org.apache.ratis.protocol.exceptions.NotLeaderException: Server 6d66a209-5955-4ddb-9fbd-8cc2f578a265@group-D6BFC1238401 is not the leader 68245269-1a37-4f90-a5e4-21c8c6b7e63d|rpc:scm1:9894|admin:|client:|dataStream:|priority:0\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:661)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:626)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:754)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$9(RaftServerProxy.java:417)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerProxy.lambda$null$7(RaftServerProxy.java:412)\r\nscm3_1      | \tat org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:115)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$8(RaftServerProxy.java:412)\r\nscm3_1      | \tat java.base/java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:1106)\r\nscm3_1      | \tat java.base/java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2235)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:411)\r\nscm3_1      | \tat org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:417)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.submitRequest(SCMRatisServerImpl.java:214)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:110)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:67)\r\nscm3_1      | \tat com.sun.proxy.$Proxy14.updatePipelineState(Unknown Source)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl.openPipeline(PipelineManagerV2Impl.java:271)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:124)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:91)\r\nscm3_1      | \tat org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:50)\r\nscm3_1      | \tat org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\nscm3_1      | \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nscm3_1      | \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nscm3_1      | \tat java.base/java.lang.Thread.run(Thread.java:834)\r\nscm2_1      | 2021-06-09 05:10:34,368 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {\r\nscm2_1      |   id: \"c499d751-b5c7-4ddf-b3ab-b71c75128fca\"\r\nscm2_1      |   uuid128 {\r\nscm2_1      |     mostSigBits: -4280153224896885281\r\nscm2_1      |     leastSigBits: -5500101187051810870\r\nscm2_1      |   }\r\nscm2_1      | }\r\nscm2_1      | isLeader: true\r\nscm2_1      | bytesWritten: 0\r\nscm2_1      |  from dn=74c73b20-b5c0-408d-ae95-2687d1835546{ip: 192.168.0.12, host: ozone-ha_datanode_3.ozone-ha_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}.\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid unncessary report processing log messages in follower"
   },
   {
      "_id": "13382650",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-06-08 08:09:55",
      "description": "{code:title=https://github.com/elek/ozone-build-results/blob/9d152107f3e9deac65180cb23b7956433eb1c92a/2021/06/02/8212/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.txt#L5-L11}\r\ntestDoubleBufferWithMixOfTransactions(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 0.221 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<16> but was:<1>\r\n  ...\r\n  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBufferWithMixOfTransactions(TestOzoneManagerDoubleBufferWithOMResponse.java:197)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestOzoneManagerDoubleBufferWithOMResponse"
   },
   {
      "_id": "13382634",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-06-08 06:48:40",
      "description": "GetSCMCertificate can happen non-leader SCM, as rootCA is only run on primary SCM.\r\nSo, when an SCM is bootstrapped, let's say it connects first to a bootstrapped SCM, we fail with a SCMSecurityResponse with status set to NOT_A_PRIMARY_SCM. As we return with a response, failOver will not happen.\r\n\r\n*SCMSecurityProtocolClientSideTranslatorPB*\r\n{code:java}\r\n  private SCMSecurityResponse handleError(SCMSecurityResponse resp)\r\n      throws SCMSecurityException {\r\n    if (resp.getStatus() != SCMSecurityProtocolProtos.Status.OK) {\r\n      throw new SCMSecurityException(resp.getMessage(),\r\n          SCMSecurityException.ErrorCode.values()[resp.getStatus().ordinal()]);\r\n    }\r\n    return resp;\r\n  }\r\n{code}\r\n\r\nTo solve this issue, one possible solution is on server check if it is SCMSecurityException with errorCode NOT_A_PRIMARY_SCM return a RetriableWithFailOverException. In this way, FailOverProxyProvider performs failOver and Retry to the next SCM.\r\n\r\nThe exception message is available in comments.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "BootStrapped SCM fails to bootstrap if it connects to another bootstrapped SCM first."
   },
   {
      "_id": "13382476",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-06-07 12:54:39",
      "description": "Some integration tests intermittently fail due to mini cluster not existing safe mode within 2 minutes timeout.  The problem is that pipeline creation interval is also 2 minutes.  It may happen that pipeline is created only while the cluster is being shut down due to timeout.\r\n\r\n{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot-output.txt}\r\n2021-06-02 03:21:03,005 [RatisPipelineUtilsThread - 0] WARN  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:filterViableNodes(151)) - Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2.\r\n...\r\n2021-06-02 03:21:04,007 [Listener at 127.0.0.1/40677] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:lambda$waitForClusterToBeReady$0(224)) - Nodes are ready. Got 3 of 3 DN Heartbeats.\r\n...\r\n2021-06-02 03:22:59,107 [Listener at 127.0.0.1/40677] INFO  ozone.MiniOzoneClusterImpl (MiniOzoneClusterImpl.java:shutdown(443)) - Shutting down the Mini Ozone Cluster\r\n...\r\n2021-06-02 03:23:03,031 [6d4e3dd1-e161-4c07-861b-817db46a0427@group-0D81E0660BF9-StateMachineUpdater] INFO  pipeline.PipelineStateManager (PipelineStateManagerV2Impl.java:addPipeline(101)) - Created pipeline Pipeline ... RATIS/THREE ...\r\n{noformat}\r\n\r\n{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt}\r\norg.apache.hadoop.hdds.scm.TestSCMInstallSnapshot  Time elapsed: 146.994 s  <<< ERROR!\r\njava.util.concurrent.TimeoutException: \r\n  ...\r\n  at org.apache.hadoop.ozone.MiniOzoneClusterImpl.waitForClusterToBeReady(MiniOzoneClusterImpl.java:217)\r\n{noformat}\r\n\r\nRelated test failures:\r\n\r\n{noformat}\r\n2021/05/26/8113/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis.txt\r\n2021/05/26/8118/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA.txt\r\n2021/05/27/8142/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt\r\n2021/05/30/8164/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestOzoneManagerRestInterface.txt\r\n2021/05/31/8166/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMSnapshot.txt\r\n2021/05/31/8177/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestPipelineClose.txt\r\n2021/06/02/8191/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt\r\n2021/06/02/8193/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestStorageContainerManagerHA.txt\r\n2021/06/02/8211/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMSnapshot.txt\r\n2021/06/02/8217/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt\r\n2021/06/07/8299/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.TestSCMInstallSnapshot.txt\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in SCM Ratis integration test"
   },
   {
      "_id": "13382401",
      "assignee": "elek",
      "components": [],
      "created": "2021-06-07 08:00:30",
      "description": "Please see: https://github.com/apache/ozone/pull/2306",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enhance freon streaing generator to support multiple threads"
   },
   {
      "_id": "13382147",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-06-04 13:18:55",
      "description": "InterSCM protocol is defined in client interface, but is concerned only with server-to-server communication.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "InterSCM protocol should be server-only"
   },
   {
      "_id": "13382124",
      "assignee": "elek",
      "components": [],
      "created": "2021-06-04 10:39:31",
      "description": "S3SecretManager is a generic interface which supposed to return with the secret key for one specific AWS access key id.\r\n\r\nIt's a generic interface which may have multiple implementation.\r\n\r\nUnfortunately, it's not possible to use any implementation as OzoneDelegationTokenSecretManager does an explicit cast to retrieve the MetadataManager.\r\n\r\nInstead of breaking the abstract contract of interface it seems to be better to directly inject the required MetadataManager to the  OzoneDelegationTokenSecretManager which makes it possible to use an implementation.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "OzoneDelegationTokenSecretManager breaks the interface contract of S3SecretManager"
   },
   {
      "_id": "13381838",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-06-03 06:34:22",
      "description": "{code:title=https://github.com/apache/ozone/runs/2732634889#step:4:852}\r\n[INFO] Running org.apache.hadoop.ozone.container.common.report.TestReportPublisher\r\n[ERROR] Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.665 s <<< FAILURE! - in org.apache.hadoop.ozone.container.common.report.TestReportPublisher\r\n[ERROR] testCRLStatusReportPublisher(org.apache.hadoop.ozone.container.common.report.TestReportPublisher)  Time elapsed: 1.518 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n\tat org.apache.hadoop.hdds.security.x509.crl.CRLInfo.<init>(CRLInfo.java:47)\r\n\tat org.apache.hadoop.hdds.security.x509.crl.CRLInfo.<init>(CRLInfo.java:38)\r\n\tat org.apache.hadoop.hdds.security.x509.crl.CRLInfo$Builder.build(CRLInfo.java:219)\r\n\tat org.apache.hadoop.ozone.container.common.report.TestReportPublisher.testCRLStatusReportPublisher(TestReportPublisher.java:188)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "testCRLStatusReportPublisher fails to create CRLInfo"
   },
   {
      "_id": "13381681",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2021-06-02 11:45:39",
      "description": "Handle SIGTERM 15 with shutdown hook to properly/clean shutdown OM/DN/SCM.\r\n\r\nIn this way in OM HA/DN/SCM HA snapshot will be called and pending transactions will be flushed to DB.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle SIGTERM to ensure clean shutdown of OM/DN/SCM"
   },
   {
      "_id": "13381669",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-06-02 10:22:49",
      "description": "Handle SIGTERM 15 with shutdown hook to properly/clean shutdown SCM.\r\n\r\nIn this way in SCM HA, the snapshot will be called and pending transactions will be flushed to DB.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle SIGTERM to ensure clean shutdown of SCM"
   },
   {
      "_id": "13381420",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-06-01 10:00:08",
      "description": "{code:java}\r\nscm.log \r\n2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:028fed4a-0087-4b70-b6e3-11f18d739094\r\n2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:a4b76016-dc24-47f2-a3ff-03c309fdcf9b\r\n2021-05-27 09:55:42,189 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 to datanode:ed9d4872-166d-41c6-96ab-437a44e4168b\r\n2021-05-27 09:55:42,199 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2021-05-27T09:55:42.189Z].\r\n2021-05-27 09:55:54,426 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:028fed4a-0087-4b70-b6e3-11f18d739094, CreationTimestamp2021-05-27T09:55:42.189Z] moved to OPEN state\r\n2021-05-27 10:06:45,920 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=cd4a2a77-9715-4437-8d1d-3618a2c93103, PipelineID=ca6100b9-b42c-4b77-bef5-35a9b1e725f2, PipelineID=875b2073-4034-4374-bba6-39011294a280]\r\n2021-05-27 10:06:45,932 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:DORMANT, leaderId:a4b76016-dc24-47f2-a3ff-03c309fdcf9b, CreationTimestamp2021-05-27T09:55:42.189Z] moved to CLOSED state\r\n2021-05-27 10:06:57,921 INFO org.apache.hadoop.hdds.scm.node.StaleNodeHandler: Datanode a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0} moved to stale state. Finalizing its pipelines [PipelineID=cd4a2a77-9715-4437-8d1d-3618a2c93103, PipelineID875b2073-4034-4374-bba6-39011294a280, PipelineID=2878c722-84dc-40f9-b1c1-46ed0f8bcdd7]\r\n2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Scrubbing pipeline: id: PipelineID=875b2073-4034-4374-bba6-39011294a280 since it stays at CLOSED stage.\r\n2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode 028fed4a-0087-4b70-b6e3-11f18d739094\r\n2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode a4b76016-dc24-47f2-a3ff-03c309fdcf9b\r\n2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider: Send pipeline:PipelineID=875b2073-4034-4374-bba6-39011294a280 close command to datanode ed9d4872-166d-41c6-96ab-437a44e4168b\r\n2021-05-27 10:07:41,075 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 875b2073-4034-4374-bba6-39011294a280, Nodes: 028fed4a-0087-4b70-b6e3-11f18d739094{ip: 172.27.167.6, host: quasar-wudsvy-6.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a4b76016-dc24-47f2-a3ff-03c309fdcf9b{ip: 172.27.12.201, host: quasar-wudsvy-4.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}ed9d4872-166d-41c6-96ab-437a44e4168b{ip: 172.27.74.4, host: quasar-wudsvy-1.quasar-wudsvy.root.hwx.site, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:a4b76016-dc24-47f2-a3ff-03c309fdcf9b, CreationTimestamp2021-05-27T09:55:42.189Z] removed.\r\n{code}\r\nThe logs indicate that, a pipeline got created, moved to open state, and then one of the datanodes went stale, thereby the pipeline moved to closed state. The pipeline got scrubbed by the pipeline scrubber and got deleted.\u00a0\r\n{code:java}\r\n2021-05-27 10:07:41,073 INFO org.apache.hadoop.hdds.scm.pipeline.PipelineManagerV2Impl: Scrubbing pipeline: id: PipelineID=875b2073-4034-4374-bba6-39011294a280 since it stays at CLOSED stage.{code}\r\nNext update for the pipeline to be moved to close state as a part of\u00a0 report from other datanodes in the pipeline will fail as the pipeline is removed from scm memory/db and hence scm terminates.\r\n\r\nThe solution would be to ignore PipelineNotFoundException in\u00a0PipelineStateManagerV2Impl#updatePipelineState.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM-HA] SCM start failed with PipelineNotFoundException"
   },
   {
      "_id": "13381270",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-05-31 11:40:30",
      "description": "Ozone has some test classes partially copied from Hadoop, located in the original packages.  Relocate these to avoid classpath conflict.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Relocate classes copied from Hadoop"
   },
   {
      "_id": "13380905",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-05-28 04:41:43",
      "description": "ContainerOperation Client creates XceiverClientManager.\r\n\r\nXceiverClientManager requires to getCA list.\r\n\r\n\r\n{code:java}\r\n      manager = new XceiverClientManager(conf,\r\n          conf.getObject(XceiverClientManager.ScmClientConfig.class),\r\n          caCertificates);\r\n{code}\r\n\r\nWe can avoid listCA which is not required for most admin commands. It is required only for ChunkKeyHandler.\r\n\r\nThis will help when ACLS are configured for SCM security protocol where only admin/service principals can make calls to the SCMSecurityProtocol server, then we don't need to add all the users to them to make these commands work.\r\n\r\nAs for few of the commands like pipeline list, safe mode status we don't require admin privilege.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make XceiverClientManager creation when necessary in ContainerOperationClient"
   },
   {
      "_id": "13380627",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2021-05-26 21:20:07",
      "description": "Decommissioned nodes have \"DECOMMISSIONING\" operational status in recon.\r\n\r\n\r\nRCA\r\nRecon relies on DN heartbeats to understand Node operational state. If the node goes down before it reports itself as DECOMMISSIONED, then there is a loss of information on Recon side. It is the SCM that moves a node form DECOMMISSIONING to DECOMMISSIONED state first, then the Datanode persists the state change locally, and then heartbeats with the new state to SCM & Recon subsequently.  If the DN is shutdown before it can heartbeat the state change to Recon, then Recon lives with the stale information.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon shows operational status as \"DECOMMISSIONING\" for \"DECOMMISSIONED\" DNs"
   },
   {
      "_id": "13380541",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-05-26 12:58:31",
      "description": "GitHub Actions now provides support to cancel duplicate workflows.  This usage of the {{cancel-workflow-runs}} action can be replaced.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use built-in cancel support for duplicates"
   },
   {
      "_id": "13380423",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2021-05-25 23:48:29",
      "description": "There is one extra report that gets published by Datanodes even after shutdown. Ideally, this should be avoided and DN should not publish reports after shutdown. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanode Report Publisher publishes one extra report after DN shutdown"
   },
   {
      "_id": "13380375",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-05-25 15:29:43",
      "description": "In SCM sub-ca certs are setup during init, if a cluster is converted to secure later, in else part of the scmInit, we need to initialize security.\r\n\r\nOM handled this scenario, SCM also needs a similar fix.\r\nhttps://github.com/apache/ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java#L963",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle unsecure cluster convert to secure cluster for SCM"
   },
   {
      "_id": "13380303",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-25 10:13:08",
      "description": "Most of our smoketests can be executed multiple times, which helps us to debug the tests locally.\r\n\r\nUnfortunately it's not the case ozonefs/ozonefs.robot. This test uses hard-coded bucket/volume names instead of using randomized bucket names.\r\n\r\nFor example:\r\n\r\n{code}\r\ndocker-compose up -d --scale datanode=3\r\ndocker-compose exec scm bash\r\nrobot smoketest/ozonefs/ozonefs.robot\r\n\r\nrobot smoketest/ozonefs/ozonefs.robot\r\n{code}\r\n\r\nThe second execution fails which makes harder to debug test problems.\r\n\r\nIt seems to be better to follow existing pattern and adding a random prefix to the user bucket names.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make ozonefs.robot execution repeatable"
   },
   {
      "_id": "13379897",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-05-22 07:52:43",
      "description": "Close container command from SCM fails due to lack of token.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM should send token for CloseContainer command"
   },
   {
      "_id": "13379836",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-05-21 18:40:08",
      "description": "After an unclean shutdown, SCM may never come out of the safe mode.\r\nAttached a document to explain the problem and the proposal.\r\nMore description with log info is available in the comments.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM may stay in safe mode forever due to incorrect open pipeline count"
   },
   {
      "_id": "13379484",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-05-20 10:24:47",
      "description": "OM loads up its signed cert, rootCA cert and CA cert during startup.\r\nSo to get CA list in OM, we can use certClient and get them, in this way we can avoid unncessary network call to OM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid SCM call to get CA certs in non-HA from OM."
   },
   {
      "_id": "13379450",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-05-20 07:25:30",
      "description": "If config is appended with serviceId and nodeId use that, else fall back to config appended without service id and node id. If that is also not defined, fallback to a default value.\r\n\r\n*Example:*\r\nozone.scm.grpc.port = 9898 is set\r\n\r\nWe should use this port for SCM Grpc Service, as there is no port config with serviceid and nodeid defined.\r\n\r\nCurrent code behavior is if port config with service id and node id not defined read from the default value.\r\n\r\n\r\nThe problematic code is\r\n\r\n{code:java}\r\n        String ratisPortKey = ConfUtils.addKeySuffixes(OZONE_SCM_RATIS_PORT_KEY,\r\n            serviceId, nodeId);\r\nint ratisPort = conf.getInt(ratisPortKey, OZONE_SCM_RATIS_PORT_DEFAULT);\r\n{code}\r\n\r\nIn this way fix similarly for all RPC services.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix fall back of config in SCM HA Cluster"
   },
   {
      "_id": "13379329",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-05-19 17:01:09",
      "description": "Currently only _integration_ check uses the {{ozone-builder}} Docker image.  The image is getting outdated and takes some effort to periodically update.  Investigate if _integration_ can be changed to build with Maven cache, like _unit_.\r\n\r\nAlso, it seems execution of Hugo (which is available in the image) is sometimes getting stuck recently:\r\n\r\n* https://github.com/apache/ozone/runs/2607134035?check_suite_focus=true#step:4:1393\r\n* https://github.com/apache/ozone/runs/2601041142?check_suite_focus=true#step:4:1436\r\n* https://github.com/apache/ozone/runs/2600238841?check_suite_focus=true#step:4:1436\r\n* https://github.com/adoroszlai/hadoop-ozone/runs/2601105245?check_suite_focus=true#step:4:1436\r\n* https://github.com/adoroszlai/hadoop-ozone/runs/2621383226#step:4:1393",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Build integration tests with Maven cache"
   },
   {
      "_id": "13379260",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-19 09:55:47",
      "description": "HDDS-5073 improves the existing \"ozone sh\" client to support ReplicationConfig. The input string is parsed to ReplicationConfig by the constructors of the ReplicationConfig classes with string parameters.\r\n\r\nAfter merging this improvement to the EC branch we need to implement the same constructor for ECReplicationConfig.\r\n\r\nThere are multiple options here:\r\n\r\n 1. Create an enum with ALL the possible ECReplicationConfig\r\n 2. Use meaningful programmatic validation rules.\r\n\r\nDuring the EC sync we agreed that 2nd option can be more flexible as we may have very huge configuration matrix with all the EC parameters.\r\n ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "EC: Create ECReplicationConfig on client side based on input string"
   },
   {
      "_id": "13379239",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-05-19 08:47:36",
      "description": "Right now during OM/DN startup we wait a total duration of 300seconds and wait in between each attempt for 10seconds to obtain CA list and check the recieved CA list size is as expected count.\r\n\r\nSo, in case DN's are started before and SCM's have not completed bootstrap after 300 seconds  DN/OM will fail to start. This Jira is to add support for retry forever and with fixed sleep, in this way, DN's can come up even if they are started before bootstrapping SCMs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Wait for ever to obtain CA list which is needed during OM/DN startup"
   },
   {
      "_id": "13379159",
      "assignee": "xyao",
      "components": [],
      "created": "2021-05-18 23:38:23",
      "description": "The TLS configuration is used by CreatePipelineCommandHandler on datanode to create pipeline across datanodes. It currently does not allow enable/disable like other TLS configurations and does not use mTLS when it is needed here.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix OzoneContainer TLS configuration"
   },
   {
      "_id": "13379091",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2021-05-18 15:55:17",
      "description": "Remove use of instance factory in OM request handling switch. Added a new point cut in the OM aspect that makes sure new OM requests can be brought in through a layout version.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allow multiple OM request versions to be supported at same layout version (HDDS-2939)."
   },
   {
      "_id": "13379078",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-05-18 14:55:52",
      "description": "Acceptance tests are split into a few suites, which can be activated by setting {{OZONE_ACCEPTANCE_SUITE}}.  CI checks execute 3 splits for faster feedback:\r\n\r\n * {{secure}}\r\n * {{unsecure}}\r\n * {{misc}} + anything without {{suite}}\r\n\r\nThere are some other tests, tagged {{suite:failing}}, which exist only to test the behavior of the acceptance test runner when failures happen.\r\n\r\nThe goal of this task is to skip the {{failing}} suite by default, i.e. when acceptance tests are run without suite filter.  This would allow running all tests in sequence (e.g. in nightly job) while retaining the ability to run these special tests by setting {{OZONE_ACCEPTANCE_SUITE=failing}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Skip failing acceptance suite by default"
   },
   {
      "_id": "13378806",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-17 12:30:31",
      "description": "Please see: https://github.com/apache/ozone/pull/2252",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable animal-sniffer maven plugin"
   },
   {
      "_id": "13378799",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-17 12:08:35",
      "description": "HDDS-5142 will introduce a new streaming API for closed container replication / snapshot download and other data movement.\r\n\r\nFor server2server communication we need to support mTLS. We should configure pure mTLS on the netty server ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add SSL support to the Ozone streaming API"
   },
   {
      "_id": "13378785",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2021-05-17 10:59:41",
      "description": "Require block token in datanode for the following operations:\r\n\r\n* CompactChunk (currently unsupported operation)\r\n* DeleteBlock\r\n* DeleteChunk\r\n* GetCommittedBlockLength\r\n* ListChunk (currently unsupported operation)\r\n\r\nRequire container token for ListBlock (currently unsupported operation).\r\n\r\nDo not require container token for ListContainer (currently unsupported operation), as it does not have container ID in the request.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Require block token for more operations"
   },
   {
      "_id": "13378748",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-05-17 08:45:38",
      "description": "The problem is SCM init because we use a new clusterID when the version writing failed.\r\n\r\n\r\n{code:java}\r\nCould not initialize SCM version file\r\njava.io.IOException: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING\r\n\tat org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)\r\n\tat org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)\r\n\tat org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:71)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:354)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:371)\r\n\tat org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.initialize(SCMRatisServerImpl.java:115)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.scmInit(StorageContainerManager.java:925)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.init(StorageContainerManagerStarter.java:173)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.initScm(StorageContainerManagerStarter.java:110)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat picocli.CommandLine.executeUserObject(CommandLine.java:1952)\r\n\tat picocli.CommandLine.access$1100(CommandLine.java:145)\r\n\tat picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2332)\r\n\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2326)\r\n\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2291)\r\n\tat picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:2152)\r\n\tat picocli.CommandLine.parseWithHandlers(CommandLine.java:2530)\r\n\tat picocli.CommandLine.parseWithHandler(CommandLine.java:2465)\r\n\tat org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:96)\r\n\tat org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:87)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:57)\r\nCaused by: java.lang.IllegalStateException: ILLEGAL TRANSITION: In SCMStateMachine:eca7c0f7-9310-45a4-bee2-76a3242dd372:group-010A6AE5EDB4, RUNNING -> STARTING\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "SCM subsequent init failed when previous scm init failed"
   },
   {
      "_id": "13378478",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-14 12:19:42",
      "description": "Please see: https://github.com/apache/ozone-docker-runner/pull/7",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "replace HADOOP_ with OZONE_ prefix in the ozone-runner docker image"
   },
   {
      "_id": "13378472",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-14 11:41:35",
      "description": "As [~jmclean]  [reported on the mailing list |https://lists.apache.org/thread.html/rb2bbf79cf136b1c129ff7205e319223c2a780af49ed270031d676ce2%40%3Cdev.ozone.apache.org%3E]:\r\n\r\nbq. While it may not matter in our lifetime, copyright can expire so it would be best to put the published year in the NOTICE file rather than \"Copyright 2006 and onwards\".\r\n\r\nWe should update all of our notice files to 2021:\r\n\r\n{code}\r\nCopyright 2021 The Apache Software Foundation\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update copyright year of NOTICE in all Ozone repositories"
   },
   {
      "_id": "13377890",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-05-11 15:25:50",
      "description": "under .../compose/ozone-ha, create a HA cluster:\r\n\r\n{code}\r\ndocker-compose up -d --scale datanode=3\r\n{code}\r\n\r\nInitial SCM roles as following:\r\n\r\n{code}\r\nbash-4.2$ ozone admin scm roles\r\n[scm1:9865:FOLLOWER, scm2:9865:FOLLOWER, scm3:9865:LEADER]\r\n{code}\r\n\r\nRunning freon random key generator as following:\r\n\r\n{code}\r\nozone freon randomkeys --numOfVolumes=10 --numOfBuckets 50 --numOfKeys 50\u00a0 --replicationType=RATIS --factor=THREE\r\n{code}\r\n\r\nWhile freon randomkeys was running,\u00a0put all SCM nodes under blockade and stop leader SCM node:\r\n\r\nblockade status:\r\n\r\n{code}\r\nNODE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CONTAINER ID\u00a0 \u00a0 STATUS\u00a0 IP\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NETWORK\u00a0 \u00a0 PARTITION \u00a0\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 18f9c1e2d52f\u00a0 \u00a0 UP\u00a0 \u00a0 \u00a0 172.31.0.9\u00a0 \u00a0 \u00a0 NORMAL \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\r\nozone-ha_scm1_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 25c74f0a9271\u00a0 \u00a0 UP\u00a0 \u00a0 \u00a0 172.31.0.6\u00a0 \u00a0 \u00a0 NORMAL \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\r\nozone-ha_scm2_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8808d10ccb3a\u00a0 \u00a0 DOWN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 UNKNOWN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\r\nozone-ha_scm3_1\r\n{code}\r\n\u00a0\r\n\r\nfreon randomkeys failed with following error message:\r\n\r\nSome test result msg as following:\r\n\r\n{code}\r\n6:00:30,131\u00a0[pool-2-thread-3]\u00a0ERROR freon.RandomKeyGenerator: Exception while adding key: key-21-80493 in bucket: bucket-44-63818 of volume: vol-1-95998.\r\nINTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No Route to Host from\u00a0 om1/172.31.0.11 to scm3:9863 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:\u00a0\u00a0[http://wiki.apache.org/hadoop/NoRouteToHost]\r\nat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:604)\r\nat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.openKey(OzoneManagerProtocolClientSideTranslatorPB.java:595)\r\nat org.apache.hadoop.ozone.client.rpc.RpcClient.createKey(RpcClient.java:756)\r\nat org.apache.hadoop.ozone.client.OzoneBucket.createKey(OzoneBucket.java:502)\r\nat org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:703)\r\nat org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:86)\r\nat org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:621)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nat java.base/java.lang.Thread.run(Thread.java:834)\r\n\u00a044.10% |?????????????????????????????????????????????\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 |\u00a0 11024/25000 Time: 0:09:002021-05-11 06:00:37,231\u00a0[pool-2-thread-7]\u00a0INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-EA7B54107DBD->4c51bca8-cc0a-4c20-84dd-b5a7cb18c4ac\r\n2021-05-11 06:00:37,231\u00a0[pool-2-thread-7]\u00a0WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.\r\n\u00a0100.00% |?????????????????????????????????????????????????????????????????????????????????????????????????????|\u00a0 25000/25000 Time: 0:15:39\r\nINTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: No Route to Host from\u00a0 om1/172.31.0.11 to scm:9863 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:\u00a0\u00a0[http://wiki.apache.org/hadoop/NoRouteToHost]\r\n***************************************************\r\nStatus: Failed\r\nGit Base Revision: 7a3bc90b05f257c8ace2f76d74264906f0f7a932\r\nNumber of Volumes created: 10\r\nNumber of Buckets created: 500\r\nNumber of Keys added: 24991\r\nRatis replication factor: THREE\r\nRatis replication type: RATIS\r\nAverage Time spent in volume creation: 00:00:00,114\r\nAverage Time spent in bucket creation: 00:00:01,263\r\nAverage Time spent in key creation: 00:02:48,698\r\nAverage Time spent in key write: 00:00:04,216\r\nTotal bytes written: 255907840\r\nTotal Execution time: 00:15:39,968\r\n***************************************************\r\n{code}\r\n\r\nIn this case, I'd expect the freon test would still finish successfully.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone freon randomkeys failed after leader SCM node is down"
   },
   {
      "_id": "13377849",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-11 11:50:33",
      "description": "Please see: https://github.com/apache/ozone/pull/2238",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Bump logical release name of Ozone 1.2"
   },
   {
      "_id": "13377689",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-05-10 21:08:44",
      "description": "{{TestRatisPipelineProvider#testCreatePipelineWithFactorThree}} fails intermittently due to same set of nodes being selected for two 3-node pipelines.\r\n\r\n2021/02/01/5639/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt\r\n2021/02/11/5875/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt\r\n2021/02/24/6111/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt\r\n2021/04/06/7045/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt\r\n2021/04/07/7095/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt\r\n2021/04/09/7175/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestRatisPipelineProvider#testCreatePipelineWithFactorThree"
   },
   {
      "_id": "13377621",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-05-10 12:18:27",
      "description": "By default, the user started principal is added to scmAdminUsernames.\r\n\r\n\r\n{code:java}\r\n    String scmUsername = UserGroupInformation.getCurrentUser().getUserName();\r\n    if (!scmAdminUsernames.contains(scmUsername)) {\r\n      scmAdminUsernames.add(scmUsername);\r\n    }\r\n{code}\r\n\r\n\r\nIn HA cluster, when kinit with scm2 principal when scm1 is leader, we get access denied as we check getUserName() and also when adding to adminlist we use getUserName.\r\n\r\nIn OM we don't have this kind of issue, as getShortUserName() is used.\r\n\r\n\r\n{code:java}\r\n  String omSPN = UserGroupInformation.getCurrentUser().getShortUserName();\r\n    if (!ozAdmins.contains(omSPN)) {\r\n      ozAdmins.add(omSPN);\r\n    }\r\n{code}\r\n\r\nAnd during admin check it compares with both userName and shortUserName.\r\n\r\n\r\n{code:java}\r\nif (ozAdmins.contains(callerUgi.getShortUserName()) ||\r\n        ozAdmins.contains(callerUgi.getUserName()) ||\r\n{code}\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make admin check work for SCM HA cluster"
   },
   {
      "_id": "13377620",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-05-10 12:13:16",
      "description": "Javadoc comments used to be required on all public types (classes, interfaces, etc.).  In Checkstyle 8.20\r\n\r\nbq. functionality for validating missing javadocs was split into a new check MissingJavadocType.\r\n\r\nSo missing javadoc on new types introduced since 2020/Oct/20 (when HDDS-4306 upgraded Checkstyle from 8.19 to 8.29) were not caught, and now we have ~50 violations.\r\n\r\n{noformat}\r\nhadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/TableCache.java\r\n 114: Missing a Javadoc comment.\r\nhadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/NodeDetails.java\r\n 25: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMCertStore.java\r\n 304: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/balancer/ContainerBalancer.java\r\n 34: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMDBCheckpointProvider.java\r\n 34: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/MockSCMHADBTransactionBuffer.java\r\n 28: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/HASecurityUtils.java\r\n 70: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/LongCodec.java\r\n 24: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/EnumCodec.java\r\n 28: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/BooleanCodec.java\r\n 24: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/ListCodec.java\r\n 28: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/StringCodec.java\r\n 24: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/Codec.java\r\n 23: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/CodecFactory.java\r\n 32: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/io/GeneratedMessageCodec.java\r\n 27: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMContext.java\r\n 180: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMHANodeDetails.java\r\n 63: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/node/DatanodeUsageInfo.java\r\n 27: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogStateManagerImpl.java\r\n 44: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/block/DeletedBlockLogStateManager.java\r\n 29: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMHAConfiguration.java\r\n 45: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMServiceManager.java\r\n 27: Missing a Javadoc comment.\r\nhadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSequenceIDGenerator.java\r\n 29: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBufferFactory.java\r\n 42: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/utils/BufferUtils.java\r\n 27: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBufferImpl.java\r\n 23: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/ha/ConfUtils.java\r\n 28: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/utils/ClientCommandsUtils.java\r\n 23: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ha/RetriableWithNoFailoverException.java\r\n 22: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/common/TestChecksumImplsComputeSameValues.java\r\n 33: Missing a Javadoc comment.\r\nhadoop-hdds/common/src/test/java/org/apache/hadoop/hdds/scm/ha/TestSCMNodeInfo.java\r\n 45: Missing a Javadoc comment.\r\nhadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/statemachine/background/BlockDeletingService.java\r\n 112: Missing a Javadoc comment.\r\nhadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationSupervisor.java\r\n 120: Missing a Javadoc comment.\r\nhadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/replication/ReplicationServer.java\r\n 126: Missing a Javadoc comment.\r\nhadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java\r\n 130: Missing a Javadoc comment.\r\nhadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/replication/TestSimpleContainerDownloader.java\r\n 41: Missing a Javadoc comment.\r\nhadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/containergenerator/BaseGenerator.java\r\n 29: Missing a Javadoc comment.\r\nhadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkCRCBatch.java\r\n 54: Missing a Javadoc comment.\r\nhadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/genesis/BenchMarkCRCStreaming.java\r\n 71: Missing a Javadoc comment.\r\nhadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/debug/ExportContainer.java\r\n 61: Missing a Javadoc comment.\r\nhadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/signature/SignatureInfo.java\r\n 110: Missing a Javadoc comment.\r\nhadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3Owner.java\r\n 27: Missing a Javadoc comment.\r\nhadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3BucketAcl.java\r\n 68: Missing a Javadoc comment.\r\n 99: Missing a Javadoc comment.\r\nhadoop-ozone/s3gateway/src/main/java/org/apache/hadoop/ozone/s3/endpoint/S3Acl.java\r\n 38: Missing a Javadoc comment.\r\nhadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/signature/TestAuthorizationV4QueryParser.java\r\n 30: Missing a Javadoc comment.\r\nhadoop-ozone/s3gateway/src/test/java/org/apache/hadoop/ozone/s3/TestEmptyContentTypeFilter.java\r\n 27: Missing a Javadoc comment.\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/MiniOzoneHAClusterImpl.java\r\n 659: Missing a Javadoc comment.\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOzoneManagerHAKeyDeletion.java\r\n 29: Missing a Javadoc comment.\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/read/TestInputStreamBase.java\r\n 60: Missing a Javadoc comment.\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMSnapshot.java\r\n 40: Missing a Javadoc comment.\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemMissingParent.java\r\n 42: Missing a Javadoc comment.\r\nhadoop-ozone/recon/src/main/java/org/apache/hadoop/ozone/recon/persistence/ContainerHistory.java\r\n 23: Missing a Javadoc comment.\r\nhadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/io/MultipartCryptoKeyInputStream.java\r\n 34: Missing a Javadoc comment.\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Missing type-level Javadoc comments"
   },
   {
      "_id": "13377608",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2021-05-10 10:54:06",
      "description": "HDDS-4525 deprecated {{HADOOP_\\*}} variables in Ozone in favor of corresponding {{OZONE_\\*}} variables.  We should allow suppressing the warning for such variables to make upgrade easier.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Allow suppressing deprecation warning for HADOOP_ variables"
   },
   {
      "_id": "13377602",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-05-10 10:23:13",
      "description": "Right now to check leader we use ScmContext#isLeader, this gets updated by notifyLeaderChanged.\r\n\r\nBut SCM server should start accepting requests when it is leader and isLeaderReady. \r\n\r\nWe need isLeaderReady also because Statemachine should apply all the log committed transactions to start accepting requests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use scm#checkLeader before processing client requests "
   },
   {
      "_id": "13377594",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-05-10 09:51:53",
      "description": "{code:java}\r\nwhile invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/172.19.0.7:9860 after 8 failover attempts. Trying to failover after sleeping for 2000ms.\r\ncom.google.protobuf.ServiceException: java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: \"scm1\":9860; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost, while invoking $Proxy19.submitRequest over nodeId=scm1,nodeAddress=scm1:9860 after 9 failover attempts. Trying to failover after sleeping for 2000ms.\r\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:7fc05067-c144-4c14-880b-e47f1e40599b is not the leader. Suggested leader is Server:scm3:9860.\r\n\tat org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)\r\n\tat org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:191)\r\n\tat org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:144)\r\n\tat org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:43838)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\r\n, while invoking $Proxy19.submitRequest over nodeId=scm2,nodeAddress=scm2/172.19.0.2:9860 after 10 failover attempts. Trying to failover after sleeping for 2000ms.\r\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.net.UnknownHostException): scm1\r\n\tat java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)\r\n\tat java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)\r\n\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)\r\n\tat java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)\r\n\tat java.base/java.net.InetAddress.getByName(InetAddress.java:1252)\r\n\tat org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.getRatisRoles(SCMRatisServerImpl.java:233)\r\n\tat org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.getScmInfo(SCMClientProtocolServer.java:579)\r\n\tat org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.getScmInfo(StorageContainerLocationProtocolServerSideTranslatorPB.java:506)\r\n\tat org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:249)\r\n\tat org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)\r\n\tat org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:149)\r\n\tat org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:43838)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\r\n, while invoking $Proxy19.submitRequest over nodeId=scm3,nodeAddress=scm3/172.19.0.7:9860 after 11 failover attempts. Trying to failover after sleeping for 2000ms.\r\n{code}\r\n\r\nIf one of the host is unresolvable roles command keep on failing.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix scm roles command if one of the host is unresolvable"
   },
   {
      "_id": "13377580",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-10 08:35:29",
      "description": "Please see: https://github.com/apache/ozone-docker/pull/19",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create docker image for Apache Ozone 1.1.0 release"
   },
   {
      "_id": "13377573",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-05-10 07:07:54",
      "description": "[Maven 3.8.1|https://maven.apache.org/docs/3.8.1/release-notes.html] blocks http repos by default.  Need to update JBoss repo definition in {{pom.xml}} to https.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use https for JBoss repo"
   },
   {
      "_id": "13377090",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-05-06 20:05:33",
      "description": "{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/05/06/7723/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient.txt}\r\nTests run: 80, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 175.065 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient\r\ntestListVolume  Time elapsed: 0.086 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<20> but was:<21>\r\n  ...\r\n  at org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testListVolume(TestOzoneRpcClientAbstract.java:1867)\r\n{noformat}\r\n\r\n{noformat:title=https://github.com/elek/ozone-build-results/blob/master/2021/05/06/7723/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClient-output.txt}\r\n2021-05-06 13:33:08,991 [Time-limited test] INFO  rpc.RpcClient (RpcClient.java:createVolume(320)) - Creating Volume: vol-71454, with jenkins1001 as owner and space quota set to -1 bytes, counts quota set to -1\r\n...\r\n2021-05-06 13:33:09,650 [Time-limited test] INFO  rpc.RpcClient (RpcClient.java:createVolume(320)) - Creating Volume: vol-714-a-0-78727, with jenkins1001 as owner and space quota set to -1 bytes, counts quota set to -1\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestOzoneRpcClient due to volume name conflict"
   },
   {
      "_id": "13376628",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-05-04 19:58:31",
      "description": "After upgrading my hugo to 0.83.1, a maven build would fail at hadoop-hdds-docs for me.\r\n\r\n{code}\r\n[INFO] --- exec-maven-plugin:1.3.1:exec (default) @ hadoop-hdds-docs ---\r\nError: Error building site: \".../hadoop-hdds/docs/content/interface/ReconApi.md:1:1\": [BUG] goldmark: runtime error: slice bounds out of range [3503:3501]: create an issue on GitHub attaching the file in: /var/folders/yp/sxzthlnd45vdwcwvdldf_12r0000gp/T/hugo_bugs/goldmark_267fdfa526d051bdccb7cf9a98e0772d.txt\r\nStart building sites \u2026\r\ngoroutine 60 [running]:\r\nruntime/debug.Stack(0x5f70c80, 0x6b7c280, 0xc00117c310)\r\n\truntime/debug/stack.go:24 +0x9f\r\ngithub.com/gohugoio/hugo/markup/goldmark.(*goldmarkConverter).Convert.func1(0xc000a2e6c0, 0xc000d25500, 0x2b24, 0x3038, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n{code}\r\n\r\nReconApi.md itself hasn't changed since Nov 2020, it's also unlikely that other markdowns have caused this. This issue can be simply repro'ed with:\r\n\r\n{code}\r\n./hadoop-hdds/docs/dev-support/bin/generate-site.sh\r\n{code}\r\n\r\non hugo 0.83.0 or 0.83.1.\r\n\r\nAfter reverting hugo to 0.82.1 (homebrew build), the problem goes away for me.\r\n\r\n{code}\r\n$ hugo version\r\nhugo v0.82.1+extended darwin/amd64 BuildDate=unknown\r\n{code}\r\n\r\nPSA: Do not upgrade hugo to 0.83.0 or 0.83.1 if you want to compile hadoop-hdds-docs, it is still broken right now.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "PSA: hugo 0.83.0 / 0.83.1 broke hadoop-hdds/docs/dev-support/bin/generate-site.sh"
   },
   {
      "_id": "13376578",
      "assignee": "elek",
      "components": [],
      "created": "2021-05-04 14:40:04",
      "description": "Today the closed containers are copied between datanodes as one big tar(.gz) file. Each datanode runs a GrpcReplicationService (with a grpc server) and when the SCM asks the destination-datanode to replicate data, it connects to the source datanode and retrieves the data.\r\n\r\nThis protocol is based on GRPC and very simple. After the first request (download(containerid)) the full container is streamed as a tar file in smaller chunks.\r\n\r\nHowever, this protocol doesn't have any back-pressure  / traffic control handling. After the first request the FULL 5g container is sent back. \r\n\r\nThis approach can fill up the netty buffers very easy:\r\n\r\n{code}\r\n\tException in thread \"grpc-default-executor-0\" org.apache.ratis.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 2097152 byte(s) of direct memory (used: 3651141911, max: 3652190208)\r\n\tat org.apache.ratis.thirdparty.io.netty.util.internal.PlatformDependent.incrementMemoryCounter(PlatformDependent.java:802)\r\n\tat org.apache.ratis.thirdparty.io.netty.util.internal.PlatformDependent.allocateDirectNoCleaner(PlatformDependent.java:731)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PoolArena$DirectArena.allocateDirect(PoolArena.java:632)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:607)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:202)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.tcacheAllocateNormal(PoolArena.java:186)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:136)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PoolArena.allocate(PoolArena.java:126)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:395)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)\r\n\tat org.apache.ratis.thirdparty.io.netty.buffer.AbstractByteBufAllocator.buffer(AbstractByteBufAllocator.java:123)\r\n\tat org.apache.ratis.thirdparty.io.grpc.netty.NettyWritableBufferAllocator.allocate(NettyWritableBufferAllocator.java:51)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writeKnownLengthUncompressed(MessageFramer.java:227)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writeUncompressed(MessageFramer.java:168)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.MessageFramer.writePayload(MessageFramer.java:141)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.AbstractStream.writeMessage(AbstractStream.java:65)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl.sendMessageInternal(ServerCallImpl.java:167)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl.sendMessage(ServerCallImpl.java:149)\r\n\tat org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$ServerCallStreamObserverImpl.onNext(ServerCalls.java:365)\r\n\tat org.apache.hadoop.ozone.container.replication.GrpcOutputStream.flushBuffer(GrpcOutputStream.java:124)\r\n\tat org.apache.hadoop.ozone.container.replication.GrpcOutputStream.write(GrpcOutputStream.java:90)\r\n\tat org.apache.hadoop.ozone.freon.ContentGenerator.write(ContentGenerator.java:76)\r\n\tat org.apache.hadoop.ozone.freon.ClosedContainerStreamGenerator.copyData(ClosedContainerStreamGenerator.java:19)\r\n\tat org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:56)\r\n\tat org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolServiceGrpc.java:219)\r\n\tat org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\r\n\tat org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\r\n\tat org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\r\n\tat org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:331)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:814)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n\tat org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n2021-05-04 16:37:47,996 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:37:48,998 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:37:49,998 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n\r\n{code}\r\n\r\nThis can be reproduced locally with a simple freon test. (See the code here: https://github.com/elek/ozone/tree/grpc-push)\r\n\r\nThe new freon test starts a GrpcServer and client. On server side the source is replaced with a simple `ContainerReplicationSource` which generates random 5g datastream (instead of reading container data from disk).\r\n\r\nOn the client side the replicator just downloads the container to the tmp location, but it's not moved to the final location.\r\n\r\nThis test works well for one container, but the test clearly shows that the full container data is streamed at the very beginning:\r\n\r\n(Duplicated lines are removed)\r\n\r\n{code}\r\n2021-05-04 16:21:04,281 INFO  replication.DownloadAndDiscardReplicator (DownloadAndDiscardReplicator.java:replicate(62)) - Starting replication of container 0 from [7369fd21-7ee9-4780-a54b-5831e951ca9c{ip: 127.0.0.1, host: localhost, ports: [REPLICATION=41379], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}]\r\n2021-05-04 16:21:04,434 INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(52)) - Streaming container data (0) to other datanode\r\n2021-05-04 16:21:05,269 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:21:06,270 INFO  freon.ProgressBar \r\n...\r\n2021-05-04 16:21:10,275 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:21:11,275 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:21:11,791 INFO  replication.GrpcOutputStream (GrpcOutputStream.java:close(104)) - Sent 5368709120 bytes for container 0\r\n...\r\n2021-05-04 16:21:33,434 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:21:33,737 INFO  replication.GrpcReplicationClient (GrpcReplicationClient.java:onCompleted(190)) - Container 0 is downloaded to /tmp/container-copy/container-0.tar.gz\r\n2021-05-04 16:21:34,434 INFO  freon.ProgressBar (ProgressBar.java:logProgressBar(163)) - Progress: 0.00 % (0 out of 1)\r\n2021-05-04 16:21:34,690 INFO  replication.DownloadAndDiscardReplicator (DownloadAndDiscardReplicator.java:replicate(71)) - Container is downloaded but deleted, as you wished /tmp/container-copy/container-0.tar.gz\r\n{code}\r\n\r\nAs you can see the full 5G data is sent out at 16:21:11 (after 6 seconds), but the data copy is finished only at  16:21:33 (22 more seconds).\r\n\r\nBetween the two time the majority of the container is kept in the GRPC/netty buffers.\r\n\r\nAs an experiment we can make the grpc client slow (GrpcReplicationClient):\r\n\r\n{code}\r\n    @Override\r\n    public void onNext(CopyContainerResponseProto chunk) {\r\n      try {\r\n        try {\r\n          Thread.sleep(1_000);\r\n        } catch (InterruptedException e) {\r\n          e.printStackTrace();\r\n        }\r\n        chunk.getData().writeTo(stream);\r\n      } catch (IOException e) {\r\n        response.completeExceptionally(e);\r\n      }\r\n    }\r\n{code}\r\n\r\nWith this method we download the beginning of the container very slowly, and this is enough to get the exception above.\r\n\r\n{code}\r\nException in thread \"grpc-default-executor-0\" org.apache.ratis.thirdparty.io.netty.util.internal.OutOfDirectMemoryError: failed to allocate 2097152 byte(s) of direct memory (used: 3651141911, max: 3652190208)\r\n{code}\r\n\r\nTemporary it can be fixed with increasing the netty memory: -Dorg.apache.ratis.thirdparty.io.netty.maxDirectMemory=16000000000  but it's not a good long-term solution.\r\n\r\nSo we need to refactor the protocol to do a request/response chunk by chunk.\r\n\r\nBut we also have another problem. GRPC is not optimal for fast streaming.\r\n\r\nThe previous log showed that we replicated the container (5G) under 30 seconds (without reading the original container and without doing tar file compression).\r\n\r\nThis is 5 / 30 = 170 Mb / sec. (I wrote to a tmpfs on the destination side, but even my nvme is significant faster).\r\n\r\nBased on [this|https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html] article the best (!) results (with C client!) were 1 Gb/s with GRPC. (with the explained black magic it is doubled).\r\n\r\nAnsh Khanna earlier did some low-level benchmarking (for ratis streaming) which showed 5x difference between pure netty and GRPC:\r\n\r\nFlatbuffers over GRPC\r\n%CPU in Buffer Copying/Allocations: >10%\r\nTime(in seconds): 16.44\r\n \r\nProtobuffers over GRPC:\r\n%CPU in Buffer Copying/Allocations: ~10%\r\nTime(in seconds): 11.66\r\n\r\nNetty Based Streaming\r\n%CPU in Buffer Copying/Allocations: 0%\r\nTime(in seconds): 2.7\r\n\r\nPure netty also supports zero copy async stream.\r\n\r\nSummary:\r\n 1. The current implementation should be refactored to avoid pushing the data\r\n 2. Netty seems to be better for long-term solution\r\n\r\n--> As a results it seems to be easier to create a POC with netty support and check how does it work.\r\n\r\nEarlier I made an attempt which can be found here: https://github.com/elek/ozone/tree/close-container-replication-refactor\r\n\r\nIt's a generic interface which may also be used in https://issues.apache.org/jira/browse/HDDS-5142 But at least it can be used to compare the netty vs GRPC performance in this situation.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Replace GRPC based closed-container replication with Netty based streaming"
   },
   {
      "_id": "13376550",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-05-04 12:17:12",
      "description": "Github Actions builds intermittently fail due to Maven transfer errors.\r\n\r\nExample:\r\n\r\n{code:title=https://github.com/apache/ozone/runs/2454639477#step:6:2576}\r\nError:  Plugin org.apache.maven.plugins:maven-shade-plugin:3.2.4 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-shade-plugin:jar:3.2.4: Could not transfer artifact org.apache.maven.plugins:maven-shade-plugin:pom:3.2.4 from/to central (https://repo.maven.apache.org/maven2): Transfer failed for https://repo.maven.apache.org/maven2/org/apache/maven/plugins/maven-shade-plugin/3.2.4/maven-shade-plugin-3.2.4.pom: Connection timed out (Read failed)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid Maven connection errors in CI"
   },
   {
      "_id": "13376514",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-05-04 08:55:58",
      "description": "Similar to HADOOP-17683 we should update despite we don't use the vulnerable API.\r\n\r\nhttps://nvd.nist.gov/vuln/detail/CVE-2021-29425\r\n\r\nIn Apache Commons IO before 2.7, When invoking the method FileNameUtils.normalize with an improper input string, like \"//../foo\", or \"\\\\..\\foo\", the result would be the same value, thus possibly providing access to files in the parent directory, but not further above (thus \"limited\" path traversal), if the calling code would use the result to construct a path value.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update commons-io to 2.8.0"
   },
   {
      "_id": "13376486",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-05-04 06:43:04",
      "description": "For AccessControlException donot perform failOver, as there is no real need.\r\n{code:java}\r\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Access denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.checkAdminAccess(StorageContainerManager.java:1454)\r\n        at org.apache.hadoop.hdds.scm.server.SCMClientProtocolServer.recommissionNodes(SCMClientProtocolServer.java:459)\r\n        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.recommissionNodes(StorageContainerLocationProtocolServerSideTranslatorPB.java:646)\r\n        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.processRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:317)\r\n        at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)\r\n        at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:155)\r\n        at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:46954)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\r\n        at java.base/java.security.AccessController.doPrivileged(Native Method)\r\n        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\r\n, while invoking $Proxy19.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.18.0.8:9860 after 14 failover attempts. Trying to failover after sleeping for 2000ms.\r\nAccess denied for user testuser2/scm@EXAMPLE.COM. Superuser privilege is required.\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "For AccessControlException do not perform failover"
   },
   {
      "_id": "13376413",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-05-03 17:02:21",
      "description": "If any acceptance test fails, {{test-all.sh}} (and in turn {{acceptance.sh}}) should exit with error code (1).  But if a successful test is run after a failing one, it will now wrongly exit with success (0).\r\n\r\n{code}\r\n$ export OZONE_TEST_SELECTOR='failing1\\|ozone-csi'\r\n$ ./hadoop-ozone/dev-support/checks/acceptance.sh\r\n...\r\n$ echo $?\r\n0\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Acceptance test may exit with 0 in case of error"
   },
   {
      "_id": "13375816",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-29 11:34:36",
      "description": "During the implementation of HDDS-5145 I realized that OmKeyArgs also uses factor/type, it seems to be easier to convert it to replicationConfig as it's an in-memory class not a protobuf which is required to be persisted.\r\n\r\nHaving a half-baked patch planning to upload it soon.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use ReplicationConfig in OmKeyArgs"
   },
   {
      "_id": "13375739",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         }
      ],
      "created": "2021-04-29 04:54:48",
      "description": "When security and ACL is enabled, but not spnego, the OMDBCheckpointServlet throws an error:\r\n{code:java}\r\n10:02:29.094 PM ERROR OMDBCheckpointServlet \r\n Permission denied: User principal 'dr.who' does not have access to /dbCheckpoint.\r\n This can happen when Ozone Manager is started with a different user.\r\n Please append 'dr.who' to OM 'ozone.administrators' config and restart OM to grant current user access to this endpoint.{code}\r\n\r\nWhen Spnego is disabled, permissions cannot be checked since HTTP request will not have an identity (kerberos principal) which is causing this error.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM DB checkpoint servlet not accessible in a secure cluster"
   },
   {
      "_id": "13375689",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2021-04-28 21:09:17",
      "description": "On the OM server side, NotLeaderException and LeaderNotReadyExceptions can be suppressed. Otherwise, OM1 log is flooded with NotLeaderExceptions as clients always try OM1 before moving to the next OM. Instead we can change these exception logs to DEBUG.\r\n\r\nSome BlockOutputStream and BlockOutputStreamEntryPool logs should be DEBUG level instead of INFO.\r\nRunning a 20GB put key operation resulted in the following console log:\r\n{code:java}\r\nozone sh key put o3://ozone1/vol2/buck2/20GB /tmp/20GB\r\n 21/03/08 19:01:05 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties\r\n 21/03/08 19:01:05 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n 21/03/08 19:01:05 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started\r\n 21/03/08 19:01:06 INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl\r\n 21/03/08 19:01:06 INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-30FCE96A6E8D->c2e9a19c-15f3-4eae-ba46-83c763d2ee8d\r\n 21/03/08 19:01:06 WARN impl.MetricRegistriesImpl: First MetricRegistry has been created without registering reporters. You may need to call MetricRegistries.global().addReporterRegistration(...) before.\r\n 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851570_chunk_43 blockID conID: 74 locID: 105855717519851570 bcsId: 7856 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 74 in CLOSING state\r\n 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851570_chunk_44 blockID conID: 74 locID: 105855717519851570 bcsId: 7856 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 74 in CLOSED state\r\n 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_1 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state\r\n 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_2 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state\r\n 21/03/08 19:02:39 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_3 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state\r\n 21/03/08 19:02:39 ERROR storage.BlockOutputStream: writing chunk failed 105855717519851571_chunk_4 blockID conID: 75 locID: 105855717519851571 bcsId: 0 with exception org.apache.ratis.protocol.exceptions.StateMachineException: org.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException from Server c2e9a19c-15f3-4eae-ba46-83c763d2ee8d@group-7EA52504D5B4: Container 75 in CLOSED state\r\n 21/03/08 19:02:41 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:02:43 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:02:45 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:02:47 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:02:48 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:02:50 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:02:52 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:02:54 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:02:56 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:02:57 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:02:59 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:01 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:02 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:04 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:06 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:07 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:10 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:11 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:13 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:15 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:17 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:18 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:20 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:22 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:23 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:28 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:30 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n21/03/08 19:03:31 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList\r\n{datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n 21/03/08 19:03:33 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [#74, #75], pipelineIds = []}\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve client and server logging"
   },
   {
      "_id": "13375560",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-04-28 09:45:26",
      "description": "Some Freon integration tests are unnecessary, as it is already widely used in acceptance tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove some Freon integration tests"
   },
   {
      "_id": "13375289",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-27 10:04:42",
      "description": "Server returns suggested leader correctly, but on client when performing regex, extraction of suggested leader address is incorrect. The issue is due to the RegEx pattern to obtain the suggested leader.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Suggested leader in Client"
   },
   {
      "_id": "13374747",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-23 23:06:26",
      "description": "Observed in this CI run:\u00a0[https://github.com/apache/ozone/pull/2179/checks?check_run_id=2423463857]\r\n\r\nBundle is attached to issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Intermittent test failure in TestContainerDeletionChoosingPolicy#testRandomChoosingPolicy"
   },
   {
      "_id": "13374647",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-23 12:06:59",
      "description": "HDDS-5047 started to use ReplicationConfig in Pipeline and pipeline related SCM service on master. ReplicationConfig was introduced in HDDS-5011 but it has two versions one for master and one for the EC branch.\r\n\r\nMerging master after HDDS-5047 requires small modification in the code to make HDDS-5047 compatible the ec version of HDDS-5011:\r\n\r\n * During the proto serialization / deserialization we should use optional ECReplicationConfig (if exists).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Extend Pipline/ReplicationConfig refactor with ECReplicationConfig"
   },
   {
      "_id": "13374638",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-23 11:44:46",
      "description": "Please see: https://github.com/apache/ozone/pull/2177",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create github check to alert when dependency tree is changed"
   },
   {
      "_id": "13374545",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-23 04:46:54",
      "description": "Currently, grpc client/server end points are used for container re-replication, reading data, scm snapshot download for SCM HA etc but have individual client and server implementations. The idea her is to define a generic interface for client/servers and use it across all components for better maintainability, better code reuse and easier debugging.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make generic streaming client/service for container re-replication,  data read, scm/om snapshot download"
   },
   {
      "_id": "13374362",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-22 09:54:46",
      "description": "This Jira is to cleanup getScmAddress and other methods in finding SCM client/block address.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor HddsUtils and HddsServerUtils"
   },
   {
      "_id": "13374188",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-21 17:47:19",
      "description": "Default timeout for one GitHub action job is 6 hours. It turned out that in some unfortunate case the acceptance tests were pending at this time, for example in https://github.com/apache/ozone/runs/2053572861?check_suite_focus=true\r\n\r\nWe need to turn on stricter timeouts for each of our jobs  to avoid unnecessary build time usage.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use timeout in github actions"
   },
   {
      "_id": "13374077",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-21 09:06:41",
      "description": "steps taken :\r\n1. SCM HA enabled in ozone cluster.\r\n2. Ran ozone sh volume list command.\r\n\r\n\u00a0\r\n\r\nexception seen :\r\n\r\n-------------------\r\n{noformat}\r\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Does not contain a valid host:port authority: <SCM CLIENT ADDRESSES> at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:213) at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:164) at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:153) at org.apache.hadoop.hdds.HddsUtils.getScmAddressForClients(HddsUtils.java:112) at org.apache.hadoop.ozone.om.OzoneManager.getServiceList(OzoneManager.java:2658) at org.apache.hadoop.ozone.om.OzoneManager.getServiceInfo(OzoneManager.java:2678) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getServiceList(OzoneManagerRequestHandler.java:451) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:176) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:190) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:132) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:122) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:986) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:914) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2887){noformat}\r\n\u00a0\r\n\r\nthis issue is due to \"ozone.scm.client.address\" config set in SCM HA setup which contains multiple SCM host addresses.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "HDDS-5127. Fix getServiceList when SCM HA is enabled"
   },
   {
      "_id": "13373931",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-04-20 19:17:29",
      "description": "As [spotted|https://github.com/apache/ozone/pull/2052#discussion_r616570112] by [~elek], {{ozonesecure}} acceptance test is run twice: with SCM Ratis enabled and disabled.  This doubles execution time.  Now that {{ozonesecure-ha}} acceptance test also has SCM HA (implies Ratis enabled), I don't think it's necessary to run {{ozonesecure}} (single SCM) with Ratis enabled.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Only test ozonesecure with SCM Ratis disabled"
   },
   {
      "_id": "13373805",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-20 09:04:41",
      "description": "Today ozonesecure compose clusters (and ozonesecure-ha and ozonesecure-mr) use an adhoc keytab issuer. The issuer is download during the image creation and uses a third party go lang application to create the keytabs on-demand.\r\n\r\nAs discussed earlier, it would be faster to use a dedicated, pre-built container image which includes the pre-created keytabs instead of issuing them on-the fly (keytab generation is slow + container creation is slow)\r\n\r\nFor each of the tagged images we can export to current keytabs to hadoop-ozone/dist/src/main/compose/ which can be mounted to to compose clusters.\r\n\r\nIt makes the overall acceptance test faster (instead of creating keytab, which is quite slow, we can start the cluster immediately). And we don't need to depend on an external utility app.\r\n\r\nPre-created keytabs are also more similar to production environment...\r\n\r\nFirst test using the apache/ozone-testkrb5 from HDDS-4938\r\n\r\nThe time between starting test.sh script and first robot test:\r\n\r\nmaster: 3:30 (01:43:08 --01:46:38)\r\nthis patch: 2:10 (12:59:29 13:02:39)\r\n\r\n(note: there are some variances between different builds, and in general the patch build was a slower one. It can be even faster).\r\n\r\n~",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use the pre-created apache/ozone-testkrb5 image during secure acceptance tests"
   },
   {
      "_id": "13373785",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-20 07:50:22",
      "description": "During SCM reinitialialisation, ratis server is spinned up to check if an existing ratis group exists or not, and closes the server without starting it. In ratis, the segmented raft log worker thraeds are started during init() itself but get closed during raftServer.close() only if the server transitions to RUNNING state which causes the issue.\r\n\r\n\u00a0\r\n{code:java}\r\nAttaching to process ID 266710, please wait...\r\nDebugger attached successfully.\r\nServer compiler detected.\r\nJVM version is 25.232-b09\r\nDeadlock Detection:No deadlocks found.Thread 266745: (state = BLOCKED)Locked ownable synchronizers:\r\n    - NoneThread 266783: (state = BLOCKED)\r\n - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)\r\n - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)\r\n - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=78, line=2078 (Compiled frame)\r\n - org.apache.ratis.util.DataBlockingQueue.poll(org.apache.ratis.util.TimeDuration) @bci=134, line=137 (Compiled frame)\r\n - org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run() @bci=16, line=292 (Interpreted frame)\r\n - org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$$Lambda$161.run() @bci=4 (Interpreted frame)\r\n - java.lang.Thread.run() @bci=11, line=748 (Interpreted frame)Locked ownable synchronizers:\r\n    - NoneThread 266761: (state = BLOCKED)Locked ownable synchronizers:\r\n    - NoneThread 266760: (state = BLOCKED)Locked ownable synchronizers:\r\n    - NoneThread 266759: (state = BLOCKED)\r\n - java.lang.Object.wait(long) @bci=0 (Interpreted frame)\r\n - java.lang.ref.ReferenceQueue.remove(long) @bci=59, line=144 (Compiled frame)\r\n - java.lang.ref.ReferenceQueue.remove() @bci=2, line=165 (Compiled frame)\r\n - java.lang.ref.Finalizer$FinalizerThread.run() @bci=36, line=216 (Interpreted frame)Locked ownable synchronizers:\r\n    - NoneThread 266758: (state = BLOCKED)\r\n - java.lang.Object.wait(long) @bci=0 (Interpreted frame)\r\n - java.lang.Object.wait() @bci=2, line=502 (Compiled frame)\r\n - java.lang.ref.Reference.tryHandlePending(boolean) @bci=54, line=191 (Compiled frame)\r\n - java.lang.ref.Reference$ReferenceHandler.run() @bci=1, line=153 (Interpreted frame)Locked ownable synchronizers:\r\n    - None\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM Reinitialization can end up leaking Ratis Segmented RaftLogWorker threads"
   },
   {
      "_id": "13373671",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-04-19 17:50:31",
      "description": "The CRLInfo class does not contain CRL Sequence ID and it would be good to include it while iterating the\u00a0 CRLInfos as a list.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "CRLInfo should include CRL Sequence ID"
   },
   {
      "_id": "13373626",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-04-19 13:54:05",
      "description": "Intermittent failure in secure acceptance tests indicates that datanode may fail to start up if SCM is not yet ready:\r\n\r\n{noformat}\r\ndatanode_3  | STARTUP_MSG: Starting HddsDatanodeService\r\n...\r\ndatanode_3  | 2021-04-19 08:20:29,030 [main] INFO ozone.HddsDatanodeService: Creating csr for DN-> subject:dn@627dcb55b990\r\n...\r\ndatanode_3  | 2021-04-19 08:20:57,660 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 627dcb55b990/172.26.0.4 to scm:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy18.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.26.0.10:9961 after 14 failover attempts. Trying to failover after sleeping for 2000ms.\r\ndatanode_3  | 2021-04-19 08:20:59,667 [main] ERROR ozone.HddsDatanodeService: Error while storing SCM signed certificate.\r\n...\r\ndatanode_3  | \tat org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.submitRequest(SCMSecurityProtocolClientSideTranslatorPB.java:104)\r\ndatanode_3  | \tat org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.getDataNodeCertificateChain(SCMSecurityProtocolClientSideTranslatorPB.java:263)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.HddsDatanodeService.getSCMSignedCert(HddsDatanodeService.java:349)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:320)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:248)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:192)\r\n...\r\ndatanode_3  | SHUTDOWN_MSG: Shutting down HddsDatanodeService at 627dcb55b990/172.26.0.4\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Secure datanode/OM may exit if cannot connect to SCM"
   },
   {
      "_id": "13372469",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-04-14 18:13:48",
      "description": "{noformat}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer\r\n-------------------------------------------------------------------------------\r\nTests run: 3, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 302.762 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer\r\ntestHttpPolicy[0](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 60.956 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  ...\r\n  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:110)\r\n\r\ntestHttpPolicy[1](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 180.331 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  ...\r\n  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:116)\r\n\r\ntestHttpPolicy[2](org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer)  Time elapsed: 60.223 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  ...\r\n  at org.apache.hadoop.hdds.scm.TestStorageContainerManagerHttpServer.testHttpPolicy(TestStorageContainerManagerHttpServer.java:110)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "TestStorageContainerManagerHttpServer fails in CI"
   },
   {
      "_id": "13372371",
      "assignee": "avijayan",
      "components": [],
      "created": "2021-04-14 14:46:20",
      "description": "Follow up from https://github.com/apache/ozone/pull/1998#discussion_r606369185.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Attempt to remove state from *UpgradeFinalizer classes."
   },
   {
      "_id": "13372181",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-14 05:04:20",
      "description": "With https://issues.apache.org/jira/browse/RATIS-1326\u00a0now fixed and ozone being updated with latest ratis, the idea here is to fix the installSnapshot behaviour in SCM HA.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Install Snapshot Mechanism in SCMStateMachine"
   },
   {
      "_id": "13371977",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-13 08:19:30",
      "description": "Please see: https://github.com/apache/ozone-site/pull/4",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Adjust download pages to use Apache Ozone (tlp) artifacts"
   },
   {
      "_id": "13371769",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-12 11:43:05",
      "description": "Today we execute ozonefs/ozonefs.robot with compose/ozone/test.sh with multiple matrix parameters:\r\n\r\n{code}\r\nfor scheme in ofs o3fs; do\r\n    for bucket in link bucket; do\r\n       #test ozonefs/ozonefs.robot\r\n   done\r\ndone\r\n{code}\r\n\r\nHDDS-2939 doubles these 4 executions with introducing the layout parameter (simple vs prefix).  At the same time the execution time of acceptance (unsecure) tests are increased from 37 minutes to 57 minutes.\r\n\r\nI would suggest suggesting to use only selected tests from this 2 x 2 x 2 matrix.\r\n\r\nFor example:\r\n\r\nbucket / o3fs / prefix\r\nlink / ofs / simple\r\nbucket / ofs / prefix\r\n\r\n ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[FSO] Reducing time of ozonefs acceptance testmatrix"
   },
   {
      "_id": "13371202",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-12 08:21:43",
      "description": "Please see: https://github.com/apache/ozone/pull/2149",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add project separation and first stable release to the HISTORY.md"
   },
   {
      "_id": "13370712",
      "assignee": "xyao",
      "components": [],
      "created": "2021-04-09 17:51:32",
      "description": "Ozone RPC Client currently create a key provider instance each time the getKeyProvider is invoked. The caller such as objectstore does not keep track of the returned KMS provider with a close(). This leads to leaks of resources associate with KMS provider (e.g. SSL).\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone RPC client leaks KeyProvider instances"
   },
   {
      "_id": "13370664",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-09 13:28:33",
      "description": "Please see: https://github.com/apache/ozone/pull/2140",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Include HISTORY.md/SECURITY.md/CONTRIBUTING.md in the release artifacts."
   },
   {
      "_id": "13370653",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-09 13:08:37",
      "description": "Please see: https://github.com/apache/ozone/pull/2139",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump version of common-compress"
   },
   {
      "_id": "13370650",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-09 13:02:25",
      "description": "See: https://github.com/apache/ozone/pull/2138",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create unit test for OzoneClient"
   },
   {
      "_id": "13370629",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337843",
            "id": "12337843",
            "name": "Go Client"
         }
      ],
      "created": "2021-04-09 10:56:14",
      "description": "Ozone Go client's {{key put}} command always (tries to) upload local {{/tmp/asd}}.  It should take the input local file from command-line parameter.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix key put implementation"
   },
   {
      "_id": "13370594",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-04-09 08:47:29",
      "description": "Enable s3 test suite for ozone-secure-ha docker which starts SCM/OM HA in secure env.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Enable s3 test suite for ozone-secure-ha"
   },
   {
      "_id": "13370353",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-04-08 10:58:00",
      "description": "Now SCM HA Security is implemented, we can remove the code added as part of HDDS-4978\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Remove code of not starting ozone services when Security is enabled on SCM HA cluster"
   },
   {
      "_id": "13370344",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-08 10:10:55",
      "description": "Please see: https://github.com/apache/ozone/pull/2131",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump Guava version"
   },
   {
      "_id": "13370342",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-08 10:07:36",
      "description": "HDDS-5011 introduced new ReplicationConfig with multiple implementations (Ratis/Standalone).\r\n\r\nTo make ozone sh client EC compatible we can use ReplicationConfig on the client side too: --replication parameter can be parsed from the string based on the ReplicationConfig. Today it can be THREE or ONE, but later ECReplicationConfig can support more sophisticated  schemes (like 3:2) ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use ReplicationConfig on client side "
   },
   {
      "_id": "13369988",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-04-07 07:59:52",
      "description": "Right now sub-ca certs use hdds.x509.default.duration.\r\nThis Jira proposes to use hdds.x509.max.duration similar to selfsigned certs",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Fix duration of sub-ca certs"
   },
   {
      "_id": "13369784",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-06 12:09:11",
      "description": "Recon build is broken since yesterday due to a new pnpm@6.0.0 release\r\n\r\n{code}\r\n[INFO] Running 'npx pnpm config set store-dir ~/.pnpm-store' in /home/elek/projects/ozone/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web\r\n[INFO] npx: installed 1 in 1.057s\r\n[INFO] ERROR: This version of pnpm requires at least Node.js v12.17\r\n[INFO] The current version of Node.js is v12.14.1\r\n[INFO] Visit https://r.pnpm.io/comp to see the list of past pnpm versions with respective Node.js version support.\r\n{code}\r\n\r\nThis is because the frontend maven plugin uses [npx|https://www.npmjs.com/package/npx] which downloads the required tool (pnpm in our case) on-demand if it's not available locally.\r\n\r\nThis download uses the latest version (by default).  \r\n\r\nI recommend using a fixed version from pnpm to avoid any unexpected error when external tools is updated.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Use fixed vesion from pnpm to build recon"
   },
   {
      "_id": "13369734",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-06 09:15:53",
      "description": "Please see: https://github.com/apache/ozone/pull/2112",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix project name in NOTICE.txt"
   },
   {
      "_id": "13369730",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-06 09:00:29",
      "description": "When we create a release artifact today it has the `hadoop-` prefix in the names. Would be better to remove to avoid confusion.\r\n\r\nNote: full `hadoop-` prefix is removed by HDDS-4936 (thanks to [~sammichen]), this patch is a very small subset of that patch just to fix the tar file name on the release branch.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hadoop- prefixes from release artifacts (release branch)"
   },
   {
      "_id": "13369711",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-04-06 07:33:45",
      "description": "IN SCM HA, the primary node starts up the ratis server while other bootstrapping nodes will get added to the ratis group. Now, if all the bootstrapping SCM's get stopped, the primary node will now step down from leadership as it will loose majority. If the bootstrapping nodes are now bootstrapped again,\u00a0 the bootsrapping node will try to first validate the cluster id from the leader SCM with the persisted cluster id , but as there is no leader existing, bootstrapping wil keep on failing and retrying until it shuts down.\u00a0\r\n\r\nThe issue can be very easily simulated in kubernetes deployments, where bootstrap and init cmds are run repeatedly on every restart.\r\n\r\nThe Jira aims to bypass the cluster id validation if a bootstrapping node already has a cluster id.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a config to bypass clusterId validation for bootstrapping SCM"
   },
   {
      "_id": "13369167",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-04-01 16:51:27",
      "description": "Currently, a bootstrapping SCM or a follower SCM can request a SCM rocks dn checkpoint from the leader via external grpc channel. The Jira here aims to ensure the channel is secured before any transfer takes place between SCMs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Make InterSCM grpc channel secure"
   },
   {
      "_id": "13369156",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2021-04-01 16:25:25",
      "description": "Currently in an OM HA setup, client by default does 15 failovers before giving up. If there are 3 OMs and a leader has not been elected yet, then the client will try each OM in a round robin way, wait for 2 secs and then try all the OMs again. This gives the client around 10 seconds before it exhausts all its failover attempts and fails.\u00a0\r\n\r\nThis Jira proposes to increase the failover attempts to 100.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Increase number of client retries/ failovers to OMs"
   },
   {
      "_id": "13369089",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-04-01 10:28:25",
      "description": "Previously during init of OM for getScmInfo we used to do RetryForEverWithFixedSleep, but during SCM HA we have removed this.\r\n\r\nThis Jira proposes to add a ceration duration to try getScmInfo, instead of retry forever with fixed sleep.\r\n\r\nIn a few docker tests CI run, we have seen this issue, after 15 retries Om init failed, as SCM is started later.\r\n\r\n\r\n{code:java}\r\nom1_1       | 2021-03-31 17:03:48,184 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.\r\nom1_1       | 2021-03-31 17:03:52,453 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:03:54,455 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:03:56,457 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:03:58,466 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:00,498 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:02,522 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:04,533 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:06,535 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:08,537 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:10,541 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:12,543 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:14,546 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/172.20.0.8:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:16,550 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/172.20.0.6:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:18,553 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From om1/172.20.0.4 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/172.20.0.7:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.\r\nom1_1       | 2021-03-31 17:04:20,795 [main] ERROR om.OzoneManager: Could not initialize OM version file\r\nom1_1       | org.apache.hadoop.ipc.RemoteException(org.apache.ratis.protocol.exceptions.NotLeaderException): Server 9cb7a7ae-4c40-401c-b1c6-55728c1f0907@group-C35E1BD0DE21 is not the leader\r\nom1_1       | \tat org.apache.hadoop.hdds.scm.ha.SCMRatisServerImpl.triggerNotLeaderException(SCMRatisServerImpl.java:245)\r\nom1_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:108)\r\nom1_1       | \tat org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13874)\r\nom1_1       | \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\nom1_1       | \tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\r\nom1_1       | \tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\r\nom1_1       | \tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\r\nom1_1       | \tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\nom1_1       | \tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\nom1_1       | \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\nom1_1       | \tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\r\nom1_1       | \r\n{code}\r\n\r\n\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make getScmInfo retry for a duration"
   },
   {
      "_id": "13369078",
      "assignee": "elek",
      "components": [],
      "created": "2021-04-01 10:02:01",
      "description": "Please see: https://github.com/apache/ozone/pull/2106",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove backward direction dependency betweeon HDDS->Ozone"
   },
   {
      "_id": "13368847",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-31 10:53:11",
      "description": "This Jira is to handle leader change between getScmInfo and getScmSignedCert.\r\n\r\n*Problem:*\r\n*Leader is SCM1 - Returned SCMID is SCM1ID*\r\nScmInfo returns the leader SCMID\r\n\r\n*Leader is SCM2 - SCM ID is SCM2ID*\r\ngetSCMSignedCert, during generate certificate it has a check compare the scmId passed in CSR, is same as current SCM scmID\r\n\r\nIn this case when the leader change between these 2 calls OM will fail to get a Certificate.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Handle leader changes between SCMInfo and getSCMSigned Cert in OM"
   },
   {
      "_id": "13368824",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-03-31 09:04:22",
      "description": "Currently, ratis suggested leader info is not propagated to rpc clients for ratis request failures. Idea here is to propagate this info and perform failover accordingly if request fail with NotLeaderException.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure failover to suggested leader if any for NotLeaderException"
   },
   {
      "_id": "13368821",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-03-31 08:52:52",
      "description": "For SCM HA, on certain exceptions , for example, LeaderNotReady, the requests must be retried on the same server. For exceptions, such as NotLeaderException, retry should happen along with a failover for rpc clients to the suggested leader if possible. The idea here is to address the retry policy requirements for SCM HA.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add retry policy for ratis requests in SCM HA."
   },
   {
      "_id": "13368819",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-03-31 08:51:28",
      "description": "In SCM HA, the requests submitted to ratis leader can potentially hang. The idea here is to timeout the request if the response is not received after a certain threshold.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add timeout support for ratis requests in SCM HA"
   },
   {
      "_id": "13368671",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-30 15:00:58",
      "description": "HDDS-5011 introduces Java ReplicationConfig classes which can be used as a replacement of replicationType and replicationFactor.\r\n\r\nFirst task is replacing type/factor with ReplicationConfig in Pipeline and related managers (PipelineManager BackgroundPipelineCreatorV2, PipelineStateManager...)\r\n\r\nWe can do it on the master without the EC related stuff... (later we will add the small part which is required for EC",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor Pipeline to use ReplicationConfig instead of factor/type"
   },
   {
      "_id": "13368669",
      "assignee": "avijayan",
      "components": [],
      "created": "2021-03-30 14:55:34",
      "description": "Merge master with SCM HA changes into HDDS-3698-nonrolling-upgrade branch.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge master with SCM HA changes into upgrade branch."
   },
   {
      "_id": "13367983",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334613",
            "id": "12334613",
            "name": "website"
         }
      ],
      "created": "2021-03-26 18:09:05",
      "description": "Apply merge and notification settings via {{.asf.yaml}} from other Ozone repos to {{ozone-site}} repo.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Apply merge/notification settings to ozone-site repo"
   },
   {
      "_id": "13367789",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334613",
            "id": "12334613",
            "name": "website"
         }
      ],
      "created": "2021-03-26 08:03:13",
      "description": "The website has references to Hadoop wiki, from where pages were moved to new space.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Update wiki links in website"
   },
   {
      "_id": "13367276",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-24 15:26:32",
      "description": "https://github.com/apache/ozone/pull/2000#discussion_r599427607",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Handle leader changes during bootstrap"
   },
   {
      "_id": "13367226",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-24 12:40:32",
      "description": "Similar to HDDS-4988, we should also cancel PR builds that are already failing.  This is an improvement over HDDS-4933, where only concurrent builds of the same check could be cancelled.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cancel failing PR workflow runs"
   },
   {
      "_id": "13367178",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2021-03-24 08:16:50",
      "description": "Async Profiler default output format is being changed in HDDS-5009 to one not supported by version 1.5.  We need to bump the version being installed in the image.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Upgrade Async Profiler"
   },
   {
      "_id": "13366982",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-23 13:05:32",
      "description": "Please see: https://github.com/apache/ozone/pull/2076",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Bump ratis version to 2.0.0"
   },
   {
      "_id": "13366766",
      "assignee": "avijayan",
      "components": [],
      "created": "2021-03-22 16:09:57",
      "description": "Documentation on upgrade design, how to perform an upgrade & upgrade framework developer primer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upload upgrade design documentation to docs module."
   },
   {
      "_id": "13366713",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-22 12:08:27",
      "description": "SCM proto file should be extended to use ECReplicationConfig which can be de-serialized to a specific ReplicationConfiguration.\r\n\r\nNote: this is the bare minimum version of HDDS-4882 which doesn't include the rafactor of the existing proto/persistent fields but de-/serialize them to the new java pojos.\r\n ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Introduce EC ReplicationConfig and Java based ReplicationConfig implementation"
   },
   {
      "_id": "13366400",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2021-03-19 17:00:16",
      "description": "We're running the official Ozone 1.0.0 release and facing S3 Multipart Upload failures with large files. The error message looks similar to that is reported in HDDS-3554 but we'd like to report what we've found so far to help the further investigation of this issue.\r\nh1. The error message recorded in OM log\r\n\r\nPlease find the following error message excerpted from our OM. Forgive us we redacted some sensitive information such as username and keyname which imply our project's topic.\r\n{code:java}\r\n2021-03-14 07:48:41,947 [IPC Server handler 88 on default port 9862] ERROR org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest: MultipartUpload Complete request failed for Key: <REDACTED_KEYNAME> in Volume/Bucket s3v/<BUCKETNAME>\r\nINVALID_PART org.apache.hadoop.ozone.om.exceptions.OMException: Complete Multipart Upload Failed: volume: s3v bucket: <BUCKETNAME> key: <REDACTED_KEYNAME>. Provided Part info is { /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282, 4}, whereas OM has partName /s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.om.request.s3.multipart.S3MultipartUploadCompleteRequest.validateAndUpdateCache(S3MultipartUploadCompleteRequest.java:199)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:224)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:145)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915){code}\r\nAnyway, OM thinks the partName for the partNumber 4 is\u00a0/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406\u00a0but COMPLETE_MULTIPART_UPLOAD request think it must be\u00a0/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282. This discrepancy is the immediate cause for this error.\r\nh1. OM audit log says both are correct\r\n\r\nPlease find attached\u00a0om-audit-HOSTNAME-2021-03-14-19-53-09-1.log.gz\u00a0(also redacted, sorry), it contains filtered output of our OM audit log, the lines which include\u00a0<REDACTED_KEYNAME>\u00a0and multipartList entry are remain.\r\n\r\nInterestingly, according to the OM audit log, there're two COMMIT_MULTIPART_UPLOAD_PARTKEY operations exist for partNumber=4 and both operations were succeeded:\r\n\r\n\u00a0\r\n{code:java}\r\n% zgrep partNumber=4, om-audit-HOSTNAME-2021-03-14-19-53-09-1.log.gz\r\n2021-03-14 07:16:04,992 | INFO\u00a0 | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=8388608, replicationType=RATIS, replicationFactor=ONE, partNumber=4, partName=/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282} | ret=SUCCESS |\u00a0\r\n2021-03-14 07:18:11,828 | INFO\u00a0 | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=8388608, replicationType=RATIS, replicationFactor=ONE, partNumber=4, partName=/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629180406} | ret=SUCCESS |\u00a0\r\n%\r\n{code}\r\n\u00a0\r\n\r\nOM seemed to have accepted both partName ending with\u00a0105884795658268282 and\u00a0\u00a0105884791629180406 for partNumber 4. And COMPLETE_MULTIPART_UPLOAD operation was called with the prior partName but OM believed it had the latter partName for partNumber 4.\r\n\r\n\u00a0\r\n{code:java}\r\n2021-03-14 07:48:41,947 | ERROR | OMAudit | user=<REDACTED_UPN> | ip=10.192.17.172 | op=COMPLETE_MULTIPART_UPLOAD {volume=s3v, bucket=<BUCKETNAME>, key=<REDACTED_KEYNAME>, dataSize=0, replicationType=\r\nRATIS, replicationFactor=ONE, multipartList=[partNumber: 1\r\npartName: \"/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791631605244\"\r\n, partNumber: 2\r\npartName: \"/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791631539707\"\r\n, partNumber: 3\r\npartName: \"/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791628262900\"\r\n, partNumber: 4\r\npartName: \"/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884795658268282\"\r\n, partNumber: 5\r\npartName: \"/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629245944\"\r\n, partNumber: 6\r\npartName: \"/s3v/<BUCKETNAME>/<REDACTED_KEYNAME>105884791629245943\"\r\n{code}\r\nWe can also find there're multiple COMMIT_MULTIPART_UPLOAD_PARTKEY operations for several partNumbers, such as partNumber 4, 13, 20, 45, 57, 67, 73, ... some partNumbers like 172 have more than three COMMIT_MULTIPART_UPLOAD_PARTKEY operations they're all succeeded.\r\n\r\n\u00a0\r\nh1. How to solve this issue?\r\n\r\nAt first we thought this issue is caused by race condition, but noticed that there're enough time between each COMMIT_MULTIPART_UPLOAD_PARKEY operation. We're not sure but noticed that write operations to OmMetadataManager are isolated with\u00a0omMetadataManager.getLock().acquireWriteLock(BUCKET_LOCK, volumeName, bucketName);\r\n\r\n\u00a0\r\n{code:java}\r\n     multipartKey = omMetadataManager.getMultipartKey(volumeName,\r\n         bucketName, keyName, uploadID);\r\n\r\n     // TODO to support S3 ACL later.\r\n\r\n     acquiredLock = omMetadataManager.getLock().acquireWriteLock(BUCKET_LOCK,\r\n         volumeName, bucketName);\r\n\r\n     validateBucketAndVolume(omMetadataManager, volumeName, bucketName);\r\n\r\n     String ozoneKey = omMetadataManager.getOzoneKey(\r\n         volumeName, bucketName, keyName);\r\n\r\n     OmMultipartKeyInfo multipartKeyInfo = omMetadataManager\r\n         .getMultipartInfoTable().get(multipartKey);\r\n{code}\r\n\u00a0\r\n\r\nSo our question is, is it normal to have multiple COMMIT_MULTIPART_UPLOAD_PARTKEY operations for a partNumber, with different partNames?\r\nh1. Other findings\r\n\r\nThis issue occurs less frequently with\u00a0aws configure set default.s3.multipart_chunksize 256MB. Almost always fails with multipart_chunksize 8MB, 1GB in our environment.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Multipart Upload fails due to partName mismatch"
   },
   {
      "_id": "13366337",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-03-19 12:11:49",
      "description": "Upgrade Jersey 2.27 to 2.32+.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upgrade Jersey2 dependency"
   },
   {
      "_id": "13366252",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-19 05:47:00",
      "description": "This Jira is to make storeValidCertificate idempotent so that during replay it does not cause any issues.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Make storeValidCertificate method idempotent"
   },
   {
      "_id": "13366176",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-18 20:28:59",
      "description": "Upgrade {{kotlin-stdlib}}, transitive dependency via {{jaeger-client}}, to 1.4.21+.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upgrade kotlin-stdlib"
   },
   {
      "_id": "13366149",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-18 17:33:34",
      "description": "Github pull requests can be created as (or converted to) draft.  This is useful for gathering early feedback from other contributors about proof-of-concept code.  Such code may not even compile, and style issues are usually ignored.\r\n\r\nSince further commits are generally expected in this case, I think it makes sense to skip CI build/test until the PR is marked as \"ready for review\".\r\n\r\nIf CI feedback is desired, the author should enable workflows in their fork so we can check push builds there.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Skip CI for draft pull requests"
   },
   {
      "_id": "13365949",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-03-17 21:38:56",
      "description": "https://github.com/apache/ozone/blob/db3c50d9fc4b1048cd83074343cc00444f0b24f7/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L148\r\n\r\n{code}\r\n    long len = info.getLen();\r\n    long offset = info.getOffset();\r\n    ByteBuffer data = ByteBuffer.allocate((int) len);\r\n{code}\r\n\r\nDN reserves a byte buffer for client's chunk read request based on whatever client specifies, without check. This is bad. Client can send a message forcing DN to allocate up to 2GB memory per request. (Fortunately the grpc handler captures the OOM exception so DN doesn't crash)\r\n\r\nPropose: add a guardrail check. XceiverServerGrpc set max inbound message size as 32MB (OZONE_SCM_CHUNK_MAX_SIZE). We should make sure the requested length is less than this too.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add guardrail for reserved buffer size when DN reads a chunk"
   },
   {
      "_id": "13365652",
      "assignee": "avijayan",
      "components": [],
      "created": "2021-03-16 22:35:21",
      "description": "* Introduce 2 new phases of upgrade action hooks (per layout feature)\r\n *Prefinalized state validation* - A layout feature (version) can use this to validate that the component is not started up in a way to use it before finalization. For example, an SCM HA validation action can make sure HA is not enabled before finalization. \r\n*First Upgrade Start action* - Run exactly once when a component is started up after an upgrade with an unfinalized layout feature.\r\n* Annotation based registration of layout actions to the layout features. After this change, an HDDS upgrade action be created like this.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Introduce First upgrade startup action and Pre-finalized state validation in Layout Feature."
   },
   {
      "_id": "13365585",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-16 16:27:48",
      "description": "CI runs for pull requests with multiple commits can take up too much resources.  We should cancel previous runs, as only the last one is considered for PR status.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cancel duplicate PR workflows"
   },
   {
      "_id": "13365428",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-16 07:11:48",
      "description": "\r\n{code:java}\r\nscm_1       | 2021-03-16 05:39:38,023 [a3c3a50a-98c8-4517-b619-4e0715dc1ef7@group-9A1DAB6BC185-StateMachineUpdater] ERROR statemachine.StateMachine: Terminating with exit status 1: org.apache.hadoop.hdds.scm.server.SCMCertStore.storeValidCertificate(java.math.BigInteger, sun.security.x509.X509CertImpl, org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType)\r\nscm_1       | com.google.protobuf.InvalidProtocolBufferException: org.apache.hadoop.hdds.scm.server.SCMCertStore.storeValidCertificate(java.math.BigInteger, sun.security.x509.X509CertImpl, org.apache.hadoop.hdds.protocol.proto.HddsProtos$NodeType)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.ha.SCMStateMachine.process(SCMStateMachine.java:164)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.ha.SCMStateMachine.applyTransaction(SCMStateMachine.java:134)\r\nscm_1       | \tat org.apache.ratis.server.impl.RaftServerImpl.applyLogToStateMachine(RaftServerImpl.java:1659)\r\nscm_1       | \tat org.apache.ratis.server.impl.StateMachineUpdater.applyLog(StateMachineUpdater.java:232)\r\nscm_1       | \tat org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:177)\r\nscm_1       | \tat java.base/java.lang.Thread.run(Thread.java:834)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] When Ratis is enabled, SCM secure cluster is not working"
   },
   {
      "_id": "13365220",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-15 11:32:13",
      "description": "SCM HA security work is still in progress.\r\n\r\n[~elek] Brought up the point that until before merge of SCM HA branch we should add safeguard check to fail bringing up the cluster",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Ozone services should be disabled in SCM HA enabled and security enabled cluster"
   },
   {
      "_id": "13363816",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-11 10:55:58",
      "description": "Add steps to check scripts for installing dependencies (eg. {{bats}} or {{spotbugs}}). This reduces complexity of Github-specific code in the workflow definition, and allows simpler prototyping/testing.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Extract check dependency installation from Github Actions workflow"
   },
   {
      "_id": "13363613",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-03-10 17:01:46",
      "description": "The test assumes that a follower node must be behind the leader during the final validation assertions. Fix is to not start the follower server itself , so that it always lags behind the leader at any point of time.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix flaky test TestSCMInstallSnapshotWithHA#testInstallCorruptedCheckpointFailure"
   },
   {
      "_id": "13363562",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-10 14:12:15",
      "description": "This Jira is to make DB updates of CertStore go via ratis, so that all SCM's can be in sync.\r\n\r\nIn this way, OM/DN/SCM Certs will be in sync across Ratis.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Make CertStore DB updates for StoreValidateCertificate go via Ratis"
   },
   {
      "_id": "13363560",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-10 14:10:04",
      "description": "This Jira is to implement an API that lists all CA's of SCM nodes along with RootCA.\r\n\r\n*Example output returned by this new API:*\r\nSCM1 CA\r\nSCM2 CA\r\nSCM3 CA\r\nSCM1 RootCA\r\n\r\nAnd to implement getRootCA which returns the root CA which has signed the certificate for other SCMs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Implement listCAs and getRootCA API"
   },
   {
      "_id": "13363554",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-10 13:37:22",
      "description": "HDDS-4896 provides a very elegant way to initialize SCM HA in containerized environment:\r\n\r\nBoth `--init` and `--bootstrap` can be executed, but when `ozone.scm.primordial.node.id` is set, the init is ignored on non-primordial, the bootstrap ignored on primordial nodes.\r\n\r\nTo make it even easier to use in k8s environments I suggest to return with exit code 0 in these cases as init proces shouldn't be repeated in these cases ignoring the init/bootstrap is part of the expected workflow.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Return with exit code 0 in case of optional scm bootstrap/init "
   },
   {
      "_id": "13363550",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-10 13:28:44",
      "description": "We already have Kubernetes examples for Ozone cluster to show how HA can be supported in Kubernetes environment seems to be good idea to provide example for full HA Ozone cluster.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide example k8s files to run full HA Ozone"
   },
   {
      "_id": "13363540",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-03-10 12:36:33",
      "description": "\r\n{code:java}\r\nscm1_1       | java.lang.NullPointerException\r\nscm1_1       | \tat org.apache.hadoop.hdds.scm.ha.SCMStateMachine.close(SCMStateMachine.java:297)\r\nscm1_1       | \tat org.apache.ratis.server.impl.StateMachineUpdater.stop(StateMachineUpdater.java:130)\r\nscm1_1       | \tat org.apache.ratis.server.impl.StateMachineUpdater.run(StateMachineUpdater.java:184)\r\nscm1_1       | \tat java.base/java.lang.Thread.run(Thread.java:834)\r\n{code}\r\n\r\nThis is due to during SCM init, RatisServer is started with StateMachine initialized false.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "NullPointerException during SCM init"
   },
   {
      "_id": "13363445",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-10 07:41:29",
      "description": "we need to display how to set up SCM HA.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[Doc] Add SCM HA Setup Doc"
   },
   {
      "_id": "13363371",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-09 20:58:30",
      "description": "Combined coverage calculation consists of:\r\n\r\n# download coverage files created by separate checks\r\n# merge coverage files\r\n# full build\r\n# extract classes from jars\r\n# produce jacoco report\r\n\r\nInstead of the full build, we could reuse the binaries created in _compile_ check, to save some time.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Reuse compiled binaries in combined coverage calculation"
   },
   {
      "_id": "13363282",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-09 14:42:35",
      "description": "Please see: https://github.com/apache/ozone-docker-testkrb5/pull/1",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide testkrb5 image for faster ozonesecure tests"
   },
   {
      "_id": "13363276",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-09 14:10:58",
      "description": "In HDDS-4882 a new ReplicationConfig is introduced. This patch shows how can it be used between OM and SCM on the protocol.\r\n\r\nThis patch is not a full refactor of SCM it focuses on the SCM protocol side only. Pipeline manager can be improved in follow-up patches...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enhance SCMServerProtocol with using ReplicationConfig"
   },
   {
      "_id": "13363225",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-09 09:04:34",
      "description": "Currently most CI checks run to completion even if other checks fail.  This is useful if we want to find as many problems as possible in a single CI run.  However, it is a waste of resources when trying to get a clean run before merge.\r\n\r\nI propose to make checks fail faster for PR runs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make CI checks fail faster"
   },
   {
      "_id": "13363139",
      "assignee": "xyao",
      "components": [],
      "created": "2021-03-09 01:37:52",
      "description": "This ticket is opened to replace the deprecated usage of Timeout(int millis) to  Timeout.seconds(). ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Replace the usage of deprecated Junit#timeout() in Ozone."
   },
   {
      "_id": "13363054",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-08 16:09:12",
      "description": "Please see: https://github.com/apache/ozone/pull/2005",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Rename Apache Hadoop Ozone to Apache Ozone in pom and markdown files"
   },
   {
      "_id": "13362937",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-08 07:49:30",
      "description": "*This Jira is to implement*\r\n1. Use RootCertificate server to issue certs for SCM\r\n2. Use scmCertificatServer to issue certs for DN/OM. (This cert server got certs from RootCertificate Server)\r\n3. Start RootCertificate server only on primary SCM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Integrate CertClient"
   },
   {
      "_id": "13362702",
      "assignee": "xyao",
      "components": [],
      "created": "2021-03-05 20:52:57",
      "description": "Current we map CREATE/DELETE to parent WRITE. All the other are just 1:1 map from child to parent. \r\n\r\nThis may not work, e.g., child WRITE_ACL does not equal to parent WRITE_ACL\r\n\r\nHere is the proposed new mapping:\r\n    // Refined the parent context\r\n    // OP         |CHILD     |PARENT\r\n\r\n    // CREATE      NONE         WRITE\r\n    // DELETE      DELETE       WRITE\r\n    // WRITE       WRITE        WRITE\r\n    // WRITE_ACL   WRITE_ACL    WRITE     (V1 WRITE_ACL=>WRITE)\r\n\r\n    // READ        READ         READ\r\n    // LIST        LIST         READ      (V1 LIST=>READ)\r\n    // READ_ACL    READ_ACL     READ      (V1 READ_ACL=>READ)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refine the native authorizer parent context right check"
   },
   {
      "_id": "13362440",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2021-03-05 00:25:00",
      "description": "Although OM Finalization is a Ratis request, it updates a persistent location outside the OM state machine maintained \"RocksDB\". Hence, in a rare case where an unfinalized OM needs a Ratis snapshot to finalize (without any logs), the VERSION file will not be updated, and hence the OM will be stuck in that state.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Layout version should be available in DB for an un-finalized OM to be finalized through a Ratis snapshot."
   },
   {
      "_id": "13362405",
      "assignee": "elek",
      "components": [],
      "created": "2021-03-04 20:05:26",
      "description": "The Ozone website prominently mentions CSI support:\r\n\r\n[https://ozone.apache.org/docs/1.0.0/]\r\n\r\n[https://ozone.apache.org/docs/1.0.0/interface/csi.html]\r\n\r\nOur docs give a false impression to users that CSI is fully functional and supported for persistent storage inside containers.\r\n\r\nThis support uses goofys+S3 gateway, so it is not appropriate for any serious usage. A real CSI solution should use an approach like the cBlocks prototype by building directly on top of HDDS containers with a real device driver.\r\n\r\nUntil that time we should not claim CSI support. Alternatively we should be honest with our users that it is a prototype and not suitable for serious usage.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Remove mention of CSI support"
   },
   {
      "_id": "13362390",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2021-03-04 18:54:07",
      "description": "Simple change to add Layout version information to Recon datanode info API.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add Layout version information to Recon datanode info API."
   },
   {
      "_id": "13362079",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-03-03 11:32:27",
      "description": "This Jira is to implement\r\n1. Create CertClient, which generates a public key, private key, and generates CSR with ClusterID, SCMID. \r\n2. Modify DefaultCA Server to work in 2 modes, SELF_SIGNED_CA and INTERMEDIARY_CA.\r\n3. Modify SCMStorageConfig to persist SCM cert serial ID.\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security]  Create SCM Cert Client and change DefaultCA to allow self signed and intermediary"
   },
   {
      "_id": "13362058",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-03-03 09:25:35",
      "description": "Need a tool to upgrade current non-HA SCM node to single node HA cluster\r\n\r\ncc [~shashikant]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Need a tool to upgrade current non-HA SCM node to single node HA cluster"
   },
   {
      "_id": "13361860",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-02 12:33:41",
      "description": "We should avoid versions {{latest}} (for runners) and {{master}} (for actions) in Github Actions workflows.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid latest/master in Github Actions"
   },
   {
      "_id": "13361844",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-03-02 12:05:34",
      "description": "With SCM HA coming in, SCM would be able to operate with/without ratis. The aim of the Jira is to ensure the existing tests work with both Ratis enabled and disabled on SCM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM Ratis enable/disable switch "
   },
   {
      "_id": "13361833",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-02 10:50:39",
      "description": "The goal of this task is to add a separate check for building {{hadoop-hdds/docs}} with Hugo.\r\n\r\nBenefits:\r\n\r\n# separation of concerns: Currently docs are implicitly built in _integration_ checks.  If there is a syntax error in the docs, all of these fail due to the same cause.  (Used to be built in _findbugs_, _unit_, too, but these no longer have Hugo available.)\r\n# better error reporting: The above checks do not collect compile errors in their summaries.  Thus one needs to dig into build logs to find any such problem.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add simple CI check for docs"
   },
   {
      "_id": "13361821",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-03-02 10:21:58",
      "description": "Some of the CI checks have code duplication, which could be reduced by using matrix build.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge basic CI checks"
   },
   {
      "_id": "13361688",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-03-01 20:12:54",
      "description": "SCM (and Recon) log this message during startup:\r\n\r\n{code}\r\nINFO net.NodeSchemaLoader: Loading file from java.lang.CompoundEnumeration@6aa3bfc\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Useless message about node schema location"
   },
   {
      "_id": "13361645",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2021-03-01 16:22:34",
      "description": "{{ozone admin datanode list}} filter by UUID is broken:\r\n\r\n{code}\r\n$ ozone admin datanode list\r\nDatanode: 8b7a289e-3449-49c7-a9d9-0dae7751559d (/rack2/10.5.0.8/ozone-topology_datanode_5_1.ozone-topology_net/3 pipelines)\r\n...\r\n$ ozone admin datanode list --id 8b7a289e-3449-49c7-a9d9-0dae7751559d\r\n$\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "ozone admin datanode list filter by UUID broken"
   },
   {
      "_id": "13361546",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2021-03-01 08:37:37",
      "description": "awscli is installed for S3 tests at runtime.  It could be pre-installed in the ozone-runner docker image to save some test execution time.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Pre-install awscli in ozone-runner"
   },
   {
      "_id": "13361526",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-03-01 06:58:33",
      "description": "This Jira is to fix an issue in removing self ID when building proxy objects to submit requests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix removing local SCM when submitting request to other SCM."
   },
   {
      "_id": "13361423",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-02-28 09:04:12",
      "description": "This Jira is to add support for FailOverProxyProvider for SCMSecurityServer which is used by OM and Datanode. (In further jira's when security work is implemented, this API will be used by SCM also)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Add failover proxy to SCM Security Server Protocol"
   },
   {
      "_id": "13361159",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-02-26 11:01:09",
      "description": "This Jira is to implement list certificates based on role.\r\nHDDS-4861 added scm Certs to scmCert Table. In this Jira, listCerts will be modified to list all SCM Certificates.\r\n\r\n(Once for other roles like SCM/DN they moved to new tables, listCertiificates will work for all the roles. This is not planned in this Jira)\r\n\r\nThis API will be used to get all the SCM certificates for SCM HA nodes during bootStrap SCM nodes startup.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": " [SCM HA Security] Implement listCertificates based on role"
   },
   {
      "_id": "13360935",
      "assignee": "elek",
      "components": [],
      "created": "2021-02-25 10:52:45",
      "description": "Please see: https://github.com/apache/ozone/pull/1964",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump jetty version"
   },
   {
      "_id": "13360933",
      "assignee": "elek",
      "components": [],
      "created": "2021-02-25 10:40:48",
      "description": "Please see: https://github.com/apache/ozone/pull/1963",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump jackson version number"
   },
   {
      "_id": "13360548",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-02-24 08:14:33",
      "description": "This Jira is to implement generatePeerSCMCertificate which will be called during bootStrap and also during init by primary SCM. This is similar to how OM and DN gets certificate.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[SCM HA Security] Implement generate SCM certificate"
   },
   {
      "_id": "13360341",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-02-23 13:54:26",
      "description": "The {{build-branch}} Github Actions workflow has a step to avoid caching Ozone artifacts:\r\n\r\n{noformat:title=https://github.com/apache/ozone/blob/1cf90150410882cd1ef88cdef33a0de18674399e/.github/workflows/post-commit.yml#L221-L225}\r\n      - name: Delete temporary build artifacts before caching\r\n        run: |\r\n          #Never cache local artifacts\r\n          rm -rf ~/.m2/repository/org/apache/hadoop/hdds\r\n          rm -rf ~/.m2/repository/org/apache/hadoop/ozone\r\n{noformat}\r\n\r\nThere are 2 problems:\r\n\r\n# the cache is in a different job\r\n# there are no {{hdds}} and {{ozone}} subdirectories, only ones like {{hadoop-hdds-common}}, etc.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Useless Maven cache cleanup"
   },
   {
      "_id": "13360108",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2021-02-22 12:49:55",
      "description": "When the very first call by Ruby client against secure setup of Ozone, the server returns 400 no matter how valid the request is. See the attached ruby-sdk-patch.diff, which adds some tests on S3 auth header signature-to-sign generation. It consists of two test additions, the \"2\" is the one generated by boto3, the \"3\" is generated by aws-ruby-sdk. Both passes the additional tests, which are definitely valid.\r\n\r\nHowever, when real HTTP request is sent by Ruby client, e.g. ozone-test.rb attached, it fails with 400. The header was like this (though the host names and domains are masked):\r\n\r\n{quote}GET //ozone.example.com:9879/sandbox?list-type=2&max-keys=1 HTTP/1.1\r\nContent-Type:\r\nAccept-Encoding:\r\nUser-Agent: aws-sdk-ruby3/3.112.0 ruby/2.7.2 x86_64-linux aws-sdk-s3/1.88.1\r\nHost: ozone.example.com:9879\r\nX-Amz-Date: 20210222T110554Z\r\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\r\nAuthorization: AWS4-HMAC-SHA256 Credential=kota@EXAMPLE.COM/20210222/foobar/s3/aws4_request, SignedHeaders=host;user-agent;x-amz-content-sha256;x-amz-date, Signature=0c9469f018f5\r\nb3fd2cff6f8d4e4963f50aa71c6704def59527634404f5fc98a9\r\nContent-Length: 0\r\nAccept: */*{quote}\r\n\r\nOn the other hand, request headers made by boto3 was:\r\n\r\n{quote}GET //ozone.example.com:9879/sandbox?list-type=2&encoding-type=url HTTP/1.1\r\nHost: ozone.example.com:9879\r\nAccept-Encoding: identity\r\nUser-Agent: Boto3/1.17.12 Python/3.9.1 Linux/5.10.14-arch1-1 Botocore/1.20.12\r\nX-Amz-Date: 20210222T110829Z\r\nX-Amz-Content-SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\r\nAuthorization: AWS4-HMAC-SHA256 Credential=kota@EXAMPLE.COM/20210222/us-east-1/s3/aws4_request, SignedHeaders=host;x-amz-content-sha256;x-amz-date, Signature=94302f21cccac8832d3e\r\n4fe25c5f6d8a0307188fb0e1b1983264339381d21dac{quote}\r\n\r\nThe difference of these requests are IMHO, \"Content-Type\" and \"Accept-Encoding\" are both empty in Ruby SDK. I'm afraid this error stems from partly Ruby SDK and partly from [Jetty Issue|https://github.com/eclipse/jetty.project/issues/2883]. The former sends empty header lines and the latter rejects them.\r\n\r\nAnd the s3g debug log (only error'ish part) follows:\r\n{quote}2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: chain=NoCacheFilter@5e600dd5==org.apache.hadoop.hdds.server.http.NoCacheFilter,inst=true,async=true-\r\n>safety@63a12c68==org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter,inst=true,async=true->info-page-redirect@576d5deb==org.apache.hadoop.ozone.s3.RootPageDis\r\nplayFilter,inst=true,async=false->jaxrs@603a422==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=1,inst=true,async=false\r\n2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter NoCacheFilter@5e600dd5==org.apache.hadoop.hdds.server.http.NoCacheFilter,inst=true,async\r\n=true\r\n2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter safety@63a12c68==org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter,inst=\r\ntrue,async=true\r\n2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call filter info-page-redirect@576d5deb==org.apache.hadoop.ozone.s3.RootPageDisplayFilter,inst=true,\r\nasync=false\r\n2021-02-22 20:55:54,450 [qtp1637061418-81] DEBUG servlet.ServletHandler: call servlet jaxrs@603a422==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=1,inst=true\r\n,async=false\r\n2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: sendError HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=false se=false i=tru\r\ne al=0}\r\n2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.session: Leaving scope org.eclipse.jetty.server.session.SessionHandler367746789==dftMaxIdleSec=-1 dispatch=REQUEST, a\r\nsync=false, session=null, oldsession=null, oldsessionhandler=null\r\n2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.Server: handled=true async=false committed=true on HttpChannelOverHttp@769bb34b{s=HttpChannelState@4893b376{s=HANDLIN\r\nG rs=BLOCKING os=OPEN is=IDLE awp=false se=true i=true al=0},r=1,c=false/false,a=HANDLING,uri=https://ozone.example.com:9879/sandbox?list-type=2&ma\r\nx-keys=1,age=2}\r\n2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: unhandle HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=false se=true i=true\r\nal=0}\r\n2021-02-22 20:55:54,451 [qtp1637061418-81] DEBUG server.HttpChannelState: nextAction(false) SEND_ERROR HttpChannelState@4893b376{s=HANDLING rs=BLOCKING os=OPEN is=IDLE awp=f\r\nalse se=false i=false al=0}\r\n{quote}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ruby S3 SDK never get authenticated by Ozone"
   },
   {
      "_id": "13359617",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-02-19 15:36:31",
      "description": "Some {{ozone fs}} operations still encounter the compatibility issue attempted to fix in HDDS-4731.\r\n\r\n{code}\r\n$ ozone fs -mkdir o3fs://bucket1.vol1/dir/\r\n$ ozone fs -ls o3fs://bucket1.vol1/dir/\r\n$ ozone fs -put /etc/passwd o3fs://bucket1.vol1/dir/\r\n-put: No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION\r\n$ ozone fs -ls o3fs://bucket1.vol1/dir/\r\n-ls: No enum constant org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "More compatibility problem with DatanodeDetails.Port.Name.REPLICATION"
   },
   {
      "_id": "13359613",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-02-19 15:10:22",
      "description": "{code}\r\nscm_1       | 2021-02-19 02:23:28,815 [IPC Server handler 28 on default port 9863] WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Datanodes may be used up.\r\nscm_1       | org.apache.hadoop.hdds.scm.exceptions.SCMException: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 1 pipelines. Required 3. Found 0\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.filterViableNodes(PipelinePlacementPolicy.java:172)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.PipelinePlacementPolicy.chooseDatanodes(PipelinePlacementPolicy.java:224)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:134)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:63)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:271)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:201)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:192)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:162)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:118)\r\nscm_1       | \tat org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:100)\r\nscm_1       | \tat org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13265)\r\nscm_1       | \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\nscm_1       | \tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086)\r\nscm_1       | \tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029)\r\nscm_1       | \tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957)\r\nscm_1       | \tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\nscm_1       | \tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\nscm_1       | \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\nscm_1       | \tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957)\r\nscm_1       | 2021-02-19 02:23:28,815 [IPC Server handler 28 on default port 9863] ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE\r\n{code}\r\n\r\nhttps://github.com/elek/ozone-build-results/tree/master/2021/02/19/6000/acceptance-secure",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in ozonesecure due to unable to allocate block"
   },
   {
      "_id": "13359396",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-02-18 17:49:18",
      "description": "Revoked Certs table in SCM DB does not have a timestamp of revocation. We need to add this to keep track of when certificates got revoked.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add timestamp to Revoked Certs table in SCM DB"
   },
   {
      "_id": "13359283",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-02-18 11:40:30",
      "description": "This Jira is to make changes required to make SCM commands works with SCM HA/non-HA",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make changes required for SCM admin commands to work with SCM HA"
   },
   {
      "_id": "13359272",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-02-18 10:58:40",
      "description": "Use SCM serviceID and nodeID to figure SCM Datanode address and port in HA from SCM address key and SCM Datanode port config or else fall back to ozone.scm.names",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use SCM service ID in finding SCM Datanode address."
   },
   {
      "_id": "13358937",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2021-02-16 20:44:00",
      "description": "The following error prevents closed container replication if {{ozone.security.enabled}} is {{true}}.\r\n\r\n{code}\r\njava.lang.IllegalArgumentException: File does not contain valid certificates: certificate.crt\r\n\tat org.apache.ratis.thirdparty.io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:345)\r\n\tat org.apache.ratis.thirdparty.io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:294)\r\n\tat org.apache.hadoop.ozone.container.replication.GrpcReplicationClient.<init>(GrpcReplicationClient.java:81)\r\n{code}\r\n\r\nCC [~sodonnell]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Replication failure in secure environment"
   },
   {
      "_id": "13358901",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-02-16 16:34:31",
      "description": "OM supports wildcard * during admin access check, but SCM does not honor this. This Jira is to fix this issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support wildcard(*) in ozone.administrators in SCM admin check"
   },
   {
      "_id": "13358635",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-02-15 11:12:18",
      "description": "To use a single config across the cluster, a few of the config keys in SCM need to support HA Style, and they need to be set to default config keys so that the conf can be reusable in other parts of code.\r\n\r\nIn this Jira, added the following config keys to support HA style config.\r\n\r\n      OZONE_SCM_DATANODE_ADDRESS_KEY,\r\n      OZONE_SCM_DATANODE_PORT_KEY,\r\n      OZONE_SCM_DATANODE_BIND_HOST_KEY,\r\n      OZONE_SCM_BLOCK_CLIENT_ADDRESS_KEY,\r\n      OZONE_SCM_BLOCK_CLIENT_PORT_KEY,\r\n      OZONE_SCM_BLOCK_CLIENT_BIND_HOST_KEY,\r\n      OZONE_SCM_CLIENT_ADDRESS_KEY,\r\n      OZONE_SCM_CLIENT_PORT_KEY,\r\n      OZONE_SCM_CLIENT_BIND_HOST_KEY,\r\n      OZONE_SCM_SECURITY_SERVICE_ADDRESS_KEY,\r\n      OZONE_SCM_SECURITY_SERVICE_PORT_KEY,\r\n      OZONE_SCM_SECURITY_SERVICE_BIND_HOST_KEY,\r\n      OZONE_SCM_RATIS_PORT_KEY,\r\n      OZONE_SCM_HTTP_BIND_HOST_KEY,\r\n      OZONE_SCM_HTTPS_BIND_HOST_KEY,\r\n      OZONE_SCM_HTTP_ADDRESS_KEY,\r\n      OZONE_SCM_HTTPS_ADDRESS_KEY,\r\n      OZONE_SCM_DB_DIRS,\r\n      OZONE_SCM_ADDRESS_KEY",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make SCM Generic config support HA Style"
   },
   {
      "_id": "13358579",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-02-15 05:04:15",
      "description": "During SCM --bootstrap, the bootstrapping SCM node will connect to primary SCM node (already running) and get the cluster Id. Once security is implemented, it will also fetch the CSR root certificates from primary SCM during SCM --bootstrap phase.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement scm --bootstrap command"
   },
   {
      "_id": "13358539",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-02-14 13:19:29",
      "description": "Use SCM service ID in SCMBlockClient and SCM Client\r\n\r\nFor existing installations, it is also important to fallback to use the ozone.scm.names as well\r\n\r\ncc [~shashikant] [~nanda] [~bharat]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use SCM service ID in SCMBlockClient and SCM Client"
   },
   {
      "_id": "13358529",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-02-14 10:49:51",
      "description": "This jira adds multiple SCMs to MiniOzoneCluster",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add multiple SCM nodes to MiniOzoneCluster"
   },
   {
      "_id": "13357999",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-02-10 18:53:01",
      "description": "By default, a new cluster should use the latest layout version while being deployed (--init). This is also true for Datanodes. If there is a new DN when the cluster is an unfinalized state, SCM layout version check will not let it become part of a pipeline until finalization.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fresh deploy of Ozone must use the highest layout version by default"
   },
   {
      "_id": "13357716",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-02-09 09:07:31",
      "description": "Currently, the ratis group creation happens post start of scm service. The idea here is move the ratis group initialization to scm --init phase and use cluster id as the SCM HA group ID.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move Ratis group creation to scm --init phase"
   },
   {
      "_id": "13357478",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-02-08 09:22:39",
      "description": "Currently,\u00a0https://issues.apache.org/jira/browse/HDDS-4773\u00a0adds functionality to download a Rocks db checkpoint from leader node. The idea here is to add functionality to install the downloaded checkpoint and reinitialise SCMStateMachine with latest downloaded checkpoint.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add install checkpoint in SCMStateMachine"
   },
   {
      "_id": "13357398",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-02-07 15:13:35",
      "description": "Currently _coverage_ CI check:\r\n\r\n# calculates combined test coverage\r\n# uploads it to Sonar only for Apache Ozone repo and only for builds on push/schedule\r\n# stores combined coverage in GitHub Actions artifact\r\n\r\nThus for PR in Apache Ozone and for all builds in forks, it only stores coverage in the artifact.  These expire in 30 days and I don't think anybody really checks them manually.\r\n\r\nI propose to completely skip _coverage_ check for PRs and in forks, instead of only skipping upload to Sonar.  This would save ~12 minutes for such builds.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Skip coverage check for PRs and in forks"
   },
   {
      "_id": "13356951",
      "assignee": "elek",
      "components": [],
      "created": "2021-02-05 07:19:43",
      "description": "As discussed [here|https://lists.apache.org/thread.html/r222da5ce97d42f945f5bbd4a10bc8d5fab1f0a2324ad2de2ff19c860%40%3Cdev.ozone.apache.org%3E], the current repository (which includes source for apache/ozone-runer, apache/ozone-build, apache/ozone containers) can be separtaed to make it more straightforward to use.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Separate source of Ozone container images to different repositories"
   },
   {
      "_id": "13356848",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-02-04 20:35:29",
      "description": "Some tests only output this:\r\n\r\n{noformat}\r\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.hdds.utils.db.RDBStore).\r\nlog4j:WARN Please initialize the log4j system properly.\r\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "No appenders could be found for logger - in tests"
   },
   {
      "_id": "13356841",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2021-02-04 19:43:23",
      "description": "Fix the following permission issue in {{ozonesecure-om-ha}} environment:\r\n\r\n{code}\r\nom1_1        | ERROR om.OMDBCheckpointServlet: Permission denied: User principal 'recon/recon@EXAMPLE.COM' does not have access to /dbCheckpoint.\r\nom1_1        | This can happen when Ozone Manager is started with a different user.\r\nom1_1        | Please append 'recon/recon@EXAMPLE.COM' to OM 'ozone.administrators' config and restart OM to grant current user access to this endpoint.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Allow recon access /dbCheckpoint in ozonesecure-om-ha"
   },
   {
      "_id": "13356462",
      "assignee": "elek",
      "components": [],
      "created": "2021-02-03 10:41:43",
      "description": "Please see: https://github.com/apache/ozone/pull/1889",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve usability of ozone s3 getsecret output"
   },
   {
      "_id": "13356422",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2021-02-03 08:50:00",
      "description": "The goal of this task is to enable TLS for Ratis in OM HA based on {{SecurityConfig#isGrpcTlsEnabled}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable mTLS for Ratis in OM HA"
   },
   {
      "_id": "13356170",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-02-02 10:43:19",
      "description": "SCMRatisSnapshotInfo and OMRatisSnapshotInfo seem to be duplicate of each other. The idea is to merge them into a single class and use as is.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge SCMRatisSnapshotInfo and OMRatisSnapshotInfo into a single class"
   },
   {
      "_id": "13356162",
      "assignee": "elek",
      "components": [],
      "created": "2021-02-02 10:21:14",
      "description": "Please see: https://github.com/apache/ozone/pull/1878",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable spotbugs for (the empty) hadoop-ozone-datanode project"
   },
   {
      "_id": "13356010",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-02-01 17:32:50",
      "description": "Pre-vote and Leader Lease are new features in Ratis which may not be required/not tested enough with ozone currently. The idea here is to disable these by default in ozone.\r\n\r\nLeader lease is yet be committed : https://issues.apache.org/jira/browse/RATIS-1273",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Disable Prevote  in Ratis for Ozone by default"
   },
   {
      "_id": "13355966",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2021-02-01 14:56:30",
      "description": "As a part of install Snapshot notification from leader to follower, the follower needs to get the latest Rocks db checkpoint from the follower first . It then needs to reload the stateMachine from the latest state , and then start participating in the ring. This jira aims to add the transfer rocks db checkpointing functionality for SCM HA.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add functionality to transfer Rocks db checkpoint from leader to follower"
   },
   {
      "_id": "13355711",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-01-31 08:29:35",
      "description": "{{ratis-thirdparty}} 0.6.0 was released recently, and now 0.6.0-SNAPSHOT is gone from the repos.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Upgrade Ratis Thirdparty to 0.6.0"
   },
   {
      "_id": "13355652",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-30 12:47:35",
      "description": "{noformat}\r\nError:  Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.986 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse\r\nError:  testDoubleBufferWithDummyResponse(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse)  Time elapsed: 0.912 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  ...\r\n  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.testDoubleBufferWithDummyResponse(TestOzoneManagerDoubleBufferWithDummyResponse.java:124)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestOzoneManagerDoubleBufferWithDummyResponse"
   },
   {
      "_id": "13355470",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337242",
            "id": "12337242",
            "name": "CI"
         }
      ],
      "created": "2021-01-29 16:15:49",
      "description": "The Github Actions workflow that looks for pending PRs to be closed does not work currently because it is limited to the old {{hadoop-ozone}} repo.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update close-pending workflow for new repo"
   },
   {
      "_id": "13355444",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-29 14:06:59",
      "description": "Delegation token of Ozone has two flavor:\r\n 1. delegation token (based public key infrastructure provided by SCM CA)\r\n 2. s3 token\r\n\r\nS3 token includes all the information which is required to validate a S3 HTTP request: aws access key id, string2sign, signature. OM can check the signature based on all this information which are stored in the OzoneTokenInfo.\r\n\r\nWhen the request is authenticated the owner field is used for all the following authentication. But the content of the follower field is not validated. It's filled by the S3g but any client can create a custom request where the Owner field contains a custom string.\r\n\r\n1. To reproduce start an ozonesecure cluster where testuser2 is not an admin. (The easiest way to achieve this is removing the hadoop.security.auth_to_local settings, as in our ozonesecure environment all users are mapped to local root which is admin)\r\n\r\nTo make the test easier, groups can also be turned off:\r\n\r\n{code}\r\n <property>\r\n    <name>hadoop.security.group.mapping</name>\r\n    <value>org.apache.hadoop.security.NullGroupsMapping</value>\r\n  </property>\r\n{code}\r\n\r\n2. Check if testuser2 is not an admin:\r\n\r\n{code}\r\nkinit -kt /etc/security/keytabs/testuser2.keytab testuser2/scm\r\n\r\nklist\r\nTicket cache: FILE:/tmp/krb5cc_1000\r\nDefault principal: testuser2/scm@EXAMPLE.COM\r\n\r\nValid starting     Expires            Service principal\r\n01/29/21 13:27:03  01/30/21 13:27:03  krbtgt/EXAMPLE.COM@EXAMPLE.COM\r\n\trenew until 02/05/21 13:27:03\r\n\r\n\r\nozone sh volume create /vol3\r\nPERMISSION_DENIED User testuser2/scm@EXAMPLE.COM doesn't have CREATE permission to access volume vol3 null null\r\n{code}\r\n\r\n3. To create a s3 type delegation token we need valid string2sign and signature strings.\r\n\r\n{code}\r\nozone s3 getsecret\r\n{code}\r\n\r\nSet the environment variables:\r\n\r\n{code}\r\nAWS_ACCESS_KEY_ID=...\r\nAWS_SECRET_ACCESS_KEY=....\r\n{code}\r\n\r\nTry to create a bucket (will be denied) with --debug flag:\r\n\r\n{code}\r\naws s3api --debug --endpoint=http://localhost:9878 create-bucket --bucket=bucket1\r\n{code}\r\n\r\nCopy the signature and string2sign from the output:\r\n\r\n{code}\r\n2021-01-29 15:03:52,269 - MainThread - botocore.auth - DEBUG - StringToSign:\r\nAWS4-HMAC-SHA256\r\n20210129T140352Z\r\n20210129/us-west-1/s3/aws4_request\r\nff6c0c767b0292cf3459d02ae1199d4c7786f3cca2f383a46b442f19d964d996\r\n2021-01-29 15:03:52,269 - MainThread - botocore.auth - DEBUG - Signature:\r\n9830423f18ac1f90ec658d1b5c47bdd7765d67fdc0dc67393c162627bfa45789\r\n{code}\r\n\r\nAnd execute a java app:\r\n\r\n{code}\r\n  public static void main(String[] args) throws Exception {\r\n    OzoneConfiguration conf = new OzoneConfiguration();\r\n    conf.set(\"ozone.om.address\", \"192.168.32.6\");\r\n\r\n    String awsAccessId = \"testuser2/scm@EXAMPLE.COM\";\r\n\r\n    UserGroupInformation.setConfiguration(conf);\r\n\r\n    UserGroupInformation remoteUser =\r\n        UserGroupInformation.createRemoteUser(awsAccessId, AuthMethod.TOKEN);\r\n\r\n    final Text omService = SecurityUtil.buildTokenService(OmUtils.\r\n        getOmAddressForClients(conf));\r\n    OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\r\n    identifier.setTokenType(S3AUTHINFO);\r\n    identifier.setStrToSign(\"AWS4-HMAC-SHA256\\n\"\r\n        + \"20210129T133557Z\\n\"\r\n        + \"20210129/us-west-1/s3/aws4_request\\n\"\r\n        + \"8fc985d9c7442c33d6f146ab123de49b18c83c4c6ccdfd182f10fc78691bdd53\");\r\n    identifier.setSignature(\r\n        \"044cf03375ea10b3e454b16887a1f5ce6ebb14d45b506dd7ac5e02fd0179ba7b\");\r\n    identifier.setAwsAccessId(awsAccessId);\r\n    identifier.setOwner(new Text(\"testuser/scm@EXAMPLE.COM\"));\r\n    Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\r\n        identifier.getSignature().getBytes(UTF_8),\r\n        identifier.getKind(),\r\n        omService);\r\n\r\n    remoteUser.addToken(token);\r\n\r\n    OzoneClient client = remoteUser.doAs(\r\n        (PrivilegedExceptionAction<OzoneClient>) () -> OzoneClientFactory.getRpcClient(conf));\r\n    client.getObjectStore().createVolume(\"vol2\");\r\n\r\n  }\r\n{code}\r\n\r\nAs a result /vol2 is created even if testuser2 is not an admin. Note: testuser IS an admin and setOwner used testuser instead of testuser2. \r\n\r\nA quick fix is to validate the owner field. A proper, long-term fix is disabling the s3 auth token type for client2server communication which can be done with HDDS-4440.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Owner field of S3AUTHINFO type delegation token should be validated"
   },
   {
      "_id": "13355443",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-29 13:52:43",
      "description": "Both {{ozone-ha}} and {{ozone-om-ha-s3}} run tests in OM HA environment.  I think they can be merged, both to reduce the number of environments and to save some runtime in CI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge ozone-ha and ozone-om-ha-s3"
   },
   {
      "_id": "13355329",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-29 04:39:15",
      "description": "{{ozone-ha}} acceptance test is failing intermittently with:\r\n\r\n{code}\r\n'Couldn't create RpcClient protocol' does not contain 'VOLUME_NOT_FOUND'\r\n{code}\r\n\r\nExamples:\r\n* https://github.com/elek/ozone-build-results/tree/master/2021/01/27/5516/acceptance-misc/\r\n* https://github.com/elek/ozone-build-results/tree/master/2021/01/28/5536/acceptance-misc/\r\n* https://github.com/elek/ozone-build-results/tree/master/2021/01/28/5550/acceptance-misc/",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in ozone-ha acceptance test"
   },
   {
      "_id": "13355166",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-28 11:47:26",
      "description": "Please see: https://github.com/apache/ozone/pull/1850",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Adjust classpath of ozone version to include log4j"
   },
   {
      "_id": "13355158",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-28 11:31:53",
      "description": "See: https://github.com/apache/ozone/pull/1849",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Unnecessary WARNING to set OZONE_CONF_DIR"
   },
   {
      "_id": "13354439",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-25 13:32:12",
      "description": "{{upgrade}} acceptance test verifies upgrade from Ozone 0.5.0 to \"snapshot\" (built from current sources).  In order to test upgrade from several earlier versions, it needs to be more modular.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Modularize upgrade test"
   },
   {
      "_id": "13354372",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-01-25 10:41:34",
      "description": "Upgrade Ratis snapshot to {{1.1.0-eb66796d-SNAPSHOT}} as we need RATIS-1290 for HDDS-4730.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upgrade Ratis to 1.1.0-eb66796d-SNAPSHOT"
   },
   {
      "_id": "13353904",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2021-01-22 05:37:51",
      "description": "In tencent, we monthly deploy latest master to our production environment.\r\n\r\nWhen running an upgrade test from old ozone version (at Dec 14 2020) to new ozone version (at Jun 14 2021), testDFSIO using jars of old ozone can not write to the new ozone cluster, met the error that\u00a0\r\n\r\nNo enum constant\u00a0org.apache.hadoop.hdds.protocol.DatanodeDetails.Port.Name.REPLICATION\r\n\r\nWe consider it is related to\u00a0HDDS-4496. Separate client and server2server GRPC services of datanode.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix compatibility issue caused by new add enum DatanodeDetails.Port.Name.REPLICATION"
   },
   {
      "_id": "13353671",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-01-21 07:51:24",
      "description": "Use separate client/server/admin ports being introduced in RATIS-1290.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use separate Ratis admin and client ports"
   },
   {
      "_id": "13353630",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2021-01-21 02:36:19",
      "description": "HDDS-2321 disabled token based authentication for container admin commands part of the DataNode admin protocol as that caused problems with requests that are not going through Ozone Manager, as token based auth support is present only there currently.\r\n\r\nWithin this feature, the followings to be added:\r\n- a new SCM request to get a new kind of token issued by the SCM\r\n- the token would be short living, without renewal or cancellation signed by SCM\r\n- the token will be required for container admin commands inside DataNodes\r\n- the token will be supplied to container admin requests from command line client, and for commands arriving via DN heartbeat responses\r\n- the token is validated on the DN side for every container admin command, and in case a token is not supplied or invalid the DN should reject the request.\r\n\r\nAlso it is part of the development to revisit all DN API requests and add the appropriate (OM or SCM) token based auth where applicable.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add token support for container admin operations"
   },
   {
      "_id": "13353493",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-20 09:34:39",
      "description": "Please see: https://github.com/apache/ozone/pull/1825",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Debug utility to export container data"
   },
   {
      "_id": "13352284",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-14 15:18:28",
      "description": "JUnit5 makes it easier to [run tests multiple times|https://junit.org/junit5/docs/current/user-guide/#writing-tests-repeated-tests] via {{\\@RepeatedTest(N)}} annotation.  Adding JUnit5 dependency for integration tests would allow upgrading flaky tests one by one for easier debug.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add JUnit5 dependency for integration tests"
   },
   {
      "_id": "13352273",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2021-01-14 14:25:52",
      "description": "bq. The version of Java installed in the scanner environment must be upgraded to at least Java 11 before 1 February 2021. Pre-11 versions of Java are already deprecated and scanners using them will stop functioning on that date. ([source|https://sonarcloud.io/documentation/appendices/end-of-support/]\r\n\r\nhttps://github.com/apache/ozone/runs/1701483011#step:6:2695",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upgrade Java for Sonar check"
   },
   {
      "_id": "13352072",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2021-01-13 18:30:11",
      "description": "We should store the state of CRLStatus of each DN in SCM. This can be done by handling the CRLStatusReport message in heartbeats received from DNs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle CRLStatusReport got from DN heartbeats and persist them"
   },
   {
      "_id": "13352025",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-13 14:32:33",
      "description": "During the measurement of closed container replication I found that the biggest bottleneck is the read side. 5 Gb container is replicated under ~3 minutes but ~2:30 was the downloading part.\r\n\r\nClosed containers are replicated via GRPC. The source side creates an OutputStream on-the-fly (OnDemandContainerReplicationSource.java) and stream all the container content as a \"tar.gz\" archive to the client.\r\n\r\nIt turned out that the compression (the .gz part) is quite expensive:\r\n\r\nI created a CLI tool to export containers to tar files (same logic as the replication but without streaming via GRPC, just saving to a file).\r\n\r\nI have seen the 2:30 time to create the archive:\r\n\r\n{code}\r\n2021-01-13 05:51:25,302 [main] INFO debug.ExportContainer: Preparation is done\r\n2021-01-13 05:53:53,472 [main] INFO debug.ExportContainer: Container is exported to /tmp/container-3.tar.gz\r\n{code}\r\n\r\nBut when I removed the compression in TarContainerPacker.java, the speed was significant better (25 sec instead of the 150 sec)\r\n\r\n{code}\r\n2021-01-13 06:11:46,254 [main] INFO debug.ExportContainer: Preparation is done\r\n2021-01-13 06:12:11,512 [main] INFO debug.ExportContainer: Container is exported to /tmp/container-3.tar\r\n{code}\r\n\r\nAs a result I suggest turning off the compression for closed container replication.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Disable compression for closed-container replication"
   },
   {
      "_id": "13351890",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2021-01-13 00:02:29",
      "description": "If a nodeID is not set explicitly (for a single node OM cluster), then it defaults to the OM storage ID which is an UUID string. Proposing to change this default to a constant (such as \"om1\") instead.\u00a0\r\nThis would help when a cluster is upgraded to HA for example. It is not straightforward to change the nodeID after Ratis server has been instantiated (as the nodeID is used to generate the RaftPeerID). Hence, it would be good to have a more readable nodeID by default.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Change default OM Node ID from UUID to a constant"
   },
   {
      "_id": "13351866",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-12 21:20:17",
      "description": "[~shashikant], I think this is caused by enabling multi-raft and creating more pipelines:\r\n\r\n{code:title=https://github.com/elek/ozone-build-results/blob/master/2021/01/12/5149/it-filesystem-hdds/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.txt}\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 31.976 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart\r\ntestPipelineWithScmRestart(org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart)  Time elapsed: 0.01 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<PipelineID=84d01728-7484-4ac8-9900-94ffbef8c967> but was:<PipelineID=c057a116-1d16-4def-985c-1c39482078eb>\r\n  ...\r\n  at org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.testPipelineWithScmRestart(TestSCMRestart.java:127)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestSCMRestart"
   },
   {
      "_id": "13351790",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-12 14:35:05",
      "description": "Please see: https://github.com/apache/ozone/pull/1786",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support scanning content of DN rocksdb instances with current scheme."
   },
   {
      "_id": "13351683",
      "assignee": "elek",
      "components": [],
      "created": "2021-01-12 08:30:55",
      "description": "Please see: https://github.com/apache/ozone/pull/1782",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "NodeStateMap leaks internal representation of container sets"
   },
   {
      "_id": "13351450",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2021-01-11 12:14:34",
      "description": "Found it during the usage of a data generator.\r\n\r\n 1. I accidentally uploaded keys without checksum data.\r\n\r\n  2. With this specific key, the client is moved to an endless loop instead of giving up after the first unexpected exceptions:\r\n\r\n{code}\r\n2021-01-11 13:01:50,031 INFO  storage.BlockInputStream (BlockInputStream.java:refreshPipeline(166)) - Unable to read information for block conID: 2 locID: 185 bcsId: 0 from pipeline PipelineID=206da15d-62f6-4e24-93d1-e2e805fc1376: Unexpected OzoneException: org.apache.hadoop.ozone.common.OzoneChecksumException: Original checksumData has no checksums\r\n2021-01-11 13:01:50,047 ERROR scm.XceiverClientGrpc (XceiverClientGrpc.java:sendCommandWithRetry(408)) - Failed to execute command cmdType: ReadChunk\r\ntraceID: \"\"\r\ncontainerID: 2\r\ndatanodeUuid: \"2c124e08-e8a5-4493-a41e-84797984e6a6\"\r\nreadChunk {\r\n  blockID {\r\n    containerID: 2\r\n    localID: 185\r\n    blockCommitSequenceId: 0\r\n  }\r\n  chunkData {\r\n    chunkName: \"chunk0\"\r\n    offset: 0\r\n    len: 4194304\r\n    checksumData {\r\n      type: CRC32\r\n      bytesPerChecksum: 1048576\r\n    }\r\n  }\r\n}\r\n on the pipeline Pipeline[ Id: 7d5ed2da-7453-4113-b766-4100458dcc16, Nodes: 2c124e08-e8a5-4493-a41e-84797984e6a6{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:, CreationTimestamp2021-01-11T12:01:50.032Z].\r\n2021-01-11 13:01:50,047 INFO  storage.BlockInputStream (BlockInputStream.java:refreshPipeline(166)) - Unable to read information for block conID: 2 locID: 185 bcsId: 0 from pipeline PipelineID=7d5ed2da-7453-4113-b766-4100458dcc16: Unexpected OzoneException: org.apache.hadoop.ozone.common.OzoneChecksumException: Original checksumData has no checksums\r\n2021-01-11 13:01:50,062 ERROR scm.XceiverClientGrpc (XceiverClientGrpc.java:sendCommandWithRetry(408)) - Failed to execute command cmdType: ReadChunk\r\ntraceID: \"\"\r\ncontainerID: 2\r\ndatanodeUuid: \"2c124e08-e8a5-4493-a41e-84797984e6a6\"\r\nreadChunk {\r\n  blockID {\r\n    containerID: 2\r\n    localID: 185\r\n    blockCommitSequenceId: 0\r\n  }\r\n  chunkData {\r\n    chunkName: \"chunk0\"\r\n    offset: 0\r\n    len: 4194304\r\n    checksumData {\r\n      type: CRC32\r\n      bytesPerChecksum: 1048576\r\n    }\r\n  }\r\n}\r\n on the pipeline Pipeline[ Id: 3a4b5032-6b2f-4297-8c4b-89d715175bb1, Nodes: 2c124e08-e8a5-4493-a41e-84797984e6a6{ip: 127.0.0.1, host: localhost, networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:, CreationTimestamp2021-01-11T12:01:50.048Z].\r\n{code}\r\n\r\nPlease note that the two attempt happens in the same milliseconds.\r\n\r\nThe problematic part seems to be in the BlockInputStream:\r\n\r\n{code}\r\n      try {\r\n        numBytesRead = current.read(b, off, numBytesToRead);\r\n      } catch (IOException e) {\r\n        handleReadError(e);\r\n        continue;\r\n      }\r\n{code}\r\n\r\nIn case of system exceptions we should \"break\" from the loop instead of \"continue\".\r\n\r\n(Normally it's not possible in a production cluster as the data is created with a bad client. But it has security implication: a malicious user can create similar keys which makes a DoS attack: all the clients will retry without sleep...)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "BlockInputStream should give up read retry if pipeline is not updated"
   },
   {
      "_id": "13351159",
      "assignee": "shashikant",
      "components": [],
      "created": "2021-01-08 20:15:40",
      "description": "Requirements:\r\n1. Rename OMTransactionInfo to TransactionInfo (use by both OM and SCM), same for the codec.\r\n2. Add extra functions in https://github.com/apache/ozone/blob/HDDS-2823/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/ha/SCMTransactionInfo.java to OMTransactionInfo.\r\n\r\nBy doing so, we can use the TransactionInfo in HDDS-2823 and remove duplicate code.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge OMTransactionInfo with SCMTransactionInfo"
   },
   {
      "_id": "13351097",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-08 14:12:56",
      "description": "TestOzoneFSWithObjectStoreCreate does not clean up mini clusters.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "No cleanup in TestOzoneFSWithObjectStoreCreate"
   },
   {
      "_id": "13350958",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2021-01-07 19:34:39",
      "description": "This Jira is to support MPU on encrypted buckets.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support TDE for MPU Keys on Encrypted Buckets"
   },
   {
      "_id": "13349193",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2021-01-05 12:30:19",
      "description": "With HDDS-4558 committed, secure acceptance test logs increased considerably (over 1GB).\r\n\r\nhttps://github.com/apache/ozone/actions/runs/462095579\r\n\r\nI think the root cause is that {{WriteChunk}} request may need to also {{ReadChunk}}, but now it fails because it only has write access:\r\n\r\n{code}\r\ndatanode_3  | 2021-01-05 10:41:23,067 [ChunkWriter-1-0] INFO impl.HddsDispatcher: Operation: ReadChunk , Trace ID:  , Message: Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission , Result: BLOCK_TOKEN_VERIFICATION_FAILED , StorageContainerException Occurred.\r\ndatanode_3  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:214)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.lambda$dispatch$0(HddsDispatcher.java:171)\r\ndatanode_3  | \tat org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:170)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:398)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:585)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$read$5(ContainerStateMachine.java:656)\r\ndatanode_3  | \tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)\r\ndatanode_3  | \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\ndatanode_3  | \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\ndatanode_3  | \tat java.base/java.lang.Thread.run(Thread.java:834)\r\ndatanode_3  | Caused by: org.apache.hadoop.hdds.security.token.BlockTokenException: Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission\r\ndatanode_3  | \tat org.apache.hadoop.hdds.security.token.BlockTokenVerifier.verify(BlockTokenVerifier.java:131)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.validateBlockToken(HddsDispatcher.java:431)\r\ndatanode_3  | \tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:211)\r\ndatanode_3  | \t... 10 more\r\ndatanode_3  | 2021-01-05 10:41:23,083 [ChunkWriter-1-0] ERROR ratis.ContainerStateMachine: gid group-5BCDF056E270 : ReadStateMachine failed. cmd ReadChunk logIndex 4 msg : Block token verification failed. Block token with conID: 1 locID: 105502689303461889 doesn't have READ permission Container Result: BLOCK_TOKEN_VERIFICATION_FAILED\r\n{code}\r\n\r\nCC [~elek] [~xyao]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Block token verification failed: no READ permission for WriteChunk"
   },
   {
      "_id": "13349013",
      "assignee": "xyao",
      "components": [],
      "created": "2021-01-04 22:35:41",
      "description": "Similar to HDDS-4571 but on OM side. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM handle expired certificate when verify token signature"
   },
   {
      "_id": "13348935",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2021-01-04 12:29:12",
      "description": "MapReduce acceptance test may fail if the same random number is generated for both O3FS and OFS tests:\r\n\r\n{noformat}\r\n<msg timestamp=\"20210104 11:37:50.503\" level=\"INFO\">${random} = 58</msg>\r\n...\r\n<msg timestamp=\"20210104 11:37:50.519\" level=\"INFO\">${result} = o3fs://bucket1.volume1.om/wordcount-58.txt</msg>\r\n...\r\n<msg timestamp=\"20210104 11:39:16.274\" level=\"INFO\">${random} = 58</msg>\r\n...\r\n<msg timestamp=\"20210104 11:39:16.295\" level=\"INFO\">${result} = ofs://om/volume1/bucket1/wordcount-58.txt</msg>\r\n...\r\nFileAlreadyExistsException: Output directory ofs://om/volume1/bucket1/wordcount-58.txt already exists\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in MapReduce test due to existing output file"
   },
   {
      "_id": "13348906",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2021-01-04 10:02:49",
      "description": "Intermittent failure in secure acceptance test:\r\n\r\n{noformat:title=output}\r\nWait for safemode exit                                                | FAIL |\r\n255 != 0\r\n{noformat}\r\n\r\n{noformat:title=log}\r\nRunning command 'ozone admin safemode wait -t 2 2>&1'.\r\n${rc} = 255\r\n${output} = WARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar) to method sun.security.krb5.Config.getInstance()\r\nWARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nSafe mode is not ended within the timeout period.\r\n{noformat}\r\n\r\nThis is executed with safemode already OFF.  Also, notice that neither {{SCM is out of safe mode}}, nor {{SCM is in safe mode}} is shown.\r\n\r\nIt indicates that {{ozone admin safemode wait}} may conclude without actually checking status, if it takes more time to create SCM client than the given timeout:\r\n\r\n{code:title=https://github.com/apache/ozone/blob/aacba62a1a1bdee45dad15e82ef64019ff67b767/hadoop-hdds/tools/src/main/java/org/apache/hadoop/hdds/scm/cli/SafeModeWaitSubcommand.java#L60-L62}\r\n    while (getRemainingTimeInSec() > 0) {\r\n      try (ScmClient scmClient = scmOption.createScmClient()) {\r\n        while (getRemainingTimeInSec() > 0) {\r\n{code}\r\n\r\nIt is reproducible by adding {{sleep}} in {{createScmClient()}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Safemode wait may end without checking"
   },
   {
      "_id": "13348103",
      "assignee": "elek",
      "components": [],
      "created": "2020-12-28 10:04:17",
      "description": "Please see: https://github.com/apache/ozone/pull/1741",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable coverage upload to codecov"
   },
   {
      "_id": "13347100",
      "assignee": "elek",
      "components": [],
      "created": "2020-12-21 08:17:42",
      "description": "The latest bouncy castle contains important fixes. Ozone is not affected by the bcrypt issue, but it seems to be safer updating to the latest version.\r\n\r\n(Thanks to [~aengineer] how reported this issue)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Update bouncycastle to 1.67"
   },
   {
      "_id": "13346650",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-12-17 19:13:07",
      "description": "On a 3 node OM HA setup, when a prepare is done with 1 OM down, it leads to a state where the leader and follower are fully prepared (Snapshot at last index and logs purged). When the 3rd node rejoins the quorum, it leads to an infinite installSnapshot loop between the leader and the 3rd node, and the system goes into a bad state until a restart is done.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix issues in 'prepare' operation with one OM down."
   },
   {
      "_id": "13346355",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-12-16 12:55:04",
      "description": "Currently in ozone cluster, when the no of datanodes is not multiple of 3, the only way to make use of all datanodes for read/write is to enable multiraft. The idea here is to enable this feature by default in ozone.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable Multi Raft by default in Ozone"
   },
   {
      "_id": "13346284",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2020-12-16 07:43:48",
      "description": "{{envtoconf}} does not work for some output formats:\r\n\r\n * {{.env}}\r\n * {{.sh}}\r\n * {{.cfg}}\r\n * {{.conf}}\r\n\r\nTo reproduce:\r\n\r\n{code:title=docker run -it --rm -e DUMMY.CONF_key=value apache/ozone:1.0.0}\r\nTraceback (most recent call last):\r\n  File \"/opt/hadoop/libexec/envtoconf.py\", line 117, in <module>\r\n    Simple(sys.argv[1:]).main()\r\n  File \"/opt/hadoop/libexec/envtoconf.py\", line 108, in main\r\n    self.transform()\r\n  File \"/opt/hadoop/libexec/envtoconf.py\", line 96, in transform\r\n    content = transformer_func(content)\r\n  File \"/opt/hadoop/libexec/transformation.py\", line 121, in to_conf\r\n    for key, val in props:\r\nValueError: too many values to unpack\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "envtoconf broken for .conf and few other formats"
   },
   {
      "_id": "13346251",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-12-16 05:20:18",
      "description": "\r\n{code:java}\r\n2020-12-14 10:04:58,612 [OM StateMachine ApplyTransaction Thread - 0] ERROR ratis.OzoneManagerStateMachine: Terminating with exit status 1: Request cmdType: AddAcl\r\ntraceID: \"\"\r\nclientId: \"client-CAE43266CDF2\"\r\nuserInfo {\r\n  userName: \"om/om-server@DEV.COM\"\r\n  remoteAddress: \"10.101.211.49\"\r\n  hostName: \"om-server\"\r\n}\r\naddAclRequest {\r\n  obj {\r\n    resType: VOLUME\r\n    storeType: OZONE\r\n    path: \"/s3v\"\r\n  }\r\n  acl {\r\n    type: USER\r\n    name: \"airflow@DEV.TAP\"\r\n    rights: \"\\200\"\r\n    aclScope: ACCESS\r\n  }\r\n}\r\nfailed with exception\r\njava.lang.IllegalArgumentException: Trying to set updateID to 213 which is not greater than the current value of 36028797018963967 for OMVolumeArgs{volume='s3v', admin='hadoop', owner='hadoop', creationTime='1606808208748', quota='1152921504606846976'}\r\n       at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142)\r\n...\r\n       at java.base/java.lang.Thread.run(Thread.java:834)\r\n2020-12-14 10:04:58,635 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG:\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OM Terminates when adding acls to \"S3v\" volume"
   },
   {
      "_id": "13346036",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-12-15 05:48:16",
      "description": "{code}\r\nWarning:  Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-datanode:jar:1.1.0-SNAPSHOT\r\nWarning:  'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdds-hadoop-dependency-server:jar -> duplicate declaration of version (?) @ line 41, column 17\r\nWarning:  \r\nWarning:  It is highly recommended to fix these problems because they threaten the stability of your build.\r\nWarning:  \r\nWarning:  For this reason, future Maven versions might no longer support building such malformed projects.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Duplicate dependency hadoop-hdds-hadoop-dependency-server in datanode"
   },
   {
      "_id": "13345744",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-12-13 17:34:52",
      "description": "Ozone's GitHub Actions CI workflow references the old {{hadoop-ozone}} repository, so coverage data has not been uploaded to Sonar and Codecov for two months.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Coverage not updated since TLP"
   },
   {
      "_id": "13345620",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-12-12 01:38:20",
      "description": "Right now we have 2 clean up policies.\r\n1. Never\r\n2. Manual\r\n\r\nNever = Full Table Cache\r\nManual = Partial Table Cache\r\n\r\nIn OM, the main purpose of Table cache is for correctness. (Because OM return response after adding to cache, does not wait for double buffer flush to complete)\r\n\r\nThe current implementation has few problems.\r\n1. Cleanup Policy Never uses ConcurrentSkipListMap, and its computeIfPresent is not atomic, so there can be a race condition between cleanup and requests adding to cache. (This might cause cleaning up entries which are not flushed to DB, and this can cause correctness issue)\r\n2. Cleanup for override entries for full cache, never removes epoch entries.\r\n\r\n*Proposal:*\r\n1. Make TableCache based on cache type and have separate implementation for full cache and partial cache.\r\n2. Fix FullCache issue, using the lock.\r\n3. Fix evict cache logic for full cache to cleanup epoch entries for override entries.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TableCache Refactor to fix issues in cleanup never policy"
   },
   {
      "_id": "13345593",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-12-11 20:44:58",
      "description": "This Jira proposes to remove volumeArgs usage in the KeyRequests, after HDDS-4308, KeyRequests does not update volume used.\r\n\r\nSo, request/response classes do not require volumeArgs during bytes calculation.\r\n\r\nThis Jira proposes to cleanup usage of volumeArgs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cleanup usage of volumeArgs in KeyRequests"
   },
   {
      "_id": "13345523",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-12-11 13:30:53",
      "description": "Add acceptance test for Freon's Ozone Client Key Validator.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add acceptance test for Ozone Client Key Validator"
   },
   {
      "_id": "13345363",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-12-10 19:10:52",
      "description": "TestOzoneClientRetriesOnException#testMaxRetriesByOzoneClient intermittently fails with:\r\n\r\n{code:title=https://github.com/elek/ozone-build-results/blob/master/2020/12/08/4407/it-client/hadoop-ozone/integration-test/org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException.txt}\r\ntestMaxRetriesByOzoneClient(org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException)  Time elapsed: 42.789 s  <<< FAILURE!\r\njava.lang.AssertionError: Expected exception not thrown\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestOzoneClientRetriesOnException.testMaxRetriesByOzoneClient(TestOzoneClientRetriesOnException.java:235)\r\n{code}\r\n\r\nThe problem happens if the number of distinct containers used for the key is less than 4.  In this case max retries (of 3) is never reached.\r\n\r\nNormal execution:\r\n\r\n{code}\r\nblock: conID: 1 locID: 105357240184995843 bcsId: 0\r\nblock: conID: 2 locID: 105357240185061380 bcsId: 0\r\nblock: conID: 3 locID: 105357240185061381 bcsId: 0\r\nblock: conID: 4 locID: 105357240185061382 bcsId: 0\r\n{code}\r\n\r\nFailing execution:\r\n\r\n{code}\r\nblock: conID: 1 locID: 105357233754603523 bcsId: 0\r\nblock: conID: 2 locID: 105357233754603524 bcsId: 0\r\nblock: conID: 1 locID: 105357233754669061 bcsId: 0\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestOzoneClientRetriesOnException#testMaxRetriesByOzoneClient"
   },
   {
      "_id": "13345293",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-12-10 12:52:16",
      "description": "{{TestDefaultCertificateClient}} passes \"UTF-8\" to {{RandomStringUtils.random(int, String)}} with the intention to use UTF-8 encoding.  It was added in HDDS-1087 to fix (intermittent?) failure in CI.\r\n\r\nHowever, the parameter is not for character encoding, rather:\r\n\r\n{code}\r\n     * @param chars  the String containing the set of characters to use,\r\n     *  may be null, but must not be empty\r\n{code}\r\n\r\nSo this results in values like: {{\"--8F8T8U8T...\"}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "TestDefaultCertificateClient misuses chars param of random()"
   },
   {
      "_id": "13344944",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-12-09 00:34:40",
      "description": "* After getting a successful response for the OMPrepareRequest, the prepare client should use the Txn ID from the response to check every OM's preparation completeness.\r\n* This JIRA is partially dependent on HDDS-4569 which plans to introduce a marker file at the end of preparation. In a follow up JIRA, this will be refined to take up that logic.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Prepare client should check every OM individually for the prepared check based on Txn Id."
   },
   {
      "_id": "13344653",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-12-07 20:19:01",
      "description": "Thanks [~elek] for reporting the issue. There is a TODO in the Ozone block token verifier to support this. Currently the block token is giving all access to the client without access mode check. This ticket is opened to set access mode properly for block token and validate them in the token verifier. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Ozone block token with access mode check"
   },
   {
      "_id": "13344593",
      "assignee": "elek",
      "components": [],
      "created": "2020-12-07 14:32:43",
      "description": "This jira will be used to track and finish the design discussion which is started in HDDS-4097.\r\n\r\nThis design doc defines the current behavior and defines a possible 3rd option which is not yet implemented. HDDS-4097 contains the implementation details of one specific option.  ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3 and HCFS interoperability"
   },
   {
      "_id": "13344576",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-12-07 13:26:31",
      "description": "{{start-ozone.sh}} and {{stop-ozone.sh}} are currently not exercised by any tests, so they can be inadvertently broken without CI feedback.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add smoketest for ozonescripts environment"
   },
   {
      "_id": "13344310",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-12-04 23:57:04",
      "description": "We currently wait for reading up till the Chunk EOF before releasing the buffers in ChunkInputStream (or when the stream is closed). Let's say a client reads first 3 MB of a chunk\u00a0 of size 4MB and does not close the stream immediately. This would lead to the 3MB of data being cached in the ChunkInputStream buffers till the stream is closed.\r\n\r\nOnce HDDS-4552 is resolved, a chunk read from DN would return the chunk data as an array of ByteBuffers. After each ByteBuffer is read, it can be released. This would greatly help with optimizing memory usage of ChunkInputStream.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ChunkInputStream should release buffer as soon as last byte in the buffer is read"
   },
   {
      "_id": "13344309",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-12-04 23:49:06",
      "description": "When a ReadChunk operation is performed, all the data to be read from one chunk is read into a single ByteBuffer.\u00a0\r\n{code:java}\r\n#ChunkUtils#readData()\r\npublic static void readData(File file, ByteBuffer buf,\r\n    long offset, long len, VolumeIOStats volumeIOStats)\r\n    throws StorageContainerException {\r\n  .....\r\n  try {\r\n    bytesRead = processFileExclusively(path, () -> {\r\n      try (FileChannel channel = open(path, READ_OPTIONS, NO_ATTRIBUTES);\r\n           FileLock ignored = channel.lock(offset, len, true)) {\r\n\r\n        return channel.read(buf, offset);\r\n      } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n      }\r\n    });\r\n  } catch (UncheckedIOException e) {\r\n    throw wrapInStorageContainerException(e.getCause());\r\n  }\r\n  .....\r\n  .....{code}\r\nThis Jira proposes to read the data from the channel and put it into an array of ByteBuffers each with a set capacity. This capacity can be configurable.\u00a0\r\n\r\nThis would help with optimizing Ozone InputStreams in terms of cached memory. Currently, data in ChunkInputStream is cached till either the stream is closed or the chunk EOF is reached. This sometimes leads to upto 4MB (default ChunkSize) of data being cached in memory per ChunkInputStream.\r\n\r\nAfter the proposed change, we can optimize ChunkInputStream to release a ByteBuffer as soon as that ByteBuffer is read instead of waiting to read the whole chunk (HDDS-4553). Read I/O performance will not be affected as the read from DN still returns the requested length of data at one go. Only difference would be that the data would be returned in an array of ByteBuffer instead of a single ByteBuffer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Read data from chunk into ByteBuffer[] instead of single ByteBuffer"
   },
   {
      "_id": "13343864",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-12-02 21:00:58",
      "description": "* Introduce a new OM client operation to \"prepare\" the OM quorum.\r\n* As a first pass, the client will just submit the request (HDDS-4480) and print out the response (Txn ID)\r\n* In a follow up JIRA, the subsequent steps to probe every individual OM for preparation completeness will be added.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a new OM admin command to submit the OMPrepareRequest."
   },
   {
      "_id": "13343666",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-12-02 00:38:45",
      "description": "In few usecases like delete which iterates and listKeys to get KeyNames to deleteKeys, we donot need to refreshPipeline, and also in S3 listKeys also we don't need Keys with PipelineInfo.\r\n\r\nDoing this will help in improving the performance of delete/rename/listKeys API.\r\n\r\nAs RpcClient is returning only info like KeyName, length, replication type,  factor, modificationTime, creationTime. So we don't really need an extra param, as previously even though server sent pipelineInfo it is not used/returned to the client. So we don't need extra param, we can remove refresh pipeline in OM Server.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove refreshPipeline in listKeys "
   },
   {
      "_id": "13343615",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-12-01 17:19:14",
      "description": "* Add test to make sure an unsupported OM request is rejected.\r\n* Add test to make sure all new OM requests have the getRequestType method defined.\r\n* Minor code refactor.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add more unit tests for OM layout version manager."
   },
   {
      "_id": "13343531",
      "assignee": "elek",
      "components": [],
      "created": "2020-12-01 11:52:04",
      "description": "Number of threads for closed container replications can be adjusted by the settings  {{hdds.datanode.replication.streams.limit}}. But this number is ignored today due to the misuse of {{ThreadPoolExecutor}}:\r\n\r\n{code}\r\nnew ThreadPoolExecutor(\r\n        0, poolSize, 60, TimeUnit.SECONDS,\r\n        new LinkedBlockingQueue<>(),\r\n        new ThreadFactoryBuilder().setDaemon(true)\r\n            .setNameFormat(\"ContainerReplicationThread-%d\")\r\n            .build())\r\n{code}\r\n\r\nHere the minimal number of threads is 0 and the maximum number of the threads is the configured value.  Threads in the thread pool supposed to be scaled up, but it doesn't.\r\n\r\n[From the JDK docs|https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#ThreadPoolExecutor(int,%20int,%20long,%20java.util.concurrent.TimeUnit,%20java.util.concurrent.BlockingQueue)]:\r\n\r\nbq. A ThreadPoolExecutor will automatically adjust the pool size (see getPoolSize()) according to the bounds set by corePoolSize (see getCorePoolSize()) and maximumPoolSize (see getMaximumPoolSize()). When a new task is submitted in method execute(java.lang.Runnable), [...] [AND]  If there are more than corePoolSize but less than maximumPoolSize threads running, a new thread will be created only if the queue is full.\r\n\r\nSo if queue is not full (and {{LinkedBlockgingQueue}} is unbounded by default) the threads will never be created.\r\n\r\nFor a quick fix we can switch to use static thread pool instead of dynamic and always keep the required number of threads.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use fixed thread pool for closed container replication "
   },
   {
      "_id": "13343465",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-12-01 05:13:55",
      "description": "In the new\u00a0{{PipelineManager}} implementation we should update the pipeline table (RocksDB) when there is a pipeline state change.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update pipeline db when pipeline state is changed"
   },
   {
      "_id": "13343327",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-30 14:38:32",
      "description": "There is an error log which can be seen frequently in the log:\r\n\r\n{code}\r\n2020-11-30 15:35:58,539 [CommandWatcher-LeaseManager#LeaseMonitor] ERROR lease.LeaseManager (LeaseManager.java:run(238)) - Execution was interrupted \r\njava.lang.InterruptedException: sleep interrupted\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.hadoop.ozone.lease.LeaseManager$LeaseMonitor.run(LeaseManager.java:234)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n\r\nThis log is introduced by HDDS-2561 to make Sonar happy, but it was not required as LeaseManager use the thread interrupt intentionally.\r\n\r\nFor a proper fix we can keep the logging on WARN level AND replace thread interrupts with thread notify/wait.\r\n ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Remove false-positive error logs from LeaseManager"
   },
   {
      "_id": "13343323",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-11-30 14:24:39",
      "description": "Currently Ozone relies on {{HADOOP_\\*}} environment variables (eg. {{HADOOP_HOME}}) for historical and practical reasons (code reuse).  This can lead to unexpected results if Hadoop and Ozone are both present on a node and they share their environment.  Eg. we had to implement a workaround for finding {{ozone-config.sh}} relative to the script being executed when {{HADOOP_HOME}} points to Hadoop, not Ozone (HDDS-1912 and HDDS-4450).\r\n\r\nAnother similar severe problem happens if we would like to access Ozone filesystem both via {{ozone}} and {{hadoop}} commands.  The latter needs shaded Ozone FS JAR in {{HADOOP_CLASSPATH}}.  The same {{HADOOP_CLASSPATH}} results in {{ClassNotFound}} for {{ozone}}.\r\n\r\nThe solution proposed in HDDS-1912:\r\n\r\nbq. Long-term we may need to replace all the HADOOP_ environment variable with an OZONE_ environment variable, but that would be a significant bigger change.\r\n\r\nThis would allow using different classpaths, logging parameters, and more.\r\n\r\nTo be backward compatible, we should use existing {{HADOOP_}} variables as fallback for the corresponding {{OZONE_}} ones, but let {{OZONE_X}} take precedence over {{HADOOP_X}} if the former is defined.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Replace Hadoop variables and functions in Ozone shell scripts with Ozone-specific ones"
   },
   {
      "_id": "13343305",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-30 12:54:12",
      "description": "Create new freon test for container download",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create freon test to measure closed container replication"
   },
   {
      "_id": "13342722",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-11-25 23:01:49",
      "description": "Ozone Datanodes should be able to persist or load Certificate Revocation List (CRL)\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanodes should be able to persist and load CRL"
   },
   {
      "_id": "13342622",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-25 14:39:41",
      "description": "Ozone uses Netty either as direct dependency (ozone-csi) or from the ratis shaded dependency (for ratis gprc server). Both use Netty 4.x.\r\n\r\nBut netty 3 is also included in share/lib/ozone which is not required. The declared netty 3 version has security issues, we need to remove it to make it clear it's not used. (And make classpath safer)\r\n\r\nIt turned out that netty (and other dependencies) came with the test-jar dependencies used from Hadoop.\r\n\r\nBased on the reference of Maven, compile time dependencies of a test dependency should be used as test dependency (https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html) but in this case it doesn't work:\r\n\r\n{code}\r\ncd hadoop-hdds/container-service\r\nmvn dependency:tree\r\n\r\n...\r\n[INFO] +- org.apache.hadoop:hadoop-hdfs:test-jar:tests:3.2.1:test\r\n[INFO] |  +- org.eclipse.jetty:jetty-server:jar:9.4.34.v20201102:test\r\n[INFO] |  |  +- org.eclipse.jetty:jetty-http:jar:9.4.34.v20201102:test\r\n[INFO] |  |  \\- org.eclipse.jetty:jetty-io:jar:9.4.34.v20201102:test\r\n[INFO] |  +- org.eclipse.jetty:jetty-util-ajax:jar:9.4.34.v20201102:test\r\n[INFO] |  +- com.sun.jersey:jersey-core:jar:1.19:test\r\n[INFO] |  |  \\- javax.ws.rs:jsr311-api:jar:1.1.1:test\r\n[INFO] |  +- com.sun.jersey:jersey-server:jar:1.19:test\r\n[INFO] |  +- commons-cli:commons-cli:jar:1.2:compile\r\n[INFO] |  +- commons-codec:commons-codec:jar:1.11:compile\r\n[INFO] |  +- commons-daemon:commons-daemon:jar:1.0.13:test\r\n[INFO] |  +- javax.servlet:javax.servlet-api:jar:3.1.0:test\r\n[INFO] |  +- io.netty:netty:jar:3.10.5.Final:compile\r\n[INFO] |  +- org.apache.htrace:htrace-core4:jar:4.1.0-incubating:compile\r\n[INFO] |  \\- com.fasterxml.jackson.core:jackson-databind:jar:2.10.3:compile\r\n[INFO] \\- junit:junit:jar:4.11:test\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n...\r\n{code}\r\n\r\nHere all the dependencies of the hadop-hdfs:test-jar suppposed to have test scope.\r\n\r\nI didn't find the exact MVN issue, but found that there are multiple open issues related to transitive dependency resolution (can be the https://issues.apache.org/jira/browse/MNG-1378, but there are other open issues, too).\r\n\r\nAs a result, we should remain on the same side. I ssugest:\r\n\r\n 1. Exclude ALL the TRANSITIVE test dependencies for hadoop test-jars. Hadoop test-jars can still be used, but if we need any other class, they should be requested with an explicit dependency\r\n\r\n 2. hadoop-ozone-dependency-test should be used everywhere instead of using hadoop-hdfs or hadoop-common test jars (because it includes all the required excludes ;-) ) ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove unused netty3 transitive dependency"
   },
   {
      "_id": "13342352",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-24 12:00:07",
      "description": "AWS S3 accepts authorization information both from headers and query parameter.\r\n\r\nOzone s3g implementation parses only the headers.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support query parameter based v4 auth in S3g"
   },
   {
      "_id": "13342313",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-11-24 09:21:07",
      "description": "Provide information about Ozone's configured block size via Hadoop-compatible file system implementations: {{getDefaultBlockSize()}} and {{getDefaultBlockSize(Path)}} in {{FileSystem}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide info on block size via FileSystem"
   },
   {
      "_id": "13342238",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-11-24 01:33:10",
      "description": "Right now OM is terminating only when IOException when reload terminate happens, OM should terminate in case of any exceptions.\r\n\r\nIn one of our customer issue we have seen reloadOMState failed with IllegalStateException due to a bug which is fixed by HDDS-3944 and caused below issue, as installSnapshot failed we donot update commitIndex, lastWrittenIndex in SegmentedRaftLogWorker.\r\n\r\n\r\n\r\n{code:java}\r\n2020-11-20 01:17:01,990 ERROR org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker: om3@group-0C953F6B62D0-SegmentedRaftLogWorker hit exception\r\njava.lang.IllegalStateException: lastWrittenIndex == 134264, entry == term: 393\r\nindex: 134278\r\nmetadataEntry {\r\n  commitIndex: 134277\r\n}\r\n\r\n        at org.apache.ratis.util.Preconditions.assertTrue(Preconditions.java:63)\r\n        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker$WriteLog.execute(SegmentedRaftLogWorker.java:507)\r\n        at org.apache.ratis.server.raftlog.segmented.SegmentedRaftLogWorker.run(SegmentedRaftLogWorker.java:302)\r\n        at java.base/java.lang.Thread.run(Thread.java:834)\r\n{code}\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Reload OM State fail should terminate OM for any exceptions"
   },
   {
      "_id": "13342190",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337619",
            "id": "12337619",
            "name": "OM"
         }
      ],
      "created": "2020-11-23 18:16:22",
      "description": "Currently, by default, Ratis is not enabled on an OM single node cluster.\u00a0This Jira proposes to change the default single node OM to use Ratis server.\u00a0\r\n\r\nOM Ratis has been tested extensively and is stable now. To convert a single node cluster to HA, the first step would be to upgrade the cluster to Ratis enabled cluster.\u00a0\r\n\r\nA non-ratis single node cluster can be upgraded to\u00a0 ratis-enabled single node cluster by just setting the \\{{ozone.om.ratis.enable}} config to true and restarting the OM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable OM Ratis by default"
   },
   {
      "_id": "13342185",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-11-23 17:45:19",
      "description": "{code}\r\nCaused by: org.jooq.exception.DataAccessException: SQL [insert into FILE_COUNT_BY_SIZE (volume, bucket, file_size, count) values (?, ?, ?, ?)]; [SQLITE_CONSTRAINT]  Abort due to constraint violation (UNIQUE constraint failed: FILE_COUNT_BY_SIZE.volume, FILE_COUNT_BY_SIZE.bucket, FILE_COUNT_BY_SIZE.file_size)\r\n        at org.jooq_3.11.9.SQLITE.debug(Unknown Source)\r\n        at org.jooq.impl.Tools.translate(Tools.java:2429)\r\n        at org.jooq.impl.DefaultExecuteContext.sqlException(DefaultExecuteContext.java:832)\r\n        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:364)\r\n        at org.jooq.impl.TableRecordImpl.storeInsert0(TableRecordImpl.java:202)\r\n        at org.jooq.impl.TableRecordImpl$1.operate(TableRecordImpl.java:173)\r\n        at org.jooq.impl.RecordDelegate.operate(RecordDelegate.java:125)\r\n        at org.jooq.impl.TableRecordImpl.storeInsert(TableRecordImpl.java:169)\r\n        at org.jooq.impl.TableRecordImpl.insert(TableRecordImpl.java:157)\r\n        at org.jooq.impl.TableRecordImpl.insert(TableRecordImpl.java:152)\r\n        at org.jooq.impl.DAOImpl.insert(DAOImpl.java:175)\r\n        at org.jooq.impl.DAOImpl.insert(DAOImpl.java:151)\r\n        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.lambda$writeCountsToDB$0(FileSizeCountTask.java:209)\r\n        at java.util.HashMap$KeySet.forEach(HashMap.java:933)\r\n        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.writeCountsToDB(FileSizeCountTask.java:181)\r\n        at org.apache.hadoop.ozone.recon.tasks.FileSizeCountTask.reprocess(FileSizeCountTask.java:100)\r\n        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$reInitializeTasks$3(ReconTaskControllerImpl.java:175)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        ... 3 more\r\nCaused by: org.sqlite.SQLiteException: [SQLITE_CONSTRAINT]  Abort due to constraint violation (UNIQUE constraint failed: FILE_COUNT_BY_SIZE.volume, FILE_COUNT_BY_SIZE.bucket, FILE_COUNT_BY_SIZE.file_size)\r\n        at org.sqlite.core.DB.newSQLException(DB.java:941)\r\n        at org.sqlite.core.DB.newSQLException(DB.java:953)\r\n        at org.sqlite.core.DB.execute(DB.java:854)\r\n        at org.sqlite.core.DB.executeUpdate(DB.java:895)\r\n        at org.sqlite.jdbc3.JDBC3PreparedStatement.executeUpdate(JDBC3PreparedStatement.java:102)\r\n        at org.jooq.tools.jdbc.DefaultPreparedStatement.executeUpdate(DefaultPreparedStatement.java:99)\r\n        at org.jooq.impl.AbstractDMLQuery.execute(AbstractDMLQuery.java:629)\r\n        at org.jooq.impl.AbstractQuery.execute(AbstractQuery.java:350)\r\n        ... 17 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Recon File Size Count task throws SQL Exception."
   },
   {
      "_id": "13342121",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-23 12:41:32",
      "description": "Today both the closed-container-replication service and datanode client service are exposed on the same datanode port which makes it impossible to use different network configuration (like mutual TLS). \r\n\r\nIn this patch I propose to separated the two service and use two network port. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "incompatible",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Separate client and server2server GRPC services of datanode"
   },
   {
      "_id": "13341983",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-22 12:59:56",
      "description": "hadoop-hdds/config project provides a light-weight annotation based configuration interface. It supports to create an object with injection:\r\n\r\n\r\n{code:java}\r\nReplicationConfig replicationConfig := ozoneConfig.getObject(replicationConfig); {code}\r\nHowever, it seems to be hard to inject configuration to the service itself as usually we inject a lot of other dependencies to the constructor, not just the configuration.\r\n\r\n\u00a0\r\n\r\nOne possible solution is using a Guice module. Guice is already used by recon, and this patch adds some optional modules to inject configuration variables to any service / instance if required.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide Guice module to inject configuration to annotated fields"
   },
   {
      "_id": "13341464",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-11-19 00:45:22",
      "description": "Previously RatisServer which is network-partitoned/split-brain does not step down from leader state, so we require custom logic to determine leader(But that is also not completely correct, as the role state is updated based on server state not a quorum based information)\r\n\r\nNow, in Ratis leader steps down after the leader election max time out, so we can use RaftServer Api isLeader check. (RATIS-981 fixed this behavior)\r\n\r\nThis also fixes when serving read requests it should check leaderReady, not just isLeader because when it is leader, it might be still applying pending commit transactions, so there is a chance that acked transactions of write, might not be visible until we wait for isLeaderReady.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use RatisServerImpl isLeader instead of periodic leader update logic in OM and isLeaderReady for read requests"
   },
   {
      "_id": "13341404",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-11-18 18:15:49",
      "description": "Datanodes should include their latest processed CRL sequence ID in heartbeats to keep SCM updated on the status of CRL.\u00a0\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanodes should send last processed CRL sequence ID in heartbeats"
   },
   {
      "_id": "13341403",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-11-18 18:13:05",
      "description": "Ozone SCM should be able to persist Certificate Revocation List (CRL). We should add a new table in SCM DB definition to persist CRL and sequence id.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM should be able to persist CRL"
   },
   {
      "_id": "13341235",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-11-18 06:46:40",
      "description": "Currently OM deletion service iterates through deleted keys table and sends blocks for deletion to SCM. I can see that blocks for same key are sent 10 times from OM to SCM. This would lead to creation of 10 transactions for the same blocks in SCM.\r\nApparently the db is not updated and OM keeps sending the same set of keys for about 10 minutes.\r\n\r\ncc [~bharat] [~hanishakoneru]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "With HA OM can send deletion blocks to SCM multiple times"
   },
   {
      "_id": "13340814",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-16 10:57:21",
      "description": "Change URL on github page to the new TLP domain.\r\n\r\n\u00a0\r\n\r\nThanks the report for [~cxorm]\r\n\r\nhttps://github.com/apache/ozone/pull/1540#issuecomment-722217690\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Update url in .asf.yaml to use TLP project"
   },
   {
      "_id": "13340496",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-11-13 11:08:36",
      "description": "Save Ozone binaries from {{compile}} check, and use these in {{acceptance}} and {{kubernetes}} checks, instead of each check performing its own full build.  Total execution time is similar, but the dependent checks are started later, so we save GitHub Actions cycles.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Reuse compiled binaries in acceptance test"
   },
   {
      "_id": "13340490",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-13 10:25:58",
      "description": "Today it's hard to understand what's going on and what is the performance of the replication.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add metrics for closed container replication"
   },
   {
      "_id": "13339996",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-11 14:29:47",
      "description": "[~AlfredChang] reported a very particular case related to the data replication:\r\n\r\nWhen a container is downloaded successfully but the content is invalid, the error the retry mechanism doesn't work very well. Downloader reports the download successfull therefore it won't be reported as failure, but import will fail again and again.\r\n\r\nTo make it more resilient we can select datanode randomly,",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Replicate closed container from random selected datanode "
   },
   {
      "_id": "13339955",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-11 10:44:44",
      "description": "./hadoop-ozone/dev-support/checks/findbugs.sh -- which is a short-cut to execute the CI findbugs check locally -- couldn't be executed locally after a full build:\r\n\r\n{code}\r\n./hadoop-ozone/dev-support/checks/findbugs.sh\r\n....\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  3.451 s\r\n[INFO] Finished at: 2020-11-11T11:42:40+01:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs (spotbugs) on project hadoop-hdds: Execution spotbugs of goal com.github.spotbugs:spotbugs-maven-plugin:3.1.12:spotbugs failed: Java returned: 1 -> [Help 1]\r\n[ERROR] \r\n{code}\r\n\r\nThe problem:\r\n\r\n`target/classes` directory should be either empty/missing or it should contain java classes to make spotbugs work.\r\n\r\nOn github it works well as an empty checkout is tested. But locally it's possible that a dummy classpath file is created under `hadoop-hdds/target/classes` which breaks spotbug local execution.\r\n\r\nThe solution is easy: execute the classpath descriptor generation only if `src/main/java` dir exists.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "findbugs.sh couldn't be executed after a full build"
   },
   {
      "_id": "13339912",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-11-11 08:08:38",
      "description": "Currently Ozone relies on {{HADOOP_\\*}} environment variables (eg. {{HADOOP_HOME}}) for historical and practical reasons (code reuse).  If Hadoop and Ozone are both present on a node, and they share their environment, then {{ozone}} (and {{stop-ozone.sh}}) exits with error:\r\n\r\n{code}\r\n$ ozone help\r\nERROR: Cannot execute /usr/local/hadoop/libexec/ozone-config.sh.\r\n{code}\r\n\r\nThe same problem was reported and fixed in HDDS-1912 for the {{start-ozone.sh}} script.  This task is limited to applying the changes from HDDS-1912 to the other two scripts.  Ideally Ozone should use its own environment variables, which is a much larger change.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cannot run ozone if HADOOP_HOME points to Hadoop install"
   },
   {
      "_id": "13339736",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-11-10 12:19:19",
      "description": "{{KeyManagerImpl#listStatus}} issues duplicate {{refreshPipeline}} for each file.\r\n\r\nHDDS-3824 moved {{refreshPipeline}} outside the bucket lock.  But HDDS-3658 added it back, while keeping the one outside the lock, probably as a result of merge conflict resolution.\r\n\r\nAlso, [~weichiu] pointed out that {{refreshPipeline}} supports lists, too, so the calls can be batched to reduce number of RPC.\r\n\r\nCC [~msingh]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Duplicate refreshPipeline in listStatus"
   },
   {
      "_id": "13339657",
      "assignee": "xyao",
      "components": [],
      "created": "2020-11-10 05:02:15",
      "description": "HDDS-4088 add the owner info to the authorizer access check context. There is a bug in the getOwner info logic which is supposed to skip volume owner info only for volume create or volume root access. \r\n\r\nThis ticket is opened to fix the issue above. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Owner info is not passed to authorizer for BUCKET/KEY create request"
   },
   {
      "_id": "13339651",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-11-10 03:19:17",
      "description": "Recon uses Derby as its SQL database by default. Switching it to Mariadb/Mysql throws an exception `Specified key was too long; max key length is 767 bytes`\r\n\r\nFix this issue by reducing the primary key length from VARCHAR(768) to VARCHAR(766) in ReconTaskSchemaDefinition.java. Also change the autocommit config to default to true since data is not persisted in MySql tables even after insert queries are executed with autocommit flag set to false by default.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon: Using Mysql database throws exception and fails startup"
   },
   {
      "_id": "13339507",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-09 11:21:22",
      "description": "Design doc will be uploaded, soon.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Design Doc: Use per-request authentication and persistent connections between S3g and OM"
   },
   {
      "_id": "13338742",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-11-03 23:18:52",
      "description": "This Jira aims to update ozone with latest Ratis snapshot which has a critical fix for \"Bootstrap new OM Node\" feature - HDDS-4330.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update Ratis version to latest snapshot"
   },
   {
      "_id": "13338726",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-11-03 21:16:16",
      "description": "The current OM has one second failover timeout. This is too short as any network hiccup, system I/O or JVM GC pause could easily trigger a failover.\r\n\r\nExample:\r\n{noformat}\r\n2020-10-29 09:02:46,557 WARN org.apache.ratis.server.impl.RaftServerImpl: om3@group-942F8267F22A-LeaderState: Lost leadership on term: 33. Election timeout: 1200ms. In charge for: 82665\r\n0319ms. Conf: 32189729: [om1:rhelnn01.ozone.cisco.local:9872:0, om3:rhelnn03.ozone.cisco.local:9872:0, om2:rhelnn02.ozone.cisco.local:9872:0], old=null. Followers: [om3@group-942F8267F2\r\n2A->om1(c34577386,m34577394,n34577395, attendVote=true, lastRpcSendTime=7, lastRpcResponseTime=0), om3@group-942F8267F22A->om2(c34577386,m34577261,n34577395, attendVote=true, lastRpcSen\r\ndTime=7, lastRpcResponseTime=0)]\r\n2020-10-29 09:02:46,558 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2236ms\r\nNo GCs detected\r\n2020-10-29 09:02:46,562 INFO org.apache.ratis.server.impl.RaftServerImpl: om3@group-942F8267F22A: changes role from    LEADER to FOLLOWER at term 33 for stepDown\r\n2020-10-29 09:02:46,563 INFO org.apache.ratis.server.impl.RoleInfo: om3: shutdown LeaderState\r\n{noformat}\r\n\r\n[~hanishakoneru]\u00a0also thinks we should increase ratis leader election timeout too.\r\n\r\n{noformat}\r\n  <property>\r\n    <name>ozone.om.ratis.minimum.timeout</name>\r\n    <value>1s</value>\r\n    <tag>OZONE, OM, RATIS, MANAGEMENT</tag>\r\n    <description>The minimum timeout duration for OM's Ratis server rpc.\r\n    </description>\r\n  </property>\r\n\r\n  <property>\r\n    <name>ozone.om.leader.election.minimum.timeout.duration</name>\r\n    <value>1s</value>\r\n    <tag>OZONE, OM, RATIS, MANAGEMENT</tag>\r\n    <description>The minimum timeout duration for OM ratis leader election.\r\n      Default is 1s.\r\n    </description\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "OM failover timeout is too short"
   },
   {
      "_id": "13338676",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-03 15:52:01",
      "description": "[~weichiu] reported that in a specific case the Datanode tried to download / replicate containers multiple times from the same datanode.\r\n\r\n\u00a0\r\n\r\nSimpleContainerDownload has a logic to try out all the available datanodes: this Jira creates a unit test t make sure the logic works well.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create unit test for SimpleContainerDownloader"
   },
   {
      "_id": "13338597",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-03 09:24:10",
      "description": "security@ozone.apache.org is created, we can add in to the README.md and we can create the SECURITY.md file (Github naming convention)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update README with information how to report security issues"
   },
   {
      "_id": "13338467",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-11-02 17:20:50",
      "description": " !Screen Shot 2020-11-01 at 11.46.40 PM.png! \r\n\r\nAs an example, the above screenshot is for a SCM who was no container reported. However, the status message \"currentContainerThreshold 0.0 >= safeModeCutoff 0.99\" which is counterintuitive. We should make it easier to tell why SCM is still in safe mode.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Misleading SCM web UI Safe mode status "
   },
   {
      "_id": "13338449",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-11-02 15:37:57",
      "description": "TestContainerMetrics is flaky since HDDS-4359. Failed in following master builds:\r\n\r\n{code}\r\n2020/10/26/3569/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/27/3581/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/28/3591/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/29/3619/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/30/3628/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/30/3642/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/31/3650/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n2020/10/31/3654/it-ozone/hadoop-ozone/integration-test/TEST-org.apache.hadoop.ozone.container.metrics.TestContainerMetrics.xml\r\n{code}\r\n\r\nSome of the added assertions couldn't be guaranteed all the time:\r\n\r\n{code}\r\n      // ReadTime and WriteTime vary from run to run, only checking non-zero\r\n      Assert.assertNotEquals(0L, getLongCounter(\"ReadTime\", volumeIOMetrics));\r\n      Assert.assertNotEquals(0L, getLongCounter(\"WriteTime\", volumeIOMetrics));\r\n{code}\r\n\r\nIn very lucky case the read/write time can be zero.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestContainerMetrics is flaky"
   },
   {
      "_id": "13338443",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-02 15:02:41",
      "description": "In HDDS-4185 we agreed to introduce a new configuration for the Ozone client to adjust the behavior of incremental byte buffer.\r\n\r\nWhen I started to add it I realized the code is already very complex as all the required configuration values are propagated manually to the Key/Block/ChunkOutputstream as induvidual constructor parameters.\r\n\r\nIn this patch I simplify the structure with using only one POJO with configuration annotations.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Simplify Ozone client code with configuration object"
   },
   {
      "_id": "13338373",
      "assignee": "elek",
      "components": [],
      "created": "2020-11-02 09:16:31",
      "description": "README.md can be updated with the new mailing lists and references to \"Hadoop subproject\" can be removed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update README.md after TLP separation"
   },
   {
      "_id": "13337773",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-10-29 00:48:29",
      "description": "{code:java}\r\n[root@uma-1 ~]# sudo -u hdfs hdfs dfs -ls\u00a0o3fs://bucket.volume.ozone1/\r\n20/10/28 23:37:50 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3.\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123)\r\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:73)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113)\r\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)\r\n, while invoking $Proxy10.submitRequest over {om1=nodeId=om1,nodeAddress=uma-1.uma.root.hwx.site:9862, om3=nodeId=om3,nodeAddress=uma-3.uma.root.hwx.site:9862, om2=nodeId=om2,nodeAddress=uma-2.uma.root.hwx.site:9862} after 1 failover attempts. Trying to failover immediately.{code}\r\n\r\nThis issue in the Apache Ozone main branch will be fixed once Hadoop version is updated. For vendors/users who backport fix to their Hadoop version and have ozone compiled with that version, this fix will help them not to see first failovers till it finds leader OM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Proxy failover is logging with out trying all OMS"
   },
   {
      "_id": "13337670",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-10-28 13:35:54",
      "description": "{code}\r\nError:  Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-ozone-ozone-manager: Compilation failure: Compilation failure: \r\nError:  /mnt/ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java:[691,6] not a statement\r\nError:  /mnt/ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerRatisServer.java:[691,23] ';' expected\r\nError:  -> [Help 1]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix compilation issue in HDDS-3698-upgrade branch."
   },
   {
      "_id": "13337648",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-10-28 11:27:50",
      "description": "The idea here is to add a config to make raft log directory removal configurable during pipeline remove.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make raft log directory deletion configurable during pipeline remove"
   },
   {
      "_id": "13337644",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-10-28 11:18:18",
      "description": "Currently, for safe mode we consider all pipelines existing in DB for safe mode exit criteria. It ma happen that, SCM has the pipelines craeted , but none of the participants datanodes ever created these datanodes. In such cases, SCM fails to come out of safemode as these pipelines are never reported back to SCM.\r\n\r\n\u00a0\r\n\r\nThe idea here is to consider pipelines which are marked open during SCM startup.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Safe mode rule for piplelines should only consider open pipelines"
   },
   {
      "_id": "13337246",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2020-10-26 16:37:33",
      "description": "I've been working on this fun project and would like to share with the community.\r\n\r\n\u00a0\r\nh1. Synopsis\r\n\r\nWe want to prove Ozone runs well at scale, in terms of number of keys (billions of keys), as well as dense DataNodes where each DN has hundreds of TB or even PB-scale capacity.\r\nh1. Challenge: Data generation\r\n\r\nThe challenge is to generate a huge data set fast so that we can benchmark the system quickly. No existing tool is capable at this scale. \r\n\r\n\u00a0\r\nh1. Proposal:\r\n\r\nThe major bottleneck is OM\u2019s key insertion performance. In addition, Ozone uses a single pipeline to write data, unless multi-raft is enabled.\r\n\r\n\u00a0\r\n\r\nInstead of using Ozone's client API to generate data, We should write directly to OM, SCM and DN\u2019s rocksdb. RocksDB can support u[p to a million key|https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks] bulk load operations.\r\n\r\n\u00a0\r\n\r\nSimilarly, we can skip the normal Ozone client write path; populate the container db and block files directly.\r\n\r\n\u00a0\r\n\r\n(more details in the design doc)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Data Generator for Fast Scale Test"
   },
   {
      "_id": "13336987",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-10-23 20:51:22",
      "description": "Recon Architecture and details are missing in the docs - https://ci-hadoop.apache.org/view/Hadoop%20Ozone/job/ozone-doc-master/lastSuccessfulBuild/artifact/hadoop-hdds/docs/public/concept/ozonemanager.html",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add Recon architecture to docs"
   },
   {
      "_id": "13336869",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-10-23 07:31:45",
      "description": "Currently, in ratis \"writeStateMachinecall\" gets retried indefinitely in event of a timeout. In case, where disks are slow/overloaded or number of chunk writer threads are not available for a period of 10s, writeStateMachine call times out in 10s. In cases like these, the same write chunk keeps on getting retried causing the same chunk of data to be overwritten. The idea here is to abort the request once the node failure timeout reaches.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make writeStateMachineTimeout retry count proportional to node failure timeout"
   },
   {
      "_id": "13334734",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-10-09 23:02:47",
      "description": "In a ratis enabled OM cluster, add support to bootstrap a new OM node and add it to OM ratis ring.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bootstrap new OM node"
   },
   {
      "_id": "13334702",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-10-09 18:52:34",
      "description": "This Jira is to expose config Ratis retry cache duration in OM, and also choose a sensible default value.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Expose Ratis retry config cache in OM"
   },
   {
      "_id": "13334482",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-10-08 18:43:00",
      "description": "there are a number of places in the code where BatchOperation is used but not closed. As a best practice, better to close them explicitly.\r\n\r\nI have a stress test code that uses BatchOperation to insert into OM rocksdb. Without closing BatchOperation explicitly, the process crashes after just a few minutes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Potential resource leakage using BatchOperation"
   },
   {
      "_id": "13334327",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-10-08 01:36:36",
      "description": "It seems that the return codes of ozone getconf -confKey are different in 1.0 and after 1.0.\r\n\r\nLooking at the code:\r\n\r\nin old code:\r\n\r\n/** Method to be overridden by sub classes for specific behavior. */\r\nint doWorkInternal(OzoneGetConf tool, String[] args) throws Exception {\r\n\r\n\r\n{code:java}\r\n String value = tool.getConf().getTrimmed(key);\r\n if (value != null) {\r\n tool.printOut(value);\r\n return 0;\r\n }\r\n tool.printError(\"Configuration \" + key + \" is missing.\");\r\n return -1;\r\n}\r\n{code}\r\n\r\nwith 1.0 code:\r\n@Override\r\n  public Void call() throws Exception {\r\n    String value = tool.getConf().getTrimmed(confKey);\r\n    if (value != null) {\r\n      tool.printOut(value);\r\n    } else {\r\n      tool.printError(\"Configuration \" + confKey + \" is missing.\");\r\n    }\r\n    return null;\r\n  }\r\n\r\nWe are returning null irrespective of the cases.\r\nSome applications/tests depending on this codes.\r\n\r\nThanks [~nmaheshwari] for helping on debug and finding the issue.\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Incompatible return codes from Ozone getconf -confKey"
   },
   {
      "_id": "13334230",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-10-07 15:55:28",
      "description": "{code:title=https://github.com/apache/hadoop-ozone/runs/1217093596#step:6:5632}\r\nError:  Failed to execute goal org.codehaus.mojo:aspectj-maven-plugin:1.10:compile (default) on project hadoop-ozone-ozone-manager: Execution default of goal org.codehaus.mojo:aspectj-maven-plugin:1.10:compile failed: Plugin org.codehaus.mojo:aspectj-maven-plugin:1.10 or one of its dependencies could not be resolved: Could not find artifact com.sun:tools:jar:11.0.8 at specified path /opt/hostedtoolcache/jdk/11.0.8/x64/../lib/tools.jar -> [Help 1]\r\n{code}\r\n\r\nhttps://github.com/mojohaus/aspectj-maven-plugin/issues/24#issuecomment-419077658",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Compile error with Java 11"
   },
   {
      "_id": "13334093",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2020-10-07 00:00:05",
      "description": "Angular versions < 1.8.0 are vulnerable to cross-site scripting\r\n\r\n[https://nvd.nist.gov/vuln/detail/CVE-2020-7676]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upgrade to angular 1.8.0 due to CVE-2020-7676"
   },
   {
      "_id": "13334083",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-10-06 22:35:48",
      "description": "In a non-Ratis OM, the transaction index used to generate ObjectID is reset on OM restart. This can lead to duplicate ObjectIDs when the OM is restarted. ObjectIDs should be unique.\u00a0\r\nHDDS-2939 and NFS are some of the features which depend on ObjectIds being unique.\r\n\r\nTo ensure that objectIDs are unique across restarts in non-ratis OM cluster, the transaction index should be updated in DB on every flush to DB. This can be done in a similar fashion to what is being done for ratis enabled cluster today. TransactionInfo table is updated with transaction index as part of every batch write operation to DB.\r\n\r\nAlso, and epoch number is introduced to ensure that objectIDs do not clash with older clusters in which this fix does not exist. From the 64 bits of ObjectID (long variable), 2 bits are reserved for epoch and 8 bits for recursive directory creation, if required.\u00a0The most significant 2 bits of objectIDs is set to epoch. For clusters before HDDS-4315 there is no epoch as such. But it can be safely assumed that the most significant 2 bits of the objectID will be 00 (as it unlikely to reach trxn index > 2^62 in an existing cluster). From HDDS-4315 onwards, the Epoch for non-ratis OM clusters will be binary 01 (= decimal 1) and for ratis\u00a0enabled OM cluster will be binary 10 (= decimal 2).\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure ObjectIDs are unique across restarts"
   },
   {
      "_id": "13334077",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-10-06 22:04:18",
      "description": "{code}\r\norg.reflections.ReflectionsException: could not get type for name mockit.MockUp\r\n\tat org.reflections.ReflectionUtils.forName(ReflectionUtils.java:312)\r\n\tat org.reflections.Reflections.expandSuperTypes(Reflections.java:382)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:140)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:182)\r\n\tat org.reflections.Reflections.<init>(Reflections.java:155)\r\n\tat org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.registerOzoneManagerRequests(OMLayoutVersionManagerImpl.java:122)\r\n\tat org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.init(OMLayoutVersionManagerImpl.java:100)\r\n\tat org.apache.hadoop.ozone.om.upgrade.OMLayoutVersionManagerImpl.initialize(OMLayoutVersionManagerImpl.java:83)\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:363)\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:930)\r\n\tat org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.createOMService(MiniOzoneHAClusterImpl.java:379)\r\n\tat org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.build(MiniOzoneHAClusterImpl.java:294)\r\n\tat org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:147)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\nCaused by: java.lang.ClassNotFoundException: mockit.MockUp\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat org.reflections.ReflectionUtils.forName(ReflectionUtils.java:310)\r\n\t... 23 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM Layout Version Manager init throws silent CNF error in integration tests."
   },
   {
      "_id": "13333937",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-10-06 13:53:57",
      "description": "Findbugs check has been silently failing but reporting success for some time now.  The problem is that {{findbugs.sh}} determines exit code based on the number of findbugs failures.  If {{compile}} step fails, exit code is 0, ie. success.\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/1210535433#step:3:866}\r\n2020-10-02T18:37:57.0699502Z [ERROR] Failed to execute goal on project hadoop-hdds-client: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-client:jar:1.1.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-common:jar:tests:1.1.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "findbugs check succeeds despite compile error"
   },
   {
      "_id": "13333931",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-10-06 13:27:21",
      "description": "Abstract and links for http://hadoop.apache.org/ozone/docs/1.0.0/design/typesafeconfig.html are wrong, reference OM HA design doc.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Type-safe config design doc points to OM HA"
   },
   {
      "_id": "13330930",
      "assignee": "xyao",
      "components": [],
      "created": "2020-10-04 02:44:05",
      "description": "CheckStyle: Move LineLength Check parent from TreeWalker to Checker, otherwise fail to import to latest IntelliJ\r\n\r\nSimilar issue has been reported here and I've verified the fix locally that the IntelliJ can import checkstyle rule after the fix. \r\nhttps://github.com/checkstyle/checkstyle/issues/2116\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone checkstyle rule can't be imported to IntelliJ"
   },
   {
      "_id": "13330452",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-10-01 21:56:49",
      "description": "This could be problematic with strict security provider such as FIPS. The default non-FIPS provider such as SunJCE and BC provider work fine though. This ticket is opened to fix it. \r\n\r\n\r\n{code:java}\r\n2020-09-30 12:01:52,962 ERROR org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer: Unable to initialize CertificateServer.\r\norg.apache.hadoop.hdds.security.exception.SCMSecurityException: java.security.cert.CertificateParsingException: cannot construct KeyUsage: java.lang.IllegalArgumentException: illegal object in getInstance: com.safelogic.cryptocomply.asn1.DEROctetString\r\n        at org.apache.hadoop.hdds.security.x509.certificate.utils.CertificateCodec.getPEMEncodedString(CertificateCodec.java:105)\r\n        at org.apache.hadoop.hdds.security.x509.certificate.utils.CertificateCodec.writeCertificate(CertificateCodec.java:182)\r\n        at org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer.generateRootCertificate(DefaultCAServer.java:495)\r\n        at org.apache.hadoop.hdds.security.x509.certificate.authority.DefaultCAServer.generateSelfSignedCA(DefaultCAServer.java:303)\r\n  \r\n{code}\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM CA certificate does not encode KeyUsage extension properly"
   },
   {
      "_id": "13330158",
      "assignee": "elek",
      "components": [],
      "created": "2020-09-30 11:59:46",
      "description": "XceiverClientManager is used everywhere in the ozone client (Key/Block Input/OutputStream) to get a client when required.\r\n\r\nTo make it easier to create genesis/real unit tests, it would be better to use a generic interface instead of XceiverClientManager which can make it easy to replace the manager with a mock implementation.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use an interface in Ozone client instead of XceiverClientManager"
   },
   {
      "_id": "13330062",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-09-29 23:58:50",
      "description": "HDDS-3560 created new ProxyInfo object in case of\u00a0IllegalAccessError exception. But, it does not return the new instance and causes NPE in Hadoop versions < 3.2\r\n\r\n\r\n{code:java}\r\n20/09/29 23:10:22 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:20/09/29 23:10:22 ERROR client.OzoneClientFactory: Couldn't create RpcClient protocol exception:java.lang.NullPointerException at org.apache.hadoop.io.retry.RetryInvocationHandler.isRpcInvocation(RetryInvocationHandler.java:435) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:354) at com.sun.proxy.$Proxy10.submitRequest(Unknown Source) at org.apache.hadoop.ozone.om.protocolPB.Hadoop3OmTransport.submitRequest(Hadoop3OmTransport.java:89) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:213) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1030) at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:175) at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:242) at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:113) at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:149) at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:51) at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:94) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:161) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3288) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:123) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3337) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3305) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476) at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361) at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:352) at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:250) at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:233) at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103) at org.apache.hadoop.fs.shell.Command.run(Command.java:177) at org.apache.hadoop.fs.FsShell.run(FsShell.java:326) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) at org.apache.hadoop.fs.FsShell.main(FsShell.java:389)ls: Couldn't create RpcClient protocol\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Client not working with Hadoop Version  < 3.2"
   },
   {
      "_id": "13329886",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-09-29 08:26:14",
      "description": "It could be a by-product of the introduction of the issue\uff1a\u00a0https://issues.apache.org/jira/browse/HDDS-4166",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "the icon of hadoop-ozone is bigger than ever"
   },
   {
      "_id": "13329759",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-09-28 14:19:20",
      "description": "Ozone read operation turned out to be slow mainly because we do a new UGI.getCurrentUser for block token for each of the calls.\r\n\r\nWe need to cache the block token / UGI.getCurrentUserCall() to make it faster.\r\n\r\n !image-2020-09-28-16-19-17-581.png! \r\n\r\nTo reproduce:\r\n\r\nCheckout: https://github.com/elek/hadoop-ozone/tree/mocked-read\r\n\r\n{code}\r\ncd hadoop-ozone/client\r\n\r\nexport MAVEN_OPTS=-agentpath:/home/elek/prog/async-profiler/build/libasyncProfiler.so=start,file=/tmp/profile-%t-%p.svg\r\n\r\nmvn compile exec:java -Dexec.mainClass=org.apache.hadoop.ozone.client.io.TestKeyOutputStreamUnit -Dexec.classpathScope=test\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Read is slow due to frequent calls to UGI.getCurrentUser() and getTokens()"
   },
   {
      "_id": "13328985",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2020-09-23 12:13:43",
      "description": "HDDS-2660 added an insight point for the datanode dispatcher.  At trace level it logs all chunk content, which can be huge and contain control characters, so I think we should avoid it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid logging chunk content in Ozone Insight"
   },
   {
      "_id": "13328981",
      "assignee": "elek",
      "components": [],
      "created": "2020-09-23 12:01:11",
      "description": "I am using https://byteman.jboss.org to debug the performance of spark + terage with different scripts. Some byteman scripts are already shared by HDDS-4095 or HDDS-342 but it seems to be a good practice to share the newer scripts to make it possible to reproduce performance problems.\r\n\r\nFor using byteman with Ozone, see this video:\r\nhttps://www.youtube.com/watch?v=_4eYsH8F50E&list=PLCaV-jpCBO8U_WqyySszmbmnL-dhlzF6o&index=5",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add more reusable byteman scripts to debug ofs/o3fs performance"
   },
   {
      "_id": "13328819",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-09-22 16:23:52",
      "description": "This is a follow up task from HDDS-4227 in which the prepare upgrade/downgrade task should purge the Raft log immediately after waiting for the last txn to be applied. This is to make sure that we dont \"apply\" transactions in different versions of the code across the quorum. A lagging follower will use a Ratis snapshot to bootstrap itself on restart.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Prepare for Upgrade step should purge the log after waiting for the last txn to be applied."
   },
   {
      "_id": "13328324",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-09-18 23:14:23",
      "description": "Use clientID and callID to uniquely identify the requests.\r\nThis will help in case when the request is retried for write requests, when the previous one is already processed, the previous result can be returned from the cache.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use ClientID and CallID from Rpc Client to detect retry requests"
   },
   {
      "_id": "13327935",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-09-16 23:35:17",
      "description": "This Jira aims to update ozone with latest Ratis snapshot which has a critical fix for OM HA - RATIS-1025.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update Ratis version to latest snapshot"
   },
   {
      "_id": "13327540",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2020-09-14 23:27:05",
      "description": "TroubleShooting S3A mentions S3 compatible servers that donot support Etags will see this server\r\n\r\nRefer [link|https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/troubleshooting_s3a.html] and look for below section content.\r\nUsing a third-party S3 implementation that doesn\u2019t support eTags might result in the following error.\r\n\r\norg.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://my-bucket/test/file.txt':\r\n Change detection policy requires ETag\r\n  at org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:153)\r\n  at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:200)\r\n  at org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:346)\r\n  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$2(Invoker.java:195)\r\n  at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)\r\n  at org.apache.hadoop.fs.s3a.Invoker.lambda$retry$3(Invoker.java:265)\r\n  at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\r\n  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:261)\r\n  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:193)\r\n  at org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:215)\r\n  at org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:339)\r\n  at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:372)\r\n\r\n\r\n{code:java}\r\norg.apache.hadoop.fs.s3a.NoVersionAttributeException: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag\r\n\tat org.apache.hadoop.fs.s3a.impl.ChangeTracker.processNewRevision(ChangeTracker.java:275)\r\n\tat org.apache.hadoop.fs.s3a.impl.ChangeTracker.processMetadata(ChangeTracker.java:261)\r\n\tat org.apache.hadoop.fs.s3a.impl.ChangeTracker.processResponse(ChangeTracker.java:195)\r\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:208)\r\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.lambda$lazySeek$1(S3AInputStream.java:359)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$3(Invoker.java:223)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:110)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.lambda$maybeRetry$5(Invoker.java:347)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:407)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:343)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:221)\r\n\tat org.apache.hadoop.fs.s3a.Invoker.maybeRetry(Invoker.java:265)\r\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.lazySeek(S3AInputStream.java:351)\r\n\tat org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:464)\r\n\tat java.io.DataInputStream.read(DataInputStream.java:100)\r\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:94)\r\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68)\r\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:494)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:416)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:351)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:286)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:271)\r\n\tat org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:367)\r\n\tat org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)\r\n\tat org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:266)\r\n\tat org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)\r\n\tat org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:237)\r\n\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:120)\r\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:177)\r\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:328)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\r\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:391)\r\nget: `s3a://sept14/dir1/dir2/dir3/key1': Change detection policy requires ETag\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OzoneS3",
         "S3A"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Get API not working from S3A filesystem with Ozone S3"
   },
   {
      "_id": "13327474",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-09-14 15:46:44",
      "description": "Currently, Ozone token CLI produce token in base64 encode format. This is not compatible with HADOOP_TOKEN_FILE_LOCATION and can't be used directly for Ozone/Hadoop CLI to authenticate. This ticket is opened to persist ozone token in a format that is compatible with HADOOP_TOKEN_FILE_LOCATION along with tests. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support HADOOP_TOKEN_FILE_LOCATION for Ozone token CLI"
   },
   {
      "_id": "13326735",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-09-09 20:38:38",
      "description": "*Why is this needed?*\r\nThrough HDDS-4143, we have a generic factory to handle multiple versions of apply transaction implementations based on layout version. Hence, this factory can be used to handle versioned requests across layout versions, whenever both the versions need to exist in the code (Let's say for HDDS-2939). \r\n\r\nHowever, it has been noticed that the OM ratis requests are still undergoing lot of minor changes (HDDS-4007, HDDS-4007, HDDS-3903), and in these cases it will become hard to maintain 2 versions of the code just to support clean upgrades. \r\n\r\nHence, the plan is to build a pre-upgrade utility (client API) that makes sure that an OM instance has no \"un-applied\" transactions in this Raft log. Invoking this client API makes sure that the upgrade starts with a clean state. Of course, this would be needed only in a HA setup. In a non HA setup, this can either be skipped, or when invoked will be a No-Op (Non Ratis) or cause no harm (Single node Ratis).\r\n\r\n*How does it work?*\r\nBefore updating the software bits, our goal is to get OMs to get to the  latest state with respect to apply transaction. The reason we want this is to make sure that the same version of the code executes the AT step in all the 3 OMs. In a high level, the flow will be as follows.\r\n\r\n* Before upgrade, *stop* the OMs.\r\n* Start OMs with a special flag --prepareUpgrade (This is something like --init,  which is a special state which stops the ephemeral OM instance after doing some work)\r\n* When OM is started with the --prepareUpgrade flag, it does not start the RPC server, so no new requests can get in.\r\n* In this state, we give every OM time to apply txn until the last txn.\r\n* We know that at least 2 OMs would have gotten the last client request transaction committed into their log. Hence, those 2 OMs are expected to apply transaction to that index faster.\r\n* At every OM, the Raft log will be purged after this wait period (so that the replay does not happen), and a Ratis snapshot taken at last txn.\r\n* Even if there is a lagger OM which is unable to get to last applied txn index, its logs will be purged after the wait time expires.\r\n* Now when OMs are started with newer version, all the OMs will start using the new code.\r\n* The lagger OM will get the new Ratis snapshot since there are no logs to replay from.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement a \"prepareForUpgrade\" step that applies all committed transactions onto the OM state machine."
   },
   {
      "_id": "13326512",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-09-08 15:16:52",
      "description": "Investigate whether we can programmatically instantiate the OM Aspect so that we can move away from static nature of OM Layout Version Manager. Moving away from static behavior will help out with easy unit testing. \r\n\r\ncc [~pifta]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Revisit 'static' nature of OM Layout Version Manager."
   },
   {
      "_id": "13326110",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-09-04 19:30:45",
      "description": "In HA, in validateAndUpdateCache when resolveBucket, it checks the permission using checkAcls. But it will have not any RpcContext and it will fail with NPE in checkAcls when getting hostName.\r\n\r\nFor this same reason, we added the required information to check ACLs into OMRequest.\r\n\r\n\r\n{code:java}\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.checkAcls(OzoneManager.java:1604)\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3497)\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3465)\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.resolveBucketLink(OzoneManager.java:3452)\r\n\tat org.apache.hadoop.ozone.om.request.key.OMKeyRequest.resolveBucketLink(OMKeyRequest.java:96)\r\n\tat org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:215)\r\n\tat org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:246)\r\n\tat java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "ResolveBucket during checkAcls fails"
   },
   {
      "_id": "13326018",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-09-04 04:55:41",
      "description": "When *ozone.om.enable.filesystem.paths* is enabled\r\n\r\n\u00a0\r\n\r\nhdfs dfs -mkdir -p s3a://b12345/d11/d12 -> Success\r\n\r\nhdfs dfs -put /tmp/file1\u00a0s3a://b12345/d11/d12/file1 -> fails with below error\r\n\r\n\u00a0\r\n{code:java}\r\n2020-09-04 03:53:51,377 ERROR org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest: Key creation failed. Volume:s3v, Bucket:b1234, Keyd11/d12/file1._COPYING_. Exception:{}\r\nNOT_A_FILE org.apache.hadoop.ozone.om.exceptions.OMException: Can not create file: cp/k1._COPYING_ as there is already file in the given path\r\n at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:256)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:227)\r\n at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:428)\r\n at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$applyTransaction$1(OzoneManagerStateMachine.java:246)\r\n at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748){code}\r\n*Reason for this*\r\n S3A filesystem when create directory creates an empty file\r\n\r\n*Now entries in Ozone KeyTable after create directory*\r\n d11/\r\n d11/d12\r\n\r\nBecause of this in OMFileRequest.VerifyInFilesPath fails with FILE_EXISTS_IN_GIVEN_PATH because d11/d12 is considered as file not a directory. (As in ozone currently, directories end with trailing \"/\")\r\n\r\nSo, when d11/d12/file is created, we check parent exists, now d11/d12 is considered as file and fails with NOT_A_FILE\r\n\r\nWhen disabled it works fine, as when disabled during key create we do not check any filesystem semantics and also does not create intermediate directories.\r\n{code:java}\r\n[root@bvoz-1 ~]# hdfs dfs -mkdir -p s3a://b12345/d11/d12\r\n[root@bvoz-1 ~]# hdfs dfs -put /etc/hadoop/conf/ozone-site.xml s3a://b12345/d11/d12/k1\r\n[root@bvoz-1 ~]# hdfs dfs -ls s3a://b12345/d11/d12\r\nFound 1 items\r\n-rw-rw-rw-   1 systest systest       2373 2020-09-04 04:45 s3a://b12345/d11/d12/k1\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "OzoneS3",
         "S3A",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "S3A Filesystem does not work with Ozone S3 in file system compat mode"
   },
   {
      "_id": "13325986",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-09-03 21:18:14",
      "description": "The Docs page does not render the table correctly in OM and SCM architecture pages.\r\n\r\nAlso, the ozone logo in the header is not visible.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix table rendering and logo display in docs"
   },
   {
      "_id": "13325938",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-09-03 14:22:07",
      "description": "Ozone {{upgrade}} Docker Compose environment fails if run with {{KEEP_RUNNING=true}}.  The variable is applied to both runs (pre- and post-upgrade), but pre-upgrade containers should be stopped anyway, since they will be replaced by the new ones.\r\n\r\n{code}\r\n$ cd hadoop-ozone/dist/target/ozone-1.1.0-SNAPSHOT/compose/upgrade\r\n$ KEEP_RUNNING=true ./test.sh\r\n...\r\nFailed: IO error: While lock file: scm.db/LOCK: Resource temporarily unavailable\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "upgrade docker environment does not work with KEEP_RUNNING=true"
   },
   {
      "_id": "13325887",
      "assignee": "elek",
      "components": [],
      "created": "2020-09-03 09:40:07",
      "description": "Docker image is based on the voted and approved artifacts which is availabe. We can create the image.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Publish docker image for ozone 1.0.0"
   },
   {
      "_id": "13325812",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-09-02 21:25:20",
      "description": "Add matrix build in GitHub Actions for compiling Ozone with both Java 8 and 11.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Compile Ozone with multiple Java versions"
   },
   {
      "_id": "13325809",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-09-02 20:27:32",
      "description": "{code}\r\n[INFO] Apache Hadoop HDDS Tools ........................... FAILURE\r\n...\r\n[ERROR] Failed to load existing service definition files: java.nio.file.NoSuchFileException: hadoop-hdds/tools/target/classes/META-INF/services/org.apache.hadoop.hdds.cli.SubcommandWithParent\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "jdk11"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Failed to load existing service definition files: ...SubcommandWithParent"
   },
   {
      "_id": "13325801",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-09-02 19:06:52",
      "description": "Recon should have an endpoint to proxy requests to the configured prometheus instance.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add an endpoint in Recon to query Prometheus"
   },
   {
      "_id": "13325776",
      "assignee": "elek",
      "components": [],
      "created": "2020-09-02 16:43:46",
      "description": "Ozone S3G implements the REST interface of AWS S3 protocol. Our robot test based scripts check if it's possible to use Ozone S3 with the AWS client tool.\r\n\r\nBut occasionally we should check if our robot test definitions are valid: robot tests should be executed with using real AWS endpoint and bucket(s) and all the test cases should be passed.\r\n\r\nThis patch provides a simple shell script to make this cross-check easier.  ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create a script to check AWS S3 compatibility"
   },
   {
      "_id": "13325774",
      "assignee": "elek",
      "components": [],
      "created": "2020-09-02 16:40:40",
      "description": "S3 API provides a feature to copy a specific range from an existing key.\r\n\r\nBased on the documentation, this range definitions is inclusive:\r\n\r\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part-copy.html\r\n\r\n{quote}\r\n-copy-source-range (string)\r\n\r\n    The range of bytes to copy from the source object. The range value must use the form bytes=first-last, where the first and last are the zero-based byte offsets to copy. For example, bytes=0-9 indicates that you want to copy the first 10 bytes of the source. You can copy a range only if the source object is greater than 5 MB.\r\n{quote}\r\n\r\nBut as it's visible from our [robot test|http://example.com], in our case we use exclusive range:\r\n\r\n{code}\r\nupload-part-copy ... --copy-source-range bytes=0-10485758\r\nupload-part-copy ... --copy-source-range bytes=10485758-10485760\r\n{code}\r\n\r\nBased on this AWS documentation it will return with a (10485758 + 1) + 3 bytes long key,  which is impossible if our original source key is just 10485760.\r\n\r\nI think the right usage to get the original key is the following:\r\n\r\n{code}\r\nupload-part-copy ... --copy-source-range bytes=0-10485757\r\nupload-part-copy ... --copy-source-range bytes=10485758-10485759\r\n{code}\r\n\r\n(Note, this bug is found with the script in HDDS-4194, which showed that AWS S3 is working in different way).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Range used by S3 MultipartUpload copy-from-source should be inclusive"
   },
   {
      "_id": "13325479",
      "assignee": "elek",
      "components": [],
      "created": "2020-09-01 08:36:00",
      "description": "During the teragen test it was identified that the IncrementalByteBuffer is one of the biggest bottlenecks. \r\n\r\nIn the PR of HDDS-4119 a long conversation has been started if it can be removed or we need other solution to optimize.\r\n\r\nThis jira is opened to continue the discussion and either remove or optimize the IncrementalByteByffer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable IncrementalByteBuffer by default in Ozone Client"
   },
   {
      "_id": "13325118",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-08-29 06:42:19",
      "description": "Acceptance test sometimes fails due to SCM not coming out of safe mode.  If this happens, the cluster is stopped without running Robot tests.  {{rebot}} command to process test results fails due to missing input, and acceptance check is abruptly stopped without fetching docker logs or running tests in other environments.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Acceptance test logs missing if SCM fails to exit safe mode"
   },
   {
      "_id": "13325070",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-08-28 19:05:30",
      "description": "Ozone source is checked out for _acceptance_ and _kubernetes_ checks to {{/mnt}}, outside of {{GITHUB_WORKSPACE}}, and only after _Cache_ steps.  Therefore no files are found for which hash would be computed to be included in cache keys.\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/44acf78aec6c3a4e1c5fea3a43971144c6da9a4c/.github/workflows/post-commit.yml#L167-L171}\r\n      - name: Cache for maven dependencies\r\n        uses: actions/cache@v2\r\n        with:\r\n          path: ~/.m2/repository\r\n          key: maven-repo-${{ hashFiles('**/pom.xml') }}\r\n{code}\r\n\r\nCache key is always {{maven-repo-}}:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/1042358389#step:2:10}\r\nCache restored from key: maven-repo-\r\n{code}\r\n\r\nThe same old cache is used for all builds, even if dependencies are changed in {{pom.xml}}, gradually resulting in more and more downloads during builds:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/1036271227#step:9:680}\r\n[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/info/picocli/picocli/4.4.0/picocli-4.4.0.jar (389 kB at 2.4 MB/s)\r\n[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ratis/ratis-server/1.0.0/ratis-server-1.0.0.jar (380 kB at 2.4 MB/s)\r\n[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-api/2.13.3/log4j-api-2.13.3.jar (292 kB at 1.9 MB/s)\r\n[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/ratis/ratis-proto/1.0.0/ratis-proto-1.0.0.jar (1.2 MB at 6.6 MB/s)\r\n[INFO] Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/logging/log4j/log4j-core/2.13.3/log4j-core-2.13.3.jar (1.7 MB at 8.5 MB/s)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "GitHub Actions cache does not work outside of workspace"
   },
   {
      "_id": "13324720",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-08-26 21:50:14",
      "description": "Scenario:\r\n\r\nCreate Key via S3, and Create Directory through Fs.\r\n # open key -> /a/b/c\r\n # CreateDirectory -> /a/b/c\r\n # CommitKey -> /a/b/c\r\n\r\nSo, now in Ozone we will have directory and file with name \"c\"\r\n\r\nWhen created through Fs interface.\r\n # create file -> /a/b/c\r\n # CreateDirectory -> /a/b/c\r\n # CommitKey -> /a/b/c\r\n\r\nSo, now in Ozone we will have directory and file with name \"c\"\r\n\r\n\u00a0\r\n # InitiateMPU\u00a0/a/b/c\r\n # Create Part1 /a/b/c\r\n # Commit Part1 /a/b/c\r\n # Create Directory /a/b/c\r\n # Complete MPU /a/b/c\r\n\r\nSo, now in Ozone, we will have directory and file with name \"c\".\u00a0 In MPU this is one example scenario.\r\n\r\n\u00a0\r\n\r\nFew proposals/ideas to solve this:\r\n # Check during commit whether a directory already exists with same name. But disadvantage is after user uploads the entire data during last stage we fail.\u00a0 (File system with create in progress acts similarly. Scenario: 1. vi t1 2. mkdir t1 3. Save t1: (Fail:\"t1\" is a directory)\r\n # During create directory check are there any open key creation with same name and fail.\r\n\r\n\u00a0\r\n\r\nAny of the above approaches are not final, this Jira is opened to discuss this issue and come up with solution.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Directory and filename can end up with same name in a path"
   },
   {
      "_id": "13324671",
      "assignee": "elek",
      "components": [],
      "created": "2020-08-26 15:49:06",
      "description": "Kubernetes tests are timing out sometimes. (eg. here: https://github.com/elek/ozone-build-results/tree/master/2020/08/26/2562/kubernetes)\r\n\r\nBased on the log, SCM couldn't move out from safe mode. It's either a real issue or github environment is slow sometimes.\r\n\r\nTo make it clear what is the problem I propose to increase the default timeout from 90 sec to 300 sec (5 min).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Increase default timeout in kubernetes tests"
   },
   {
      "_id": "13324646",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-08-26 12:23:55",
      "description": "_kubernetes_ check archives only Robot results.  It should also include logs from all pods, similar to compose-based acceptance tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Archive container logs for kubernetes check"
   },
   {
      "_id": "13324644",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2020-08-26 11:55:03",
      "description": "\r\nFailed on the PR:\r\nhttps://github.com/apache/hadoop-ozone/pull/1349\r\n\r\nAnd on the master:\r\n\r\nhttps://github.com/elek/ozone-build-results/blob/master/2020/08/25/2533/unit/hadoop-ozone/recon/org.apache.hadoop.ozone.recon.api.TestEndpoints.txt\r\n\r\nand here:\r\n\r\nhttps://github.com/elek/ozone-build-results/blob/master/2020/08/22/2499/unit/hadoop-ozone/recon/org.apache.hadoop.ozone.recon.api.TestEndpoints.txt",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "recon.api.TestEndpoints is flaky"
   },
   {
      "_id": "13324618",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-08-26 09:33:32",
      "description": "{{OzoneFileStatus}} should implement {{toString}} for debug purposes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Implement OzoneFileStatus#toString"
   },
   {
      "_id": "13324467",
      "assignee": "elek",
      "components": [],
      "created": "2020-08-25 12:43:33",
      "description": "s/0.6.0-SNAPSHOT/1.1.0-SNAPSHOT/g",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump version to 1.1.0-SNAPSHOT on master"
   },
   {
      "_id": "13324327",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-08-24 18:44:22",
      "description": "* Add the current layout version (MLV) to the OM Ratis request. If there is no layout version   present, we can default to '0'.\r\n* Implement Generic factory which stores different instances of Type 'T' sharded by a key & version. A single key can be associated with different versions of 'T'. This is to support a typical use case during upgrade to have multiple versions of a class / method / object and chose them based on current layout version at runtime. Before finalizing, an older version is typically needed, and after finalize, a newer version is needed.\r\n* Using the generic factory, we scan all the different OM \"write\" requests and associate them with versions.\r\n* Layout feature code refactoring. Added more comments and tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement a factory for OM Requests that returns an instance based on layout version."
   },
   {
      "_id": "13324306",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-08-24 16:26:31",
      "description": "Earlier we introduced a way to mark the inactive pull requests with \"pending\" label (with the help of /pending comment).\r\n\r\nThis pull requests introduce a new scheduled build which closes the \"pending\" pull requests after 21 days of inactivity.\r\n\r\nIMPORTANT: Only the pull requests  which are pending on the author will be closed.\r\n\r\nWe should NEVER close a pull requests which are waiting for the attention of a committer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Auto-close /pending pull requests after 21 days of inactivity"
   },
   {
      "_id": "13324290",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-08-24 14:53:26",
      "description": "Ozone 0.6.0 release is renamed to Ozone 1.0.0, but there are a few leftover references to 0.6.0, mostly in {{upgrade}} acceptance test.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Update version number in upgrade tests"
   },
   {
      "_id": "13323056",
      "assignee": "elek",
      "components": [],
      "created": "2020-08-17 13:53:48",
      "description": "[~sammichen] reported the problem that the ozone-0.6.0 branch couldn't be compiled after changing the version to 1.0.0.\r\n\r\n{code}\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:3.0.2:unpack (copy-common-html) on project hadoop-hdds-container-service: Unable to find/resolve artifact.: Could not find artifact org.apache.hadoop:hadoop-hdds-docs:jar:1.0.0 in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]\r\n[ERROR]\r\n{code}\r\n\r\nWeb components include the compiled docs (in case of hugo is on the path)\r\n\r\n{code}\r\n  <plugin>\r\n        <groupId>org.apache.maven.plugins</groupId>\r\n        <artifactId>maven-dependency-plugin</artifactId>\r\n        <executions>\r\n          <execution>\r\n            <id>copy-common-html</id>\r\n            <phase>prepare-package</phase>\r\n            <goals>\r\n              <goal>unpack</goal>\r\n            </goals>\r\n            <configuration>\r\n              <artifactItems>\r\n                <artifactItem>\r\n                  <groupId>org.apache.hadoop</groupId>\r\n                  <artifactId>hadoop-hdds-server-framework</artifactId>\r\n                  <outputDirectory>${project.build.outputDirectory}\r\n                  </outputDirectory>\r\n                  <includes>webapps/static/**/*.*</includes>\r\n                </artifactItem>\r\n                <artifactItem>\r\n                  <groupId>org.apache.hadoop</groupId>\r\n                  <artifactId>hadoop-hdds-docs</artifactId>\r\n                  <outputDirectory>${project.build.outputDirectory}/webapps/ozoneManager</outputDirectory>\r\n                  <includes>docs/**/*.*</includes>\r\n                </artifactItem>\r\n              </artifactItems>\r\n              <overWriteSnapshots>true</overWriteSnapshots>\r\n            </configuration>\r\n          </execution>\r\n        </executions>\r\n      </plugin>\r\n{code}\r\n\r\nBut the explicit dependency between the container-service and hdds-docs accidentally missing. With adding a provided dependency, it can be fixed (maven will compile hdds-docs first)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Components with web interface should depend on hdds-docs"
   },
   {
      "_id": "13322745",
      "assignee": "elek",
      "components": [],
      "created": "2020-08-14 12:09:55",
      "description": "Teragen reported to be slow with low number of mappers compared to HDFS.\r\n\r\nIn my test (one pipeline, 3 yarn nodes) 10 g teragen with HDFS was ~3 mins but with Ozone it was 6 mins. It could be fixed with using more mappers, but when I investigated the execution I found a few problems reagrding to the BufferPool management.\r\n\r\n 1. IncrementalChunkBuffer is slow and it might not be required as BufferPool itself is incremental\r\n 2. For each write operation the bufferPool.allocateBufferIfNeeded is called which can be a slow operation (positions should be calculated).\r\n 3. There is no explicit support for write(byte) operations\r\n\r\nIn the flamegraph it's clearly visible that with low number of mappers the client is busy with buffer operations. After the patch the rpc call and the checksum calculation give the majority of the time. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Improve performance of the BufferPool management of Ozone client"
   },
   {
      "_id": "13322723",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-08-14 09:39:40",
      "description": "Kubernetes example tests currently only run on local cluster.  With a minor change we can make the tests work on k3d, too.\r\n\r\nbq. [k3d|https://k3d.io/] makes it very easy to create single- and multi-node k3s clusters in docker, e.g. for local development on Kubernetes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allow running Kubernetes example tests on k3d"
   },
   {
      "_id": "13322617",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-08-13 21:58:47",
      "description": "When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.\r\n\r\nWhen listKeys uses given keyName(not normalized key path) as prefix and Starkey the list-keys will return empty result.\r\n\r\nSimilar to HDDS-4102, we should normalize startKey and keyPrefix.\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": " Normalize Keypath for listKeys."
   },
   {
      "_id": "13322494",
      "assignee": "elek",
      "components": [],
      "created": "2020-08-13 08:47:18",
      "description": "There are bunch of bugfixes and improvements since the used 2.10:\r\n\r\nhttps://logging.apache.org/log4j/2.x/changes-report.html#a2.13.3",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump log4j2 version"
   },
   {
      "_id": "13322090",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-08-11 18:02:07",
      "description": "When ozone.om.enable.filesystem.paths, OM normalizes path, and stores the Keyname.\r\n\r\nNow when user tries to read the file from S3 using the keyName which user has used to create the Key, it will return error KEY_NOT_FOUND\r\n\r\nThe issue is, lookupKey need to normalize path, when ozone.om.enable.filesystem.paths is enabled. This is common API used by S3/FS. \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Normalize Keypath for lookupKey"
   },
   {
      "_id": "13321868",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-08-10 19:53:31",
      "description": "This Jira is to implement changes required to use Ozone buckets when data is ingested via S3 and use the bucket/volume via OzoneFileSystem. Initial implementation for this is done as part of HDDS-3955. There are few API's which have missed the changes during the implementation of HDDS-3955. \r\n\r\nAttached design document which discusses each API,  and what changes are required.\r\n\r\nExcel sheet has information about each API, from what all interfaces the OM API is used, and what changes are required for the API to support inter-operability.\r\n\r\nNote: The proposal for delete/rename is still under discussion, not yet finalized. \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3/Ozone Filesystem inter-op"
   },
   {
      "_id": "13321725",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-08-10 04:28:42",
      "description": "I am testing writing delta, OSS not databricks, data to Ozone FS since my company is looking to replace Hadoop if feasible. However, whenever I write delta table, the parquet files are writing, the\u00a0delta log directory is created, but the json is never writing.\u00a0\r\n\r\nI am using the spark operator to submit a batch test job to write about 5mb of data.\r\n\r\nNeither on the driver nor on the executor is there an error. The driver never finishes since the creation of the json hangs.\r\n\r\n\u00a0\r\n\r\nCode I used for testing spark operator and then I ran the pieces in the shell for testing. In the save path, update bucket and volume info for your data store.\r\n{code:java}\r\npackage app.OzoneTest\r\n\r\nimport org.apache.spark.sql.{DataFrame, SparkSession}\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types.{BinaryType, StringType}\r\n\r\nobject CreateData {\r\n\r\n  def main(args: Array[String]): Unit = {\r\n\r\n    val spark: SparkSession = SparkSession\r\n      .builder()\r\n      .appName(s\"Create Ozone Mock Data\")\r\n      .enableHiveSupport()\r\n      .getOrCreate()\r\n\r\n    import spark.implicits._\r\n\r\n    val df: DataFrame = Seq.fill(100000)\r\n    {(randomID, randomLat, randomLong, randomDates, randomHour)}\r\n      .toDF(\"msisdn\", \"latitude\", \"longitude\", \"par_day\", \"par_hour\")\r\n      .withColumn(\"msisdn\", $\"msisdn\".cast(StringType))\r\n      .withColumn(\"msisdn\", sha1($\"msisdn\".cast(BinaryType)))\r\n      .select(\"msisdn\", \"latitude\", \"longitude\", \"par_day\", \"par_hour\")\r\n\r\n    df\r\n      .repartition(3, $\"msisdn\")\r\n      .sortWithinPartitions(\"latitude\", \"longitude\")\r\n      .write\r\n      .partitionBy(\"par_day\", \"par_hour\")\r\n      .format(\"delta\")\r\n      .save(\"o3fs://your_bucker.your_volume/location_data\")\r\n\r\n  }\r\n\r\n  def randomID: Int = scala.util.Random.nextInt(10) + 1\r\n\r\n  def randomDates: Int = 20200101 + scala.util.Random.nextInt((20200131 - 20200101) + 1)\r\n\r\n  def randomHour: Int = scala.util.Random.nextInt(24)\r\n\r\n  def randomLat: Double = 13.5 + scala.util.Random.nextFloat()\r\n\r\n  def randomLong: Double = 100 + scala.util.Random.nextFloat()\r\n}\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "delta",
         "filesystem",
         "scala",
         "spark"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Writing delta to Ozone hangs when creating the _delta_log json"
   },
   {
      "_id": "13321567",
      "assignee": "xyao",
      "components": [],
      "created": "2020-08-07 17:26:41",
      "description": "Currently external authorizer does not have the owner info of the volume bucket keys when authorizing requests. Explicit rules/policies must be set before volume/bucket/key creation is allowed even for owner themselves. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Adding Owner info for Authorizer plugin to honor owner access rights"
   },
   {
      "_id": "13321518",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-08-07 13:40:29",
      "description": "OzoneFileSystem does not record some of the operations that are defined in [Statistic|https://github.com/apache/hadoop-ozone/blob/d7ea4966656cfdb0b53a368eac52d71adb717104/hadoop-ozone/ozonefs-common/src/main/java/org/apache/hadoop/fs/ozone/Statistic.java#L44-L75].",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Incomplete OzoneFileSystem statistics"
   },
   {
      "_id": "13321378",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-08-06 20:16:09",
      "description": "If a client attempts a request on an OM which has not caught up with leader OM and hence does have the delegation token, the request could fail with AccessControlException without trying it on other OMs.\r\nOn AccessControlException, all OMs must be tried once before the request is failed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Retry request on different OM on AccessControlException"
   },
   {
      "_id": "13321332",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-08-06 16:13:18",
      "description": "An unused Robot test ({{robot.robot}}) was accidentally added in HDDS-3612.  It was refactored to separate {{string_tests.robot}} and {{fs_tests.robot}}, but the original file was not removed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Remove leftover robot.robot"
   },
   {
      "_id": "13321197",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-08-06 00:25:49",
      "description": "Right now retry logic on client to OM is, it will try connect to OM1, if it is leader fine, else try with next OM and so on. If OM1, is down, client retries for 50 times when ipc.client.connect.max.retries is set to 50 and ipc.client.connect.retry.interval default to 1sec, so a total of 50seconds is spent in retry and then move to next OM. \r\nI think here client -> OM should have its own retry policy, in this way if the first OM is down, to complete request, the user does not need to wait for 50sec.\r\n\r\nAs ipc.client.connect.retry.interval and ipc.client.connect.max.retries  are common configurations for RPC, creating a new default retry policy with smaller values would be nice. \r\n\r\n\r\n\r\n{code:java}\r\n20/08/06 00:21:29 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)\r\n20/08/06 00:21:30 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)\r\n20/08/06 00:21:31 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)\r\n20/08/06 00:21:32 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)\r\n20/08/06 00:21:33 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)\r\n20/08/06 00:21:34 INFO ipc.Client: Retrying connect to server: bv-oz-2.bv-oz.root.hwx.site/172.27.23.204:9862. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=50, sleepTime=1000 MILLISECONDS)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Client should not retry same OM on network connection failure"
   },
   {
      "_id": "13320980",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-08-05 00:26:44",
      "description": "OzoneManagerStateMachine#notifyInstallSnapshotFromLeader() checks the incoming roleInfoProto and proceeds with install snapshot request only if the role is Leader. This check is wrong and the roleInfoProto will contain the self node ID and not the leaders.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix InstallSnapshot in OM HA"
   },
   {
      "_id": "13320596",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-08-03 05:59:25",
      "description": "* {{AtomicBoolean isStopped}} should be {{final}}, not {{volatile}}, since the reference is not being changed\r\n* {{stop()}} should use atomic {{getAndSet()}} instead of {{get()}} followed by {{set()}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Wrong use of AtomicBoolean in HddsDatanodeService"
   },
   {
      "_id": "13320552",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-08-02 16:57:19",
      "description": "Acceptance test run is stopped after any failed test, and its logs are missing from the bundle.\r\n\r\n{code:title=misc suite}\r\nhadoop27\r\nhadoop31\r\nhadoop32\r\nozone-csi\r\nozone-mr\r\nozone-om-ha-s3\r\nozone-topology\r\nozones3-haproxy\r\nozonesecure-mr\r\nozonesecure-om-ha\r\nupgrade\r\n{code}\r\n\r\n* {{ozone-topology}} failed in [this run|https://github.com/apache/hadoop-ozone/runs/927545620], {{ozones3-haproxy}} and subsequent tests were skipped, {{ozone-topology}} robot log is missing from [bundle|https://github.com/apache/hadoop-ozone/suites/984510888/artifacts/12629978]\r\n* {{ozone-secure}} failed in [this run|https://github.com/adoroszlai/hadoop-ozone/runs/933098576], it is the only test in {{secure}} suite, so {{acceptance-secure.zip}} was not even created",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Failed acceptance test missing from bundle"
   },
   {
      "_id": "13320400",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-07-31 12:41:03",
      "description": "Cleanup GitHub workflow definition:\r\n\r\n# Provide name for all steps\r\n# Apply HDDS-4038 to new {{bats}} and {{kubernetes}} checks\r\n# Apply HDDS-3877 to new {{bats}} check\r\n# Fix mixed use of 2, 3, 4 spaces indentation\r\n# Remove unnecessary job names (where job name and ID are the same)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cleanup GitHub workflow"
   },
   {
      "_id": "13319987",
      "assignee": "elek",
      "components": [],
      "created": "2020-07-29 14:40:45",
      "description": "We have separated rat ignore list for Hdds and Ozone projects but some files (which are ignored by .gitignore) not added to there. It's not a problem on github as rat is executed on a clean repo, but locally it can make the execution easier (exclude files which are already ignored).\r\n\r\nRats can read ignore list from VCS ignore file, unfortunately only from directory of the current project not from the root of the git repository.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Add more ignore rules to the RAT ignore list"
   },
   {
      "_id": "13319822",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2020-07-28 18:54:07",
      "description": "HDDS-3612 introduced bucket links.\r\nAfter this feature now we don't need this parameter, any volume/bucket can be exposed to S3 via using bucket links.\r\n\r\nozone bucket link srcvol/srcbucket destvol/destbucket\r\n\r\nSo now to expose any ozone bucket to S3G\r\n\r\nFor example, the user wants to expose a bucket named bucket1 under volume1 to S3G, they can run below command\r\n\r\n{code:java}\r\nozone bucket link volume1/bucket1 s3v/bucket2\r\n{code}\r\n\r\nNow, the user can access all the keys in volume/bucket1 using s3v/bucket2 and also ingest data to the volume/bucket1 using using s3v/bucket2\r\n\r\nThis Jira is opened to remove the config from ozone-default.xml\r\nAnd also log a warning message to use bucket links, when it does not have default value s3v.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Deprecate ozone.s3g.volume.name"
   },
   {
      "_id": "13319704",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-07-28 08:33:37",
      "description": "HDDS-3413 is opened to add OM HA related documentation to the Ozone docs but it turned out that it contains additional out-of-date (and missing) information.\r\n\r\nThis issue is opened to track a big documentation update.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Update documentation for the GA release"
   },
   {
      "_id": "13319693",
      "assignee": "xyao",
      "components": [],
      "created": "2020-07-28 07:46:55",
      "description": "{code}\r\ncurl  -k --negotiate -X GET -u : \"https://quasar-jsajkc-8.quasar-jsajkc.root.hwx.site:9877/conf\"\r\n<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\r\n<title>Error 403 GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</title>\r\n</head>\r\n<body><h2>HTTP ERROR 403 GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</h2>\r\n<table>\r\n<tr><th>URI:</th><td>/conf</td></tr>\r\n<tr><th>STATUS:</th><td>403</td></tr>\r\n<tr><th>MESSAGE:</th><td>GSSException: Failure unspecified at GSS-API level (Mechanism level: Request is a replay (34))</td></tr>\r\n<tr><th>SERVLET:</th><td>conf</td></tr>\r\n</table>\r\n\r\n</body>\r\n</html>\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone /conf endpoint triggers kerberos replay error when SPNEGO is enabled "
   },
   {
      "_id": "13319606",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-07-27 19:42:44",
      "description": "GitHub Actions warns that \r\n\r\nbq. The \"master\" branch is no longer the default branch name for actions/upload-artifact\r\n\r\nand similarly for actions/checkout.\r\n\r\nSo we should use specific version instead of master.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Eliminate GitHub check warnings"
   },
   {
      "_id": "13319511",
      "assignee": "elek",
      "components": [],
      "created": "2020-07-27 09:44:20",
      "description": "Acceptance test reports of today  uses a generated name for each of the executed robot tests.\r\n\r\nInstead of using a flat structure with generated name it seems to be better to use a hierarchical structure which represents the directory structure.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make the acceptance test reports hierarchical"
   },
   {
      "_id": "13319495",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-07-27 08:03:52",
      "description": "{{author.sh}} can be run in CI without docker container, since it's mostly just a recursive grep.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Run author check without docker"
   },
   {
      "_id": "13319473",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-07-27 06:35:28",
      "description": "Shell scripts can be tested using [bats|https://github.com/bats-core/bats-core].  Such tests should be run as part of CI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Run shell tests in CI"
   },
   {
      "_id": "13319221",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-07-24 19:56:33",
      "description": "{code}\r\n2020-07-24 19:56:11,777 INFO org.apache.hadoop.ozone.recon.scm.ReconContainerManager: Exception while adding container #1 .\r\njava.io.IOException: Pipeline PipelineID=ccfb3a54-848c-4ed2-91bf-a174267e3435 not found. Cannot add container #1\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.addNewContainer(ReconContainerManager.java:119)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.checkAndAddNewContainer(ReconContainerManager.java:92)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:62)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:38)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n2020-07-24 19:56:11,777 ERROR org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler: Exception while checking and adding new container.\r\norg.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=06bf6a83-9afb-4477-b18d-de4c6556ce4b not found\r\n        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.removeContainerFromPipeline(PipelineStateMap.java:372)\r\n        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.removeContainerFromPipeline(PipelineStateManager.java:111)\r\n        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removeContainerFromPipeline(SCMPipelineManager.java:350)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.addNewContainer(ReconContainerManager.java:126)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerManager.checkAndAddNewContainer(ReconContainerManager.java:92)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:62)\r\n        at org.apache.hadoop.ozone.recon.scm.ReconContainerReportHandler.onMessage(ReconContainerReportHandler.java:38)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Recon unable to add a new container which is in CLOSED state."
   },
   {
      "_id": "13319194",
      "assignee": "xyao",
      "components": [],
      "created": "2020-07-24 16:43:47",
      "description": "This gives false negative errors and can flood SCM log. \r\n\r\n\r\n{code:java}\r\nscm_1       | 2020-07-24 16:39:51,756 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor ONE. Exception: Cannot create pipeline of factor 1 using 0 nodes. Used 3 nodes. Healthy nodes 3\r\nscm_1       | 2020-07-24 16:39:51,757 [RatisPipelineUtilsThread] ERROR pipeline.SCMPipelineManager: Failed to create pipeline of type RATIS and factor THREE. Exception: Pipeline creation failed because nodes are engaged in other pipelines and every node can only be engaged in max 2 pipelines. Required 3. Found 0\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Suppress ERROR message when SCM attempt to create additional pipelines"
   },
   {
      "_id": "13319173",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-24 13:24:20",
      "description": "Sets\u00a0ozone.om.enable.filesystem.paths=true, then starts the Ozone cluster.\r\n{code:java}\r\n[root~]$ ozone fs -mkdir o3fs://bucket2.vol2.ozone1/subdir2\r\n[root~]$ ozone fs -mv o3fs://bucket2.vol2.ozone1/subdir2 o3fs://bucket2.vol2.ozone1/subdir2-renamedmv: Key not found /vol2/bucket2/subdir2\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true"
   },
   {
      "_id": "13319135",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-07-24 09:52:22",
      "description": "Add acceptance test to create a key in encrypted bucket.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add test for creating encrypted key"
   },
   {
      "_id": "13319052",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2020-07-23 22:16:00",
      "description": "Ozone s3 API returns 400 Bad Request for head-bucket for non-existing bucket.\r\n\r\nhrt_qa$ aws s3api  --ca-bundle=/usr/local/share/ca-certificates/ca.crt --endpoint https://s3g:9879/  head-bucket --bucket fsdghj\r\n\r\nAn error occurred (400) when calling the HeadBucket operation: Bad Request\r\n\r\nIt should return 404 as per AWS documentation:\r\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/head-bucket.html\r\n\r\nA client error (404) occurred when calling the HeadBucket operation: Not Found ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Ozone s3 API return 400 Bad Request for head-bucket for non existing bucket"
   },
   {
      "_id": "13319046",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-07-23 21:35:06",
      "description": "* ReconNodeManager uses node db in an old format which is not part of ReconDBDefinition. Move the definition to ReconDBDefinition.\r\n* Create DB Definition for Recon Container DB.\r\n* Modify DBScanner tool to allow it to read Recon DBs. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Organize Recon DBs into a 'DBDefinition'."
   },
   {
      "_id": "13318998",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-07-23 16:49:15",
      "description": "Currently, the getacl and setacl commands return wrong information when an external authorizer such as Ranger is enabled. There should be a check to verify if Native Authorizer is enabled before returning any response for these two commands.\r\n\r\nIf an external authorizer is enabled, it should show a nice message about managing acls in external authorizer.\u00a0\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ACL commands like getacl and setacl should return a response only when Native Authorizer is enabled"
   },
   {
      "_id": "13318928",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-07-23 12:29:55",
      "description": "{code}\r\ndatanode_1  | 2020-07-22 13:11:47,845 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 2 seconds.\r\ndatanode_1  | 2020-07-22 13:11:47,846 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.\r\ndatanode_1  | java.lang.NullPointerException\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)\r\ndatanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)\r\ndatanode_1  | 2020-07-22 13:11:47,847 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.\r\ndatanode_1  | java.lang.NullPointerException\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)\r\ndatanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)\r\ndatanode_1  | 2020-07-22 13:11:47,848 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.\r\ndatanode_1  | java.lang.NullPointerException\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)\r\ndatanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)\r\ndatanode_1  | 2020-07-22 13:11:47,848 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.\r\ndatanode_1  | java.lang.NullPointerException\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)\r\ndatanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)\r\ndatanode_1  | 2020-07-22 13:11:47,851 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.\r\ndatanode_1  | java.lang.NullPointerException\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)\r\ndatanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)\r\n...\r\nthread in pool for past 22 seconds.\r\ndatanode_1  | 2020-07-22 13:11:47,854 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine: Unable to finish the execution.\r\ndatanode_1  | java.lang.NullPointerException\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:218)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:451)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:225)\r\ndatanode_1  |   at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:396)\r\ndatanode_1  |   at java.base/java.lang.Thread.run(Thread.java:834)\r\n{code}\r\n\r\nThis increases acceptance test logs to several hundred MBs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Datanode log spammed by NPE"
   },
   {
      "_id": "13318926",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-07-23 12:26:42",
      "description": "For push builds, acceptance check may build and test a different commit than the one that was pushed.\r\n\r\nThe check for [HDDS-3991|https://github.com/apache/hadoop-ozone/commit/404ec6d0725cfe9c80aa912f150c6474037b10bb] built [HDDS-3933|https://github.com/apache/hadoop-ozone/commit/ff7b5a3367eccc0969bfd92a2cafe48899a2aaa5]:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/898449998#step:4:30}\r\nHEAD is now at ff7b5a336 HDDS-3933. Fix memory leak because of too many Datanode State Machine Thread (#1185)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Acceptance check may run against wrong commit"
   },
   {
      "_id": "13318805",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-07-23 01:53:13",
      "description": "[INFO] Running org.apache.hadoop.ozone.client.rpc.TestWatchForCommit\r\n[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 211.911 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestWatchForCommit\r\n[ERROR] testWatchForCommitForGroupMismatchException(org.apache.hadoop.ozone.client.rpc.TestWatchForCommit)  Time elapsed: 38.862 s  <<< ERROR!\r\njava.io.IOException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.\r\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:740)\r\n\tat org.apache.hadoop.ozone.container.TestHelper.waitForPipelineClose(TestHelper.java:220)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForGroupMismatchException(TestWatchForCommit.java:344)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\nat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\nCaused by: org.apache.ratis.protocol.GroupMismatchException: 3ebb4735-6541-4db2-ae37-b2d193544ce0: Group group-29B91FB82A4C not found.\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupRemoveAsync(RaftServerProxy.java:414)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupManagementAsync(RaftServerProxy.java:372)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.groupManagement(RaftServerProxy.java:355)\r\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.removeGroup(XceiverServerRatis.java:738)\r\n\t... 29 more",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "FLAKY-UT: TestWatchForCommit#testWatchForCommitForGroupMismatchException"
   },
   {
      "_id": "13318795",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-22 23:34:57",
      "description": "HDDS-3993 created volume required for S3G during the OM startup.\r\nSo, remove the step that s3v volume needs to be created.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update S3 related documentation"
   },
   {
      "_id": "13318794",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-22 23:26:30",
      "description": "This Jira is to fix this TODO.\r\n\r\nOzoneServiceProvider.java L59:\r\n{code:java}\r\n      // HA cluster.\r\n      //For now if multiple service id's are configured we throw exception.\r\n      // As if multiple service id's are configured, S3Gateway will not be\r\n      // knowing which one to talk to. In future, if OM federation is supported\r\n      // we can resolve this by having another property like\r\n      // ozone.om.internal.service.id.\r\n      // TODO: Revisit this later.\r\n      if (serviceIdList.size() > 1) {\r\n        throw new IllegalArgumentException(\"Multiple serviceIds are \" +\r\n            \"configured. \" + Arrays.toString(serviceIdList.toArray()));\r\n{code}\r\n\r\n      ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3G startup fails when multiple service ids are configured."
   },
   {
      "_id": "13318790",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-07-22 23:03:45",
      "description": "The counts shown in the overview page are not accurate due to the usage of\u00a0\"rocksdb.estimate-num-keys\" to get the counts. Instead, keep track of accurate counts by updating the counter in a global table every time an event is triggered via FileSizeCount Task in Recon.\u00a0\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon Overview page: The volume, bucket and key counts are not accurate"
   },
   {
      "_id": "13318770",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-07-22 19:42:15",
      "description": "Recon connects to OM via RPC using the \"ozone.om.internal.service.id\" to get updates. If the above config is not defined, but the ozone.om.service.ids is defined, Recon should use the latter as a fallback. Currently, a single Recon instance supports only 1 OM HA cluster at a time. Hence, if multiple ids are defined, Recon will pick the first.\r\n\r\nThanks to [~vivekratnavel] for reporting the issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Recon should fallback to ozone.om.service.ids when the internal service id is not defined."
   },
   {
      "_id": "13318768",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-07-22 19:26:38",
      "description": "This Jira is to generate FileEncryption for a key outside the bucket lock.\r\nAs right now, we hold the lock when making a network call to KMS to obtain encryption info.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Generate encryption info for the bucket outside bucket lock"
   },
   {
      "_id": "13318759",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-22 18:22:03",
      "description": "With HDDS-3612 buckets created via ozone are also accessible via S3.\r\nThis has caused a problem when the bucket is encrypted, the keys are not encrypted on disk.\r\n\r\n*2 Issues:*\r\n1. On OM, for each part a new encryption info is generated. During complete Multipart upload, the encryption info is not stored in KeyInfo.\r\n2. On the client, for part upload, the encryption info is silently ignored.\r\n\r\nIf we don't throw an error, on an encrypted bucket, key data is not encrypted on disks.\r\nFor 0.6.0 release, we can mark this as not supported, and this will be fixed in next release by HDDS-4005\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Disallow MPU on encrypted buckets."
   },
   {
      "_id": "13318619",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-07-22 06:07:54",
      "description": "CI checks run integration tests in parallel using GitHub Actions matrix build.  I propose to use the same approach for acceptance tests, which are currently run in a single check, taking about 1.5 hours.\r\n\r\nAlso, some of the integration test splits could be combined to reduce the number of checks, as we have a limit on parallel actions.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Split acceptance tests to reduce CI feedback time"
   },
   {
      "_id": "13318602",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-07-22 04:17:39",
      "description": "The name of {{hadoop-ozone-filesystem-hadoopX}} modules is so long that Maven output looks ugly:\r\n\r\n{code}\r\n...\r\n[INFO] Apache Hadoop Ozone Insight Tool ................... SUCCESS [  3.940 s]\r\n[INFO] Apache Hadoop Ozone FileSystem Shaded .............. SUCCESS [02:28 min]\r\n[INFO] Apache Hadoop Ozone FileSystem Hadoop 2.x compatibility SUCCESS [ 14.397 s]\r\n[INFO] Apache Hadoop Ozone FileSystem Hadoop 3.x compatibility SUCCESS [ 13.095 s]\r\n[INFO] Apache Hadoop Ozone Distribution ................... SUCCESS [ 12.380 s]\r\n[INFO] Apache Hadoop Ozone Fault Injection Tests .......... SUCCESS [  0.725 s]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Shorten Ozone FS Hadoop compatibility module names"
   },
   {
      "_id": "13318585",
      "assignee": "xyao",
      "components": [],
      "created": "2020-07-22 00:17:51",
      "description": "Current Ozone certificate are good for sign/verify tokens but can't do SSL handshake. \r\n\r\nHere are a few missing pieces: \r\n1. Caused by: java.security.cert.CertificateException: No subject alternative names present\r\n        at java.base/sun.security.util.HostnameChecker.matchIP(HostnameChecker.java:137)\r\n\r\n2. Caused by: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures\r\n        at java.base/sun.security.validator.EndEntityChecker.checkTLSServer(EndEntityChecker.java:278)\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone certificate needs additional flags and SAN extension for GRPC TLS."
   },
   {
      "_id": "13318582",
      "assignee": "xyao",
      "components": [],
      "created": "2020-07-22 00:14:19",
      "description": "As a result, when ozone.grpc.tls.enabled, RATIS THREE pipeline will not work as DN failed in SSL handshaking without the TLS configuration. \r\n\r\n\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Missing TLS client configurations to allow ozone.grpc.tls.enabled."
   },
   {
      "_id": "13318363",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-21 00:10:14",
      "description": "Create volume required for S3G operations during OM startup",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create volume required for S3G during OM startup"
   },
   {
      "_id": "13318225",
      "assignee": "elek",
      "components": [],
      "created": "2020-07-20 10:36:44",
      "description": "HDDS-3595 introduced a new build time check to make sure protobuf files are changed only with backward compatible way. A lock file which contains the structure of the proto files are committed to the repository and during the build the new state of the proto files are compared with the committed version.\r\n\r\nUnfortunately the plugin always generate a new lock file, even if it should be updated only after the releases. To make it more safe (and less confusing) I suggest putting the lock files to the git ignore list",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ignore protobuf lock files"
   },
   {
      "_id": "13318220",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335609",
            "id": "12335609",
            "name": "kubernetes"
         }
      ],
      "created": "2020-07-20 10:20:40",
      "description": "hadoop-ozone/dist/src/main/k8s/example directory contains example kubernetes resources to start Ozone in kubernetes environment. To make sure those resources are working and up-to-date I propose to test them during standard build.\r\n\r\nK3s project provides a lightweight Kubernetes distribution which can be installed easily in Github Actions environment and kubernetes based clusters can be tested.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Test Kubernetes examples with acceptance tests"
   },
   {
      "_id": "13318171",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-07-20 06:22:27",
      "description": "Bucket creation with encrypted key fails.\r\n\r\nSteps:\r\n\r\n# Created encryption key\r\n# Created volume\r\n# Tried to create bucket with encryption key\r\n\r\nResult:\r\n\r\n{code}\r\nINVALID_REQUEST Encryption cannot be set for bucket links\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Encrypted bucket creation failed with INVALID_REQUEST Encryption cannot be set for bucket links"
   },
   {
      "_id": "13318156",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-07-20 05:01:55",
      "description": "{code}\r\nTests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 84.876 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestCommitWatcher\r\ntestReleaseBuffersOnException(org.apache.hadoop.ozone.client.rpc.TestCommitWatcher)  Time elapsed: 44.888 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n        at org.junit.Assert.fail(Assert.java:86)\r\n        at org.junit.Assert.assertTrue(Assert.java:41)\r\n        at org.junit.Assert.assertTrue(Assert.java:52)\r\n        at org.apache.hadoop.ozone.client.rpc.TestCommitWatcher.testReleaseBuffersOnException(TestCommitWatcher.java:320)\r\n{code}\r\n\r\nhttps://github.com/elek/ozone-build-results/tree/master/2020/07/20/1837/it-client\r\nhttps://github.com/elek/ozone-build-results/tree/master/2020/07/19/1830/it-client\r\nhttps://github.com/apache/hadoop-ozone/runs/885336971?check_suite_focus=true\r\nhttps://github.com/elek/ozone-build-results/tree/master/2020/07/17/1811/it-client\r\nhttps://github.com/apache/hadoop-ozone/runs/880734025?check_suite_focus=true",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Frequent failure in TestCommitWatcher#testReleaseBuffersOnException"
   },
   {
      "_id": "13317592",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-07-19 05:03:33",
      "description": "HDDS-3807 and HDDS-3612 introduced new additions to proto files but failed to update proto.lock files.\u00a0\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update proto.lock files"
   },
   {
      "_id": "13317177",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-07-16 14:22:52",
      "description": "Change parameter and return type of time-related config methods in {{RatisClientConfig}} to {{Duration}}.  This results in more readable parameter values and type safety.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use Duration for time in RatisClientConfig"
   },
   {
      "_id": "13317055",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-16 01:21:44",
      "description": "This jira is to validate KeyNames which are created with OzoneFileSystem.\r\nSimilar to how hdfs handles using DFSUtil. isValidName()",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Validate KeyNames created in FileSystem requests."
   },
   {
      "_id": "13317036",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-15 22:14:13",
      "description": "*[root@bv-cml-1 ~]# ozone debug ldb --db=/var/lib/hadoop-ozone/om/data/om.db/ scan --column_family=transactionInfoTable\r\nTable with specified name does not exist*\r\n\r\nThis is because DBDefinition is missing transactionInfo table.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "LDB scan fails to read from transactionInfoTable"
   },
   {
      "_id": "13316926",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-07-15 13:03:35",
      "description": "HDDS-3821 [accidentally|https://github.com/apache/hadoop-ozone/pull/1101#issuecomment-658750232] set some [log level to DEBUG|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/resources/log4j.properties#L23-L24] for integration tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Remove leftover debug setting"
   },
   {
      "_id": "13316909",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-07-15 12:19:33",
      "description": "SCM LOG\uff1a\r\n{code}\r\n2020-07-15 19:25:09,768 [main] ERROR org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SCM start failed with exception\r\njava.io.IOException: Duplicate pipeline ID PipelineID=db5966ec-140f-48d8-b0d6-e6f2ff777a77 detected.\r\n        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.addPipeline(PipelineStateMap.java:89)\r\n        at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(PipelineStateManager.java:53)\r\n        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.initializePipelineState(SCMPipelineManager.java:165)\r\n        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.<init>(SCMPipelineManager.java:100)\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:410)\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:281)\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:213)\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:624)\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:144)\r\n        at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:119)\r\n\r\n\r\nRocksDB dump, string,\r\nrocksdb_ldb --db=scm.db scan --column_family=pipelines\r\n\r\n$db5966ec-140f-48d8-b0d6-e6f2ff777a77\u0611????\u066c??????\u07b9? : \r\n?\r\n$02d3c9b4-7972-4471-a520-fff108b8d32e\r\n                                     10.73.33.62\r\n                                                10.73.33.62\"\r\n\r\nRATIS?M\"\r\n\r\n/default-rack???\u01f6?????\u0150???? *?71-a520-fff108b8d32e:\r\n$db5966ec-140f-48d8-b0d6-e6f2ff777a77\u0611????\u066c??????\u07b9?2\r\n?Yf?H\u0630????wzw : \r\n?\r\n$02d3c9b4-7972-4471-a520-fff108b8d32e\r\n                                     10.73.33.62\r\n                                                10.73.33.62\"\r\n\r\nRATIS?M\"\r\n\r\nHEX:\r\n0x0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB001 : 0x0AAA010A2430326433633962342D373937322D343437312D613532302D666666313038623864333265120B31302E37332E33332E36321A0B31302E37332E33332E3632220A0A05524154495310824D220F0A0A5354414E44414C4F4E4510834D322430326433633962342D373937322D343437312D613532302D6666663130386238643332653A0D2F64656661756C742D7261636BA2061508F188C9CBC7B6F2E90210AEA6E3C590FEBF90A5011001180120012A3F0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB00132004085A7C1E5B42E\r\n0xDB5966EC140F48D8B0D6E6F2FF777A77 : 0x0AAC010A2430326433633962342D373937322D343437312D613532302D666666313038623864333265120B31302E37332E33332E36321A0B31302E37332E33332E3632220A0A05524154495310824D220F0A0A5354414E44414C4F4E4510834D322430326433633962342D373937322D343437312D613532302D6666663130386238643332653A0D2F64656661756C742D7261636B4800A2061508F188C9CBC7B6F2E90210AEA6E3C590FEBF90A5011001180120012A3F0A2464623539363665632D313430662D343864382D623064362D653666326666373737613737A2061608D891BDA0C1DDD9ACDB0110F7F4DDFBAFDEB9EBB0013200409DFCAF8BB52E\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "upgrade-p0"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "SCM failed to start up for duplicated pipeline detected"
   },
   {
      "_id": "13316901",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-07-15 11:39:47",
      "description": "Some of the Ratis configurations in integration tests are not applied due to mismatch in config keys.\r\n # [Ratis|https://github.com/apache/incubator-ratis/blob/master/ratis-client/src/main/java/org/apache/ratis/client/RaftClientConfigKeys.java#L41-L53]: {{raft.client.rpc.watch.request.timeout}}\r\n [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestCommitWatcher.java#L119-L122]: {{raft.client.watch.request.timeout}}\r\n # [Ratis|https://github.com/apache/incubator-ratis/blob/4db4f804aa90f9900cda08c79b54a45f80f4213b/ratis-server/src/main/java/org/apache/ratis/server/RaftServerConfigKeys.java#L470-L473]: {{raft.server.notification.no-leader.timeout}}\r\n [Ozone|https://github.com/apache/hadoop-ozone/blob/926048403d115ddcb59ff130e5c46e518874b8aa/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/conf/DatanodeRatisServerConfig.java#L42]: {{raft.server.Notification.no-leader.timeout}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ratis config key mismatch"
   },
   {
      "_id": "13316718",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-07-14 14:46:23",
      "description": "{{PipelineID}} was recently changed to have integer-based ID in addition to the string ID.  Now log messages including {{PipelineID}} span multiple lines:\r\n\r\n{code:title=https://github.com/elek/ozone-build-results/blob/92d31c9b58065b37a371c71c97b346f99163318d/2020/07/11/1626/acceptance/docker-ozone-ozone-freon-scm.log#L218-L223}\r\ndatanode_1  | 2020-07-11 13:07:00,540 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE #id: \"8101dcbf-1a28-4f20-863a-0616b4e4bc4b\"\r\ndatanode_1  | uuid128 {\r\ndatanode_1  |   mostSigBits: -9150790254504423648\r\ndatanode_1  |   leastSigBits: -8774694229384053685\r\ndatanode_1  | }\r\ndatanode_1  | .\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid HddsProtos.PipelineID#toString"
   },
   {
      "_id": "13316677",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-07-14 11:19:17",
      "description": "Recon acceptance test failed with:\r\n\r\n{code}\r\nCheck if Recon picks up OM data                                       | FAIL |\r\n...\r\n...,{\"volume\":\"vol-0-40306\",\"bucket\":\"bucket-0-15468\",\"fileSize\":204\r\n100  4177  100  4177    0     0   343k      0 --:--:-- --:--:-- --:--:--  370k\r\n* Connection #0 to host recon left intact\r\n8,\"count\":10},...' does not contain '\"fileSize\":2048,\"count\":10'\r\n{code}\r\n\r\nIt seems stdout and stderr was mixed, breaking {{\"fileSize\":2048}} into two parts, thus search string was not found in the output.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in Recon acceptance test due to mixed stdout and stderr"
   },
   {
      "_id": "13316504",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-07-13 17:01:18",
      "description": "Keys created via the S3 Gateway currently use the createKey OM API to create the ozone key. Hence, when using a hdfs client to list intermediate directories in the key, OM returns key not found error. This was encountered while using fluentd to write Hive logs to Ozone via the s3 gateway.\r\ncc [~bharat]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Unable to list intermediate paths on keys created using S3G."
   },
   {
      "_id": "13315871",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-07-09 11:38:20",
      "description": "Noticed this NPE in OM logs for OM HA [acceptance test|https://github.com/apache/hadoop-ozone/pull/1173/checks?check_run_id=847204159]:\r\n\r\n{code}\r\n2020-07-07 20:54:23 WARN  RaftServerImpl:1247 - om2@group-D66704EFC61C: Failed to notify StateMachine to InstallSnapshot. Exception: java.lang.NullPointerException: When ratis is enabled indexToTerm should not be null\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OM StateMachine unpause fails with NPE"
   },
   {
      "_id": "13315733",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-07-08 21:31:02",
      "description": "HDDS-426 introduced new additions to proto files but failed to update proto.lock files.\u00a0\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update proto.lock files"
   },
   {
      "_id": "13315673",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-07-08 16:09:20",
      "description": "In OM HA, followers log latest snapshot information twice per second:\r\n\r\n{code}\r\nom1_1       | 2020-07-08 15:46:47,097 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1\r\nom2_1       | 2020-07-08 15:46:47,097 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1\r\nom1_1       | 2020-07-08 15:46:47,604 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1\r\nom2_1       | 2020-07-08 15:46:47,604 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1\r\nom1_1       | 2020-07-08 15:46:48,110 [grpc-default-executor-3] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1\r\nom2_1       | 2020-07-08 15:46:48,110 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Latest Snapshot Info 0#-1\r\n{code}\r\n\r\nI think this should be debug-level message.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Change latest snapshot log to debug"
   },
   {
      "_id": "13315327",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-07-07 07:27:07",
      "description": "{code:title=mvn clean}\r\n[INFO] Scanning for projects...\r\n[WARNING]\r\n[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-interface-client:jar:0.6.0-SNAPSHOT\r\n[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.\r\n[WARNING]\r\n[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-common:jar:0.6.0-SNAPSHOT\r\n[WARNING] The expression ${pom.artifactId} is deprecated. Please use ${project.artifactId} instead.\r\n...\r\n{code}\r\n\r\nSame warning in {{hadoop-hdds/pom.xml}} was fixed during review of HDDS-3875, but the one in {{hadoop-ozone/pom.xml}} was left.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Maven warning due to deprecated expression pom.artifactId"
   },
   {
      "_id": "13315266",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-07-07 00:36:14",
      "description": "Cache of key should be updated in ValidateAndUpdateCache, as we return response once after adding to cache, and before DoubleBuffer flushes to disk using OmClientResponse#addToDBBatch.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix OMKeyDeletesRequest"
   },
   {
      "_id": "13315225",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2020-07-06 19:45:11",
      "description": "Fix the way in which S3g endpoint is being displayed in S3 gateway webpage.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix endpoint display in S3 Gateway webpage"
   },
   {
      "_id": "13314840",
      "assignee": "xyao",
      "components": [],
      "created": "2020-07-03 07:05:07",
      "description": "2020-07-03 14:51:45,489 [EventQueue-ContainerReportForContainerReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@8f6e7cb\r\njava.util.ConcurrentModificationException\r\n        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)\r\n        at java.util.HashMap$KeyIterator.next(HashMap.java:1469)\r\n        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)\r\n        at java.util.AbstractCollection.addAll(AbstractCollection.java:343)\r\n        at java.util.HashSet.<init>(HashSet.java:120)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:127)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n2020-07-03 14:51:45,648 [EventQueue-ContainerReportForContainerReportHandler] ERROR org.apache.hadoop.hdds.server.events.SingleThreadExecutor: Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@49d2b84b\r\njava.util.ConcurrentModificationException\r\n        at java.util.HashMap$HashIterator.nextNode(HashMap.java:1445)\r\n        at java.util.HashMap$KeyIterator.next(HashMap.java:1469)\r\n        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1044)\r\n        at java.util.AbstractCollection.addAll(AbstractCollection.java:343)\r\n        at java.util.HashSet.<init>(HashSet.java:120)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:127)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:50)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ConcurrentModificationException in ContainerReportHandler.onMessage"
   },
   {
      "_id": "13314556",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-07-01 21:14:15",
      "description": "LevelDB support was removed for OM and SCM DBs but DN Metastore can still be configured to use LevelDB or RocksDB.\u00a0\r\nThis Jira proposes to remove LevelDB configuration option for DN Metastore (ozone.metastore.impl) and use RocksDB only.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Remove LevelDB configuration option for DN Metastore"
   },
   {
      "_id": "13314484",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337225",
            "id": "12337225",
            "name": "SCM HA"
         }
      ],
      "created": "2020-07-01 13:57:14",
      "description": "{code}\r\n[INFO] --- hadoop-maven-plugins:3.2.1:protoc (compile-protoc) @ hadoop-hdds-server-scm ---\r\n[WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program \"protoc\": error=2, No such file or directory\r\n[ERROR] stdout: []\r\n{code}\r\n\r\nhttps://github.com/apache/hadoop-ozone/runs/814218639",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Compile error in acceptance test on HDDS-2823"
   },
   {
      "_id": "13314457",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-07-01 11:27:05",
      "description": "{{HddsPrometheusConfig}} includes {{.}} at the end of the prefix, thus generated config name has double dot: {{hdds.prometheus..endpoint.token}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Duplicate dot in Prometheus endpoint config name"
   },
   {
      "_id": "13313784",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-06-27 18:39:15",
      "description": "ratis.thirdpary.version -> ratis.thirdparty.version",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix typo in pom.xml"
   },
   {
      "_id": "13313673",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-06-26 19:21:01",
      "description": "This Jira aims to address the following:\r\n1. Add robot test for Install Snapshot feature\u00a0\r\n2. Fix the flakiness in OM HA robot tests (HDDS-3313)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve OM HA Robot tests"
   },
   {
      "_id": "13313592",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-26 11:52:08",
      "description": "om.serviceId is required on case of OM.HA in all the client parameters even if there is only one om.serviceId and it can be chosen.\r\n\r\nMy goal is:\r\n\r\n 1. Provide better usability\r\n 2. Simplify the documentation task ;-)\r\n\r\nWith using the om.serviceId from the config if \r\n\r\n 1. config is available\r\n 2. om ha is configured \r\n 3. only one service is configured\r\n\r\nIt also makes easier to run the same tests with/without HA",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make OMHA serviceID optional if one (but only one) is defined in the config "
   },
   {
      "_id": "13313585",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337242",
            "id": "12337242",
            "name": "CI"
         }
      ],
      "created": "2020-06-26 11:09:56",
      "description": "This workflow failed only due to temporary problem during artifact upload after successful tests: https://github.com/apache/hadoop-ozone/runs/809777550\r\n\r\nTo save time and resources, checks should not fail in this case.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Do not fail CI check for log upload failure"
   },
   {
      "_id": "13313583",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337242",
            "id": "12337242",
            "name": "CI"
         }
      ],
      "created": "2020-06-26 10:50:39",
      "description": "Most CI checks print a summary (failed tests, checkstyle/rat violations, etc.) to stdout at the end of the test run, as well as into {{summary.txt}}.  Currently we have the following ways to view this output:\r\n\r\n* drill down to the test step, scroll past lots of output\r\n* download raw log, scroll past lots of output\r\n* download artifact, unzip, open {{summary.txt}}\r\n\r\nI propose displaying contents of {{summary.txt}} as a separate step.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Display summary of failures as a separate job step"
   },
   {
      "_id": "13313580",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337267",
            "id": "12337267",
            "name": "0.6.0"
         }
      ],
      "created": "2020-06-26 10:38:27",
      "description": "During the earlier release we realized that Apache Nexus couldn't handle the classpath files well.\r\n\r\nClasspath files are generated during the build and copied to the final distribution to use separated classpath definition for each of the services.\r\n\r\nIt turned out that the Apache Nexus couldn't handle it well (INFRA-18344)  and HDDS-1510 was an attempt to fix this problem. But during 0.5.0 release we have seen the same problem.\r\n\r\nTo make the 0.6.0 release more seamless I propose to update the logic and instead of copy the classpath file via an artifact I would pack/unpack them to/from the final artifact jar.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Package classpath files to the jar files instead of uploading them as artifacts"
   },
   {
      "_id": "13313517",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-06-25 22:51:15",
      "description": "If users add the properties of ozone to core-site, then during loading of OzoneConfiguration, it addsResource ozone-default.xml. This overrides the properties of ozone which are added to core-site.\r\n\r\nTo avoid this kind of override issue, we can addResource core-site.xml after ozone-default.xml",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add resource core-site during loading of ozoneconfiguration"
   },
   {
      "_id": "13313265",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2020-06-24 14:19:32",
      "description": "ozone freon s3kg is a key generator which uses the s3 interface. It uses simple put objects but it turned out that s3 mpu implementation has some specific problem. I would improve the key generator to make it possible to test mpu upload with AWS Java SDK.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support multi-part-upload with Freon S3 key generator"
   },
   {
      "_id": "13313095",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2020-06-23 18:20:19",
      "description": "Upgrade from Ozone 0.5.0 to 0.6.0 (snapshot) requires some workaround steps (at least for HDDS-3499).  Smoke test for upgrade requires a docker image with the tools necessary for the workaround.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ldb to ozone-runner docker image"
   },
   {
      "_id": "13313075",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-23 15:39:37",
      "description": "With a few thousands issues ago Ozone was integral part of Hadoop/HDFS project. At that time there were two options to start datanode:\r\n\r\n\u00a01. Start in a separated JVM\r\n\r\n\u00a02. Start in the same JVM with the HDFS\r\n\r\nToday only 1 is the standard way, this is tested and working. 2nd is not working any more but still documented.\r\n\r\nI propose to drop the support of this use case as I can't see any benefit to support it anymore:\r\n\r\n\u00a01. I think 100% of the users will use Ozone as a separated process not as a HDFS Datanode plugin\r\n\u00a02. Fixing the classpath issues is significant effort as the classpath of HDFS and Ozone are diverged.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove support to start Ozone and HDFS datanodes in the same JVM"
   },
   {
      "_id": "13313074",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-23 15:31:31",
      "description": "Datanode in compose/ozonscripts can't be started due to an old configuration (OzoneHddsDatanodeService is removed in HDDS-738)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanode in compose/ozonescripts can't be started "
   },
   {
      "_id": "13312922",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-06-22 23:09:20",
      "description": "Introduce ratis.thirdparty.version in main pom.xml\r\nThis will help to override ratis.version if ozone is required to compile with a different version.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ratis.thirdparty.version in main pom.xml"
   },
   {
      "_id": "13312899",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-06-22 20:06:39",
      "description": "In a OM HA setup, client tries the OM's in a round robin fashion to find the leader OM. It is not required to log the retry information for every client call. It creates noise on the console. Instead, we should just log the retry attempts only when all the retries fail.\u00a0\r\n\r\n\r\n{code:java}\r\n20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om1 is not the leader. Suggested leader is OM:om3.20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om1 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)\r\n20/06/22 19:58:54 INFO protocolPB.Hadoop3OmTransport: RetryProxy: OM:om2 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:198) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:123) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:74) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:113) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:985) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:913) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882){code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Change OMNotLeaderException logging to DEBUG"
   },
   {
      "_id": "13312241",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-06-18 14:00:06",
      "description": "* Implement the concept of a 'Layout Feature' in Ozone (with sample usage in Ozone Manager), which defines a specific change in on-disk layout in Ozone.\r\n* Every feature is associated with a layout version, and an API corresponding to the feature cannot be invoked (throws NOT_SUPPORTED_OPERATION) before finalization.\r\n* Created an annotation based 'aspect' for \"guarding\" new APIs that are introduced by Layout Features. Check out TestOMLayoutFeatureAspect#testCheckLayoutFeature.\r\n* Added sample features and tests for ease of review (To be removed before commit).\r\n* Created an abstract VersionManager and an inherited OM Version manager to initialize features, check if feature is allowed, check need to finalize, do finalization.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Introduce Layout Feature interface in Ozone"
   },
   {
      "_id": "13312203",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-18 11:57:01",
      "description": "Ozone insight command can print out configuration related to a specific ozone component with parsing the @Config object.\r\n\r\n\u00a0\r\n\r\nBut the usage of config annotations has been changed since HDDS-2661. We should check the annotated fields not methods.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Configuration parsing of ozone insight should be based on fields"
   },
   {
      "_id": "13312195",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-06-18 11:24:58",
      "description": "{code}\r\nTests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.637 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestKeyManagerUnit\r\nlistMultipartUploads(org.apache.hadoop.ozone.om.TestKeyManagerUnit)  Time elapsed: 0.102 s  <<< FAILURE!\r\njava.lang.AssertionError: Creation date is too old\r\n  ...\r\n  at org.apache.hadoop.ozone.om.TestKeyManagerUnit.listMultipartUploads(TestKeyManagerUnit.java:148)\r\n{code}\r\n\r\nhttps://github.com/elek/ozone-build-results/blob/master/2020/06/14/1034/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.TestKeyManagerUnit.txt",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestKeyManagerUnit#listMultipartUploads"
   },
   {
      "_id": "13312190",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-06-18 10:46:30",
      "description": "Ozone FS acceptance test is currently a monolithic one, a single test case for each of the filesystems (ofs and o3fs).  Finding out which assertion failed is harder than necessary.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Split Ozone FS acceptance tests"
   },
   {
      "_id": "13312158",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-18 08:36:51",
      "description": "ozonefs-hadoop3 is an all-in-one ozonefs client which can be used as a single jar file.\r\n\r\nUnfortunately it uses wrong dependency (ozonefs-common instead of ozonefs-common-shaded) which means that it downloads additional dependencies (netty-all, ...) if it's used from maven.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Hadoop3 artifact should depend on the ozonefs-shaded"
   },
   {
      "_id": "13312097",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-18 02:52:50",
      "description": "GitHub Actions workflows for Ozone CI have two flavors: post-commit and pull-request.  The only difference between the two is that Sonar is not updated for PRs:\r\n\r\n{code}\r\n-       - uses: ./.github/buildenv\r\n-         if: github.repository == 'apache/hadoop-ozone'\r\n-         with:\r\n-           args: ./hadoop-ozone/dev-support/checks/sonar.sh\r\n-         env:\r\n-           SONAR_TOKEN: ${{ secrets.SONARCLOUD_TOKEN }}\r\n-           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\r\n{code}\r\n\r\nThis issue proposes to keep only the post-commit definition and execute {{sonar.sh}} conditionally.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Eliminate duplicated GitHub Actions workflow"
   },
   {
      "_id": "13312079",
      "assignee": "xyao",
      "components": [],
      "created": "2020-06-17 23:31:19",
      "description": "HDDS-3282 adds hdds.[compoment].http.auth.[type] for scm/dn and ozone.[component].http.auth.[type] for om/s3g/recon. The type can be simple and kerberos. When the type is kerberos, it will setup the spnego authentication filter for the servlets. \r\n\r\nHowever, when the filter prefix was not setup properly when the type is simple. As a result, it will fallback to configuration from hadoop.http.authentication.*.\r\n\r\nThis ticket is opened to fix the authentication filter prefix so that the simple configuration can be honored when SPNEGO is disabled. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable Ozone SPNEGO should not fall back to hadoop.http.authentication configuration "
   },
   {
      "_id": "13311991",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-17 15:44:55",
      "description": "Mukul suggested to schedule cron based build to have more frequent data points to identify flaky tests.\r\n\r\nWe can start with two additional daily build which can be independent from the commit frequency (today we build master only after the commits).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "build"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Schedule daily 2 builds from master branch build"
   },
   {
      "_id": "13311637",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-06-16 05:50:23",
      "description": "A datanode can participate in multiple pipelines based on no of raft log disks as well the disk type. SCM should make the distribution of open containers among these set of pipelines evenly.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add the logic to distribute open containers among the pipelines of a datanode"
   },
   {
      "_id": "13311636",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-06-16 05:48:35",
      "description": "The no of open containers on a datanode is to be driven by factor of no of data disks available multiplied by no of open containers per disk. The aim here is to add the logic here and verify it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make number of open containers on a datanode a function of no of volumes reported by it"
   },
   {
      "_id": "13311634",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-06-16 05:43:18",
      "description": "No of pipelines to be created on a datanode is to be driven by the no of raft log disks configured on datanode. The Jira here is to add support for propagation of raft log volume info to SCM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Propagate raft log disks info to SCM from datanode"
   },
   {
      "_id": "13311455",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-15 10:11:06",
      "description": "Noticed performance degradation while running {{SynteticLoadGenerator}} benchmark test with {{fs.defaultFS}}=HDFS *Vs* {{fs.defaultFS}}=O3FS.\r\n{code:java}\r\nRunning LoadGenerator against fileSystem: hdfs://vb0929.halxg.cloudera.com:8020\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0*Vs*\r\nRunning LoadGenerator against fileSystem: o3fs://bucket2.vol2/\r\n{code}\r\n\u00a0\r\n\r\nCommand to run SyntheticLoadGenerator:-\r\n{code:java}\r\nyarn jar /opt/cloudera/parcels/CDH-7.2.0-1.cdh7.2.0.p0.3738720/jars/hadoop-mapreduce-client-jobclient-3.1.1.7.2.0.0-236-tests.jar NNloadGenerator -writeProbability 1 -readProbability 0.00 -elapsedTime 120 -numOfThreads 300 -root o3fs://bucket2.vol2/fsperf-Jun-11-2020/\r\n{code}\r\n\u00a0\r\n\r\nWith HDFS as 'fs.defaultFS', I could see that configuration is reloading resources always for each FileContext#create API call and is causing delay in submitting requests to OM server.\r\n{code:java}\r\n\"Thread-304\" #327 prio=5 os_prio=0 tid=0x00007f1609d42000 nid=0x2990b waiting for monitor entry [0x00007f15cb990000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n        at sun.misc.URLClassPath.getNextLoader(URLClassPath.java:479)\r\n        - waiting to lock <0x00000000f01097e8> (a sun.misc.URLClassPath)\r\n        at sun.misc.URLClassPath.findResource(URLClassPath.java:224)\r\n        at java.net.URLClassLoader$2.run(URLClassLoader.java:572)\r\n        at java.net.URLClassLoader$2.run(URLClassLoader.java:570)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.net.URLClassLoader.findResource(URLClassLoader.java:569)\r\n        at java.lang.ClassLoader.getResource(ClassLoader.java:1089)\r\n        at java.lang.ClassLoader.getResource(ClassLoader.java:1084)\r\n        at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2803)\r\n        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3059)\r\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3018)\r\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2991)\r\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2871)\r\n        - locked <0x00000000f1da5f10> (a org.apache.hadoop.hdds.conf.OzoneConfiguration)\r\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1223)\r\n        at org.apache.hadoop.ozone.OmUtils.isServiceIdsDefined(OmUtils.java:175)\r\n        at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:125)\r\n        at org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:51)\r\n        at org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)\r\n        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:167)\r\n        at org.apache.hadoop.fs.DelegateToFileSystem.<init>(DelegateToFileSystem.java:54)\r\n        at org.apache.hadoop.fs.ozone.OzFs.<init>(OzFs.java:41)\r\n        at sun.reflect.GeneratedConstructorAccessor4.newInstance(Unknown Source)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:142)\r\n        at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:180)\r\n        at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:265)\r\n        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:341)\r\n        at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:338)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)\r\n        at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:338)\r\n        at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:330)\r\n        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:85)\r\n        at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.genFile(LoadGenerator.java:330)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.write(LoadGenerator.java:304)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.nextOp(LoadGenerator.java:271)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.run(LoadGenerator.java:236)\r\n{code}\r\n\u00a0\r\n\r\nWith O3FS as 'fs.defaultFS', there is no config#loading.\r\n{code:java}\r\n\"Thread-400\" #424 prio=5 os_prio=0 tid=0x00007f1988fe6800 nid=0x58441 in Object.wait() [0x00007f1942080000]\r\n   java.lang.Thread.State: WAITING (on object monitor)\r\n        at java.lang.Object.wait(Native Method)\r\n        at java.lang.Object.wait(Object.java:502)\r\n        at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)\r\n        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1541)\r\n        - locked <0x00000000f60c3ab8> (a org.apache.hadoop.ipc.Client$Call)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1499)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1396)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n        at com.sun.proxy.$Proxy10.submitRequest(Unknown Source)\r\n        at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n        - locked <0x00000000f60a1690> (a org.apache.hadoop.io.retry.RetryInvocationHandler$Call)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n        at com.sun.proxy.$Proxy10.submitRequest(Unknown Source)\r\n        at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:424)\r\n        at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createFile(OzoneManagerProtocolClientSideTranslatorPB.java:1632)\r\n        at org.apache.hadoop.ozone.client.rpc.RpcClient.createFile(RpcClient.java:1088)\r\n        at org.apache.hadoop.ozone.client.OzoneBucket.createFile(OzoneBucket.java:538)\r\n        at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.createFile(BasicOzoneClientAdapterImpl.java:227)\r\n        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.createOutputStream(BasicOzoneFileSystem.java:270)\r\n        at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.create(BasicOzoneFileSystem.java:250)\r\n        at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1263)\r\n        at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\r\n        at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:615)\r\n        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\r\n        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\r\n        at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n        at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.genFile(LoadGenerator.java:330)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.write(LoadGenerator.java:304)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.nextOp(LoadGenerator.java:271)\r\n        at org.apache.hadoop.fs.loadGenerator.LoadGenerator$DFSClientThread.run(LoadGenerator.java:236)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "om-perf",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Config being reloaded with hdfs as 'fs.defaultFS'"
   },
   {
      "_id": "13311236",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-13 05:02:58",
      "description": "Currently {{coverage-report.sh}} only works in GitHub Actions environment.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Allow running coverage locally"
   },
   {
      "_id": "13311233",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-13 04:38:33",
      "description": "Ozone FS classes are ignored by JaCoCo agent, hence [reported coverage is 0%|https://sonarcloud.io/component_measures?id=hadoop-ozone&metric=coverage&selected=hadoop-ozone%3Ahadoop-ozone%2Fozonefs-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fozone&view=list].",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "No coverage reported for Ozone FS"
   },
   {
      "_id": "13311130",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-06-12 12:18:18",
      "description": "Today 2.7.7 is used but to be compatible with older Hadoop (including 2.7.4 which is included in spark), it's easier to test with an older version",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use Hadoop 2.7.2 for ozone-mr/hadoop27 acceptance tests"
   },
   {
      "_id": "13311128",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-12 12:15:48",
      "description": "Similar to the previous patch, we need to move the Ozone client interfaces to a separated project. It would help us to reuse it from different projects and monitor to backward compatibility.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Separate client proto files of Ozone to separated subprojects"
   },
   {
      "_id": "13310916",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-11 12:56:22",
      "description": "Integration test profiles can be simplified with matrix builds in GitHub Actions.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use matrix build for integration test"
   },
   {
      "_id": "13310885",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-11 09:47:14",
      "description": "PRs get code coverage feedback from Codecov thanks to changes implemented in HDDS-3726.  Codecov needs coverage data for each new revision on master.  However, _coverage_ check is currently [skipped|https://github.com/apache/hadoop-ozone/runs/759894317?check_suite_focus=true] if any tests fail.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upload coverage even if tests failed"
   },
   {
      "_id": "13310820",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-06-11 06:23:02",
      "description": "Currently, with concurrent allocate block calls, the block allocation among the open containers of a pipeline is not uniform as with concurrent logic, block allocation logic with last used notion does not hold up. The idea here is to address this.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Block distribution in a pipeline among open containers is not uniform"
   },
   {
      "_id": "13310817",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-11 06:07:17",
      "description": "Hi [~vivekratnavel], [~elek],\r\n\r\nThe change in HDDS-3682 increased Recon build time by ~10 minutes, eg.:\r\n\r\n{code:title=before}\r\n[INFO] Apache Hadoop Ozone Recon .......................... SUCCESS [04:04 min]\r\n{code}\r\n\r\n{code:title=after}\r\n[INFO] Apache Hadoop Ozone Recon .......................... SUCCESS [16:34 min]\r\n{code}\r\n\r\nRecon UI is built in _compile_, _acceptance_ and _coverage_ checks.  Since _coverage_ runs after all other checks, the increment affects overall CI response time twice.  (Effect was 3x before HDDS-3726, thanks for the fix in {{integration.sh}}.)\r\n\r\nIs there any way to speed this up, eg. by installing dependencies in the ozone-build docker image?\r\n\r\nbefore: https://github.com/apache/hadoop-ozone/actions/runs/130155633\r\npre-HDDS-3726: https://github.com/apache/hadoop-ozone/actions/runs/130301719\r\ncurrent: https://github.com/apache/hadoop-ozone/actions/runs/131217499",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Reduce recon UI build time"
   },
   {
      "_id": "13310801",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335213",
            "id": "12335213",
            "name": "upgrade"
         }
      ],
      "created": "2020-06-11 02:38:50",
      "description": "Currently we have rocksdb 6.6.4 as major version and there are some jvm issues in tests (happened in [https://github.com/apache/hadoop-ozone/pull/1019]) related to rocksdb core dump. We may upgrade to 6.8.1 to avoid this issue.\r\n\r\n{{JRE version: Java(TM) SE Runtime Environment (8.0_211-b12) (build 1.8.0_211-b12)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.211-b12 mixed mode bsd-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [librocksdbjni2954960755376440018.jnilib+0x602b8]  rocksdb::GetColumnFamilyID(rocksdb::ColumnFamilyHandle*)+0x8\r\n\r\nSee full dump at [https://the-asf.slack.com/files/U0159PV5Z6U/F0152UAJF0S/hs_err_pid90655.log?origin_team=T4S1WH2J3&origin_channel=D014L2URB6E](url)}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upgrading RocksDB version to avoid java heap issue"
   },
   {
      "_id": "13310419",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-06-09 15:07:47",
      "description": "Error on fluentd side.\r\n{code}\r\nopened\r\nstarting SSL for host1:9879...\r\nSSL established\r\n<- \"PUT /logs-bucket-1/logs/mytag/2020/06/05//202006052222_190411.gz HTTP/1.1\\r\\nContent-Type: application/x-gzip\\r\\nAccept-Encoding: \\r\\nUser-Agent: aws-sdk-ruby3/3.94.0 ruby/2.4.10 x86_64-l\r\ninux aws-sdk-s3/1.63.0\\r\\nX-Amz-Storage-Class: STANDARD\\r\\nExpect: 100-continue\\r\\nContent-Md5: zGKVGGaD/U5WUK3cdWQiSA==\\r\\nHost: host1:9879\\r\\nX-Amz-Content-Sha256:\r\n 277fe97f57a1127ee1a0765979ffd3270a6c18c6f75ff6a0f843e7163a338de2\\r\\nContent-Length: 44726\\r\\nX-Amz-Date: 20200608T190412Z\\r\\nAuthorization: AWS4-HMAC-SHA256 Credential=hdfs@ROOT.HWX.SITE/202\r\n00608/us-east-1/s3/aws4_request, SignedHeaders=content-md5;content-type;expect;host;user-agent;x-amz-content-sha256;x-amz-date;x-amz-storage-class, Signature=11c1d0a43325d3f7b9d25dbd02023cef2\r\n69b66f6a93fa4e1c935b52e3e372f70\\r\\nAccept: */*\\r\\n\\r\\n\"\r\n-> \"HTTP/1.1 100 Continue\\r\\n\"\r\n-> \"\\r\\n\"\r\n-> \"HTTP/1.1 500 Server Error\\r\\n\"\r\n-> \"Pragma: no-cache\\r\\n\"\r\n-> \"X-Content-Type-Options: nosniff\\r\\n\"\r\n-> \"X-FRAME-OPTIONS: SAMEORIGIN\\r\\n\"\r\n-> \"X-XSS-Protection: 1; mode=block\\r\\n\"\r\n-> \"Connection: close\\r\\n\"\r\n-> \"\\r\\n\"\r\nreading all...\r\n-> \"\"\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fluentd writing to secure Ozone S3 API fails with 500 Error."
   },
   {
      "_id": "13310385",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-06-09 12:25:07",
      "description": "TestDeleteWithSlowFollower failed soon after it was re-enabled in HDDS-3330.\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/753363338}\r\n[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.647 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\r\n[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 0.163 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  ...\r\n  at org.junit.Assert.assertNotNull(Assert.java:631)\r\n  at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)\r\n{code}\r\n\r\nCC [~shashikant] [~elek]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestDeleteWithSlowFollower"
   },
   {
      "_id": "13310169",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-08 15:29:43",
      "description": "Acceptance test coverage should be added to the generic coverage numbers. We have a lot of important tests there...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add test coverage of the acceptance tests to overall test coverage "
   },
   {
      "_id": "13310141",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-08 13:42:26",
      "description": "Use a storage-class as an abstraction which combines replication configuration, container states and transitions. \r\n\r\nSee this thread for the detailed design doc:\r\n\r\n\u00a0\r\n\r\n[https://lists.apache.org/thread.html/r1e2a5d5581abe9dd09834305ca65a6807f37bd229a07b8b31bda32ad%40%3Cozone-dev.hadoop.apache.org%3E]\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Storage-class support for Ozone"
   },
   {
      "_id": "13310133",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-08 12:58:32",
      "description": "I started to document the project hierarchy of Ozone and found that to subproject name is very confusing:\r\n\r\n\u00a0\r\n\r\n\u00a01. framework: it supposed to be similar to the common but share the classes only between server side projects (om/scm/...). I propose to call it server-common\r\n\r\n\u00a02. container-service: While we have historical reasons why it's a separated project, but today we can call it datanode and remove hadoop-ozone/datanode.\r\n\r\n\u00a0\r\n\r\nWhile I can learn both the specific names, for new contributors it can help if we use more straightforward names.\r\n\r\n\r\nIn this patch I would like to fix the first item (framework --> common-server)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "TriagePending",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Rename framework to common-server"
   },
   {
      "_id": "13309797",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-06-05 21:13:49",
      "description": "Follower OM issues a pause on its services before installing new checkpoint from Leader OM (Install Snapshot). If this installation fails for some reason, the OM stays in paused state. It should be unpaused and the old state should be reloaded.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Reload old OM state if Install Snapshot from Leader fails"
   },
   {
      "_id": "13309606",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-05 05:27:20",
      "description": "HDDS-3170 aggregates code coverage across all components. We need to upload the reports to codecov to be able to keep track of coverage and coverage diffs to be able to tell if a PR does not do a good job on writing unit tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Upload code coverage to Codecov and enable checks in PR workflow of Github Actions"
   },
   {
      "_id": "13309478",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-06-04 15:42:18",
      "description": "Regardless of values configured for {{hdds.datanode.replication.streams.limit}} and {{hdds.datanode.container.delete.threads.max}}, these config items always use 10 and 2, respectively.\r\n\r\nAlso, no warning is logged for invalid values ({{< 1}}).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanode configuration object has wrong values"
   },
   {
      "_id": "13309462",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-06-04 13:59:43",
      "description": "TestOzoneManagerListVolumes timed out after 2 minutes while trying to stop the mini cluster.  It seems one of the datanodes was stuck in an infinite loop trying to execute a task on a terminated executor:\r\n\r\n{code:title=}\r\n2020-06-03 15:28:19,475 [Datanode State Machine Thread - 0] ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(221)) - Unable to finish the execution.\r\njava.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ExecutorCompletionService$QueueingFuture@73c36b6c rejected from org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor@5c021707[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\r\n\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\r\n\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\r\n\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\r\n\tat java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:181)\r\n\tat org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.execute(RunningDatanodeState.java:143)\r\n\tat org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:411)\r\n\tat org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:208)\r\n\tat org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:375)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n\r\n{code:title=grep -c 'Unable to finish the execution' hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestOzoneManagerListVolumes-output.txt}\r\n2087250\r\n{code}\r\n\r\nTest output eventually grew to 2.6GB.\r\n\r\nhttps://github.com/apache/hadoop-ozone/runs/735169623",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Datanode may fail to stop"
   },
   {
      "_id": "13309450",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-06-04 13:17:43",
      "description": "HDDS-3461 introduced a new CSI smoketest (to be sure that the CSI daemon can be started).\r\n\r\nIt was reverted because a failure on the master and commited after an additional check is added to wait until the CSI socket is created.\r\n\r\nUnfortunately this check is bad. In some cases it can fail:\r\n\r\nFor example in here:\r\n\r\n[https://github.com/jsoft88/hadoop-ozone/runs/734147343?check_suite_focus=true]\r\n\r\n\u00a0\r\n{code:java}\r\nconnection error: desc = \"transport: Error while dialing dial unix /tmp/csi.sock: connect: no such file or directory\" {code}\r\nThanks to [~jsoft88] , who reported this problem.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "CSI smoketest fails if socket file is not created on time"
   },
   {
      "_id": "13309425",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-04 11:32:29",
      "description": "@PostConstruct annotation is removed from JDK (it's Java EE) in the recent JDKs.\r\n\r\nIt's used by the Configuration annotation but we don't need to use Java EE annotations:\r\n{code:java}\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-hdds-config: Compilation failure: Compilation failure: \r\n[ERROR] /Users/sbanerjee/ozone_fork/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationReflectionUtil.java:[20,24] cannot find symbol\r\n[ERROR]  symbol:  class PostConstruct\r\n[ERROR]  location: package javax.annotation\r\n[ERROR] /Users/sbanerjee/ozone_fork/hadoop-hdds/config/src/main/java/org/apache/hadoop/hdds/conf/ConfigurationReflectionUtil.java:[139,38] cannot find symbol\r\n[ERROR]  symbol:  class PostConstruct\r\n{code}\r\n\u00a0\r\n\r\nThanks\u00a0 Shashikant for the bug report.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Compile of Ozone fails with JDK 11+"
   },
   {
      "_id": "13309175",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-06-03 11:44:10",
      "description": "HDDS-3635 started to archive the jacoco coverage data for each of the unit and integration tests (unit, it-*).\r\n\r\n\u00a0\r\n\r\nThis patch introduces a new build step to combine all of them together and archive the coverage report in HTML as a build artifact.\r\n\r\nNotes:\r\n\r\n\u00a01. acceptance test coverage is not yet included\r\n\r\n\u00a02. I decided to do it only for master (branch) builds as it requires a new build which adds ~15 minutes to the full build. As the coverage data is not (yet) used for PR we don't need to enable it (yet)\r\n\r\n\u00a03. We can further improve it to upload the merged data to somewhere (sonar?) Can be done in the next Jira",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge archived jacoco coverage results"
   },
   {
      "_id": "13308940",
      "assignee": "elek",
      "components": [],
      "created": "2020-06-02 12:18:40",
      "description": "Using legacy jar is legacy. We need to updated all the docs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Update all the documentation to use ozonefs-hadoop2/3  instead of legacy/current"
   },
   {
      "_id": "13308939",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-06-02 12:07:03",
      "description": "In getChunkFile internally, it invokes \"verifyChunkDirExists\". This causes file existence checks for the directory and throws IO exception accordingly. If the file is anyways going to be written, it is better to handle it later and throw the same exception. This could avoid file checks for every \"writechunk\"\r\n\r\n[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L106]\r\n\r\n\u00a0\r\n\r\nFile channels are cached anyways in \"OpenFiles\". So if we can avoid \"file.getAbsolutePath()\", this could save memory and resolving paths.\u00a0\r\n\r\n[https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerBlockStrategy.java#L118]\r\n\r\n\u00a0\r\n\r\nAlso \"validateChunkForOverwrite\" can be optimised, as \"isOverWritePermitted\" would be false most of the times.\r\n\r\n\u00a0\r\n\r\n!Screenshot 2020-06-02 at 5.28.59 PM.png|width=835,height=510!\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Consider avoiding file lookup calls in writeChunk hotpath"
   },
   {
      "_id": "13308874",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-06-02 06:26:20",
      "description": "Currently, \"ozone.scm.pipeline.owner.container.count\" is configured by default to 3. The default should ideally be a function of the no of disks on a datanode. A static value may lead to uneven utilisation during active IO .",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Number of open containers per pipeline should be tuned as per the number of disks on datanode"
   },
   {
      "_id": "13308787",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-06-01 19:23:37",
      "description": "Support for Non-rolling upgrades in Ozone.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Non-Rolling upgrades"
   },
   {
      "_id": "13308225",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-05-29 04:45:58",
      "description": "HDDS-3476 used the transaction info persisted in OM DB during double buffer flush when OM is restarted. This transaction info log index and the term are used as a snapshot index. So, we can remove the replay logic from actual request logic. (As now we shall never have the transaction which is applied to OM DB will never be again replayed to DB)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Remove replay logic from actual request logic"
   },
   {
      "_id": "13308166",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-05-28 21:40:20",
      "description": "Include a histogram to interactively view file size counts across each volume/bucket",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon UI: Add interactive visualization for file size counts"
   },
   {
      "_id": "13308164",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-05-28 21:38:31",
      "description": "Update file size count task to keep track of file size counts across each volume/bucket. Make the necessary changes to the underlying schema.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon: Add support to store file size counts in each volume/bucket"
   },
   {
      "_id": "13308021",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-05-28 12:19:11",
      "description": "Hadoop 3.3.0 upgraded protocol buffers to 3.7.1 and RPC code have been changed. This change will cause compile failure in Ozone.\r\n\r\nVinayakumar is fixing this in Hadoop-side (HADOOP-17046) but it would be better for Ozone to avoid the usage of Hadoop {{@Private}} classes to make Ozone a separate project from Hadoop.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove usage of DFSUtil.addPBProtocol method"
   },
   {
      "_id": "13307486",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-05-26 17:07:01",
      "description": "OM start fails with RocksDB error when downgrading to older version that does not have all the column families that may have been created  in the newer version.\r\n\r\n{code}\r\njava.io.IOException: Failed init RocksDB, db path : /tmp/ozone/data/metadata/om.db, exception :org.rocksdb.RocksDBE\r\nxception You have to open all column families. Column families not opened: transactionInfoTable; status : InvalidAr\r\ngument; message : You have to open all column families. Column families not opened: transactionInfoTable\r\n        at org.apache.hadoop.hdds.utils.db.RDBStore.toIOException(RDBStore.java:159)\r\n        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:141)\r\n        at org.apache.hadoop.hdds.utils.db.DBStoreBuilder.build(DBStoreBuilder.java:181)\r\n        at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.start(OmMetadataManagerImpl.java:267)\r\n        at org.apache.hadoop.ozone.om.OmMetadataManagerImpl.<init>(OmMetadataManagerImpl.java:164)\r\n        at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:478)\r\n        at org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:416)\r\n        at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:884)\r\n        at org.apache.hadoop.ozone.om.OzoneManagerStarter$OMStarterHelper.start(OzoneManagerStarter.java:123)\r\n        at org.apache.hadoop.ozone.om.OzoneManagerStarter.startOm(OzoneManagerStarter.java:78)\r\n        at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:66)\r\n        at org.apache.hadoop.ozone.om.OzoneManagerStarter.call(OzoneManagerStarter.java:37)\r\n        at picocli.CommandLine.execute(CommandLine.java:1173)\r\n        at picocli.CommandLine.access$800(CommandLine.java:141)\r\n        at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)\r\n        at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)\r\n        at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)\r\n        at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)\r\n        at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)\r\n        at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:75)\r\n        at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:66)\r\n        at org.apache.hadoop.ozone.om.OzoneManagerStarter.main(OzoneManagerStarter.java:50)\r\nCaused by: org.rocksdb.RocksDBException: You have to open all column families. Column families not opened: transact\r\nionInfoTable\r\n        at org.rocksdb.RocksDB.open(Native Method)\r\n        at org.rocksdb.RocksDB.open(RocksDB.java:290)\r\n        at org.apache.hadoop.hdds.utils.db.RDBStore.<init>(RDBStore.java:97)\r\n        ... 20 more\r\n{code}\r\n\r\nThanks to [~bharat] for reporting this issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "pull-request-available",
         "upgrade"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager start fails with RocksDB error on downgrade to older version."
   },
   {
      "_id": "13306668",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-05-22 04:09:50",
      "description": "LOGs\r\n\r\n{noformat}\r\n2020-05-19 13:45:30,493 [BlockDeletingService#8] WARN org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy: Chunk file doe not exist. chunk info :ChunkInfo{chunkName='104079328540607246_chunk_1, offset=0, len=4194304}\r\n2020-05-19 13:45:30,493 [BlockDeletingService#8] ERROR org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}\r\n2020-05-19 13:45:30,494 [BlockDeletingService#8] ERROR org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService: Failed to delete files for block #deleting#104079328540607246\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}\r\n        at org.apache.hadoop.ozone.container.keyvalue.impl.FilePerChunkStrategy.deleteChunks(FilePerChunkStrategy.java:286)\r\n        at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDispatcher.deleteChunks(ChunkManagerDispatcher.java:111)\r\n        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.deleteBlock(KeyValueHandler.java:1043)\r\n        at org.apache.hadoop.ozone.container.keyvalue.statemachine.background.BlockDeletingService$BlockDeletingTask.lambda$call$0(BlockDeletingService.java:286)\r\nCaused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Not Supported Operation. Trying to delete a chunk that is in shared file. chunk info : ChunkInfo{chunkName='104079328540607246_chunk_2, offset=4194304, len=1048576}\r\n{noformat}\r\n\r\n\r\nchunk_1 is 4MB and chunk_2 is 1MB in block info.  \r\nchunk_1 doesn't exit(might been deleted successfully)  and chunk_2 is 5MB on disk. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Failed to delete chunk file due to chunk size mismatch"
   },
   {
      "_id": "13306566",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-05-21 18:37:17",
      "description": "When a follower OM needs to replace its DB with a checkpoint from Leader (to catch up on the transactions), it should pause or stop services which read/ write to the DB.\u00a0\r\n\r\n\r\n\r\nDuring OM HA testing, found that OM could crash with JVM error on RocksDB. This happened because KeyDeletingService was trying to access a memory which is already freed up.\r\n{code:java}\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x00007f19de835af0, pid=1389, tid=1712\r\n#\r\n# JRE version: OpenJDK Runtime Environment (11.0.6+10) (build 11.0.6+10-LTS)\r\n# Java VM: OpenJDK 64-Bit Server VM (11.0.6+10-LTS, mixed mode, sharing, tiered, compressed oops, concurrent mark sweep gc, linux-amd64)\r\n# Problematic frame:\r\n# C  [librocksdbjni10001996641283911793.so+0x1aeaf0]  Java_org_rocksdb_RocksIterator_seekToFirst0+0x0\r\n#\r\n# Core dump will be written. Default location: Core dumps may be processed with \"/usr/share/apport/apport %p %s %c %d %P %E\" (or dumping to /opt/core.1389)\r\n#\r\n# An error report file with more information is saved as:\r\n# /opt/hs_err_pid1389.log\r\n\r\n{code}\r\nFrom the hs_error log file:\r\n{code:java}\r\n---------------  T H R E A D  ---------------Current thread (0x00000000011a4000):  JavaThread \"KeyDeletingService#1\" daemon [_thread_in_native, id=1712, stack(0x00007f19d2443000,0x00007f19d2544000)]Stack: [0x00007f19d2443000,0x00007f19d2544000],  sp=0x00007f19d2541e78,  free space=1019k\r\nNative frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [librocksdbjni10001996641283911793.so+0x1aeaf0]  Java_org_rocksdb_RocksIterator_seekToFirst0+0x0\r\nj  org.rocksdb.AbstractRocksIterator.seekToFirst()V+26\r\nj  org.apache.hadoop.hdds.utils.db.RDBStoreIterator.<init>(Lorg/rocksdb/RocksIterator;)V+13\r\nj  org.apache.hadoop.hdds.utils.db.RDBTable.iterator()Lorg/apache/hadoop/hdds/utils/db/TableIterator;+30\r\nj  org.apache.hadoop.hdds.utils.db.TypedTable.iterator()Lorg/apache/hadoop/hdds/utils/db/TableIterator;+4\r\nj  org.apache.hadoop.ozone.om.OmMetadataManagerImpl.getPendingDeletionKeys(I)Ljava/util/List;+8\r\nj  org.apache.hadoop.ozone.om.KeyManagerImpl.getPendingDeletionKeys(I)Ljava/util/List;+5\r\nj  org.apache.hadoop.ozone.om.KeyDeletingService$KeyDeletingTask.call()Lorg/apache/hadoop/hdds/utils/BackgroundTaskResult;+39\r\nj  org.apache.hadoop.ozone.om.KeyDeletingService$KeyDeletingTask.call()Ljava/lang/Object;+1\r\nJ 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]\r\nJ 4802 c1 java.util.concurrent.Executors$RunnableAdapter.call()Ljava/lang/Object; java.base@11.0.6 (14 bytes) @ 0x00007f19f0c87214 [0x00007f19f0c870e0+0x0000000000000134]\r\nJ 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]\r\nJ 4802 c1 java.util.concurrent.Executors$RunnableAdapter.call()Ljava/lang/Object; java.base@11.0.6 (14 bytes) @ 0x00007f19f0c87214 [0x00007f19f0c870e0+0x0000000000000134]\r\nJ 4791 c1 java.util.concurrent.FutureTask.run()V java.base@11.0.6 (123 bytes) @ 0x00007f19f0c7b414 [0x00007f19f0c7ad20+0x00000000000006f4]\r\nJ 4954 c1 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run()V java.base@11.0.6 (57 bytes) @ 0x00007f19f0cfe10c [0x00007f19f0cfde40+0x00000000000002cc]\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Stop/Pause Background services while replacing OM DB with checkpoint from Leader"
   },
   {
      "_id": "13306437",
      "assignee": "elek",
      "components": [],
      "created": "2020-05-21 09:38:19",
      "description": "Earlier we configured jacoco maven plugin to calculate the code coverage. But the jacoco.exec files (which contain the coverage information) are not stored in the artifacts created by the github actions.\r\n\r\nWe can add it easily.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Archive jacoco coverage files for unit/integration tests"
   },
   {
      "_id": "13306373",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-05-21 06:09:49",
      "description": "since the service names are same and they both referring to same location for pid files, we can not start both services at once.\r\n\r\nTweak is to export HADOOP_PID_DIR to different location after starting one service and start other one.\r\n\r\nIt would be better to have different pid file names.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n{noformat}\r\nUmas-MacBook-Pro ozone-0.5.0-beta % bin/ozone --daemon start datanode\r\ndatanode is running as process 25167.\u00a0 Stop it first.\r\n{noformat}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "HddsDatanodeService cannot be started if HDFS datanode running in same machine with same user."
   },
   {
      "_id": "13306173",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-05-20 11:45:05",
      "description": "[https://github.com/apache/hadoop-ozone/pull/767/checks?check_run_id=691722296]\u00a0\r\n{code:java}\r\n3083 at org.junit.Assert.fail(Assert.java:86) \r\n\r\n\r\n3084 at org.junit.Assert.assertTrue(Assert.java:41) \r\n\r\n\r\n3085 at org.junit.Assert.assertTrue(Assert.java:52) \r\n\r\n\r\n3086 at org.apache.hadoop.ozone.client.rpc.TestBlockOutputStreamWithFailures.test2DatanodesFailure(TestBlockOutputStreamWithFailures.java:335)\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestBlockOutputStreamWithFailures#test2DatanodesFailure"
   },
   {
      "_id": "13306164",
      "assignee": "elek",
      "components": [],
      "created": "2020-05-20 11:10:22",
      "description": "As described in the parent issue, the final step is to create a Hadoop independent shaded client and hadoop2/hadoop3 related separated client jars.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Remove FilteredClassloader and replace with maven based hadoop2/hadoop3 ozonefs generation"
   },
   {
      "_id": "13306022",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-05-19 22:51:55",
      "description": "Currently when GC parameters or any -XX are not set, it logs \r\n\r\n\"No '-XX:...' jvm parameters are used. Adding safer GC settings to the HADOOP_OPTS\r\n\r\nIt would be nice to improve this message with settings that are being set.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve error message when GC parameters are not set"
   },
   {
      "_id": "13306006",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-05-19 20:55:15",
      "description": "Introduce a getReadCopy in table method.\r\n\r\nAs right now, get when a value exists in the cache it returns the cloned copy, so that when it used during double-buffer flush, if other threads modify the object during the flush time we will see some exceptions which are mentioned in HDDS-2344 and HDDS-2322. To avoid this, all the get() values returned are cloned copy if it exists in the cache.\r\n\r\nBut for a few of the requests like OMBucketCreateRequest, we need Volume info (OmVolumeArgs, but we don't use this info during double buffer flush, so we can safely get a cached copy without doing a clone.  In this Jira, fixed only Bucket requests. I Will file a new Jira to see where all this new API can be safely used.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement getReadCopy in Table."
   },
   {
      "_id": "13305925",
      "assignee": "elek",
      "components": [],
      "created": "2020-05-19 13:15:52",
      "description": "As described in the parent Jira admin/client/server protocols need different compatibility guarantees. It's better to separate them to separated maven project to make it easy to follow the changes.\r\n\r\nI propose to create 3 new projects\r\n\r\nhadoop-hdds/interface-client\r\nhadoop-hdds/interface-server\r\nhadoop-hdds/interface-admin\r\n\r\nI called it -interface instead of -proto because we can include some basic Java classes not just proto files (for example utilities to serialize/deserialize proto files)\r\n\r\nThis first patch will include only the move without refactoring any RPC. While the new proto file names represent the proposed naming convention (Datanode/Scm + Server/Client/Admin + additional postfix) the name of the generated classes and the name of the RPC interfaces (Client/Server translator) not yet renamed to make the patch small.\r\n\r\nAlso: some methods should be moved between admin/server but it can be done in separated issue to make it easier to follow the change.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Separate client/server/admin proto files of HDDS to separated subprojects "
   },
   {
      "_id": "13305782",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-05-18 23:44:41",
      "description": "For volume/bucket table currently it is full cache, and we need to cleanup entries only when they are marked for delete.\r\n\r\nSo, it is unnecessary to call cleanup and waste the CPU resources on OM.\r\n\r\nSimilarly for other tables, when the double buffer has transaction entries that touch those tables, then only call cleanup.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Call cleanup on tables only when double buffer has transactions related to tables."
   },
   {
      "_id": "13305781",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-05-18 23:34:44",
      "description": "Remove the S3 table, after HDDS-3385, we don't have any use case for S3Table. We can remove this table from OmMetadataManager.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove S3Table from OmMetadataManager"
   },
   {
      "_id": "13305772",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-05-18 22:06:29",
      "description": "Fix JVMPause monitor logic, right now it is started only in restart.\r\n\r\nThis should be started during OM start, and stopped during OM Stop. In restart() we can start this again.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix JVMPause monitor start in OzoneManager"
   },
   {
      "_id": "13305747",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-05-18 19:28:24",
      "description": "Step 2 from S3 [volume mapping design doc|https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/design/ozone-volume-management.md#solving-the-mapping-problem-2-4-from-the-problem-listing]:\r\n\r\nImplement a bind mount mechanic which makes it possible to mount any volume/buckets to the specific \"s3\" volume.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Allow mounting bucket under other volume"
   },
   {
      "_id": "13305671",
      "assignee": "elek",
      "components": [],
      "created": "2020-05-18 12:58:44",
      "description": "To support Hadoop 2.x it's better to avoid to use Hadoop 3.x specific utility methods from IOUtils",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid to use Hadoop3.x IOUtils in Ozone Client "
   },
   {
      "_id": "13305064",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-05-14 22:19:35",
      "description": "Recon has an API endpoint to get file size distribution in Ozone. Add visualization in Recon UI for this using histograms.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon UI: Add visualization for file size distribution "
   },
   {
      "_id": "13304880",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-05-14 08:46:53",
      "description": "Steps to reproduce:\r\n\r\nImagine that I have 3 different om with the following DNS names:\r\n\r\n{code}\r\nozone-om-0.ozone-om\r\nozone-om-1.ozone-om\r\nozone-om-2.ozone-om\r\n{code}\r\n\r\nI configured the three hosts as the following:\r\n\r\n{code}\r\n  OZONE-SITE.XML_ozone.om.nodes.omservice: om1,om2,om3\r\n  OZONE-SITE.XML_ozone.om.address.omservice.om1: ozone-om-0\r\n  OZONE-SITE.XML_ozone.om.address.omservice.om2: ozone-om-1\r\n  OZONE-SITE.XML_ozone.om.address.omservice.om3: ozone-om-2\r\n  OZONE-SITE.XML_ozone.om.ratis.enable: \"true\"\r\n{code}\r\n\r\nBut unfortunately the DNS is not reliable. All the hosts can resolve only the LOCAL hostname.\r\n\r\nOMHANodeDetails.java ignores ALL the configuration which are not resolvable:\r\n\r\n{code}\r\n if (!addr.isUnresolved()) {\r\n          if (!isPeer && OmUtils.isAddressLocal(addr)) {\r\n            localRpcAddress = addr;\r\n            localOMServiceId = serviceId;\r\n            localOMNodeId = nodeId;\r\n            localRatisPort = ratisPort;\r\n            found++;\r\n          } else {\r\n            // This OMNode belongs to same OM service as the current OMNode.\r\n            // Add it to peerNodes list.\r\n            // This OMNode belongs to same OM service as the current OMNode.\r\n            // Add it to peerNodes list.\r\n            peerNodesList.add(getHAOMNodeDetails(conf, serviceId,\r\n                nodeId, addr, ratisPort));\r\n          }\r\n        }\r\n{code}\r\n\r\nAs a result I will have 3 running server but each has 1 one-node Ratis ring (peerNodesList is empty as only the local hostname can be resolved).\r\n\r\nGroup ID is the same for all. But they have separated database and they work as separated OM which is VERY dangerous.\r\n\r\n 1. Option one: we can accept any unresolved address and retry with connection create if it couldn't be connected\r\n\r\n2. Option two: at least the error handling should be fixed. When I configured 3 om, there supposed to be 3 om.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "OM HA can be started with 3 isolated LEADER instead of one OM ring"
   },
   {
      "_id": "13304224",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-05-11 22:28:21",
      "description": "Datanodes page should have \"Leader Count\" column that displays the leader count for how many Ratis pipelines the given datanode is elected as a leader.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon: Display leader count in Datanodes page"
   },
   {
      "_id": "13304193",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-05-11 19:38:35",
      "description": "Recon UI unit tests and linter checks should be added to Github actions CI.\u00a0\r\n\r\nOptimization: Try to add a workflow in such a way that these checks are run only if there is a change related to Recon UI files.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add Recon UI lint checks and unit tests to Github Actions CI workflow"
   },
   {
      "_id": "13303461",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-05-07 18:12:37",
      "description": "Add a linter like ([xojs|https://github.com/xojs/xo]) and fix all linter errors and warnings.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon UI: Add strict linter rules to improve code quality"
   },
   {
      "_id": "13303379",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-05-07 12:18:10",
      "description": "I think it's good to have a generic overview  of Ozonein presentation format and shared with the community. It can be used anytime by anybody to share details of Ozone on any meetup / conferences.\r\n\r\nI am not sure what is the best place for this but documentation seems to be a natural choice:\r\n\r\n * This is version dependent (can be updated with new releases)\r\n * Topics and diagrams can be shared between documentations",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide generic introduction / deep-dive slides as part of the documentation"
   },
   {
      "_id": "13303128",
      "assignee": "elek",
      "components": [],
      "created": "2020-05-06 14:31:45",
      "description": "Reported by \"Andrey Mindrin\" on the the-asf Slack:\r\n\r\n{quote}\r\nWe have made a few tests to compare OZONE (0.4 and 0.5 on Cloudera Runtime 7.0.3 with 3 nodes) performance with HDFS and OZONE is slower in most cases. For example, Spark application with 18 containers that copies 6 Gb parquet file is about 50% slower on OzoneFS. The only one case shows the same performance - Hive queries over partitioned tables.\r\n\r\n simple spark code we used:\r\n\r\n{code}\r\nval file = spark.read.format(format).load(path_input)\r\nfile.write.format(format).save(path_output)\r\n{code}\r\n\r\nTested on CSV file with 800 million records, 2 columns and parquet file converted from CSV above. Just copied file from HDFS to HDFS and from Ozone to Ozone. Application time is 1m 14s on HDFS and  1m 51s (+50%) on Ozone (parquet file). Ozone has default settings. (edited) \r\n{quote}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneFS is slow compared to HDFS using Spark job"
   },
   {
      "_id": "13303106",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-05-06 12:57:02",
      "description": "{code:java}\r\n-----------------------------------------------------------------Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 55.385 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestReadRetriestestPutKeyAndGetKeyThreeNodes(org.apache.hadoop.ozone.client.rpc.TestReadRetries)\u00a0 Time elapsed: 10.086 s\u00a0 <<< FAILURE!java.lang.AssertionError: expected same:<OPEN> was not:<CLOSING> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotSame(Assert.java:737) at org.junit.Assert.assertSame(Assert.java:680) at org.junit.Assert.assertSame(Assert.java:691) at org.apache.hadoop.ozone.client.rpc.TestReadRetries.testPutKeyAndGetKeyThreeNodes(TestReadRetries.java:181)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestReadRetries"
   },
   {
      "_id": "13302149",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-04-30 16:37:55",
      "description": "Currently OMFailoverProxyProvider#computeDelegationTokenService calculate the canonical token service name based on the enumeration order of the configured OM instances. An example service field can be like TS1: \"om1addr:port,om2addr:port,om3addr:port\"\r\n\r\nThis could be problematic\r\n1) clients have different omId to omRpcAddresses mappings\r\n2) configuration enumeration orders are different among clients\r\n\r\nDepend on the client configuration and enumeration order, the client may got its canonnical token service in different order like TS2: \"om2addr:port,om1addr:port,om3:addr:port\"\r\n\r\nMR/Yarn/Spark on Yarn relies on token service as key to check the UGI credential when building token cache map. When client got TS2 even though it has an OM token with TS1, client will try to collect OM token again. This will not work in YARN container (e.g., Spark on Yarn cluster mode) which may not have the kerberos ticket to fetch the token.\r\n\r\nThe proposed fix it to provide a consistent canonical token service for all OM clients in order.\r\n\r\n \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure consistent OM token service field in HA environment"
   },
   {
      "_id": "13301792",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-04-29 13:02:44",
      "description": "Upgrade Ratis to {{0.6.0-2816ea6-SNAPSHOT}} to get the fix for RATIS-840.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Upgrade Ratis to 0.6.0-2816ea6-SNAPSHOT"
   },
   {
      "_id": "13301707",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-04-29 05:15:43",
      "description": "Jaeger tracing is enabled by default, lets disable it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable Jaeger tracing by default."
   },
   {
      "_id": "13301548",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-04-28 12:07:51",
      "description": "Few parts of Ozone still use {{commons-lang}}, while most are already on {{commons-lang3}}.  Let's update those remaining usages and remove the unnecessary dependency.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Remove dependence on commons-lang"
   },
   {
      "_id": "13301545",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-28 11:50:43",
      "description": "FileStatus is a Hadoop specific class. The return type of getFileStatus OM call should be Hadoop independent and a simple POJO can be used.\r\n\r\nOzoneFileSystem can create the appropriate FileStatus implementation based on the information in this simple pojo.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneFileStatus should not extend FileStatus"
   },
   {
      "_id": "13301543",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-28 11:49:09",
      "description": "OzoneManagerProtocolClientSideTranslatorPB uses OmFailoverProxyProvider to access OM HA, but this class is not supported in Hadoop 2.x environment.\r\n\r\nIt would be better to \r\n 1. separated ProtocolClientSideTranslator from the transport layer logic\r\n 2. Remove implementation specific method from the OzoneManagerProtocol (getOMFailoverProxyProvider should be removed)\r\n 3. Use a simple OMTransport interface to handle all the connection logic in one, isolated place",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Hide OMFailoverProxyProvider usage behind an interface"
   },
   {
      "_id": "13301487",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-04-28 08:39:19",
      "description": "After\u00a0https://issues.apache.org/jira/browse/HDDS-3172, SCM now has one single rocksdb instance instead of multiple db instances.\u00a0\r\n\r\nFor running Ozone cluster, we need to address compatibility issues. One possible way is to have a side-way tool to migrate old metadata from multiple dbs to current single db.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Address compatibility issue by SCM DB instances change"
   },
   {
      "_id": "13300907",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-04-24 23:36:02",
      "description": "{code}\r\n\t\r\nSCM start failed with exception\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.hdds.conf.ConfigurationSource.getPassword(ConfigurationSource.java:112)\r\n\tat org.apache.hadoop.hdds.server.http.BaseHttpServer.getPassword(BaseHttpServer.java:348)\r\n\tat org.apache.hadoop.hdds.server.http.BaseHttpServer.loadSslConfToHttpServerBuilder(BaseHttpServer.java:311)\r\n\tat org.apache.hadoop.hdds.server.http.BaseHttpServer.newHttpServer2BuilderForOzone(BaseHttpServer.java:179)\r\n\tat org.apache.hadoop.hdds.server.http.BaseHttpServer.<init>(BaseHttpServer.java:95)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerHttpServer.<init>(StorageContainerManagerHttpServer.java:33)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:334)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:215)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Ozone start fails with NullPointerException in TLS enabled cluster"
   },
   {
      "_id": "13300468",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-04-23 07:27:46",
      "description": "It failed without any reason during a a master build\r\n\r\n[https://github.com/elek/ozone-build-results/blob/master/2020/04/22/808/it-ozone/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestSCMNodeMetrics.txt]\r\n\r\nChecking it closely, I can't see any reason to have a MiniOzoneCluster there as it can be a simple unit test. The easiest fix seems to be a conversion to real unit test.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestSCMNodeMetrics is flaky"
   },
   {
      "_id": "13300115",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-04-21 23:34:54",
      "description": "HDDS-3475 persisted transaction info into DB. This Jira is to use transactionInfo persisted to DB during OM startup. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Use persisted transaction info during OM startup in OM StateMachine"
   },
   {
      "_id": "13300063",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-04-21 19:04:02",
      "description": "This Jira is to flush the transaction information of the term and last transaction index applied during double buffer flush to OM DB.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use transactionInfo table to persist transaction information"
   },
   {
      "_id": "13300046",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-04-21 18:00:55",
      "description": "This Jira is to create a transaction info table which stores the current term and last transaction index applied to DB.\r\n*In this Jira following will be done:*\r\n1. introduce a new transaction info table which stores transactionInfo.\r\nKey = TRANSACTIONINFO\r\nvalue = currentTerm-transactionIndex\r\n2. Add new UT's for this table.\r\n3. Provide utility/helper methods to parse the transaction info table value",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create transactionInfo Table in OmMetadataManager"
   },
   {
      "_id": "13299973",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-04-21 11:44:03",
      "description": "Update ozone to latest ratis snapshot.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update to latest Ratis Snapshot 0.6.0-490b689-SNAPSHOT"
   },
   {
      "_id": "13299906",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-04-21 08:03:55",
      "description": "Currently, many third library dependencies are hardcoded in the dependency tag in the pom file. Idea is to re-organize the structure a bit by defining the jar version as a property in the pom file and reuse the defined version in the dependency tag\r\n\r\nas done in\u00a0https://issues.apache.org/jira/browse/HDDS-3468",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add third party jar versions as properties in pom.xml"
   },
   {
      "_id": "13299898",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-04-21 07:39:24",
      "description": "Currently, dependency of log4j in ozone is added as following:\r\n{code:java}\r\n<dependency>\r\n  <groupId>log4j</groupId>\r\n  <artifactId>log4j</artifactId>\r\n  <version>1.2.17</version>\r\n{code}\r\nIdea here is to add log4j.version as a property in pom.xml and reuse the same while defining the dependency.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Organize log4j dependency in pom.xml"
   },
   {
      "_id": "13299774",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-20 18:16:17",
      "description": "Even with HDDS-3456 it seems to be a good idea to run acceptance test from /mnt in the github actions environment where we have dedicated 14GB space.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use dedicated build partition for acceptance tests in github actions environment"
   },
   {
      "_id": "13299687",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-20 12:56:41",
      "description": "Ozone CSI service implement Container Storage Interface specification to provide volumes for container orchestrators such as Yarn and Kubernetes.\r\n\r\nWe don't have any acceptance test which makes easy to break the classpath / functionality.\r\n\r\nAs a first step I would create a simple smoketest to check if the csi service can be started and the identitiy service can be called.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add acceptance test to smoketest CSI service startup"
   },
   {
      "_id": "13299599",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-04-20 07:47:20",
      "description": "Apache Hadoop Ozone is a Hadoop subproject. It depends on the released Hadoop 3.2. But as Hadoop 3.2 is very rare in production, older versions should be supported to make it possible to work together with Spark, Hive, HBase and older clusters.\r\n\r\nOur current approach is using classloader based separation (ozonefs \"legacy\" jar), which has multiple problems:\r\n\r\n 1. It's quite complex and hard to debug\r\n 2. It couldn't work together with security\r\n\r\nThe issue proposes to use a different approach\r\n 1. Reduce the dependency on Hadoop (including the replacement of hadoop metrics and cleanup of the usage of configuration)\r\n 2. Create multiple version from ozonefs-client with different compile time dependency. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Support Hadoop 2.x with build-time classpath separation instead of isolated classloader"
   },
   {
      "_id": "13299239",
      "assignee": "xyao",
      "components": [],
      "created": "2020-04-17 22:09:58",
      "description": "Some of the places that need to be fixed, otherwise those http client won't be able to access the endpoints when SPNEGO is enabled on the server side. \r\n\r\nReconUtils#makeHttpCall\r\nOzoneManagerSnapshotProvider#getOzoneManagerDBSnapshot\r\n\r\nThe right API to use should be URLConnectionFactory\r\npublic URLConnection openConnection(URL url, boolean isSpnego)\r\n\r\nThe isSpnego should be based on OzoneSecurityUtil.isHttpSecurityEnabled()",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use UrlConnectionFactory to handle HTTP Client SPNEGO for ozone components"
   },
   {
      "_id": "13299075",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-17 10:15:01",
      "description": "Fix and enable TestWatchForCommit test cases",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable TestWatchForCommit test cases"
   },
   {
      "_id": "13298990",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-04-17 07:03:55",
      "description": "As OM HA is now supported in current version , We might need to update all documentation pages wherever service id is applicable and any new parameters that we might need to configure in core-site.xml for service id access for remote clusters etc. And all volume/bucket/key CLI syntaxes including service id for OM HA enabled clusters should be included in documentation",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone documentation to be revised for OM HA Support"
   },
   {
      "_id": "13298940",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-04-17 03:09:59",
      "description": "Recon currently uses Sqlite as its defacto SQL DB with an option to configure other JDBC compatible databases. However, on some platforms like the IBM power pc, this causes problems from compile time since it does not have the sqlite native driver. This task aims to change the default SQL DB used by Recon to Derby, but retains the out of the box support (no need to supply the driver) for Sqlite as well. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Switch Recon SQL DB to Derby."
   },
   {
      "_id": "13298880",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-16 18:06:42",
      "description": "In OM HA failover proxy provider, RetryInvocationHandler logs error stack trace when client tries contacting non-leader OM. Instead we can just log a message that the failover will happen and not include the stack trace.\r\n{code:java}\r\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException): OM:om2 is not the leader. Suggested leader is OM:om3. at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:186) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:174) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:110) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682), while invoking $Proxy16.submitRequest over nodeId=om2,nodeAddress=om2:9862 after 1 failover attempts. Trying to failover immediately.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove RetryInvocation INFO logging from ozone CLI output"
   },
   {
      "_id": "13298741",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-16 08:57:35",
      "description": "FSProtos.proto is copied from the Hadoop and used during the proto file generation **BUT** the types defined in FSProtos.proto are generated by Hadoop subproject.\r\n\r\nThis makes it hard to use Ozone with older Hadoop versions as the types of FSProtos are available only from Hadoop 3.x. \r\n\r\nAn easy fix is to generate our own version based on the existing FSProtos.proto",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Generate ozone specific version from type in FSProto.proto"
   },
   {
      "_id": "13298704",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-16 05:34:23",
      "description": "2020-04-14 09:44:55,089 | INFO\u00a0 | OMAudit | user=root | ip=172.25.40.156 | op=CREATE_VOLUME\r\n\r\n{admin=root, owner=hdfs, volume=hive2, creationTime=1586857495055, *quotaInBytes=1099511627776*, objectID=1792, updateID=7}\r\n\r\n| ret=SUCCESS |\r\n\r\n2020-04-14 09:58:09,634 | INFO\u00a0 | OMAudit | user=root | ip=172.25.40.156 | op=SET_QUOTA\r\n\r\n{volume=hive, *quota=536870912000*}\r\n\r\n| ret=SUCCESS |\r\n\r\n\u00a0\r\n\r\nOMVolumeSetQuotaRequest.java -> auditMap.put(OzoneConsts.QUOTA, \u00a0 \u00a0 String.valueOf(setVolumePropertyRequest.getQuotaInBytes()));\r\n\r\n\u00a0\r\n\r\nWe can use OzoneConsts.QUOTA_IN_BYTES instead of OzoneConsts.QUOTA",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Ozone audit entries could be consistent among volume creation with quota and update quota"
   },
   {
      "_id": "13298662",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-15 21:29:47",
      "description": "TimedOutTestsListener cannot be added globally because it is in hadoop-hdds-common, which is not accessible in hadoop-hdds-config (since the latter is a dependency of the former).  The listener and related classes (GenericTestUtils, etc.) should be extracted into a separate module to be used by all others.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Extract test utilities to separate module"
   },
   {
      "_id": "13298612",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-15 16:34:37",
      "description": "We currently use JaegerTracing 0.34.0. The latest is 1.2.0. We are several versions behind and should update. Note this update requires the latest version fo OpenTracing and has several breaking changes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Update JaegerTracing"
   },
   {
      "_id": "13298601",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-15 15:31:20",
      "description": "jmh dependencies used by `ozone genesis` are licensed by GPL + classpath exception\r\n\r\nIt's better to make it optional and download it on demand (and exclude it from the release package).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make jmh jar dependencies optional"
   },
   {
      "_id": "13298461",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-04-15 05:04:28",
      "description": "While reading code observed that we are generating encryption info for create directory operation. This jira is to skip the generation of encryption info.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Skip generation of encryptionkey for directory create operation"
   },
   {
      "_id": "13298435",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-04-15 02:44:51",
      "description": "Recon throws NPE while trying to get number of volumes from omMetadataManager instance (omMetadataManager.getVolumeTable().getEstimatedKeyCount()).\r\n\r\nThis happens before Recon gets its first snapshot db from OM, because only after first snapshot db, omMetadataManager initializes its tables.\r\n{code:java}\r\nUnable to get Volumes count in ClusterStateResponse.\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.ozone.recon.api.ClusterStateEndpoint.getClusterState(ClusterStateEndpoint.java:85)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:76)\r\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:148)\r\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:191)\r\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:200)\r\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:103)\r\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:493)\r\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:415)\r\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:104)\r\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:277)\r\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)\r\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:268)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:316)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:298)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:268)\r\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:289)\r\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:256)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:703)\r\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:416)\r\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)\r\n\tat com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\r\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\r\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\r\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\r\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\r\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1596)\r\n\tat org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1615)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat org.apache.hadoop.hdds.server.http.NoCacheFilter.doFilter(NoCacheFilter.java:48)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1604)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:545)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:590)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:235)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:500)\r\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)\r\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon throws NPE in clusterState endpoint"
   },
   {
      "_id": "13298366",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-04-14 18:28:55",
      "description": "The problem with generate different Data encryption key for the same file across different OM instances are that when the OM leader changes, the client may not be able to read the data correctly. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OM create key/file should not generate different data encryption key during validateAndUpdateCache"
   },
   {
      "_id": "13298364",
      "assignee": "xyao",
      "components": [],
      "created": "2020-04-14 18:20:24",
      "description": "Currently, the admin operation check are not sent to authorizer plugin. As a result, the audit log are not shown up in plug-in like ranger authorizer. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Delegate admin ACL checks to Ozone authorizer plugin"
   },
   {
      "_id": "13298346",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-14 16:56:48",
      "description": "The current audit log does not include the bucket encryption key information. This ticket is opened to add that. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Add bucket encryption key info to bucket create audit log"
   },
   {
      "_id": "13298260",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-04-14 12:34:34",
      "description": "See the design doc for more details: \r\n\r\nhttps://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/docs/content/design/ozone-volume-management.md",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "backward-incompatible",
         "imcompatible",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Simplify S3 -> Ozone volume mapping"
   },
   {
      "_id": "13297884",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-12 15:02:24",
      "description": "OzoneManager starts 2 OzoneManagerDoubleBuffer for HA clusters. In the following example for 3 OM HA instances, 6 OzoneManagerDoubleBuffer instances were created.\r\n{code}\r\n\u279c  chaos-2020-04-12-20-21-11-IST grep canFlush stack1\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.canFlush(OzoneManagerDoubleBuffer.java:344)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager starts 2 OzoneManagerDoubleBuffer for HA clusters"
   },
   {
      "_id": "13297772",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-11 14:48:55",
      "description": "OzoneManager group init failed because of incorrect snapshot directory location\r\n\r\n{code}\r\n2020-04-11 20:07:57,180 [pool-59-thread-1] INFO  server.RaftServerConfigKeys (ConfUtils.java:logGet(44)) - raft.server.storage.dir = [/tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis] (custom)\r\n2020-04-11 20:07:57,180 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(191)) - omNode-3: found a subdirectory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/snapshot\r\n2020-04-11 20:07:57,181 [pool-59-thread-1] WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(197)) - omNode-3: Failed to initialize the group directory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/snapshot.  Ignoring it\r\njava.lang.IllegalArgumentException: Invalid UUID string: snapshot\r\n        at java.util.UUID.fromString(UUID.java:194)\r\n        at org.apache.ratis.server.impl.RaftServerProxy.lambda$null$0(RaftServerProxy.java:192)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\r\n        at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\r\n        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\r\n        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n        at org.apache.ratis.server.impl.RaftServerProxy.lambda$initGroups$1(RaftServerProxy.java:189)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\r\n        at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)\r\n        at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)\r\n        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n        at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)\r\n        at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)\r\n        at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\n        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)\r\n        at org.apache.ratis.server.impl.RaftServerProxy.initGroups(RaftServerProxy.java:186)\r\n        at org.apache.ratis.server.impl.ServerImplUtils.newRaftServer(ServerImplUtils.java:41)\r\n        at org.apache.ratis.server.RaftServer$Builder.build(RaftServer.java:76)\r\n        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.<init>(OzoneManagerRatisServer.java:277)\r\n        at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.newOMRatisServer(OzoneManagerRatisServer.java:328)\r\n        at org.apache.hadoop.ozone.om.OzoneManager.initializeRatisServer(OzoneManager.java:1249)\r\n        at org.apache.hadoop.ozone.om.OzoneManager.restart(OzoneManager.java:1190)\r\n        at org.apache.hadoop.ozone.MiniOzoneHAClusterImpl.restartOzoneManager(MiniOzoneHAClusterImpl.java:229)\r\n        at org.apache.hadoop.ozone.failure.Failures$OzoneManagerRestartFailure.lambda$fail$0(Failures.java:112)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\r\n        at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)\r\n        at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)\r\n        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n        at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)\r\n        at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)\r\n        at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\n        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n        at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)\r\n        at org.apache.hadoop.ozone.failure.Failures$OzoneManagerRestartFailure.fail(Failures.java:109)\r\n        at org.apache.hadoop.ozone.failure.FailureManager.fail(FailureManager.java:58)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n2020-04-11 20:07:57,182 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:lambda$null$0(191)) - omNode-3: found a subdirectory /tmp/chaos-2020-04-11-20-05-25-IST/MiniOzoneClusterImpl-80aafc97-1b12-4bc0-9baf-7f42185b0995/omNode-3/ratis/b870c9eb-edfb-36b5-b758-d62218d261de\r\n2020-04-11 20:07:57,183 [pool-59-thread-1] INFO  impl.RaftServerProxy (RaftServerProxy.java:addNew(89)) - omNode-3: addNew group-D62218D261DE:[omNode-3:localhost:12408, omNode-1:localhost:12396, omNode-2:localhost:12402] returns group-D62218D261DE:java.util.concurrent.CompletableFuture@2fc3d657[Not completed]\r\n2020-04-11 20:07:57,183 [pool-1382-thread-1] INFO  impl.RaftServerImpl (RaftServerImpl.java:<init>(97)) - omNode-3: new RaftServerImpl for group-D62218D261DE:[omNode-3:localhost:12408, omNode-1:localhost:12396, omNode-2:localhost:12402] with OzoneManagerStateMachine:uninitialized\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager group init failed because of incorrect snapshot directory location"
   },
   {
      "_id": "13297648",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-04-10 17:04:28",
      "description": "\r\n{code:java}\r\njavax.xml.bind.UnmarshalException: unexpected element (uri:\"\", local:\"CompleteMultipartUpload\"). Expected elements are <{http://s3.amazonaws.com/doc/2006-03-01/}CompleteMultipartUpload>,<{http://s3.amazonaws.com/doc/2006-03-01/}Part>\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.handleEvent(UnmarshallingContext.java:744)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:262)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportError(Loader.java:257)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.Loader.reportUnexpectedChildElement(Loader.java:124)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext$DefaultRootLoader.childElement(UnmarshallingContext.java:1149)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:574)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:556)\r\n        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:168)\r\n        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:509)\r\n        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:374)\r\n        at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl$NSContentDriver.scanRootElementHook(XMLNSDocumentScannerImpl.java:613)\r\n        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3132)\r\n        at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:852)\r\n\r\n{code}\r\n\r\n\r\nIt seems http://s3.amazonaws.com/doc/2006-03-01/ is expected in the element.\r\nBut in class CompleteMultipartUploadRequest,  namespace http://s3.amazonaws.com/doc/2006-03-01/ is not defined here.\r\n\r\nReported by [~sammichen]\r\n\r\n\r\n\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3A failing complete multipart upload with Ozone S3"
   },
   {
      "_id": "13297558",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-10 07:40:40",
      "description": "{code:title=https://github.com/apache/hadoop-ozone/pull/783/checks?check_run_id=576054872}\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 18.987 s <<< FAILURE! - in org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser\r\n[ERROR] testRatisLogParsing(org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser)  Time elapsed: 18.882 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  ...\r\n  at org.apache.hadoop.ozone.dn.ratis.TestDnRatisLogParser.testRatisLogParsing(TestDnRatisLogParser.java:75)\r\n{code}\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/pull/846/checks?check_run_id=608153672}\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.606 s <<< FAILURE! - in org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser\r\n[ERROR] testRatisLogParsing(org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser)  Time elapsed: 30.476 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n  at org.apache.hadoop.ozone.om.parser.TestOMRatisLogParser.testRatisLogParsing(TestOMRatisLogParser.java:112)\r\n{code}\r\n\r\nCC [~msingh]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestDnRatisLogParser and TestOMRatisLogParser"
   },
   {
      "_id": "13297513",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-04-10 00:15:12",
      "description": "hadoop-ozone-filesystem-lib-current jar includes webapps folder of hdds datanode.\u00a0\r\n\r\nThis should not be included in the filesystem jar.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone filesystem jar should not include webapps folder"
   },
   {
      "_id": "13297252",
      "assignee": "xyao",
      "components": [],
      "created": "2020-04-09 03:17:12",
      "description": "HDDS-3319 added a new method to get RPCClient based on the omserviceID in OzoneToken. However, some om.service.id compare logic is based on a Hadoop Configuration object. As a result, the om configuration may not be loaded if the Configuration is a Hadoop Configuration only (e.g., RM). ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure OzoneConfiguration is initialized in OzoneClientFactory#getOzoneClient"
   },
   {
      "_id": "13297219",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-04-08 21:03:41",
      "description": "In few CI runs, ozone-security robot tests are timing out at 5 minutes.\u00a0\r\n[https://github.com/apache/hadoop-ozone/pull/784/checks?check_run_id=569548444]\r\n\r\nWe should increase the timeout to avoid this.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Increase test timeout for ozonesecure-security robot tests"
   },
   {
      "_id": "13297212",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-08 20:40:37",
      "description": "{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/571992849}\r\n2020-04-08T20:30:49.0510599Z [ERROR] Tests run: 22, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.669 s <<< FAILURE! - in org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer\r\n2020-04-08T20:30:49.0535678Z [ERROR] testContainerImportExport[1](org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.079 s  <<< ERROR!\r\n2020-04-08T20:30:49.0552584Z java.io.IOException: request to write '4096' bytes exceeds size in header of '19906' bytes for entry 'db/LOG'\r\n2020-04-08T20:30:49.0572746Z \tat org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.write(TarArchiveOutputStream.java:385)\r\n2020-04-08T20:30:49.0572897Z \tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2147)\r\n2020-04-08T20:30:49.0582579Z \tat org.apache.commons.io.IOUtils.copy(IOUtils.java:2102)\r\n2020-04-08T20:30:49.0593659Z \tat org.apache.commons.io.IOUtils.copyLarge(IOUtils.java:2123)\r\n2020-04-08T20:30:49.0603340Z \tat org.apache.commons.io.IOUtils.copy(IOUtils.java:2078)\r\n2020-04-08T20:30:49.0613502Z \tat org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includeFile(TarContainerPacker.java:225)\r\n2020-04-08T20:30:49.0631425Z \tat org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.includePath(TarContainerPacker.java:215)\r\n2020-04-08T20:30:49.0637525Z \tat org.apache.hadoop.ozone.container.keyvalue.TarContainerPacker.pack(TarContainerPacker.java:155)\r\n2020-04-08T20:30:49.0648504Z \tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:549)\r\n2020-04-08T20:30:49.0659852Z \tat org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testContainerImportExport(TestKeyValueContainer.java:233)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in testContainerImportExport"
   },
   {
      "_id": "13297060",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-08 12:33:36",
      "description": "The test which is failed:\r\n\r\nTestSCMNodeManager\r\n\r\nThe end of the log is:\r\n\r\n{code}\r\n2020-04-08 10:49:44,544 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(84)) - Error on execution message 19844615-0d70-4172-8c34-96e5b7295ef2{ip: 196.189.243.187, host: localhost-196.189.243.187, networkLocation: /default-rack, certSerialId: null}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.finalizeAndDestroyPipeline(SCMPipelineManager.java:380)\r\n        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:63)\r\n        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:38)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n2020-04-08 10:49:44,544 INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=fd1f9e92-2f90-43e7-8406-94ba6ac356b0, PipelineID=8d380e3c-b632-4bda-aa7a-554774fba09d]\r\n2020-04-08 10:49:44,544 INFO  pipeline.SCMPipelineManager (SCMPipelineManager.java:finalizeAndDestroyPipeline(373)) - Destroying pipeline:Pipeline[ Id: fd1f9e92-2f90-43e7-8406-94ba6ac356b0, Nodes: 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-04-08T10:49:37.441Z]\r\n2020-04-08 10:49:44,544 INFO  pipeline.PipelineStateManager (PipelineStateManager.java:finalizePipeline(120)) - Pipeline Pipeline[ Id: fd1f9e92-2f90-43e7-8406-94ba6ac356b0, Nodes: 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:CLOSED, leaderId:null, CreationTimestamp2020-04-08T10:49:37.441Z] moved to CLOSED state\r\n2020-04-08 10:49:44,544 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(84)) - Error on execution message 0914e56d-c7f8-4e0a-8fd1-845a9806172b{ip: 57.46.156.17, host: localhost-57.46.156.17, networkLocation: /default-rack, certSerialId: null}\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.finalizeAndDestroyPipeline(SCMPipelineManager.java:380)\r\n        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:63)\r\n        at org.apache.hadoop.hdds.scm.node.StaleNodeHandler.onMessage(StaleNodeHandler.java:38)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:81)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n2020-04-08 10:49:44,544 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$close$4(208)) - Send pipeline:PipelineID=e0e155c6-9fbe-46a7-b742-e805ea9baacf close command to datanode 30a24b04-1289-4c30-a28a-034edfe29e3d\r\n2020-04-08 10:49:44,545 WARN  events.EventQueue (EventQueue.java:fireEvent(151)) - Processing of TypedEvent{payloadType=CommandForDatanode, name='Datanode_Command'} is skipped, EventQueue is not running\r\n2020-04-08 10:49:44,544 INFO  node.StaleNodeHandler (StaleNodeHandler.java:onMessage(58)) - Datanode 59bdd26b-05da-47d1-8c3f-8350d55d7299{ip: 248.147.58.17, host: localhost-248.147.58.17, networkLocation: /default-rack, certSerialId: null} moved to stale state. Finalizing its pipelines [PipelineID=17b032b7-b9c4-41eb-bba6-50106881886d, PipelineID=60de1ca6-4115-415b-bbf1-06b86113df94]\r\n2020-04-08 10:49:44,576 WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.\r\n2020-04-08 10:49:44,579 WARN  server.ServerUtils (ServerUtils.java:getScmDbDir(148)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.\r\n2020-04-08 10:49:44,579 WARN  db.DBDefinition (DBDefinition.java:createDBStoreBuilder(63)) - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "TriagePending",
         "flaky-test",
         "ozone-flaky-test"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent test failure related to a race conditon during PipelineManager close"
   },
   {
      "_id": "13297051",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-04-08 11:49:18",
      "description": "Add a checkstyle rule to reject import from shaded packages.  This would help avoid accidentally using shaded transitive dependencies.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add check for import from shaded package"
   },
   {
      "_id": "13296898",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-04-07 19:19:59",
      "description": "Currently, client keeps retrying to connect to leader OM in a tight loop and fails after configured number of retires/ failovers.\r\nIf the Leader OM is not ready, the client can timeout quickly. So, we should instead try each OM once, and then wait before retrying again.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add wait time between client retries to OM"
   },
   {
      "_id": "13296879",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-07 18:03:53",
      "description": "This Jira is to improve the OM HA replay scenario.\r\nAttached the design document which discusses about the proposal and issue in detail.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM HA replay optimization"
   },
   {
      "_id": "13296835",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-07 14:05:26",
      "description": "Similar to HDDS-3312 we can exclude dependencies coming from hadoop-ozone. (Eg. curator / zookeeper.)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove unnecessary transitive hadoop-common dependencies on server side."
   },
   {
      "_id": "13296109",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-04-03 20:05:41",
      "description": "Ozone shell commands such as `ozone admin container create` does not print any information on failure which makes it very difficult to debug the underlying issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone admin shell commands do not print or log exceptions on failures"
   },
   {
      "_id": "13296055",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-04-03 15:44:18",
      "description": "{{checkstyle.sh}} fails\u00a0for any new intermediate module (used by other modules) or new version (eg. bump from 0.5.0-SNAPSHOT to 0.6.0-SNAPSHOT).  It does not perform a complete build and tries to fetch these artifacts from Maven repo.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Checkstyle fails for new modules/versions"
   },
   {
      "_id": "13296046",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-04-03 15:03:56",
      "description": "Ozone Shell is currently part of the {{ozone-manager}} module.  I think it would be more at home in the {{tools}} module.\r\n\r\nAlso rename the package name {{ozShell}} to {{shell}}, as package names should be all lowercase.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move Ozone Shell from ozone-manager to tools"
   },
   {
      "_id": "13296028",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-03 13:25:31",
      "description": "If we follow the naming convention suggested by github:\r\n\r\nhttps://help.github.com/en/github/building-a-strong-community/setting-guidelines-for-repository-contributors\r\n\r\nThe contribution guideline will be displayed at multiple locations (eg. when a new contributor create a new PR).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Rename CONTRIBUTION.md to CONTRIBUTING.md"
   },
   {
      "_id": "13295956",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-04-03 06:56:50",
      "description": "Some useful metrics for tracking how Recon's OM DB requests are going on.\r\n\r\n{code}\r\n  @Metric(about = \"Number of OM snapshot requests made by Recon.\")\r\n  @Metric(about = \"Number of OM snapshot requests that failed.\")\r\n  @Metric(about = \"OM snapshot request latency\")\r\n  @Metric(about = \"Number of OM delta requests made by Recon.\")\r\n  @Metric(about = \"Number of OM delta requests that failed.\")\r\n  @Metric(about = \"OM delta request latency\")\r\n  @Metric(about = \"Total number of updates got through OM delta request\")\r\n  @Metric(about = \"Average number of updates got per OM delta request\")\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Metrics for Recon OzoneManager DB sync."
   },
   {
      "_id": "13295920",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-04-03 00:51:27",
      "description": "The Guice bindings required for some of the unit tests are hard to follow and need some cleanup.\r\n\r\n\r\n*Work done*\r\n\r\n* Created class to setup a recon test injector, with any combination of sub modules that are specified.\r\n* Created class that provides a Recon SQL DB with all the tables created, and APIs to access the DAOs easily.\r\n* Cleaned up injector usage in Recon tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon unit tests cleanup."
   },
   {
      "_id": "13295868",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-04-02 18:18:13",
      "description": "On receiving NotLeaderException from OM server, we should failover to the next OM in the list instead of the suggested Leader. This will ensure that all the OMs are contacted in a round robin way. Failing always to the suggested leader is not robust.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM Client failover to next OM on NotLeaderException"
   },
   {
      "_id": "13295830",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-04-02 16:10:10",
      "description": "Overview page should auto refresh to fetch updated data and alerts if any.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon UI: All the pages should auto reload"
   },
   {
      "_id": "13295792",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-02 13:07:19",
      "description": "The current s3bucket -> ozone volume/bucket mapping is very confusing. Let's improve it.\r\n\r\n(See the design docs for more details).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Simplify s3bucket -> ozone volume/bucket mapping"
   },
   {
      "_id": "13295789",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-02 12:36:32",
      "description": "{code}\r\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 666.209 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\r\ntestDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 640.745 s  <<< ERROR!\r\njava.io.IOException: INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:229)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:402)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:347)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:458)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:509)\r\n        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)\r\n        at org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:225)\r\n{code}\r\n\r\nI learned this from [~shashikant]\r\n\r\nbq. we kill a datanode after some IO, SCM is out of safe mode by then . SCM takes time to destroy a pipeline and form a new one\r\nbq. With only minimal set of dn in cluster, if we want to write again, we need to wait for a new pipeline to open up before writing again\r\n\r\nWill turn off this test until the fix.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "TriagePending",
         "ozone-flaky-test",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestDeleteWithSlowFollower is still flaky"
   },
   {
      "_id": "13295767",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2020-04-02 11:17:07",
      "description": "In HDDS-1659 a new generic way was introduced  to make the design docs more public (collect them as part of the documentation whether they are written as part of the docs or as separated pdf/google docs).\r\n\r\nThis patch:\r\n \r\n * List the design doc as part of the documentation\r\n * Adds link to some of the existing / earlier design docs\r\n\r\nI believe that this can be a good extension to the documentation page as some of the low level internals are discussed only in design docs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "List design docs as part of the documentation page"
   },
   {
      "_id": "13295698",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-04-02 06:22:29",
      "description": "Right now, when the future fails with an exception, we send that exception to the client, and retry with a new server. but when using ratis server when resource unavailable exception future fails with exceptionally. So, in this case we need to wrap the exception and retry to the same server with some retry policy like MultiLinearRandomRetry or some retry policy.\r\n\r\n{code:java}\r\ntry {\r\n raftClientReply = server.submitClientRequestAsync(raftClientRequest)\r\n          .get();\r\n    } catch (Exception ex) {\r\n      throw new ServiceException(ex.getMessage(), ex);\r\n    }\r\n{code}\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Resource Unavailable exception in OM HA"
   },
   {
      "_id": "13295660",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-04-02 00:01:08",
      "description": "OM Client fails with StringIndexOutOfBoundsException:\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\n2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: Caused by: java.io.IOException: Couldn't create RpcClient protocol\r\n2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:199)\r\n2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:175)\r\n2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:86)\r\n2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:168)\r\n2020-03-31T09:53:03,673 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:158)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3391)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:158)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3451)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3419)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:513)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:558)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \t... 26 more\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat java.base/java.lang.String.substring(String.java:1841)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.computeDelegationTokenService(OMFailoverProxyProvider.java:207)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:84)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:208)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:154)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:192)\r\n2020-03-31T09:53:03,674 INFO  [Thread-16] jdbc.TestDriver: \t... 39 more\r\n\r\n {code}\r\n\u00a0\r\n\r\nThis can happen for two reasons:\r\n # If user has configured OM addresses incorrectly which are not resolvable to any host.\r\n # In Docker world when the client starts if all OM containers are down, then OM addresses will be unresolvable.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM Client fails with StringIndexOutOfBoundsException"
   },
   {
      "_id": "13295610",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-04-01 19:23:39",
      "description": "_BackgroundPipelineCreator_\u00a0keeps creating pipelines of configured Replication type and all available Replication factors until some exception occurs while creating the pipeline such as no more available nodes.\r\n\r\nWhen Replication Type is set to STAND_ALONE, we do not check if a DN has already been used to create a pipeline of same factor or not and keep reusing the same DNs to create new pipelines. This causes the pipeline creation to happen in an infinite loop.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "StandAlone Pipelines are created in an infinite loop"
   },
   {
      "_id": "13295584",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-04-01 17:03:51",
      "description": "Prometheus does not support targets which have Kerberos SPNEGO based authentication. Hence, on a secure cluster, if we have prometheus endpoint enabled, it makes sense to skip the authentication filter for it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Prometheus endpoint should have an option to be configured with Token based authentication. "
   },
   {
      "_id": "13295558",
      "assignee": "xyao",
      "components": [],
      "created": "2020-04-01 15:23:07",
      "description": "Currently, only the getDelegationtoken has been updated with the proper HA client proxy setup. renew and cancel will fail in OM HA setup.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle HA for BasicOzoneClientAdapterImpl$Renewer#renew/cancel()"
   },
   {
      "_id": "13295554",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-01 15:00:56",
      "description": "As it's discussed during the last community meeting we can close pending pull requests after 30 days. /close command can make it easier.\r\n\r\nWe can also add pending label to the pending pull requests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support /close command in the Github comments"
   },
   {
      "_id": "13295517",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-01 11:40:03",
      "description": "From the last master build:\r\n\r\nhttps://github.com/apache/hadoop-ozone/runs/550746880?check_suite_focus=true\r\n\r\nCheckstyle is failed due to a maven error:\r\n\r\n{code}\r\n[ERROR] Failed to execute goal on project hadoop-hdds-common: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-config:jar:0.6.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]\r\n[ERROR] Failed to execute goal on project hadoop-hdds-client: Could not resolve dependencies for project org.apache.hadoop:hadoop-hdds-client:jar:0.6.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT, org.apache.hadoop:hadoop-hdds-config:jar:0.6.0-SNAPSHOT: Could not find artifact org.apache.hadoop:hadoop-hdds-common:jar:0.6.0-SNAPSHOT in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]\r\n{code}\r\n\r\nBut it remained green.\r\n\r\nWe should fail the check if the maven run couldn't succeed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Checkstyle check fails silently in case of mvn related errors"
   },
   {
      "_id": "13295515",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-04-01 11:26:06",
      "description": "SCM is built from loosely coupled components which communicate with async event with each other.\r\n\r\nUsing the same abstraction (EventQueue) has the benefit that we can use the same visibility / testing tools such as the 'ozone insight' definition (which makes visible all the messages) or the test handler (which can wait until all the event queue messages are processed) \r\n\r\nDuring the review of HDDS-3221 it was suggested (by me) to use the EventQueue instead of the new SafeModeNotification interface. \r\n\r\nThere was only one counter argument against it:\r\n\r\nbq. I personally find the event queue logic hard to follow due to its async nature (you cannot just follow method calls in the IDE). Its not bad, but more difficult when you don't yet understand it, while registering some instances to be notified is easy to follow in an IDE. This is of course a subjective opinion :)\r\n\r\nI respect this opinion, but I think it's better to use one abstraction and a consistent architecture inside one component (together with all the existing limitations). The EventQueue is not the only one possible solution, but an existing one. We can either design and switch to a new one or use the existing one.\r\n\r\nIn this patch I would like to show how the previous listener interface can be replaced by the EventQueue.\r\n\r\nIt (hopefully) shows that this is not complex, and in fact can help us to decouple different component from each other    ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use EventQueue for delayed/immediate safe mode rule notification"
   },
   {
      "_id": "13295478",
      "assignee": "elek",
      "components": [],
      "created": "2020-04-01 09:29:04",
      "description": "hadoop-ozone/common and hadoop-hdds/common projects are common between *client and server*. Therefore we should remove any server side utilities / dependencies from them as they would be added to the client classpath which makes harder the Ozone client / ozonefs adoption.\r\n\r\n * hadoop-hdds should depend on a minimal set of hadoop dependencies (can be managed a separated technical project / pom.xml)\r\n* code shared between server side projects (and not with the client) should be moved to the framework\r\n* OM related code should be moved to the ozone-manager instead of ozone/common",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Remove sever-side dependencies from hdds/ozone-common"
   },
   {
      "_id": "13295453",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-04-01 08:13:02",
      "description": "Logs collected for OM HA acceptance test have no information about what's happening during the test.  Only the output of {{om --init}} is present:\r\n\r\n{code:title=last few lines of log}\r\n...\r\nom1_1       | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-0d8a21ae-1a87-400b-bba3-d61cd4c01ce4\r\nom1_1       | 2020-04-01 03:27:44 INFO  OzoneManagerStarter:51 - SHUTDOWN_MSG:\r\nom1_1       | /************************************************************\r\nom1_1       | SHUTDOWN_MSG: Shutting down OzoneManager at 2f4e39e19fea/172.21.0.6\r\nom1_1       | ************************************************************/\r\nom1_1       | Enabled profiling in kernel\r\n{code}\r\n\r\nThus we have no information about why the test is failing intermittently:\r\n\r\nhttps://github.com/apache/hadoop-ozone/runs/549544110",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM logs not available for OM HA acceptance test"
   },
   {
      "_id": "13295319",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-03-31 17:52:25",
      "description": "Update Ratis snapshot version to {{7c5b30d}}, which includes RATIS-816, required for HDDS-3023.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update Ratis snapshot"
   },
   {
      "_id": "13294858",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-29 18:35:37",
      "description": "See the attachments.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "flaky-test",
         "ozone-flaky-test"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "TestFailureHandlingByClient.testDatanodeExclusionWithMajorityCommit is intermittent"
   },
   {
      "_id": "13294584",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-03-28 01:14:13",
      "description": "Ozone admin should always have read/write acl permission to ozone objects. This way, if owner incorrectly set the acls and lose access, admin can always help to get acces back. \r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone admin should always have read/write ACL permission on ozone objects"
   },
   {
      "_id": "13294562",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-03-27 22:06:48",
      "description": "Even when a user is added to ozone.administrators,\u00a0\u00a0Permission Denied error is thrown while creating a new volume.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone admins getting Permission Denied error while creating volume "
   },
   {
      "_id": "13294533",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-03-27 18:37:06",
      "description": "steps taken :\r\n\r\n1)\u00a0Mounted noise injection FUSE on all datanodes.\r\n\r\n2) Write a key ( multi blocks)\r\n\r\n3) Select one of the container ids ,\u00a0 inject error on 2 container replicas for that container id.\r\n\r\n4) Run GET key operation.\r\n\r\nGET key operation fails intermittenly.\r\n\r\nError seen :\r\n\r\n-------------\r\n\r\n\u00a0\r\n{noformat}\r\n20/03/27 18:30:40 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-xceiverclientmetrics.properties,hadoop-metrics2.properties\r\nE 20/03/27 18:30:40 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\nE 20/03/27 18:30:40 INFO impl.MetricsSystemImpl: XceiverClientMetrics metrics system started\r\nE 20/03/27 18:31:12 ERROR scm.XceiverClientGrpc: Failed to execute command cmdType: ReadChunk\r\nE traceID: \"f80a51eaec481a1c:cbb8e92869015a53:f80a51eaec481a1c:0\"\r\nE containerID: 67\r\nE datanodeUuid: \"96101390-2446-40e6-a54e-36e170497e57\"\r\nE readChunk {\r\nE blockID {\r\nE containerID: 67\r\nE localID: 103896435892617248\r\nE blockCommitSequenceId: 1010\r\nE }\r\nE chunkData {\r\nE chunkName: \"103896435892617248_chunk_28\"\r\nE offset: 113246208\r\nE len: 4194304\r\nE checksumData {\r\nE type: CRC32\r\nE bytesPerChecksum: 1048576\r\nE checksums: \"\\034\\376\\313\\031\"\r\nE checksums: \";U\\225\\037\"\r\nE checksums: \"\\327m\\332.\"\r\nE checksums: \"|\\307\\004E\"\r\nE }\r\nE }\r\nE }\r\nE on the pipeline Pipeline[ Id: bce6316c-9690-452b-80e3-0f3590533444, Nodes: 96101390-2446-40e6-a54e-36e170497e57{ip: 172.27.111.129, host: quasar-olrywk-3.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}3e85204d-2399-43b5-952a-55b837eb4c1d{ip: 172.27.100.0, host: quasar-olrywk-1.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}5af0340a-6fee-4ce8-9f68-37fa35566a5a{ip: 172.27.73.0, host: quasar-olrywk-9.quasar-olrywk.root.hwx.site, networkLocation: /default-rack, certSerialId: null}, Type:STAND_ALONE, Factor:THREE, State:OPEN, leaderId:96101390-2446-40e6-a54e-36e170497e57, CreationTimestamp2020-03-27T03:36:51.880Z].\r\nE Unexpected OzoneException: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 84603913ns. [remote_addr=/172.27.73.0:9859]]{noformat}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "fault_injection"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "read operation failing when two container replicas are corrupted"
   },
   {
      "_id": "13294514",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-03-27 17:05:07",
      "description": "Hadoop 3.3.1 is coming out soon. We should start testing Ozone on Hadoop 3.3",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Hadoop 3.3"
   },
   {
      "_id": "13294510",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-03-27 16:44:16",
      "description": "steps taken :\r\n--------------\r\n1. In OM HA environment, shutdown both OM followers.\r\n2. Start PUT key operation.\r\n\r\nPUT key operation is hung.\r\n\r\nCluster details : https://quasar-vwryte-1.quasar-vwryte.root.hwx.site:7183/cmf/home\r\n\r\nSnippet of OM log on LEADER:\r\n\r\n\r\n{code:java}\r\n2020-03-24 04:16:46,249 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:46,249 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:46,250 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:46,250 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:46,750 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:46,750 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:46,750 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:46,750 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:47,250 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:47,251 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:47,251 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:47,251 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:47,751 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:47,751 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:47,752 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:47,752 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:48,252 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:48,252 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:48,252 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:48,252 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:48,752 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:48,752 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:48,753 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:48,753 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:49,254 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:49,254 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:49,254 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:49,254 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:49,754 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:49,754 WARN org.apache.ratis.grpc.server.GrpcLogAppender: om1@group-9F198C4C3682->om3-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception\r\n2020-03-24 04:16:49,754 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om2: nextIndex: updateUnconditionally 360 -> 359\r\n2020-03-24 04:16:49,754 INFO org.apache.ratis.server.impl.FollowerInfo: om1@group-9F198C4C3682->om3: nextIndex: updateUnconditionally 360 -> 359\r\n{code}\r\n\r\nReported by [~nilotpalnandi]\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Write operation when both OM followers are shutdown"
   },
   {
      "_id": "13294235",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-03-26 16:06:10",
      "description": "2020-03-26 21:26:48,869 [pool-326-thread-2] INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status 1: java.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.\r\ngrpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now\r\n\r\n{code}\r\n2020-03-26 21:26:48,866 [pool-326-thread-2] ERROR loadgenerators.LoadExecutors (LoadExecutors.java:load(64)) - FileSystem LOADGEN: null Exiting due to exception\r\njava.io.IOException: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now\r\n        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:359)\r\n        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithTraceIDAndRetry(XceiverClientGrpc.java:281)\r\n        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommand(XceiverClientGrpc.java:259)\r\n        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getBlock(ContainerProtocolCalls.java:119)\r\n        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.getChunkInfos(BlockInputStream.java:199)\r\n        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.initialize(BlockInputStream.java:133)\r\n        at org.apache.hadoop.hdds.scm.storage.BlockInputStream.read(BlockInputStream.java:254)\r\n        at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:197)\r\n        at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:63)\r\n        at java.io.DataInputStream.read(DataInputStream.java:100)\r\n        at org.apache.hadoop.ozone.utils.LoadBucket$ReadOp.doPostOp(LoadBucket.java:205)\r\n        at org.apache.hadoop.ozone.utils.LoadBucket$Op.execute(LoadBucket.java:121)\r\n        at org.apache.hadoop.ozone.utils.LoadBucket$ReadOp.execute(LoadBucket.java:180)\r\n        at org.apache.hadoop.ozone.utils.LoadBucket.readKey(LoadBucket.java:82)\r\n        at org.apache.hadoop.ozone.loadgenerators.FilesystemLoadGenerator.generateLoad(FilesystemLoadGenerator.java:54)\r\n        at org.apache.hadoop.ozone.loadgenerators.LoadExecutors.load(LoadExecutors.java:62)\r\n        at org.apache.hadoop.ozone.loadgenerators.LoadExecutors.lambda$startLoad$0(LoadExecutors.java:78)\r\n        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now\r\n        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\r\n        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\r\n        at org.apache.hadoop.hdds.scm.XceiverClientGrpc.sendCommandWithRetry(XceiverClientGrpc.java:336)\r\n        ... 20 more\r\nCaused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: ClientCall started after deadline exceeded: -4.330590725s from now\r\n        at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:533)\r\n        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:442)\r\n        at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\r\n       at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:700)\r\n        at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:399)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:510)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:66)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:630)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$700(ClientCallImpl.java:518)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInternal(ClientCallImpl.java:692)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:681)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "MiniOzoneChaosCluster exits because of deadline exceeding"
   },
   {
      "_id": "13294147",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-26 09:14:04",
      "description": "{{ozonesecure-mr}} acceptance test is failing with {{No space available in any of the local directories.}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozonesecure-mr test fails due to lack of disk space"
   },
   {
      "_id": "13294048",
      "assignee": "xyao",
      "components": [],
      "created": "2020-03-26 00:22:44",
      "description": "After HDDS-2950, we change to use ozone's own initializer defined by ozone.http.filter.initializers instead the one configured with hadoop.http.filter.initializers.\r\n\r\nThe FilterInitializer interface was also forked from hadoop common  that prevents us from using org.apache.hadoop.security.AuthenticationFilterInitializer with the following error. \r\n\r\nThis ticket is opened to fix it. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "ozone.http.filter.initializers can't be set properly for SPNEGO auth"
   },
   {
      "_id": "13294045",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-25 23:39:20",
      "description": "We have seen in some CI runs that the acceptance test suit is getting cancelled as it runs for more than 6 hours. Because of this, the test results and logs are also not saved.\u00a0\r\n\r\nThis Jira aims to add a 5 minute timeout to all robot tests. In case some tests require more time, we can update the timeout. This would help to isolate the test which could be causing the whole acceptance test suit to time out.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add timeouts to all robot tests"
   },
   {
      "_id": "13294040",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-03-25 23:09:33",
      "description": "Ozone BaseHTTPServer tries to start HTTP server with Spnego Principal and Keytab even if\u00a0ozone.security.enabled flag is set to false. It should honor\u00a0ozone.security.enabled flag and start the server accordingly.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone BaseHTTPServer should honor ozone.security.enabled config"
   },
   {
      "_id": "13293713",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-25 04:34:34",
      "description": "Discovered by [~bharat]\u00a0when testing 0.5.0-beta RC2.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nissue when running hdfs commands on hadoop 27\r\ndocker-compose. I see the same test failing when running the smoke test.\r\n\r\n\r\n$ docker exec -it c7fe17804044 bash\r\n\r\nbash-4.4$ hdfs dfs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/kk\r\n\r\n2020-03-22 04:40:14 WARN\u00a0 NativeCodeLoader:60 - Unable to load\r\nnative-hadoop library for your platform... using builtin-java classes where\r\napplicable\r\n\r\n2020-03-22 04:40:15 INFO\u00a0 MetricsConfig:118 - Loaded properties from\r\nhadoop-metrics2.properties\r\n\r\n2020-03-22 04:40:16 INFO\u00a0 MetricsSystemImpl:374 - Scheduled Metric snapshot\r\nperiod at 10 second(s).\r\n\r\n2020-03-22 04:40:16 INFO\u00a0 MetricsSystemImpl:191 - XceiverClientMetrics\r\nmetrics system started\r\n\r\n-put: Fatal internal error\r\n\r\njava.lang.NullPointerException: client is null\r\n\r\nat java.util.Objects.requireNonNull(Objects.java:228)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.XceiverClientRatis.getClient(XceiverClientRatis.java:201)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:227)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:305)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:315)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:599)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:452)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:463)\r\n\r\nat\r\norg.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:486)\r\n\r\nat\r\norg.apache.hadoop.ozone.[client.io|http://client.io/].BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:144)\r\n\r\nat\r\norg.apache.hadoop.ozone.client.io.KeyOutputStream.handleStreamAction(KeyOutputStream.java:481)\r\n\r\nat\r\norg.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:455)\r\n\r\nat\r\norg.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:508)\r\n\r\nat\r\norg.apache.hadoop.fs.ozone.OzoneFSOutputStream.close(OzoneFSOutputStream.java:56)\r\n\r\nat\r\norg.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\r\n\r\nat\r\norg.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\r\n\r\nat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:62)\r\n\r\nat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:120)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem.writeStreamToFile(CommandWithDestination.java:466)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination.copyStreamToTarget(CommandWithDestination.java:391)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination.copyFileToTarget(CommandWithDestination.java:328)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:263)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination.processPath(CommandWithDestination.java:248)\r\n\r\nat org.apache.hadoop.fs.shell.Command.processPaths(Command.java:317)\r\n\r\nat org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:289)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination.processPathArgument(CommandWithDestination.java:243)\r\n\r\nat org.apache.hadoop.fs.shell.Command.processArgument(Command.java:271)\r\n\r\nat org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CommandWithDestination.processArguments(CommandWithDestination.java:220)\r\n\r\nat\r\norg.apache.hadoop.fs.shell.CopyCommands$Put.processArguments(CopyCommands.java:267)\r\n\r\nat org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:201)\r\n\r\nat org.apache.hadoop.fs.shell.Command.run(Command.java:165)\r\n\r\nat org.apache.hadoop.fs.FsShell.run(FsShell.java:287)\r\n\r\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\r\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\r\n\r\nat org.apache.hadoop.fs.FsShell.main(FsShell.java:340)\r\n\r\n\r\nThe same command when using ozone fs is working fine.\r\n\r\n\u00a0docker exec -it fe5d39cf6eed bash\r\n\r\nbash-4.2$ ozone fs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/kk\r\n\r\n2020-03-22 04:41:10,999 [main] INFO impl.MetricsConfig: Loaded properties\r\nfrom hadoop-metrics2.properties\r\n\r\n2020-03-22 04:41:11,123 [main] INFO impl.MetricsSystemImpl: Scheduled\r\nMetric snapshot period at 10 second(s).\r\n\r\n2020-03-22 04:41:11,127 [main] INFO impl.MetricsSystemImpl:\r\nXceiverClientMetrics metrics system started\r\n\r\nbash-4.2$ ozone fs -ls o3fs://bucket1.vol1/\r\n\r\nFound 1 items\r\n\r\n-rw-rw-rw-\u00a0 \u00a03 hadoop hadoop\u00a0 \u00a0 \u00a0 17540 2020-03-22 04:41\r\no3fs://bucket1.vol1/kk",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Smoke Test: hdfs commands failing on hadoop 27 docker-compose"
   },
   {
      "_id": "13293648",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-03-24 21:45:24",
      "description": "Seems the property *ozone.client.watch.request.timeout* was removed by HDDS-2920.  Note this is a client side property to wait for the future return. Without it, the client may wait for the future return forever in certain cases.  \r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "CommitWatcher#watchForCommit does not timeout"
   },
   {
      "_id": "13293561",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-03-24 14:02:22",
      "description": "Even after the changes done in HDDS-3086, some integration tests (especially in it-freon) are intermittently timing out.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Intermittent timeout in integration tests"
   },
   {
      "_id": "13293402",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-23 22:35:13",
      "description": "The endpoint /prom is available for Recon from Base HTTP server, but the metrics are not initialized.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Initialize Recon metrics for prometheus at /prom endpoint"
   },
   {
      "_id": "13293358",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-23 17:52:06",
      "description": "As 0.5.0 release is almost here, our daily builds should use a newer version.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump version to 0.6.0-SNAPSHOT"
   },
   {
      "_id": "13292893",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-20 10:55:19",
      "description": "Ozone RPC protocols (due to the limitation of protobuf) are very simple: There is only one message per service and the message is routed to the appropriate method based on the type. \r\n\r\nIt makes very easy to add tracing / metrics as we have one generic dispatcher (OzoneProtocolMessageDispatcher.java) where we implemented opentracing support and basic metric collection.\r\n\r\nIn this patch I propose to improve this metric to add the message type to the metrics as a tag to make it easier to follow what's going on... (eg. see the distribution of the incoming message types)\r\n\r\nWe can also measure the total amount of time spent to serve one specific type of requests which can be used to calculate the moving average for latency.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide message-level metrics from the generic protocol dispatch "
   },
   {
      "_id": "13292806",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-20 00:26:22",
      "description": "Currently, Recon does not have a way to determine the datanodes that had replica of a missing container. Add this information to the missing containers endpoint response.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon should provide the list of datanodes that a missing container was present in."
   },
   {
      "_id": "13292687",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-19 12:07:28",
      "description": "Ratis use dropwizard metrics where the key parameters of the metrics (like group name or instance id) are part of the name of the metrics instead of using a tag.\r\n\r\nFor example\r\n{code:java}\r\nratis.log_appender.851cb00a-af97-455a-b079-d94a77d2a936@group-C14654DE8C2C.follower_65f881ea-8794-403d-be77-a030ed79c341_match_index {code}\r\nInstead of\r\n{code:java}\r\nratis.log_appender_match_index{group=\"group-C14654DE8C2C\",...} {code}\r\nIt makes hard to combine the same metrics (match_index) from different sources.\r\n\r\nHDDS-2950 implemented a regexp based workaround, but the regexp doesn't match for the latest Ratis metrics.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Dropwizard metrics mapping for latest Ratis metrics"
   },
   {
      "_id": "13292617",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-03-19 04:43:46",
      "description": "{code:java}\r\n    <name>dfs.ratis.client.request.retry.interval</name>\r\n    <value>1000ms</value>\r\n{code}\r\n\r\nChange the default value to 15s. As with 1s sleep, we will see retry happening at a very fast interval. \r\nIn the billion object test, we see after changing the value to 15s, queue limit has never reached the limit.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "billiontest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix retry interval default in Ozone client"
   },
   {
      "_id": "13292454",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-03-18 12:19:46",
      "description": "This is an extension of bug\u00a0HDDS-3214.\r\n\r\nsteps taken :\r\n\r\n1)\u00a0Mounted noise injection FUSE on all datanodes\r\n\r\n2) Selected 1 datanode from each open pipeline (factor=3)\r\n\r\n3) Injected WRITE FAILURE noise with error code -\u00a0ENOENT on \"hdds.datanode.dir\" path of list of datanodes selected in step 2)\r\n\r\n4) start PUT key operation of size\u00a0 32 MB.\r\n\r\n\u00a0\r\n\r\nObservation :\r\n\r\n----------------\r\n\r\nPUT key operation failed.\u00a0\r\n\r\nAs there is a WRITE failure in one of the datanodes in the pipeline, 3 way commit should fail.\r\n\r\nBut it should proceed with 2-way commit and the operation should have been successful.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "fault_injection"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "2-way commit did not happen when WRITE failure injected in one of the datanodes of a piepeline"
   },
   {
      "_id": "13292444",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-03-18 11:28:00",
      "description": "Currently, the data in the StateMachineCache is evicted as soon as the applyTransaction call is issued for a transaction in Ratis. In our testing with keys in few kbs of size, it was figured that the data is evicted from the cache before append requests can be processed in a slightly slow follower thereby making leader read the chunk data from underlying fs/disk very frequently. This leads to slowing down the leader as well as well as overall throughput of the pipeline.\u00a0\r\n\r\nThe idea here is to ensure the data is evicted from the cache only when both followers have caught up with the match index. If a follower is really slow, it will eventually be marked slow after nodeFailureTimeout and pipeline will be destroyed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure eviction of stateMachineData from cache only when both followers catch up"
   },
   {
      "_id": "13292292",
      "assignee": "xyao",
      "components": [],
      "created": "2020-03-17 23:18:49",
      "description": "Currently the OM DB write is asynchronously handled in OzoneManagerDoubleBuffer. But the OM response does not have traceID properly populated. As a result, we can't get insight for the DB part of the OM request handling. \r\n\r\nThis ticket is opened to add traceID properly so that the addBatch and commitBatch cost of the request can be shown up in properly in Opentracing/Jaeger. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Tracing Ozone Manager DB write batch operations "
   },
   {
      "_id": "13292084",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-17 00:09:05",
      "description": "Recon tracks the containers that are missing in the cluster. We have to add an integration tests that mimics this scenario to make sure there are no regressions along Recon's receipt of this information and subsequent processing. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add integration test for Recon FSCK."
   },
   {
      "_id": "13292028",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-03-16 17:52:36",
      "description": "During Datanode startup, for each container we iterate 2 times entire DB\r\n1. For Setting block length\r\n2. For finding delete Key count.\r\n\r\nAnd for open containers, we do step 1 again.\r\n\r\n*Code Snippet:*\r\n*ContainerReader.java:*\r\n\r\n*For setting Bytes Used:*\r\n{code:java}\r\n      List<Map.Entry<byte[], byte[]>> liveKeys = metadata.getStore()\r\n          .getRangeKVs(null, Integer.MAX_VALUE,\r\n              MetadataKeyFilters.getNormalKeyFilter());\r\n\r\n      bytesUsed = liveKeys.parallelStream().mapToLong(e-> {\r\n        BlockData blockData;\r\n        try {\r\n          blockData = BlockUtils.getBlockData(e.getValue());\r\n          return blockData.getSize();\r\n        } catch (IOException ex) {\r\n          return 0L;\r\n        }\r\n      }).sum();\r\n      kvContainerData.setBytesUsed(bytesUsed);\r\n{code}\r\n\r\n*For setting pending deleted Key count*\r\n\r\n{code:java}\r\n          MetadataKeyFilters.KeyPrefixFilter filter =\r\n              new MetadataKeyFilters.KeyPrefixFilter()\r\n                  .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\r\n          int numPendingDeletionBlocks =\r\n              containerDB.getStore().getSequentialRangeKVs(null,\r\n                  Integer.MAX_VALUE, filter)\r\n                  .size();\r\n          kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\r\n{code}\r\n\r\n*For open Containers*\r\n\r\n{code:java}\r\n          if (kvContainer.getContainerState()\r\n              == ContainerProtos.ContainerDataProto.State.OPEN) {\r\n            // commitSpace for Open Containers relies on usedBytes\r\n            initializeUsedBytes(kvContainer);\r\n          }\r\n{code}\r\n\r\n\r\n*Jstack of DN during startup*\r\n{code:java}\r\n\"Thread-8\" #34 prio=5 os_prio=0 tid=0x00007f5df5070000 nid=0x8ee runnable [0x00007f4d840f3000]\r\n   java.lang.Thread.State: RUNNABLE\r\n        at org.rocksdb.RocksIterator.next0(Native Method)\r\n        at org.rocksdb.AbstractRocksIterator.next(AbstractRocksIterator.java:70)\r\n        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:195)\r\n        at org.apache.hadoop.hdds.utils.RocksDBStore.getRangeKVs(RocksDBStore.java:155)\r\n        at org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:158)\r\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyAndFixupContainerData(ContainerReader.java:191)\r\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:168)\r\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:146)\r\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.run(ContainerReader.java:101)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "billiontest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Datanode startup is slow due to iterating container DB 2-3 times"
   },
   {
      "_id": "13291780",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-15 07:18:07",
      "description": "The current config key for Recon HTTP Server Keytab file is `ozone.recon.keytab.file`. It needs to renamed to `ozone.recon.http.kerberos.keytab.file` for consistency.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Deprecate old Recon HTTP Server Keytab config key"
   },
   {
      "_id": "13291779",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-15 07:12:32",
      "description": "Add enzyme and jest libraries as dev dependencies and improve code coverage with unit tests in Recon UI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add unit tests to Recon Frontend"
   },
   {
      "_id": "13291773",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-15 06:14:08",
      "description": "TestReconWithOzoneManager may fail with BindException:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/pull/677/checks?check_run_id=507376007}\r\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 19.707 s <<< FAILURE! - in org.apache.hadoop.ozone.recon.TestReconWithOzoneManager\r\norg.apache.hadoop.ozone.recon.TestReconWithOzoneManager  Time elapsed: 19.706 s  <<< ERROR!\r\npicocli.CommandLine$ExecutionException: Error while calling command (org.apache.hadoop.ozone.recon.ReconServer@23f74a49): java.net.BindException: Port in use: 0.0.0.0:36263\r\n\t...\r\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.build(MiniOzoneClusterImpl.java:534)\r\n\tat org.apache.hadoop.ozone.recon.TestReconWithOzoneManager.init(TestReconWithOzoneManager.java:109)\r\n\t...\r\nCaused by: java.net.BindException: Port in use: 0.0.0.0:36263\r\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.constructBindException(HttpServer2.java:1200)\r\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.bindForSinglePort(HttpServer2.java:1222)\r\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.openListeners(HttpServer2.java:1281)\r\n\tat org.apache.hadoop.hdds.server.http.HttpServer2.start(HttpServer2.java:1136)\r\n\tat org.apache.hadoop.hdds.server.http.BaseHttpServer.start(BaseHttpServer.java:252)\r\n\tat org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:128)\r\n\tat org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:106)\r\n\tat org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50)\r\n\tat picocli.CommandLine.execute(CommandLine.java:1173)\r\n\t... 27 more\r\n{code}\r\n\r\n{code:title=test output}\r\n2020-03-14 06:17:08,677 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(284)) - HTTP server of ozoneManager listening at http://0.0.0.0:36263\r\n...\r\n2020-03-14 06:17:11,589 [main] INFO  http.BaseHttpServer (BaseHttpServer.java:newHttpServer2BuilderForOzone(170)) - Starting Web-server for recon at: http://0.0.0.0:36263\r\n...\r\n2020-03-14 06:17:12,756 [main] INFO  recon.ReconServer (ReconServer.java:start(125)) - Starting Recon server\r\n2020-03-14 06:17:12,757 [main] INFO  http.HttpServer2 (HttpServer2.java:start(1139)) - HttpServer.start() threw a non Bind IOException\r\njava.net.BindException: Port in use: 0.0.0.0:36263\r\n...\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestReconWithOzoneManager due to BindException"
   },
   {
      "_id": "13291618",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-03-13 17:21:24",
      "description": "After the repo was split from hadoop, there are a few unused dependencies/version strings left in pom.xml. They can be removed.\r\n\r\nExample: \r\n\r\n{code}\r\n    <hbase.one.version>1.2.6</hbase.one.version>\r\n    <hbase.two.version>2.0.0-beta-1</hbase.two.version>\r\n{code}\r\nThere may be more.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Remove unused dependency version strings"
   },
   {
      "_id": "13291549",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-13 13:08:21",
      "description": "With more integration tests enabled recently, it-client-and-hdds takes much more time (~50 mins) than other splits (20-25 mins), increasing overall delay in getting test results.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Rebalance integration tests"
   },
   {
      "_id": "13291510",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-13 08:58:08",
      "description": "The MetadataStore interface provides a generic view to any key / value store with a LevelDB and RocksDB implementation.\r\n\r\nSince the early version of MetadataStore we also go the DBStore interface which is more andvanced (it supports DB profiles and ColumnFamilies).\r\n\r\nTo simplify the introduction of new features (like versioning or rocksdb tuning) we should use the new interface everywhere instead of the old interface.\r\n\r\nWe should update SCM and Datanode to use the DBStore instead of MetadataStore. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "backward-incompatible",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Use DBStore instead of  MetadataStore in SCM"
   },
   {
      "_id": "13291413",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-12 19:34:59",
      "description": "* Handle DELETE key operation correctly.\r\n* Handle PUT key operation for an existing key.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix issues in File count by size task."
   },
   {
      "_id": "13291166",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-11 20:18:30",
      "description": "Integrate missing containers endpoint (/api/v1/containers/missing) with Recon UI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Integrate Recon missing containers UI with endpoint."
   },
   {
      "_id": "13291165",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-11 20:16:50",
      "description": "Add a Recon API endpoint to serve missing containers information from Recon DB.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add Recon endpoint to serve missing containers and its metadata."
   },
   {
      "_id": "13291071",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-11 12:31:23",
      "description": "During preformance tests It was noticed that the OM performance is dropped after 10-20 million of keys. (see the screenshot).\r\n\r\nBy default cache_index_and_filter_blocks is enabled for all of our RocksDB instances (see DBProfile) which is not the best option. (For example see this thread: https://github.com/facebook/rocksdb/issues/3961#)\r\n\r\nWith turning on this cache the indexes and bloom filters are cached **inside the block cache** which makes slower the cache when we have significant data.\r\n\r\nWithout turning it on (based on my understanding) all the indexes will remain open without any cache. With our current settings we have only a few number of sst files (even with million of keys) therefore it seems to be safe to turn this option off.\r\n\r\nWith turning this option of I was able to write >100M keys with high throughput. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Disable index and filter block cache for RocksDB"
   },
   {
      "_id": "13291070",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-11 12:30:23",
      "description": "6.0.1 -- our current version from RocksDB -- released one year ago. Since than many new versions are released with important bug fixes.\r\n\r\nI propose to update to the latest one...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Bump RocksDB version to the latest one"
   },
   {
      "_id": "13290917",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-03-10 21:33:24",
      "description": "Test2WayCommitInRatis may fail due to {{TimeoutIOException: Request #8 timeout 3s}} from Ratis while closing the container.  [~shashikant], can you please take a look? \r\n Logs with RaftClient set to debug level attached.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in Test2WayCommitInRatis"
   },
   {
      "_id": "13290901",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-10 19:11:18",
      "description": "Add a REST API to serve information required for recon dashboard\r\n\r\n!Screen Shot 2020-03-10 at 12.10.41 PM.png!",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create REST API to serve Recon Dashboard and integrate with UI in Recon."
   },
   {
      "_id": "13290799",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-10 12:55:21",
      "description": "Integration tests run multiple datanodes in the same JVM.  Each datanode comes with 60 chunk writer threads by default (may be decreased in HDDS-3053).  This makes thread dumps (eg. produced by {{GenericTestUtils.waitFor}} on timeout) really hard to navigate, as there may be 300+ such threads.\r\n\r\nSince integration tests are generally run with a single disk which is even shared among the datanodes, a few threads per datanode should be enough.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Reduce number of chunkwriter threads in integration tests"
   },
   {
      "_id": "13290642",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-03-09 20:34:44",
      "description": "With replay, now we use directly get() API.\r\n\r\nPreviously the code\r\n\r\nOMKeyRequest.java\r\n\r\n\u00a0\r\n{code:java}\r\nelse if (omMetadataManager.getKeyTable().isExist(dbKeyName)) {\r\n // TODO: Need to be fixed, as when key already exists, we are\r\n // appending new blocks to existing key.\r\n keyInfo = omMetadataManager.getKeyTable().get(dbKeyName);{code}\r\n\u00a0\r\n\r\nNow for every create key/File we use get API, this is changed for replay\r\n{code:java}\r\nOmKeyInfo dbKeyInfo =\r\n omMetadataManager.getKeyTable().get(dbKeyName);\r\nif (dbKeyInfo != null) {{code}\r\n\r\nThe proposal is to change get with getIfExist, and make use of keyMayExist.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Implement getIfExist in Table and use it in CreateKey/File"
   },
   {
      "_id": "13290532",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-09 13:34:54",
      "description": "Surefire plugin is configured to run {{Test*}} classes, but there are two test classes named {{*Test}}:\r\n\r\n{code}\r\n$ find */*/src/test/java -name '*Test.java' | xargs grep -l '@Test'\r\nhadoop-hdds/server-scm/src/test/java/org/apache/hadoop/hdds/scm/HddsServerUtilTest.java\r\nhadoop-ozone/insight/src/test/java/org/apache/hadoop/ozone/insight/LogSubcommandTest.java\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Rename silently ignored tests"
   },
   {
      "_id": "13290503",
      "assignee": "elek",
      "components": [],
      "created": "2020-03-09 10:59:23",
      "description": "OmKeyGenerator class from Freon can generate keys (open key + commit key). But this test tests both OM and SCM performance. It seems to be useful to have a method to test only the OM performance with faking the response from SCM.  \r\n\r\nCan be done easily with the same approach what we have in HDDS-3023: A simple utility class can be implemented and with byteman we can replace the client calls with the fake method.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create isolated environment for OM to test it without SCM"
   },
   {
      "_id": "13290498",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-09 10:10:22",
      "description": "This was observed in unit check run for 0.5.0 RC.\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/490978126?check_suite_focus=true}\r\n2020-03-06T19:13:08.6122969Z [ERROR] Failed to execute goal on project hadoop-ozone-insight: Could not resolve dependencies for project org.apache.hadoop:hadoop-ozone-insight:jar:0.5.0-beta: Could not find artifact org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots) -> [Help 1]\r\n2020-03-06T19:13:08.6180318Z [ERROR] Failed to execute goal on project mini-chaos-tests: Could not resolve dependencies for project org.apache.hadoop:mini-chaos-tests:jar:0.5.0-beta: Failure to find org.apache.hadoop:hadoop-ozone-integration-test:jar:tests:0.5.0-beta in https://repository.apache.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots.https has elapsed or updates are forced -> [Help 1]\r\n{code}\r\n\r\nUnit check skips {{integration-test}}, but these 2 modules depend on it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Unit check fails to execute insight and mini-chaos-tests modules"
   },
   {
      "_id": "13290495",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-03-09 09:59:24",
      "description": "Ozone's GitHub Actions workflows only work with SNAPSHOT versions due to hard-coded {{ozone-*-SNAPSHOT}} in target path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hard-coded SNAPSHOT version from GitHub workflows"
   },
   {
      "_id": "13290187",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         }
      ],
      "created": "2020-03-06 20:45:31",
      "description": "In OM HA cluster, when one of the om service is down, during creation of RpcClient it will fail with below error.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\njava.lang.IllegalArgumentException: java.net.UnknownHostException: om1\r\n at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:447)\r\n at org.apache.hadoop.ozone.om.ha.OMProxyInfo.<init>(OMProxyInfo.java:40)\r\n at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:115)\r\n at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83)\r\n at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:207)\r\n at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)\r\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:198)\r\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:124)\r\n at org.apache.hadoop.ozone.freon.RandomKeyGenerator.init(RandomKeyGenerator.java:249)\r\n at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:274)\r\n at org.apache.hadoop.ozone.freon.RandomKeyGenerator.call(RandomKeyGenerator.java:82)\r\n at picocli.CommandLine.execute(CommandLine.java:1173)\r\n at picocli.CommandLine.access$800(CommandLine.java:141)\r\n at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)\r\n at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)\r\n at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)\r\n at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)\r\n at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)\r\n at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)\r\n at org.apache.hadoop.ozone.freon.Freon.execute(Freon.java:72)\r\n at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)\r\n at org.apache.hadoop.ozone.freon.Freon.main(Freon.java:98)\r\nCaused by: java.net.UnknownHostException: om1\r\n ... 22 more\r\n\u00a0\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OM RpcClient fail with java.lang.IllegalArgumentException"
   },
   {
      "_id": "13289684",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-03-05 01:27:32",
      "description": "With replay logic, we have additional keyTable check to detect whether it is replay or not.\r\n\r\nIn non-HA case, we don't need this check. So this Jira is to skip that check in case of non-HA when ratis is not enabled.\r\n\r\n\u00a0\r\n\r\n*Ran simple test to know the perf impact:*\r\n\u00a0\r\n2295 Keys/sec with Additional Key Table check\r\n2824 Keys/sec with removing that check\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Skip KeyTable check in OMKeyCommit when ratis is disabled in OM."
   },
   {
      "_id": "13289442",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-04 06:29:26",
      "description": "HDDS-1391 introduce a new OM RPC to allow Recon server to get delta of OM metadata update. However, the delta itself could be large. This causes ERROR on OM like below. \r\n\r\nShould we consider sending update in chunks over hadoop RPC instead of all in one piece (1.5 GB in this case)?\r\n\r\n4:34:56.403 PM\tWARN\tOzoneManagerProtocolServerSideTranslatorPB\t\r\n##Response for request DBUpdates is too big size 1584040343\r\n4:34:57.022 PM\tWARN\tServer\t\r\nError serializing call response for call Call#12 Retry#15 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.17.112.109:58674\r\ncom.google.protobuf.CodedOutputStream$OutOfSpaceException: CodedOutputStream was writing to a flat byte array and ran out of space.\r\n\tat com.google.protobuf.CodedOutputStream.refreshBuffer(CodedOutputStream.java:828)\r\n\tat com.google.protobuf.CodedOutputStream.writeRawBytes(CodedOutputStream.java:959)\r\n\tat com.google.protobuf.CodedOutputStream.writeRawBytes(CodedOutputStream.java:905)\r\n\tat com.google.protobuf.CodedOutputStream.writeBytesNoTag(CodedOutputStream.java:386)\r\n\tat com.google.protobuf.CodedOutputStream.writeBytes(CodedOutputStream.java:229)\r\n\tat org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$DBUpdatesResponse.writeTo(OzoneManagerProtocolProtos.java)\r\n\tat com.google.protobuf.CodedOutputStream.writeMessageNoTag(CodedOutputStream.java:380)\r\n\tat com.google.protobuf.CodedOutputStream.writeMessage(CodedOutputStream.java:222)\r\n\tat org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OMResponse.writeTo(OzoneManagerProtocolProtos.java:15959)\r\n\tat org.apache.hadoop.ipc.Server.setupResponseForProtobuf(Server.java:3216)\r\n\tat org.apache.hadoop.ipc.Server.setupResponse(Server.java:3165)\r\n\tat org.apache.hadoop.ipc.Server.setupResponse(Server.java:3141)\r\n\tat org.apache.hadoop.ipc.Server.access$200(Server.java:139)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1061)\r\n\tat org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:858)\r\n\tat org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:844)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1001)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "DBUpdateResponse message could be much larger than ipc.maximum.data.length"
   },
   {
      "_id": "13289350",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-03-03 22:59:40",
      "description": "We need a REST API to serve Pipeline information in Recon and integrate with existing Recon UI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create REST API to serve Pipeline information and integrate with UI in Recon."
   },
   {
      "_id": "13289083",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-03-03 01:33:18",
      "description": "In the test, we have Thread.sleep and then check the metric value. It will be better to use GenerictestUtils.waitFor and check the value of the metric. In few of the runs we have seen this test failed.\r\n{code:java}\r\nThread.sleep(100 * 1000L);\r\nmetrics =\r\n getMetrics(SCMPipelineMetrics.class.getSimpleName());\r\nfor (Pipeline pipeline : cluster.getStorageContainerManager()\r\n .getPipelineManager().getPipelines()) {\r\n Assert.assertEquals(bytesWritten, getLongCounter(\r\n SCMPipelineMetrics.getBytesWrittenMetricName(pipeline), metrics));\r\n}{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestSCMPipelineBytesWrittenMetrics"
   },
   {
      "_id": "13289072",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-03-02 23:58:05",
      "description": "Make Freon commands work with OM HA",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Freon work with OM HA"
   },
   {
      "_id": "13289065",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-03-02 23:11:47",
      "description": "DoubleBuffer metrics are not getting updated when ratis is enabled in OM.\r\n\r\nThere is no issue when ratis is not enabled, double buffer metrics are updating fine.\r\n{code:java}\r\n{\"name\":\u00a0\"Hadoop:service=OzoneManager,name=OzoneManagerDoubleBufferMetrics\",\"modelerType\":\u00a0\"OzoneManagerDoubleBufferMetrics\",\"tag.Hostname\":\u00a0\"hw13865.hitronhub.home\",\"TotalNumOfFlushOperations\":\u00a00,\"TotalNumOfFlushedTransactions\":\u00a00,\"MaxNumberOfTransactionsFlushedInOneIteration\":\u00a00,\"FlushTimeNumOps\":\u00a00,\"FlushTimeAvgTime\":\u00a00,\"AvgFlushTransactionsInOneIteration\":\u00a00},{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "When ratis is enabled in OM, double Buffer metrics not getting updated "
   },
   {
      "_id": "13289052",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-03-02 21:52:59",
      "description": "{{LockManager}} has a possible deadlock.\r\n\r\n# Number of locks is limited by using a {{GenericObjectPool}}.  If N locks are already acquired, new requestors need to wait.  This wait in {{getLockForLocking}} happens in a callback executed from {{ConcurrentHashMap#compute}} while holding a lock on a map entry.\r\n# While releasing a lock, {{decrementActiveLockCount}} implicitly requires a lock on an entry in {{ConcurrentHashMap}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Possible deadlock in LockManager"
   },
   {
      "_id": "13288858",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-03-02 12:07:30",
      "description": "The goal of this task is to introduce a new Freon test that issues putBlock commands.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add new Freon test for putBlock"
   },
   {
      "_id": "13288383",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-28 19:42:12",
      "description": "Fix the following issues in Recon\r\n* Both the Incremental container report handler and the regular container report handler add new containers from SCM whenever they see a new container. This test and add step must be synchronized between the 2 handlers to avoid any inconsistent metadata state.\r\n* NodeStateMap in allow does not addition of a single container to the Map of Node -> Set of Containers since it instantiates with a Collections.emptySet(), and then relies on a map.put() to update the value. Changing this to a \"new HashSet\" allows addition of a container one by one which is possible in Recon.\r\n* Improve logging in Recon Container Manager when it receives a container report from a node before receiving the pipeline report for a newly created pipeline.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix race condition in Recon's container and pipeline handling."
   },
   {
      "_id": "13288382",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-02-28 19:37:12",
      "description": "Currently, Recon uses an ephemeral port only in the integration test for Recon. In all other integration tests, we end up using the default (9888) that causes failures in other integration tests that start up a Mini ozone cluster. In addition, we want to start up Recon in MiniOzoneCluster by explicitly requesting it rather than by default.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor 'Recon' in MiniOzoneCluster to use ephemeral port."
   },
   {
      "_id": "13288269",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-28 12:11:20",
      "description": "{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/474452740}\r\n[ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 505.227 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse\r\n[ERROR] testDoubleBuffer(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 500.142 s  <<< ERROR!\r\njava.lang.Exception: test timed out after 500000 milliseconds\r\n  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:394)\r\n  at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:130)\r\n{code}\r\n\r\nAlso in: https://github.com/apache/hadoop-ozone/pull/590/checks?check_run_id=467388979\r\n\r\nCC [~bharat]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Intermittent timeout in TestOzoneManagerDoubleBufferWithOMResponse#testDoubleBuffer"
   },
   {
      "_id": "13288225",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-28 08:49:02",
      "description": "To make it possible to create different client jars compiled with different version of Hadoop we need clear and Hadoop independent hdds-common (and hdds-client) projects.\r\n\r\n(For more details about the motivation, check this design doc: https://lists.apache.org/thread.html/rd0ea00f958368e888db1947eb71e514fb977df0b7baaad928ac50e94%40%3Cozone-dev.hadoop.apache.org%3E)\r\n\r\nOur current blocker is the usage of `org.apache.hadoop.conf.Configuration`. Configuration class is a heavyweight object from hadoop-common which introduce a lot of unnecessary dependencies. It also violates multiple [OOP principles|https://en.wikipedia.org/wiki/SOLID], for example the *Dependency inversion principle*.\r\n\r\nTo make our components more independent I propose to depend on a lightweight ConfigurationSource interface which includes all the required getXXX methods. OzoneConfiguration can implement that interface (and with older Hadoop we can create direct adapters).\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Depend on lightweight ConfigurationSource interface instead of Hadoop Configuration"
   },
   {
      "_id": "13288209",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-02-28 07:11:51",
      "description": "{code}\r\n[INFO] Running org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\r\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 641.212 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower\r\n[ERROR] testDeleteKeyWithSlowFollower(org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower)  Time elapsed: 617.198 s  <<< ERROR!\r\njava.io.IOException: INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:228)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:401)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:346)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:457)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:508)\r\n\tat org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestDeleteWithSlowFollower.testDeleteKeyWithSlowFollower(TestDeleteWithSlowFollower.java:224)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestDeleteWithSlowFollower is failing intermittently"
   },
   {
      "_id": "13288122",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-02-27 18:39:20",
      "description": "In a few scenarios, like Disks are gone, the datanode is not up or any other case, we may try to close pipelines.\r\n\r\nIf we close pipelines after SCM restart, SCM will not come out of safe mode. This is because of the current implementation where we get the count of the pipeline from DB when creating a SafeMode rule object. During this, if any pipeline is closed/removed from DB, the Rule does not know about it, and it PipelineSafeMode rule is never met, this causes a situation where we never come out of safe mode.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "SCM does not exit Safe mode"
   },
   {
      "_id": "13288114",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-27 17:56:27",
      "description": "Acceptance tests may invoke the same smoketest multiple times to verify behaviour in different states.  Currently output is saved to a file named based on _environment_, _test_ and _container_, so each execution's output overwrites the previous one.  We should check if the file already exists and add a suffix if necessary to avoid overwriting previous logs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Save each output of smoketest executed multiple times"
   },
   {
      "_id": "13288109",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2020-02-27 17:40:53",
      "description": "{{ozone sh key get}} refuses to overwrite existing local file.  I would like to add a {{--force}} flag (default: false) to allow overriding this behavior, to make it easier to repeatedly get a key without forcing me to delete it locally first.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Allow forced overwrite of local file"
   },
   {
      "_id": "13288074",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-27 14:42:41",
      "description": "{{TestDataValidate}} has 2 large key tests:\r\n\r\n* {{ratisTestLargeKey}}\r\n* {{standaloneTestLargeKey}}\r\n\r\nBut both of these test RATIS/3 replication since HDDS-675.  I think {{standaloneTestLargeKey}} can be removed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Duplicate large key test"
   },
   {
      "_id": "13287960",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-27 08:04:23",
      "description": "TestSCMNodeManager crashed in one of the runs, although it passes usually:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/pull/601/checks?check_run_id=471611827}\r\n[ERROR] Crashed tests:\r\n[ERROR] org.apache.hadoop.hdds.scm.node.TestSCMNodeManager\r\n{code}\r\n\r\n{code:title=hs_err_pid9082.log}\r\nsiginfo: si_signo: 11 (SIGSEGV), si_code: 2 (SEGV_ACCERR), si_addr: 0x00007f378cf6f340\r\n\r\nStack: [0x00007f37626fb000,0x00007f37627fc000],  sp=0x00007f37627f9e48,  free space=1019k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  0x00007f378cf6f340\r\nC  [librocksdbjni3775377216204452319.so+0x2a05dd]  rocksdb::DB::Delete(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&)+0x4d\r\nC  [librocksdbjni3775377216204452319.so+0x2a0641]  rocksdb::DBImpl::Delete(rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, rocksdb::Slice const&)+0x11\r\nC  [librocksdbjni3775377216204452319.so+0x1a931a]  rocksdb::DB::Delete(rocksdb::WriteOptions const&, rocksdb::Slice const&)+0xba\r\nC  [librocksdbjni3775377216204452319.so+0x19f3e0]  rocksdb_delete_helper(JNIEnv_*, rocksdb::DB*, rocksdb::WriteOptions const&, rocksdb::ColumnFamilyHandle*, _jbyteArray*, int, int)+0x130\r\nC  [librocksdbjni3775377216204452319.so+0x19f4a1]  Java_org_rocksdb_RocksDB_delete__J_3BII+0x41\r\nj  org.rocksdb.RocksDB.delete(J[BII)V+0\r\nj  org.rocksdb.RocksDB.delete([B)V+13\r\nj  org.apache.hadoop.hdds.utils.RocksDBStore.delete([B)V+9\r\nj  org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.removePipeline(Lorg/apache/hadoop/hdds/scm/pipeline/PipelineID;)V+35\r\nj  org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.destroyPipeline(Lorg/apache/hadoop/hdds/scm/pipeline/Pipeline;)V+27\r\n...\r\nj  org.apache.hadoop.hdds.scm.node.DeadNodeHandler.destroyPipelines(Lorg/apache/hadoop/hdds/protocol/DatanodeDetails;)V+28\r\nj  org.apache.hadoop.hdds.scm.node.DeadNodeHandler.onMessage(Lorg/apache/hadoop/hdds/protocol/DatanodeDetails;Lorg/apache/hadoop/hdds/server/events/EventPublisher;)V+6\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestSCMNodeManager intermittent crash"
   },
   {
      "_id": "13287915",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2020-02-27 04:10:53",
      "description": "Observed a time-out during pr-check/it-freon for HDDS-2940. Failure appears unrelated to the changes in the patch. \r\n\r\n[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 67.193 s - in org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations\r\n2862\r\n[INFO] Running org.apache.hadoop.ozone.freon.TestFreonWithDatanodeRestart\r\n2863\r\n[WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 30.559 s - in org.apache.hadoop.ozone.freon.TestFreonWithDatanodeRestart\r\n2864\r\n[INFO] \r\n2865\r\n[INFO] Results:\r\n2866\r\n[INFO] \r\n2867\r\n[WARNING] Tests run: 16, Failures: 0, Errors: 0, Skipped: 3\r\n2868\r\n[INFO] \r\n2869\r\n[INFO] ------------------------------------------------------------------------\r\n2870\r\n[INFO] BUILD FAILURE\r\n2871\r\n[INFO] ------------------------------------------------------------------------\r\n2872\r\n[INFO] Total time:  28:58 min\r\n2873\r\n[INFO] Finished at: 2020-02-26T17:55:42Z\r\n2874\r\n[INFO] ------------------------------------------------------------------------\r\n2875\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork -> [Help 1]\r\n2876\r\n[ERROR] \r\n2877\r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n2878\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n2879\r\n[ERROR] \r\n2880\r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n2881\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Failure running integration test it-freon "
   },
   {
      "_id": "13287869",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-26 23:06:59",
      "description": "* Introduced kerberos principal and keytab file configs for Recon\r\n* Login Recon user with KDC while Recon starts up",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM Delta updates request in Recon should work with secure Ozone Manager."
   },
   {
      "_id": "13287717",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-26 10:45:15",
      "description": "Sometimes a unit/integration test does not complete, nor does it crash.  We should collect the output of such tests in the result bundle for analysis.\r\n\r\nExample:\r\n\r\n{code:title=https://github.com/adoroszlai/hadoop-ozone/runs/469172863}\r\n2020-02-26T08:15:58.2297584Z [INFO] Running org.apache.hadoop.ozone.freon.TestRandomKeyGenerator\r\n2020-02-26T08:30:59.6189916Z [INFO] Running org.apache.hadoop.ozone.freon.TestDataValidateWithUnsafeByteOperations\r\n...\r\n2020-02-26T08:32:47.6155975Z [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-integration-test: There was a timeout or other error in the fork\r\n{code}\r\n\r\nIn this case TestRandomKeyGenerator had this problem.  It might be a bit tricky to find such tests, since these are not explicitly listed at the end, unlike failed or crashed tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Include output of timed out test in bundle"
   },
   {
      "_id": "13287592",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-02-25 20:17:55",
      "description": "We should start scrubbing pipelines after SCM is out of safe mode.\r\n\r\nReasons to do this:\r\n # Right now, we do scrub pipeline as part of triggerPipelineCreation, now when we scrub pipelines in allocated state for more than \"ozone.scm.pipeline.allocated.timeout\", we might close some pipelines and with this, we might not be able to come out of safeMode. As in SafeModeRules, we get pipeline count from pipelineDB during initialization.\r\n\r\nExample scenario:\r\n # Stop 3 Datanodes.\u00a0\r\n # Restart SCM.\r\n # Start Datanode after 6 mts. We shall never come out of safe mode, as pipeline in allocated state will meet scrubber time out condition.\r\n\r\nTo not to be in these kinds of scenarios, better thing to be done here is scrub pipelines after SCM out of the safe mode\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "SCM scrub pipeline should be started after coming out of safe mode"
   },
   {
      "_id": "13287563",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-25 17:52:46",
      "description": "Datanodes throw this exception while connecting to recon.\r\n{code:java}\r\ndatanode_1\u00a0 | java.io.IOException: DestHost:destPort recon:9891 , LocalHost:localPort 6a99ad69685d/192.168.48.4:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1\u00a0 | java.io.IOException: DestHost:destPort recon:9891 , LocalHost:localPort 6a99ad69685d/192.168.48.4:0. Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1\u00a0 |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)datanode_1\u00a0 |  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)datanode_1\u00a0 |  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)datanode_1\u00a0 |  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)datanode_1\u00a0 |  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)datanode_1\u00a0 |  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client.call(Client.java:1457)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client.call(Client.java:1367)datanode_1\u00a0 |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)datanode_1\u00a0 |  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)datanode_1\u00a0 |  at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)datanode_1\u00a0 |  at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)datanode_1\u00a0 |  at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)datanode_1\u00a0 |  at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)datanode_1\u00a0 |  at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)datanode_1\u00a0 |  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)datanode_1\u00a0 |  at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)datanode_1\u00a0 |  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)datanode_1\u00a0 |  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)datanode_1\u00a0 |  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)datanode_1\u00a0 |  at java.base/java.lang.Thread.run(Thread.java:834)datanode_1\u00a0 | Caused by: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:866)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client.call(Client.java:1403)datanode_1\u00a0 |  ... 14 moredatanode_1\u00a0 | Caused by: java.lang.IllegalArgumentException: Empty nameString not alloweddatanode_1\u00a0 |  at java.security.jgss/sun.security.krb5.PrincipalName.validateNameStrings(PrincipalName.java:174)datanode_1\u00a0 |  at java.security.jgss/sun.security.krb5.PrincipalName.<init>(PrincipalName.java:397)datanode_1\u00a0 |  at java.security.jgss/sun.security.krb5.PrincipalName.<init>(PrincipalName.java:471)datanode_1\u00a0 |  at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:172)datanode_1\u00a0 |  at org.apache.hadoop.security.SaslRpcClient.getServerPrincipal(SaslRpcClient.java:305)datanode_1\u00a0 |  at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:234)datanode_1\u00a0 |  at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:160)datanode_1\u00a0 |  at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)datanode_1\u00a0 |  at java.base/java.security.AccessController.doPrivileged(Native Method)datanode_1\u00a0 |  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)datanode_1\u00a0 |  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)datanode_1\u00a0 |  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)datanode_1\u00a0 |  ... 17 more\r\n{code}\r\nRecon throws an exception while connecting to SCM:\r\n{code:java}\r\nrecon_1\u00a0 \u00a0 \u00a0| 2020-02-25 17:48:14,506 [main] ERROR scm.ReconStorageContainerManagerFacade: Exception encountered while getting pipelines from SCM.recon_1\u00a0 \u00a0 \u00a0| 2020-02-25 17:48:14,506 [main] ERROR scm.ReconStorageContainerManagerFacade: Exception encountered while getting pipelines from SCM.recon_1\u00a0 \u00a0 \u00a0| java.io.IOException: DestHost:destPort scm:9860 , LocalHost:localPort recon/192.168.48.8:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1\u00a0 \u00a0 \u00a0|  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)recon_1\u00a0 \u00a0 \u00a0|  at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)recon_1\u00a0 \u00a0 \u00a0|  at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)recon_1\u00a0 \u00a0 \u00a0|  at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client.call(Client.java:1457)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client.call(Client.java:1367)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)recon_1\u00a0 \u00a0 \u00a0|  at com.sun.proxy.$Proxy41.submitRequest(Unknown Source)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.submitRequest(StorageContainerLocationProtocolClientSideTranslatorPB.java:114)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.hdds.scm.protocolPB.StorageContainerLocationProtocolClientSideTranslatorPB.listPipelines(StorageContainerLocationProtocolClientSideTranslatorPB.java:322)recon_1\u00a0 \u00a0 \u00a0|  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)recon_1\u00a0 \u00a0 \u00a0|  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)recon_1\u00a0 \u00a0 \u00a0|  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)recon_1\u00a0 \u00a0 \u00a0|  at java.base/java.lang.reflect.Method.invoke(Method.java:566)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71)recon_1\u00a0 \u00a0 \u00a0|  at com.sun.proxy.$Proxy42.listPipelines(Unknown Source)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.spi.impl.StorageContainerServiceProviderImpl.getPipelines(StorageContainerServiceProviderImpl.java:49)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.scm.ReconStorageContainerManagerFacade.initializePipelinesFromScm(ReconStorageContainerManagerFacade.java:223)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.scm.ReconStorageContainerManagerFacade.start(ReconStorageContainerManagerFacade.java:183)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.ReconServer.start(ReconServer.java:118)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:95)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:39)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine.execute(CommandLine.java:1173)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine.access$800(CommandLine.java:141)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)recon_1\u00a0 \u00a0 \u00a0|  at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:52)recon_1\u00a0 \u00a0 \u00a0| Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:760)recon_1\u00a0 \u00a0 \u00a0|  at java.base/java.security.AccessController.doPrivileged(Native Method)recon_1\u00a0 \u00a0 \u00a0|  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:723)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:817)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client.call(Client.java:1403)recon_1\u00a0 \u00a0 \u00a0|  ... 28 morerecon_1\u00a0 \u00a0 \u00a0| Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[KERBEROS]recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)recon_1\u00a0 \u00a0 \u00a0|  at java.base/java.security.AccessController.doPrivileged(Native Method)recon_1\u00a0 \u00a0 \u00a0|  at java.base/javax.security.auth.Subject.doAs(Subject.java:423)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)recon_1\u00a0 \u00a0 \u00a0|  at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)recon_1\u00a0 \u00a0 \u00a0|  ... 31 more\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanodes unable to connect to recon in Secure Environment"
   },
   {
      "_id": "13287468",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-02-25 11:04:09",
      "description": "Delete key is failing . Here is the stack trace of the failure:\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n{noformat}\r\nINFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 26 which is not greater than the current value of 433 for OMKeyInfo{volume='vol-test-restartcomponentozonereaddata-1582093704', bucket='buck-test-restartcomponentozonereaddata-1582093704', key='ReadOzoneFile_1582093709', dataSize='10485760', creationTime='1582093712218', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862. Trying to failover immediately.\r\n\u00a0\r\n..\r\n..\r\n..\r\n..\r\n\u00a0\r\n20/02/19 03:37:17 INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.IllegalArgumentException): Trying to set updateID to 22 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'} E at com.google.common.base.Preconditions.checkArgument(Preconditions.java:142) E at org.apache.hadoop.ozone.om.helpers.WithObjectID.setUpdateID(WithObjectID.java:79) E at org.apache.hadoop.ozone.om.request.key.OMKeyDeleteRequest.validateAndUpdateCache(OMKeyDeleteRequest.java:147) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:230) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:210) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:130) E at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) E at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:98) E at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) E at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) E at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) E at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) E at java.base/java.security.AccessController.doPrivileged(Native Method) E at java.base/javax.security.auth.Subject.doAs(Subject.java:423) E at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) E at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) E , while invoking $Proxy16.submitRequest over nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862 after 15 failover attempts. Trying to failover immediately. E 2020-02-19 03:37:17,895 [main] ERROR ha.OMFailoverProxyProvider (OzoneManagerProtocolClientSideTranslatorPB.java:getRetryAction(287)) - Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E 20/02/19 03:37:17 ERROR ha.OMFailoverProxyProvider: Failed to connect to OMs: [nodeId=null,nodeAddress=quasar-vbncen-3.quasar-vbncen.root.hwx.site:9862]. Attempted 15 failovers. E Trying to set updateID to 23 which is not greater than the current value of 1143 for OMKeyInfo{volume='vol-test-kill-datanode-1582075168', bucket='buck-test-kill-datanode-1582075168', key='replication_test1_1582075173', dataSize='104857600', creationTime='1582075177268', type='RATIS', factor='THREE'}]\r\n{noformat}\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "UpdateID check should be skipped for non-HA OzoneManager"
   },
   {
      "_id": "13287409",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-02-25 04:07:54",
      "description": "During code read, found a similar thing, we don't log for OM start also. As OM startup also using similar code for the startup.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "OMHATest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM crash during startup does not print any error message to log"
   },
   {
      "_id": "13287376",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-02-24 23:50:46",
      "description": "Currently, the scrubber is run as part of create pipeline.\u00a0\r\n\r\nWhen SCM is started, scrubber is coming up and cleaning up all the containers in SCM. Because when loading pipelines, the pipelineCreationTimeStamp is set from when the pipeline is created.\r\n\r\n\u00a0\r\n\r\nBecause of this, below condition is satisfied and destroying all the pipelines when SCM is restarted. This can be easily reproduced start SCM, wait for 10 minutes and restart SCM.\r\n\r\n\u00a0\r\n{code:java}\r\nList<Pipeline> needToSrubPipelines = stateManager.getPipelines(type, factor,\r\n Pipeline.PipelineState.ALLOCATED).stream()\r\n .filter(p -> currentTime.toEpochMilli() - p.getCreationTimestamp()\r\n .toEpochMilli() >= pipelineScrubTimeoutInMills)\r\n .collect(Collectors.toList());\r\nfor (Pipeline p : needToSrubPipelines) {\r\n LOG.info(\"srubbing pipeline: id: \" + p.getId().toString() +\r\n \" since it stays at ALLOCATED stage for \" +\r\n Duration.between(currentTime, p.getCreationTimestamp()).toMinutes() +\r\n \" mins.\");\r\n finalizeAndDestroyPipeline(p, false);\r\n}{code}\r\n\u00a0\r\n\r\n*Log showing scrubbing of pipeline*\r\n\r\n\u00a0\r\n{code:java}\r\n2020-02-20 12:42:18,946 [RatisPipelineUtilsThread] INFO org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager: srubbing pipeline: id: PipelineID=35dff62d-9bfa-449b-b6e8-6f00cc8c1b6e since it stays at ALLOCATED stage for -1003 mins.{code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix Bug in Scrub Pipeline causing destory pipelines after SCM restart"
   },
   {
      "_id": "13287285",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-02-24 18:33:00",
      "description": "\u00a0This is happening because pipeline scrubber came and removed pipeline, and it closed pipeline and removed from DB and triggered close containers to set them to CLOSING. When SCM is restarted before close container command is handled and change the state to CLOSING, the below issue can happen.\r\n\r\n\u00a0\r\n\r\nThis can happen in other scenarios like when safeModeHandler calls finalizeAndDestroyPipeline and do SCM restart.\u00a0\r\n\r\n\u00a0\r\n\r\nThe root cause for this is Pipeline removed from DB and the container is in open state in this scenario, and when trying to get pipeline we will crash SCM due to the\u00a0{{PipelineNotFoundException error.}}\r\n\r\n{{}}\r\n{code:java}\r\n 2020-02-21 13:57:34,888 [main] ERROR org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SCM start failed with exception org.apache.hadoop.hdds.scm.pipeline.PipelineNotFoundException: PipelineID=35dff62d-9bfa-449b-b6e8-6f00cc8c1b6e not found at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.getPipeline(PipelineStateMap.java:133) at org.apache.hadoop.hdds.scm.pipeline.PipelineStateMap.addContainerToPipeline(PipelineStateMap.java:110) at org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addContainerToPipeline(PipelineStateManager.java:59) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.addContainerToPipeline(SCMPipelineManager.java:309) at org.apache.hadoop.hdds.scm.container.SCMContainerManager.loadExistingContainers(SCMContainerManager.java:121) at org.apache.hadoop.hdds.scm.container.SCMContainerManager.<init>(SCMContainerManager.java:107) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:412) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:283) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:215) at org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:612) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:142) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:117) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:66) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:42) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:55) 2020-02-21 13:57:34,892 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down StorageContainerManager at om-ha-1.vpc.cloudera.com/10.65.51.49 ************************************************************/{code}\r\n{{}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "SCM startup failed  during loading containers from DB"
   },
   {
      "_id": "13287250",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-02-24 15:58:57",
      "description": "Ozone {{FileSystem}} implementation should return the actual configured replication factor for {{getDefaultReplication()}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Filesystem should return real default replication"
   },
   {
      "_id": "13287229",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-02-24 14:55:07",
      "description": "Steps taken :\r\n\r\n------------------\r\n # Mounted noise injection FUSE on all datanodes.\r\n # Write a key.\r\n # In one of the container replicas of the key, Inject READ delay of 5 seconds on chunk file directory path.\r\n # Run get Key operation.\r\n\r\nGet Key operation is stuck and does not return any success/error .",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "fault_injection",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Get Key is hung when READ delay is injected in chunk file path"
   },
   {
      "_id": "13287221",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-24 14:14:13",
      "description": "Currently no test verifies that {{ozone fs}} creates keys for files with the expected replication factor.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Add test to verify replication factor of ozone fs"
   },
   {
      "_id": "13287203",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-24 12:26:26",
      "description": "TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel is disabled due to intermittent issues. It should be fixed / rewritten or deleted.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestOzoneRpcClientAbstract.testPutKeyRatisThreeNodesParallel"
   },
   {
      "_id": "13287161",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337242",
            "id": "12337242",
            "name": "CI"
         }
      ],
      "created": "2020-02-24 09:15:39",
      "description": "Earlier we introduced an additional github workflow to process `/command` style commands from github comments. But in this case `/retest` didn't create the new commit to trigger a new build:\r\n\r\nPull request:\r\n\r\n https://github.com/apache/hadoop-ozone/pull/393\r\n\r\nRun:\r\n\r\nhttps://github.com/apache/hadoop-ozone/runs/463225330?check_suite_focus=true",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "/retest github comment does not work"
   },
   {
      "_id": "13286899",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-02-21 22:07:01",
      "description": "SCM start up failed due to a pipelineNotFoundException, there is no error message logged in to SCM log.\r\n\r\nIn the log file, we can see just below log message no reason for the crash is logged.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\n2020-02-20 15:37:56,079 [shutdown-hook-0] INFO org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter: SHUTDOWN_MSG:\r\n/************************************************************\r\nSHUTDOWN_MSG: Shutting down StorageContainerManager at xx.xx.xx/10.65.51.49\r\n{code}\r\nIn the out file, we can see below, but not complete exception message.\r\n{code:java}\r\nPipelineID=xxxxx\u00a0not found{code}\r\n\u00a0\r\n\r\nThe actual reason for failure is not clearly logged if an exception has occurred during SCM startup.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM crash during startup does not print any error message to log"
   },
   {
      "_id": "13286779",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-21 12:48:27",
      "description": "As of now we create 60 threads (\r\ndfs.container.ratis.num.write.chunk.threads) to write chunk data to the disk. As the write is limited by the IO I can't see any benefit to have so many threads. High number of thread means a high context switch overhead, therefore it seems to be more reasonable to use only a limited number of threads.\r\n\r\nFor example 10 threads should be enough even with 5 external disk.\r\n\r\nIf you know any reason to keep the number 60, please let me know...\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Decrease the number of the chunk writer threads"
   },
   {
      "_id": "13286749",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-21 11:58:37",
      "description": "ChunkWriter threads acreated with a naming schema 'pool-[x]-thread-[y]'. We can use better naming (especially as we have 60 threads...)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use meaningful name for ChunkWriter threads"
   },
   {
      "_id": "13286563",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-02-20 20:34:39",
      "description": "Right now for all other exceptions other than serviceException we use FailOverOnNetworkException.\r\n\r\nThis Exception policy is created with 15 max fail overs and 15 retries.\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\nretryPolicyOnNetworkException.shouldRetry(\r\n exception, retries, failovers, isIdempotentOrAtMostOnce);{code}\r\n*2 issues with this:*\r\n # When shouldRetry returns action FAILOVER_AND_RETRY, it will stuck with same OM, and does not perform failover to next OM.\u00a0 As OMFailoverProxyProvider#performFailover() is a dummy call does not perform any failover.\r\n # When ozone.client.failover.max.attempts is set to 15, now with 2 policies with each set to 15, we will retry 15*2 times in worst scenario.\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OMHA",
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Retry handling in ozone RPC Client"
   },
   {
      "_id": "13286422",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-20 09:14:37",
      "description": "HDDS-1522 introduced a method to run full cluster in IntelliJ. The runner configurations can be copied with a shell script and a basic ozone-site.xml and log configuration to make it easy to run ozone from IDE.\r\n\r\nUnfortunately this setup supports only one Datanode and it's harder to debug full Ozone pipeline (3 datanodes) from IDE.\r\n\r\nThis patch provides 3 different configuration for 3 datanodes with different ports to make it possible to run them on the same host from the IDE.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support running full Ratis pipeline from IDE (IntelliJ) "
   },
   {
      "_id": "13286349",
      "assignee": "xyao",
      "components": [],
      "created": "2020-02-19 23:25:51",
      "description": "Update Ozone to use latest released version of Ratis 0.5.0.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update Ratis version to 0.5.0 released."
   },
   {
      "_id": "13286231",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-19 12:40:00",
      "description": "HDDS-2915 fixed unit/integration check result in case of Maven error.  However, return code check was broken by output redirection via pipeline added in HDDS-2833 and HDDS-2960:\r\n\r\nbq. The return status of a pipeline is the exit status of the last command, unless the pipefail option is enabled.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Broken return code check in unit/integration"
   },
   {
      "_id": "13286167",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-02-19 07:25:38",
      "description": "Writing 32MB chunks fails with various errors.\r\n\r\n{code:title=steps to reproduce}\r\nozone freon dcg -t 1 -n 1 -s 33554432\r\n{code}\r\n\r\n1. With Ratis 0.5.0-90cd474-SNAPSHOT (used by current Ozone master):\r\n{code}\r\norg.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: RESOURCE_EXHAUSTED: gRPC message exceeds maximum size 33554432: 33554686\r\n  at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:524)\r\n  at org.apache.ratis.thirdparty.io.grpc.internal.MessageDeframer.processHeader(MessageDeframer.java:387)\r\n{code}\r\n\r\nWhich is strange, because [Datanode attempts to set max. message size|https://github.com/apache/hadoop-ozone/blob/4ba1932dab4692a9cc1bcfb8903ef650e32ec7ba/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/XceiverServerRatis.java#L204-L206] to 32MB + 16KB.\r\n\r\n2. With Ratis built locally from current Ratis master (46f255cb):\r\n\r\n{code}\r\nCaused by: org.apache.ratis.protocol.StateMachineException: org.apache.ratis.server.raftlog.RaftLogIOException from Server ccb25fbf-9bd1-4094-a632-00f4168213bb@group-B1FA90A78F31: Log entry size 33554666 exceeds the max buffer limit of 33554432\r\n\tat org.apache.ratis.server.raftlog.RaftLog.appendImpl(RaftLog.java:178)\r\n\tat org.apache.ratis.server.raftlog.RaftLog.lambda$append$2(RaftLog.java:157)\r\n\tat org.apache.ratis.server.raftlog.RaftLogSequentialOps$Runner.runSequentially(RaftLogSequentialOps.java:68)\r\n\tat org.apache.ratis.server.raftlog.RaftLog.append(RaftLog.java:157)\r\n\tat org.apache.ratis.server.impl.ServerState.appendLog(ServerState.java:282)\r\n\tat org.apache.ratis.server.impl.RaftServerImpl.appendTransaction(RaftServerImpl.java:518)\r\n\tat org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:604)\r\n{code}\r\n\r\nWith {{ozone.scm.chunk.size=32MB}} setting, {{ozone freon ockg -n 1 -t 1 -s 33554432}} also fails, but without apparent errors in the datanode log.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cannot write 32MB chunks"
   },
   {
      "_id": "13286129",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-02-19 00:48:37",
      "description": "Scenario:\r\n\r\n1.Set up OM HA cluster.\r\n\r\n2. Perform some write operations.\r\n\r\n3. Restart OM's.\r\n\r\n4. Now try any write operation.\r\n\r\nBelow error will be thrown for 15 times, and finally, client request will fail.\r\n{code:java}\r\n\u00a0\r\n2020-02-15 10:11:23,244 [qtp2025269734-19] INFO org.apache.hadoop.io.retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.om.exceptions.OMLeaderNotReadyException): om1@group-D0D586AF6951 is in LEADER state but not ready yet.\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.processReply(OzoneManagerRatisServer.java:177)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.submitRequest(OzoneManagerRatisServer.java:136)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:162)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:118)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n, while invoking $Proxy81.submitRequest over nodeId=om1,nodeAddress=om-ha-1.vpc.cloudera.com:9862 after 1 failover attempts. Trying to failover immediately.\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OMHA",
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM HA- Client requests get LeaderNotReadyException after OM restart"
   },
   {
      "_id": "13286128",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-02-19 00:34:11",
      "description": "On Key Renames, objectID should be preserved from the original Key.\u00a0\r\nCurrently it is being set to the new transactionLogIndex of the rename request.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OMHA",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Key Rename should preserve the ObjectID"
   },
   {
      "_id": "13286020",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-18 15:40:18",
      "description": "Current Ozone code uses the Hadoop version from @InterfaceAudience and @InterfaceStability annotations.\r\n\r\nWhile Hadoop uses the annotations during the javadoc generation, in Ozone they are used only as markers as Ozone doesn't generate javadoc during the releases.\r\n\r\nThe two annotations are in the Hadoop common project. I propose to copy them and use the copied annotations instead of the original one. It would help us to reduce the dependencies on Hadoop (the hadoop-common which contains the original annotations has 87 transitive dependencies!!)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use own version from InterfaceAudience/Stability version"
   },
   {
      "_id": "13285828",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-17 17:14:41",
      "description": "When we do a dist build with -Psrc the README.md of the root project is not packaged to the tar file which makes it impossible to do a build from the source package as the README.md is required by the dist script.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "README is missing from the source release tar"
   },
   {
      "_id": "13285777",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-17 13:00:57",
      "description": "This is the pair/follow-up of HDDS-2974. It provides a new freon test which can set up a new Ratis ring. If the Datanode was instrumented earlier to mock leaders a real Ratis leader can be tested / measured without any overhead on the leader side...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create Freon test to test isolated Ratis LEADER"
   },
   {
      "_id": "13285460",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-02-15 00:42:24",
      "description": "Datanode gets into a loop and keeps throwing errors while trying to close pipeline\r\n\r\n\r\n{code:java}\r\n2020-02-14 00:25:10,208 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07: changes role from  FOLLOWER to CANDIDATE at term 6240 for changeToCandidate\r\n2020-02-14 00:25:10,208 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=02e7e10e-2d50-4ace-a18b-701265ec9f07.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 is in candidate state for 31898494ms\r\n2020-02-14 00:25:10,208 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start LeaderElection\r\n2020-02-14 00:25:10,223 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032: begin an election at term 6241 for 0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null\r\n2020-02-14 00:25:10,259 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-701265EC9F07 not found.\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-701265EC9F07 not found.\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-LeaderElection37032: Election REJECTED; received 0 response(s) [] and 2 exception(s); 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07:t6241, leader=null, voted=285cac09-7622-45e6-be02-b3c68ebf8b10, raftlog=285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07-SegmentedRaftLog:OPENED:c4,f4,i14, conf=0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-701265EC9F07 not found.\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-701265EC9F07 not found.\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-701265EC9F07: changes role from CANDIDATE to FOLLOWER at term 6241 for DISCOVERED_A_NEW_TERM\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown LeaderElection\r\n2020-02-14 00:25:10,270 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start FollowerState\r\n2020-02-14 00:25:10,680 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-DD847EC75388->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12669,entriesCount=0,lastEntry=null\r\n2020-02-14 00:25:10,752 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=7ad5ce51-d3fa-4e71-99f2-dd847ec75388.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31623987ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31618878ms\r\n2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.FollowerState: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-FollowerState: change to CANDIDATE, lastRpcTime:5021ms, electionTimeout:5017ms\r\n2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown FollowerState\r\n2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9: changes role from  FOLLOWER to CANDIDATE at term 6220 for changeToCandidate\r\n2020-02-14 00:25:10,894 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=179ac1d0-e5d5-4898-bef7-0068fd2ea2c9.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 is in candidate state for 31805092ms\r\n2020-02-14 00:25:10,894 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start LeaderElection\r\n2020-02-14 00:25:10,917 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033: begin an election at term 6221 for 0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-0068FD2EA2C9 not found.\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-0068FD2EA2C9 not found.\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-LeaderElection37033: Election REJECTED; received 0 response(s) [] and 2 exception(s); 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9:t6221, leader=null, voted=285cac09-7622-45e6-be02-b3c68ebf8b10, raftlog=285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9-SegmentedRaftLog:OPENED:c0,f0,i8, conf=0: [d432c890-5ec4-4cf1-9078-28497a08ab85:10.65.6.227:9858, 285cac09-7622-45e6-be02-b3c68ebf8b10:10.65.24.80:9858, cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e:10.65.8.165:9858], old=null\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e: group-0068FD2EA2C9 not found.\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: d432c890-5ec4-4cf1-9078-28497a08ab85: group-0068FD2EA2C9 not found.\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RaftServerImpl: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-0068FD2EA2C9: changes role from CANDIDATE to FOLLOWER at term 6221 for DISCOVERED_A_NEW_TERM\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown LeaderElection\r\n2020-02-14 00:25:10,921 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: start FollowerState\r\n2020-02-14 00:25:11,134 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-DD847EC75388->cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12669,entriesCount=0,lastEntry=null\r\n2020-02-14 00:25:11,218 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=7ad5ce51-d3fa-4e71-99f2-dd847ec75388.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31624453ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31619344ms\r\n2020-02-14 00:25:11,347 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-2338B042C07B->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12579,entriesCount=0,lastEntry=null\r\n2020-02-14 00:25:11,361 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-2338B042C07B->cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12577,entriesCount=0,lastEntry=null\r\n2020-02-14 00:25:11,399 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=6a851c59-0345-4ad8-ac31-2338b042c07b.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31396085ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31391530ms\r\n2020-02-14 00:25:11,406 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=6a851c59-0345-4ad8-ac31-2338b042c07b.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31396092ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31391537ms\r\n2020-02-14 00:25:11,423 WARN org.apache.ratis.grpc.server.GrpcLogAppender: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-BA1E8724EE74->d432c890-5ec4-4cf1-9078-28497a08ab85-GrpcLogAppender: HEARTBEAT appendEntries Timeout, request=AppendEntriesRequest:cid=12817,entriesCount=0,lastEntry=null\r\n2020-02-14 00:25:11,490 ERROR org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis: pipeline Action CLOSE  on pipeline PipelineID=1ed1be53-b526-41af-bdf9-ba1e8724ee74.Reason : 285cac09-7622-45e6-be02-b3c68ebf8b10 has not seen follower/s d432c890-5ec4-4cf1-9078-28497a08ab85 for 31946345ms cabbdef8-ed6c-4fc7-b7b2-d1ddd07da47e for 31945978ms\r\n2020-02-14 00:25:11,909 INFO org.apache.ratis.server.impl.FollowerState: 285cac09-7622-45e6-be02-b3c68ebf8b10@group-D506E1A1894E-FollowerState: change to CANDIDATE, lastRpcTime:5094ms, electionTimeout:5093ms\r\n2020-02-14 00:25:11,909 INFO org.apache.ratis.server.impl.RoleInfo: 285cac09-7622-45e6-be02-b3c68ebf8b10: shutdown FollowerState\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Datanode unable to close Pipeline after disk out of space"
   },
   {
      "_id": "13285409",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2020-02-14 20:17:29",
      "description": "ozone s3 path <<bucketname>>\r\n\r\nozone s3 path are not working on OM HA cluster\r\n\r\n\u00a0\r\n\r\nBecause these commands do not take URI as a parameter. And for shell in HA, passing URI is mandatory.\u00a0\r\n\r\n\u00a0\r\n\r\nBelow is the output when running on OM HA cluster:\r\n\r\n\u00a0\r\n{code:java}\r\n$ozone s3 path\r\nService ID or host name must not be omitted when ozone.om.service.ids is defined.\r\n{code}\r\n\u00a0\r\n\r\nHDDS-2279 fixed only getsecret, and this is missed because parameter added om-service-id is applicable only getsecret command.\r\n\r\n\u00a0\r\n\r\nThis Jira suggests to do, move parameter to common class named S3Handler, and make all S3 commands inherit this, so in future any new commands, this parameter will be applicable.\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nThank You [~chinseone]\u00a0for reporting this issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "OMHA",
         "OMHATest",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": " Ozone S3 CLI path command not working on HA cluster"
   },
   {
      "_id": "13285251",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-02-14 04:31:23",
      "description": "The test failure is bcoz the exception msg in the KeyOutputStream is overriden with a hardcoded string in case all failures where the test expects the underlying exception msg to be propagated.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestOzoneClientRetriesOnException.java"
   },
   {
      "_id": "13285250",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-02-14 04:30:46",
      "description": "The Jira aims to enable\u00a0{color:#172b4d}TestContainerStateMachine tests.{color}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestContainerStateMachine.java"
   },
   {
      "_id": "13285249",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-02-14 04:30:12",
      "description": "The unit tests are written withe single node ratis into consideration. The expectation is the datanode fails, client should see an exception after next io as there is no new dn for new pipeline to form which is not happening as the cluster is created with multiple dns.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestContainerStateMachineFailures.java"
   },
   {
      "_id": "13285213",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-13 23:12:33",
      "description": "Refreshing any URL of Recon UI (ex:\u00a0[http://localhost:9888/Datanodes|http://vsubramanian-cdh-4.vpc.cloudera.com:9888/Datanodes]) throws 404 error.\r\n\r\n\r\n{code:java}\r\nHTTP ERROR 404 Not Found\r\nURI:/Datanodes\r\nSTATUS:404\r\nMESSAGE:Not Found\r\nSERVLET:org.eclipse.jetty.servlet.DefaultServlet-4d9d1b69\r\n{code}\r\n\r\nThis 404 should only happen with URLs that match \"/api\" pattern and not with the \"/\".\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Refreshing Recon UI results in 404"
   },
   {
      "_id": "13285074",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-02-13 12:56:20",
      "description": "With RATIS-755, a log dump utility for ratis logs has been added. however to parse SM data, a toString supplier is needed to dump the log to printable form. This can be in the form of JSON , XML.\r\n\r\ncc:  [~hanishakoneru][~bharat]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "parse and dump ozonemanager ratis segment file to printable text"
   },
   {
      "_id": "13284749",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-02-12 08:47:01",
      "description": "To conclude a little,\u00a0_+{color:#ff0000}major issues{color}+_\u00a0that I find:\r\n # When I do a long running s3g writing to cluster with OM HA and I stop the Om leader to force a re-election, the writing will stop and can never recover.\r\n\r\n--updates 2020-02-20:\r\n\r\nhttps://issues.apache.org/jira/browse/HDDS-3031\u00a0{color:#ff0000}fixes{color} this issue.\r\n\r\n\u00a0\r\n\r\n2. If I force a OM re-election and do a scm restart after that, the cluster cannot see any leader datanode and no datanodes are able to send pipeline reports, which makes the cluster unavailable as well. I consider this a multi-failover case when the leader OM and SCM are on the same node and there is a short outage happen to the node.\r\n\r\n\u00a0\r\n\r\n--updates 2020-02-20:\r\n\r\n\u00a0When you do a jar swap for a new version of Ozone and enable OM HA while keeping the same ozone-site.xml as last time, if you've written some data into the last Ozone cluster (and therefore there are existing versions and metadata for om and scm), SCM cannot be up after the jar swap.\r\n\r\n{color:#ff0000}Error logs{color}:\u00a0PipelineID=aae4f728-82ef-4bbb-a0a5-7b3f2af030cc not found in scm out logs when scm process cannot be started.\r\n\r\n\u00a0\r\n\r\n--updates 2020-02-24:\r\n\r\nAfter I add some logs to SCM starter:\r\nAssuming SCM is only bounced after the leader OM is stopped\r\n1. If SCM is bounced {color:#de350b}after{color} former leader OM is restarted, meaning all OMs are up, SCM will be bootstrapped correctly but there will be missing pipeline report from the node who doesn't have OM process on it (it's always him tho). This would cause all pipelines stay at ALLOCATED state and cluster will be in safemode. At this point, if I {color:#de350b}restart the blacksheep datanode{color}, it will come back and send the pipeline report to SCM and all pipelines will be at OPEN state.\r\n2. If SCM is bounced {color:#de350b}before{color} the former leader OM is restarted, meaning not all OMs in ratis ring are up, SCM {color:#de350b}cannot{color} be bootstrapped correctly and it shows Pipeline not found.\r\n\r\n\u00a0\r\n\r\nOriginal posting:\r\n\r\nUse S3 gateway to keep writing data into a specific s3 gateway endpoint. After the writer starts to work, I kill the OM process on the OM leader host. After that, the s3 gateway can never allow writing data and keeps reporting InternalError for all new coming keys.\r\n\r\nProcess Process-488:\r\n{noformat}\r\n S3UploadFailedError: Failed to upload ./20191204/file1056.dat to ozone-test-reproduce-123/./20191204/file1056.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\r\n Process Process-489:\r\n S3UploadFailedError: Failed to upload ./20191204/file9631.dat to ozone-test-reproduce-123/./20191204/file9631.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\r\n Process Process-490:\r\n S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\r\n Process Process-491:\r\n S3UploadFailedError: Failed to upload ./20191204/file4220.dat to ozone-test-reproduce-123/./20191204/file4220.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\r\n Process Process-492:\r\n S3UploadFailedError: Failed to upload ./20191204/file5523.dat to ozone-test-reproduce-123/./20191204/file5523.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\r\n Process Process-493:\r\n S3UploadFailedError: Failed to upload ./20191204/file7520.dat to ozone-test-reproduce-123/./20191204/file7520.dat: An error occurred (500) when calling the PutObject operation (reached max retries: 4): Internal Server Error\r\n{noformat}\r\n\r\nThat's a partial list and note that all keys are different. I also tried re-enable the OM process on previous leader OM, but it doesn't help since the leader has changed. Also attach partial OM logs:\r\n{noformat}\r\n 2020-02-12 14:57:11,128 [IPC Server handler 72 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 72 on 9862, call Call#4859 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561\r\n org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)\r\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\r\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n 2020-02-12 14:57:11,918 [IPC Server handler 159 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 159 on 9862, call Call#4864 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561\r\n org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)\r\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\r\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n 2020-02-12 14:57:15,395 [IPC Server handler 23 on 9862] INFO org.apache.hadoop.ipc.Server: IPC Server handler 23 on 9862, call Call#4869 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 9.134.50.210:36561\r\n org.apache.hadoop.ozone.om.exceptions.OMNotLeaderException: OM:om1 is not the leader. Suggested leader is OM:om2.\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.createNotLeaderException(OzoneManagerProtocolServerSideTranslatorPB.java:183)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:171)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:107)\r\n at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:97)\r\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n{noformat}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nAlso attach the ozone-site.xml config to enable OM HA:\r\n{noformat}\r\n<property>\r\n <name>ozone.om.service.ids</name>\r\n <value>OMHA</value>\r\n </property>\r\n <property>\r\n <name>ozone.om.nodes.OMHA</name>\r\n <value>om1,om2,om3</value>\r\n </property>\r\n <property>\r\n <name>ozone.om.node.id</name>\r\n <value>om1</value>\r\n </property>\r\n <property>\r\n <name>ozone.om.address.OMHA.om1</name>\r\n <value>9.134.50.210:9862</value>\r\n </property>\r\n <property>\r\n <name>ozone.om.address.OMHA.om2</name>\r\n <value>9.134.51.215:9862</value>\r\n </property>\r\n <property>\r\n <name>ozone.om.address.OMHA.om3</name>\r\n <value>9.134.51.25:9862</value>\r\n </property>\r\n <property>\r\n <name>ozone.om.ratis.enable</name>\r\n <value>true</value>\r\n </property>\r\n <property>\r\n <name>ozone.enabled</name>\r\n <value>true</value>\r\n <tag>OZONE, REQUIRED</tag>\r\n <description>\r\n Status of the Ozone Object Storage service is enabled.\r\n Set to true to enable Ozone.\r\n Set to false to disable Ozone.\r\n Unless this value is set to true, Ozone services will not be started in\r\n the cluster.\r\n\r\nPlease note: By default ozone is disabled on a hadoop cluster.\r\n </description>\r\n </property>\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OMHATest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OM HA stability issues"
   },
   {
      "_id": "13284596",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-11 14:22:18",
      "description": "hdds-common project is shared between all the client and server projects. hdds-server-framework project is shared between all the server side services.\r\n\r\nTo reduce unnecessary dependencies (to Hadoop, for example) we can move all the server-side related classes (eg. rocksdb layer, certificate tools) to the framework from the common.\r\n\r\nWe don't need the rocksdb utilities and certificate tools on the client side.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move server-related shared utilities from common to framework"
   },
   {
      "_id": "13284523",
      "assignee": "elek",
      "components": [],
      "created": "2020-02-11 08:33:47",
      "description": "Before we started to use github actions we had the opportunity to use some \"commands\" in github commants. For example when a `/label xxx` comment has been added to a PR, a bot added the label (by default just the committers can use labels, but with this approach it was possible for everyone).\r\n\r\n\u00a0\r\n\r\nSince the move to use github actions I got multiple question about re-triggering the test. Even it it's possible to do with pushing an empty commit (only by the owner or committer) I think it would be better to restore the support of comment commands.\r\n\r\n\u00a0\r\n\r\nThis patch follows a very simple approach. The available commands are store in a separated subdirectory as shell scripts and they are called by a lightweight wrapper.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support github comment based commands"
   },
   {
      "_id": "13284447",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-10 23:44:42",
      "description": "We need a REST API in Recon to serve up information for the Datanodes page (HDDS-2827). The REST API can also include other useful methods present in NodeManager that gives the user information about the Nodes in the cluster.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create REST API to serve Node information and integrate with UI in Recon."
   },
   {
      "_id": "13284430",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-10 23:01:01",
      "description": "* Verify Recon gets pipeline, node and container report from Datanode.\r\n* Verify SCM metadata state == Recon metadata state (Create pipeline , Close pipeline, create container)\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add integration test for Recon's Passive SCM state."
   },
   {
      "_id": "13284345",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-10 15:11:34",
      "description": "Contract tests use a random volume/bucket for each test case.  Volume/bucket creation fails if the same random number happens to be generated for another test case in the same class.\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/runs/432759893}\r\n2020-02-07T21:45:36.7489239Z [ERROR] Tests run: 18, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 43.647 s <<< FAILURE! - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus\r\n2020-02-07T21:45:36.7492173Z [ERROR] testListStatusNoDir(org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus)  Time elapsed: 0.053 s  <<< ERROR!\r\n2020-02-07T21:45:36.7493036Z VOLUME_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Volume already exists\r\n...\r\n2020-02-07T21:45:36.7776813Z \tat org.apache.hadoop.ozone.TestDataUtil.createVolumeAndBucket(TestDataUtil.java:60)\r\n2020-02-07T21:45:36.7779898Z \tat org.apache.hadoop.ozone.TestDataUtil.createVolumeAndBucket(TestDataUtil.java:93)\r\n2020-02-07T21:45:36.7780272Z \tat org.apache.hadoop.fs.ozone.contract.OzoneContract.getTestFileSystem(OzoneContract.java:83)\r\n2020-02-07T21:45:36.7784467Z \tat org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:181)\r\n2020-02-07T21:45:36.7784916Z \tat org.apache.hadoop.fs.contract.AbstractContractGetFileStatusTest.setup(AbstractContractGetFileStatusTest.java:56)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Handle existing volume/bucket in contract tests"
   },
   {
      "_id": "13283951",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-02-07 14:27:35",
      "description": "The grpc flow control window by default is set to 1Mb by default. During performance tests, it was observed that the flow control window has to be greater than the chunk size for optimum performance.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Set the default value of grpc flow control window for ratis client and ratis server"
   },
   {
      "_id": "13283858",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-07 07:51:31",
      "description": "{code:title=https://github.com/apache/hadoop-ozone/runs/430663688}\r\n2020-02-06T21:44:53.5319531Z [ERROR] Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.344 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.block.TestBlockManager\r\n2020-02-06T21:44:53.5319796Z [ERROR] testMultipleBlockAllocation(org.apache.hadoop.hdds.scm.block.TestBlockManager)  Time elapsed: 1.167 s  <<< ERROR!\r\n2020-02-06T21:44:53.5319942Z java.util.concurrent.TimeoutException: \r\n2020-02-06T21:44:53.5320496Z Timed out waiting for condition. Thread diagnostics:\r\n2020-02-06T21:44:53.5320839Z Timestamp: 2020-02-06 09:44:52,261\r\n2020-02-06T21:44:53.5320901Z \r\n2020-02-06T21:44:53.5321178Z \"Thread-26\"  prio=5 tid=46 runnable\r\n2020-02-06T21:44:53.5321292Z java.lang.Thread.State: RUNNABLE\r\n2020-02-06T21:44:53.5321391Z         at java.lang.Thread.dumpThreads(Native Method)\r\n2020-02-06T21:44:53.5326891Z         at java.lang.Thread.getAllStackTraces(Thread.java:1610)\r\n2020-02-06T21:44:53.5327144Z         at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87)\r\n2020-02-06T21:44:53.5327309Z         at org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73)\r\n2020-02-06T21:44:53.5327465Z         at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)\r\n2020-02-06T21:44:53.5327618Z         at org.apache.hadoop.hdds.scm.block.TestBlockManager.testMultipleBlockAllocation(TestBlockManager.java:280)\r\n2020-02-06T21:44:53.5388042Z         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n2020-02-06T21:44:53.5388702Z         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n2020-02-06T21:44:53.5388905Z         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n2020-02-06T21:44:53.5389045Z         at java.lang.reflect.Method.invoke(Method.java:498)\r\n2020-02-06T21:44:53.5389195Z         at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n2020-02-06T21:44:53.5389331Z         at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n2020-02-06T21:44:53.5389662Z         at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n2020-02-06T21:44:53.5389776Z         at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n2020-02-06T21:44:53.5389916Z         at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n2020-02-06T21:44:53.5390040Z \"Signal Dispatcher\" daemon prio=9 tid=4 runnable\r\n2020-02-06T21:44:53.5390156Z java.lang.Thread.State: RUNNABLE\r\n2020-02-06T21:44:53.5390783Z \"EventQueue-CloseContainerForCloseContainerEventHandler\"  prio=5 tid=32 in Object.wait()\r\n2020-02-06T21:44:53.5390916Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5391019Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5391149Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n2020-02-06T21:44:53.5391299Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n2020-02-06T21:44:53.5391448Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\r\n2020-02-06T21:44:53.5391587Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5391721Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5391844Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5391971Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5392100Z \"IPC Server idle connection scanner for port 43801\" daemon prio=5 tid=24 in Object.wait()\r\n2020-02-06T21:44:53.5392227Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5392347Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5392463Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5392567Z         at java.util.TimerThread.mainLoop(Timer.java:526)\r\n2020-02-06T21:44:53.5392694Z         at java.util.TimerThread.run(Timer.java:505)\r\n2020-02-06T21:44:53.5393004Z \"Thread-28\" daemon prio=5 tid=48 timed_waiting\r\n2020-02-06T21:44:53.5393121Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5393232Z         at java.lang.Thread.sleep(Native Method)\r\n2020-02-06T21:44:53.5393352Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler.lambda$onMessage$0(SafeModeHandler.java:113)\r\n2020-02-06T21:44:53.5393504Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler$$Lambda$38/725596393.run(Unknown Source)\r\n2020-02-06T21:44:53.5393634Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5393927Z \"pool-9-thread-1\"  prio=5 tid=45 timed_waiting\r\n2020-02-06T21:44:53.5394061Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5394260Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5406780Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\r\n2020-02-06T21:44:53.5427435Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\r\n2020-02-06T21:44:53.5428120Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\r\n2020-02-06T21:44:53.5428601Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\r\n2020-02-06T21:44:53.5428758Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5428918Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5429052Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5429184Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5429297Z \"main\"  prio=5 tid=1 timed_waiting\r\n2020-02-06T21:44:53.5429405Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5429501Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5429775Z         at java.lang.Thread.join(Thread.java:1260)\r\n2020-02-06T21:44:53.5429900Z         at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:26)\r\n2020-02-06T21:44:53.5430034Z         at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:17)\r\n2020-02-06T21:44:53.5430162Z         at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n2020-02-06T21:44:53.5468996Z         at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n2020-02-06T21:44:53.5469178Z         at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)\r\n2020-02-06T21:44:53.5469334Z         at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n2020-02-06T21:44:53.5469466Z         at org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n2020-02-06T21:44:53.5469587Z         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n2020-02-06T21:44:53.5469708Z         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n2020-02-06T21:44:53.5469841Z         at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n2020-02-06T21:44:53.5469967Z         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n2020-02-06T21:44:53.5470091Z         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n2020-02-06T21:44:53.5470215Z         at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n2020-02-06T21:44:53.5470339Z         at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n2020-02-06T21:44:53.5470449Z         at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n2020-02-06T21:44:53.5470577Z         at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n2020-02-06T21:44:53.5470706Z         at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n2020-02-06T21:44:53.5470851Z         at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n2020-02-06T21:44:53.5470989Z         at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n2020-02-06T21:44:53.5471123Z         at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n2020-02-06T21:44:53.5471252Z         at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n2020-02-06T21:44:53.5471391Z         at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n2020-02-06T21:44:53.5471525Z         at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n2020-02-06T21:44:53.5471791Z         at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n2020-02-06T21:44:53.5471932Z \"IPC Server idle connection scanner for port 37265\" daemon prio=5 tid=39 in Object.wait()\r\n2020-02-06T21:44:53.5472061Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5472164Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5472277Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5472395Z         at java.util.TimerThread.mainLoop(Timer.java:526)\r\n2020-02-06T21:44:53.5509093Z         at java.util.TimerThread.run(Timer.java:505)\r\n2020-02-06T21:44:53.5509324Z \"Socket Reader #1 for port 37265\"  prio=5 tid=38 runnable\r\n2020-02-06T21:44:53.5509481Z java.lang.Thread.State: RUNNABLE\r\n2020-02-06T21:44:53.5509580Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\r\n2020-02-06T21:44:53.5509772Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\r\n2020-02-06T21:44:53.5509940Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\r\n2020-02-06T21:44:53.5510121Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\r\n2020-02-06T21:44:53.5510287Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\r\n2020-02-06T21:44:53.5510624Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\r\n2020-02-06T21:44:53.5510740Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)\r\n2020-02-06T21:44:53.5510912Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)\r\n2020-02-06T21:44:53.5511550Z \"EventQueue-DatanodeCommandForDatanodeCommandHandler\"  prio=5 tid=31 in Object.wait()\r\n2020-02-06T21:44:53.5511733Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5511887Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5511993Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n2020-02-06T21:44:53.5512181Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n2020-02-06T21:44:53.5512357Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\r\n2020-02-06T21:44:53.5512530Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5512725Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5512895Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5513065Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5513394Z \"ForkJoinPool.commonPool-worker-1\" daemon prio=5 tid=30 timed_waiting\r\n2020-02-06T21:44:53.5513552Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5513695Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5513849Z         at java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)\r\n2020-02-06T21:44:53.5753126Z         at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)\r\n2020-02-06T21:44:53.5806455Z         at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\r\n2020-02-06T21:44:53.5807208Z \"EventQueue-DatanodeCommandForDatanodeCommandHandler\"  prio=5 tid=49 in Object.wait()\r\n2020-02-06T21:44:53.5807347Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5807455Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5807562Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n2020-02-06T21:44:53.5807703Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n2020-02-06T21:44:53.5807837Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\r\n2020-02-06T21:44:53.5807968Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5808098Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5808388Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5808510Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5808851Z \"EventQueue-SafemodestatusForSafeModeHandler\"  prio=5 tid=47 in Object.wait()\r\n2020-02-06T21:44:53.5808977Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5809088Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5809206Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n2020-02-06T21:44:53.5809346Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n2020-02-06T21:44:53.5809471Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\r\n2020-02-06T21:44:53.5809599Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5809731Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5809862Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5809982Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5810366Z \"Thread-14\" daemon prio=5 tid=29 timed_waiting\r\n2020-02-06T21:44:53.5810463Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5810573Z         at java.lang.Thread.sleep(Native Method)\r\n2020-02-06T21:44:53.5810706Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler.lambda$onMessage$0(SafeModeHandler.java:113)\r\n2020-02-06T21:44:53.5810853Z         at org.apache.hadoop.hdds.scm.safemode.SafeModeHandler$$Lambda$38/725596393.run(Unknown Source)\r\n2020-02-06T21:44:53.5810983Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5811095Z \"Finalizer\" daemon prio=8 tid=3 in Object.wait()\r\n2020-02-06T21:44:53.5811193Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5811312Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5811435Z         at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)\r\n2020-02-06T21:44:53.5811563Z         at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)\r\n2020-02-06T21:44:53.5811691Z         at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)\r\n2020-02-06T21:44:53.5811875Z \"IPC Server idle connection scanner for port 39545\" daemon prio=5 tid=16 in Object.wait()\r\n2020-02-06T21:44:53.5811991Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5812106Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5812217Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5812333Z         at java.util.TimerThread.mainLoop(Timer.java:526)\r\n2020-02-06T21:44:53.5812449Z         at java.util.TimerThread.run(Timer.java:505)\r\n2020-02-06T21:44:53.5812553Z \"Reference Handler\" daemon prio=10 tid=2 in Object.wait()\r\n2020-02-06T21:44:53.5812666Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5812784Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5812896Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5813013Z         at java.lang.ref.Reference.tryHandlePending(Reference.java:191)\r\n2020-02-06T21:44:53.5813143Z         at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)\r\n2020-02-06T21:44:53.5813253Z \"Socket Reader #1 for port 45489\"  prio=5 tid=34 runnable\r\n2020-02-06T21:44:53.5813364Z java.lang.Thread.State: RUNNABLE\r\n2020-02-06T21:44:53.5813478Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\r\n2020-02-06T21:44:53.5813599Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\r\n2020-02-06T21:44:53.5813725Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\r\n2020-02-06T21:44:53.5813836Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\r\n2020-02-06T21:44:53.5813963Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\r\n2020-02-06T21:44:53.5814141Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\r\n2020-02-06T21:44:53.5814273Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)\r\n2020-02-06T21:44:53.5814402Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)\r\n2020-02-06T21:44:53.5814533Z \"Socket Reader #1 for port 39771\"  prio=5 tid=42 runnable\r\n2020-02-06T21:44:53.5814630Z java.lang.Thread.State: RUNNABLE\r\n2020-02-06T21:44:53.5814744Z         at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\r\n2020-02-06T21:44:53.5814866Z         at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\r\n2020-02-06T21:44:53.5814990Z         at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93)\r\n2020-02-06T21:44:53.5815117Z         at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\r\n2020-02-06T21:44:53.5815222Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\r\n2020-02-06T21:44:53.5815342Z         at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\r\n2020-02-06T21:44:53.5815472Z         at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1097)\r\n2020-02-06T21:44:53.5815601Z         at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1076)\r\n2020-02-06T21:44:53.5816007Z \"EventQueue-SafemodestatusForSafeModeHandler\"  prio=5 tid=28 in Object.wait()\r\n2020-02-06T21:44:53.5816130Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5816230Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5816350Z         at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n2020-02-06T21:44:53.5816493Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n2020-02-06T21:44:53.5816633Z         at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\r\n2020-02-06T21:44:53.5816769Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5816906Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5817026Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5817155Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5817278Z \"IPC Server idle connection scanner for port 39771\" daemon prio=5 tid=43 in Object.wait()\r\n2020-02-06T21:44:53.5817404Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5817520Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5817628Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5817728Z         at java.util.TimerThread.mainLoop(Timer.java:526)\r\n2020-02-06T21:44:53.5817844Z         at java.util.TimerThread.run(Timer.java:505)\r\n2020-02-06T21:44:53.5817967Z \"IPC Server idle connection scanner for port 45489\" daemon prio=5 tid=35 in Object.wait()\r\n2020-02-06T21:44:53.5818089Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5818205Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5818301Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5818413Z         at java.util.TimerThread.mainLoop(Timer.java:526)\r\n2020-02-06T21:44:53.5818533Z         at java.util.TimerThread.run(Timer.java:505)\r\n2020-02-06T21:44:53.5818647Z \"process reaper\" daemon prio=10 tid=11 timed_waiting\r\n2020-02-06T21:44:53.5818757Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5818864Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5818970Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\r\n2020-02-06T21:44:53.5819108Z         at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)\r\n2020-02-06T21:44:53.5819245Z         at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)\r\n2020-02-06T21:44:53.5819376Z         at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)\r\n2020-02-06T21:44:53.5819581Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073)\r\n2020-02-06T21:44:53.5819716Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5819837Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5819951Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5820283Z \"surefire-forkedjvm-command-thread\" daemon prio=5 tid=9 runnable\r\n2020-02-06T21:44:53.5820398Z java.lang.Thread.State: RUNNABLE\r\n2020-02-06T21:44:53.5820509Z         at java.io.FileInputStream.readBytes(Native Method)\r\n2020-02-06T21:44:53.5820617Z         at java.io.FileInputStream.read(FileInputStream.java:255)\r\n2020-02-06T21:44:53.5820740Z         at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n2020-02-06T21:44:53.5820867Z         at java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n2020-02-06T21:44:53.5820996Z         at java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n2020-02-06T21:44:53.5821131Z         at org.apache.maven.surefire.booter.MasterProcessCommand.decode(MasterProcessCommand.java:115)\r\n2020-02-06T21:44:53.5821272Z         at org.apache.maven.surefire.booter.CommandReader$CommandRunnable.run(CommandReader.java:390)\r\n2020-02-06T21:44:53.5821432Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5821557Z \"IPC Server idle connection scanner for port 40327\" daemon prio=5 tid=20 in Object.wait()\r\n2020-02-06T21:44:53.5821681Z java.lang.Thread.State: WAITING (on object monitor)\r\n2020-02-06T21:44:53.5821796Z         at java.lang.Object.wait(Native Method)\r\n2020-02-06T21:44:53.5821906Z         at java.lang.Object.wait(Object.java:502)\r\n2020-02-06T21:44:53.5822020Z         at java.util.TimerThread.mainLoop(Timer.java:526)\r\n2020-02-06T21:44:53.5822121Z         at java.util.TimerThread.run(Timer.java:505)\r\n2020-02-06T21:44:53.5822442Z \"surefire-forkedjvm-ping-30s\" daemon prio=5 tid=10 timed_waiting\r\n2020-02-06T21:44:53.5822562Z java.lang.Thread.State: TIMED_WAITING\r\n2020-02-06T21:44:53.5822668Z         at sun.misc.Unsafe.park(Native Method)\r\n2020-02-06T21:44:53.5822785Z         at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\r\n2020-02-06T21:44:53.5822918Z         at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\r\n2020-02-06T21:44:53.5823066Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\r\n2020-02-06T21:44:53.5823218Z         at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\r\n2020-02-06T21:44:53.5823353Z         at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n2020-02-06T21:44:53.5823483Z         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n2020-02-06T21:44:53.5823618Z         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n2020-02-06T21:44:53.5823739Z         at java.lang.Thread.run(Thread.java:748)\r\n2020-02-06T21:44:53.5823800Z \r\n2020-02-06T21:44:53.5823847Z \r\n2020-02-06T21:44:53.5823955Z \tat org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)\r\n2020-02-06T21:44:53.5824106Z \tat org.apache.hadoop.hdds.scm.block.TestBlockManager.testMultipleBlockAllocation(TestBlockManager.java:280)\r\n2020-02-06T21:44:53.5824242Z \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n2020-02-06T21:44:53.5824367Z \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n2020-02-06T21:44:53.5824639Z \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n2020-02-06T21:44:53.5824876Z \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n2020-02-06T21:44:53.5825000Z \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n2020-02-06T21:44:53.5825204Z \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n2020-02-06T21:44:53.5825339Z \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n2020-02-06T21:44:53.5825465Z \tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n2020-02-06T21:44:53.5825599Z \tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Intermittent timeout in TestBlockManager"
   },
   {
      "_id": "13283797",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-02-06 22:43:02",
      "description": "This Jira is to use getPropertiesByPrefix that match with \"datanode.ratis\" and set them during ratisClient and ratisServer creation.\r\n\r\n\u00a0\r\n\r\nRight now in the current code we use getValByRegex to match with regex, we can avoid that.\r\n\r\n*2 Changes will go in this Jira.*\r\n\r\n1.\u00a0 Use getPropertiesByPrefix and set RatisClient and Server Conf.\r\n\r\n2. Use the same prefix for RatisClient and Server Conf. (Right now we use prefix only for matching Ratis Server Conf)\r\n\r\n\u00a0\r\n\r\ncc [~aengineer]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use getPropertiesByPrefix instead of regex in matching ratis client and server properties"
   },
   {
      "_id": "13283787",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-02-06 21:51:33",
      "description": "Add a metric that will help in understanding the Average flush time which will help in understanding how the flush time increases over time.\r\n\r\nAdd another metric to show the average number of flush transactions in an iteration. This will show how many transactions are flushed in a single iteration over time.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add metrics to OM DoubleBuffer"
   },
   {
      "_id": "13283590",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-02-05 22:48:27",
      "description": "When running teragen in one of the run observed this error.\r\n{code:java}\r\n05 14:43:16,635 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: java.lang.NullPointerException at java.util.Objects.requireNonNull(Objects.java:203) at java.util.Optional.<init>(Optional.java:96) at java.util.Optional.of(Optional.java:108) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineMetrics.incNumBlocksAllocated(SCMPipelineMetrics.java:118) at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.incNumBlocksAllocatedMetric(SCMPipelineManager.java:520) at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.newBlock(BlockManagerImpl.java:265) at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:233) at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:188) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:159) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:117) at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72) at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:98) at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:984) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:912) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2882) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:792) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createFile(OzoneManagerProtocolClientSideTranslatorPB.java:1596) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71) at com.sun.proxy.$Proxy18.createFile(Unknown Source) at org.apache.hadoop.ozone.client.rpc.RpcClient.createFile(RpcClient.java:1071) at org.apache.hadoop.ozone.client.OzoneBucket.createFile(OzoneBucket.java:538) at org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.createFile(BasicOzoneClientAdapterImpl.java:208) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.createOutputStream(BasicOzoneFileSystem.java:256) at org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.create(BasicOzoneFileSystem.java:237) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1133) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1113) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1002) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:990) at org.apache.hadoop.examples.terasort.TeraOutputFormat.getRecordWriter(TeraOutputFormat.java:141) at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:659) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:779) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168){code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allocate Block failing with NPE"
   },
   {
      "_id": "13283476",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-05 13:28:32",
      "description": "Recently acceptance tests have been failing with {{DISK_OUT_OF_SPACE}} error and/or timeouts.\r\n\r\n{code}\r\nContainer creation failed, due to disk out of space , Result: DISK_OUT_OF_SPACE\r\n...\r\nDiskOutOfSpaceException: Out of space: The volume with the most available space (=5069360506 B) is less than the container size (=5368709120 B).\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Acceptance test failures due to lack of disk space"
   },
   {
      "_id": "13283377",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-02-05 06:02:23",
      "description": "{code}\r\nCaused by: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ssl/SslContextFactory$Server\r\n        at org.apache.hadoop.hdfs.DFSUtil.httpServerTemplateForNNAndJN(DFSUtil.java:1590)\r\n        at org.apache.hadoop.hdds.server.BaseHttpServer.<init>(BaseHttpServer.java:83)\r\n        at org.apache.hadoop.ozone.recon.ReconHttpServer.<init>(ReconHttpServer.java:35)\r\n        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n        at com.google.inject.internal.DefaultConstructionProxyFactory$2.newInstance(DefaultConstructionProxyFactory.java:86)\r\n        at com.google.inject.internal.ConstructorInjector.provision(ConstructorInjector.java:105)\r\n        at com.google.inject.internal.ConstructorInjector.construct(ConstructorInjector.java:85)\r\n        at com.google.inject.internal.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:267)\r\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:46)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1103)\r\n        at com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\r\n        at com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:145)\r\n        at com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:41)\r\n        at com.google.inject.internal.InjectorImpl$2$1.call(InjectorImpl.java:1016)\r\n        at com.google.inject.internal.InjectorImpl.callInContext(InjectorImpl.java:1092)\r\n        at com.google.inject.internal.InjectorImpl$2.get(InjectorImpl.java:1012)\r\n        ... 13 more\r\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.ssl.SslContextFactory$Server\r\n        at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:583)\r\n        at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\r\n        at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n        ... 32 more\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon server start failed with CNF exception in a cluster with Auto TLS enabled."
   },
   {
      "_id": "13283341",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-02-05 00:07:17",
      "description": "During KeyCreate (and S3InitiateMultipartUpload), we do not check the OpenKeyTable if the key already exists. If it does exist and the transaction is replayed, we just override the key in OpenKeyTable. This is done to avoid extra DB reads.\r\n\r\nDuring KeyCommit (or S3MultipartUploadCommit), if the key was already committed, then we do not replay the transaction. This would result in the OpenKeyTable entry to remain in the DB till it is garbage collected.\u00a0\r\n\r\nTo avoid storing stale entries in OpenKeyTable, during commit replays, we should check the openKeyTable and delete the entry if it exists.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Delete replayed entry from OpenKeyTable during commit"
   },
   {
      "_id": "13283218",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-02-04 10:36:52",
      "description": "{code}\r\nCreate volume with non-admin user                                     | FAIL |\r\n...\r\nPERMISSION_DENIED Only admin users are authorized to create Ozone volumes. User: testuser2/scm@EXAMPLE.COM' does not contain 'Client cannot authenticate via'\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozonesecure acceptance test fails due to unexpected error message"
   },
   {
      "_id": "13282995",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2020-02-03 15:01:41",
      "description": "Freon prints progress to log in non-interactive mode since HDDS-2861, but summary statistics are still printed to standard output.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Print Freon summary to log in non-interactive mode"
   },
   {
      "_id": "13282460",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-30 19:31:24",
      "description": "To ensure that Prefix acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMPrefixAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of OM Prefix ACL requests"
   },
   {
      "_id": "13282347",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-01-30 10:05:09",
      "description": "Save output of crashed unit/integration tests to the diagnostic bundle, similar to how it's done for failed tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Collect output of crashed tests"
   },
   {
      "_id": "13282296",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-30 01:11:33",
      "description": "To ensure that Key acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMKeyAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of OM Key ACL requests"
   },
   {
      "_id": "13282295",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-30 01:09:03",
      "description": "To ensure that volume acl operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMVolumeAclRequests (Add, Remove and Set ACL requests) are made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of OM Volume ACL requests"
   },
   {
      "_id": "13282287",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-30 00:08:00",
      "description": "To ensure that allocate block operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMAllocateBlockRequest is made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Replay of AllocateBlock request"
   },
   {
      "_id": "13282220",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-01-29 15:41:08",
      "description": "DBStoreBuilder logs some table-related at INFO level.  This is fine for DBs that are created once per run, eg. OM or SCM, but Recon builds a new DB for each OM snapshot:\r\n\r\n{code}\r\nrecon_1     | 2020-01-29 15:20:32,466 [pool-7-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/recon/om.snapshot.db_1580311232241\r\nrecon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: userTable\r\nrecon_1     | 2020-01-29 15:20:32,475 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTable\r\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: volumeTable\r\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTable\r\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: bucketTable\r\nrecon_1     | 2020-01-29 15:20:32,476 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTable\r\nrecon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: keyTable\r\nrecon_1     | 2020-01-29 15:20:32,477 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTable\r\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: deletedTable\r\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTable\r\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: openKeyTable\r\nrecon_1     | 2020-01-29 15:20:32,478 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTable\r\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3Table\r\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Table\r\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTable\r\nrecon_1     | 2020-01-29 15:20:32,479 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTable\r\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: dTokenTable\r\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTable\r\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTable\r\nrecon_1     | 2020-01-29 15:20:32,480 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTable\r\nrecon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: prefixTable\r\nrecon_1     | 2020-01-29 15:20:32,481 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTable\r\nrecon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: using custom profile for table: default\r\nrecon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:default\r\nrecon_1     | 2020-01-29 15:20:32,482 [pool-7-thread-1] INFO db.DBStoreBuilder: Using default options. DBProfile.DISK\r\nrecon_1     | 2020-01-29 15:20:32,514 [pool-7-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB snapshot at /data/metadata/recon/om.snapshot.db_1580311232241.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Unnecessary log messages in DBStoreBuilder"
   },
   {
      "_id": "13282027",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-28 18:28:04",
      "description": "To ensure that S3 operations is idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nIn this Jira, the following requests are made idempotent:\r\n* S3InitiateMultipartUploadRequest\r\n* S3MultipartUploadCommitPartRequest\r\n* S3MultipartUploadCompleteRequest\r\n* S3MultipartUploadAbortRequest",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of S3 requests"
   },
   {
      "_id": "13282026",
      "assignee": "xyao",
      "components": [],
      "created": "2020-01-28 18:24:13",
      "description": "Current we only add scm service principal to scmadmins at runtime. The ozone manager service principal is not honored as om admins. As a result, if user does not specify any user in\u00a0\r\n\r\nozone.administrators, they will not be able to create a volume.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure ozone manager service user is part of ozone.administrators"
   },
   {
      "_id": "13282025",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-01-28 18:19:54",
      "description": "This is an umbrella Jira to improve usability issues around\u00a0ozone.administrators.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve usability issues of ozone.administrators"
   },
   {
      "_id": "13281922",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-28 11:40:27",
      "description": "The jetty which is used by web interfaces of Ozone is from the September of 2018.\r\n\r\nSince then many bug and security fixes added to the Jetty project. I would suggest to use the latest jetty (from January of 2020).\r\n\r\nAs HttpServer2 (hadoop 3.2 class) has a strong dependency on the older version of Jetty (it uses SessionManager which is removed), it seems to be easier to clone HttpServer2 and move it to the Ozone project.\r\n\r\nIt provides us the flexibility:\r\n # To upgrade jetty independent from the Hadoop releases\r\n # To remove unused features\r\n # To support ozone style configuration\r\n # Add additional insights/metrics to our own HttpServer\r\n # Simplify the current server initialization (current BaseHttpServer class of hdds/ozone is a wrapper around the Hadoop class, but we can combine the two functionalities)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Upgrade jetty to the latest 9.4 release"
   },
   {
      "_id": "13281816",
      "assignee": "xyao",
      "components": [],
      "created": "2020-01-27 23:03:56",
      "description": "We need to add extra robot tests case (in addition to the existing ones adapted from o3fs) for ofs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement ofs://: Add robot tests for mkdir"
   },
   {
      "_id": "13281800",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-27 20:48:00",
      "description": "To ensure that key commit operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMKeyCommitRequest is made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of KeyCommitRequest and DirectoryCreateRequest"
   },
   {
      "_id": "13281759",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-27 16:44:48",
      "description": "Some existing unit tests rely on implementation details of existing ChunkManager (eg. TestKeyValueContainerCheck).  These should run common test cases on both implementations.  Implementation-specific test cases should be extracted to separate test classes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Parameterize unit tests for chunk manager implementation"
   },
   {
      "_id": "13281320",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-01-24 01:41:16",
      "description": "This Jira is to use regex which are matching withratis grpc properties and set them when creating ratis server.\u00a0\r\n\r\nAdvantages:\r\n # We can use ratis properties directly, don't need to create corresponding ozone config.\r\n # When new properties are added in ratis server, we can set them and use them without any ozone code changes.\r\n\r\nIn this Jira not removed the existing properties, if this looks fine, we can remove in a clean up Jira to remove ozone config for ratis server or leave as it is for existing ones.\r\n\r\n\u00a0\r\n\r\nHDDS-2903 already taken care of properties matching with raft.server.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use regex to match with ratis  grpc properties when creating ratis server"
   },
   {
      "_id": "13281237",
      "assignee": "xyao",
      "components": [],
      "created": "2020-01-23 16:44:57",
      "description": "--bucketkey option is missing from\u00a0shell/BucketCommands.md\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Document bucket encryption option in shell/BucketCommands.md"
   },
   {
      "_id": "13281228",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-01-23 15:50:22",
      "description": "When running Hive queries on a 1TB dataset for TPC-DS tests, we started to see an exception coming out from FSInputStream.readFully.\r\nThis does not happen with a smaller 100GB dataset, so possibly multi block long files are the cause of the trouble, and the issue was not seen with a build from early december, so we most likely to blame a recent change since then. The build I am running with is from the hash 929f2f85d0379aab5aabeded8a4d3a5056777706 of master branch but with HDDS-2188 reverted from the code.\r\n\r\nThe exception I see:\r\n{code}\r\nError while running task ( failure ) : attempt_1579615091731_0060_9_05_000029_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: java.io.EOFException: End of file reached before reading fully.\r\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)\r\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)\r\n        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)\r\n        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n        at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)\r\n        at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)\r\n        at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: java.io.EOFException: End of file reached before reading fully.\r\n        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n        at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n        at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n        at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n        at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n        at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n        at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:532)\r\n        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:178)\r\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266)\r\n        ... 16 more\r\nCaused by: java.io.IOException: java.io.EOFException: End of file reached before reading fully.\r\n        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:422)\r\n        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n        ... 27 more\r\nCaused by: java.io.EOFException: End of file reached before reading fully.\r\n        at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:126)\r\n        at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)\r\n        at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readStripeFooter(RecordReaderUtils.java:269)\r\n        at org.apache.orc.impl.RecordReaderImpl.readStripeFooter(RecordReaderImpl.java:308)\r\n        at org.apache.orc.impl.RecordReaderImpl.beginReadStripe(RecordReaderImpl.java:1089)\r\n        at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1051)\r\n        at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1219)\r\n        at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1254)\r\n        at org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:284)\r\n        at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:67)\r\n        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:83)\r\n        at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:145)\r\n        at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.<init>(VectorizedOrcAcidRowBatchReader.java:135)\r\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:2046)\r\n        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:419)\r\n        ... 28 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Hive queries fail at readFully"
   },
   {
      "_id": "13280947",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-22 11:08:59",
      "description": "As of now we have a hdds-common > hdfs-client dependency but in reality we don't use an important thing only a few string utils.--\r\n\r\nI would propose to remove this dependency to have a safer / smaller and more independent ozone package.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hdfs-client dependency from hdds-common"
   },
   {
      "_id": "13280913",
      "assignee": "xyao",
      "components": [],
      "created": "2020-01-22 07:40:28",
      "description": "Currently SCM relies on the XOR of the three node ID hash to detect overlapped pipelines. Per\u00a0offline discussion with [~aengineer], this could potentially give false positive result when detecting overlapped pipelines. This ticket is opened to fix the issue.\u00a0\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Pipeline#nodeIdsHash collision issue"
   },
   {
      "_id": "13280873",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-01-22 01:07:58",
      "description": "Once HDDS-2903 went in, now we can use direct ratis server configurations in XceiverClientRatis. This Jira is to clean up the old configuration and add any new required configuration.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove ozone ratis server specific config keys"
   },
   {
      "_id": "13280868",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-01-22 00:30:46",
      "description": "HDDS-2896 went in, now we can use direct ratis client configurations in OzoneClient. This Jira is to clean up the old configuration and add any new required configuration.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove ozone ratis client specific config keys"
   },
   {
      "_id": "13280790",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-21 15:21:06",
      "description": "Acceptance tests seem to be randomly failing recently with SCM not exiting safe mode within the 90 seconds timeout.  This task proposes to increase the timeout to check if it helps.\r\n\r\nAlso, when acceptance test fails with this condition, containers are not stopped and logs not copied.  This should be fixed.\r\n\r\n{code:title=example}\r\n2020-01-21T10:48:10.2663685Z WARNING! Safemode is still on. Please check the docker-compose files\r\n2020-01-21T10:48:10.2683558Z ERROR: Test execution of /home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31 is FAILED!!!!\r\n2020-01-21T10:48:10.2712438Z cp: cannot stat '/home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31/result/robot-*.xml': No such file or directory\r\n2020-01-21T10:48:10.2716708Z cp: cannot stat '/home/runner/work/hadoop-ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31/result/docker-*.log': No such file or directory\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Increase timeout of safe-mode exit in acceptance tests"
   },
   {
      "_id": "13280718",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-01-21 10:22:26",
      "description": "Currently, every request submitted to client is retried with a default retry policy in ozone.\r\n\r\nFor watch requests , ideally the timeout is higher and there should be no retries. The idea is to address that.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a different retry policy for watch requests"
   },
   {
      "_id": "13280680",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-21 08:52:42",
      "description": "started docker based cluster with \"ozone.om.ratis.enable\" = true\r\n\r\nOM started with ratis backend.\r\n\r\n\u00a0\r\n{noformat}\r\nom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:48,116 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:48,116 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:49,213 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled clusterom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:49,279 [main] INFO ha.OMHANodeDetails: Configuration either no ozone.om.address set. Falling back to the default OM address om/172.18.0.2:9862om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:49,280 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefaultom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:49,294 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:49,315 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,268 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,309 [main] INFO util.log: Logging initialized @3941msom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: using custom profile for table: userTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:userTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: using custom profile for table: volumeTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,406 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:volumeTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: bucketTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:bucketTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: keyTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:keyTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: using custom profile for table: deletedTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,407 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:deletedTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: openKeyTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:openKeyTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: s3Tableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3Tableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,408 [main] INFO db.DBStoreBuilder: using custom profile for table: multipartInfoTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:multipartInfoTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: dTokenTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:dTokenTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: s3SecretTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:s3SecretTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: using custom profile for table: prefixTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,409 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:prefixTableom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,433 [main] INFO db.DBStoreBuilder: using custom profile for table: defaultom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,433 [main] INFO db.DBStoreBuilder: Using default column profile:DBProfile.DISK for Table:defaultom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,435 [main] INFO db.DBStoreBuilder: Using default options. DBProfile.DISKom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,620 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirsom_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,647 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,682 [main] INFO impl.RaftServerProxy: raft.rpc.type = GRPC (default)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,762 [main] INFO grpc.GrpcFactory: PERFORMANCE WARNING: useCacheForAllThreads is true that may cause Netty to create a lot garbage objects and, as a result, trigger GC.om_1\u00a0 \u00a0 \u00a0 \u00a0 | It is recommended to disable useCacheForAllThreads by setting -Dorg.apache.ratis.thirdparty.io.netty.allocator.useCacheForAllThreads=false in command line.om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,767 [main] INFO grpc.GrpcConfigKeys$Server: raft.grpc.server.port = 9872 (custom)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,768 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,770 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,771 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:50,771 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:51,110 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:51,117 [main] INFO impl.RaftServerProxy: a7718018-f8c6-4b70-90b7-aadd8f920710: addNew group-C5BA1605619E:[a7718018-f8c6-4b70-90b7-aadd8f920710:om:9872] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@2b0b7e5a[Not completed]om_1\u00a0 \u00a0 \u00a0 \u00a0 | 2020-01-20 09:02:51,123 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872\r\n\u00a0\r\n{noformat}\r\n\u00a0\r\n\r\nRan \"getserviceroles\" command from CLI. \"getserviceroles\" API not working\r\n\r\n\u00a0\r\n{noformat}\r\n/opt/hadoop/bin/ozone admin om getserviceroles -id=omServiceIdDefault/opt/hadoop/bin/ozone admin om getserviceroles -id=omServiceIdDefaultCouldn't create RpcClient protocol exception:java.lang.IllegalArgumentException: Could not find any configured addresses for OM. Please configure the system with ozone.om.address at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.loadOMClientConfigs(OMFailoverProxyProvider.java:138) at org.apache.hadoop.ozone.om.ha.OMFailoverProxyProvider.<init>(OMFailoverProxyProvider.java:83) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:208) at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:155) at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:190) at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:122) at org.apache.hadoop.ozone.admin.om.OMAdmin.createClient(OMAdmin.java:59) at org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand.call(GetServiceRolesSubcommand.java:49) at org.apache.hadoop.ozone.admin.om.GetServiceRolesSubcommand.call(GetServiceRolesSubcommand.java:32) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.ozone.admin.OzoneAdmin.main(OzoneAdmin.java:66)Couldn't create RpcClient protocol{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM HA cli getserviceroles not working"
   },
   {
      "_id": "13280540",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-01-20 12:00:01",
      "description": "Currently unit tests are broken due to some Surefire exception (HDDS-2898), but unit check is still passing.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Unit check passes despite Maven error"
   },
   {
      "_id": "13280289",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-18 12:38:55",
      "description": "We have two classes which use org.apache.hadoop.hdfs.protocol.DatanodeInfo (HDFS!!) inside common. They are not used any more.\r\n\r\nEven if they keep memories from the good old times, I would suggest to remove them to reduce the Hadoop dependency...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove unusued BlockLocation protocol related classes"
   },
   {
      "_id": "13280277",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-18 08:40:03",
      "description": "There are two ways to add certain set of dependencies to all the maven projects.\r\n # You can add it to the parent project which will be inherited to all the children projects\r\n # You can add it only to the required project and will be used via transitive dependencies\r\n\r\nI think the 2nd approach is safest as we might need to create a new child project *without* Hadoop dependencies which is not possible with the 1st approach.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove default dependencies from hadoop-hdds/pom.xml"
   },
   {
      "_id": "13280213",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2020-01-17 19:48:11",
      "description": "This Jira is to use regex which are matching with ratis server and ratis grpc properties and set them when creating ratis server.\u00a0\r\n\r\nAdvantages:\r\n # We can use ratis properties directly, don't need to create corresponding ozone config.\r\n # When new properties are added in ratis server, we can set them and use them without any ozone code changes.\r\n\r\nIn this Jira not removed the existing properties, if this looks fine, we can remove in a clean up Jira to remove ozone config for ratis server or leave as it is for existing ones.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": " Use regex to match with ratis properties when creating ratis server"
   },
   {
      "_id": "13280206",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-17 19:15:54",
      "description": "Before the fix for HDDS-2897, {{recon}} container in {{ozonesecure}} environment exited with error soon after start.  While working on the fix [~avijayan] noticed that the following test case (which tries to run some test on {{recon}} container) does not catch this error:\r\n\r\n{code}\r\nexecute_robot_test recon <any test>\r\n{code}\r\n\r\nIdeally this should fail, since {{recon}} container is down.  But all commands that reference the container are run with [{{set +e}} in effect|https://github.com/apache/hadoop-ozone/blob/1caf1e30865aa2b380a7fca6d87f5ae8034fee4e/hadoop-ozone/dist/src/main/compose/testlib.sh#L99-L107], so the container's unavailibility is basically ignored, and acceptance test passes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "execute_robot_test on unknown/unavailable container should fail acceptance test"
   },
   {
      "_id": "13280095",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2020-01-17 09:32:18",
      "description": "If {{hdds.datanode.du.factory.classname}} is not configured, a harmless but annoying NPE appears in datanode log during startup.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid logging NPE when space usage check is not configured"
   },
   {
      "_id": "13280040",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-17 04:05:11",
      "description": "output of mvn clean package, not sure if it's normal\r\n{code:java}\r\n[INFO] Reactor Summary for Apache Hadoop Ozone Main 0.5.0-SNAPSHOT:\r\n[INFO]\r\n[INFO] Apache Hadoop Ozone Main ........................... SUCCESS [  0.789 s]\r\n[INFO] Apache Hadoop HDDS ................................. SUCCESS [  2.994 s]\r\n[INFO] Apache Hadoop HDDS Config .......................... SUCCESS [  4.201 s]\r\n[INFO] Apache Hadoop HDDS Common .......................... SUCCESS [02:16 min]\r\n[INFO] Apache Hadoop HDDS Client .......................... FAILURE [  1.052 s]\r\n[INFO] Apache Hadoop HDDS Server Framework ................ SKIPPED\r\n[INFO] Apache Hadoop HDDS Container Service ............... SKIPPED\r\n[INFO] Apache Hadoop HDDS/Ozone Documentation ............. SKIPPED\r\n[INFO] Apache Hadoop HDDS SCM Server ...................... SKIPPED\r\n[INFO] Apache Hadoop HDDS Tools ........................... SKIPPED\r\n[INFO] Apache Hadoop Ozone ................................ SKIPPED\r\n[INFO] Apache Hadoop Ozone Common ......................... SKIPPED\r\n[INFO] Apache Hadoop Ozone Client ......................... SKIPPED\r\n[INFO] Apache Hadoop Ozone Manager Server ................. SKIPPED\r\n[INFO] Apache Hadoop Ozone FileSystem ..................... SKIPPED\r\n[INFO] Apache Hadoop Ozone Tools .......................... SKIPPED\r\n[INFO] Apache Hadoop Ozone S3 Gateway ..................... SKIPPED\r\n[INFO] Apache Hadoop Ozone CSI service .................... SKIPPED\r\n[INFO] Apache Hadoop Ozone Recon CodeGen .................. SKIPPED\r\n[INFO] Apache Hadoop Ozone Recon .......................... SKIPPED\r\n[INFO] Apache Hadoop Ozone Integration Tests .............. SKIPPED\r\n[INFO] Apache Hadoop Ozone Datanode ....................... SKIPPED\r\n[INFO] Apache Hadoop Ozone In-Place Upgrade ............... SKIPPED\r\n[INFO] Apache Hadoop Ozone Insight Tool ................... SKIPPED\r\n[INFO] Apache Hadoop Ozone FileSystem Single Jar Library .. SKIPPED\r\n[INFO] Apache Hadoop Ozone FileSystem Legacy Jar Library .. SKIPPED\r\n[INFO] Apache Hadoop Ozone Distribution ................... SKIPPED\r\n[INFO] Apache Hadoop Ozone Fault Injection Tests .......... SKIPPED\r\n[INFO] Apache Hadoop Ozone Network Tests .................. SKIPPED\r\n[INFO] Apache Hadoop Ozone Mini Ozone Chaos Tests ......... SKIPPED\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD FAILURE\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time:  02:26 min\r\n[INFO] Finished at: 2020-01-17T11:31:57+08:00\r\n[INFO] ------------------------------------------------------------------------\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-hdds-client: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test failed: java.lang.ClassFormatError: Illegal field name \"org.apache.hadoop.hdds.scm.storage.TestBlockInputStream$this\" in class org/apache/hadoop/hdds/scm/storage/TestBlockInputStream$DummyBlockInputStreamWithRetry -> [Help 1]\r\n[ERROR]\r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR]\r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException\r\n[ERROR]\r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <goals> -rf :hadoop-hdds-client\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Build with test fails due to ClassFormatError due to hadoop-hdds-client test"
   },
   {
      "_id": "13279983",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-01-16 21:08:12",
      "description": "Ozone recon Start failed due to Kerberos principal not being found.\r\n{code}\r\nCaused by: javax.servlet.ServletException: javax.servlet.ServletException: Principal not defined in configuration\r\n\tat org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)\r\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194)\r\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180)\r\n\tat org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:881)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131)\r\n\tat org.eclipse.jetty.server.Server.start(Server.java:427)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61)\r\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:394)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1199)\r\n\t... 13 more\r\nCaused by: javax.servlet.ServletException: Principal not defined in configuration\r\n\tat org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:137)\r\n\t... 35 more {code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone recon Start failed due to Kerberos principal not being found."
   },
   {
      "_id": "13279858",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-01-16 15:39:45",
      "description": "This Jira is to use regex which are matching with ratis client and ratis grpc properties and set them when creating ratis client.\u00a0\r\n\r\nAdvantages:\r\n # We can use ratis properties directly, don't need to create corresponding ozone config.\r\n # When new properties are added in ratis client, we can set them and use them with out any ozone code changes.\r\n\r\nIn this Jira not removed the existing properties, if this looks fine, we can remove a clean up Jira to remove ozone config for ratis client or leave as it is for existing ones.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "teragentest"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use regex to match with ratis properties when creating ratis client"
   },
   {
      "_id": "13279779",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-16 08:58:12",
      "description": "We have acceptance tests with the help of docker/docker-compose where we generate the keytab files on the fly with the help of a lightweight (unsecure) REST endpoint.\r\n\r\nBut this generation can be very slow especially when the DNS is slow. When I start a VPN the secure cluster can't be started in the 90 sec period. (>100 keytabs are generated and one keytab generation is ~5 sec).\r\n\r\nThe solutions is to generate only the *required* keystabs in each of the containers.\r\n\r\nInstead of request all the possible keytabs in the *generic* docker-config:\r\n{code:java}\r\nKERBEROS_KEYTABS=dn om scm HTTP testuser testuser2 s3g {code}\r\nWe can defined the required keytabs per service (in docker-compose.yaml)\r\n{code:java}\r\nenvironment:\r\n  KERBEROS_KEYTABS=scm HTTP{code}\r\nWith this approach ~20 keytab file will be generated instead of >100 and the secure tests will be significant faster.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Generate only the required keytabs for docker based secure tests"
   },
   {
      "_id": "13279695",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-15 23:05:35",
      "description": "To ensure that key deletion and rename operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMKeyDeleteRequest and OMKeyRenameRequest are made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of KeyDelete and KeyRename Requests"
   },
   {
      "_id": "13279694",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-15 23:02:26",
      "description": "If KeyPurgeRequest is replayed, we do not want to purge the keys which were created after the original purge request was received. This could happen if a key was deleted, purged and then created and deleted again. The the purge request was replayed, it would purge the key deleted after the original purge request was completed.\r\nHence, to maintain idempotence, we should only purge those keys from DeletedKeys table that have updateID < transactionLogIndex of the request.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of KeyPurge Request"
   },
   {
      "_id": "13279555",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-15 10:24:15",
      "description": "\u00a0\r\n\r\n(1) Create a simple PutS3Object processor in NiFi\r\n\r\n(2) The request from NiFi to S3g will fail with HTTP 500\r\n\r\n(3) The exception in the s3g log:\r\n\r\n\u00a0\r\n{code:java}\r\n s3g_1       | Caused by: java.io.IOException: Couldn't create RpcClient protocol\r\ns3g_1       | \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:197)\r\ns3g_1       | \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:173)\r\ns3g_1       | \tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClient(OzoneClientFactory.java:74)\r\ns3g_1       | \tat org.apache.hadoop.ozone.s3.OzoneClientProducer.getClient(OzoneClientProducer.java:114)\r\ns3g_1       | \tat org.apache.hadoop.ozone.s3.OzoneClientProducer.createClient(OzoneClientProducer.java:71)\r\ns3g_1       | \tat jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)\r\ns3g_1       | \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\ns3g_1       | \tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\ns3g_1       | \tat org.jboss.weld.injection.StaticMethodInjectionPoint.invoke(StaticMethodInjectionPoint.java:88)\r\ns3g_1       | \t... 92 more\r\ns3g_1       | Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Invalid S3 identifier:OzoneToken owner=testuser/scm@EXAMPLE.COM, renewer=, realUser=, issueDate=0, maxDate=0, sequenceNumber=0, masterKeyId=0, strToSign=AWS4-HMAC-SHA256\r\ns3g_1       | 20200115T101329Z\r\ns3g_1       | 20200115/us-east-1/s3/aws4_request\r\ns3g_1       | (hash), signature=(sign), awsAccessKeyId=testuser/scm@EXAMPLE.COM{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Apache NiFi PutFile processor is failing with secure Ozone S3G"
   },
   {
      "_id": "13279441",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-14 21:07:55",
      "description": "Currently the StateContext in Datanode has the set of actions and reports to be pushed to SCM(s) in the next Heartbeat. Since the state context is shared across all the configured SCM (& Recon) endpoints, an action or report is only pushed to one SCM server based on FCFS. This needs to be changed to a model every SCM server gets the reports and actions. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor Datanode StateContext to send reports and actions to all configured SCMs"
   },
   {
      "_id": "13279379",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-01-14 15:13:30",
      "description": "Currently, while sending watch requests in ozone client, it sends watch requests with ratis replication level set to ALL_COMMITTED and in case it fails, it sends the request\u00a0 with MAJORITY_COMMITTED semantics. The idea is to configure the replication level for watch requests so as to measure performance.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add config to tune replication level of watch requests in Ozone client"
   },
   {
      "_id": "13279227",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-01-14 00:14:24",
      "description": "This Jira proposes to change the following values\r\n\r\nozone.client.stream.buffer.flush.size - 64MB -> 16MB\r\n\r\nozone.client.stream.buffer.max.size - 128 MB -> 32MB\r\n\r\nozone.scm.chunk.size - 16MB -> 4MB\r\n\r\n\u00a0\r\n\r\nIn this way, the client-side holding buffer size will be reduced, and data is transferred at smaller size intervals.\r\n\r\n\u00a0\r\n\r\nThis Jira is to discuss about these config settings changes and change these values accordingly.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Change the default client settings accordingly with change in default chunk size"
   },
   {
      "_id": "13279225",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-01-14 00:10:53",
      "description": "This Jira proposes to change the default value of\u00a0dfs.ratis.client.request.timeout.duration and\u00a0dfs.ratis.server.request.timeout.duration.\r\n\r\n\u00a0\r\n\r\nIn Teragen testing, we have seen many requests failing with timeout, so this Jira is to change the value of the above config from default 3s -> 10s",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Change the default setting of request.timeout.duration for ratis client and server"
   },
   {
      "_id": "13279033",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-13 09:58:56",
      "description": "The unsecure ozonefs is tested together with different version of Hadoop (2.7, 3.1, 3.2). The \"legacy\" (isolated) ozonefs package makes it possible to use it together with older Hadoop versions.\r\n\r\nBut the ozonesecure-mr (testing ozonefs with mapreduce in secure Ozone environment) is executed only with Hadoop 3.2 (which uses the \"current\" (shared) ozonefs instead of the \"legacy\" (isolated))\r\n\r\nIt turned on that the \"legacy\" ozonefs couldn't work in secure environment which means Ozone can be used only together with Hadoop 3.2 if the Hadoop/Ozone environment is secure\r\n\r\nThere are multiple problems. The first visible one is the following (run cd compose/ozonesecure-mr && ./test.sh after s/current/legacy/ in docker-config/docker-compose.yaml):\r\n{code:java}\r\n2020-01-13 10:03:39 ERROR OzoneClientFactory:193 - Couldn't create RpcClient protocol exception: \r\njava.io.IOException: DestHost:destPort om:9862 , LocalHost:localPort rm/172.21.0.10:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\r\n\tat sun.reflect.GeneratedConstructorAccessor2.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:806)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\tat com.sun.proxy.$Proxy13.submitRequest(Unknown Source)\r\n\tat sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\tat com.sun.proxy.$Proxy13.submitRequest(Unknown Source)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:394)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceInfo(OzoneManagerProtocolClientSideTranslatorPB.java:1283)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:71) {code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Legacy (isolated) ozonefs couldn't work in secure environments"
   },
   {
      "_id": "13278790",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-10 20:40:48",
      "description": "We use ObjectID and BucketID in OMVolumeArgs, OMBucketInfo, OMKeyInfo and OMMultipartKeyInfo. We can consolidate by having these Info objects extend a \"WithObjectID\" class which can host the common fields - objectID and updateID.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Consolidate ObjectID and UpdateID from Info objects into one class"
   },
   {
      "_id": "13278642",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2020-01-10 08:13:19",
      "description": "Add a config to tune the config value of \"raft.client.async.outstanding-requests.max\" config in raft client.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a config in ozone to tune max outstanding requests in raft client"
   },
   {
      "_id": "13278633",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-10 07:34:50",
      "description": "{{ozone.recon.scm.db.dirs}} is reported by {{TestOzoneConfigurationFields}} to be missing from {{ozone-default.xml}}. If it is to be documented, then please add the property to {{ozone-default.xml}}. If it's a developer-only setting, please add as exception in {{TestOzoneConfigurationFields#addPropertiesNotInXml}}.\r\n\r\n(Sorry for reporting this post-commit. {{TestOzoneConfigurationFields}} will be run by CI once we have integration tests enabled again.)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone.recon.scm.db.dirs missing from ozone-default.xml"
   },
   {
      "_id": "13278590",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-10 01:52:34",
      "description": "To ensure that key creation operation is idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.\r\n\r\nOMKeyCreateRequest and OMFileCreateRequest are made idempotent in this Jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replay of KeyCreate requests"
   },
   {
      "_id": "13278546",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2020-01-09 22:05:36",
      "description": "Similar to OMVolumeInfo and OMBucketInfo, we need objectID and updateID in OMKeyInfo as well to ensure that transactions are idempotent.\r\nThis Jira adds objectID and updateID to OMKeyInfo proto and sets these values when creating/ updating keys. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ObjectID and UpdateID to OMKeyInfo"
   },
   {
      "_id": "13278540",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2020-01-09 21:43:12",
      "description": "This is the umbrella Jira to add generic token support across ozone components. I will attach a design spec for review and comments.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Generic Extensible Token support for Ozone"
   },
   {
      "_id": "13278530",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-09 20:24:35",
      "description": "{{TestOzoneManagerRocksDBLogging}} fails intermittently.  I think the problem is that RocksDB-specific string may appear in the log in the {{Finalizer}} thread after after OM is shutdown and restarted in {{Thread-1}}:\r\n\r\n{code:title=output}\r\n2020-01-09 16:17:29,372 [Thread-1] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:390] Shutdown: canceling all background work\r\n2020-01-09 16:17:29,373 [Thread-1] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:563] Shutdown complete\r\n2020-01-09 16:17:29,373 [Thread-1] INFO  om.OzoneManager (OzoneManager.java:restart(1127)) - OzoneManager RPC server is listening at localhost/127.0.0.1:33899\r\n...\r\n2020-01-09 16:17:29,561 [Thread-1] INFO  server.BaseHttpServer (BaseHttpServer.java:updateConnectorAddress(216)) - HTTP server of ozoneManager listening at http://0.0.0.0:34495\r\n...\r\n2020-01-09 16:17:37,988 [Finalizer] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:390] Shutdown: canceling all background work\r\n2020-01-09 16:17:37,989 [Finalizer] INFO  rocksdb.RocksDB (DBStoreBuilder.java:log(228)) - [/db_impl.cc:563] Shutdown complete\r\n{code}\r\n\r\nSwapping the order of tests (ie. running with logging disabled first) would guarantee that stray messages from the logger do not affect it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Intermittent failure in TestOzoneManagerRocksDBLogging"
   },
   {
      "_id": "13278506",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-09 18:04:16",
      "description": "TestOMDbCheckpointServlet verifies that no checkspoints have been taken before it invokes the servlet.  However, the same servlet is used by the actual Recon server in the mini cluster.\r\n\r\n{code:title=test failure}\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.313 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet\r\ntestDoGet(org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet)  Time elapsed: 30.192 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n...\r\n\tat org.apache.hadoop.ozone.om.TestOMDbCheckpointServlet.testDoGet(TestOMDbCheckpointServlet.java:155)\r\n{code}\r\n\r\n{code:title=output}\r\n2020-01-09 16:13:56,913 [pool-61-thread-1] INFO  impl.OzoneManagerServiceProviderImpl (OzoneManagerServiceProviderImpl.java:syncDataFromOM(325)) - Syncing data from Ozone Manager.\r\n2020-01-09 16:13:56,914 [pool-61-thread-1] INFO  impl.OzoneManagerServiceProviderImpl (OzoneManagerServiceProviderImpl.java:syncDataFromOM(356)) - Obtaining full snapshot from Ozone Manager\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOMDbCheckpointServlet fails due to real Recon"
   },
   {
      "_id": "13278499",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2020-01-09 17:46:17",
      "description": "Recon exposes SCM-like RPC endpoints on (possibly) different port than SCM. However, when the RPC server [updates the config with the actual address|https://github.com/apache/hadoop-ozone/blob/046a06f02783da516179ee8d8d1bed862d22f78d/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDatanodeProtocolServer.java#L166-L168]\u00a0after startup, it does so using the SCM-specific config key. Now that Recon is part of {{MiniOzoneCluster}}, this causes BindException in {{TestSCMRestart}} (and possibly other integration tests):\r\n{code:java|title=output}\r\n2020-01-09 16:07:45,370 [main] INFO  server.StorageContainerManager (StorageContainerManager.java:start(775)) - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:36225\r\n...\r\n2020-01-09 16:07:51,594 [main] INFO  scm.ReconStorageContainerManager (ReconStorageContainerManager.java:start(91)) - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:38907\r\n{code}\r\n{code:java|title=test failure}\r\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 33.653 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart\r\norg.apache.hadoop.hdds.scm.pipeline.TestSCMRestart  Time elapsed: 33.642 s  <<< ERROR!\r\njava.net.BindException: Problem binding to [0.0.0.0:38907]\r\n...\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.startRpcServer(StorageContainerManager.java:579)\r\n\tat org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.<init>(SCMDatanodeProtocolServer.java:158)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:327)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:212)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:594)\r\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl.restartStorageContainerManager(MiniOzoneClusterImpl.java:295)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.TestSCMRestart.init(TestSCMRestart.java:78)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "BindException in TestSCMRestart"
   },
   {
      "_id": "13278470",
      "assignee": "elek",
      "components": [],
      "created": "2020-01-09 14:41:17",
      "description": "Freon tests use the ProgressBar to show the current progress of the Freon tests.\r\n\r\n\u00a0\r\n\r\nProgress bar use \"\\r\" to print out a nice, interactive progressbar to the same line.\r\n\r\n\u00a0\r\n\r\nUnfortunately this doesn't work very well when the standard output is redirected to a file (or in a containerized environment). I would suggest to use a simplified method using log4j in non-interactive environments.\r\n\r\n\u00a0\r\n\r\nThe easiest way to do is to use \"System.console()\" java method. If it returns with null we are in a non interactive environment. We can also increase the logging period from 1s to 10s in case of non-interactive environment (==long term testing).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Freon progressbar in non-interactive environment"
   },
   {
      "_id": "13278456",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2020-01-09 13:56:24",
      "description": "Currently build is successful even if {{hugo}} encounters error.  Example:\r\n\r\n{code}\r\nError: Error building site: \"hadoop-hdds/docs/content/start/_index.zh.md:39:119\": unterminated quoted string in shortcode parameter-argument: 'start\r\n'\r\n...\r\n[INFO] BUILD SUCCESS\r\n{code}\r\n\r\nThe error itself is [being fixed|https://github.com/apache/hadoop-ozone/pull/417/files#diff-3553ca3378ab646dea469e6a1c58c100] in HDDS-2726.  However, it would be nice to make such a Hugo error result in failed build, so that such issues can be caught by CI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Hugo error should be propagated to build"
   },
   {
      "_id": "13278330",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-01-09 01:19:07",
      "description": "Right now each OM generates file encryption Info. In OM HA, the leader should generate encryptionInfo, and followers need to use the same information and store it in OM DB.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "EncryptionInfo should be generated by leader OM"
   },
   {
      "_id": "13278250",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-08 21:30:06",
      "description": "While trying to install ozone in cdpd cluster, ozone Recon failed to start.\r\n\r\n{code}\r\nexec /opt/cloudera/parcels/CDH-7.1.0-1.cdh7.1.0.p0.1728961/lib/hadoop-ozone/../../bin/ozone recon\r\nlog4j:ERROR Could not instantiate class [org.cloudera.log4j.redactor.RedactorAppender].\r\njava.lang.ClassNotFoundException: org.cloudera.log4j.redactor.RedactorAppender\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\tat java.lang.Class.forName0(Native Method)\r\n\tat java.lang.Class.forName(Class.java:264)\r\n\tat org.apache.log4j.helpers.Loader.loadClass(Loader.java:198)\r\n\tat org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:327)\r\n\tat org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:124)\r\n\tat org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:785)\r\n\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\r\n\tat org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)\r\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)\r\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\r\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\r\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\r\n\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\r\n\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\r\n\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\r\n\tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)\r\n\tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)\r\n\tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)\r\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)\r\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\r\n\tat org.apache.hadoop.ozone.recon.ReconServer.<clinit>(ReconServer.java:38)\r\nlog4j:ERROR Could not instantiate appender named \"redactorForRootLogger\".\r\nERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Recon fails to start while ozone install"
   },
   {
      "_id": "13278247",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2020-01-08 21:17:52",
      "description": "Following the example of OM and SCM, Recon should by default limit the number of containers returned in the getContainers API to 1000. Currently, the default is ALL.\r\n\r\nSuggested by [~aengineer]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon getContainers API should return a maximum of 1000 containers by default."
   },
   {
      "_id": "13278054",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2020-01-08 02:08:19",
      "description": "{code:java}\r\nUnable to communicate to SCM server at ozone-test-bh-3.vpc.cloudera.com:9861 for past 2400 seconds.\r\n java.io.IOException: java.lang.NullPointerException\r\n at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)\r\n at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)\r\n at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:70)\r\n at org.apache.ratis.server.impl.RaftServerProxy.getImpls(RaftServerProxy.java:284)\r\n at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:296)\r\n at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:433)\r\n at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:232)\r\n at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:110)\r\n at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)\r\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n Caused by: java.lang.NullPointerException\r\n at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.triggerHeartbeat(DatanodeStateMachine.java:384)\r\n at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.sendPipelineReport(XceiverServerRatis.java:756)\r\n at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.notifyGroupAdd(XceiverServerRatis.java:737)\r\n at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.initialize(ContainerStateMachine.java:213)\r\n at org.apache.ratis.server.impl.ServerState.initStatemachine(ServerState.java:160)\r\n at org.apache.ratis.server.impl.ServerState.<init>(ServerState.java:112)\r\n at org.apache.ratis.server.impl.RaftServerImpl.<init>(RaftServerImpl.java:113)\r\n at org.apache.ratis.server.impl.RaftServerProxy.lambda$newRaftServerImpl$2(RaftServerProxy.java:208)\r\n at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)\r\n ... 3 more{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "NPE in OzoneContainer Start"
   },
   {
      "_id": "13278017",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2020-01-07 21:19:26",
      "description": "The following config properties missing from {{ozone-default.xml}} were pointed out by {{TestOzoneConfigurationFields}}:\r\n * {{ozone.om.ratis.client.request.timeout.duration}}\r\n * {{ozone.om.ratis.segment.preallocated.size}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Some OM Ratis config properties missing from ozone-default.xml"
   },
   {
      "_id": "13277987",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-07 18:47:40",
      "description": "CREATE container needs to be handled differently in Recon since these actions are initiated in the SCM, and Recon does not know about that. It should not throw ContainerNotFoundException when it suddenly sees a new container.\r\n\r\nThe idea is to let Recon ask SCM about the new container whenever it sees a new one.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Create container use case in Recon."
   },
   {
      "_id": "13277802",
      "assignee": "avijayan",
      "components": [],
      "created": "2020-01-06 23:27:41",
      "description": "* Recon should track the list of containers that have no replicas in its own SQL DB. This information will be used to serve the Missing containers endpoint that returns the list of containers missing along with keys that were part of it.\r\n* If SCM CLI is used to close a pipeline in SCM, Recon does not get any ACK from the Datanode. This patch adds a pipeline sync task in Recon that asks SCM for a list of pipelines and cleans up invalid pipelines.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add Recon tasks for tracking missing containers (FSCK) and syncing deleted pipelines from SCM."
   },
   {
      "_id": "13277800",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-06 23:24:23",
      "description": "Recon should be able to process Container and Pipeline reports from registered Datanodes. It still wont be able to understand new containers and pipelines though. Along with HDDS-2846, HDDS-2852 and HDDS-2869, Recon will be a fully functional passive SCM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Datanode Pipeline & Container Reports reports in Recon"
   },
   {
      "_id": "13277742",
      "assignee": "shashikant",
      "components": [],
      "created": "2020-01-06 17:13:49",
      "description": "There are some critical fixes that went in latest ratis . Need the changes available in ozone.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "update ozone to latest ratis"
   },
   {
      "_id": "13277728",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2020-01-06 16:10:35",
      "description": "{{OzoneFSInputStream#read(ByteBuffer)}} can avoid buffer copy if the target buffer has an array by directly reading into it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Directly read into ByteBuffer if it has array"
   },
   {
      "_id": "13277720",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-06 14:48:33",
      "description": "When we switched to use github actions the integration tests are disabled due to the flakyness.\r\n\r\nWe should disable all the flaky tests and enable the remaining integration tests...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable integrations tests for github actions"
   },
   {
      "_id": "13277246",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2020-01-02 17:45:03",
      "description": "{{start_docker_env}} has a 10-second sleep after environment is up.  I think it was originally added to give SCM a chance to get out of safe mode after all datanodes are up.  Now that acceptance test has a proper {{wait_for_safemode_exit}} condition, I think the sleep is no longer necessary.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Remove unnecessary sleep in acceptance test"
   },
   {
      "_id": "13277159",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2020-01-02 08:33:59",
      "description": "Docker image definition and associated files are duplicated between {{ozonesecure}} and {{ozonesecure-mr}} environments.  They should be defined in one place and referenced by both.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Deduplicate KDC docker image definition"
   },
   {
      "_id": "13277134",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-02 05:16:01",
      "description": "The Pipelines page in Recon should give the Recon admin users a detailed view of active Ratis Data Pipelines in Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this pipelines page is attached.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add initial UI of Pipelines in Recon"
   },
   {
      "_id": "13277132",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-02 05:13:01",
      "description": "The Datanodes page in Recon should give the Recon admin users a detailed view of datanodes present in the Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this Datanodes page is attached.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add initial UI of Datanodes in Recon"
   },
   {
      "_id": "13277131",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2020-01-02 05:11:36",
      "description": "The dashboard for Recon should give the Recon admin users an overview of Ozone file system and its current state. A mockup of what I will try to achieve in the initial version of this dashboard is attached.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add initial UI of Dashboard for Recon"
   },
   {
      "_id": "13276769",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-12-29 14:27:13",
      "description": "OM Ratis dir creation is attempted from 2 threads:\r\n\r\n# Ratis server initializes storage dir in async CompletableFuture\r\n# OM init directly creates it\r\n\r\nThe latter may encounter the following exception:\r\n\r\n{code}\r\njava.lang.IllegalArgumentException: Unable to create path: /github/workspace/hadoop-ozone/integration-test/target/test-dir/MiniOzoneClusterImpl-bdc6aaad-da50-4196-ab1e-80aad183d7b2/omNode-2/ratis\r\n \tat org.apache.hadoop.ozone.OmUtils.createOMDir(OmUtils.java:493)\r\n \tat org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:426)\r\n \tat org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:862)\r\n \tat org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.createOMService(MiniOzoneHAClusterImpl.java:267)\r\n \tat org.apache.hadoop.ozone.MiniOzoneHAClusterImpl$Builder.build(MiniOzoneHAClusterImpl.java:199)\r\n{code}\r\n\r\nat:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/529438a28882b085d1095feccf6c7d6782a6a833/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java#L448-L454}\r\n  public static File createOMDir(String dirPath) {\r\n    File dirFile = new File(dirPath);\r\n    if (!dirFile.exists() && !dirFile.mkdirs()) {\r\n      throw new IllegalArgumentException(\"Unable to create path: \" + dirFile);\r\n    }\r\n    return dirFile;\r\n  }\r\n{code}\r\n\r\nIt may happen that {{exists()}} returns {{false}} because the dir is does not exist yet, but {{mkdirs()}} also returns {{false}} because the dir is created in the meantime by the Ratis thread.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "OM Ratis dir creation may fail"
   },
   {
      "_id": "13275975",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-12-23 03:49:37",
      "description": "In several files, resources are not closed in \"finally\" block.\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5rBvceSc1R0gwbF9Y1&resolved=false&severities=BLOCKER&types=BUG]\u00a0\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5rBvc0Sc1R0gwbF9ZC&resolved=false&severities=BLOCKER&types=BUG]\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-c9KcVY8lQ4Zr3s&resolved=false&severities=BLOCKER&types=BUG]\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-c9KcVY8lQ4Zr3r&resolved=false&severities=BLOCKER&types=BUG]\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&open=AW5md-cqKcVY8lQ4Zr3N&resolved=false&severities=BLOCKER&types=BUG]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use try-with-resources or close resource in finally block"
   },
   {
      "_id": "13275763",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-12-20 17:03:13",
      "description": "{{ITestOzoneContractSeek}} fails at {{testReadFullyZeroByteFile}} and {{testSeekZeroByteFile}} with:\r\n\r\n{code}\r\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\tat java.util.ArrayList.rangeCheck(ArrayList.java:657)\r\n\tat java.util.ArrayList.get(ArrayList.java:433)\r\n\tat org.apache.hadoop.ozone.client.io.KeyInputStream.seek(KeyInputStream.java:241)\r\n\tat org.apache.hadoop.fs.ozone.OzoneFSInputStream.seek(OzoneFSInputStream.java:65)\r\n\tat org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ITestOzoneContractSeek zero byte file failures"
   },
   {
      "_id": "13275704",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-12-20 10:48:26",
      "description": "{{ozonefs}} and {{tools}} modules in {{hadoop-ozone}} have a mix of unit and integration tests.  This issue proposes to\r\n\r\n# switch dependency order: let {{integration-test}} depend on these modules instead of the other way around\r\n# move integration tests (those that use {{Mini*Cluster}}) from these modules to {{integration-test}}\r\n# let {{unit}} check run remaining tests in these modules\r\n\r\nThis improves code coverage in CI.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move ozonefs and tools minicluster tests to integration-test"
   },
   {
      "_id": "13275694",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-12-20 09:47:37",
      "description": "{{OzoneFSInputStream#read(ByteBuffer)}} uses the target buffer's position for offsetting into the temporary array:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/b834fa48afef4ee4c73577c7af564e1e97cb9d5b/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFSInputStream.java#L90-L97}\r\n  public int read(ByteBuffer buf) throws IOException {\r\n\r\n    int bufInitPos = buf.position();\r\n    int readLen = Math.min(buf.remaining(), inputStream.available());\r\n\r\n    byte[] readData = new byte[readLen];\r\n    int bytesRead = inputStream.read(readData, bufInitPos, readLen);\r\n    buf.put(readData);\r\n{code}\r\n\r\nGiven a buffer with capacity=10 and position=8, this results in the following:\r\n\r\n * {{readLen}} = 2 => {{readData.length}} = 2\r\n * {{bufInitPos}} = 8\r\n\r\nSo {{inputStream}} reads 2 bytes and writes it into {{readData}} starting at offset 8, which results in an {{IndexOutOfBoundsException}}.\r\n\r\noffset should always be 0, since the temporary array is sized exactly for the length to read, and it has no extra data at the start.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Read to ByteBuffer uses wrong offset"
   },
   {
      "_id": "13275594",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-12-19 20:55:10",
      "description": "This Jira has 2 objectives:\r\n1. Add objectID and updateID to BucketInfo proto persisted to DB.\r\n2. To ensure that bucket operations are idempotent, compare the transactionID with the objectID and updateID to make sure that the transaction is not a replay. If the transactionID <= updateID, then it implies that the transaction is a replay and hence it should be skipped.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ObjectID and updateID to BucketInfo to avoid replaying transactions"
   },
   {
      "_id": "13275187",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-12-18 08:13:53",
      "description": "{{ozone-mr/hadoop31}} test is failing with:\r\n\r\n{code}\r\nsudo: apk: command not found\r\n{code}\r\n\r\nNew hadoop:3.1.2 image is based on CentOS, not Alpine.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Hadoop 3.1 acceptance test fails with apk command not found"
   },
   {
      "_id": "13275167",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-12-18 07:08:02",
      "description": "Starting with Hugo 0.60, the new Goldmark renderer is configured to skip HTML fragments.  This breaks the doc layout in a few places, eg.:\r\n\r\n * _Easy start_, _Recommended_ etc. headers\r\n * tables\r\n * warning {{div}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Configure Goldmark renderer"
   },
   {
      "_id": "13275103",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-12-17 21:59:42",
      "description": "* Datanode should use Recon address to register and heartbeat to it.\r\n* Recon will act as a \"passive\" SCM to the Datanode. Here \"passive\" means that the Datanode will not get any commands from this Recon SCM.\r\n* Integrating SCM Node Manager into Recon.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Datanode registration and SCM Node management in Recon."
   },
   {
      "_id": "13274817",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-12-16 18:36:37",
      "description": "On OM restarts, it is possible that already applied ratis logs will be replayed. Hence, we need to ensure that all OM write operations are idempotent.\r\nThis Jira aims to add unit tests for testing that OM volume and bucket related operations are idempotent. \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Test OM Volume and Bucket operations are idempotent"
   },
   {
      "_id": "13274622",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-12-15 09:36:07",
      "description": "{{isNoneEmpty}} and {{isAllEmpty}} check variable number of strings.  For single string they can be replaced with {{isNotEmpty}} and {{isEmpty}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Unnecessary calls to isNoneEmpty and isAllEmpty"
   },
   {
      "_id": "13274620",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-12-15 09:16:56",
      "description": "Fix log messages where number of placeholders does not match the number of parameters provided.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Wrong number of placeholders in log message"
   },
   {
      "_id": "13274568",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-12-14 13:44:52",
      "description": "S3 acceptance test attempts to install {{awscli}} prior to each test case.  It is enough to do so before each suite.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "No need to try install awscli before each test"
   },
   {
      "_id": "13274546",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-12-14 08:02:40",
      "description": "Currently GitHub Actions workflows are configured to run all checks in parallel, except acceptance test.  The rationale is that acceptance test takes the most time, and there is no reason to run it if a cheaper check catches some problem.\r\n\r\nI propose to let GitHub Actions run acceptance test in parallel to address the following concerns:\r\n\r\n# Although acceptance test is the slowest (~60 minutes), unit test also takes quite some time (~20-25 minutes).  Serializing these two checks increases the time to get feedback on PRs and commits by ~33-40%.\r\n# For PRs and post-commit builds in forks, running all checks regardless of the result of independent checks allows authors to reduce the number of rounds they need to address any problems.\r\n# For post-commit builds in Apache master, we expect all checks to pass.  However, checks sometime fail eg. due to transient network errors.  Skipping acceptance test due to such a problem in another check provides no benefit.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Let GitHub Actions run acceptance check in parallel"
   },
   {
      "_id": "13274407",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-12-13 12:42:38",
      "description": "We need to reconfigure hugo to support multilingual site:\r\n\r\nhttps://gohugo.io/content-management/multilingual/",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable multilingual Hugo features in ozone docs"
   },
   {
      "_id": "13274404",
      "assignee": "elek",
      "components": [],
      "created": "2019-12-13 12:21:29",
      "description": "# DatanodeAdminMonitorInterface has some setter and getters but they are not part of the contract they are used by the implementation. An other implementation of the interface may require different setters. Therefore the interface should contain only the required fields.\r\n # DatanodeAdminMonitorInterface can be renamed to DatanodeAdminMonitor to follow the naming convention in the code (*Impl)\r\n # PipelineManager is unused field can be removed from DatanodeAdminMonitorImpl",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove methods of internal representation from DatanodeAdminMontor interface "
   },
   {
      "_id": "13274224",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-12-12 18:53:36",
      "description": "Change {{ChunkManager}} read/write methods to accept/return {{ChunkBuffer}} instead of {{ByteBuffer}}.  This allows seamlessly passing multiple buffers without further interface change.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Let ChunkManager read/write ChunkBuffer instead of ByteBuffer"
   },
   {
      "_id": "13274223",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-12-12 18:51:16",
      "description": "{{ChunkBuffer}} is a useful abstraction over {{ByteBuffer}} to hide whether it's backed by a single {{ByteBuffer}} or multiple ones ({{IncrementalChunkBuffer}}).  However, {{IncrementalChunkBuffer}} allocates its own {{ByteBuffer}} instances and only works with uniform buffer sizes.  The goal of this task is to allow wrapping an existing  {{List<ByteBuffer>}} in {{ChunkBuffer}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allow wrapping list of ByteBuffers with ChunkBuffer"
   },
   {
      "_id": "13274084",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-12-12 09:44:53",
      "description": "Let datanode handle incremental additions to chunks (data with non-zero offset).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle chunk increments in datanode"
   },
   {
      "_id": "13273948",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-12-11 19:30:56",
      "description": "JVM metrics are available from Datanode and SCM, but not from OM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM does not report JVM metrics"
   },
   {
      "_id": "13273672",
      "assignee": "elek",
      "components": [],
      "created": "2019-12-10 17:27:06",
      "description": "We have three places where we need the LICENSE/NOTICE files:\r\n\r\n\u00a0\r\n # \u00a0In the binary tar file (binary specific license)\r\n # \u00a0In the src tar file (source specific license)\r\n # In the root of the git repository\r\n\r\n1 and 2 are fine now (we have dedicated file under hadoop-ozone/dist) but as we have shared the repository with main hadoop the root LICENSE/NOTICE (3) still contains the hadoop specific content.\r\n\r\nWe need to use the files from (2) for (3).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Use Ozone specific LICENCE and NOTICE in the root of the git repo"
   },
   {
      "_id": "13273669",
      "assignee": "elek",
      "components": [],
      "created": "2019-12-10 17:13:22",
      "description": "Thanks to [~dineshchitlangia] who reported this problem.\r\n\r\nWith a release build:\r\n{code:java}\r\nmvn clean install -Dmaven.javadoc.skip=true -DskipTests -Psign,dist,src -Dtar -Dgpg.keyname=$CODESIGNINGKEY {code}\r\nThe source package (*the* release) is not created.\r\n\r\nIn fact it's created, but the problem with the order of clean and install:\r\n * clean is executed in the root project\r\n * install is executed in the root project (creates hadoop-ozone/dist/target/..src.tar.gx\r\n * .....\r\n * clean is executed in the hadoop-ozone/dist project *(here the src package is deleted)*\r\n * install is executed in the hadoop-ozone/dist project\r\n\r\nOne possible fix is to move the creation of the src package to the hadoop-ozone/dist project (but do it from the project root)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Source tar file is not created during the relase build"
   },
   {
      "_id": "13273596",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-12-10 11:47:48",
      "description": "Run teragen, and it failed to enter the mapreduce stage and print the following warnning message on console endlessly. \r\n\r\n\r\n{noformat}\r\n\r\n19/12/10 19:23:54 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-FBD45C9313A5->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0\r\n19/12/10 19:23:54 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}\r\n19/12/10 19:26:16 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-7C5A7B5CFC31->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0\r\n19/12/10 19:26:16 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}\r\n19/12/10 19:28:38 WARN io.KeyOutputStream: Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client:client-B3D8C0052C4E->RAFT is closed. on the pipeline Pipeline[ Id: 90deb863-e511-4a5e-ae86-dc8035e8fa7d, Nodes: ed90869c-317e-4303-8922-9fa83a3983cb{ip: 10.120.113.172, host: host172, networkLocation: /rack2, certSerialId: null}1da74a1d-f64d-4ad4-b04c-85f26687e683{ip: 10.121.124.44, host: host044, networkLocation: /rack2, certSerialId: null}515cab4b-39b5-4439-b1a8-a7b725f5784a{ip: 10.120.139.122, host: host122, networkLocation: /rack1, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN, leaderId:515cab4b-39b5-4439-b1a8-a7b725f5784a ]. The last committed block length is 0, uncommitted data length is 295833 retry count 0\r\n19/12/10 19:28:38 INFO io.BlockOutputStreamEntryPool: Allocating block with ExcludeList {datanodes = [], containerIds = [], pipelineIds = [PipelineID=90deb863-e511-4a5e-ae86-dc8035e8fa7d]}\r\n\r\n{noformat}\r\n\r\n\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Client failed to recover from ratis AlreadyClosedException exception"
   },
   {
      "_id": "13273334",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-12-09 11:42:47",
      "description": "{{HddsVolume#layoutVersion}} is a version number, supposed to be used for handling upgrades from older versions.  Currently only one version is defined.  But should a new version be introduced, HddsVolume would fail to read older version file.  This is caused by a check in {{HddsVolumeUtil}} that only considers the latest version as valid:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java#L137-L153}\r\n  /**\r\n   * Returns layOutVersion if it is valid. Throws an exception otherwise.\r\n   */\r\n  @VisibleForTesting\r\n  public static int getLayOutVersion(Properties props, File versionFile) throws\r\n      InconsistentStorageStateException {\r\n    String lvStr = getProperty(props, OzoneConsts.LAYOUTVERSION, versionFile);\r\n\r\n    int lv = Integer.parseInt(lvStr);\r\n    if(DataNodeLayoutVersion.getLatestVersion().getVersion() != lv) {\r\n      throw new InconsistentStorageStateException(\"Invalid layOutVersion. \" +\r\n          \"Version file has layOutVersion as \" + lv + \" and latest Datanode \" +\r\n          \"layOutVersion is \" +\r\n          DataNodeLayoutVersion.getLatestVersion().getVersion());\r\n    }\r\n    return lv;\r\n  }\r\n{code}\r\n\r\nI think this should check whether the version number identifies a known {{DataNodeLayoutVersion}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "upgrade"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "HddsVolume#readVersionFile fails when reading older versions"
   },
   {
      "_id": "13273328",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-12-09 11:11:58",
      "description": "{{HddsVolume}} [initializes {{layoutVersion}} using latest {{ChunkLayOutVersion}}|https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java#L268].  But when writing the same info to file, it [verifies {{layoutVersion}} matches the latest {{DataNodeLayoutVersion}}|https://github.com/apache/hadoop-ozone/blob/1d56bc244995e857b842f62d3d1e544ee100bbc1/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/HddsVolume.java#L292-L293].  To satisfy the condition {{ChunkLayOutVersion}} and {{DataNodeLayoutVersion}} have to be in sync, which means only one of them is necessary.  I think the intention was to use {{DataNodeLayoutVersion}} in both cases, as {{ChunkLayOutVersion}} is for key-value container internal structure.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "HddsVolume mixes ChunkLayOutVersion and DataNodeLayoutVersion"
   },
   {
      "_id": "13272954",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-12-07 16:54:33",
      "description": "Sonar reports lots of duplication around {{AclHandler}} classes in Ozone Shell.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Shell code cleanup"
   },
   {
      "_id": "13272939",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-12-07 13:23:19",
      "description": "{code:java}\r\nayush@ayushpc:~/ozone/hadoop-ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/bin$ ./ozone fs -fs o3fs://bucket.hive -put -d ozone /dir/ozone/file\r\nput: NOT_A_FILE: Can not create file: dir/ozone/fileas there is already file in the given path\r\n{code}\r\n\r\n\"as\" got attached to the name of the file.\r\n\r\nOMFileCreateRequest L211 and L246 : Need to add space before \"as\"",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "OMException NOT_A_FILE missing space in the exception message"
   },
   {
      "_id": "13272909",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-12-07 10:05:01",
      "description": "{{ContainerUtils}} and {{BlockUtils}} have some helper functions to build responses to container commands.  These would be useful for client-side unit tests, but {{client}} does not depend on {{container-service}} since the interfaces and messages it needs are defined in {{common}}.  This issue proposes to move these helpers to {{common}} to avoid duplicating the functionality for tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Refactor container response builders to hdds-common"
   },
   {
      "_id": "13272880",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-12-07 00:34:30",
      "description": "Fix all sonar issues in package org.apache.hadoop.ozone.recon.api.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix sonar issues in package org.apache.hadoop.ozone.recon.api."
   },
   {
      "_id": "13272728",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-12-06 11:45:13",
      "description": "Expose test code from {{hadoop-hdds/common}} to other modules.  Move some \"common\" test utilities.  Example: random {{DatanodeDetails}} creation.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor common test utilities to hadoop-hdds/common"
   },
   {
      "_id": "13272609",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-12-05 22:32:02",
      "description": "This Jira is to fix a bug that has been caused by HDDS-2637.\r\n\r\nNow we have applyTransactionMap where we put all entries of ratisTransactionIndex before complete.\r\n\r\n\u00a0\r\n\r\nAnd when updating lastAppliedIndex, now there is a chance that we updateLastAppliedIndex before DoubleBuffer flush has committed transactions to DB.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nLet's take 1-10 are apply transaction entries which are not flushed, 11th is indexUpdate a metadata/conf entry transaction, now when notifyIndexUpdate is called with 11, we updateLastAppliedIndex to 11. (Which we should not do) This Jira is to fix this issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix updating lastAppliedIndex in OzoneManagerStateMachine"
   },
   {
      "_id": "13272507",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-12-05 13:43:13",
      "description": "Ozone's default log4j patterns should include thread name, as it helps a bit in understanding events.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Add thread name to log pattern"
   },
   {
      "_id": "13272505",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-12-05 13:39:16",
      "description": "Found this in a local acceptance test run:\r\n\r\n{code}\r\nStart freon testing                                                   | FAIL |\r\n'2019-12-05 12:25:24,744 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n2019-12-05 12:25:24,934 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n2019-12-05 12:25:24,935 [main] INFO impl.MetricsSystemImpl: ozone-freon metrics system started\r\n2019-12-05 12:25:26,690 [main] INFO freon.RandomKeyGenerator: Number of Threads: 1\r\n2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Volumes: 5.\r\n2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Buckets per Volume: 5.\r\n2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Number of Keys per Bucket: 5.\r\n2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Key size: 10240 bytes\r\n2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: Buffer size: 4096 bytes\r\n2019-12-05 12:25:26,691 [main] INFO freon.RandomKeyGenerator: validateWrites : false\r\n    [ Message content over the limit has been removed. ]\r\n....util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\r\n***************************************************\r\nStatus: Success\r\nGit Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf\r\nNumber of Volumes created: 5\r\nNumber of Buckets created: 25\r\nNumber of Keys added: 125\r\nRatis replication factor: THREE\r\nRatis replication type: RATIS\r\nAverage Time spent in volume creation: 00:00:00,210\r\nAverage Time spent in bucket creation: 00:00:00,213\r\nAverage Time spent in key creation: 00:00:37,506\r\nAverage Time spent in key write: 00:01:42,157\r\nTotal bytes written: 1280000\r\nTotal Execution time: 00:02:31,516\r\n***************************************************' contains 'ERROR'\r\n{code}\r\n\r\nNeed to check if {{Status: Success}} is true (ie. if keys were indeed successfully created), and if so, {{ERROR}} in output should make not the test failed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Acceptance test may fail despite success status"
   },
   {
      "_id": "13272398",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-12-05 06:41:32",
      "description": "Promethues version 2.14.0\r\n\r\nconfiguration set in promethues.yml, in which 10.120.110.183 is the master with OM and SCM. All others are datanodes.\r\n\r\n{code}\r\nscrape_configs:\r\n  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\r\n  - job_name: 'ozone'\r\n    metrics_path: /prom\r\n\r\n    # metrics_path defaults to '/metrics'\r\n    # scheme defaults to 'http'.\r\n\r\n    static_configs:\r\n    - targets:\r\n      - \"10.120.110.183:8080\"\r\n      - \"10.120.110.183:8081\"\r\n      - \"10.120.139.122:9882\"\r\n      - \"10.120.139.111:9882\"\r\n      - \"10.120.113.172:9882\"\r\n      - \"10.121.124.44:9882\"\r\n{code}\r\n\r\nThe UP datanoe 122 is not part of the THREE factor pipeline.  The later is formed by the other 3 datanodes which are all DOWN. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Prometheus reports invalid metric type"
   },
   {
      "_id": "13272226",
      "assignee": "elek",
      "components": [],
      "created": "2019-12-04 13:22:19",
      "description": "HDDS-2413 proposes an additional usage of the @Config annotation: to set configuration based on an existing configuration class.\r\n\r\nBut as of now we annotate the setters instead of the fields. To avoid annotation duplication (we need to read the values from the getters or the fields) I propose to switch to use field based annotations instead of setter based annotation.\r\n\r\nI think it's more readable and additional validation (even the class level validations) can be done in a @PostConstruct method.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use field based Config annotation instead of method based"
   },
   {
      "_id": "13272190",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-12-04 11:25:41",
      "description": "The goal of this task is to create a new insight point for the datanode container protocol ({{HddsDispatcher}}) to be able to debug {{client<->datanode}} communication.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create insight point for datanode container protocol"
   },
   {
      "_id": "13271888",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-12-03 08:11:20",
      "description": "{{ozone insight log}} command changes log level to debug or trace.  After streaming is stopped, it attempts to reset to info.  This does not seem to work, probably because the process is abruptly stopped (Ctrl-C).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Insight log level reset does not work"
   },
   {
      "_id": "13271626",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-12-02 07:37:05",
      "description": "{{Future submit(Runnable)}} and {{void execute(Runnable)}} in {{ExecutorService}} have the same result.  If the returned {{Future}} is ignored, {{execute}} can be used instead of {{submit}} to avoid creating some objects.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Prefer execute() over submit() if the returned Future is ignored"
   },
   {
      "_id": "13271520",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-30 19:13:11",
      "description": "{{NetUtils#normalize}} uses {{String#replaceAll}}, which creates a {{Pattern}} for each call.  It could be replaced with a pre-compiled {{Pattern}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Use pre-compiled Pattern in NetUtils#normalize"
   },
   {
      "_id": "13271512",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-11-30 17:04:27",
      "description": "Extend {{entrypoint.sh}} to set the kernel parameters required for profiling if the {{ASYNC_PROFILER_HOME}} environment variable (used by {{ProfileServlet}}) is set.\r\n\r\nRef:\r\n\r\n{code:title=https://cwiki.apache.org/confluence/display/HADOOP/Java+Profiling+of+Ozone}\r\necho 1 > /proc/sys/kernel/perf_event_paranoid\r\necho 0 > /proc/sys/kernel/kptr_restrict\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Conditionally enable profiling at the kernel level"
   },
   {
      "_id": "13271503",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-11-30 14:33:39",
      "description": "New Freon tests (descendants of {{BaseFreonGenerator}}) suffer from a similar memory issue due to concurrency handling as HDDS-1785.\r\n\r\nSteps to reproduce:\r\n\r\n{code}\r\nexport HADOOP_OPTS='-Xmx256M -XX:+HeapDumpOnOutOfMemoryError'\r\nozone freon omkg -F ONE -n 33554432 -t 10 -p omkg\r\n{code}\r\n\r\nFreon attempts to submit 32M tasks to the executor, requiring at least 1.5GB memory.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve executor memory usage in new Freon tests"
   },
   {
      "_id": "13271333",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-29 08:04:04",
      "description": "The test is flaky:\r\n\r\n\u00a0\r\n\r\nExample run: [https://github.com/apache/hadoop-ozone/runs/325281277]\r\n\r\n\u00a0\r\n\r\nFailure:\r\n{code:java}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse\r\n-------------------------------------------------------------------------------\r\nTests run: 3, Failures: 1, Errors: 0, Skipped: 1, Time elapsed: 5.31 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse\r\ntestDoubleBufferWithMixOfTransactionsParallel(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 0.282 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<32> but was:<29>\r\n        at org.junit.Assert.fail(Assert.java:88)\r\n        at org.junit.Assert.failNotEquals(Assert.java:743)\r\n        at org.junit.Assert.assertEquals(Assert.java:118)\r\n        at org.junit.Assert.assertEquals(Assert.java:555)\r\n        at org.junit.Assert.assertEquals(Assert.java:542)\r\n        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBufferWithMixOfTransactionsParallel(TestOzoneManagerDoubleBufferWithOMResponse.java:247)\r\n {code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerDoubleBufferWithOMResponse"
   },
   {
      "_id": "13271258",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-28 16:01:00",
      "description": "After HDDS-2034 (or even before?) pipeline creation (or the status transition from ALLOCATE to OPEN) requires at least one pipeline report from all of the datanodes. Which means that the cluster might not be usable even if it's out from the safe mode AND there are at least three datanodes.\r\n\r\nIt makes all the acceptance tests unstable.\r\n\r\nFor example in [this|https://github.com/apache/hadoop-ozone/pull/263/checks?check_run_id=324489319] run.\r\n{code:java}\r\nscm_1         | 2019-11-28 11:22:54,401 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode 548f146f-2166-440a-b9f1-83086591ae26\r\nscm_1         | 2019-11-28 11:22:54,402 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c\r\nscm_1         | 2019-11-28 11:22:54,404 INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb create command to datanode 47dbb8e4-bbde-4164-a798-e47e8c696fb5\r\nscm_1         | 2019-11-28 11:22:54,405 INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 8dc4aeb6-5ae2-46a0-948d-287c97dd81fb, Nodes: 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}47dbb8e4-bbde-4164-a798-e47e8c696fb5{ip: 172.24.0.2, host: ozoneperf_datanode_2.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED]\r\nscm_1         | 2019-11-28 11:22:56,975 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}\r\nscm_1         | 2019-11-28 11:22:58,018 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}\r\nscm_1         | 2019-11-28 11:23:01,871 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}\r\nscm_1         | 2019-11-28 11:23:02,817 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by 548f146f-2166-440a-b9f1-83086591ae26{ip: 172.24.0.10, host: ozoneperf_datanode_3.ozoneperf_default, networkLocation: /default-rack, certSerialId: null}\r\nscm_1         | 2019-11-28 11:23:02,847 INFO pipeline.PipelineReportHandler: Pipeline THREE PipelineID=8dc4aeb6-5ae2-46a0-948d-287c97dd81fb reported by dccee7c4-19b3-41b8-a3f7-b47b0ed45f6c{ip: 172.24.0.5, host: ozoneperf_datanode_1.ozoneperf_default, networkLocation: /default-rack, certSerialId: null} {code}\r\nAs you can see the pipeline is created but the the cluster is not usable as it's not yet reporter back by datanode_2:\r\n{code:java}\r\nscm_1         | 2019-11-28 11:23:13,879 WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Retrying get pipelines c\r\nall once.\r\nscm_1         | org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 0 nodes.{code}\r\n\u00a0The quick fix is to configure all the compose clusters to wait until one pipeline is available. This can be done by adjusting the number of the required datanodes:\r\n{code:java}\r\n// We only care about THREE replica pipeline\r\nint minHealthyPipelines = minDatanodes /\r\n    HddsProtos.ReplicationFactor.THREE_VALUE; {code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Start acceptance tests only if at least one THREE pipeline is available"
   },
   {
      "_id": "13271173",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-28 10:13:36",
      "description": "Run(master): [https://github.com/apache/hadoop-ozone/runs/324342299]\r\n\r\n\u00a0\r\n{code:java}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl\r\n-------------------------------------------------------------------------------\r\nTests run: 10, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 2.955 s <<< FAILURE! - in org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl\r\ntestPartialTableCacheWithOverrideAndDelete[0](org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl)  Time elapsed: 0.039 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<2> but was:<6>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat org.junit.Assert.assertEquals(Assert.java:555)\r\n\tat org.junit.Assert.assertEquals(Assert.java:542)\r\n\tat org.apache.hadoop.hdds.utils.db.cache.TestTableCacheImpl.testPartialTableCacheWithOverrideAndDelete(TestTableCacheImpl.java:308)\r\n\r\n {code}\r\n*How to reproduce it locally?*\r\n\r\nReplace the last tableCache.evict call of testPartialTableCacheWithOverrideAndDelete to System.out.println(tableCache.size()).\r\n\r\nYou will see that the cache size is 2 even before the cleanup therefore the next GeneriTestUtils.waitFor is useless (it doesn't guarantee that the cleanup is finished).\r\n\r\n*Fix:*\r\n\r\nI propose to call the cleanup sync with using the Impl class instead of the interface. It simplifies the test but still validates the behavior.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestTableCacheImpl is flaky"
   },
   {
      "_id": "13271103",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-27 22:05:55",
      "description": "This Jira is to handle LeaderNotReadyException in OM and also update to latest ratis version.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle LeaderNot ready exception in OzoneManager StateMachine and upgrade ratis to latest version."
   },
   {
      "_id": "13270983",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-11-27 12:59:08",
      "description": "Findbugs/Spotbugs only checks Java code.  We can skip frontend plugin execution for Recon to save ~2 minutes.  Makes a difference mostly when running it locally.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Let findbugs.sh skip frontend plugin for Recon"
   },
   {
      "_id": "13270961",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-11-27 11:23:09",
      "description": "{{TestContainerPersistence#testDeleteChunk}} is failing due to unexpected exception message.  This is caused by mix of two commits:\r\n\r\n * https://github.com/apache/hadoop-ozone/commit/fe7fccf2b changed actual message\r\n * https://github.com/apache/hadoop-ozone/commit/4a9174500 moved the test case from integration to unit\r\n\r\nEach of these was tested without the other.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix TestContainerPersistence#testDeleteChunk"
   },
   {
      "_id": "13270902",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-11-27 07:43:34",
      "description": "See attached log file",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "NullPointerException in S3g"
   },
   {
      "_id": "13270898",
      "assignee": "xyao",
      "components": [],
      "created": "2019-11-27 06:54:52",
      "description": "There is a regression from HDDS-738, where the time stamp formatting was lost during the json output. (Detail can be found from the removed methods from OzoneClientUtils#asVolumeInfo, asBucketInfo, asKeyInfo). As a result, only raw time is output in the json output for volume/bucket/key info.\u00a0\r\n{code:java}\r\nbash-4.2$ ozone sh vol create /vol1\r\nobash-4.2$ ozone sh vol info /vol1\r\n{\r\n \"metadata\" : { },\r\n \"name\" : \"vol1\",\r\n \"admin\" : \"hadoop\",\r\n \"owner\" : \"hadoop\",\r\n \"creationTime\" : 1574836577298,\r\n \"acls\" : [ {\r\n \"type\" : \"USER\",\r\n \"name\" : \"hadoop\",\r\n \"aclScope\" : \"ACCESS\",\r\n \"aclList\" : [ \"ALL\" ]\r\n }, {\r\n \"type\" : \"GROUP\",\r\n \"name\" : \"users\",\r\n \"aclScope\" : \"ACCESS\",\r\n \"aclList\" : [ \"ALL\" ]\r\n } ],\r\n \"quota\" : 1152921504606846976\r\n}{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone CLI: CreationTime/modifyTime of volume/bucket/key info are not formatted"
   },
   {
      "_id": "13270874",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-27 03:45:55",
      "description": "Improve type safety in {{AuditMessage$Builder}} for methods {{forOperation}} and {{withResult}} by using existing {{interface AuditAction}} and {{enum AuditEventStatus}} respectively instead of Strings.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Make AuditMessage parameters strongly typed"
   },
   {
      "_id": "13270320",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-11-24 17:26:07",
      "description": "_unit_ step of Github Actions-based CI checks is failing for commits pushed to forks due to lack of {{SONARCLOUD_TOKEN}}.\r\n\r\nBackground: HDDS-2587 added Sonar check in post-commit workflow, publishing results to SonarCloud.  It does not work in forks, as it requires SonarCloud token.  This causes _unit_ step to fail completely.  Example: https://github.com/bharatviswa504/hadoop-ozone/runs/316829850",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Skip sonar check in forks"
   },
   {
      "_id": "13270317",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-24 15:51:13",
      "description": "{{OzoneSecurityUtil#getValidInetsForCurrentHost}} performs hostname lookup for all local network interfaces, even for invalid addresses.  This significantly slows down some secure tests ({{TestHddsSecureDatanodeInit}}, {{TestSecureOzoneCluster}}) when run on a machine with special IPv6 network interfaces due to timeout reaching IPv6 DNS servers.\r\n\r\nThis issue proposes to disable the lookup for invalid addresses.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid hostname lookup for invalid local IP addresses"
   },
   {
      "_id": "13270289",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-11-24 05:51:18",
      "description": "SCM should expose the 'SCMDatanodeProtocol' RPC protocol server as an endpoint. This is the first step in sending DN reports to Recon.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Expose SCMDatanodeProtocolServer RPC endpoint through Recon."
   },
   {
      "_id": "13270185",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-11-22 22:00:16",
      "description": "RATIS-714 introduced a config setting for limiting the max number of bytes of pending requests. This Jira aims to add a config in Ozone to set the same in DN Ratis.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add config parameter for setting limit on total bytes of pending requests in Ratis"
   },
   {
      "_id": "13270180",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-22 20:32:41",
      "description": "OM HA robot tests were disable in HDDS-2533 as they were failing intermittently. HDDS-2454 fixes some issues in the HA tests. Creating this Jira so as to run re-enable the HA acceptance tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable OM HA acceptance tests"
   },
   {
      "_id": "13270163",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-22 19:20:37",
      "description": "This Jira is to fix listMultiparts API in HA code path.\r\n\r\nIn HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listParts of a MPU key, it should use both in-memory cache and rocksdb mpu table to list parts of a mpu key.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix listMultipartupload API"
   },
   {
      "_id": "13270019",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-11-22 05:47:40",
      "description": "https://sonarcloud.io/project/issues?directories=hadoop-ozone%2Frecon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Frecon%2Ftasks&id=hadoop-ozone&open=AW5md-jJKcVY8lQ4Zr9D&resolved=false",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Sonar issues in ReconTaskControllerImpl"
   },
   {
      "_id": "13269884",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-11-21 16:01:41",
      "description": "Some test classes in {{integration-test}} are actually unit tests: they do not start a mini cluster, nor even multiple components.  These can be moved to the subprojects they belong to (eg. {{container-service}}.  The benefit is that it will be easier to spot if they are broken, since integration tests are executed less frequently.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move plain unit tests out of integration tests"
   },
   {
      "_id": "13269847",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-11-21 13:31:37",
      "description": "The safe mode can be checked with \"ozone scmcli safemode status\". But for acceptance tests there is no easy way to check if the cluster is ready to execute the tests (See HDDS-2606 for example).\r\n\r\nOne easy solution is to create a polling version from \"safemode status\".\r\n\r\n\"safemode wait --timeout ...\" can be blocked until the scm is out from the safe mode.\r\n\r\nWit proper safe mode rules (min datanodes + min pipline numbers) it can help us to check if the acceptance tests are ready to test.\r\n\r\nSame command can be used in k8s as well to test if the cluster is ready to start the freon commands...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide command to wait until SCM is out from the safe-mode"
   },
   {
      "_id": "13269798",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-11-21 10:44:29",
      "description": "Some of the acceptance tests are failing because the first Ratis.THREE pipeline is not created on time:\r\n\r\nFor example in a HA proxy acceptance test the first block allocation is tarted before moving the Ratis.THREE pipeline to OPEN state.\r\n{code:java}\r\nscm_1       | 2019-11-20 14:45:38,359 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 4d27f405-d257-4b1b-a7b3-51bbd57356db, Nodes: 2c010ef7-8c0e-45e3-b230-326bf759773b{ip: 172.25.0.7, host: ozones3-haproxy_datanode_2.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] moved to OPEN state\r\nscm_1       | 2019-11-20 14:45:46,944 WARN block.BlockManagerImpl: Pipeline creation failed for type:RATIS factor:THREE. Retrying get pipelines call once.\r\nscm_1       | org.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 0 nodes.\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:153)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:58)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:155)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:198)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:187)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:159)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.processMessage(ScmBlockLocationProtocolServerSideTranslatorPB.java:117)\r\nscm_1       | \tat org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:72)\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:98)\r\nscm_1       | \tat org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:13157)\r\nscm_1       | \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\nscm_1       | \tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\nscm_1       | \tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\nscm_1       | \tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\nscm_1       | \tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\nscm_1       | \tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\nscm_1       | \tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\nscm_1       | \tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\nscm_1       | 2019-11-20 14:45:46,944 INFO block.BlockManagerImpl: Could not find available pipeline of type:RATIS and factor:THREE even after retrying\r\nscm_1       | 2019-11-20 14:45:46,945 ERROR block.BlockManagerImpl: Unable to allocate a block for the size: 268435456, type: RATIS, factor: THREE\r\nscm_1       | 2019-11-20 14:45:49,274 INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: e208588c-9a16-4519-89dc-7bad5bae4331, Nodes: 1da10d32-12be-4328-bc0a-f3d8de21b056{ip: 172.25.0.8, host: ozones3-haproxy_datanode_3.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}d9edb776-ee02-48a1-8c73-40f33bc0d128{ip: 172.25.0.5, host: ozones3-haproxy_datanode_1.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}2c010ef7-8c0e-45e3-b230-326bf759773b{ip: 172.25.0.7, host: ozones3-haproxy_datanode_2.ozones3-haproxy_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] moved to OPEN state {code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Acceptance tests are flaky due to async pipeline creation"
   },
   {
      "_id": "13269794",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-11-21 10:25:58",
      "description": "Freon's {{ProgressBar}} uses {{Supplier<Long>}}, which could be replaced with {{LongSupplier}} to avoid boxing.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Use LongSupplier to avoid boxing"
   },
   {
      "_id": "13269787",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-21 10:01:55",
      "description": "{{XceiverClientReply#logIndex}} is unnecessarily boxed/unboxed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid unnecessary boxing in XceiverClientReply"
   },
   {
      "_id": "13269732",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-11-21 04:39:56",
      "description": "This is a simple refactoring change where all the chaos test are moved to  org.apache.hadoop.ozone.chaos package",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Move chaos test to org.apache.hadoop.ozone.chaos package"
   },
   {
      "_id": "13269699",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-11-21 00:05:51",
      "description": "\u00a0\r\n{code:java}\r\n2019-11-20 15:32:04,684 WARN org.eclipse.jetty.servlet.ServletHandler:\r\njavax.servlet.ServletException: java.lang.NumberFormatException: For input string: \"3977248768\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1780)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1609)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.Server.handle(Server.java:539)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:748)\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3 RangeReads failing with NumberFormatException"
   },
   {
      "_id": "13269665",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-11-20 20:05:50",
      "description": "{{ContainerSet#listContainer}} has this code:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/3c334f6a7b344e0e5f52fec95071c369286cfdcb/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerSet.java#L198}\r\nmap = containerMap.tailMap(containerMap.firstKey(), true);\r\n{code}\r\n\r\nThis is equivalent to:\r\n\r\n{code}\r\nmap = containerMap;\r\n{code}\r\n\r\nsince {{tailMap}} is a sub-map with all keys larger than or equal to ({{inclusive=true}}) {{firstKey}}, which is the lowest key in the map.  So it is a sub-map with all keys, ie. the whole map.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "No tailMap needed for startIndex 0 in ContainerSet#listContainer"
   },
   {
      "_id": "13269613",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-11-20 15:24:52",
      "description": "There are a few slightly different sample docker compose environments: ozone, ozoneperf, ozones3, ozone-recon. This issue proposes to merge these 4 by minor additions to ozoneperf:\r\n\r\n # add {{recon}} service from {{ozone-recon}}\r\n # run GDPR and S3 tests\r\n # expose datanode web port (eg. for profiling)\r\n\r\nPlus: also run ozone-shell test (from basic suite), which is currently run only in ozonesecure\r\n\r\nI also propose to rename {{ozoneperf}} to {{ozone}} for simplicity.\r\n\r\nConsolidating these 4 environments would slightly reduce both code duplication and the time needed for acceptance tests.\r\n\r\nCC [~elek]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Consolidate compose environments"
   },
   {
      "_id": "13269563",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-20 10:28:38",
      "description": "As sonarcloud has been started to use, it would be great to upload measurement data from github actions steps...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable sonarcloud measurement as part of CI builds"
   },
   {
      "_id": "13269503",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-20 04:48:26",
      "description": "Use try-with-resources or close this \"FileOutputStream\" in a \"finally\" clause.\r\n\r\nGetKeyHandler: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6HHKTfdBVcJdcVFsvC&open=AW6HHKTfdBVcJdcVFsvC]\r\n\r\n\u00a0\r\n\r\nUse try-with-resources or close this \"OzoneOutputStream\" in a \"finally\" clause.\r\n\r\nPutKeyHandler: [https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6HHKRodBVcJdcVFsvB&open=AW6HHKRodBVcJdcVFsvB]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure resources are closed in Get/PutKeyHandler"
   },
   {
      "_id": "13269501",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-11-20 04:41:57",
      "description": "Currently, if the client reads from all Datanodes in the pipleine fail, the read fails altogether. There may be a case when the container is moved to a new pipeline by the time client reads. In this case, the client should request for a refresh pipeline from OM, and read it again if the new pipeline returned from OM is different. \r\n\r\nThis behavior is consistent with that of HDFS.\r\ncc [~msingh] / [~shashikant] / [~hanishakoneru]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone client should refresh pipeline info if reads from all Datanodes fail."
   },
   {
      "_id": "13269498",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-11-20 04:31:20",
      "description": "https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-Z7KcVY8lQ4Zr1l&open=AW5md-Z7KcVY8lQ4Zr1l",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in OzoneManagerProtocolServerSideTranslatorPB"
   },
   {
      "_id": "13269494",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-11-20 04:25:22",
      "description": "https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-m5KcVY8lQ4ZsAc&open=AW5md-m5KcVY8lQ4ZsAc",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in KeyOutputStream"
   },
   {
      "_id": "13269492",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-11-20 04:23:32",
      "description": "[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW6BMuREm2E_7tGaNiTh&open=AW6BMuREm2E_7tGaNiTh]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in SCMPipelineManager"
   },
   {
      "_id": "13269490",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-11-20 04:22:32",
      "description": "https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-x8KcVY8lQ4ZsIJ&open=AW5md-x8KcVY8lQ4ZsIJ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in ProfileServlet"
   },
   {
      "_id": "13269479",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-11-20 04:11:28",
      "description": "[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-0nKcVY8lQ4ZsLH&open=AW5md-0nKcVY8lQ4ZsLH]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in Scheduler"
   },
   {
      "_id": "13269478",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-11-20 04:10:00",
      "description": "Fix 2 instances:\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-02KcVY8lQ4ZsLU&open=AW5md-02KcVY8lQ4ZsLU]\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-02KcVY8lQ4ZsLV&open=AW5md-02KcVY8lQ4ZsLV]\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in BackgroundService"
   },
   {
      "_id": "13269477",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-11-20 04:08:44",
      "description": "Fix 2 instances:\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2aKcVY8lQ4ZsNW&open=AW5md-2aKcVY8lQ4ZsNW]\r\n\r\n\u00a0\r\n\r\n[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2aKcVY8lQ4ZsNX&open=AW5md-2aKcVY8lQ4ZsNX]\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in XceiverClientSpi"
   },
   {
      "_id": "13269476",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-11-20 04:07:45",
      "description": "[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_8KcVY8lQ4ZsVw&open=AW5md-_8KcVY8lQ4ZsVw]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle InterruptedException in CommitWatcher"
   },
   {
      "_id": "13269470",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-11-20 03:32:40",
      "description": "[https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-cLKcVY8lQ4Zr2o&open=AW5md-cLKcVY8lQ4Zr2o]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Sonar: Save and reuse Random object in GenesisUtil"
   },
   {
      "_id": "13269299",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-11-19 11:40:39",
      "description": "As of now the github actions based CI runs uses the branch of the PR which is the forked repo most of the time.\r\n\r\nIt would be better to force a rebase/merge (without push) before the builds to test the possible state after the merge not before.\r\n\r\nFor example if a PR branch uses elek/hadoop-ozone:HDDS-1234 and request a merge to apache/hadoop-ozone:master then the build should download the HDDS-1234 from elek/hadoop-ozone AND *rebase/merge* to the apache/hadoop-ozone *before* the build.\r\n\r\nThis merge is temporary just for the build/checks (no push at all).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "CI builds should use merged code state instead of the forked branch"
   },
   {
      "_id": "13269175",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-18 22:03:48",
      "description": "This Jira is to add ozone.om.internal.serviceid to let OM knows it belong to a particular service.\r\n\r\n\u00a0\r\n\r\nAs now we have ozone.om.service.ids -\u2265 where we can define all service id's in a cluster.(This can happen if the same config is shared across the cluster)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ozone.om.internal.service.id to OM HA configuration"
   },
   {
      "_id": "13269074",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-18 13:28:30",
      "description": "Flakiness can be reproduced locally. Usually it passes, but when I started to run it 100 times parallel with high cpu load it failed with the 3rd attempt (timed out)\r\n{code:java}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse\r\n-------------------------------------------------------------------------------\r\nTests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 503.297 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse\r\ntestDoubleBuffer(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse)  Time elapsed: 500.122 s  <<< ERROR!\r\njava.lang.Exception: test timed out after 500000 milliseconds\r\n        at java.lang.Thread.sleep(Native Method)\r\n        at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:382)\r\n        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:385)\r\n        at org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:129)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n {code}\r\nIndependent from the flakiness I think a test where the timeout is 8 minutes and starts 1000 threads to insert 500 buckets (500_000 buckets all together) it's more like an integration test and would be better to move the slowest part to the integration-test project.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerDoubleBufferWithOMResponse is flaky"
   },
   {
      "_id": "13269015",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-18 09:03:04",
      "description": "As of now we have 40 pull requests in open change but the CI gate are broken by earlier commit.\r\n\r\n\u00a0\r\n\r\nI propose the following solution to make it possible to safely merge pull requests:\r\n # Disable the failing tests (we identified them at [https://github.com/apache/hadoop-ozone/pull/203)]\r\n # Commit the immediate fixes (like HDDS-2521)\r\n # Start a discussion on ozone-dev to use a more strict revert policy (revert immediately)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Disable failing acceptance and unit tests"
   },
   {
      "_id": "13268960",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-11-18 00:07:50",
      "description": "{code}\r\n//BufferPool\r\n  public void releaseBuffer(ByteBuffer byteBuffer) {\r\n    // always remove from head of the list and append at last\r\n    ByteBuffer buffer = bufferList.remove(0);\r\n    // Ensure the buffer to be removed is always at the head of the list.\r\n    Preconditions.checkArgument(buffer.equals(byteBuffer));\r\n    buffer.clear();\r\n    bufferList.add(buffer);\r\n    Preconditions.checkArgument(currentBufferIndex >= 0);\r\n    currentBufferIndex--;\r\n  }\r\n{code}\r\nIn the code above, it expects buffer and byteBuffer are the same object, i.e.  buffer == byteBuffer.  However the precondition is checking buffer.equals(byteBuffer). Unfortunately, both buffer and byteBuffer have remaining() == 0 so that equals(..) returns true and the precondition does not catch the bug.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "BufferPool.releaseBuffer may release a buffer different than the head of the list"
   },
   {
      "_id": "13268922",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-11-17 14:45:03",
      "description": "TestSecureOzoneCluster is failing with {{failure to login}}.\r\n\r\n{code:title=https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2291-5997d/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestSecureOzoneCluster.txt}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.ozone.TestSecureOzoneCluster\r\n-------------------------------------------------------------------------------\r\nTests run: 10, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 23.937 s <<< FAILURE! - in org.apache.hadoop.ozone.TestSecureOzoneCluster\r\ntestSCMSecurityProtocol(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 2.474 s  <<< ERROR!\r\norg.apache.hadoop.security.KerberosAuthException: \r\nfailure to login: for principal: scm/pr-hdds-2291-5997d-4279494527@EXAMPLE.COM from keytab /workdir/hadoop-ozone/integration-test/target/test-dir/TestSecureOzoneCluster/scm.keytab javax.security.auth.login.LoginException: Unable to obtain password from user\r\n\r\n\tat org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1847)\r\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1215)\r\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1008)\r\n\tat org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:315)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.loginAsSCMUser(StorageContainerManager.java:508)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:254)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:212)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:600)\r\n\tat org.apache.hadoop.hdds.scm.HddsTestUtils.getScm(HddsTestUtils.java:91)\r\n\tat org.apache.hadoop.ozone.TestSecureOzoneCluster.testSCMSecurityProtocol(TestSecureOzoneCluster.java:299)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestSecureOzoneCluster"
   },
   {
      "_id": "13268908",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-11-17 09:07:04",
      "description": "S3 multipart upload is [failing|https://elek.github.io/ozone-ci-03/pr/pr-hdds-2501-b5dhd/acceptance/summary.html#s1-s11-s5] with [NPE|https://github.com/elek/ozone-ci-03/blob/ddbaf4dd92ee5f855fea3e84c59b702fb2dda663/pr/pr-hdds-2501-b5dhd/acceptance/docker-ozones3-ozones3-s3-scm.log#L740-L747].",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Multipart upload failing with NPE"
   },
   {
      "_id": "13268718",
      "assignee": "xyao",
      "components": [],
      "created": "2019-11-15 20:03:12",
      "description": "HDDS-2034 added async pipeline creation and report handling to SCM. The leader information is not properly populated as manifested in the test failures from TestSCMPipelineManager#testPipelineReport. This ticket is opened to fix it. cc: [~sammichen]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure RATIS leader info is properly updated with pipeline report. "
   },
   {
      "_id": "13268659",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-15 19:34:37",
      "description": "\r\n\r\nhttps://sonarcloud.io/project/issues?fileUuids=AW5md-HgKcVY8lQ4ZrfB&id=hadoop-ozone&resolved=false",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Code cleanup in EventQueue"
   },
   {
      "_id": "13268628",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-11-15 17:10:21",
      "description": "Link to the list of issues : https://sonarcloud.io/project/issues?fileUuids=AW5md-HdKcVY8lQ4ZrUn&id=hadoop-ozone&resolved=false",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix Sonar issues in OzoneManagerServiceProviderImpl"
   },
   {
      "_id": "13268625",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-15 17:01:31",
      "description": "Ozone Manager has a number of instances where the following sonar rule is flagged.\r\n\r\nbq. Using Collection.size() to test for emptiness works, but using Collection.isEmpty() makes the code more readable and can be more performant. The time complexity of any isEmpty() method implementation should be O(1) whereas some implementations of size() can be O(n).\r\n\r\nAn example of a flagged instance - https://sonarcloud.io/issues?myIssues=true&open=AW5md-W4KcVY8lQ4Zrv_&resolved=false\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use isEmpty() to check whether the collection is empty or not in Ozone Manager module"
   },
   {
      "_id": "13268599",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-15 15:30:04",
      "description": "Fix couple of [issues reported|https://sonarcloud.io/project/issues?directories=hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Freplication%2Chadoop-hdds%2Fcontainer-service%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Freplication&id=hadoop-ozone&resolved=false] in {{org.apache.hadoop.ozone.container.replication}} package.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Code cleanup in replication package"
   },
   {
      "_id": "13268575",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-15 13:23:30",
      "description": "We excluded the execution of TestMiniChaosOzoneCluster from the hadoop-ozone/dev-support/checks/integration.sh because it was not stable enough.\r\n\r\nUnfortunately this exclusion makes it impossible to use custom exclusion lists (-Dsurefire.excludesFile=....) as excludesFile can't be used if -Dtest=!... is already used.\r\n\r\nI propose to remove this exclusion to make it possible to use different exclusion for different runs (pr check, daily, etc.)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove the hard-coded exclusion of TestMiniChaosOzoneCluster"
   },
   {
      "_id": "13268567",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-15 12:50:16",
      "description": "During the review of HDDS-2470 I found that the full keyLocationInfo is added to the audit log for s3 operations:\r\n\r\n\u00a0\r\n{code:java}\r\n\r\n2019-11-15 12:34:18,538 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_KEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[]} | ret=SUCCESS |  2019-11-15 12:34:20,576 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_KEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[]} | ret=SUCCESS |  2019-11-15 12:34:20,626 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=ALLOCATE_BLOCK {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=THREE, keyLocationInfo=[], clientID=103141950132977668} | ret=SUCCESS |  2019-11-15 12:34:51,705 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=3813, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {  containerBlockID {    containerID: 1    localID: 103141950135009280  }  blockCommitSequenceId: 2}offset: 0length: 3813createVersion: 0pipeline {  members {    uuid: \"eefe54e8-5723-458e-9204-207c6b97c9b3\"    ipAddress: \"192.168.16.3\"    hostName: \"ozones3_datanode_1.ozones3_default\"    ports {      name: \"RATIS\"      value: 9858    }    ports {      name: \"STANDALONE\"      value: 9859    }    networkName: \"eefe54e8-5723-458e-9204-207c6b97c9b3\"    networkLocation: \"/default-rack\"  }  members {    uuid: \"ebf127d7-90a9-4f06-8fe5-a0c9c9adb743\"    ipAddress: \"192.168.16.7\"    hostName: \"ozones3_datanode_2.ozones3_default\"    ports {      name: \"RATIS\"      value: 9858    }    ports {      name: \"STANDALONE\"      value: 9859    }    networkName: \"ebf127d7-90a9-4f06-8fe5-a0c9c9adb743\"    networkLocation: \"/default-rack\"  }  members {    uuid: \"9979c326-4982-4a4c-b34e-e70c1d825f5f\"    ipAddress: \"192.168.16.6\"    hostName: \"ozones3_datanode_3.ozones3_default\"    ports {      name: \"RATIS\"      value: 9858    }    ports {      name: \"STANDALONE\"      value: 9859    }    networkName: \"9979c326-4982-4a4c-b34e-e70c1d825f5f\"    networkLocation: \"/default-rack\"  }  state: PIPELINE_OPEN  type: RATIS  factor: THREE  id {    id: \"69ba305b-fe89-4f5c-97cd-b894d5ee8f2b\"  }  leaderID: \"\"}], partNumber=1, partName=/s3b607288814a5da737a92fb067500396e/bucket1/key1103141950132977668} | ret=SUCCESS |  2019-11-15 12:42:10,883 | INFO  | OMAudit | user=hadoop | ip=192.168.16.2 | op=COMPLETE_MULTIPART_UPLOAD {volume=s3b607288814a5da737a92fb067500396e, bucket=bucket1, key=key1, dataSize=0, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[], multipartList=[partNumber: 1partName: \"/s3b607288814a5da737a92fb067500396e/bucket1/key1103141950132977668\"]} | ret=SUCCESS |  \r\n {code}\r\nIncluding the full keyLocation info in the audit log may cause some problems:\r\n * It makes the the audit log slower\r\n * It makes harder to parse the audit log\r\n\r\nI think it's better to separate the debug log (which can be provided easily with ozone insight tool) from the audit log. Therefore I suggest to remove the keyLocationInfo, replicationType, replicationFactor from the aduit log.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove keyAllocationInfo and replication info from the auditLog"
   },
   {
      "_id": "13268565",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-11-15 12:45:48",
      "description": "{{HddsUtils}} has 3 methods to calculate SCM address for various client types.  All have an unreachable {{if}} branch, because:\r\n\r\n# {{iterator().next()}} throws exception for empty list\r\n# {{getSCMAddresses}} never returns empty list anyway, it throws exception\r\n\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPX&open=AW5md-4qKcVY8lQ4ZsPX\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPY&open=AW5md-4qKcVY8lQ4ZsPY\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPW&open=AW5md-4qKcVY8lQ4ZsPW\r\n\r\nIdeally code duplication among these methods should be reduced, too.\r\n\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPU&open=AW5md-4qKcVY8lQ4ZsPU\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPT&open=AW5md-4qKcVY8lQ4ZsPT\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-4qKcVY8lQ4ZsPV&open=AW5md-4qKcVY8lQ4ZsPV\r\n\r\nComplete list of issues in the same file:\r\n\r\nhttps://sonarcloud.io/project/issues?fileUuids=AW5md-HhKcVY8lQ4Zrjn&id=hadoop-ozone&resolved=false",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix logic related to SCM address calculation in HddsUtils"
   },
   {
      "_id": "13268531",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-15 09:41:27",
      "description": "{{FlushOptions}} should be closed after use.\r\n\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ4&open=AW5md-zwKcVY8lQ4ZsJ4\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ5&open=AW5md-zwKcVY8lQ4ZsJ5\r\n\r\nSonar also reported 15 further issues in the same file:\r\n\r\nhttps://sonarcloud.io/project/issues?fileUuids=AW5md-HgKcVY8lQ4Zrga&id=hadoop-ozone&resolved=false",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Close FlushOptions in RDBStore"
   },
   {
      "_id": "13268436",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2019-11-14 22:27:27",
      "description": "Today Ozone volume create/list ACL check are not sent to authorization plugins. This cause problem when authorization plugin is enabled. Admin still need to modify ozone-site.xml to change ozone.administrators to configure admin to create volume\r\n\r\n\u00a0\r\n\r\nThis ticket is opened to have a consistent ACL check for all Ozone resources requests including admin request like volume create. This way, the admin defined by the authorization plugin can be honored during volume provision without restart ozone services.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Delegate Ozone volume create/list ACL check to authorizer plugin"
   },
   {
      "_id": "13268398",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-11-14 18:41:46",
      "description": "GrpcXceiverService has log messages with too few arguments for placeholders.  Only one of them is flagged by Sonar, but all seem to have the same problem.\r\n\r\nhttps://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-69KcVY8lQ4ZsRZ&open=AW5md-69KcVY8lQ4ZsRZ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Not enough arguments for log messages in GrpcXceiverService"
   },
   {
      "_id": "13268397",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-14 18:38:26",
      "description": "* ContainerDataYaml: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-6IKcVY8lQ4ZsQU&open=AW5md-6IKcVY8lQ4ZsQU\r\n* OmUtils: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-hdKcVY8lQ4Zr76&open=AW5md-hdKcVY8lQ4Zr76",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure streams are closed"
   },
   {
      "_id": "13268393",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2019-11-14 18:14:11",
      "description": "Disable XML external entity processing in\r\n\r\n* NodeSchemaLoader: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-2nKcVY8lQ4ZsNm&open=AW5md-2nKcVY8lQ4ZsNm\r\n* ConfigFileAppender:\r\n** https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_uKcVY8lQ4ZsVY&open=AW5md-_uKcVY8lQ4ZsVY\r\n** https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-_uKcVY8lQ4ZsVZ&open=AW5md-_uKcVY8lQ4ZsVZ\r\n* MultiDeleteRequestUnmarshaller: https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-kDKcVY8lQ4Zr-N&open=AW5md-kDKcVY8lQ4Zr-N",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable XML external entity processing"
   },
   {
      "_id": "13268331",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-14 14:30:16",
      "description": "HDDS-2400 introduced a github actions workflow for each \"push\" event. It turned out that pushing to a forked repository doesn't trigger this event even if it's part of a PR.\r\n\r\n\u00a0\r\n\r\nWe need to enable the execution for pull_request events:\r\n\r\nReferences:\r\n\r\n\u00a0[https://github.community/t5/GitHub-Actions/Run-a-GitHub-action-on-pull-request-for-PR-opened-from-a-forked/m-p/31147#M690]\r\n\r\n[https://help.github.com/en/actions/automating-your-workflow-with-github-actions/events-that-trigger-workflows#pull-request-events-for-forked-repositories]\r\n{noformat}\r\nNote: By default, a workflow only runs when a pull_request's activity type is opened, synchronize, or reopened. To trigger workflows for more activity types, use the types keyword.{noformat}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable github actions for pull requests"
   },
   {
      "_id": "13268251",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-11-14 09:33:06",
      "description": "Ensure various streams are closed in {{TarContainerPacker}}:\r\n\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUH&open=AW5md-9bKcVY8lQ4ZsUH\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUL&open=AW5md-9bKcVY8lQ4ZsUL\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUK&open=AW5md-9bKcVY8lQ4ZsUK\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUJ&open=AW5md-9bKcVY8lQ4ZsUJ\r\n* https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-9bKcVY8lQ4ZsUI&open=AW5md-9bKcVY8lQ4ZsUI",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Close streams in TarContainerPacker"
   },
   {
      "_id": "13268160",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-14 00:21:47",
      "description": "In OM in non-HA case, the ratisTransactionLogIndex is generated by OmProtocolServersideTranslatorPB.java. And in OM non-HA validateAndUpdateCache is called from multipleHandler threads. So think of a case where one thread which has an index - 10 has added to doubleBuffer. (0-9 still have not added). DoubleBuffer flush thread flushes and call cleanup. (So, now cleanup will go and cleanup all cache entries with less than 10 epoch) This should not have cleanup those which might have put in to cache later and which are in process of flush to DB. This will cause inconsitency for few OM requests.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nExample:\r\n\r\n4 threads Committing 4 parts.\r\n\r\n1st thread - part 1 - ratis Index - 3\r\n\r\n2nd thread - part 2 - ratis index - 2\r\n\r\n3rd thread - part3 - ratis index - 1\r\n\r\n\u00a0\r\n\r\nFirst thread got lock, and put in to doubleBuffer and cache with OmMultipartInfo (with part1). And cleanup is called to cleanup all entries in cache with less than 3. In the mean time 2nd thread and 1st thread put 2,3 parts in to OmMultipartInfo in to Cache and doubleBuffer. But first thread might cleanup those entries, as it is called with index 3 for cleanup.\r\n\r\n\u00a0\r\n\r\nNow when the 4th part upload came -> when it is commit Multipart upload when it gets multipartinfo it get Only part1 in OmMultipartInfo, as the OmMultipartInfo (with 1,2,3 is still in process of committing to DB). So now after 4th part upload is complete in DB and Cache we will have 1,4 parts only. We will miss part2,3 information.\r\n\r\n\u00a0\r\n\r\nSo for non-HA case cleanup will be called with list of epochs that need to be cleanedup.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TableCache cleanup issue for OM non-HA"
   },
   {
      "_id": "13268149",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-11-13 23:00:27",
      "description": "{{ContainerMetadataScanner}} thread should call {{ContainerMetadataScrubberMetrics#unregister}} before exiting.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Unregister ContainerMetadataScrubberMetrics on thread exit"
   },
   {
      "_id": "13268144",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-11-13 22:11:59",
      "description": "If RaftCleintReply encounters an exception other than\u00a0NotLeaderException,\u00a0NotReplicatedException,\u00a0StateMachineException or\u00a0LeaderNotReady, then it sets success to false but there is no exception set. This causes a Precondition check failure in XceiverClientRatis which expects that there should be an exception if success=false.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove OzoneClient exception Precondition check"
   },
   {
      "_id": "13268142",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-11-13 21:57:18",
      "description": "sonarcloud.io has flagged a number of code reliability issues in Ozone recon (https://sonarcloud.io/code?id=hadoop-ozone&selected=hadoop-ozone%3Ahadoop-ozone%2Frecon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Frecon).\r\n\r\nFollowing issues will be triaged / fixed.\r\n* Double Brace Initialization should not be used\r\n* Resources should be closed\r\n* InterruptedException should not be ignored\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix code reliability issues found by Sonar in Ozone Recon module."
   },
   {
      "_id": "13268141",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-11-13 21:50:41",
      "description": "Link to the sonar issue flag - https://sonarcloud.io/project/issues?id=hadoop-ozone&issues=AW5md-zwKcVY8lQ4ZsJ4&open=AW5md-zwKcVY8lQ4ZsJ4. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available",
         "sonar"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use try-with-resources while creating FlushOptions in RDBStore."
   },
   {
      "_id": "13268130",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-11-13 20:51:58",
      "description": "When InvalidPart error occurs, the exception message does not have any information about partName and partNumber, it will be good to have this information.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve exception message for CompleteMultipartUpload"
   },
   {
      "_id": "13268112",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-11-13 19:02:12",
      "description": "Right now when complete Multipart Upload is not printing partName and\u00a0 partNumber into the audit log. This will help in analyzing audit logs for MPU.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n2019-11-13 15:14:10,191 | INFO\u00a0 | OMAudit | user=root | ip=xx.xx.xx.xx | op=COMMIT_MULTIPART_UPLOAD_PARTKEY {volume=s325d55ad283aa400af464c76d713c07ad, bucket=ozone-test, key=plc_1570850798896_2991, dataSize=5242880, replicationType=RATIS, replicationFactor=ONE, keyLocationInfo=[blockID {\r\n\r\n\u00a0 containerBlockID\r\n\r\n{ \u00a0 \u00a0 containerID: 2 \u00a0 \u00a0 localID: 103129366531867089 \u00a0 }\r\n\r\n\u00a0 blockCommitSequenceId: 4978\r\n\r\n}\r\n\r\noffset: 0\r\n\r\nlength: 5242880\r\n\r\ncreateVersion: 0\r\n\r\npipeline {\r\n\r\n\u00a0 leaderID: \"\"\r\n\r\n\u00a0 members {\r\n\r\n\u00a0 \u00a0 uuid: \"5d03aed5-cfb3-4689-b168-0c9a94316551\"\r\n\r\n\u00a0 \u00a0 ipAddress: \"xx.xx.xx.xx\"\r\n\r\n\u00a0 \u00a0 hostName: \"xx.xx.xx.xx\"\r\n\r\n\u00a0 \u00a0 ports\r\n\r\n{ \u00a0 \u00a0 \u00a0 name: \"RATIS\" \u00a0 \u00a0 \u00a0 value: 9858 \u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 ports\r\n\r\n{ \u00a0 \u00a0 \u00a0 name: \"STANDALONE\" \u00a0 \u00a0 \u00a0 value: 9859 \u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 networkName: \"5d03aed5-cfb3-4689-b168-0c9a94316551\"\r\n\r\n\u00a0 \u00a0 networkLocation: \"/default-rack\"\r\n\r\n\u00a0 }\r\n\r\n\u00a0 members {\r\n\r\n\u00a0 \u00a0 uuid: \"a71462ae-7865-4ed5-b84e-60616df60a0d\"\r\n\r\n\u00a0 \u00a0 ipAddress: \"9.134.51.25\"\r\n\r\n\u00a0 \u00a0 hostName: \"9.134.51.25\"\r\n\r\n\u00a0 \u00a0 ports\r\n\r\n{ \u00a0 \u00a0 \u00a0 name: \"RATIS\" \u00a0 \u00a0 \u00a0 value: 9858 \u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 ports\r\n\r\n{ \u00a0 \u00a0 \u00a0 name: \"STANDALONE\" \u00a0 \u00a0 \u00a0 value: 9859 \u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 networkName: \"a71462ae-7865-4ed5-b84e-60616df60a0d\"\r\n\r\n\u00a0 \u00a0 networkLocation: \"/default-rack\"\r\n\r\n\u00a0 }\r\n\r\n\u00a0 members {\r\n\r\n\u00a0 \u00a0 uuid: \"79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03\"\r\n\r\n\u00a0 \u00a0 ipAddress: \"9.134.51.215\"\r\n\r\n\u00a0 \u00a0 hostName: \"9.134.51.215\"\r\n\r\n\u00a0 \u00a0 ports\r\n\r\n{ \u00a0 \u00a0 \u00a0 name: \"RATIS\" \u00a0 \u00a0 \u00a0 value: 9858 \u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 ports\r\n\r\n{ \u00a0 \u00a0 \u00a0 name: \"STANDALONE\" \u00a0 \u00a0 \u00a0 value: 9859 \u00a0 \u00a0 }\r\n\r\n\u00a0 \u00a0 networkName: \"79bf7bdf-ed29-49d4-bf7c-e88fdbd2ce03\"\r\n\r\n\u00a0 \u00a0 networkLocation: \"/default-rack\"\r\n\r\n\u00a0 }\r\n\r\n\u00a0 state: PIPELINE_OPEN\r\n\r\n\u00a0 type: RATIS\r\n\r\n\u00a0 factor: THREE\r\n\r\n\u00a0 id\r\n\r\n{ \u00a0 \u00a0 id: \"ec6b06c5-193f-4c30-879b-5a12284dc4f8\" \u00a0 }\r\n\r\n}\r\n\r\n]} | ret=SUCCESS |",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add partName, partNumber for CommitMultipartUpload"
   },
   {
      "_id": "13268103",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-11-13 18:23:24",
      "description": "Ozone RPC client should not change input map from client while creating keys.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid changing client-side key metadata"
   },
   {
      "_id": "13267996",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-11-13 11:26:08",
      "description": "Freon validators read each item to be validated completely into a {{byte[]}} buffer.  This allows timing only the read (and buffer allocation), but not the subsequent digest calculation.  However, it also means that memory required for running the validators is proportional to key size.\r\n\r\nI propose to add a command-line flag to allow calculating the digest while reading the input stream.  This changes timing results a bit, since values will include the time required for digest calculation.  On the other hand, Freon will be able to validate huge keys with limited memory.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allow running Freon validators with limited memory"
   },
   {
      "_id": "13267846",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-11-12 22:09:51",
      "description": "{{ChunkUtils}} calls {{FileChannel#open(Path, OpenOption...)}}.  Vararg array elements are then added to a new {{HashSet}} to call {{FileChannel#open(Path, Set<? extends OpenOption>, FileAttribute<?>...)}}.  We can call the latter directly instead.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid unnecessary allocations for FileChannel.open call"
   },
   {
      "_id": "13267834",
      "assignee": "xyao",
      "components": [],
      "created": "2019-11-12 20:57:55",
      "description": "OzoneManagerProtocolClientSideTranslatorPB.java Line 766-772 has multiple impl.getServiceInfo() which can be reduced by adding a local variable.\u00a0\r\n{code:java}\r\n\u00a0\r\nresp.addAllServiceInfo(impl.getServiceInfo().getServiceInfoList().stream()\r\n .map(ServiceInfo::getProtobuf)\r\n .collect(Collectors.toList()));\r\nif (impl.getServiceInfo().getCaCertificate() != null) {\r\n resp.setCaCertificate(impl.getServiceInfo().getCaCertificate());\u00a0{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Reduce unnecessary getServiceInfo calls"
   },
   {
      "_id": "13267744",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-11-12 15:10:17",
      "description": "During a k8s based test I found a lot of log message like:\r\n{code:java}\r\n2019-11-12 14:27:13 WARN  ChunkManagerImpl:209 - Duplicate write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='A9UrLxiEUN_testdata_chunk_4465025, offset=0, len=1024} {code}\r\nI was very surprised as at ChunkManagerImpl:209 there was no similar lines.\r\n\r\nIt turned out that it's logged by ChunkUtils but it's used the logger of ChunkManagerImpl.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Logging by ChunkUtils is misleading"
   },
   {
      "_id": "13267415",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-11-11 07:35:52",
      "description": "{{ChecksumData}} is initially created with empty list of checksums, then it is updated with computed checksums, copying the list.  The computed list can be set directly.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid list copy in ChecksumData"
   },
   {
      "_id": "13267351",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-11-10 13:38:16",
      "description": "{{ozone-om-ha}} and {{ozonescripts}} build images based on {{apache/ozone-runner}}.\r\n\r\nProblem: They do not specify base image versions, so it defaults to {{latest}}.  If a new {{ozone-runner}} image is published on Docker Hub, developers needs to manually pull the {{latest}} image for it to take effect on these derived images.\r\n\r\nSolution: Use explicit base image version (defined by {{OZONE_RUNNER_VERSION}} variable in {{.env}} file.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Add explicit base image version for images derived from ozone-runner"
   },
   {
      "_id": "13267214",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-11-09 00:01:38",
      "description": "In one CI run, testOMHA.robot failed because robot framework SSH commands failed. This Jira aims to verify that the command execution succeeds.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve OM HA robot tests"
   },
   {
      "_id": "13267171",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-08 18:49:12",
      "description": "Avoid eagerly evaluating error messages of preconditions (similarly to HDDS-2318, but there may be other occurrences of the same issue).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use lazy string evaluation in preconditions"
   },
   {
      "_id": "13267078",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-08 10:49:49",
      "description": "{{BlockData#toString}} uses {{ToStringBuilder}} for ease of implementation.  This has a few problems:\r\n\r\n# {{ToStringBuilder}} uses {{StringBuffer}}, which is synchronized\r\n# the default buffer is 512 bytes, more than needed here\r\n# {{BlockID}} and {{ContainerBlockID}} both use another {{StringBuilder}} or {{StringBuffer}} for their {{toString}} implementation, leading to several allocations and copies\r\n\r\nThe flame graph shows that {{BlockData#toString}} may be responsible for 1.5% of total allocations while putting keys.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Replace ToStringBuilder in BlockData"
   },
   {
      "_id": "13267063",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-11-08 10:00:30",
      "description": "During the review of HDDS-2427 we found that some of the server side dependencies (container-service, framework) are added to the ozonefs library jars. Server side dependencies should be excluded from the client side to make the client safer and the build faster.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove server side dependencies from ozonefs jar files"
   },
   {
      "_id": "13266942",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-11-07 22:40:50",
      "description": "This has caused issue for DN UI loading.\r\n\r\nhadoop-ozone-filesystem-lib-current-xx.jar is in the classpath which accidentally loaded Ozone datanode web application instead of Hadoop datanode application. This leads to the reported error.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Exclude webapps from hadoop-ozone-filesystem-lib-current uber jar"
   },
   {
      "_id": "13266856",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-07 15:39:56",
      "description": "There is a config setting to enable/disable OpenTracing-based distributed tracing in Ozone ({{hdds.tracing.enabled}}).  However, setting it to false does not prevent tracer initialization, which causes unnecessary object allocations.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Completely disable tracer if hdds.tracing.enabled=false"
   },
   {
      "_id": "13266796",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-07 10:26:56",
      "description": "HDDS-1469 introduced a new method to handle configuration. Configuration can be injected directly to java objects which makes all the java constants unnecessary.\r\n\r\n\u00a0\r\n\r\nAlmost.\r\n\r\n\u00a0\r\n\r\nTo read the configuration it's enough to have an annotated java object. For example:\r\n\r\n\u00a0\r\n{code:java}\r\n@ConfigGroup(prefix = \"hdds.scm\")\r\npublic class ScmConfig {\r\n  private String principal;\r\n  private String keytab;  @Config(key = \"kerberos.principal\",\r\n        type = ConfigType.STRING,\r\n        defaultValue = \"\",\r\n        tags = { ConfigTag.SECURITY, ConfigTag.OZONE },\r\n        description = \"This Kerberos principal is used by the SCM service.\"\r\n  )\r\n  public void setKerberosPrincipal(String kerberosPrincipal) {\r\n    this.principal = kerberosPrincipal; {code}\r\nAnd the configuration can be set in ozone-site.xml\r\n\r\nUnfortunately during the unit testing we need to inject the configuration variables programmatically which requires a String constant:\r\n{code:java}\r\nconfiguration.set(ScmConfig.ConfigStrings.HDDS_SCM_KERBEROS_PRINCIPAL_KEY,      \r\n            \"scm/\" + host + \"@\" + realm); {code}\r\nI propose to implement a simple setter in the OzoneConfiguration which may help to set configuration based on an annotated configuration object instance:\r\n{code:java}\r\nOzoneConfiguration conf = new OzoneConfiguration();\r\n\r\nSCMHTTPServerConfig httpConfig = SCMHTTPServerConfig(principal1,...);\r\n\r\nconf.setFromObject(httpConfig){code}\r\nThis is the opposite direction of the existing OzoneConfiguration.getObject() and can be implemented with a similar approach.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Set configuration variables from annotated java objects"
   },
   {
      "_id": "13266786",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-07 09:58:11",
      "description": ".asf.yaml helps to set different parameters on github repositories without admin privileges:\r\n\r\n[https://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories]\r\n\r\nThis basic .asf.yaml defines description/url/topics and the allowed merge buttons.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Define description/topics/merge strategy for the github repository with .asf.yaml"
   },
   {
      "_id": "13266764",
      "assignee": "elek",
      "components": [],
      "created": "2019-11-07 08:38:00",
      "description": "The profiler [servlet|https://github.com/elek/hadoop-ozone/blob/master/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java] (which helps to run java profiler in the background and publishes the result on the web interface) requires privileged docker containers.\r\n\r\n\u00a0\r\n\r\nThis flag is missing from the ozoneperf docker-compose cluster (which is designed to run performance tests).\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozoneperf docker cluster should use privileged containers"
   },
   {
      "_id": "13266490",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-11-06 05:39:38",
      "description": "When reading from a pipeline, client should not care if some datanode could not service the request, as long as the pipeline as a whole is OK.  The [log message|https://github.com/apache/hadoop-ozone/blob/2529cee1a7dd27c51cb9aed0dc57af283ff24e26/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java#L303-L304] indicating node failure was [increased to error level|https://github.com/apache/hadoop-ozone/commit/a79dc4609a975d46a3e051ad6904fb1eb40705ee#diff-b9b6f3ccb12829d90886e041d11395b1R288] in HDDS-1780.  This task proposes to change it back to debug.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Reduce log level of per-node failure in XceiverClientGrpc"
   },
   {
      "_id": "13266204",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-11-04 22:20:44",
      "description": "{{int2ByteString}} implementations (currently duplicated in [RatisHelper|https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/ratis/RatisHelper.java#L280-L289] and [Checksum|https://github.com/apache/hadoop-ozone/blob/6b2cda125b3647870ef5b01cf64e3b3e4cdc55db/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/Checksum.java#L64-L73], but the first one is being removed in HDDS-2375) result in unnecessary byte array allocations:\r\n\r\n# {{ByteString.Output}} creates 128-byte buffer by default, which is too large for writing a single int\r\n# {{DataOutputStream}} allocates an [extra 8-byte array|https://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/io/DataOutputStream.java#l204], used only for writing longs\r\n# {{ByteString.Output}} also creates 10-element array for {{flushedBuffers}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "int2ByteString unnecessary byte array allocation"
   },
   {
      "_id": "13266096",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-11-04 11:25:13",
      "description": "Current PR checks are executed in a private branch based on the scripts in [https://github.com/elek/argo-ozone]\r\n\r\nbut the results are stored in a public repositories:\r\n\r\n[https://github.com/elek/ozone-ci-q4|https://github.com/elek/ozone-ci-q3]\r\n\r\n[https://github.com/elek/ozone-ci-03]\r\n\r\n\u00a0\r\n\r\nAs we discussed during the community calls, it would be great to use github actions (or any other cloud based build) to make all the build definitions more accessible for the community.\r\n\r\n[~vivekratnavel] checked CircleCI which has better reporting capabilities. But INFRA has concerns about the permission model of circle-ci:\r\n{quote}it is highly unlikley we will allow a bot to be able to commit code (whether or not that is the intention, allowing circle-ci will make this possible, and is a complete no)\r\n{quote}\r\nSee:\r\n\r\nhttps://issues.apache.org/jira/browse/INFRA-18131\r\n\r\n[https://lists.apache.org/thread.html/af52e2a3e865c01596d46374e8b294f2740587dbd59d85e132429b6c@%3Cbuilds.apache.org%3E]\r\n\r\n\u00a0\r\n\r\nFortunately we have a clear contract. Or build scripts are stored under _hadoop-ozone/dev-support/checks_ (return code show the result, details are printed out to the console output). It's very easy to experiment with different build systems.\r\n\r\n\u00a0\r\n\r\nGithub action seems to be an obvious choice: it's integrated well with GitHub and it has more generous resource limitations.\r\n\r\n\u00a0\r\n\r\nWith this Jira I propose to enable github actions based PR checks for a few tests (author, rat, unit, acceptance, checkstyle, findbugs) as an experiment.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable github actions based builds for Ozone"
   },
   {
      "_id": "13265830",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-11-01 20:20:43",
      "description": "MiniOzoneChaoasCluster.java for setting log level it uses LogUtils from ratis-common. But this is removed from LogUtils as part of Ratis-508.\r\n\r\nWe can avoid depending on ratis for this, and use GenericTestUtils from hadoop-common test.\r\n\r\nLogUtils.setLogLevel(GrpcClientProtocolClient.LOG, Level.WARN);",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove usage of LogUtils class from ratis-common"
   },
   {
      "_id": "13265692",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-11-01 04:29:51",
      "description": "After DoubleBuffer flushes, we call cleanup cache to cleanup tables cache.\r\n\r\nFor few tables cleanup of cache is missed:\r\n # PrefixTable\r\n # S3SecretTable\r\n # DelegationTable",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix calling cleanup for few missing tables in OM"
   },
   {
      "_id": "13265654",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-10-31 22:40:46",
      "description": "# When uploaded 2 parts, and when complete upload 1 part no error\r\n # During complete multipart upload name/part number not matching with uploaded part and part number then InvalidPart error\r\n # When parts are not specified in sorted order InvalidPartOrder\r\n # During\u00a0complete multipart upload when no uploaded parts, and we specify some parts then also InvalidPart\r\n # Uploaded parts 1,2,3 and during complete we can do upload 1,3 (No error)\r\n # When part 3 uploaded, complete with part 3 can be done",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Ozone S3 completeMPU to match with aws s3 behavior."
   },
   {
      "_id": "13265648",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-31 22:11:00",
      "description": "Steps to reproduce:\r\naws s3api --endpoint http://localhost:9878 create-bucket --bucket ozone_test\r\n\r\naws s3api --endpoint http://localhost:9878 put-object --bucket ozone_test --key ozone-site.xml --body /etc/hadoop/conf/ozone-site.xml\r\n\r\nS3 gateway throws a warning:\r\n{code:java}\r\njavax.servlet.ServletException: javax.servlet.ServletException: java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : _\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:139)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: javax.servlet.ServletException: java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : _\r\n\tat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:432)\r\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:370)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:389)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:342)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:229)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1780)\r\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1628)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)\r\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1767)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n\t... 13 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone S3 Gateway allows bucket name with underscore to be created but throws an error during put key operation"
   },
   {
      "_id": "13265263",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-30 10:35:40",
      "description": "During large file writes, it ends up writing {{16 MB}} chunks.  \r\n\r\nhttps://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java#L691\r\n\r\nIn large clusters, 100s of clients may connect to DN. In such cases, depending on the incoming write workload mem load on DN can increase significantly. \r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Large chunks during write can have memory pressure on DN with multiple clients"
   },
   {
      "_id": "13265237",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-30 09:15:53",
      "description": "When writing 100-200 MB files with multiple threads, observed lots of {{[file::exists(])}} checks.\r\n\r\nFor every 16 MB chunk, it ends up checking whether {{chunksLoc}} directory exists or not. (ref: [https://github.com/apache/hadoop-ozone/blob/master/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L239])\r\n\r\nAlso, this check ({{ChunkUtils.getChunkFile}}) happens from 2 places.\r\n\r\n1.org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk\r\n\r\n2.org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$applyTransaction\r\n\r\nNote that these are folders and not actual chunk filenames. It would be helpful to reduce this check, if we track create/delete of these folders.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Consider reducing number of file::exists() calls during write operation"
   },
   {
      "_id": "13265138",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-29 22:45:46",
      "description": "Created based on comment from [~chinseone]\u00a0in HDDS-2356\r\n\r\nhttps://issues.apache.org/jira/browse/HDDS-2356?focusedCommentId=16960796&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16960796\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "In ExcludeList, add if not exist only"
   },
   {
      "_id": "13265109",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-29 18:58:25",
      "description": "Currently, when OM creates a file/directory, it checks the absence of all prefix paths of the key in its RocksDB. Since we don't care about the deserialization of the actual value, we should use the isExist API added in org.apache.hadoop.hdds.utils.db.Table which internally uses the more performant keyMayExist API of RocksDB.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use the Table.isExist API instead of get() call while checking for presence of key."
   },
   {
      "_id": "13265106",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-29 18:41:07",
      "description": "Exception trace after writing around 800,000 keys.\r\n\r\n\r\n{code}\r\n2019-10-29 11:15:15,131 ERROR org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer: Terminating with exit status 1: During flush to DB encountered err\r\nor in OMDoubleBuffer flush thread OMDoubleBufferFlushThread\r\njava.io.IOException: Unable to write the batch.\r\n        at org.apache.hadoop.hdds.utils.db.RDBBatchOperation.commit(RDBBatchOperation.java:48)\r\n        at org.apache.hadoop.hdds.utils.db.RDBStore.commitBatchOperation(RDBStore.java:240)\r\n        at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:146)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.rocksdb.RocksDBException: unknown WriteBatch tag\r\n        at org.rocksdb.RocksDB.write0(Native Method)\r\n        at org.rocksdb.RocksDB.write(RocksDB.java:1421)\r\n        at org.apache.hadoop.hdds.utils.db.RDBBatchOperation.commit(RDBBatchOperation.java:46)\r\n        ... 3 more\r\n{code}\r\n\r\nAssigning to [~bharat] since he has already started work on this. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "OM terminates with RocksDB error while continuously writing keys."
   },
   {
      "_id": "13264797",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-28 10:53:27",
      "description": "Found it on a k8s based test cluster using a simple 3 node cluster and HDDS-2327 freon test. After a while the StateMachine become unhealthy after this error:\r\n{code:java}\r\ndatanode-0 datanode java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: java.nio.file.NoSuchFileException: /data/storage/hdds/2a77fab9-9dc5-4f73-9501-b5347ac6145c/current/containerDir0/1/chunks/gGYYgiTTeg_testdata_chunk_13931.tmp.2.20830 {code}\r\nCan be reproduced.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Datanode pipeline is failing with NoSuchFileException"
   },
   {
      "_id": "13264785",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-28 10:04:06",
      "description": "Ozone components printing out the current version during the startup:\r\n\r\n\u00a0\r\n{code:java}\r\nSTARTUP_MSG: Starting StorageContainerManager\r\nSTARTUP_MSG:   host = om/10.8.0.145\r\nSTARTUP_MSG:   args = []\r\nSTARTUP_MSG:   version = 3.2.0\r\nSTARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e97acb3bd8f3befd27418996fa5d4b50bf2e17bf; compiled by 'sunilg' on 2019-01-{code}\r\nBut as it's visible the build / compiled information is about hadoop not about hadoop-ozone.\r\n\r\n(And personally I prefer to use a github compatible url instead of the SVN style -r. Something like:\r\n{code:java}\r\nSTARTUP_MSG: build =  https://github.com/apache/hadoop-ozone/commit/8541c5694efebb58f53cf4665d3e4e6e4a12845c ; compiled by '....' on ...{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Print out the ozone version during the startup instead of hadoop version"
   },
   {
      "_id": "13264617",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-26 17:33:52",
      "description": "{noformat}\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.479 s <<< FAILURE! - in org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse\r\ntestDoubleBufferWithDummyResponse(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse)  Time elapsed: 1.404 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n...\r\n\tat org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.testDoubleBufferWithDummyResponse(TestOzoneManagerDoubleBufferWithDummyResponse.java:116)\r\n{noformat}\r\n\r\n* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2345-jsf2s/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.txt\u00a0\r\n* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2272-bfh6s/unit/hadoop-ozone/ozone-manager/org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithDummyResponse.txt\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerDoubleBufferWithDummyResponse failing intermittently"
   },
   {
      "_id": "13264412",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-25 05:20:21",
      "description": "TestRatisPipelineProvider#testCreatePipelinesDnExclude is flaky, failing in CI intermittently:\r\n\r\n* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2360-9pxww/integration/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt\r\n* https://github.com/elek/ozone-ci-03/blob/master/pr/pr-hdds-2352-cxhw9/integration/hadoop-ozone/integration-test/org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.txt",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "TestRatisPipelineProvider#testCreatePipelinesDnExclude is flaky"
   },
   {
      "_id": "13264409",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-25 04:54:27",
      "description": "Add a OM metrics to find the false positive rate for the keyMayExist.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a OM metrics to find the false positive rate for the keyMayExist"
   },
   {
      "_id": "13264316",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-24 19:40:50",
      "description": "Update Ratis dependency version to snapshot [d6d58d0|https://github.com/apache/incubator-ratis/commit/d6d58d0], to fix memory issues (RATIS-726, RATIS-728).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update Ratis snapshot to d6d58d0"
   },
   {
      "_id": "13264247",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-10-24 12:15:01",
      "description": "During Hive testing we found the following exception:\r\n\r\n{code}\r\nTaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1569246922012_0214_1_03_000000_3:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: error iterating\r\n    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296)\r\n    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250)\r\n    at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n    at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n    at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n    at java.security.AccessController.doPrivileged(Native Method)\r\n    at javax.security.auth.Subject.doAs(Subject.java:422)\r\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876)\r\n    at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n    at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n    at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n    at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)\r\n    at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)\r\n    at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n    at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: error iterating\r\n    at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:80)\r\n    at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:426)\r\n    at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:267)\r\n    ... 16 more\r\nCaused by: java.io.IOException: java.io.IOException: error iterating\r\n    at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\r\n    at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\r\n    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:366)\r\n    at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:79)\r\n    at org.apache.hadoop.hive.ql.io.HiveRecordReader.doNext(HiveRecordReader.java:33)\r\n    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)\r\n    at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.next(TezGroupedSplitsInputFormat.java:151)\r\n    at org.apache.tez.mapreduce.lib.MRReaderMapred.next(MRReaderMapred.java:116)\r\n    at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)\r\n    ... 18 more\r\nCaused by: java.io.IOException: error iterating\r\n    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:835)\r\n    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:74)\r\n    at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:361)\r\n    ... 24 more\r\nCaused by: java.io.IOException: Error reading file: o3fs://hive.warehouse.vc0136.halxg.cloudera.com:9862/data/inventory/delta_0000001_0000001_0000/bucket_00000\r\n    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1283)\r\n    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.nextBatch(RecordReaderImpl.java:156)\r\n    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$1.next(VectorizedOrcAcidRowBatchReader.java:150)\r\n    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader$1.next(VectorizedOrcAcidRowBatchReader.java:146)\r\n    at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.next(VectorizedOrcAcidRowBatchReader.java:831)\r\n    ... 26 more\r\nCaused by: java.io.IOException: Inconsistent read for blockID=conID: 2 locID: 102851451236759576 bcsId: 14608 length=26398272 numBytesRead=6084153\r\n    at org.apache.hadoop.ozone.client.io.KeyInputStream.read(KeyInputStream.java:176)\r\n    at org.apache.hadoop.fs.ozone.OzoneFSInputStream.read(OzoneFSInputStream.java:52)\r\n    at org.apache.hadoop.fs.FSInputStream.read(FSInputStream.java:75)\r\n    at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)\r\n    at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:112)\r\n    at org.apache.orc.impl.RecordReaderUtils.readDiskRanges(RecordReaderUtils.java:557)\r\n    at org.apache.orc.impl.RecordReaderUtils$DefaultDataReader.readFileData(RecordReaderUtils.java:276)\r\n    at org.apache.orc.impl.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:1189)\r\n    at org.apache.orc.impl.RecordReaderImpl.readStripe(RecordReaderImpl.java:1057)\r\n    at org.apache.orc.impl.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:1208)\r\n    at org.apache.orc.impl.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1243)\r\n    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1279)\r\n    ... 30 more\r\n{code}\r\n\r\nEvaluating the code path, the following is the issue:\r\ngiven a file with more data than 2 blocks\r\nwhen there are random seeks in the file to the end then to the beginning\r\nthen the read fails with the final cause of the exception above.\r\n\r\n[~shashikant] has a solution already for this issue, which we have successfully tested internally with Hive, I am assigning this JIRA to him to post the PR.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Seeking randomly in a key with more than 2 blocks of data leads to inconsistent reads"
   },
   {
      "_id": "13264168",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-10-24 03:28:20",
      "description": "Make them Debug logs.\r\n\r\n2019-10-24 03:17:43,087 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \\{datanodes = [], containerIds = [], pipelineIds = []}\r\n\r\nscm_1 \u00a0 \u00a0 \u00a0 | 2019-10-24 03:17:43,088 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \\{datanodes = [], containerIds = [], pipelineIds = []}\r\n\r\nscm_1 \u00a0 \u00a0 \u00a0 | 2019-10-24 03:17:43,089 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \\{datanodes = [], containerIds = [], pipelineIds = []}\r\n\r\nscm_1 \u00a0 \u00a0 \u00a0 | 2019-10-24 03:17:43,093 INFO server.SCMBlockProtocolServer: Allocating 1 blocks of size 268435456, with ExcludeList \\{datanodes = [], containerIds = [], pipelineIds = []}\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "SCM log is full of AllocateBlock logs"
   },
   {
      "_id": "13264100",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-23 18:38:25",
      "description": "New Freon generators create volume and bucket if necessary.  This does not work in secure cluster for volume, but works for bucket:\r\n\r\n{code}\r\n$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozonesecure\r\n$ docker-compose exec scm bash\r\n$ kinit -kt /etc/security/keytabs/testuser.keytab testuser/scm@EXAMPLE.COM\r\n$ ozone freon ockg -n 1\r\n...\r\nCheck access operation failed for volume:vol1\r\n...\r\nSuccessful executions: 0\r\n$ ozone sh volume create vol1\r\n$ ozone freon ockg -n 1\r\n...\r\n2019-10-23 18:30:27,279 [main] INFO       - Creating Bucket: vol1/bucket1, with Versioning false and Storage Type set to DISK and Encryption set to false\r\n...\r\nSuccessful executions: 1\r\n{code}\r\n\r\nThe problem is that {{VOLUME_NOT_FOUND}} result is lost during ACL check, and client gets {{INTERNAL_ERROR}} instead.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Client gets internal error instead of volume not found in secure cluster"
   },
   {
      "_id": "13263742",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-10-22 08:24:58",
      "description": "Replication manager's configuration for its own interval:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/eb1d77e3206fab1a4ac0573507c9deb2b56b9ea1/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java#L808-L822}\r\n    @Config(key = \"thread.interval\",\r\n        type = ConfigType.TIME,\r\n        defaultValue = \"300s\",\r\n        tags = {SCM, OZONE},\r\n        description = \"When a heartbeat from the data node arrives on SCM, \"\r\n            + \"It is queued for processing with the time stamp of when the \"\r\n            + \"heartbeat arrived. There is a heartbeat processing thread \"\r\n            + \"inside \"\r\n            + \"SCM that runs at a specified interval. This value controls how \"\r\n            + \"frequently this thread is run.\\n\\n\"\r\n            + \"There are some assumptions build into SCM such as this \"\r\n            + \"value should allow the heartbeat processing thread to run at \"\r\n            + \"least three times more frequently than heartbeats and at least \"\r\n            + \"five times more than stale node detection time. \"\r\n            + \"If you specify a wrong value, SCM will gracefully refuse to \"\r\n            + \"run. \"\r\n            + \"For more info look at the node manager tests in SCM.\\n\"\r\n            + \"\\n\"\r\n            + \"In short, you don't need to change this.\"\r\n    )\r\n{code}\r\n\r\nduplicates SCM heartbeat interval doc:\r\n\r\n{code:title=https://github.com/apache/hadoop-ozone/blob/eb1d77e3206fab1a4ac0573507c9deb2b56b9ea1/hadoop-hdds/common/src/main/resources/ozone-default.xml#L973-L991}\r\n  <property>\r\n    <name>ozone.scm.heartbeat.thread.interval</name>\r\n    <value>3s</value>\r\n    <tag>OZONE, MANAGEMENT</tag>\r\n    <description>\r\n      When a heartbeat from the data node arrives on SCM, It is queued for\r\n      processing with the time stamp of when the heartbeat arrived. There is a\r\n      heartbeat processing thread inside SCM that runs at a specified interval.\r\n      This value controls how frequently this thread is run.\r\n\r\n      There are some assumptions build into SCM such as this value should allow\r\n      the heartbeat processing thread to run at least three times more\r\n      frequently than heartbeats and at least five times more than stale node\r\n      detection time. If you specify a wrong value, SCM will gracefully refuse\r\n      to run. For more info look at the node manager tests in SCM.\r\n\r\n      In short, you don't need to change this.\r\n    </description>\r\n  </property>\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Replication manager config has wrong description"
   },
   {
      "_id": "13263699",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-22 05:07:07",
      "description": "OMVolumeCreateRequest.java L159:\r\n{code:java}\r\nomClientResponse =\r\n new OMVolumeCreateResponse(omVolumeArgs,volumeList, omResponse.build());{code}\r\n\u00a0\r\n\r\nWe add this to double-buffer, and double-buffer flushThread which is running in the background when picks up, converts to protoBuf and to ByteArray and write to rocksDB tables. So, during this conversion(This conversion will be done without any lock acquire), if any other request changes internal structure(like acls list) of OmVolumeArgs we might get ConcurrentModificationException.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add immutable entries in to the DoubleBuffer for Volume requests."
   },
   {
      "_id": "13263654",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-21 21:13:13",
      "description": "OMBucketCreateRequest.java L181:\r\n\r\nomClientResponse =\r\n new OMBucketCreateResponse(omBucketInfo,\r\n omResponse.build());\r\n\r\n\u00a0\r\n\r\nWe add this to double-buffer, and double-buffer flushThread which is running in the background when picks up, converts to protoBuf and to ByteArray and write to rocksDB tables. So, during this conversion(This conversion will be done without any lock acquire), if any other request changes internal structure(like acls list) of OMBucketInfo we might get ConcurrentModificationException.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add immutable entries in to the DoubleBuffer for Bucket requests."
   },
   {
      "_id": "13263637",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-21 19:27:30",
      "description": "Containers extracted from tar.gz should be validated to confine entries to the archive's root directory.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Validate tar entry path during extraction"
   },
   {
      "_id": "13263543",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-21 13:26:08",
      "description": "This jira proposes to add OzoneManager to MiniOzoneChaosCluster with OzoneHA implementation done. This will help in discovering bugs in Ozone Manager HA",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add OzoneManager to MiniOzoneChaosCluster"
   },
   {
      "_id": "13263401",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-20 11:40:25",
      "description": "Checkstyle errors intoduced in HDDS-2281:\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2281-wfpgn/checkstyle/summary.txt}\r\nhadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java\r\n 465: Line is longer than 80 characters (found 81).\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/container/ContainerTestHelper.java\r\n 244: Line is longer than 80 characters (found 84).\r\nhadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestContainerStateMachineFailures.java\r\n 30: Unused import - org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException.\r\n 506: &apos;;&apos; is preceded with whitespace.\r\n 517: &apos;;&apos; is preceded with whitespace.\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix checkstyle errors"
   },
   {
      "_id": "13263399",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-20 11:25:15",
      "description": "TestKeyValueContainer#testRocksDBCreateUsesCachedOptions, introduced in HDDS-2283, is failing:\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2283-cnrrq/unit/hadoop-hdds/container-service/org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.txt}\r\ntestRocksDBCreateUsesCachedOptions(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.135 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<1> but was:<11>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat org.junit.Assert.assertEquals(Assert.java:555)\r\n\tat org.junit.Assert.assertEquals(Assert.java:542)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testRocksDBCreateUsesCachedOptions(TestKeyValueContainer.java:406)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestKeyValueContainer#testRocksDBCreateUsesCachedOptions"
   },
   {
      "_id": "13263389",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-20 08:42:29",
      "description": "HDDS-2323 introduced the following Findbugs violation:\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci-q4/blob/master/trunk/trunk-nightly-20191020-r5wzl/findbugs/summary.txt}\r\nM P UrF: Unread field: org.apache.hadoop.ozone.audit.AuditMessage$Builder.params  At AuditMessage.java:[line 106]\r\n{noformat}\r\n\r\nWhich reveals that {{params}} is now not logged in audit messages:\r\n\r\n{noformat}\r\n2019-10-20 08:41:35,248 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=CREATE_VOLUME | ret=SUCCESS |\r\n2019-10-20 08:41:35,312 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=CREATE_BUCKET | ret=SUCCESS |\r\n2019-10-20 08:41:35,407 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=ALLOCATE_KEY | ret=SUCCESS |\r\n2019-10-20 08:41:37,355 | INFO  | OMAudit | user=hadoop | ip=192.168.128.2 | op=COMMIT_KEY | ret=SUCCESS |\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Params not included in AuditMessage"
   },
   {
      "_id": "13263388",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-20 08:06:36",
      "description": "HDDS-1094 added a config option ({{hdds.container.chunk.persistdata=false}}) to drop chunks instead of writing them to disk.  Currently this option triggers the following error with any key size:\r\n\r\n{noformat}\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: data array does not match the length specified. DataLen: 16777216 Byte Array: 16777478\r\n\tat org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerDummyImpl.writeChunk(ChunkManagerDummyImpl.java:87)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleWriteChunk(KeyValueHandler.java:695)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:176)\r\n\tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:277)\r\n\tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:150)\r\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:413)\r\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:423)\r\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$1(ContainerStateMachine.java:458)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Dummy chunk manager fails with length mismatch error"
   },
   {
      "_id": "13263286",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-18 23:34:40",
      "description": "In OM non-HA when double buffer flushes, it should commit with sync turned on. As in non-HA when power failure/system crashes, the operations which are acknowledged by OM might be lost in this kind of scenario. (As in rocks DB with Sync false, the flush is asynchronous and it will not persist to storage system)\r\n\r\n\u00a0\r\n\r\nIn HA, this is not a problem because the guarantee is provided by ratis and ratis logs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable sync option for OM non-HA "
   },
   {
      "_id": "13263070",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-10-18 10:53:03",
      "description": "Freon's random key generator can get stuck waiting for completion (without any hint to what's happening) if object creation encounters any non-IOException.\r\n\r\nSteps to reproduce:\r\n\r\n# Start Ozone cluster with 1 datanode\r\n# Start Freon (5K keys of size 1MB)\r\n\r\nResult: after a few hundred keys progress stops.\r\n\r\n{noformat}\r\n$ docker-compose exec scm ozone freon rk --numOfThreads 1 --numOfVolumes 1 --numOfBuckets 1 --replicationType RATIS --factor ONE --keySize $(echo '2^20' | bc -lq) --numOfKeys $(echo '5 * 2^10' | bc -lq) --bufferSize $(echo '2^16' | bc -lq)\r\n2019-10-18 10:44:45,224 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\r\n2019-10-18 10:44:45,381 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\r\n2019-10-18 10:44:45,381 INFO impl.MetricsSystemImpl: ozone-freon metrics system started\r\n2019-10-18 10:44:47,140 [main] INFO       - Number of Threads: 1\r\n2019-10-18 10:44:47,145 [main] INFO       - Number of Volumes: 1.\r\n2019-10-18 10:44:47,146 [main] INFO       - Number of Buckets per Volume: 1.\r\n2019-10-18 10:44:47,146 [main] INFO       - Number of Keys per Bucket: 5120.\r\n2019-10-18 10:44:47,147 [main] INFO       - Key size: 1048576 bytes\r\n2019-10-18 10:44:47,147 [main] INFO       - Buffer size: 65536 bytes\r\n2019-10-18 10:44:47,147 [main] INFO       - validateWrites : false\r\n2019-10-18 10:44:47,151 [main] INFO       - Starting progress bar Thread.\r\n...\r\n 7.07% |????????                                                                                             |  362/5120 \r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Random key generator can get stuck"
   },
   {
      "_id": "13263052",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-18 09:16:22",
      "description": "Large-scale listing of directory contents takes a lot longer time and also has the potential to run into OOM. I have > 1 million entries in the same level and it took lot longer time with {{RemoteIterator}} (didn't complete as it was stuck in RDB::seek).\r\n\r\nS3A batches it with 5K listing per fetch IIRC.\u00a0 It would be good to have this feature in ozone as well.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Support large-scale listing "
   },
   {
      "_id": "13263050",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-10-18 09:01:56",
      "description": "[~xyao] suggested during an offline talk to implement one additional Freon test to test the ratis part only.\r\n\r\nIt can use XceiverClientManager which creates a pure XceiverClientRatis. The client can be used to generate chunks as the datanode accepts any container id / block id.\r\n\r\nWith this approach we can stress-test one selected ratis pipeline without having full end2end overhead of the key creation (OM, SCM, etc.)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide new Freon test to test Ratis pipeline with pure XceiverClientRatis"
   },
   {
      "_id": "13263048",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-10-18 08:58:59",
      "description": "HDDS-2022 introduced new Freon tests but the Freon http server is not started for the new tests.\r\n\r\nFreon includes a http server which can be turned on with the '\u2013server' flag. It helps to monitor and profile the freon as the http server contains by default the prometheus and profiler servlets.\r\n\r\nThe server should be started if's requested.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Http server of Freon is not started for new Freon tests"
   },
   {
      "_id": "13263047",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-18 08:56:53",
      "description": "## What changes were proposed in this pull request?\r\n\r\nGenesis is a microbenchmark tool for Ozone based on JMH ([https://openjdk.java.net/projects/code-tools/jmh/).]\r\n\r\n\u00a0\r\n\r\nDue to the recent Datanode changes the BenchMarkDatanodeDispatcher is failing with NPE:\r\n\r\n\u00a0\r\n{code:java}\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.ozone.container.common.interfaces.Handler.<init>(Handler.java:69)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.<init>(KeyValueHandler.java:114)\r\n\tat org.apache.hadoop.ozone.container.common.interfaces.Handler.getHandlerForContainerType(Handler.java:78)\r\n\tat org.apache.hadoop.ozone.genesis.BenchMarkDatanodeDispatcher.initialize(BenchMarkDatanodeDispatcher.java:115)\r\n\tat org.apache.hadoop.ozone.genesis.generated.BenchMarkDatanodeDispatcher_createContainer_jmhTest._jmh_tryInit_f_benchmarkdatanodedispatcher0_G(BenchMarkDatanodeDispatcher_createContainer_jmhTest.java:438)\r\n\tat org.apache.hadoop.ozone.genesis.generated.BenchMarkDatanodeDispatcher_createContainer_jmhTest.createContainer_Throughput(BenchMarkDatanodeDispatcher_createContainer_jmhTest.java:71)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)\r\n\tat org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n {code}\r\nAnd this is the just the biggest problem there are a few other problems. I propose the following fixes:\r\n\r\n*fix 1*: NPE is thrown because the 'context' object is required by KeyValueHandler/Handler classes.\r\n\r\nIn fact the context is not required, we need two functionalities/info from the context: the ability to send icr (IncrementalContainerReport) and the ID of the datanode.\r\n\r\nLaw of Demeter principle suggests to have only the minimum required information from other classes.\r\n\r\nFor example instead of having context but using only context.getParent().getDatanodeDetails().getUuidString() we can have only the UUID string which makes more easy to test (unit and benchmark) the Handler/KeyValueHandler.\r\n\r\nThis is the biggest (but still small change) in this patch: I started to use the datanodeId and an icrSender instead of having the full context.\r\n\r\n*fix 2,3:* There were a few other problems. The scmId was missing if the writeChunk was called from Benchmark and and the Checksum was also missing.\r\n\r\n*fix 4:* I also had a few other problems: very huge containers are used (default 5G) and as the benchmark starts with creating 100 containers it requires 500G space by default. I adjusted the container size to make it possible to run on local machine.\r\n\r\n\u00a0\r\n\r\n## How this patch can be tested?\r\n{code:java}\r\n./ozone genesis -benchmark=BenchMarkDatanodeDispatcher.writeChunk{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "BenchMarkDatanodeDispatcher genesis test is failing with NPE"
   },
   {
      "_id": "13262983",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-18 01:06:00",
      "description": "om1_1 \u00a0 \u00a0 \u00a0 | 2019-10-18 00:34:45,317 [OMDoubleBufferFlushThread] ERROR\u00a0 \u00a0 \u00a0 - Terminating with exit status 2: OMDoubleBuffer flush threadOMDoubleBufferFlushThreadencountered Throwable error\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | java.util.ConcurrentModificationException\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1660)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.helpers.OmKeyLocationInfoGroup.getProtobuf(OmKeyLocationInfoGroup.java:65)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.Collections$2.tryAdvance(Collections.java:4745)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.Collections$2.forEachRemaining(Collections.java:4753)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.helpers.OmKeyInfo.getProtobuf(OmKeyInfo.java:362)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.toPersistedFormat(OmKeyInfoCodec.java:37)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.codec.OmKeyInfoCodec.toPersistedFormat(OmKeyInfoCodec.java:31)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.utils.db.CodecRegistry.asRawData(CodecRegistry.java:68)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.utils.db.TypedTable.putWithBatch(TypedTable.java:125)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.response.key.OMKeyCreateResponse.addToDBBatch(OMKeyCreateResponse.java:58)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.lambda$flushTransactions$0(OzoneManagerDoubleBuffer.java:139)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.flushTransactions(OzoneManagerDoubleBuffer.java:137)\r\n\r\nom1_1 \u00a0 \u00a0 \u00a0 | at java.base/java.lang.Thread.run(Thread.java:834)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "DoubleBuffer flush termination and OM shutdown's after that."
   },
   {
      "_id": "13262932",
      "assignee": "xyao",
      "components": [],
      "created": "2019-10-17 19:15:58",
      "description": "DN container protocol has cmd send from SCM or other DN, which do not bear OM block token like OM client. We should restrict the OM Block token check only for those issued from OM client.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Block Token verify should not apply to all datanode cmd"
   },
   {
      "_id": "13262927",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-17 18:39:27",
      "description": "While running teragen/terasort on a cluster and verifying number of keys created on Ozone Manager, I noticed that the value of NumKeys counter metric to be a negative value !Screen Shot 2019-10-17 at 11.31.08 AM.png! .\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Negative value seen for OM NumKeys Metric in JMX."
   },
   {
      "_id": "13262855",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-17 12:44:56",
      "description": "[https://github.com/apache/hadoop-ozone/blob/61f4aa30f502b34fd778d9b37b1168721abafb2f/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/protocolPB/OzoneManagerProtocolServerSideTranslatorPB.java#L117]\r\n\r\n\u00a0\r\n\r\nThis ends up converting proto toString in precondition checks and burns CPU cycles. {{request.toString()}} can be added in debug log on need basis.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid proto::tostring in preconditions to save CPU cycles"
   },
   {
      "_id": "13262617",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-16 12:11:05",
      "description": "(I almost use this Jira summary: \"Fast-lane to ozone build\" It was very hard to resist...)\r\n\r\n\u00a0\r\n\r\n\u00a0The two slowest part of Ozone build as of now:\r\n\r\n\r\n # The (multiple) shading of ozonefs\r\n # And the frontend build/obfuscation of ozone recon\r\n\r\n[~aengineer] suggested to introduce options to skip them as they are not required for the build all the time.\r\n\r\nThis patch introduces '-DskipRecon' and '-DskipShade' options to provide a faster way to create a *partial* build.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support to skip recon and/or ozonefs during the build"
   },
   {
      "_id": "13262515",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-16 05:39:52",
      "description": "{noformat}\r\nom_1        | 2019-10-16 05:33:57,413 [IPC Server handler 19 on 9862] ERROR      - Trying to release the lock on /bypdd/mybucket4, which was never acquired.\r\nom_1        | 2019-10-16 05:33:57,414 WARN ipc.Server: IPC Server handler 19 on 9862, call Call#4 Retry#8 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 172.29.0.4:37018\r\nom_1        | java.lang.IllegalMonitorStateException: Releasing lock on resource /bypdd/mybucket4 without acquiring lock\r\nom_1        | \tat org.apache.hadoop.ozone.lock.LockManager.getLockForReleasing(LockManager.java:220)\r\nom_1        | \tat org.apache.hadoop.ozone.lock.LockManager.release(LockManager.java:168)\r\nom_1        | \tat org.apache.hadoop.ozone.lock.LockManager.writeUnlock(LockManager.java:148)\r\nom_1        | \tat org.apache.hadoop.ozone.om.lock.OzoneManagerLock.unlock(OzoneManagerLock.java:364)\r\nom_1        | \tat org.apache.hadoop.ozone.om.lock.OzoneManagerLock.releaseWriteLock(OzoneManagerLock.java:329)\r\nom_1        | \tat org.apache.hadoop.ozone.om.request.key.OMKeyCommitRequest.validateAndUpdateCache(OMKeyCommitRequest.java:177)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Duplicate release of lock in OMKeyCommitRequest"
   },
   {
      "_id": "13262505",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2019-10-16 04:44:39",
      "description": "{noformat:title=ozone}\r\nUsage: ozone [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\r\n...\r\ninsight       tool to get runtime opeartion information\r\n...\r\n{noformat}\r\n\r\nShould be \"operation\".",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Fix typo in ozone command"
   },
   {
      "_id": "13262470",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-15 21:57:59",
      "description": "OzoneManagerProtocolClientSideTranslatorPB.java\r\n\r\nL251: if (cause instanceof NotLeaderException) {\r\n NotLeaderException notLeaderException = (NotLeaderException) cause;\r\n omFailoverProxyProvider.performFailoverIfRequired(\r\n notLeaderException.getSuggestedLeaderNodeId());\r\n return getRetryAction(RetryAction.RETRY, retries, failovers);\r\n }\r\n\r\n\u00a0\r\n\r\nThe suggested leader returned from Server is not used during failOver, as the cause is a type of RemoteException. So with current code, it does not use suggested leader for failOver at all and by default with each OM, it tries max retries.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix logic of RetryPolicy in OzoneClientSideTranslatorPB"
   },
   {
      "_id": "13262454",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-15 20:41:36",
      "description": "Currently, there is no way to add Ozone Ranger plugin to Ozone Manager classpath.\u00a0\r\n\r\nWe should be able to set an environment variable that will be respected by ozone and added to Ozone Manager classpath.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add support to add ozone ranger plugin to Ozone Manager classpath"
   },
   {
      "_id": "13262333",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-15 10:55:10",
      "description": "When running a write heavy benchmark, {{{color:#000000}org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.flushTransactions{color}}} was invoked for pretty much every write.\r\n\r\nThis forces {{cleanupCache}} to be invoked which ends up choking in single thread executor. Attaching the profiler information which gives more details.\r\n\r\nIdeally, {{flushTransactions}} should batch up the work to reduce load on rocksDB.\r\n\r\n\u00a0\r\n\r\n[https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java#L130]\r\n\r\n\u00a0\r\n\r\n[https://github.com/apache/hadoop-ozone/blob/master/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/ratis/OzoneManagerDoubleBuffer.java#L322]\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "performance"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Optimise OzoneManagerDoubleBuffer::flushTransactions to flush in batches"
   },
   {
      "_id": "13262307",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-15 07:20:16",
      "description": "I realized multiple JVM crashes in the daily builds:\r\n\r\n\u00a0\r\n{code:java}\r\n\r\nERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\r\n      \r\n      \r\n        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter9018689154779946208.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7569723928289175829tmp surefire_947955725320624341206tmp\r\n      \r\n      \r\n        [ERROR] Error occurred in starting fork, check output in log\r\n      \r\n      \r\n        [ERROR] Process Exit Code: 139\r\n      \r\n      \r\n        [ERROR] Crashed tests:\r\n      \r\n      \r\n        [ERROR] org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename\r\n      \r\n      \r\n        [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\r\n      \r\n      \r\n        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter5429192218879128313.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7227403571189445391tmp surefire_1011197392458143645283tmp\r\n      \r\n      \r\n        [ERROR] Error occurred in starting fork, check output in log\r\n      \r\n      \r\n        [ERROR] Process Exit Code: 139\r\n      \r\n      \r\n        [ERROR] Crashed tests:\r\n      \r\n      \r\n        [ERROR] org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp\r\n      \r\n      \r\n        [ERROR] org.apache.maven.surefire.booter.SurefireBooterForkException: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\r\n      \r\n      \r\n        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter1355604543311368443.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire3938612864214747736tmp surefire_933162535733309260236tmp\r\n      \r\n      \r\n        [ERROR] Error occurred in starting fork, check output in log\r\n      \r\n      \r\n        [ERROR] Process Exit Code: 139\r\n      \r\n      \r\n        [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\r\n      \r\n      \r\n        [ERROR] Command was /bin/sh -c cd /workdir/hadoop-ozone/ozonefs && /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Xmx2048m -XX:+HeapDumpOnOutOfMemoryError -jar /workdir/hadoop-ozone/ozonefs/target/surefire/surefirebooter9018689154779946208.jar /workdir/hadoop-ozone/ozonefs/target/surefire 2019-10-06T14-52-40_697-jvmRun1 surefire7569723928289175829tmp surefire_947955725320624341206tmp\r\n      \r\n      \r\n        [ERROR] Error occurred in starting fork, check output in log\r\n      \r\n      \r\n        [ERROR] Process Exit Code: 139 {code}\r\n\u00a0\r\n\r\nBased on the crash log (uploaded) it's related to the rocksdb JNI interface.\r\n\r\nIn the current ozone-build docker image (which provides the environment for build) we use alpine where musl libc is used instead of the main glibc. I think it would be more safe to use the same glibc what is used in production.\r\n\r\nI tested with centos based docker image and it seems to be more stable. Didn't see any more JVM crashes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Switch to centos with the apache/ozone-build docker image"
   },
   {
      "_id": "13262155",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-10-14 12:56:11",
      "description": "Some of the versions (eg. ozone.version, hdds.version, ratis.version) are required for both ozone and hdds subprojects. As we have a common pom.xml it can be safer to manage them in one common place at the root pom.xml instead of managing them multiple times.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Manage common pom versions in one common place"
   },
   {
      "_id": "13262135",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-14 10:41:46",
      "description": "Latest Ratis contains very good metrics about the status of the ratis ring.\r\n\r\nAfter RATIS-702 it will be possible to adjust the repoter of the Dropwizard based ratis metrics and export them directly to the /prom http endpoint (used by ozone insight and ratis).\r\n\r\nUnfortunately Dropwizard is very simple, there is no tag support. All of the instance specific strings are part of the metric name. For example:\r\n{code:java}\r\n\"ratis_grpc.log_appender.72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67@group\"\r\n + \"-72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67\"\r\n + \".grpc_log_appender_follower_75fa730a-59f0-4547\"\r\n + \"-bd68-216162c263eb_latency\", {code}\r\nIn this patch I will use a simple method: during the export of the dropwizard metrics based on the well known format of the ratis metrics, they are converted to proper prometheus metrics where the instance information is included as tags:\r\n{code:java}\r\nratis_grpc.log_appender.grpc_log_appender_follower_latency{instance=\"72caaf3a-fb1c-4da4-9cc0-a2ce21bb8e67\"}\r\n {code}\r\nWith this approach we can:\r\n\r\n\u00a01. monitor easily all the Ratis pipelines with one simple query\r\n\r\n\u00a02. Use the metrics for ozone insight which will show health state of the Ratis pipeline",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Publish normalized Ratis metrics via the prometheus endpoint"
   },
   {
      "_id": "13262119",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-10-14 09:36:45",
      "description": "Maven build of Ozone is starting with a warning:\r\n{code:java}\r\n[WARNING] \r\n[WARNING] Some problems were encountered while building the effective model for org.apache.hadoop:hadoop-ozone-tools:jar:0.5.0-SNAPSHOT\r\n[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: io.dropwizard.metrics:metrics-core:jar -> version 3.2.4 vs (?) @ line 94, column 17\r\n[WARNING] \r\n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\r\n[WARNING] \r\n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.\r\n[WARNING] \r\n {code}\r\nIt's better to avoid it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix maven warning about duplicated metrics-core jar"
   },
   {
      "_id": "13262109",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12336011",
            "id": "12336011",
            "name": "freon"
         }
      ],
      "created": "2019-10-14 09:08:55",
      "description": "HDDS-2022 introduced new freon tests, but the initial root span of opentracing is not created before the test execution. We need to enable opentracing to get better view about the executions of the new freon test.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable Opentracing for new Freon tests"
   },
   {
      "_id": "13262105",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-14 08:58:23",
      "description": "During the original creation of the compose/ozoneperf we added an example freon execution to make it clean how the data can be generated. This freon process starts all the time when ozoneperf cluster is started (usually I notice it when my CPU starts to use 100% of the available resources).\r\n\r\nSince the creation of this cluster definition we implemented multiple type of freon tests and it's hard predict which tests should be executed. I propose to remove the default execution of the random key generation but keep the opportunity to run any of the tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozoneperf compose cluster shouln't start freon by default"
   },
   {
      "_id": "13262104",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-14 08:42:08",
      "description": "HDDS-2042 disabled the console logging for all of the ozone command line tools including freon.\r\n\r\nBut freon is different, it has a different error handling model. For freon we need all the log on the console.\r\n\r\n\u00a01. To follow all the different errors\r\n\r\n\u00a02. To get information about the used (random) prefix which can be reused during the validation phase.\r\n\r\n\u00a0\r\n\r\nI propose to restore the original behavior for Ozone.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Display log of freon on the standard output"
   },
   {
      "_id": "13262103",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-14 08:35:52",
      "description": "During the apache/hadoop.git --> apache/hadoop-ozone.git move we rewrote the (git) history to simplify the work. Unfortunately some of the early work of HDDS-7280 is not part of the hadoop-ozone repository just the hadoop repository. As it suggested by Anu Engineer, we can explain this in a separated file and show how the origin of Ozone can be found.\r\n\r\n\u00a0\r\n\r\ncc [~aengineer]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create a new HISTORY.md in the new repository"
   },
   {
      "_id": "13262098",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-14 07:59:16",
      "description": "Github supports CONTIRUTION.md which is displayed during the creation of a new Github PR. We can copy the content of the wiki page about how to contribut / how to build.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create a new CONTRIBUTION.md for the new repository"
   },
   {
      "_id": "13262097",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-14 07:58:32",
      "description": "The current README is main Hadoop specific. We can create an ozone specific.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create Ozone specific README.md to the new hadoop-ozone repository"
   },
   {
      "_id": "13262023",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-13 19:57:18",
      "description": "Add robot tests to test OM HA functionality.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Acceptance tests for OM HA"
   },
   {
      "_id": "13261986",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-13 08:31:00",
      "description": "This is suggested by [~aengineer] during an offline discussion to add more information to the github PR template based on the template of ambari (by Vivek):\r\n\r\nhttps://github.com/apache/ambari/commit/579cec8cf5bcfe1a1a0feacf055ed6569f674e6a",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Put testing information and a problem description to the github PR template"
   },
   {
      "_id": "13261708",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-10-11 06:28:53",
      "description": "I can be observed that the GetBlock and ReadChunk command is sent to 2 different datanodes. It should be sent to the same datanode to re-use the connection.\r\n\r\n{code}\r\n19/10/10 00:43:42 INFO scm.XceiverClientGrpc: Send command GetBlock to datanode 172.26.32.224\r\n19/10/10 00:43:42 INFO scm.XceiverClientGrpc: Send command ReadChunk to datanode 172.26.32.231\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "GetBlock and ReadChunk commands should be sent to the same datanode"
   },
   {
      "_id": "13261649",
      "assignee": "xyao",
      "components": [],
      "created": "2019-10-10 21:17:38",
      "description": "ozone scmcli pipeline list\r\n{noformat}\r\njava.lang.NullPointerException\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientManager.<init>(XceiverClientManager.java:98)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientManager.<init>(XceiverClientManager.java:83)\r\n\tat org.apache.hadoop.hdds.scm.cli.SCMCLI.createScmClient(SCMCLI.java:139)\r\n\tat org.apache.hadoop.hdds.scm.cli.pipeline.ListPipelinesSubcommand.call(ListPipelinesSubcommand.java:55)\r\n\tat org.apache.hadoop.hdds.scm.cli.pipeline.ListPipelinesSubcommand.call(ListPipelinesSubcommand.java:30)\r\n\tat picocli.CommandLine.execute(CommandLine.java:1173)\r\n\tat picocli.CommandLine.access$800(CommandLine.java:141)\r\n\tat picocli.CommandLine$RunLast.handle(CommandLine.java:1367)\r\n\tat picocli.CommandLine$RunLast.handle(CommandLine.java:1335)\r\n\tat picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)\r\n\tat picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)\r\n\tat picocli.CommandLine.parseWithHandler(CommandLine.java:1465)\r\n\tat org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)\r\n\tat org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)\r\n\tat org.apache.hadoop.hdds.scm.cli.SCMCLI.main(SCMCLI.java:101){noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "scmcli pipeline list command throws NullPointerException"
   },
   {
      "_id": "13261629",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-10 18:40:46",
      "description": "Currently, ContainerStateMachine#applyTrannsaction ignores close container exception.Similarly,ContainerStateMachine#handleWriteChunk call also should ignore close container exception.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ContainerStateMachine#handleWriteChunk should ignore close container exception "
   },
   {
      "_id": "13261549",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-10-10 11:37:44",
      "description": "HddsUtils#CheckForException checks for the cause to be set properly to one of the defined/expected exceptions. In case, ratis throws up any runtime exception,\u00a0HddsUtils#CheckForException can return null and lead to NullPointerException while write.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "HddsUtils#CheckForException should not return null in case the ratis exception cause is not set"
   },
   {
      "_id": "13261472",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-10 01:05:45",
      "description": "ozone s3 getSecret\r\n\r\nozone s3 path are not working on OM HA cluster\r\n\r\n\u00a0\r\n\r\nBecause these commands do not take URI as a parameter. And for shell in HA, passing URI is mandatory.\u00a0\r\n\r\n\u00a0\r\n\r\nBelow is the output when running on OM HA cluster:\r\n\r\n\u00a0\r\n{code:java}\r\n$ozone s3 getsecret\r\nService ID or host name must not be omitted when ozone.om.service.ids is defined.\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone S3 CLI commands not working on HA cluster"
   },
   {
      "_id": "13261469",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-10 00:47:15",
      "description": "This will add a new compose setup with 3 OM's and start SCM, S3G, Datanode.\r\n\r\nRun the existing test suite against this new docker-compose cluster.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Run S3 test suite on OM HA cluster"
   },
   {
      "_id": "13261237",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-09 06:13:14",
      "description": "In GrpcOutputStream, it writes data to a ByteArrayOutputStream and copies them to a ByteString.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid buffer copying in GrpcReplicationService"
   },
   {
      "_id": "13261236",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-09 06:10:42",
      "description": "In StreamDownloader.onNext, CopyContainerResponseProto is copied to a byte[] and then it is written out to the stream.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid buffer copying in GrpcReplicationClient"
   },
   {
      "_id": "13261234",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-09 05:49:45",
      "description": "ContainerStateMachine:\r\n- In loadSnapshot(..), it first reads the snapshotFile to a  byte[] and then parses it to ContainerProtos.Container2BCSIDMapProto.  The buffer copying can be avoided.\r\n{code}\r\n    try (FileInputStream fin = new FileInputStream(snapshotFile)) {\r\n      byte[] container2BCSIDData = IOUtils.toByteArray(fin);\r\n      ContainerProtos.Container2BCSIDMapProto proto =\r\n          ContainerProtos.Container2BCSIDMapProto\r\n              .parseFrom(container2BCSIDData);\r\n      ...\r\n    }\r\n{code}\r\n\r\n- persistContainerSet(..) has similar problem.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "performance",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid buffer copying in ContainerStateMachine.loadSnapshot/persistContainerSet"
   },
   {
      "_id": "13261233",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-09 05:12:34",
      "description": "Provide config in OzoneManager Lock for fair/non-fair for OM RW Lock.\r\n\r\nCreated based on review comments during HDDS-2244.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide config for fair/non-fair for OM RW Lock"
   },
   {
      "_id": "13261192",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-08 20:45:48",
      "description": "Container file checksum is calculated based on all YAML fields in a given Ozone version.  If the same container file is used in older Ozone, which has fewer fields, the expected checksum will be different.\r\n\r\nExample: origin pipeline ID and origin node ID were added for HDDS-837 in Ozone 0.4.0.  Starting Ozone 0.3.0 with the same data results in checksum error.\r\n\r\n{noformat}\r\ndatanode_1  | ... ERROR ContainerReader:166 - Failed to parse ContainerFile for ContainerID: 1\r\ndatanode_1  | org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Container checksum error for ContainerID: 1.\r\ndatanode_1  | Stored Checksum: 7a6ec508d6e3796c5fe5fd52574b3d3437b0a0eaa4e053f7a96a5e39f4abb374\r\ndatanode_1  | Expected Checksum: fee023a02d3ced2f7b0b42c116cce5f03da6b57b29965ca878dc46d1213230b6\r\ndatanode_1  | \tat org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.verifyChecksum(ContainerUtils.java:259)\r\ndatanode_1  | \tat org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:165)\r\ndatanode_1  | \tat org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerData(ContainerReader.java:180)\r\ndatanode_1  | \tat org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.verifyContainerFile(ContainerReader.java:164)\r\ndatanode_1  | \tat org.apache.hadoop.ozone.container.ozoneimpl.ContainerReader.readVolume(ContainerReader.java:142)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "upgrade"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Incorrect container checksum upon downgrade"
   },
   {
      "_id": "13261161",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-08 17:51:31",
      "description": "Container metadata scanner can be configured to run at specific time intervals, eg. hourly ({{hdds.containerscrub.metadata.scan.interval}}).  However, the actual run interval does not match the configuration.  After a datanode restart, it runs in quick succession, later it runs at apparently random intervals.\r\n\r\n{noformat:title=sample log}\r\ndatanode_1  | 2019-10-08 14:05:30 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 1, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0\r\ndatanode_1  | 2019-10-08 14:09:33 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 1, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0\r\n...\r\ndatanode_1  | 2019-10-08 14:09:33 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 28, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0\r\ndatanode_1  | 2019-10-08 14:21:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 29, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0\r\ndatanode_1  | 2019-10-08 14:21:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 30, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0\r\ndatanode_1  | 2019-10-08 15:30:38 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 31, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0\r\ndatanode_1  | 2019-10-08 16:45:01 INFO  ContainerMetadataScanner:88 - Completed an iteration of container metadata scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 32, Number of containers scanned in this iteration : 6, Number of unhealthy containers found in this iteration : 0\r\n{noformat}\r\n\r\nThe problem is that time elapsed is measured in nanoseconds, while the configuration is in milliseconds.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Container metadata scanner interval mismatch"
   },
   {
      "_id": "13260867",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-07 11:34:25",
      "description": "Sometimes integration test run gets killed, and {{integration.sh}} incorrectly reports \"success\".  Example:\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci-q4/tree/ae930d6f7f10c7d2aeaf1f2f21b18ada954ea444/pr/pr-hdds-2259-hlwmv/integration/result}\r\nsuccess\r\n{noformat}\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci-q4/blob/ae930d6f7f10c7d2aeaf1f2f21b18ada954ea444/pr/pr-hdds-2259-hlwmv/integration/output.log#L2457}\r\n/workdir/hadoop-ozone/dev-support/checks/integration.sh: line 22:   369 Killed                  mvn -B -fn test -f pom.ozone.xml -pl :hadoop-ozone-integration-test,:hadoop-ozone-filesystem,:hadoop-ozone-tools -Dtest=\\!TestMiniChaosOzoneCluster \"$@\"\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "integration.sh may report false negative"
   },
   {
      "_id": "13260863",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-07 10:59:34",
      "description": "TestOzoneContainer#testContainerCreateDiskFull fails intermittently (HDDS-2263), but test output does not reveal too much about the reason.  The goal of this task is to improve the assertion/output to make it easier to fix the failure.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Improve output of TestOzoneContainer"
   },
   {
      "_id": "13260852",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-07 10:06:18",
      "description": "{noformat}\r\ndatanode_1  | /opt/hadoop/bin/docker/entrypoint.sh: line 66: SLEEP_SECONDS: command not found\r\ndatanode_1  | Sleeping for  seconds\r\n{noformat}\r\n\r\nEg. https://raw.githubusercontent.com/elek/ozone-ci-q4/master/pr/pr-hdds-2238-79fll/acceptance/docker-ozonesecure-ozonesecure-s3-s3g.log",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "SLEEP_SECONDS: command not found"
   },
   {
      "_id": "13260747",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-06 06:44:08",
      "description": "Chunk checksum verification fails for (almost) any file.  This is caused by computing checksum for the entire buffer, regardless of the actual size of the chunk.\r\n\r\n{code:title=https://github.com/apache/hadoop/blob/55c5436f39120da0d7dabf43d7e5e6404307123b/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainerCheck.java#L259-L273}\r\n            byte[] buffer = new byte[cData.getBytesPerChecksum()];\r\n...\r\n                v = fs.read(buffer);\r\n...\r\n                bytesRead += v;\r\n...\r\n                ByteString actual = cal.computeChecksum(buffer)\r\n                    .getChecksums().get(0);\r\n{code}\r\n\r\nThis results in marking all closed containers as unhealthy.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Container Data Scrubber computes wrong checksum"
   },
   {
      "_id": "13260645",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-10-04 21:11:06",
      "description": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChecksumByteBuffer.java\r\n 84: Inner assignments should be avoided.\r\n 85: Inner assignments should be avoided.\r\n 101: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 102: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 103: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 104: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 105: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 106: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 107: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.\r\n 108: &apos;case&apos; child has incorrect indentation level 8, expected level should be 6.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix checkstyle issues in ChecksumByteBuffer"
   },
   {
      "_id": "13260611",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-10-04 17:33:04",
      "description": "Test always fails with assertion error:\r\n\r\n{code}\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertTrue(Assert.java:52)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestContainerStateMachine.testRatisSnapshotRetention(TestContainerStateMachine.java:188)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix flaky unit testTestContainerStateMachine#testRatisSnapshotRetention"
   },
   {
      "_id": "13260578",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-04 14:00:31",
      "description": "hadoop-ozone/dev-support/checks/unit.sh (and same with integration) provides an easy entrypoint to execute all the unit/integration test. But in same cases it would be great to use the script but further specify the scope of the test.\r\n\r\nWith this simple patch it will be possible to adjust the surefire parameters.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add an option to customize unit.sh and integration.sh parameters"
   },
   {
      "_id": "13260565",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-10-04 12:22:46",
      "description": "Hadoop 3.1 and 3.2 acceptance tests started failing with HDDS-1720, which added a new, annotated configuration class.\r\n\r\nThe [change itself|https://github.com/apache/hadoop/pull/1538/files] looks fine.  The problem is that the packaging process for {{ozone-filesystem-lib}} jars keeps only 1 or 2 {{ozone-default-generated.xml}} files.  With the new config in place, client configs are missing, so Ratis client gets evicted immediately due to {{scm.container.client.idle.threshold}} = 0.  This results in NPE:\r\n\r\n{code:title=https://elek.github.io/ozone-ci-q4/pr/pr-hdds-1720-trunk-rd9ht/acceptance/summary.html#s1-s5-t1-k2-k2}\r\nRunning command 'hdfs dfs -put /opt/hadoop/NOTICE.txt o3fs://bucket1.vol1/ozone-14607\r\n...\r\n-put: Fatal internal error\r\njava.lang.NullPointerException: client is null\r\n\tat java.util.Objects.requireNonNull(Objects.java:228)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientRatis.getClient(XceiverClientRatis.java:208)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:234)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:332)\r\n\tat org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:310)\r\n...\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Generated configs missing from ozone-filesystem-lib jars"
   },
   {
      "_id": "13260495",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-10-04 04:58:52",
      "description": "Use new ReadWriteLock added in HDDS-2223.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use new ReadWrite lock in OzoneManager"
   },
   {
      "_id": "13260435",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-03 20:38:38",
      "description": "Currently, while looking up a key, the Ozone Manager gets the pipeline information from SCM through an RPC for every block in the key. For large files > 1GB, we may end up making a lot of RPC calls for this. This can be optimized in a couple of ways\r\n\r\n* We can implement a batch getContainerWithPipeline API in SCM using which we can get the pipeline info locations for all the blocks for a file. To keep the number of containers passed in to SCM in a single call, we can have a fixed container batch size on the OM side. _Here, Number of calls = 1 (or k depending on batch size)_\r\n* Instead, a simpler change would be to have a map (method local) of ContainerID -> Pipeline that we get from SCM so that we don't need to make repeated calls to SCM for the same containerID for a key. _Here, Number of calls = Number of unique containerIDs_",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Optimize the refresh pipeline logic used by KeyManagerImpl to obtain the pipelines for a key"
   },
   {
      "_id": "13260360",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-03 18:53:13",
      "description": "A command line tool (*ozone omha*) to get information related to OM HA.\u00a0\r\nThis Jira proposes to add the\u00a0_getServiceState_ option for OM HA which lists all the OMs in the service and their corresponding Ratis server roles (LEADER/ FOLLOWER).\u00a0\r\nWe can later add more options to this tool.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Command line tool for OM Admin"
   },
   {
      "_id": "13260356",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-03 18:35:12",
      "description": "[https://github.com/elek/ozone-ci-q4/blob/master/pr/pr-hdds-2162-pj84x/integration/hadoop-ozone/ozonefs/org.apache.hadoop.fs.ozone.TestOzoneFsHAURLs.txt]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestOzoneFsHAUrls"
   },
   {
      "_id": "13260334",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-10-03 15:51:25",
      "description": "In an empty cluster (without closed containers) logs are filled with messages from completed data scrubber iterations (~3600 per second for me), if Container Scanner is enabled ({{hdds.containerscrub.enabled=true}}), eg.:\r\n\r\n{noformat}\r\ndatanode_1  | 2019-10-03 15:43:57 INFO  ContainerDataScanner:114 - Completed an iteration of container data scrubber in 0 minutes. Number of  iterations (since the data-node restart) : 6763, Number of containers scanned in this iteration : 0, Number of unhealthy containers found in this iteration : 0\r\n{noformat} \r\n\r\nAlso CPU usage is quite high.\r\n\r\nI think:\r\n\r\n# there should be a small sleep between iterations\r\n# it should log only if any containers were scanned",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Container Data Scrubber spams log in empty cluster"
   },
   {
      "_id": "13260331",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-03 15:17:43",
      "description": "1. OzoneManager starts KeyManager\r\n\r\n2. KeyManager starts KeyDeletingService\r\n\r\n3. KeyDeletingService uses OzoneManager.isLeader()\r\n\r\n4. OzoneManager.isLeader() uses omRatisServer\r\n\r\n5. omRatisServer can be null (bumm)\r\n\r\n\u00a0\r\n\r\nNow the initialization order in OzoneManager:\r\n\r\n\u00a0\r\n\r\nnew KeymanagerServer() *Includes start()!!!!*\r\n\r\nomRatisServer initialization\r\n\r\nstart() (includes KeyManager.start())\r\n\r\n\u00a0\r\n\r\nThe solution seems to be easy: start the key manager only from the OzoneManager.start() and not from the OzoneManager.instantiateServices()",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "KeyDeletingService throws NPE if it's started too early"
   },
   {
      "_id": "13260158",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-10-02 17:58:07",
      "description": "[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR] mvn <goals> -rf :hadoop-ozone-recon\r\n[INFO] Build failures were ignored.\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/index.html\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/2.8943d5a3.chunk.css\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/2.8943d5a3.chunk.css.map\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/css/main.96eebd44.chunk.css\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/runtime~main.a8a9905a.js.map\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/runtime~main.a8a9905a.js\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/2.ea549bfe.chunk.js\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/main.5bb53989.chunk.js\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/static/js/2.ea549bfe.chunk.js.map\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/precache-manifest.1d05d7a103ee9d6b280ef7adfcab3c01.js\r\nhadoop-ozone/recon/target/rat.txt: !????? /Users/aengineer/apache/hadoop/hadoop-ozone/recon/src/main/resources/webapps/recon/ozone-recon-web/build/service-worker.js",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "rat.sh fails due to ozone-recon-web/build files"
   },
   {
      "_id": "13260111",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-02 13:05:43",
      "description": "We have a dedicated apache/ozone-build image which contains all of the required build and test tools to build ozone.\r\n\r\nUnfortunately\u00a0 it's not working as one shell script was not added to the original patch.\r\n\r\nThis patch (to the hadoop-docker-ozone repo!!) remove the requirement of the entrypoint.sh (no more docker in docker)\r\n\r\n\u00a0\r\n\r\nAnd installs additional tools (blockade, kubectl, mailsend)\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone-build docker image is failing due to a missing entrypoint script"
   },
   {
      "_id": "13260085",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-02 10:35:04",
      "description": "Previously {{result}} directory was created by simply {{source}}-ing {{testlib.sh}}, but HDDS-2185 changed it to avoid lost results.  {{test-single.sh}} needs to be adjusted accordingly.\r\n\r\n{noformat}\r\n$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone\r\n$ docker-compose up -d --scale datanode=3\r\n$ ../test-single.sh scm basic/basic.robot\r\n...\r\ninvalid output path: directory \"hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone/result\" does not exist\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "test-single.sh cannot copy results"
   },
   {
      "_id": "13260052",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-02 07:01:48",
      "description": "Some of the entries in {{ozonesecure-mr/docker-config}} are in invalid format, thus they end up missing from the generated config files.\r\n\r\n{noformat}\r\n$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozonesecure-mr\r\n$ ./test.sh # configs are generated during container startup\r\n$ cd ../..\r\n\r\n$ grep -c 'ozone.administrators' compose/ozonesecure-mr/docker-config\r\n1\r\n$ grep -c 'ozone.administrators' etc/hadoop/ozone-site.xml\r\n0\r\n\r\n$ grep -c 'yarn.timeline-service' compose/ozonesecure-mr/docker-config\r\n5\r\n$ grep -c 'yarn.timeline-service' etc/hadoop/yarn-site.xml\r\n2\r\n\r\n$ grep -c 'container-executor' compose/ozonesecure-mr/docker-config\r\n3\r\n$ grep -c 'container-executor' etc/hadoop/yarn-site.xml\r\n0\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Invalid entries in ozonesecure-mr config"
   },
   {
      "_id": "13260009",
      "assignee": "xyao",
      "components": [],
      "created": "2019-10-01 22:35:48",
      "description": "The certClient was not initialized in proper order as a result, when OM restart with delegation token issued, the ozone delegation token secret manager NPE.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix NPE in OzoneDelegationTokenManager#addPersistedDelegationToken"
   },
   {
      "_id": "13259992",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-01 20:24:08",
      "description": "Intermittent failure of {{ozone-recon}} and some other acceptance tests where SCM container is not available is caused by leftover secure config in {{core-site.xml}}.\r\n\r\nInitially the config file is [empty|https://raw.githubusercontent.com/apache/hadoop/trunk/hadoop-hdds/common/src/main/conf/core-site.xml].  Various test environments populate it with different settings.  The problem happens when a test does not specify any config for {{core-site.xml}}, in which case the previous test's config file is retained.\r\n\r\n{code}\r\nscm_1       | 2019-10-01 19:42:05 WARN  WebAppContext:531 - Failed startup of context o.e.j.w.WebAppContext@1cc680e{/,file:///tmp/jetty-0.0.0.0-9876-scm-_-any-1272594486261557815.dir/webapp/,UNAVAILABLE}{/scm}\r\nscm_1       | javax.servlet.ServletException: javax.servlet.ServletException: Keytab does not exist: /etc/security/keytabs/HTTP.keytab\r\nscm_1       | \tat org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188)\r\n...\r\nscm_1       | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:791)\r\n...\r\nscm_1       | Unable to initialize WebAppContext\r\nscm_1       | 2019-10-01 19:42:05 INFO  StorageContainerManagerStarter:51 - SHUTDOWN_MSG:\r\nscm_1       | /************************************************************\r\nscm_1       | SHUTDOWN_MSG: Shutting down StorageContainerManager at 8724df7131bb/192.168.128.6\r\nscm_1       | ************************************************************/\r\n{code}\r\n\r\nThe problem is intermittent due to ordering of test cases being different in different runs.  If a secure test is run earlier, more tests are affected.  If secure tests are run last, the issue does not happen.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM fails to start in most unsecure environments due to leftover secure config"
   },
   {
      "_id": "13259968",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-10-01 17:34:35",
      "description": "During initial startup/restart of OM, if table has cache cleanup policy set to NEVER, we fill the table cache and also epochEntries. We do not need to add entries to epochEntries, as the epochEntries is used for eviction from the cache, once double buffer flushes to disk.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix loadup cache for cache cleanup policy NEVER"
   },
   {
      "_id": "13259925",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-10-01 14:15:39",
      "description": "ozoneperf compose cluster contains a prometheus but as of now it collects the data only from scm and om.\r\n\r\nWe don't know the exact number of datanodes (can be scaled up and down) therefor it's harder to configure the datanode host names. I would suggest to configure the first 10 datanodes (which covers most of the use cases)\r\n\r\nHow to test?\r\n{code:java}\r\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozoneperf\r\ndocker-compose up -d\r\nfirefox http://localhost:9090/targets\r\n\u00a0 {code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Monitor datanodes in ozoneperf compose cluster"
   },
   {
      "_id": "13259852",
      "assignee": "elek",
      "components": [],
      "created": "2019-10-01 09:28:39",
      "description": "For example from the nightly build:\r\n{code:java}\r\n  <testcase name=\"testNoFallback[8]\" classname=\"org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware\" time=\"0.014\">\r\n      \r\n      \r\n            <failure type=\"java.lang.AssertionError\">java.lang.AssertionError\r\n   \r\n      \r\n        \tat org.junit.Assert.fail(Assert.java:86)\r\n      \r\n      \r\n        \tat org.junit.Assert.assertTrue(Assert.java:41)\r\n      \r\n      \r\n        \tat org.junit.Assert.assertTrue(Assert.java:52)\r\n      \r\n      \r\n        \tat org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware.testNoFallback(TestSCMContainerPlacementRackAware.java:276)\r\n      \r\n      \r\n        \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n      \r\n      \r\n        \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n      \r\n      \r\n        \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n      \r\n      \r\n        \tat java.lang.reflect.Method.invoke(Method.java:498)\r\n      \r\n      \r\n        \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n {code}\r\nThe problem is in the testNoFallback:\r\n\r\nLet's say we have 11 nodes (from parameter) and we would like to choose 5 nodes (hard coded in the test).\r\n\r\nAs the first two replicas are chosen from the same rack an all the other from different racks it's not possible, so we except a failure.\r\n\r\nBut we have an assertion that the success count is at least 3. But this is true only if the first two replicas are placed to the rack1 (5 nodes) or rack2 (5nodes). If the replica is placed to the rack3 (one node) it will fail immediately:\r\n\r\n\u00a0\r\n\r\nLucky case when we have success count > 3\r\n{code:java}\r\n rack1 -- node1 \r\n rack1 -- node2 -- FIRST replica\r\n rack1 -- node3 -- SECOND replica\r\n rack1 -- node4\r\n rack1 -- node5 \r\n rack2 -- node6\r\n rack2 -- node7 -- THIRD replica\r\n rack2 -- node8\r\n rack2 -- node9 \r\n rack2 -- node10\r\n rack3 -- node11 -- FOURTH replica{code}\r\n\u00a0The specific case when we have success count == 1, as we can't choose the second replica on rack3 (This is when the test is failing)\r\n{code:java}\r\n rack1 -- node1 \r\n rack1 -- node2\r\n rack1 -- node3\r\n rack1 -- node4\r\n rack1 -- node5 \r\n rack2 -- node6\r\n rack2 -- node7\r\n rack2 -- node8\r\n rack2 -- node9 \r\n rack2 -- node10\r\n rack3 -- node11 -- FIRST replica{code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestSCMContainerPlacementRackAware has an intermittent failure"
   },
   {
      "_id": "13259738",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-30 18:28:26",
      "description": "Occasionally some acceptance test docker environment fails to start up properly.  We need docker logs for analysis, but they are not being collected.\r\n\r\nhttps://github.com/elek/ozone-ci-q4/blob/master/trunk/trunk-nightly-extra-20190930-74rp4/acceptance/output.log#L3765-L3768",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Collect docker logs if env fails to start"
   },
   {
      "_id": "13259734",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-09-30 17:52:34",
      "description": "Currently, if applyTransaction fails, the stateMachine is marked unhealthy and next snapshot creation will fail. As a result of which the the raftServer will close down leading to pipeline failure. ClosedContainer exception should be ignored while marking the stateMachine unhealthy.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ContainerStateMachine should not be marked unhealthy if applyTransaction fails with closed container exception"
   },
   {
      "_id": "13259624",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-09-30 09:37:59",
      "description": "This Jira aims to update ozone with latest ratis snapshot which has a crtical fix for retry behaviour on getting not leader exception in client.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update Ratis to latest snapshot"
   },
   {
      "_id": "13259596",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-30 07:32:21",
      "description": "{{checkstyle.sh}} outputs files with checkstyle violations and the violations themselves on separate lines.  It then reports line count as number of failures.\r\n\r\n{code:title=target/checkstyle/summary.txt}\r\nhadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/OmUtils.java\r\n 49: Unused import - org.apache.hadoop.ozone.om.OMMetadataManager.\r\n{code}\r\n\r\n{code:title=target/checkstyle/failures}\r\n2\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "checkstyle.sh reports wrong failure count"
   },
   {
      "_id": "13259334",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-09-27 18:57:59",
      "description": "{code}\r\n2019-09-27 11:35:19,835 [pool-9-thread-1] ERROR      - Null snapshot location got from OM.\r\n2019-09-27 11:35:19,839 [pool-9-thread-1] INFO       - Calling reprocess on Recon tasks.\r\n2019-09-27 11:35:19,840 [pool-7-thread-1] INFO       - Starting a 'reprocess' run of ContainerKeyMapperTask.\r\n2019-09-27 11:35:20,069 [pool-7-thread-1] INFO       - Creating new Recon Container DB at /tmp/recon/db/recon-container.db_1569609319840\r\n2019-09-27 11:35:20,069 [pool-7-thread-1] INFO       - Cleaning up old Recon Container DB at /tmp/recon/db/recon-container.db_1569609258721.\r\n2019-09-27 11:35:20,144 [pool-9-thread-1] ERROR      - Unexpected error :\r\njava.util.concurrent.ExecutionException: java.lang.NullPointerException\r\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.reInitializeTasks(ReconTaskControllerImpl.java:181)\r\n        at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:333)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\r\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.hadoop.ozone.recon.tasks.ContainerKeyMapperTask.reprocess(ContainerKeyMapperTask.java:81)\r\n        at org.apache.hadoop.ozone.recon.tasks.ReconTaskControllerImpl.lambda$reInitializeTasks$3(ReconTaskControllerImpl.java:176)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon does not handle the NULL snapshot from OM DB cleanly."
   },
   {
      "_id": "13259166",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-09-27 01:17:59",
      "description": "Replication of Container fails with \"Only closed containers could be exported\"\r\n\r\ncc: [~nanda]\r\n\r\n{code}\r\n2019-09-26 15:00:17,640 [grpc-default-executor-13] INFO  replication.GrpcReplicationService (GrpcReplicationService.java:download(57)) - Streaming container data (37) to other\r\ndatanode\r\nSep 26, 2019 3:00:17 PM org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor run\r\nSEVERE: Exception while executing runnable org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed@70e641f2\r\njava.lang.IllegalStateException: Only closed containers could be exported: ContainerId=37\r\n2019-09-26 15:00:17,644 [grpc-default-executor-17] ERROR replication.GrpcReplicationClient (GrpcReplicationClient.java:onError(142)) - Container download was unsuccessfull\r\n        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:527)\r\norg.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNKNOWN\r\n        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.exportContainer(KeyValueHandler.java:875)\r\n        at org.apache.ratis.thirdparty.io.grpc.Status.asRuntimeException(Status.java:526)\r\n        at org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.exportContainer(ContainerController.java:134)\r\n        at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onClose(ClientCalls.java:434)\r\n        at org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\r\n.java:64)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\r\n        at org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClient at org.apache.hadoop.hdds.protocol.datanode.proto.IntraDatanodeProtocolServiceGrpc$MethodHandlers.invoke(IntraDatanodeProtocolSCallListener.java:40)\r\nerviceGrpc.java:217)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:678)\r\n        at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls. at org.apache.ratis.thirdparty.io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39)\r\njava:171)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:283)\r\n        at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClient at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:710)\r\nCallListener.java:40)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.ja at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\nva:397)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:459)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:546)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:467)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:584)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n:\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Replication of Container fails with \"Only closed containers could be exported\""
   },
   {
      "_id": "13259144",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-09-26 22:21:11",
      "description": "This jira aims to add more container related metrics to SCM.\r\n Following metrics will be added as part of this jira:\r\n * Number of successful create container calls\r\n * Number of failed create container calls\r\n * Number of successful delete container calls\r\n * Number of failed delete container calls\r\n * Number of list container ops.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Adding container related metrics in SCM"
   },
   {
      "_id": "13259120",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2019-09-26 19:08:08",
      "description": "Currently, when trying to read a key, three requests are sent to the authorizer:\r\nvolume read, bucket read, key read.\r\n\r\n\u00a0\r\n\r\nIt should instead be just one request to the authorizer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Optimize Ozone CLI commands to send one ACL request to authorizers instead of sending multiple requests"
   },
   {
      "_id": "13259087",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-09-26 15:22:57",
      "description": "For applications like Hive/MapReduce to take advantage of the data locality in Ozone, Ozone should return the location of the Ozone blocks. This is needed for better read performance for Hadoop Applications.\r\n{code}\r\n        if (file instanceof LocatedFileStatus) {\r\n          blkLocations = ((LocatedFileStatus) file).getBlockLocations();\r\n        } else {\r\n          blkLocations = fs.getFileBlockLocations(file, 0, length);\r\n        }\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement LocatedFileStatus & getFileBlockLocations to provide node/localization information to Yarn/Mapreduce"
   },
   {
      "_id": "13259080",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-26 14:55:46",
      "description": "HDDS-2101 changed how Ozone filesystem provider is configured.  {{ozone-mr}} tests [started failing|https://github.com/elek/ozone-ci/blob/2f2c99652af6b26a95f08eece9e545f0d72ccf45/pr/pr-hdds-2101-rtz55/acceptance/output.log#L255-L263], but it [wasn't noticed|https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-2101-rtz55/acceptance/result] due to HDDS-2185.\r\n\r\n{code}\r\nRunning command 'ozone fs -mkdir /user'\r\n${output} = mkdir: No FileSystem for scheme \"o3fs\"\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone-mr test fails with No FileSystem for scheme \"o3fs\""
   },
   {
      "_id": "13259059",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-26 13:45:03",
      "description": "Part of the MR tests fail, but it's not reflected in the test report, which shows all green.\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci/blob/679228c146628cd4d1a416e1ffc9c513d19fb43d/pr/pr-hdds-2179-9bnxk/acceptance/output.log#L718-L730}\r\n==============================================================================\r\nhadoop31-createmrenv :: Create directories required for MR test               \r\n==============================================================================\r\nCreate test volume, bucket and key                                    | PASS |\r\n------------------------------------------------------------------------------\r\nCreate user dir for hadoop                                            | FAIL |\r\n1 != 0\r\n------------------------------------------------------------------------------\r\nhadoop31-createmrenv :: Create directories required for MR test       | FAIL |\r\n2 critical tests, 1 passed, 1 failed\r\n2 tests total, 1 passed, 1 failed\r\n==============================================================================\r\nOutput:  /tmp/smoketest/hadoop31/result/robot-hadoop31-hadoop31-createmrenv-scm.xml\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "createmrenv failure not reflected in acceptance test result"
   },
   {
      "_id": "13259017",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-26 10:50:29",
      "description": "Originally ozone scmcli designed to be used only by the developers. A very cryptic name is chosen intentionally to frighten away the beginner users.\r\n\r\nAs we realized recently we started to use \"ozone scmcli\" as a generic admin tool. More and more tools has been added which are useful not only for the developers but for the administrators.\r\n\r\nTherefore I suggest to rename \"ozone scmcli\" to something more meaningful.\r\n\r\nFor example to \"ozone admin\"\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Rename ozone scmcli to ozone admin"
   },
   {
      "_id": "13258924",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-25 23:54:24",
      "description": "Currently, Ozone manager sends \"WRITE\" as ACLType for key create, key delete and bucket create operation. Fix the acl type in all requests to the authorizer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Manager should send correct ACL type in ACL requests to Authorizer"
   },
   {
      "_id": "13258884",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-09-25 19:40:32",
      "description": "{code:title=mvn -f pom.ozone.xml -DskipTests -am -pl :hadoop-hdds-config clean package}\r\n...\r\n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hadoop-hdds-config ---\r\n[INFO] Compiling 3 source files to hadoop-hdds/config/target/test-classes\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-hdds-config: Compilation failure\r\n[ERROR] Can't generate the config file from annotation: hadoop-hdds/config/target/test-classes/ozone-default-generated.xml\r\n{code}\r\n\r\nThe root cause is that new Java (I guess it's 9+, but tried only on 10+) throws a different {{IOException}} subclass: {{NoSuchFileException}} instead of {{FileNotFoundException}}.\r\n\r\n{code}\r\njava.nio.file.NoSuchFileException: hadoop-hdds/config/target/test-classes/ozone-default-generated.xml\r\n\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)\r\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)\r\n\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)\r\n\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219)\r\n\tat java.base/java.nio.file.Files.newByteChannel(Files.java:374)\r\n\tat java.base/java.nio.file.Files.newByteChannel(Files.java:425)\r\n\tat java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420)\r\n\tat java.base/java.nio.file.Files.newInputStream(Files.java:159)\r\n\tat jdk.compiler/com.sun.tools.javac.file.PathFileObject.openInputStream(PathFileObject.java:461)\r\n\tat java.compiler@13/javax.tools.ForwardingFileObject.openInputStream(ForwardingFileObject.java:74)\r\n\tat org.apache.hadoop.hdds.conf.ConfigFileGenerator.process(ConfigFileGenerator.java:62)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "ConfigFileGenerator fails with Java 10 or newer"
   },
   {
      "_id": "13258559",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-09-24 14:01:58",
      "description": "Test summaries point to wrong locations, eg.:\r\n\r\n{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/trunk/trunk-nightly-20190924-mj2km/integration/summary.md}\r\n * [org.apache.hadoop.ozone.scm.node.TestQueryNode](/tmp/log/trunk/trunk-nightly-20190924-mj2km/integration/workdir/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestQueryNode.txt) ([output](/tmp/log/trunk/trunk-nightly-20190924-mj2km/integration/workdir/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.node.TestQueryNode-output.txt/))\r\n{code}\r\n\r\nshouldn't include {{/workdir}}, nor {{/tmp/log/}}.\r\n\r\nThe root cause is that Busybox {{realpath}} does not accept options, rather returns absolute path:\r\n\r\n{code:title=elek/ozone-build:20190825-1}\r\n$ cd /etc\r\n$ realpath --relative-to=$(pwd) motd\r\nrealpath: --relative-to=/etc: No such file or directory\r\n/etc/motd\r\n{code}\r\n\r\nIt worked previously because the docker image [was|https://github.com/elek/argo-ozone/commit/bad4b6747fa06c227dfcbff1f098f8d9c8179b79] based on a more complete Linux.\r\n\r\n{code:title=elek/ozone-build:test}\r\n$ cd /etc\r\n$ realpath --relative-to=$(pwd) motd\r\nmotd\r\n{code}\r\n\r\nCC [~elek]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Dangling links in test report due to incompatible realpath"
   },
   {
      "_id": "13258401",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-23 21:37:32",
      "description": "testDoubleBuffer() in\u00a0TestOzoneManagerDoubleBufferWithOMResponse fails with outofmemory exceptions at times in dev machines.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerDoubleBufferWithOMResponse sometimes fails with out of memory error"
   },
   {
      "_id": "13258339",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-23 15:11:16",
      "description": "From the daily build:\r\n\r\n{code}\r\n \tException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/ozone/shaded/org/apache/http/client/utils/URIBuilder\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:138)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)\r\n\tat org.apache.hadoop.fs.shell.CommandWithDestination.getRemoteDestination(CommandWithDestination.java:195)\r\n\tat org.apache.hadoop.fs.shell.CopyCommands$Put.processOptions(CopyCommands.java:259)\r\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:175)\r\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:328)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\r\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:391)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.ozone.shaded.org.apache.http.client.utils.URIBuilder\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\t... 15 more\r\n{code}\r\n\r\nIt can be reproduced locally with executing the tests:\r\n\r\n{code}\r\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-mr/hadoop31\r\n./test.sh\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Hadoop31-mr acceptance test is failing due to the shading"
   },
   {
      "_id": "13258303",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-23 13:54:11",
      "description": "In Hadoop metrics it's possible to register multiple metrics with the same name but with different tags. For example each RpcServere has an own metrics instance in SCM.\r\n\r\n{code}\r\n    \"name\" : \"Hadoop:service=StorageContainerManager,name=RpcActivityForPort9860\",\r\n    \"name\" : \"Hadoop:service=StorageContainerManager,name=RpcActivityForPort9863\",\r\n{code}\r\n\r\nThey are converted by PrometheusSink to a prometheus metric line with proper name and tags. For example:\r\n\r\n{code}\r\nrpc_rpc_queue_time60s_num_ops{port=\"9860\",servername=\"StorageContainerLocationProtocolService\",context=\"rpc\",hostname=\"72736061cbc5\"} 0\r\n{code}\r\n\r\nThe PrometheusSink uses a Map to cache all the recent values but unfortunately the key contains only the name (rpc_rpc_queue_time60s_num_ops in our example) but not the tags (port=...)\r\n\r\nFor this reason if there are multiple metrics with the same name, only the first one will be displayed.\r\n\r\nAs a result in SCM only the metrics of the first RPC server can be exported to the prometheus endpoint. \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Some RPC metrics are missing from SCM prometheus endpoint"
   },
   {
      "_id": "13258288",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-09-23 13:18:00",
      "description": "{code:title=ozone freon ockg}\r\nBucket not found\r\n...\r\nFailures: 0\r\nSuccessful executions: 0\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Freon fails if bucket does not exists"
   },
   {
      "_id": "13258155",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-22 15:39:42",
      "description": "{{om.db.checkpoints}} is filling up fast, we should also clean this up.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "om.db.checkpoints is getting filling up fast"
   },
   {
      "_id": "13258060",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2019-09-20 23:26:55",
      "description": "The output of \"ozone sh key list /vol1/bucket1\" does not include replication factor and it will be good to have it in the output.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add \"Replication factor\" to the output of list keys "
   },
   {
      "_id": "13258055",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-20 23:08:03",
      "description": "To have a single configuration to use across OM cluster, few of the configs like\u00a0\r\n\r\n-OZONE_OM_KERBEROS_KEYTAB_FILE_KEY,-\r\n\r\n-OZONE_OM_KERBEROS_PRINCIPAL_KEY,-\r\n\r\n-OZONE_OM_HTTP_KERBEROS_KEYTAB_FILE,-\r\n\r\n-OZONE_OM_HTTP_KERBEROS_PRINCIPAL_KEY need to support configs which append with service id and node id.-\r\n\r\n\u00a0\r\n\r\nAddressed OM_DB_DIRS, OZONE_OM_ADDRESS_KEY also in this patch.\r\n\r\n\u00a0\r\n\r\nThis Jira is to fix the above configs.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make OM Generic related configuration support HA style config"
   },
   {
      "_id": "13258040",
      "assignee": "xyao",
      "components": [],
      "created": "2019-09-20 21:24:41",
      "description": "This will give us coverage of running basic MR jobs on security enabled OZONE cluster against YARN.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add acceptance test for ozonesecure-mr compose"
   },
   {
      "_id": "13258034",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-09-20 20:03:28",
      "description": "There is a race condition in ProfileServlet. The Servlet member field pid should not be used for local assignment. It could lead to race condition.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Race condition in ProfileServlet#pid"
   },
   {
      "_id": "13258023",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-09-20 19:02:02",
      "description": "JsonUtils#toJsonStringWithDefaultPrettyPrinter() does not validate the Json String\u00a0 before serializing it which could result in Json Injection.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Json Injection in JsonUtils"
   },
   {
      "_id": "13258018",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-09-20 18:43:15",
      "description": "Currently {{checkstyle.sh}} prints files with violations using full path, eg:\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190920-4x9x8/checkstyle/summary.txt}\r\n...\r\n/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadList.java\r\n 23: Unused import - org.apache.hadoop.hdds.client.ReplicationType.\r\n 24: Unused import - org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor.\r\n/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartUploadListParts.java\r\n 23: Unused import - org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType.\r\n/workdir/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmMultipartKeyInfo.java\r\n 19: Unused import - org.apache.hadoop.hdds.client.ReplicationFactor.\r\n 20: Unused import - org.apache.hadoop.hdds.client.ReplicationType.\r\n 26: Unused import - java.time.Instant.\r\n...\r\n{noformat}\r\n\r\n{{/workdir}} is specific to the CI environment.  Similarly, local checkout directory is specific to each developer.\r\n\r\nPrinting only path relative to project root ({{/workdir}} here) would make handling these paths easier (eg. reporting errors in JIRA or opening files locally for editing).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "checkstyle: print filenames relative to project root"
   },
   {
      "_id": "13257783",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-09-19 21:21:15",
      "description": "The cards in HDDS doc pages don't align properly and needs to be fixed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix alignment issues in HDDS doc pages"
   },
   {
      "_id": "13257634",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-19 10:06:35",
      "description": "Unfortunately checkstyle checks didn't work well from HDDS-2106 to HDDS-2119. \r\n\r\nThis patch fixes all the issues which are accidentally merged in the mean time. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Checkstyle issues"
   },
   {
      "_id": "13257571",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-09-19 05:12:02",
      "description": "{code}\r\ndd if=/dev/zero of=testfile bs=1024 count=307200\r\nozone sh key put /vol1/bucket1/key testfile\r\n{code}\r\n\r\n{code}\r\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) at org.apache.hadoop.hdds.scm.storage.BufferPool.allocateBufferIfNeeded(BufferPool.java:66) at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:234) at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129) at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:211) at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193) at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49) at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96) at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:117) at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:55) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone client fails with OOM while writing a large (~300MB) key."
   },
   {
      "_id": "13257570",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-09-19 05:07:10",
      "description": "In XceiverClientRatis.java:221, we have the following snippet where we have a DEBUG line that prints out the entire Container Request proto. \r\n\r\n{code}\r\n      ContainerCommandRequestProto finalPayload =\r\n          ContainerCommandRequestProto.newBuilder(request)\r\n              .setTraceID(TracingUtil.exportCurrentSpan())\r\n              .build();\r\n      boolean isReadOnlyRequest = HddsUtils.isReadOnly(finalPayload);\r\n      ByteString byteString = finalPayload.toByteString();\r\n      LOG.debug(\"sendCommandAsync {} {}\", isReadOnlyRequest, finalPayload);\r\n      return isReadOnlyRequest ?\r\n          getClient().sendReadOnlyAsync(() -> byteString) :\r\n          getClient().sendAsync(() -> byteString);\r\n{code}\r\n\r\nThis causes OOM while writing large (~300MB) keys. \r\n\r\n{code}\r\nSLF4J: Failed toString() invocation on an object of type [org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos$ContainerCommandRequestProto]\r\nReported exception:\r\njava.lang.OutOfMemoryError: Java heap space\r\n\tat java.util.Arrays.copyOf(Arrays.java:3332)\r\n\tat java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)\r\n\tat java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:649)\r\n\tat java.lang.StringBuilder.append(StringBuilder.java:202)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormatEscaper.escapeBytes(TextFormatEscaper.java:75)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormatEscaper.escapeBytes(TextFormatEscaper.java:94)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.escapeBytes(TextFormat.java:1836)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:436)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:376)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:338)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.print(TextFormat.java:325)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:449)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:376)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:338)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.print(TextFormat.java:325)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat$Printer.access$000(TextFormat.java:307)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.print(TextFormat.java:68)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.TextFormat.printToString(TextFormat.java:148)\r\n\tat org.apache.ratis.thirdparty.com.google.protobuf.AbstractMessage.toString(AbstractMessage.java:117)\r\n\tat org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)\r\n\tat org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)\r\n\tat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)\r\n\tat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)\r\n\tat org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)\r\n\tat org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:252)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientRatis.sendRequestAsync(XceiverClientRatis.java:221)\r\n\tat org.apache.hadoop.hdds.scm.XceiverClientRatis.sendCommandAsync(XceiverClientRatis.java:302)\r\n\tat org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:310)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:601)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:459)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:240)\r\n\tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:129)\r\nSLF4J: Failed toString() invocation on an object of type [org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos$ContainerCommandRequestProto]\r\nReported exception:\r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone client prints the entire request payload in DEBUG level."
   },
   {
      "_id": "13257519",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-09-18 21:37:00",
      "description": "The following dependency versions have known security vulnerabilities. We should update them to recent/ later versions.\r\n * Apache Thrift 0.11.0\r\n * Apache Zookeeper 3.4.13\r\n * Jetty Servlet 9.3.24",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update dependency versions to avoid security vulnerabilities"
   },
   {
      "_id": "13257511",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-09-18 21:17:50",
      "description": "Findbugs has been marked deprecated and all future work is now happening under SpotBugs project.\r\n\r\nThis Jira is to investigate and possibly transition to Spotbugs in Ozone\r\n\r\n\u00a0\r\n\r\nRef1 -\u00a0[https://mailman.cs.umd.edu/pipermail/findbugs-discuss/2017-September/004383.html]\r\n\r\nRef2 -\u00a0[https://spotbugs.github.io/]\r\n\r\n\u00a0\r\n\r\nA turn off for developers is that IntelliJ does not yet have a plugin for Spotbugs -\u00a0[https://youtrack.jetbrains.com/issue/IDEA-201846]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Replace findbugs with spotbugs"
   },
   {
      "_id": "13257434",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-09-18 16:45:22",
      "description": "Include {{*.dumpstream}} in the unit test report, which may help finding out the cause of {{Corrupted STDOUT}} warning of forked JVM.\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci/blob/5429d0982c3b13d311ec353dba198f2f5253757c/pr/pr-hdds-2141-4zm8s/unit/output.log#L333-L334}\r\n[INFO] Running org.apache.hadoop.utils.TestMetadataStore\r\n[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /workdir/hadoop-hdds/common/target/surefire-reports/2019-09-18T12-58-05_531-jvmRun1.dumpstream\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Include dumpstream in test report"
   },
   {
      "_id": "13257427",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-09-18 16:24:45",
      "description": "Currently, the watchForCommit calls from client to Ratis server for All replicated semantics happens when the max buffer limit is reached which can potentially be called 4 times as per the default configs for a single full block write. The idea here is inspect and add optimizations to reduce the no of watchForCommit calls.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Optimize block write path performance by reducing no of watchForCommit calls"
   },
   {
      "_id": "13257236",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-09-17 23:24:22",
      "description": "Failing with below error:\r\nCaused by: Client cannot authenticate via:[TOKEN, KERBEROS]\r\norg.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\r\nat org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)\r\nat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)\r\nat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:617)\r\nat org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:411)\r\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:804)\r\nat org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\nat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:800)\r\nat org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\r\nat org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1403)\r\nat org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\nat com.sun.proxy.$Proxy79.submitRequest(Unknown Source)\r\nat sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\nat com.sun.proxy.$Proxy79.submitRequest(Unknown Source)\r\nat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:332)\r\nat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1163)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\r\nat com.sun.proxy.$Proxy80.getServiceList(Unknown Source)\r\nat org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:248)\r\nat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:167)\r\nat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:256)\r\nat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:239)\r\nat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:203)\r\nat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:161)\r\nat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\r\nat org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:102)\r\nat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:155)\r\nat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\nat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\nat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\nat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\nat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\nat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\nat org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:268)\r\nat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)\r\nat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)\r\nat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\nat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)\r\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:237)\r\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:230)\r\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:218)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "MR job failing on secure Ozone cluster"
   },
   {
      "_id": "13257232",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-09-17 22:40:49",
      "description": "Rename classes under package org.apache.hadoop.utils -> org.apache.hadoop.hdds.utils in hadoop-hdds-common\r\n\r\n\u00a0\r\n\r\nNow, with current way, we might collide with hadoop classes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Rename classes under package org.apache.hadoop.utils"
   },
   {
      "_id": "13257087",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-17 11:28:54",
      "description": "AbortMultipartUpload failure count can be higher than request count.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "OM metrics mismatch (abort multipart request)"
   },
   {
      "_id": "13257063",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-17 08:59:03",
      "description": "Total number of operations is missing from some metrics graphs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Missing total number of operations"
   },
   {
      "_id": "13257000",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-09-17 01:16:53",
      "description": "The following Ozone dependencies have known security vulnerabilities. We should update them to newer/ latest versions.\r\n * Apache Common BeanUtils version 1.9.3\r\n * Fasterxml Jackson version 2.9.5",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update BeanUtils and Jackson Databind dependency versions"
   },
   {
      "_id": "13256936",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-16 18:00:49",
      "description": "Total OM bucket operations may be higher than sum of counts for individual operation type, because S3 bucket operations are displayed in separate charts.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "OM bucket operations do not add up"
   },
   {
      "_id": "13256866",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-16 12:25:54",
      "description": "Block allocation count and block allocation failure count are shown in separate graphs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "OM block allocation metric not paired with its failures"
   },
   {
      "_id": "13256838",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-16 10:18:10",
      "description": "{{incNumCommitMultipartUploadPartFails()}} increments {{numInitiateMultipartUploadFails}} instead of the counter for commit failures.\r\n\r\nhttps://github.com/apache/hadoop/blob/85b1c728e4ed22f03db255f5ef34a2a79eb20d52/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java#L310-L312",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "OM Metric mismatch (MultipartUpload failures)"
   },
   {
      "_id": "13256832",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-09-16 09:42:51",
      "description": "Ozone Manager Metrics seems to include an odd empty request type \"s\".",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM Metrics graphs include empty request type"
   },
   {
      "_id": "13256684",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-14 09:43:48",
      "description": "{{TestKeyValueContainer}} is failing with the following exception \r\n{noformat}\r\n[ERROR] testContainerImportExport(org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer)  Time elapsed: 0.173 s  <<< ERROR!\r\njava.lang.NullPointerException\r\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil.parseKVContainerData(KeyValueContainerUtil.java:201)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.importContainerData(KeyValueContainer.java:500)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.TestKeyValueContainer.testContainerImportExport(TestKeyValueContainer.java:235)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestKeyValueContainer is failing"
   },
   {
      "_id": "13256632",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-13 22:20:12",
      "description": "The build fails with the {{dist}} profile. Details in a comment below.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Using dist profile fails with pom.ozone.xml as parent pom"
   },
   {
      "_id": "13256599",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-09-13 19:14:16",
      "description": "There are two doc pages for tools:\r\n * docs/beyond/tools.html\r\n * docs/tools.html\r\n\r\nThe latter is more detailed (has subpages for several tools), but it is not reachable (even indirectly) from the start page.\u00a0 Not sure if this is intentional.\r\n\r\nOn a related note, it has two \"Testing tools\" sub-pages. One of them is empty and should be removed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Detailed Tools doc not reachable"
   },
   {
      "_id": "13256516",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-09-13 12:17:30",
      "description": "_Next>>_ links at the bottom of some documentation pages seem to be out of order.\r\n\r\n * _Simple Single Ozone_ (\"easy start\") should link to one of the intermediate level pages, but has no _Next_ link\r\n * _Building From Sources_ (ninja) should be the last (no _Next_ link), but points to _Minikube_ (intermediate)\r\n * _Pseudo-cluster_ (intermediate) should point to the ninja level, but leads to _Simple Single Ozone_ (easy start)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Random next links "
   },
   {
      "_id": "13256495",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-09-13 09:47:58",
      "description": "Ozone logo at the top left is broken on sub-pages, eg. _Recipes / Monitoring with Prometheus_ and _Programming Interfaces / Java API_.\r\n\r\n !broken_image.png! ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Broken logo image on category sub-pages"
   },
   {
      "_id": "13256411",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-09-12 19:24:59",
      "description": "We need a shaded Ozonefs jar that does not include Hadoop ecosystem components (Hadoop, HDFS, Ratis, Zookeeper).\r\n\r\nA common expected use case for Ozone is Hadoop clients (3.2.0 and later) wanting to access Ozone via the Ozone Filesystem interface. For these clients, we want to add Ozone file system jar to the classpath, however we want to use Hadoop ecosystem dependencies that are `provided` and already expected to be in the client classpath.\r\n\r\nNote that this is different from the legacy jar which bundles a shaded Hadoop 3.2.0.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Create a shaded ozone filesystem (client) jar"
   },
   {
      "_id": "13256402",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-12 18:06:18",
      "description": "We have two kind of ozone file system jars: current and legacy. current is designed to work only with exactly the same hadoop version which is used for compilation (3.2 as of now).\r\n\r\nBut as of now the hadoop classes are included in the current jar which is not necessary as the jar is expected to be used in an environment where  the hadoop classes (exactly the same hadoop classes) are already there. They can be excluded.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hadoop classes from ozonefs-current jar"
   },
   {
      "_id": "13256305",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-09-12 09:02:26",
      "description": "The issue seems to be happening because the below precondition check fails in case two writeChunk gets executed in parallel and the runtime exception thrown is handled correctly in ContainerStateMachine.\r\n\r\n\u00a0\r\n\r\nHddsDispatcher.java:239\r\n{code:java}\r\nPreconditions\r\n    .checkArgument(!container2BCSIDMap.containsKey(containerID));\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ContainerStateMachine#writeStateMachineData times out"
   },
   {
      "_id": "13256106",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-09-11 12:35:54",
      "description": "VULNERABILITY DETAILS\r\nThere is a way to bypass anti-XSS filter for DOM XSS exploiting a \"window.location.href\".\r\n\r\nConsidering a typical URL:\r\n\r\nscheme://domain:port/path?query_string#fragment_id\r\n\r\nBrowsers encode correctly both \"path\" and \"query_string\", but not the \"fragment_id\".\u00a0\r\n\r\nSo if used \"fragment_id\" the vector is also not logged on Web Server.\r\n\r\nVERSION\r\nChrome Version: 10.0.648.134 (Official Build 77917) beta\r\n\r\nREPRODUCTION CASE\r\nThis is an index.html page:\r\n\r\n\r\n{code:java}\r\naws s3api --endpoint <script>document.write(window.location.href.replace(\"static/\", \"\"))</script> create-bucket --bucket=wordcount</pre>\r\n{code}\r\n\r\n\r\nThe attack vector is:\r\nindex.html?#<script>alert('XSS');</script>\r\n\r\n* PoC:\r\nFor your convenience, a minimalist PoC is located on:\r\nhttp://security.onofri.org/xss_location.html?#<script>alert('XSS');</script>\r\n\r\n* References\r\n- DOM Based Cross-Site Scripting or XSS of the Third Kind - http://www.webappsec.org/projects/articles/071105.shtml\r\n\r\n\r\nreference:-\u00a0\r\n\r\nhttps://bugs.chromium.org/p/chromium/issues/detail?id=76796",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "XSS fragments can be injected to the S3g landing page  "
   },
   {
      "_id": "13256103",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333902",
            "id": "12333902",
            "name": "Native",
            "description": "Native code components"
         }
      ],
      "created": "2019-09-11 12:18:57",
      "description": "The LOC 324 in the file\u00a0[ProfileServlet.java|https://github.com/apache/hadoop/blob/217bdbd940a96986df3b96899b43caae2b5a9ed2/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/ProfileServlet.java]\u00a0is prone to an arbitrary file download:-\r\n{code:java}\r\nprotected void doGetDownload(String fileName, final HttpServletRequest req,      final HttpServletResponse resp) throws IOException {\r\n\r\nFile requestedFile = ProfileServlet.OUTPUT_DIR.resolve(fileName).toAbsolutePath().toFile();{code}\r\nAs the String fileName is directly considered as the requested file.\r\n\r\n\u00a0\r\n\r\nWhich is called at LOC 180 with HTTP request directly passed:-\r\n{code:java}\r\nif (req.getParameter(\"file\") != null) {      doGetDownload(req.getParameter(\"file\"), req, resp);      \r\nreturn;    \r\n}\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Arbitrary file can be downloaded with the help of ProfilerServlet"
   },
   {
      "_id": "13256092",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333909",
            "id": "12333909",
            "name": "SCM Client",
            "description": "HDDS client"
         }
      ],
      "created": "2019-09-11 10:21:00",
      "description": "Extract typesafe config related to HDDS client with prefix {{scm.container.client}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor scm.container.client config"
   },
   {
      "_id": "13256040",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-11 07:22:43",
      "description": "{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190910-vk757/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.scm.TestContainerSmallFile.txt}\r\nTests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 384.415 s <<< FAILURE! - in org.apache.hadoop.ozone.scm.TestContainerSmallFile\r\ntestReadWriteWithBCSId(org.apache.hadoop.ozone.scm.TestContainerSmallFile)  Time elapsed: 364.439 s  <<< ERROR!\r\njava.io.IOException: \r\nFailed to command cmdType: PutSmallFile\r\n...\r\nCaused by: org.apache.ratis.protocol.AlreadyClosedException: client-8C96F0B39BBE->72902ab5-0e57-412f-a398-68ab8a9029d1 is closed.\r\n{code}\r\n\r\nHi [~shashikant], this failure is consistently reproducible starting with the [commit|https://github.com/apache/hadoop/commit/469165e6f29] for HDDS-1843.  Can you please check?",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestContainerSmallFile#testReadWriteWithBCSId failure"
   },
   {
      "_id": "13255980",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-09-10 21:38:48",
      "description": "In an unsecure environment, the datanodes try upto 10 times after waiting for 1000 milliseconds each time before throwing this error:\r\n{code:java}\r\nUnable to communicate to SCM server at scm:9861 for past 0 seconds.\r\njava.net.ConnectException: Call From scm/10.65.36.118 to scm:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\tat com.sun.proxy.$Proxy33.getVersion(Unknown Source)\r\n\tat org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:112)\r\n\tat org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)\r\n\tat org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.ConnectException: Connection refused\r\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1403)\r\n\t... 13 more\r\n{code}\r\nThe datanodes should try forever to connect with SCM and not throw any errors.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanodes should retry forever to connect to SCM in an unsecure environment"
   },
   {
      "_id": "13255751",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-09 22:32:47",
      "description": "Ozone uses hadoop as a dependency. The dependency defined on multiple level:\r\n\r\n 1. the hadoop artifacts are defined in the <dependency> sections\r\n 2. both hadoop-ozone and hadoop-hdds projects uses \"hadoop-project\" as the parent\r\n\r\nAs we already have a slightly different assembly process it could be more resilient to use a dedicated parent project instead of the hadoop one. With this approach it will be easier to upgrade the versions as we don't need to be careful about the pom contents only about the used dependencies.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Avoid usage of hadoop projects as parent of hdds/ozone"
   },
   {
      "_id": "13255722",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-09 20:30:15",
      "description": "{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190907-l8mkd/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.container.TestContainerReplication.txt}\r\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 12.771 s <<< FAILURE! - in org.apache.hadoop.ozone.container.TestContainerReplication\r\ntestContainerReplication(org.apache.hadoop.ozone.container.TestContainerReplication)  Time elapsed: 12.702 s  <<< FAILURE!\r\njava.lang.AssertionError: Container is not replicated to the destination datanode\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertNotNull(Assert.java:621)\r\n\tat org.apache.hadoop.ozone.container.TestContainerReplication.testContainerReplication(TestContainerReplication.java:153)\r\n{code}\r\n\r\ncaused by:\r\n\r\n{code:title=https://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190907-l8mkd/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.container.TestContainerReplication-output.txt}\r\njava.lang.IllegalStateException: Only closed containers could be exported: ContainerId=1\r\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.exportContainerData(KeyValueContainer.java:525)\r\n\tat org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.exportContainer(KeyValueHandler.java:875)\r\n\tat org.apache.hadoop.ozone.container.ozoneimpl.ContainerController.exportContainer(ContainerController.java:134)\r\n\tat org.apache.hadoop.ozone.container.replication.OnDemandContainerReplicationSource.copyData(OnDemandContainerReplicationSource.java:64)\r\n\tat org.apache.hadoop.ozone.container.replication.GrpcReplicationService.download(GrpcReplicationService.java:63)\r\n{code}\r\n\r\nContainer is in unhealthy state because pipeline is not found for it in {{CloseContainerCommandHandler}}.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestContainerReplication fails due to unhealthy container"
   },
   {
      "_id": "13255403",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-09-07 04:22:01",
      "description": "We don't have a filesystem provider in META-INF. \r\ni.e. following file doesn't exist.\r\n{{hadoop-ozone/ozonefs/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}}\r\n\r\nSee for example\r\n{{hadoop-tools/hadoop-aws/src/main/resources/META-INF/services/org.apache.hadoop.fs.FileSystem}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Ozone filesystem provider doesn't exist"
   },
   {
      "_id": "13255347",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2019-09-06 18:51:06",
      "description": "*Exception Trace*\r\n{code}\r\nlog4j:ERROR Could not read configuration file from URL [file:/etc/ozone/conf/ozone-shell-log4j.properties].\r\njava.io.FileNotFoundException: /etc/ozone/conf/ozone-shell-log4j.properties (No such file or directory)\r\n\tat java.io.FileInputStream.open0(Native Method)\r\n\tat java.io.FileInputStream.open(FileInputStream.java:195)\r\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138)\r\n\tat java.io.FileInputStream.<init>(FileInputStream.java:93)\r\n\tat sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)\r\n\tat sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)\r\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:557)\r\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\r\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\r\n\tat org.slf4j.impl.Log4jLoggerFactory.<init>(Log4jLoggerFactory.java:66)\r\n\tat org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)\r\n\tat org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)\r\n\tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)\r\n\tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)\r\n\tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:412)\r\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)\r\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\r\n\tat org.apache.hadoop.ozone.web.ozShell.Shell.<clinit>(Shell.java:35)\r\nlog4j:ERROR Ignoring configuration file [file:/etc/ozone/conf/ozone-shell-log4j.properties].\r\nlog4j:WARN No appenders could be found for logger (io.jaegertracing.thrift.internal.senders.ThriftSenderFactory).\r\nlog4j:WARN Please initialize the log4j system properly.\r\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\r\n{\r\n  \"metadata\" : { },\r\n  \"name\" : \"vol-test-putfile-1567740142\",\r\n  \"admin\" : \"root\",\r\n  \"owner\" : \"root\",\r\n  \"creationTime\" : 1567740146501,\r\n  \"acls\" : [ {\r\n    \"type\" : \"USER\",\r\n    \"name\" : \"root\",\r\n    \"aclScope\" : \"ACCESS\",\r\n    \"aclList\" : [ \"ALL\" ]\r\n  }, {\r\n    \"type\" : \"GROUP\",\r\n    \"name\" : \"root\",\r\n    \"aclScope\" : \"ACCESS\",\r\n    \"aclList\" : [ \"ALL\" ]\r\n  } ],\r\n  \"quota\" : 1152921504606846976\r\n}\r\n{code}\r\n\r\n\r\n*Fix*\r\nWhen a log4j file is not present, the default should be console.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone shell command prints out ERROR when the log4j file is not present."
   },
   {
      "_id": "13255316",
      "assignee": "xyao",
      "components": [],
      "created": "2019-09-06 16:26:19",
      "description": "Current Ozone Native ACL APIs document looks like below, the AddAcl is missing.\r\n\r\n\u00a0\r\nh3. Ozone Native ACL APIs\r\n\r\nThe ACLs can be manipulated by a set of APIs supported by Ozone. The APIs supported are:\r\n # *SetAcl*\u00a0\u2013 This API will take user principal, the name, type of the ozone object and a list of ACLs.\r\n # *GetAcl*\u00a0\u2013 This API will take the name and type of the ozone object and will return a list of ACLs.\r\n # *RemoveAcl*\u00a0- This API will take the name, type of the ozone object and the ACL that has to be removed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone ACL document missing AddAcl API"
   },
   {
      "_id": "13254999",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-05 12:58:09",
      "description": "{{\"smoketests.ozonesecure-s3.MultipartUpload.Test Multipart Upload with the simplified aws s3 cp API\"}} acceptance test is failing.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Failing acceptance test - smoketests.ozonesecure-s3.MultipartUpload"
   },
   {
      "_id": "13254906",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-09-05 03:43:50",
      "description": "We have a hard-coded config key in the {{ChunkManagerFactory.java.}}\r\n\r\n\u00a0\r\n{code}\r\nboolean scrubber = config.getBoolean(\r\n \"hdds.containerscrub.enabled\",\r\n false);\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove the hard coded config key in ChunkManager"
   },
   {
      "_id": "13254529",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-09-04 02:14:17",
      "description": "\r\n{code:java}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider\r\n-------------------------------------------------------------------------------\r\nTests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.374 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider\r\ntestCreatePipelinesDnExclude(org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider)  Time elapsed: 0.044 s  <<< ERROR!\r\norg.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 2 nodes.\r\n\tat org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:151)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineProvider.testCreatePipelinesDnExclude(TestRatisPipelineProvider.java:182)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n\r\n\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestRatisPipelineProvider#testCreatePipelinesDnExclude"
   },
   {
      "_id": "13254474",
      "assignee": "xyao",
      "components": [],
      "created": "2019-09-03 20:26:39",
      "description": "[https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-1909-plfbr/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.om.TestSecureOzoneManager.txt]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestSecureOzoneManager"
   },
   {
      "_id": "13254472",
      "assignee": "xyao",
      "components": [],
      "created": "2019-09-03 20:25:22",
      "description": "[https://github.com/elek/ozone-ci/blob/master/pr/pr-hdds-1909-plfbr/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestSecureOzoneCluster.txt]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Get/Renew DelegationToken NPE after HDDS-1909"
   },
   {
      "_id": "13254328",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-09-03 06:11:02",
      "description": "Read fails as the client is not able to read the block from the container.\r\n\r\n{code}\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Unable to find the block with bcsID 2515 .Container 7 bcsId is 0.\r\n        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:536)\r\n        at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.lambd2a0$getValid1a9to-08-30 12:51:20,081 | INFO  | SCMAudit | user=msingh | ip=192.168.0.r103 |List$0(ContainerP\r\nrotocolCalls.java:569)\r\n{code}\r\n\r\n\r\nThe client eventually exits here\r\n{code}\r\n2019-08-30 12:51:20,081 [pool-224-thread-6] ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:readData(176)) - LOADGEN: Read key:pool-224-thread-6_330651 failed with ex\r\nception\r\nERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(121)) - LOADGEN: Exiting due to exception\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Read fails because the block cannot be located in the container"
   },
   {
      "_id": "13254203",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-09-02 09:49:45",
      "description": "As you can see in the attached screenshot the OzoneManager.createBucket (server side) tracing information is the children of the freon.createBucket instead of the freon OzoneManagerProtocolPB.submitRequest.\r\n\r\nTo avoid confusion the hierarchy should be fixed (Most probably we generate the child span AFTER we already serialized the parent one to the message) ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Tracing in OzoneManager call is propagated with wrong parent"
   },
   {
      "_id": "13254198",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-09-02 09:21:41",
      "description": "We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).\r\n\r\nSCMSecurityProtocol.proto is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make SCMSecurityProtocol message based"
   },
   {
      "_id": "13254197",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-09-02 09:20:48",
      "description": "We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).\r\n\r\nStorageContainerLocationProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make StorageContainerLocationProtocolService message based"
   },
   {
      "_id": "13254196",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-02 09:19:43",
      "description": "With Ozone insight we can print out all the logs / metrics of one specific component s (eg. scm.node-manager or scm.node-manager).\r\n\r\nIt would be great to support additional filtering capabilities where the output is filtered based on specific keys.\r\n\r\nFor example to print out all of the logs related to one datanode or related to one type of RPC request.\r\n\r\nFilter should be a key value map (eg. --filter datanode=sjdhfhf,rpc=createChunk) which can be defined in the ozone insight CLI.\r\n\r\nAs we have no option to add additional tags to the logs (it may be supported by log4j2 but not with slf4k), the first implementation can be implemented by pattern matching.\r\n\r\nFor example in SCMNodeManager.processNodeReport contains trace/debug logs which includes the \" [datanode={}]\" part. This formatting convention can be used to print out the only the related information. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support filters in ozone insight point"
   },
   {
      "_id": "13254195",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-02 09:15:13",
      "description": "Wit the first implementation of ozone insight tool we had a demo insight-point to debug Ratis pipelines. It was not stable enough to include in the first patch, this patch is about fixing it.\r\n\r\nThe goal is to implement a new insight point (eg. datanode.pipeline) which can show information about one pipeline.\r\n\r\nIt can be done with retrieving the hosts of the pipeline and generate the loggers metrics (InsightPoint.getRelatedLoggers and InsightPoint.getMetrics) based on the pipeline information (same loggers should be displayed from all the three datanodes.\r\n\r\nThe pipeline id can be defined as a filter parameter which (in this case) should be required.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create insight point to debug one specific pipeline"
   },
   {
      "_id": "13254192",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-09-02 09:10:33",
      "description": "We started to use a generic pattern where we have only one method in the grpc service and the main message contains all the required common information (eg. tracing).\r\n\r\nStorageContainerDatanodeProtocolService is not yet migrated to this approach. To make our generic debug tool more powerful and unify our protocols I suggest to transform this protocol as well.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make StorageContainerDatanodeProtocolService message based"
   },
   {
      "_id": "13254191",
      "assignee": "elek",
      "components": [],
      "created": "2019-09-02 09:07:42",
      "description": "We started to use a message based GRPC approach. Wen have only one method and the requests are routed based on a \"type\" field in the proto message. \r\n\r\nFor example in OM protocol:\r\n\r\n{code}\r\n/**\r\n The OM service that takes care of Ozone namespace.\r\n*/\r\nservice OzoneManagerService {\r\n    // A client-to-OM RPC to send client requests to OM Ratis server\r\n    rpc submitRequest(OMRequest)\r\n          returns(OMResponse);\r\n}\r\n{code}\r\n\r\nAnd \r\n\r\n{code}\r\n\r\nmessage OMRequest {\r\n  required Type cmdType = 1; // Type of the command\r\n\r\n...\r\n{code}\r\n\r\nThis approach makes it possible to use the same code to process incoming messages in the server side.\r\n\r\nScmBlockLocationProtocolServerSideTranslatorPB.send method contains the logic of:\r\n\r\n * Logging the request/response message (can be displayed with ozone insight)\r\n * Updated metrics\r\n * Handle open tracing context propagation.\r\n\r\n\r\nThese functions are generic. For example OzoneManagerProtocolServerSideTranslatorPB use the same (=similar) code.\r\n\r\nThe goal in this jira is to provide a generic utility and move the common code for tracing/request logging/response logging/metrics calculation to a common utility which can be used from all the ServerSide translators.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create generic service facade with tracing/metrics/logging support"
   },
   {
      "_id": "13254190",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-09-02 08:59:56",
      "description": "To improve the observability is a key requirement to achieve better correctness and performance with Ozone.\r\n\r\nThis jira collects some of the tasks which can provide better visibility to the ozone internals.\r\n\r\nWe have two main tools:\r\n\r\n * Distributed tracing (opentracing) can help to detected performance battlenecks\r\n * Ozone insight tool (a simple cli frontend for Hadoop metrics and log4j logging) can help to get better understanding about the current state/behavior of specific components.\r\n\r\nBoth of them can be improved to make it more powerful.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve the observability inside Ozone"
   },
   {
      "_id": "13253920",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-30 12:23:20",
      "description": "An untracked {{audit.log}} file is created during integration test run.  Eg:\r\n\r\n{code}\r\n$ mvn -Phdds -pl :hadoop-ozone-integration-test test -Dtest=Test2WayCommitInRatis\r\n...\r\n$ git status\r\n...\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n\thadoop-ozone/integration-test/audit.log\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Integration tests create untracked file audit.log"
   },
   {
      "_id": "13253878",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-08-30 09:26:41",
      "description": "HDDS-1094 introduced a new config key ([hdds.container.chunk.persistdata|https://github.com/apache/hadoop/blob/96f7dc1992246a16031f613e55dc39ea0d64acd1/hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/HddsConfigKeys.java#L241-L245]), which needs to be added to {{ozone-default.xml}}, too.\r\n\r\nhttps://github.com/elek/ozone-ci/blob/master/trunk/trunk-nightly-20190830-rr75b/integration/hadoop-ozone/integration-test/org.apache.hadoop.ozone.TestOzoneConfigurationFields.txt",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add hdds.container.chunk.persistdata as exception to TestOzoneConfigurationFields"
   },
   {
      "_id": "13253864",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-30 08:25:11",
      "description": "With HDDS-2058 the Ozone (source) release package doesn't contains the hadoop sources any more. We need to create an adjusted LICENSE file for the Ozone source package (We already created a specific LICENSE file for the binary package which is not changed).\r\n\r\nIn the new LICENSE file we should include entries only for the sources which are part of the Ozone release.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Create Ozone specific LICENSE file for the Ozone source and binary packages"
   },
   {
      "_id": "13253589",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-28 23:26:56",
      "description": "{{TestOzoneManagerRatisServer}} is failing on trunk with the following error\r\n{noformat}\r\n[ERROR] verifyRaftGroupIdGenerationWithCustomOmServiceId(org.apache.hadoop.ozone.om.ratis.TestOzoneManagerRatisServer)  Time elapsed: 0.418 s  <<< ERROR!\r\norg.apache.hadoop.metrics2.MetricsException: Metrics source OzoneManagerDoubleBufferMetrics already exists!\r\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)\r\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)\r\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)\r\n\tat org.apache.hadoop.ozone.om.ratis.metrics.OzoneManagerDoubleBufferMetrics.create(OzoneManagerDoubleBufferMetrics.java:50)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.<init>(OzoneManagerDoubleBuffer.java:110)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerDoubleBuffer.<init>(OzoneManagerDoubleBuffer.java:88)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.<init>(OzoneManagerStateMachine.java:87)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.getStateMachine(OzoneManagerRatisServer.java:314)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.<init>(OzoneManagerRatisServer.java:244)\r\n\tat org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisServer.newOMRatisServer(OzoneManagerRatisServer.java:302)\r\n\tat org.apache.hadoop.ozone.om.ratis.TestOzoneManagerRatisServer.verifyRaftGroupIdGenerationWithCustomOmServiceId(TestOzoneManagerRatisServer.java:209)\r\n...\r\n{noformat}\r\n\r\n(Thanks [~nandakumar131] for the stack trace.)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix TestOzoneManagerRatisServer failure"
   },
   {
      "_id": "13253586",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2019-08-28 23:19:13",
      "description": "Currently, certificates and keys are stored in ozone.metadata.dirs and this needs to be moved to specific metadata dir for each service.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triage"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Separate the metadata directories to store security certificates and keys for different services"
   },
   {
      "_id": "13253564",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-08-28 19:41:56",
      "description": "{code}\r\nhadoop-hdds/docs/target/rat.txt: !????? /var/jenkins_home/workspace/ozone/hadoop-hdds/docs/content/design/decommissioning.md\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Rat check failure in decommissioning.md"
   },
   {
      "_id": "13253554",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-08-28 18:03:28",
      "description": "The following error is seen while compiling {{ozone-recon-web}}\r\n{noformat}\r\n[INFO] Running 'yarn install' in /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web\r\n[INFO] yarn install v1.9.2\r\n[INFO] [1/4] Resolving packages...\r\n[INFO] [2/4] Fetching packages...\r\n[ERROR] (node:31190) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.\r\n[INFO] [3/4] Linking dependencies...\r\n[ERROR] warning \" > less-loader@5.0.0\" has unmet peer dependency \"webpack@^2.0.0 || ^3.0.0 || ^4.0.0\".\r\n[INFO] [4/4] Building fresh packages...\r\n[ERROR] warning Error running install script for optional dependency: \"/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents: Command failed.\r\n[ERROR] Exit code: 1\r\n[ERROR] Command: node install\r\n[ERROR] Arguments:\r\n[ERROR] Directory: /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents\r\n[ERROR] Output:\r\n[ERROR] node-pre-gyp info it worked if it ends with ok\r\n[INFO] info This module is OPTIONAL, you can safely ignore this error\r\n[ERROR] node-pre-gyp info using node-pre-gyp@0.12.0\r\n[ERROR] node-pre-gyp info using node@12.1.0 | darwin | x64\r\n[ERROR] node-pre-gyp WARN Using request for node-pre-gyp https download\r\n[ERROR] node-pre-gyp info check checked for \\\"/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/lib/binding/Release/node-v72-darwin-x64/fse.node\\\" (not found)\r\n[ERROR] node-pre-gyp http GET https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz\r\n[ERROR] node-pre-gyp http 404 https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz\r\n[ERROR] node-pre-gyp WARN Tried to download(404): https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz\r\n[ERROR] node-pre-gyp WARN Pre-built binaries not found for fsevents@1.2.8 and node@12.1.0 (node-v72 ABI, unknown) (falling back to source compile with node-gyp)\r\n[ERROR] node-pre-gyp http 404 status code downloading tarball https://fsevents-binaries.s3-us-west-2.amazonaws.com/v1.2.8/fse-v1.2.8-node-v72-darwin-x64.tar.gz\r\n[ERROR] node-pre-gyp ERR! build error\r\n[ERROR] node-pre-gyp ERR! stack Error: Failed to execute 'node-gyp clean' (Error: spawn node-gyp ENOENT)\r\n[ERROR] node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/node_modules/node-pre-gyp/lib/util/compile.js:77:29)\r\n[ERROR] node-pre-gyp ERR! stack     at ChildProcess.emit (events.js:196:13)\r\n[ERROR] node-pre-gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)\r\n[ERROR] node-pre-gyp ERR! stack     at onErrorNT (internal/child_process.js:431:16)\r\n[ERROR] node-pre-gyp ERR! stack     at processTicksAndRejections (internal/process/task_queues.js:84:17)\r\n[ERROR] node-pre-gyp ERR! System Darwin 18.5.0\r\n[ERROR] node-pre-gyp ERR! command \\\"/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/target/node/node\\\" \\\"/Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents/node_modules/node-pre-gyp/bin/node-pre-gyp\\\" \\\"install\\\" \\\"--fallback-to-build\\\"\r\n[ERROR] node-pre-gyp ERR! cwd /Users/nvadivelu/codebase/apache/hadoop/hadoop-ozone/ozone-recon/src/main/resources/webapps/recon/ozone-recon-web/node_modules/fsevents\r\n[ERROR] node-pre-gyp ERR! node -v v12.1.0\r\n[ERROR] node-pre-gyp ERR! node-pre-gyp -v v0.12.0\r\n[ERROR] node-pre-gyp ERR! not ok\r\n[ERROR] Failed to execute 'node-gyp clean' (Error: spawn node-gyp ENOENT)\"\r\n[INFO] Done in 102.54s.\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Error while compiling ozone-recon-web"
   },
   {
      "_id": "13253353",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2019-08-27 22:29:21",
      "description": "{code:java}\r\n10:06:36.585 PM\u00a0\u00a0\u00a0\u00a0ERROR\u00a0\u00a0\u00a0\u00a0HddsDatanodeService\u00a0\u00a0\u00a0\u00a0\r\nError while storing SCM signed certificate.\r\njava.net.ConnectException: Call From jmccarthy-ozone-secure-2.vpc.cloudera.com/10.65.50.127 to jmccarthy-ozone-secure-1.vpc.cloudera.com:9961 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: \u00a0http://wiki.apache.org/hadoop/ConnectionRefused\r\n\u00a0\u00a0\u00a0\u00a0at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u00a0\u00a0\u00a0\u00a0at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u00a0\u00a0\u00a0\u00a0at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u00a0\u00a0\u00a0\u00a0at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\u00a0\u00a0\u00a0\u00a0at com.sun.proxy.$Proxy15.getDataNodeCertificate(Unknown Source)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.hdds.protocolPB.SCMSecurityProtocolClientSideTranslatorPB.getDataNodeCertificateChain(SCMSecurityProtocolClientSideTranslatorPB.java:156)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.getSCMSignedCert(HddsDatanodeService.java:278)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.initializeCertificateClient(HddsDatanodeService.java:248)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:211)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.start(HddsDatanodeService.java:168)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:143)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.call(HddsDatanodeService.java:70)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine.execute(CommandLine.java:1173)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine.access$800(CommandLine.java:141)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine$RunLast.handle(CommandLine.java:1367)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine$RunLast.handle(CommandLine.java:1335)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)\r\n\u00a0\u00a0\u00a0\u00a0at picocli.CommandLine.parseWithHandler(CommandLine.java:1465)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ozone.HddsDatanodeService.main(HddsDatanodeService.java:126)\r\nCaused by: java.net.ConnectException: Connection refused\r\n\u00a0\u00a0\u00a0\u00a0at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n\u00a0\u00a0\u00a0\u00a0at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\r\n\u00a0\u00a0\u00a0\u00a0at org.apache.hadoop.ipc.Client.call(Client.java:1403)\r\n\u00a0\u00a0\u00a0\u00a0... 21 more\r\n{code}\r\n\r\nDatanodes try to get SCM signed certificate for just 10 times with interval of 1 sec. When SCM takes a little longer to come up, datanodes throw an exception and fail.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanodes fail to come up after 10 retries in a secure environment"
   },
   {
      "_id": "13253310",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-27 17:40:14",
      "description": "If any container in the sample cluster [fails to start|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L24], all successfully started containers are left running.  This [prevents|https://github.com/elek/ozone-ci/blob/5c64f77f3ab64aed0826d8f40991fe621f843efd/pr/pr-hdds-2026-p4f6m/acceptance/output.log#L59] any further acceptance tests from normal completion.  This is only a minor inconvenience, since acceptance test as a whole fails either way.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Partially started compose cluster left running"
   },
   {
      "_id": "13253188",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-08-27 06:21:07",
      "description": "HDDS-1489 fixed several sample docker compose configs to avoid unnecessary messages on console when running eg. {{ozone sh key put}}.  The goal of this task is to fix the remaining ones.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Avoid log on console with Ozone shell"
   },
   {
      "_id": "13253150",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334613",
            "id": "12334613",
            "name": "website"
         }
      ],
      "created": "2019-08-26 23:12:19",
      "description": "Currently, BaseHttpServer uses DFSUtil to get Http policy. With this, when http policy is set to HTTPS on hdfs-site.xml, ozone http servers try to come up with HTTPS and fail if SSL certificates are not present in the required location.\r\n\r\nOzone web UIs should not depend on HDFS config to determine HTTP policy. Instead, it should have its own config to determine the policy.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Don't depend on DFSUtil to check HTTP policy"
   },
   {
      "_id": "13252912",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-08-26 05:34:22",
      "description": "Currently, Ozone client retry writes on a different pipeline or container in case of some specific exceptions. But in case, it sees exception such as DISK_FULL, CONTAINER_UNHEALTHY or any corruption , it just aborts the write. In general, the every such exception on the client should be a retriable  exception in ozone client and on some specific exceptions, it should take some more specific exception like excluding certain containers or pipelines while retrying or informing SCM of a corrupt replica etc.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone client should retry writes in case of any ratis/stateMachine exceptions"
   },
   {
      "_id": "13252834",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-08-24 21:56:39",
      "description": "hadoop-ozone/dev-support/checks directory contains shell scripts to execute different type of code checks (findbugs, checkstyle, etc.)\r\n\r\nCurrently the contract is very simple. Every shell script executes one (and only one) check and the shell response code is set according to the result (non-zero code if failed).\r\n\r\nTo have better reporting in the github pr build, it would be great to improve the scripts to generate simple summary files and save the relevant files for archiving.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Generate simplifed reports by the dev-support/checks/*.sh scripts"
   },
   {
      "_id": "13252711",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-08-23 16:26:01",
      "description": "Concurrent requests to datanode for the same chunk may result in the following exception in datanode:\r\n\r\n{code}\r\njava.nio.channels.OverlappingFileLockException\r\n   at java.base/sun.nio.ch.FileLockTable.checkList(FileLockTable.java:229)\r\n   at java.base/sun.nio.ch.FileLockTable.add(FileLockTable.java:123)\r\n   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.addToFileLockTable(AsynchronousFileChannelImpl.java:178)\r\n   at java.base/sun.nio.ch.SimpleAsynchronousFileChannelImpl.implLock(SimpleAsynchronousFileChannelImpl.java:185)\r\n   at java.base/sun.nio.ch.AsynchronousFileChannelImpl.lock(AsynchronousFileChannelImpl.java:118)\r\n   at org.apache.hadoop.ozone.container.keyvalue.helpers.ChunkUtils.readData(ChunkUtils.java:175)\r\n   at org.apache.hadoop.ozone.container.keyvalue.impl.ChunkManagerImpl.readChunk(ChunkManagerImpl.java:213)\r\n   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleReadChunk(KeyValueHandler.java:574)\r\n   at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:195)\r\n   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:271)\r\n   at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)\r\n   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)\r\n   at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)\r\n{code}\r\n\r\nIt seems this is covered by retry logic, as key read is eventually successful at client side.\r\n\r\nThe problem is that:\r\n\r\nbq. File locks are held on behalf of the entire Java virtual machine. They are not suitable for controlling access to a file by multiple threads within the same virtual machine. ([source|https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileLock.html])\r\n\r\ncode ref: [{{ChunkUtils.readData}}|https://github.com/apache/hadoop/blob/c92de8209d1c7da9e7ce607abeecb777c4a52c6a/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/ChunkUtils.java#L175]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Overlapping chunk region cannot be read concurrently"
   },
   {
      "_id": "13252640",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-23 11:25:16",
      "description": "The hadoop-docker-ozone repository contains the definition of the apache/ozone image. \r\n\r\nhttps://github.com/apache/hadoop-docker-ozone/tree/ozone-latest\r\n\r\nIt creates a docker packaging for the voted and released artifact, therefore it can be released after the final vote.\r\n\r\nSince the latest release we did some modification in our Dockerfiles. We need to apply the changes to the official image as well. Especially:\r\n\r\n 1. use ozone-runner as a base image instead of hadoop-runner\r\n 2. rename ozoneManager service to om as we did everywhere\r\n 3. Adjust the starter location (the script is moved to the released tar file)\r\n ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update the Dockerfile of the official apache/ozone image and use latest 0.4.1 release"
   },
   {
      "_id": "13252619",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-08-23 09:35:53",
      "description": "Running {{rat.sh}} locally fails with the following error message (after the two Maven runs):\r\n\r\n{code:title=./hadoop-ozone/dev-support/checks/rat.sh}\r\n...\r\ngrep: warning: recursive search of stdin\r\n{code}\r\n\r\nThis happens if {{grep}} is not the GNU one.\r\n\r\nFurther, {{rat.sh}} runs into: {{cat: target/rat-aggregated.txt: No such file or directory}} in subshell due to a typo, and so always exits with success:\r\n\r\n{code}\r\n$ ./hadoop-ozone/dev-support/checks/rat.sh\r\n...\r\ncat: target/rat-aggregated.txt: No such file or directory\r\n\r\n$ echo $?\r\n0\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "rat.sh: grep: warning: recursive search of stdin"
   },
   {
      "_id": "13252601",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-08-23 08:20:09",
      "description": "Several files in hadop-ozone do not have apache license headers and cause a failure in trunk.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix rat check failures in trunk"
   },
   {
      "_id": "13252597",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-08-23 08:01:27",
      "description": "Freon is a generic load generator tool for ozone (ozone freon) which supports multiple generation pattern.\r\n\r\nAs of now only the random-key-generator is implemented which uses ozone rpc client.\r\n\r\nIt would be great to add additional tests:\r\n\r\n * Test key generation via s3 interface\r\n * Test key generation via the hadoop fs interface\r\n * Test key reads (validation)\r\n * Test OM with direct RPC calls\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add additional freon tests"
   },
   {
      "_id": "13252549",
      "assignee": "xyao",
      "components": [],
      "created": "2019-08-22 23:35:02",
      "description": "Generic GRPC support mTLS for mutual authentication. However, Ozone has built in block token mechanism for server to authenticate the client. We only need TLS for client to authenticate the server and wire encryption.\u00a0\r\n\r\nRemove the mTLS support also simplify the GRPC server/client configuration.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove mTLS from Ozone GRPC"
   },
   {
      "_id": "13252477",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-22 20:11:05",
      "description": "When OM HA is enabled, when tokens are generated, the service name should be set with address of all OM's.\r\n\r\n\u00a0\r\n\r\nCurrent without HA, it is set with Om RpcAddress string. This Jira is to handle:\r\n # Set dtService with all OM address. Right now in OMClientProducer, UGI is created with S3 token, and serviceName of token is set with OMAddress, for HA case, this should be set with all OM RPC addresses.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Handle Set DtService of token in S3Gateway for OM HA"
   },
   {
      "_id": "13252476",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-22 20:10:21",
      "description": "When OM HA is enabled, when tokens are generated, the service name should be set with address of all OM's.\r\n\r\n\u00a0\r\n\r\nCurrent without HA, it is set with Om RpcAddress string. This Jira is to handle:\r\n # Set dtService with all OM address.\r\n # Update token selector to return tokens if there is a match with Service. Because SaslRpcClient calls token selector with server address.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Set DtService of token for OM HA"
   },
   {
      "_id": "13252282",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-08-22 05:54:40",
      "description": "scm web ui should publish the list of scm pipeline by type and factor, this helps in monitoring the cluster in real time.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "scm web ui should publish the list of scm pipeline by type and factor"
   },
   {
      "_id": "13252274",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-08-22 05:15:22",
      "description": "It is currently org.apache.ratis.RatisHelper. \r\n\r\nIt should be org.apache.hadoop.hdds.ratis.RatisHelper.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Wrong package for RatisHelper class in hadoop-hdds/common module."
   },
   {
      "_id": "13252150",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-08-21 14:41:19",
      "description": "The OM/SCM web pages are broken due to the upgrade in HDFS-14729 (which is a great patch on the Hadoop side). To have more stability I propose to use our own instance from jquery/bootstrap instead of copying the actual version from hadoop trunk which is a SNAPSHOT build.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Don't depend on bootstrap/jquery versions from hadoop-trunk snapshot"
   },
   {
      "_id": "13252115",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-21 12:02:53",
      "description": "{code:title=https://elek.github.io/ozone-ci/trunk/trunk-nightly-9stkx/acceptance/smokeresult/log.html#s1-s8-t1}\r\n$ curl --negotiate -u : -s -I http://scm:9876/static/bootstrap-3.3.7/js/bootstrap.min.js 2>&1\r\nHTTP/1.1 404 Not Found\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Basic acceptance test and SCM/OM web UI broken by Bootstrap upgrade"
   },
   {
      "_id": "13251954",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-21 00:00:16",
      "description": "Use renewTime generated by OM leader, across quorum of OM's.\r\n\r\n\u00a0\r\n\r\nRight now each OM generates renew time when updating token in-memory and DB.\r\n\r\n*OzoneDelegationTokenSecretManager.java*\r\n\r\npublic long updateToken(Token<OzoneTokenIdentifier> token,\r\n OzoneTokenIdentifier ozoneTokenIdentifier) {\r\n long renewTime =\r\n ozoneTokenIdentifier.getIssueDate() + getTokenRenewInterval();\r\n\r\n\u00a0\r\n\r\nIf different OM's have different token renew interval set, for the same token we will have different renewal time across a quorum of OM's.\r\n\r\n\u00a0\r\n\r\nThis Jira is to fix this issue.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Generate renewTime on OMLeader for GetDelegationToken"
   },
   {
      "_id": "13251703",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:25:16",
      "description": "Once HA and Non-HA code are merged to use newly OM HA code. We can merge these classes, and remove the unused code.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge OzoneManagerRequestHandler and OzoneManagerHARequestHandlerImpl"
   },
   {
      "_id": "13251698",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:16:00",
      "description": "In OM, we use ratis server api's to submit request. We can remove the RatisClient code from OM, which is no more used in submitting requests to ratis.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove RatisClient in OM HA"
   },
   {
      "_id": "13251695",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:12:14",
      "description": "Right now, applyTransaction calls validateAndUpdateCache. This does not return any error. We should check the OmResponse Status code, and see if it is critical error we should completeExceptionally and stop ratis server.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix ApplyTransaction error handling in OzoneManagerStateMachine"
   },
   {
      "_id": "13251693",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:09:47",
      "description": "This Jira is to fix listParts API in HA code path.\r\n\r\nIn HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listParts of a MPU key, it should use both in-memory cache and rocksdb mpu table to list parts of a mpu key.\r\n\r\n\u00a0\r\n\r\nNo fix is required for this, as the information is retrieved from the MPU Key table, this information is not retrieved through RocksDB Table iteration. (As when we use get() this checks from cache first, and then it checks table)\r\n\r\n\u00a0\r\n\r\nUsed this Jira to add an integration test to verify the behavior.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix listParts API"
   },
   {
      "_id": "13251690",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:06:41",
      "description": "This Jira is to fix listKeys API in HA code path.\r\n\r\nIn HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listkeys, it should use both in-memory cache and rocksdb key table to list keys in a bucket.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix listkeys API"
   },
   {
      "_id": "13251689",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:05:47",
      "description": "This Jira is to fix lisVolumes API in HA code path.\r\n\r\nIn HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listVolumes, it should use both in-memory cache and rocksdb volume table to list volumes for a user.\r\n\r\n\u00a0\r\n\r\nNo fix is required for this, as the information is retrieved from the MPU Key table, this information is not retrieved through RocksDB Table iteration. (As when we use get() this checks from cache first, and then it checks table)\r\n\r\n\u00a0\r\n\r\nUsed this Jira to add an integration test to verify the behavior.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix listVolumes API"
   },
   {
      "_id": "13251688",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-19 21:05:14",
      "description": "This Jira is to fix listBucket API in HA code path.\r\n\r\nIn HA, we have an in-memory cache, where we put the result to in-memory cache and return the response, later it will be picked by double buffer thread and it will flush to disk. So, now when do listBuckets, it should use both in-memory cache and rocksdb bucket table to list buckets in a volume.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix listBucket API"
   },
   {
      "_id": "13251417",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-08-17 22:41:50",
      "description": "There are checkstyle errors in ListPipelinesSubcommand.java that needs to be fixed.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix checkstyle errors"
   },
   {
      "_id": "13251345",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-16 22:16:20",
      "description": "This Jira is to implement default ACLs for Ozone volume/bucket/key.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement default acls for bucket/volume/key for OM HA code"
   },
   {
      "_id": "13251119",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-16 00:02:23",
      "description": "Implement OM CancelDelegationToken request to use OM Cache, double buffer.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement OM CancelDelegationToken request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13251118",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-16 00:01:59",
      "description": "Implement OM RenewDelegationToken request to use OM Cache, double buffer.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement OM RenewDelegationToken request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13251100",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-08-15 21:07:44",
      "description": "In this Jira, we shall provide docker-compose files where we start 3 s3 gateway servers, and ha-proxy is used to load balance these S3 Gateway Servers.\r\n\r\n\u00a0\r\n\r\nIn this Jira, all are proxy configurations are hardcoded, we can make improvements to scale and automatically configure with environment variables as a future improvement. This is just a starter example.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide example ha proxy with multiple s3 servers back end."
   },
   {
      "_id": "13250863",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-14 19:59:55",
      "description": "Implement OM GetDelegationToken request to use OM Cache, double buffer.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement OM GetDelegationToken request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13250780",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-14 11:59:56",
      "description": "Acceptance test fails at ACL checks:\r\n\r\n{code:title=https://elek.github.io/ozone-ci/trunk/trunk-nightly-wxhxr/acceptance/smokeresult/log.html#s1-s16-s2-t4-k2}\r\n[ {\r\n  \"type\" : \"USER\",\r\n  \"name\" : \"testuser/scm@EXAMPLE.COM\",\r\n  \"aclScope\" : \"ACCESS\",\r\n  \"aclList\" : [ \"ALL\" ]\r\n}, {\r\n  \"type\" : \"GROUP\",\r\n  \"name\" : \"root\",\r\n  \"aclScope\" : \"ACCESS\",\r\n  \"aclList\" : [ \"ALL\" ]\r\n}, {\r\n  \"type\" : \"GROUP\",\r\n  \"name\" : \"superuser1\",\r\n  \"aclScope\" : \"ACCESS\",\r\n  \"aclList\" : [ \"ALL\" ]\r\n}, {\r\n  \"type\" : \"USER\",\r\n  \"name\" : \"superuser1\",\r\n  \"aclScope\" : \"ACCESS\",\r\n  \"aclList\" : [ \"READ\", \"WRITE\", \"READ_ACL\", \"WRITE_ACL\" ]\r\n} ]' does not match '\"type\" : \"GROUP\",\r\n.*\"name\" : \"superuser1*\",\r\n.*\"aclScope\" : \"ACCESS\",\r\n.*\"aclList\" : . \"READ\", \"WRITE\", \"READ_ACL\", \"WRITE_ACL\"'\r\n{code}\r\n\r\nThe test [sets user ACL|https://github.com/apache/hadoop/blob/0e4b757955ae8da1651b870c12458e3344c0b613/hadoop-ozone/dist/src/main/smoketest/basic/ozone-shell.robot#L123], but [checks group ACL|https://github.com/apache/hadoop/blob/0e4b757955ae8da1651b870c12458e3344c0b613/hadoop-ozone/dist/src/main/smoketest/basic/ozone-shell.robot#L125].  I think this passed previously due to a bug that was [fixed|https://github.com/apache/hadoop/pull/1234/files#diff-2d061b57a9838854d07da9e0eca64f31] by HDDS-1917.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Wrong expected key ACL in acceptance test"
   },
   {
      "_id": "13250760",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-08-14 11:00:12",
      "description": "{code:title=https://ci.anzix.net/job/ozone/17667/consoleText}\r\n[ERROR] COMPILATION ERROR : \r\n[INFO] -------------------------------------------------------------\r\n[ERROR] /var/jenkins_home/workspace/ozone@2/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestIngClient.java:[65,8] class ScmBlockLocationTestingClient is public, should be declared in a file named ScmBlockLocationTestingClient.java\r\n[ERROR] /var/jenkins_home/workspace/ozone@2/hadoop-ozone/ozone-manager/src/test/java/org/apache/hadoop/ozone/om/ScmBlockLocationTestingClient.java:[65,8] duplicate class: org.apache.hadoop.ozone.om.ScmBlockLocationTestingClient\r\n[INFO] 2 errors \r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Compile error due to leftover ScmBlockLocationTestIngClient file"
   },
   {
      "_id": "13250740",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-14 09:15:00",
      "description": "{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/trunk/trunk-nightly-wxhxr/unit/hadoop-ozone/s3gateway/org.apache.hadoop.ozone.s3.TestOzoneClientProducer.txt}\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.ozone.s3.TestOzoneClientProducer\r\n-------------------------------------------------------------------------------\r\nTests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 222.239 s <<< FAILURE! - in org.apache.hadoop.ozone.s3.TestOzoneClientProducer\r\ntestGetClientFailure[0](org.apache.hadoop.ozone.s3.TestOzoneClientProducer)  Time elapsed: 111.036 s  <<< FAILURE!\r\njava.lang.AssertionError: \r\n Expected to find 'Couldn't create protocol ' but got unexpected exception: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort\r\n{code}\r\n\r\nLog output (with local log4j config) reveals that connection is attempted to 0.0.0.0:9862:\r\n\r\n{code:title=log output}\r\n2019-08-14 10:49:14,225 [main] INFO  ipc.Client (Client.java:handleConnectionFailure(948)) - Retrying connect to server: 0.0.0.0/0.0.0.0:9862. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\r\n{code}\r\n\r\nThe address 0.0.0.0:9862 was added as default in [HDDS-1920|https://github.com/apache/hadoop/commit/bf457797f607f3aeeb2292e63f440cb13e15a2d9].",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneClientProducer fails with ConnectException"
   },
   {
      "_id": "13250452",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-13 07:22:39",
      "description": "Aged IO Thread in {{TestMiniChaosOzoneCluster}} exits on first read due to exception:\r\n\r\n{code}\r\n2019-08-12 22:55:37,799 [pool-245-thread-1] INFO  ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(194)) - AGED LOADGEN: Started Aged IO Thread:2139.\r\n...\r\n2019-08-12 22:55:47,147 [pool-245-thread-1] ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(213)) - AGED LOADGEN: 0 Exiting due to exception\r\njava.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.apache.hadoop.ozone.MiniOzoneLoadGenerator.readData(MiniOzoneLoadGenerator.java:151)\r\n\tat org.apache.hadoop.ozone.MiniOzoneLoadGenerator.startAgedFilesLoad(MiniOzoneLoadGenerator.java:209)\r\n\tat org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$1(MiniOzoneLoadGenerator.java:235)\r\n2019-08-12 22:55:47,149 [pool-245-thread-1] INFO  ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:startAgedFilesLoad(219)) - Terminating IO thread:2139.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Aged IO Thread exits on first read"
   },
   {
      "_id": "13250198",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-08-12 07:07:32",
      "description": "Happens if log level for {{org.apache.hadoop.ozone.client}} is set to TRACE.\r\n\r\n{code}\r\nSLF4J: Failed toString() invocation on an object of type [com.sun.proxy.$Proxy85]\r\nReported exception:\r\njava.lang.StackOverflowError\r\n...\r\n\tat org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)\r\n\tat com.sun.proxy.$Proxy85.toString(Unknown Source)\r\n\tat org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:299)\r\n\tat org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:271)\r\n\tat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:233)\r\n\tat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:173)\r\n\tat org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:151)\r\n\tat org.slf4j.impl.Log4jLoggerAdapter.trace(Log4jLoggerAdapter.java:156)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:51)\r\n\tat com.sun.proxy.$Proxy85.toString(Unknown Source)\r\n...\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "StackOverflowError in OzoneClientInvocationHandler"
   },
   {
      "_id": "13250047",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-10 14:48:42",
      "description": "{{TestMiniChaosOzoneCluster}} runs load generator on a cluster for supposedly 1 minute, but it may run indefinitely until JVM crashes due to OutOfMemoryError.\r\n\r\nIn 0.4.1 nightly build it crashed 29/30 times (and no tests were executed in the remaining one run due to some other error).\r\n\r\nLatest:\r\nhttps://github.com/elek/ozone-ci/blob/3f553ed6ad358ba61a302967617de737d7fea01a/byscane/byscane-nightly-wggqd/integration/output.log#L5661-L5662\r\n\r\nWhen it crashes, it leaves GBs of data lying around.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "TestMiniChaosOzoneCluster may run until OOME"
   },
   {
      "_id": "13250039",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-10 13:38:51",
      "description": "Should be Biscayne instead of Crater lake according to the Roadmap:\r\n\r\nhttps://cwiki.apache.org/confluence/display/HADOOP/Ozone+Road+Map\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Wrong symbolic release name on 0.4.1 branch"
   },
   {
      "_id": "13250037",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-08-10 12:32:27",
      "description": "If an S3 multipart upload is created but no part is upload the part list can't be called because it throws HTTP 500:\r\n\r\nCreate an MPU:\r\n\r\n{code}\r\naws s3api --endpoint http://localhost:9999 create-multipart-upload --bucket=docker --key=testkeu                                         \r\n{\r\n    \"Bucket\": \"docker\",\r\n    \"Key\": \"testkeu\",\r\n    \"UploadId\": \"85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234\"\r\n}\r\n{code}\r\n\r\nList the parts:\r\n\r\n{code}\r\naws s3api --endpoint http://localhost:9999 list-parts  --bucket=docker --key=testkeu --upload-id=85343e71-4c16-4a75-bb55-01f56a9339b2-102592678478217234\r\n{code}\r\n\r\nIt throws an exception on the server side, because in the KeyManagerImpl.listParts the  ReplicationType is retrieved from the first part:\r\n\r\n{code}\r\n        HddsProtos.ReplicationType replicationType =\r\n            partKeyInfoMap.firstEntry().getValue().getPartKeyInfo().getType();\r\n{code}\r\n\r\nWhich is not yet available in this use case.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3 MPU part-list call fails if there are no parts"
   },
   {
      "_id": "13250036",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-10 11:38:51",
      "description": "Some integration tests do not clean up after themselves.  Some only clean up if the test is successful.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Missing or error-prone test cleanup"
   },
   {
      "_id": "13250014",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-08-10 07:04:32",
      "description": "This problem is reported offline by [~shanekumpf@gmail.com].\r\n\r\nWhen aws-sdk-go is used to access to s3 gateway of Ozone it sends the Multi Part Upload initialize message with \"application/octet-stream\" Content-Type. \r\n\r\nThis Content-Type is missing from the aws-cli which is used to reimplement s3 endpoint.\r\n\r\nThe problem is that we use the same rest endpoint for initialize and complete Multipart Upload request. For the completion we need the CompleteMultipartUploadRequest parameter which is parsed from the body.\r\n\r\nFor initialize we have an empty body which can't be serialized to CompleteMultipartUploadRequest.\r\n\r\nThe workaround is to set a specific content type from a filter which help up to create two different REST method for initialize and completion message.\r\n\r\nHere is an example to test (using bogus AWS credentials).\r\n\r\n{code}\r\ncurl -H 'Host:yourhost' -H 'User-Agent:aws-sdk-go/1.15.11 (go1.11.2; linux; amd64)' -H 'Content-Length:0' -H 'Authorization:AWS4-HMAC-SHA256 Credential=qwe/20190809/ozone/s3/aws4_request, SignedHeaders=content-type;host;x-amz-acl;x-amz-content-sha256;x-amz-date;x-amz-storage-class, Signature=7726ed63990ba3f4f1f796d4ab263f5d9c3374528840f5e49d106dbef491f22c' -H 'Content-Type:application/octet-stream' -H 'X-Amz-Acl:private' -H 'X-Amz-Content-Sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' -H 'X-Amz-Date:20190809T070142Z' -H 'X-Amz-Storage-Class:STANDARD' -H 'Accept-Encoding:gzip' -X POST 'http://localhost:9999/docker/docker/registry/v2/repositories/apache/ozone-runner/_uploads/2173f019-09c3-466b-bb7d-c31ce749d826/data?uploads\r\n{code}\r\n\r\nWithout the patch it returns with HTTP 405 (Not supported Media Type).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "S3 MPU can't be created with octet-stream content-type "
   },
   {
      "_id": "13249995",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-08-10 00:11:23",
      "description": "For example, when OM and SCM are deployed on the same host with ozone.metadata.dir defined. SCM can start successfully but OM can not because the key/cert from OM will collide with SCM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "CertificateClient should not persist keys/certs to ozone.metadata.dir"
   },
   {
      "_id": "13249942",
      "assignee": "xyao",
      "components": [],
      "created": "2019-08-09 17:06:52",
      "description": "{code}\r\n[ERROR]   TestKeyManagerImpl.testLookupKeyWithLocation:757 expected:<102ad7e3-4226-4966-af79-2b12a56f83cb{ip: 32.53.16.224, host: localhost-32.53.16.224, networkLocation: /default-rack, certSerialId: null}> but was:<d3e07bc3-6d24-4d80-9cdb-057a475084e7{ip: 238.199.149.19, host: localhost-238.199.149.19, networkLocation: /default-rack, certSerialId: null}>\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestKeyManagerImpl.testLookupKeyWithLocation is failing"
   },
   {
      "_id": "13249939",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-08-09 17:01:22",
      "description": "Uploads a part by copying data from an existing object as data source\r\n\r\nDocumented here:\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Support copy during S3 multipart upload part creation"
   },
   {
      "_id": "13249930",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-08-09 15:41:17",
      "description": "{{SimpleContainerDownloader}} has an {{executor}} that's created and shut down, but never used.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Unused executor in SimpleContainerDownloader"
   },
   {
      "_id": "13249697",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-08 15:54:34",
      "description": "Acceptance test of a nightly build is failed with the following error:\r\n\r\n{code}\r\nCreating ozonesecure_datanode_3 ... \r\n\u001b[7A\u001b[2K\r\nCreating ozonesecure_kdc_1      ... \u001b[32mdone\u001b[0m\r\n\u001b[7B\u001b[6A\u001b[2K\r\nCreating ozonesecure_om_1       ... \u001b[32mdone\u001b[0m\r\n\u001b[6B\u001b[8A\u001b[2K\r\nCreating ozonesecure_scm_1      ... \u001b[32mdone\u001b[0m\r\n\u001b[8B\u001b[1A\u001b[2K\r\nCreating ozonesecure_datanode_3 ... \u001b[32mdone\u001b[0m\r\n\u001b[1B\u001b[5A\u001b[2K\r\nCreating ozonesecure_kms_1      ... \u001b[32mdone\u001b[0m\r\n\u001b[5B\u001b[4A\u001b[2K\r\nCreating ozonesecure_s3g_1      ... \u001b[32mdone\u001b[0m\r\n\u001b[4B\u001b[2A\u001b[2K\r\nCreating ozonesecure_datanode_2 ... \u001b[32mdone\u001b[0m\r\n\u001b[2B\u001b[3A\u001b[2K\r\nCreating ozonesecure_datanode_1 ... \u001b[32mdone\u001b[0m\r\n\u001b[3Bparse error: Invalid numeric literal at line 2, column 0\r\n{code}\r\n\r\nhttps://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-5b87q/acceptance/output.log\r\n\r\nThe problem is in the script which checks the number of available datanodes.\r\n\r\nIf the HTTP endpoint of the SCM is already started BUT not ready yet it may return with a simple HTML error message instead of json. Which can not be parsed by jq:\r\n\r\nIn testlib.sh:\r\n\r\n{code}\r\n  37   \u2502   if [[ \"${SECURITY_ENABLED}\" == 'true' ]]; then\r\n  38   \u2502     docker-compose -f \"${compose_file}\" exec -T scm bash -c \"kinit -k HTTP/scm@EXAMPL\r\n       \u2502 E.COM -t /etc/security/keytabs/HTTP.keytab && curl --negotiate -u : -s '${jmx_url}'\"\r\n  39   \u2502   else\r\n  40   \u2502     docker-compose -f \"${compose_file}\" exec -T scm curl -s \"${jmx_url}\"\r\n  41   \u2502   fi \\\r\n  42   \u2502     | jq -r '.beans[0].NodeCount[] | select(.key==\"HEALTHY\") | .value'\r\n{code}\r\n\r\nOne possible fix is to adjust the error handling (set +x / set -x) per method instead of using a generic set -x at the beginning. It would provide a more predictable behavior. In our case count_datanode should not fail evert (as the caller method: wait_for_datanodes can retry anyway).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Acceptance tests fail if scm webui shows invalid json"
   },
   {
      "_id": "13249660",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-08 13:24:26",
      "description": "Visibility is a key aspect for the operation of any Ozone cluster. We need better visibility to improve correctnes and performance. While the distributed tracing is a good tool for improving the visibility of performance we have no powerful tool which can be used to check the internal state of the Ozone cluster and debug certain correctness issues.\r\n\r\nTo improve the visibility of the internal components I propose to introduce a new command line application `ozone insight`.\r\n\r\nThe new tool will show the selected metrics / logs / configuration for any of the internal components (like replication-manager, pipeline, etc.).\r\n\r\nFor each insight points we can define the required logs and log levels, metrics and configuration and the tool can display only the component specific information during the debug.\r\n\r\nh2. Usage\r\n\r\nFirst we can check the available insight point:\r\n\r\n{code}\r\nbash-4.2$ ozone insight list\r\nAvailable insight points:\r\n\r\n\r\n  scm.node-manager                     SCM Datanode management related information.\r\n  scm.replica-manager                  SCM closed container replication manager\r\n  scm.event-queue                      Information about the internal async event delivery\r\n  scm.protocol.block-location          SCM Block location protocol endpoint\r\n  scm.protocol.container-location      Planned insight point which is not yet implemented.\r\n  scm.protocol.datanode                Planned insight point which is not yet implemented.\r\n  scm.protocol.security                Planned insight point which is not yet implemented.\r\n  scm.http                             Planned insight point which is not yet implemented.\r\n  om.key-manager                       OM Key Manager\r\n  om.protocol.client                   Ozone Manager RPC endpoint\r\n  om.http                              Planned insight point which is not yet implemented.\r\n  datanode.pipeline[id]                More information about one ratis datanode ring.\r\n  datanode.rocksdb                     More information about one ratis datanode ring.\r\n  s3g.http                             Planned insight point which is not yet implemented.\r\n{code}\r\n\r\nInsight points can define configuration, metrics and/or logs. Configuration can be displayed based on the configuration objects:\r\n\r\n{code}\r\nozone insight config scm.protocol.block-location\r\nConfiguration for `scm.protocol.block-location` (SCM Block location protocol endpoint)\r\n\r\n>>> ozone.scm.block.client.bind.host\r\n       default: 0.0.0.0\r\n       current: 0.0.0.0\r\n\r\nThe hostname or IP address used by the SCM block client  endpoint to bind\r\n\r\n\r\n>>> ozone.scm.block.client.port\r\n       default: 9863\r\n       current: 9863\r\n\r\nThe port number of the Ozone SCM block client service.\r\n\r\n\r\n>>> ozone.scm.block.client.address\r\n       default: ${ozone.scm.client.address}\r\n       current: scm\r\n\r\nThe address of the Ozone SCM block client service. If not defined value of ozone.scm.client.address is used\r\n\r\n{code}\r\n\r\nMetrics can be retrieved from the prometheus entrypoint:\r\n\r\n{code}\r\nozone insight metrics scm.protocol.block-location\r\nMetrics for `scm.protocol.block-location` (SCM Block location protocol endpoint)\r\n\r\nRPC connections\r\n\r\n  Open connections: 0\r\n  Dropped connections: 0\r\n  Received bytes: 0\r\n  Sent bytes: 0\r\n\r\n\r\nRPC queue\r\n\r\n  RPC average queue time: 0.0\r\n  RPC call queue length: 0\r\n\r\n\r\nRPC performance\r\n\r\n  RPC processing time average: 0.0\r\n  Number of slow calls: 0\r\n\r\n\r\nMessage type counters\r\n\r\n  Number of AllocateScmBlock: 0\r\n  Number of DeleteScmKeyBlocks: 0\r\n  Number of GetScmInfo: 2\r\n  Number of SortDatanodes: 0\r\n{code}\r\n\r\nLog levels can be adjusted with the existing logLevel servlet and can be collected / streamd via a simple logstream servlet:\r\n\r\n{code}\r\nozone insight log scm.node-manager\r\n[SCM] 2019-08-08 12:42:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]\r\n[SCM] 2019-08-08 12:43:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]\r\n[SCM] 2019-08-08 12:44:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]\r\n[SCM] 2019-08-08 12:45:37,393 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]\r\n[SCM] 2019-08-08 12:46:37,392 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]\r\n{code}\r\n\r\nThe verbose mode can display the raw messages as well:\r\n\r\n{code}\r\n[SCM] 2019-08-08 13:16:37,398 [DEBUG|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] Processing node report from [datanode=ozone_datanode_1.ozone_default]\r\n[SCM] 2019-08-08 13:16:37,400 [TRACE|org.apache.hadoop.hdds.scm.node.SCMNodeManager|SCMNodeManager] HB is received from [datanode=ozone_datanode_1.ozone_default]: \r\nstorageReport {\r\n  storageUuid: \"DS-bffe6bee-1166-4502-acf5-57fc16c5aa98\"\r\n  storageLocation: \"/data/hdds\"\r\n  capacity: 470282264576\r\n  scmUsed: 16384\r\n  remaining: 205695963136\r\n  storageType: DISK\r\n  failed: false\r\n}\r\n\r\n{code}\r\n\r\nh2. Use cases\r\n\r\nOzone insight can be used for any kind of debuging. Some problem examples from my yesterday\r\n\r\n 1. Due to a cache problem the volumes were created twice without any error at the second time. With this tool I can check the state of the internal cache, or check if the volume is added to the rocksdb itself.\r\n\r\n 2. After fixing this problem we found an DNS caching issue. The OM responded with an error but it was not clear where the error was propagated from (it was created in OzoneManagerProtocolClientSideTranslatorPB.handleError). With checking the traffic between SCM and OM it can be easy to track the origin of a specific error.\r\n \r\n 4. After fixing this problem we found some pipline problem (reported later at HDDS-1933). With this tool I could check the content of the reports and messages to the pipeline manager.\r\n\r\n \r\n\r\n\r\nh2. Implementation\r\n\r\nWe can implement the tool without any significant code change as it uses existing features:\r\n\r\n * Metrics can be downloaded from the `/prom` endpoint\r\n * Log Level can be set with the existing `/logLevel` servlet endpoint (from hadoop-common)\r\n * Log lines can be streamed with a very simple new servlet\r\n * Configuration can be displayed based on configuration points\r\n\r\nA new interface can be introduced for `InsightPoint`s where all the affected logs/levels, metrics and config classes can be defined for each components.\r\n\r\nPrometheus servlet endpoint can be changed to be turned on by default.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve the visibility with Ozone Insight tool"
   },
   {
      "_id": "13249629",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-08 10:06:47",
      "description": "{{TestSecureOzoneCluster}} fails if SCM is already running on same host.\r\n\r\nSteps to reproduce:\r\n\r\n# Start {{ozone}} docker compose cluster\r\n# Run {{TestSecureOzoneCluster}} test\r\n\r\n{noformat:title=https://ci.anzix.net/job/ozone/17602/consoleText}\r\n[ERROR] Tests run: 10, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 49.821 s <<< FAILURE! - in org.apache.hadoop.ozone.TestSecureOzoneCluster\r\n[ERROR] testSCMSecurityProtocol(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 6.59 s  <<< ERROR!\r\njava.net.BindException: Port in use: 0.0.0.0:9876\r\n\tat org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)\r\n\tat org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)\r\n\tat org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)\r\n\tat org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)\r\n\tat org.apache.hadoop.ozone.TestSecureOzoneCluster.testSCMSecurityProtocol(TestSecureOzoneCluster.java:277)\r\n...\r\n\r\n[ERROR] testSecureOmReInit(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 5.312 s  <<< ERROR!\r\njava.net.BindException: Port in use: 0.0.0.0:9876\r\n\tat org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)\r\n\tat org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)\r\n\tat org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)\r\n\tat org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)\r\n\tat org.apache.hadoop.ozone.TestSecureOzoneCluster.testSecureOmReInit(TestSecureOzoneCluster.java:743)\r\n...\r\n\r\n[ERROR] testSecureOmInitSuccess(org.apache.hadoop.ozone.TestSecureOzoneCluster)  Time elapsed: 5.312 s  <<< ERROR!\r\njava.net.BindException: Port in use: 0.0.0.0:9876\r\n\tat org.apache.hadoop.http.HttpServer2.constructBindException(HttpServer2.java:1203)\r\n\tat org.apache.hadoop.http.HttpServer2.bindForSinglePort(HttpServer2.java:1225)\r\n\tat org.apache.hadoop.http.HttpServer2.openListeners(HttpServer2.java:1284)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1139)\r\n\tat org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:181)\r\n\tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.start(StorageContainerManager.java:779)\r\n\tat org.apache.hadoop.ozone.TestSecureOzoneCluster.testSecureOmInitSuccess(TestSecureOzoneCluster.java:789)\r\n...\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "TestSecureOzoneCluster may fail due to port conflict"
   },
   {
      "_id": "13249515",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-08-07 19:48:59",
      "description": "OM is started temporarily on {{recon}} host in {{ozonesecure}} compose:\r\n\r\n{noformat}\r\nrecon_1     | 2019-08-07 19:41:46 INFO  OzoneManagerStarter:51 - STARTUP_MSG:\r\nrecon_1     | /************************************************************\r\nrecon_1     | STARTUP_MSG: Starting OzoneManager\r\nrecon_1     | STARTUP_MSG:   host = recon/192.168.16.4\r\nrecon_1     | STARTUP_MSG:   args = [--init]\r\n...\r\nrecon_1     | SHUTDOWN_MSG: Shutting down OzoneManager at recon/192.168.16.4\r\n...\r\nrecon_1     | 2019-08-07 19:41:52 INFO  ReconServer:81 - Initializing Recon server...\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "OM started on recon host in ozonesecure compose "
   },
   {
      "_id": "13249510",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-08-07 18:45:36",
      "description": "{noformat}\r\n$ cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-recon\r\n$ docker-compose up -d --scale datanode=3\r\nERROR: yaml.scanner.ScannerError: mapping values are not allowed here\r\n  in \"./docker-compose.yaml\", line 20, column 33\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cannot run ozone-recon compose due to syntax error"
   },
   {
      "_id": "13249501",
      "assignee": "xyao",
      "components": [],
      "created": "2019-08-07 18:12:49",
      "description": "This Jira is created based on @xiaoyu comment on HDDS-1884\r\n\r\nCan we abstract these add/remove logic into common AclUtil class as we can see similar logic in both bucket manager and key manager? For example,\r\n\r\npublic static boolean addAcl(List existingAcls, OzoneAcl newAcl)\r\npublic static boolean removeAcl(List existingAcls, OzoneAcl newAcl)\r\n\r\n\u00a0\r\n\r\nBut to do this, we need both OmKeyInfo and OMBucketInfo to use list of OzoneAcl/OzoneAclInfo.\r\n\r\nThis Jira is to do that refactor, and also address above comment to move common logic to AclUtils.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Consolidate add/remove Acl into OzoneAclUtil class"
   },
   {
      "_id": "13249345",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-07 12:43:28",
      "description": "HDDS-1499 introduced a new caching layer together with a double-buffer based db writer to support OM HA.\r\n\r\nTLDR: I think the caching layer is not updated for new volume creation. And (slightly related to this problem) I suggest to separated the TypedTable and the caching layer.\r\n\r\n## How to reproduce the problem?\r\n\r\n1. Start a docker compose cluster\r\n2. Create one volume (let's say `/vol1`)\r\n3. Restart the om (!)\r\n4. Try to create an _other_ volume twice!\r\n\r\n```\r\nbash-4.2$ ozone sh volume create /vol2\r\n2019-08-07 12:29:47 INFO  RpcClient:288 - Creating Volume: vol2, with hadoop as owner.\r\nbash-4.2$ ozone sh volume create /vol2\r\n2019-08-07 12:29:50 INFO  RpcClient:288 - Creating Volume: vol2, with hadoop as owner.\r\n```\r\n\r\nExpected behavior is an error:\r\n\r\n{code}\r\nbash-4.2$ ozone sh volume create /vol1\r\n2019-08-07 09:48:39 INFO  RpcClient:288 - Creating Volume: vol1, with hadoop as owner.\r\nbash-4.2$ ozone sh volume create /vol1\r\n2019-08-07 09:48:42 INFO  RpcClient:288 - Creating Volume: vol1, with hadoop as owner.\r\nVOLUME_ALREADY_EXISTS \r\n{code}\r\n\r\nThe problem is that the new cache is used even for the old code path (TypedTable):\r\n\r\n{code}\r\n @Override\r\n  public VALUE get(KEY key) throws IOException {\r\n    // Here the metadata lock will guarantee that cache is not updated for same\r\n    // key during get key.\r\n\r\n    CacheResult<CacheValue<VALUE>> cacheResult =\r\n        cache.lookup(new CacheKey<>(key));\r\n\r\n    if (cacheResult.getCacheStatus() == EXISTS) {\r\n      return cacheResult.getValue().getCacheValue();\r\n    } else if (cacheResult.getCacheStatus() == NOT_EXIST) {\r\n      return null;\r\n    } else {\r\n      return getFromTable(key);\r\n    }\r\n  }\r\n{code}\r\n\r\nFor volume table after the FIRST start it always returns with `getFromTable(key)` due to the condition in the `TableCacheImpl.lookup`:\r\n\r\n{code}\r\n\r\n  public CacheResult<CACHEVALUE> lookup(CACHEKEY cachekey) {\r\n\r\n    if (cache.size() == 0) {\r\n      return new CacheResult<>(CacheResult.CacheStatus.MAY_EXIST,\r\n          null);\r\n    }\r\n{code}\r\n\r\nBut after a restart the cache is pre-loaded by the TypedTable.constructor. After the restart, the real caching logic will be used (as cache.size()>0), which cause a problem as the cache is NOT updated from the old code path.\r\n\r\nAn additional problem is that the cache is turned on for all the metadata table even if the cache is not required... \r\n\r\n## Proposed solution\r\n\r\nAs I commented at HDDS-1499 this caching layer is not a \"traditional cache\". It's not updated during the typedTable.put() call but updated by a separated component during double-buffer flash.\r\n\r\nI would suggest to remove the cache related methods from TypedTable (move to a separated implementation). I think this kind of caching can be independent from the TypedTable implementation. We can continue to use the simple TypedTable everywhere where we don't need to use any kind of caching.\r\n\r\nFor caching we can use a separated object. It would make it more visible that the cache should always be updated manually all the time. This separated caching utility may include a reference to the original TypedTable/Table. With this approach we can separate the different responsibilities but provide the same functionality.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "The new caching layer is used for old OM requests but not updated"
   },
   {
      "_id": "13249316",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-07 10:16:57",
      "description": "Acceptance test is failing at {{ozonesecure}} with the following error from {{jq}}:\r\n\r\n{noformat:title=https://github.com/elek/ozone-ci/blob/325779d34623061e27b80ade3b749210648086d1/byscane/byscane-nightly-ds7lx/acceptance/output.log#L2779}\r\nparse error: Invalid numeric literal at line 2, column 0\r\n{noformat}\r\n\r\nExample compose environments wait for datanodes to be up:\r\n\r\n{code:title=https://github.com/apache/hadoop/blob/9cd211ac86bb1124bdee572fddb6f86655b19b73/hadoop-ozone/dist/src/main/compose/testlib.sh#L71-L72}\r\n  docker-compose -f \"$COMPOSE_FILE\" up -d --scale datanode=\"${datanode_count}\"\r\n  wait_for_datanodes \"$COMPOSE_FILE\" \"${datanode_count}\"\r\n{code}\r\n\r\nThe number of datanodes up is determined via HTTP query of JMX endpoint:\r\n\r\n{code:title=https://github.com/apache/hadoop/blob/9cd211ac86bb1124bdee572fddb6f86655b19b73/hadoop-ozone/dist/src/main/compose/testlib.sh#L44-L46}\r\n     #This line checks the number of HEALTHY datanodes registered in scm over the\r\n     # jmx HTTP servlet\r\n     datanodes=$(docker-compose -f \"${compose_file}\" exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo' | jq -r '.beans[0].NodeCount[] | select(.key==\"HEALTHY\") | .value')\r\n{code}\r\n\r\nThe problem is that no authentication is performed before or during the request, which is no longer allowed since HDDS-1901:\r\n\r\n{code}\r\n$ docker-compose exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo'\r\n<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\r\n<title>Error 401 Authentication required</title>\r\n</head>\r\n<body><h2>HTTP ERROR 401</h2>\r\n<p>Problem accessing /jmx. Reason:\r\n<pre>    Authentication required</pre></p>\r\n</body>\r\n</html>\r\n{code}\r\n\r\n{code}\r\n$ docker-compose exec -T scm curl -s 'http://localhost:9876/jmx?qry=Hadoop:service=SCMNodeManager,name=SCMNodeManagerInfo' | jq -r '.beans[0].NodeCount[] | select(.key==\"HEALTHY\") | .value'\r\nparse error: Invalid numeric literal at line 2, column 0\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "ozonesecure acceptance test broken by HTTP auth requirement"
   },
   {
      "_id": "13249260",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-07 06:32:03",
      "description": "ozone sh bucket path command does not exist but it is mentioned in the static/docs/interface/s3.html. The command should either be added back or a the documentation should be improved.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "ozone sh bucket path command does not exist"
   },
   {
      "_id": "13249216",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-06 21:36:49",
      "description": "{noformat:title=https://ci.anzix.net/job/ozone/17588/testReport/org.apache.hadoop.ozone.om.ratis/TestOzoneManagerDoubleBufferWithOMResponse/testDoubleBuffer/}\r\njava.lang.AssertionError: expected:<11> but was:<9>\r\n...\r\n\tat org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:362)\r\n\tat org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:104)\r\n{noformat}\r\n\r\n{noformat:title=https://ci.anzix.net/job/ozone/17587/testReport/org.apache.hadoop.ozone.om.ratis/TestOzoneManagerDoubleBufferWithOMResponse/unit___testDoubleBuffer/}\r\njava.lang.AssertionError: expected:<11> but was:<3>\r\n...\r\n\tat org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:362)\r\n\tat org.apache.hadoop.ozone.om.ratis.TestOzoneManagerDoubleBufferWithOMResponse.testDoubleBuffer(TestOzoneManagerDoubleBufferWithOMResponse.java:104)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerDoubleBufferWithOMResponse is flaky"
   },
   {
      "_id": "13249067",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-06 09:54:03",
      "description": "HDDS-1735 created separate test runner scripts for unit and integration tests.\r\n\r\nProblem: {{hadoop-ozone-tools}} tests are currently run as part of the unit tests, but most of them start a {{MiniOzoneCluster}}, which is defined in {{hadoop-ozone-integration-test}}.  Thus I think these tests are really integration tests, and should be run by {{integration.sh}} instead.  There are currently only 3 real unit tests:\r\n\r\n{noformat}\r\nhadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/audit/parser/TestAuditParser.java\r\nhadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/freon/TestProgressBar.java\r\nhadoop-ozone/tools/src/test/java/org/apache/hadoop/ozone/genconf/TestGenerateOzoneRequiredConfigurations.java\r\n{noformat}\r\n\r\n{{hadoop-ozone-tools}} tests take ~6 minutes.\r\n\r\nPossible solutions in order of increasing complexity:\r\n\r\n# Run {{hadoop-ozone-tools}} tests in {{integration.sh}} instead of {{unit.sh}} (This is similar to {{hadoop-ozone-filesystem}}, which is already run by {{integration.sh}} and has 2 real unit tests.)\r\n# Move all integration test classes to the {{hadoop-ozone-integration-test}} module, and make it depend on {{hadoop-ozone-tools}} and {{hadoop-ozone-filesystem}} instead of the other way around.\r\n# Rename integration test classes to {{\\*IT.java}} or {{IT\\*.java}}, add filters for Surefire runs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "hadoop-ozone-tools has integration tests run as unit"
   },
   {
      "_id": "13249055",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-06 08:46:28",
      "description": "{{hadoop-ozone-filesystem}} has 6 test classes that are not being run:\r\n\r\n{code}\r\nhadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestFilteredClassLoader.java\r\nhadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFSInputStream.java\r\nhadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileInterfaces.java\r\nhadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystem.java\r\nhadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFileSystemWithMocks.java\r\nhadoop-ozone/ozonefs/src/test/java/org/apache/hadoop/fs/ozone/TestOzoneFsRenameDir.java\r\n{code}\r\n\r\n{code:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-vxsck/integration/output.log}\r\n[INFO] -------------------------------------------------------\r\n[INFO]  T E S T S\r\n[INFO] -------------------------------------------------------\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDelete\r\n[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.956 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDelete\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir\r\n[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.528 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractSeek\r\n[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.245 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractSeek\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractOpen\r\n[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.996 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractOpen\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename\r\n[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.816 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp\r\n[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.418 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractDistCp\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus\r\n[INFO] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.042 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractGetFileStatus\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractCreate\r\n[WARNING] Tests run: 11, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 35.144 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractCreate\r\n[INFO] Running org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRootDir\r\n[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.986 s - in org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRootDir\r\n[INFO] \r\n[INFO] Results:\r\n[INFO] \r\n[WARNING] Tests run: 92, Failures: 0, Errors: 0, Skipped: 2\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Only contract tests are run in ozonefs module"
   },
   {
      "_id": "13249046",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-06 08:10:21",
      "description": "/bin/hadoop script is included in the ozone distribution even if we a dedicated /bin/ozone\r\n\r\n[~arp] reported that it can be confusing, for example \"hadoop classpath\" returns with a bad classpath (ozone classpath <projectname>) should be used instead.\r\n\r\nTo avoid such confusions I suggest to remove the hadoop script from distribution as ozone script already provides all the functionalities.\r\n\r\nIt also helps as to reduce the dependencies between hadoop 3.2-SNAPSHOT and ozone as we use the snapshot hadoop script as of now.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hadoop script from ozone distribution"
   },
   {
      "_id": "13249041",
      "assignee": "elek",
      "components": [],
      "created": "2019-08-06 08:03:22",
      "description": "the compose/ozonescripts cluster provides an example environment to test the start-ozone.sh and stop-ozone.sh scripts.\r\n\r\nIt starts containers with sshd daemon but witout starting the ozone which makes it possible to start those scripts.\r\n\r\nUnfortunately the docker files are broken since:\r\n * we switched from debian to centos with the base image\r\n * we started to use /etc/hadoop instead of /opt/hadoop/etc/hadoop for configuring the hadoop (workers file should be copied there)\r\n * we started to use jdk11 to execute ozone (instead of java8)\r\n\r\nThe configuration files should be updated according to these changes. \r\n\r\n# How to test this patch?\r\n\r\n(1) Do a full build and try to start the ./compose/ozonescripts cluster (check the related README file in the directory):\r\n\r\n```\r\ndocker-compose up -d\r\n```\r\n\r\n(2) start the ozone processes with ./start.sh (from compose/ozonescripts)\r\n\r\n(3) wait and check if om/scm webui are working and you have 1 healthy datanode\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozonescript example docker-compose cluster can't be started"
   },
   {
      "_id": "13248992",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-08-06 02:30:01",
      "description": "Fix addAcl,removeAcl in OzoneBucket to use newly added acl API's addAcl/removeAcl as part of HDDS-1739.\r\n\r\nRemove\u00a0addBucketAcls, removeBucketAcls from RpcClient. We should use addAcl/removeAcl.\r\n\r\n\u00a0\r\n\r\nAnd also fix @xiaoyu comment on HDDS-1900 jira. BucketManagerImpl#setBucketProperty() as they now require a different permission (WRITE_ACL instead of WRITE)?",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix OzoneBucket and RpcClient APIS for acl"
   },
   {
      "_id": "13248984",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-05 23:18:45",
      "description": "+-HDDS-1608-+\u00a0adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Prefix ACL operations for OM HA."
   },
   {
      "_id": "13248887",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-08-05 14:10:30",
      "description": "Building {{hadoop-hdds-config}} from scratch (eg. right after checkout or after {{mvn clean}}) in IDEA fails with the following error:\r\n\r\n{code}\r\nError:java: Bad service configuration file, or exception thrown while constructing Processor object: javax.annotation.processing.Processor: Provider org.apache.hadoop.hdds.conf.ConfigFileGenerator not found\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Cannot build hadoop-hdds-config from scratch in IDEA"
   },
   {
      "_id": "13248792",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-08-04 22:29:20",
      "description": "This Jira is to use new HA code of OM in Non-HA code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use new HA code for Non-HA in OM"
   },
   {
      "_id": "13248766",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-04 13:56:15",
      "description": "TestMultiBlockWritesWithDnFailures is failing with the following exception\r\n{noformat}\r\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.992 s <<< FAILURE! - in org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures\r\n[ERROR] testMultiBlockWritesWithDnFailures(org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures)  Time elapsed: 30.941 s  <<< ERROR!\r\nINTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Allocated 0 blocks. Requested 1 blocks\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:720)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.allocateBlock(OzoneManagerProtocolClientSideTranslatorPB.java:752)\r\n\tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool.allocateNewBlock(BlockOutputStreamEntryPool.java:248)\r\n\tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntryPool.allocateBlockIfNeeded(BlockOutputStreamEntryPool.java:296)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:201)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:376)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:325)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:231)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:193)\r\n\tat org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)\r\n\tat java.io.OutputStream.write(OutputStream.java:75)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestMultiBlockWritesWithDnFailures.testMultiBlockWritesWithDnFailures(TestMultiBlockWritesWithDnFailures.java:144)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestMultiBlockWritesWithDnFailures is failing"
   },
   {
      "_id": "13248765",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-04 13:20:31",
      "description": "{noformat}\r\n[ERROR] testNativeAclsForKey(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.176 s  <<< FAILURE!\r\njava.lang.AssertionError: Current acls:,[user:nvadivelu:a[ACCESS], group:staff:a[ACCESS], group:everyone:a[ACCESS], group:localaccounts:a[ACCESS], group:_appserverusr:a[ACCESS], group:admin:a[ACCESS], group:_appserveradm:a[ACCESS], group:_lpadmin:a[ACCESS], group:com.apple.sharepoint.group.1:a[ACCESS], group:com.apple.sharepoint.group.2:a[ACCESS], group:_appstore:a[ACCESS], group:_lpoperator:a[ACCESS], group:_developer:a[ACCESS], group:_analyticsusers:a[ACCESS], group:com.apple.access_ftp:a[ACCESS], group:com.apple.access_screensharing:a[ACCESS], group:com.apple.access_ssh:a[ACCESS], group:com.apple.sharepoint.group.3:a[ACCESS]] inheritedUserAcl:user:remoteUser:r[ACCESS]\r\n\r\n[ERROR] testNativeAclsForBucket(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.074 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n\r\n[ERROR] testNativeAclsForPrefix(org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientWithRatis)  Time elapsed: 0.061 s  <<< FAILURE!\r\njava.lang.AssertionError: Current acls:,[user:nvadivelu:a[ACCESS], group:staff:a[ACCESS], group:everyone:a[ACCESS], group:localaccounts:a[ACCESS], group:_appserverusr:a[ACCESS], group:admin:a[ACCESS], group:_appserveradm:a[ACCESS], group:_lpadmin:a[ACCESS], group:com.apple.sharepoint.group.1:a[ACCESS], group:com.apple.sharepoint.group.2:a[ACCESS], group:_appstore:a[ACCESS], group:_lpoperator:a[ACCESS], group:_developer:a[ACCESS], group:_analyticsusers:a[ACCESS], group:com.apple.access_ftp:a[ACCESS], group:com.apple.access_screensharing:a[ACCESS], group:com.apple.access_ssh:a[ACCESS], group:com.apple.sharepoint.group.3:a[ACCESS]] inheritedUserAcl:user:remoteUser:r[ACCESS]\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneRpcClientWithRatis is failing with ACL errors"
   },
   {
      "_id": "13248702",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-08-03 07:28:24",
      "description": "We should use dynamic port for SCM in the following test-cases\r\n* TestSCMClientProtocolServer\r\n* TestSCMSecurityProtocolServer",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use dynamic ports for SCM in TestSCMClientProtocolServer and TestSCMSecurityProtocolServer"
   },
   {
      "_id": "13248699",
      "assignee": "xyao",
      "components": [],
      "created": "2019-08-03 06:50:57",
      "description": "This was found during integration testing where the http authentication is enabled but anonymous can still access the ozone http web console like scm:9876 or om:9874. This can be reproed with the following configurations added to the ozonesecure docker-compose.\r\n\r\n{code}\r\n\r\nCORE-SITE.XML_hadoop.http.authentication.simple.anonymous.allowed=false\r\n\r\nCORE-SITE.XML_hadoop.http.authentication.signature.secret.file=/etc/security/http_secret\r\n\r\nCORE-SITE.XML_hadoop.http.authentication.type=kerberos\r\n\r\nCORE-SITE.XML_hadoop.http.authentication.kerberos.principal=HTTP/_HOST@EXAMPLE.COM\r\n\r\nCORE-SITE.XML_hadoop.http.authentication.kerberos.keytab=/etc/security/keytabs/HTTP.keytab\r\n\r\nCORE-SITE.XML_hadoop.http.filter.initializers=org.apache.hadoop.security.AuthenticationFilterInitializer\r\n\r\n{code}\r\n\r\nAfter debugging into the KerberosAuthenticationFilter, the root cause is the name of the keytab does not follow the AuthenticationFilter tradition. The fix is to change\u00a0\r\n\r\nhdds.scm.http.kerberos.keytab.file to hdds.scm.http.kerberos.keytab and\r\nhdds.om.http.kerberos.keytab.file to hdds.om.http.kerberos.keytab\r\n\r\nI will also add an integration test for this under ozonesecure docker-compose. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Ozone HTTP WebConsole Authentication"
   },
   {
      "_id": "13248679",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-08-02 22:59:07",
      "description": "This Jira is to remove bucket update handler.\r\n\r\nTo add acl/remove acl we should use\u00a0ozone sh bucket addacl/ozone sh bucket removeacl.\r\n\r\n\u00a0\r\n\r\nOtherwise, when security is enabled, old Bucket update handler, uses setBucketProperty and that checks acl acces for WRITE, whereas when add/remove Acl we should check access for WRITE_ACL.\r\n\r\n\u00a0\r\n\r\nIf we have both ways, even if a USER does not have WRITE_ACL can still add/remove Acls on a bucket.\r\n\r\n\u00a0\r\n\r\nThis Jira is to clean up the old code and fix this security issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Remove UpdateBucket handler which supports add/remove Acl"
   },
   {
      "_id": "13248497",
      "assignee": "xyao",
      "components": [],
      "created": "2019-08-02 06:12:58",
      "description": "When RackAwareness is enabled and client from outside, the distance calculation flood SCM log with the following messages. This ticket is opened to suppress the WARN log.\r\n\r\n{code}\r\n\r\n2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology\r\n2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology\r\n2019-08-01 23:08:05,011 WARN org.apache.hadoop.hdds.scm.net.NetworkTopology: One of the nodes is outside of network topology\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Suppress WARN log from NetworkTopology#getDistanceCost "
   },
   {
      "_id": "13248490",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-08-02 04:53:33",
      "description": "+HDDS-1541+ adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Key ACL operations for OM HA."
   },
   {
      "_id": "13248455",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         }
      ],
      "created": "2019-08-02 00:50:38",
      "description": "{code:java}\r\n// When we are removing subset of rights from existing acl.\r\nfor(OzoneAcl a: bucketInfo.getAcls()) {\r\n if(a.getName().equals(acl.getName()) &&\r\n a.getType().equals(acl.getType())) {\r\n BitSet bits = (BitSet) acl.getAclBitSet().clone();\r\n bits.and(a.getAclBitSet());\r\n\r\n if (bits.equals(ZERO_BITSET)) {\r\n return false;\r\n }\r\n bits = (BitSet) acl.getAclBitSet().clone();\r\n bits.and(a.getAclBitSet());\r\n a.getAclBitSet().xor(bits);\r\n\r\n if(a.getAclBitSet().equals(ZERO_BITSET)) {\r\n bucketInfo.getAcls().remove(a);\r\n }\r\n break;\r\n } else {\r\n return false;\r\n }{code}\r\nIn for loop, if first one is not matching with name and type, in else we return false. We should iterate entire acl list and then return response.\r\n}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix bug in removeAcl in Bucket"
   },
   {
      "_id": "13248165",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-07-31 17:32:00",
      "description": "For HA scenario, checkAcls we pass the param UGI and remoteAddress. They need to be used in the checkAcls.\u00a0HDDS-909 changed this behavior and not using the passed params.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix bug in checkAcls in OzoneManager"
   },
   {
      "_id": "13248162",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-31 17:10:18",
      "description": "-HDDS-15+40+-\u00a0adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Bucket ACL operations for OM HA."
   },
   {
      "_id": "13248091",
      "assignee": "elek",
      "components": [],
      "created": "2019-07-31 11:56:51",
      "description": "Design doc can be attached to the documentation. In this jira the design doc will be attached and merged to the documentation page.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "design",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Design doc: decommissioning in Ozone"
   },
   {
      "_id": "13248084",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-31 11:32:05",
      "description": "{noformat:title=https://ci.anzix.net/job/ozone/17488/artifact/build/checkstyle.out}\r\n[ERROR] src/main/java/org/apache/hadoop/ozone/container/common/transport/server/ratis/ContainerStateMachine.java:[186] (sizes) LineLength: Line is longer than 80 characters (found 85).\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "checkstyle error in ContainerStateMachine"
   },
   {
      "_id": "13248047",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-07-31 08:49:22",
      "description": "hadoop31-mapreduce fails with:\r\n\r\n{noformat:title=https://elek.github.io/ozone-ci/byscane/byscane-nightly-gl52x/acceptance/smokeresult/log.html#s1-s2-t2-k2-k2}\r\nJAR does not exist or is not a normal file: /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar\r\n{noformat}\r\n\r\nbecause 3.1 test is being run with {{HADOOP_VERSION=3}}:\r\n\r\n{noformat:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-gl52x/acceptance/output.log}\r\nCreating network \"hadoop31_default\" with the default driver\r\nPulling nm (flokkr/hadoop:3)...\r\n3: Pulling from flokkr/hadoop\r\nDigest: sha256:62e3488e64ff8c0406752fc4f263ae2549e04fedf02534469913c496c6a89d78\r\nStatus: Downloaded newer image for flokkr/hadoop:3\r\n{noformat}\r\n\r\nwhich has Hadoop 3.2.0 instead of 3.1.2:\r\n\r\n{noformat:title=docker run -it --entrypoint /bin/bash flokkr/hadoop:3 -c 'ls -la /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples*'}\r\n-rw-r--r--    1 hadoop   flokkr      316570 Jan  8  2019 /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar\r\n{noformat}\r\n\r\n{noformat:title=docker run -it --entrypoint /bin/bash flokkr/hadoop:3.1.2 -c 'ls -la /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples*'}\r\n-rw-r--r--    1 hadoop   flokkr      316380 Jan 29  2019 /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar\r\n{noformat}\r\n\r\nThis only happens with {{acceptance.sh}}, not when directly using {{test-all.sh}}, because the former explicitly defines {{HADOOP_VERSION}}:\r\n\r\n{noformat:title=https://github.com/apache/hadoop/blob/d4ab9aea6f9cbcdcaf48b821e5be04b4e952b133/hadoop-ozone/dev-support/checks/acceptance.sh#L19}\r\nexport HADOOP_VERSION=3\r\n{noformat}\r\n\r\nso the correct value from {{.env}} file is ignored:\r\n\r\n{noformat:title=https://github.com/apache/hadoop/blob/d4ab9aea6f9cbcdcaf48b821e5be04b4e952b133/hadoop-ozone/dist/src/main/compose/ozone-mr/hadoop31/.env#L21}\r\nHADOOP_VERSION=3.1.2\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "hadoop31-mapreduce fails due to wrong HADOOP_VERSION"
   },
   {
      "_id": "13248027",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-31 07:20:35",
      "description": "{noformat:title=https://raw.githubusercontent.com/elek/ozone-ci/master/byscane/byscane-nightly-gl52x/acceptance/output.log}\r\nExecuting test in /workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27\r\nThe HADOOP_RUNNER_VERSION variable is not set. Defaulting to a blank string.\r\nThe HADOOP_IMAGE variable is not set. Defaulting to a blank string.\r\nRemoving network hadoop27_default\r\nNetwork hadoop27_default not found.\r\nThe HADOOP_RUNNER_VERSION variable is not set. Defaulting to a blank string.\r\nThe HADOOP_IMAGE variable is not set. Defaulting to a blank string.\r\nCreating network \"hadoop27_default\" with the default driver\r\nno such image: apache/ozone-runner:: invalid reference format\r\nERROR: Test execution of /workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27 is FAILED!!!!\r\ncp: cannot stat '/workdir/hadoop-ozone/dist/target/ozone-0.4.1-SNAPSHOT/compose/ozone-mr/hadoop27/result/robot-*.xml': No such file or directory\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "hadoop27 acceptance test cannot be run"
   },
   {
      "_id": "13247999",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-31 03:55:35",
      "description": "[https://ci.anzix.net//job/ozone/17503//testReport/junit/org.apache.hadoop.ozone.om.response.s3.multipart/TestS3MultipartUploadAbortResponse/testAddDBToBatchWithParts/]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix failures in TestS3MultipartUploadAbortResponse"
   },
   {
      "_id": "13247744",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-07-29 22:47:21",
      "description": "# Initiate MPU adds entry to openKeyTable and multipartInfo table.\r\n # When completeMPU, we add the entry to keyTable and delete from multipartInfo table.\r\n\r\nDeleting from openKeyTable is missing\u00a0in complete MPU.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix entry clean up from openKeyTable during complete MPU"
   },
   {
      "_id": "13247649",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335609",
            "id": "12335609",
            "name": "kubernetes"
         }
      ],
      "created": "2019-07-29 12:44:31",
      "description": "HDDS-1646 introduced real persistence for k8s example deployment files which means that we need anti-affinity scheduling rules: Even if we use statefulset instead of daemonset we would like to start one datanode per real nodes.\r\n\r\nWith minikube we have only one node therefore the scheduling rule should be removed to enable at least 3 datanodes on the same physical nodes.\r\n\r\nHow to test:\r\n\r\n{code}\r\n mvn clean install -DskipTests -f pom.ozone.xml\r\ncd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/kubernetes/examples/minikube\r\nminikube start\r\nkubectl apply -f .\r\nkc get pod\r\n{code}\r\n\r\nYou should see 3 datanode instances.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Remove anti-affinity rules from k8s minkube example"
   },
   {
      "_id": "13247620",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-29 09:59:40",
      "description": "Encountered on {{ozoneperf}} compose env when running low on CPU:\r\n\r\n{code}\r\nom_1          | java.util.ConcurrentModificationException\r\nom_1          | \tat java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1493)\r\nom_1          | \tat java.base/java.util.HashMap$ValueIterator.next(HashMap.java:1521)\r\nom_1          | \tat org.apache.hadoop.hdds.server.PrometheusMetricsSink.writeMetrics(PrometheusMetricsSink.java:123)\r\nom_1          | \tat org.apache.hadoop.hdds.server.PrometheusServlet.doGet(PrometheusServlet.java:43)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "ConcurrentModification at PrometheusMetricsSink"
   },
   {
      "_id": "13247311",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-26 12:26:17",
      "description": "{noformat}\r\ntarget=http://scm:9876/prom msg=\"append failed\" err=\"invalid metric type \\\"_old _generation counter\\\"\"\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Invalid Prometheus metric name from JvmMetrics"
   },
   {
      "_id": "13247180",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-07-26 01:07:04",
      "description": "\u00a0\r\n{code:java}\r\n***************************************************\r\nStatus: Success\r\nGit Base Revision: e97acb3bd8f3befd27418996fa5d4b50bf2e17bf\r\nNumber of Volumes created: 1\r\nNumber of Buckets created: 1\r\nNumber of Keys added: 1\r\nRatis replication factor: THREE\r\nRatis replication type: STAND_ALONE\r\nAverage Time spent in volume creation: 00:00:00,002\r\nAverage Time spent in bucket creation: 00:00:00,000\r\nAverage Time spent in key creation: 00:00:00,002\r\nAverage Time spent in key write: 00:00:00,101\r\nTotal bytes written: 0\r\nTotal Execution time: 00:00:05,699\r\n\u00a0\r\n{code}\r\n***************************************************\r\n\r\n[root@ozoneha-2 ozone-0.5.0-SNAPSHOT]# bin/ozone sh key list /vol-0-28271/bucket-0-95211\r\n\r\n[\r\n\r\n{ \u00a0 \"version\" : 0, \u00a0 \"md5hash\" : null, \u00a0 \"createdOn\" : \"Fri, 26 Jul 2019 01:02:08 GMT\", \u00a0 \"modifiedOn\" : \"Fri, 26 Jul 2019 01:02:09 GMT\", \u00a0 \"size\" : 36, \u00a0 \"keyName\" : \"key-0-98235\", \u00a0 \"type\" : null }\r\n\r\n]\r\n\r\n\u00a0\r\n\r\nThis is because of the below code in RandomKeyGenerator:\r\n{code:java}\r\nfor (long nrRemaining = keySize - randomValue.length;\r\n nrRemaining > 0; nrRemaining -= bufferSize) {\r\n int curSize = (int) Math.min(bufferSize, nrRemaining);\r\n os.write(keyValueBuffer, 0, curSize);\r\n}\r\nos.write(randomValue);\r\nos.close();{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Freon RandomKeyGenerator even if keySize is set to 0, it returns some random data to key"
   },
   {
      "_id": "13247165",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-25 23:04:54",
      "description": "Currently in cleanup, we iterate over epochEntries and cleaup the entries from cache and epochEntries set.\r\n\r\n\u00a0\r\n\r\nepochEntries is a TreeSet<> which is not a concurrent datastructure of java. We may see issue some times, when cleanup tries to remove entries and some other thread tries to add entries to cache. So, we need to use some concurrent set over there.\r\n\r\n\u00a0\r\n\r\nDuring cluster testing, seen this some times randomly:\r\n\u00a0\r\n{code:java}\r\n019-07-25 15:28:41,087 WARN org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9862, call Call#8974 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.65.15.233:35222 java.lang.NullPointerException at java.util.TreeMap.fixAfterInsertion(TreeMap.java:2295) at java.util.TreeMap.put(TreeMap.java:582) at java.util.TreeSet.add(TreeSet.java:255) at org.apache.hadoop.utils.db.cache.TableCacheImpl.put(TableCacheImpl.java:75) at org.apache.hadoop.utils.db.TypedTable.addCacheEntry(TypedTable.java:218) at org.apache.hadoop.ozone.om.request.key.OMKeyRequest.prepareCreateKeyResponse(OMKeyRequest.java:292) at org.apache.hadoop.ozone.om.request.key.OMKeyCreateRequest.validateAndUpdateCache(OMKeyCreateRequest.java:188) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:134) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method){code}\r\n\u00a0\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TableCacheImpl cleanup logic"
   },
   {
      "_id": "13246913",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-24 21:28:58",
      "description": "In this Jira following things will be implemented:\r\n # Make the necessary changes for non-HA code path to use Cache and DoubleBuffer.\r\n\r\n ## When adding to double buffer, return future. This future will be used in the non-HA path to wait for this, and when it is completed return response to the client.\r\n ## Add to double-buffer will happen inside validateAndUpdateCache. In this way, in non-HA, when multiple RPC handler threads are calling preExecute and validateAndUpdateCache, the order inserted in to double buffer will happen in the order requests are received.\r\n\r\n\u00a0\r\n\r\nIn this Jira, we shall not convert non-ha code path to use this, as security and acl work is not completed to use this new model.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make changes required for Non-HA to use new HA code in OM."
   },
   {
      "_id": "13246723",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-07-24 04:39:45",
      "description": "In test class TestOmAcls.java, correct the typo {code}OzoneAccessAuthrizerTest{code}\r\n\r\n{code:java}\r\nclass OzoneAccessAuthrizerTest implements IAccessAuthorizer {\r\n\r\n  @Override\r\n  public boolean checkAccess(IOzoneObj ozoneObject, RequestContext context)\r\n      throws OMException {\r\n    return false;\r\n  }\r\n{code}\r\n\r\nChange {code}OzoneAccessAuthrizerTest{code} to {code}OzoneAccessAuthorizerTest{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Fix typo in TestOmAcls"
   },
   {
      "_id": "13246691",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-23 22:51:29",
      "description": "Implement S3\u00a0Complete MPU\u00a0request to use OM Cache, double buffer.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement S3 Complete MPU request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13246688",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-23 22:26:37",
      "description": "All tests in TestOzoneManagerHA are failing with below exception.\r\n\r\n\u00a0\r\n\r\nBroken by HDDS-1649. Not sure why this test is not running in CI.\u00a0\r\n\r\nFrom PR HDDS-1845 run, not seeing this test run.\u00a0\r\n\r\n[https://ci.anzix.net/job/ozone/17452/testReport/org.apache.hadoop.ozone.om/]\r\n\r\n\u00a0\r\n{code:java}\r\njava.lang.Exception: test timed out after 300000 milliseconds\r\nat java.lang.Object.wait(Native Method)\r\n at java.lang.Object.wait(Object.java:502)\r\n at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59)\r\n at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)\r\n at org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n at org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\r\n at com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\r\n at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:326)\r\n at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getServiceList(OzoneManagerProtocolClientSideTranslatorPB.java:1155)\r\n at org.apache.hadoop.ozone.client.rpc.RpcClient.getScmAddressForClient(RpcClient.java:234)\r\n at org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:156)\r\n at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)\r\n at org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)\r\n at org.apache.hadoop.ozone.om.TestOzoneManagerHA.init(TestOzoneManagerHA.java:126)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)\r\n at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)\r\n at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n\u00a0\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestOzoneManagerHA and TestOzoneManagerSnapShotProvider"
   },
   {
      "_id": "13246440",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-07-23 00:28:30",
      "description": "<property>\r\n <name>ozone.client.checksum.type</name>\r\n <value>CRC32</value>\r\n <tag>OZONE, CLIENT, MANAGEMENT</tag>\r\n <description>The checksum type [NONE/ CRC32/ CRC32C/ SHA256/ MD5] determines\r\n which algorithm would be used to compute checksum for chunk data.\r\n Default checksum type is SHA256.\r\n </description>\r\n</property>\r\n\r\nOzoneConfigKeys.java\r\n\r\npublic static final String OZONE_CLIENT_CHECKSUM_TYPE_DEFAULT = \"SHA256\";\r\n\r\n\u00a0\r\n\r\nHDDS-1149 changes in ozone-default.xml, but not changed in java code. This Jira is to fix this issue.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Default value for checksum bytes is different in ozone-site.xml and code"
   },
   {
      "_id": "13246395",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-22 19:22:13",
      "description": "OMVolumeSetQuotaRequest#validateAndUpdateCache Line 115, we should return\u00a0\r\n\r\nOMVolumeSetQuotaResponse in the failure case.\r\n\r\n\u00a0\r\n\r\n{code}\r\n\r\nreturn new OMVolumeCreateResponse(null, null,\r\n createErrorOMResponse(omResponse, ex));\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix OMVolumeSetQuota|OwnerRequest#validateAndUpdateCache return response."
   },
   {
      "_id": "13246353",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-22 15:27:30",
      "description": "Currently, the stateMachineData which is the actual chunk data is maintained in the stateMachineCache inside ContainerStateMachine. Right now, the cache expiry is time based as well sized as per the no of parallel write chunks possible in the datanode. In case of optimal throughput, we may need to tune to it to a fraction of heap configured for the datanode process.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Tune the stateMachineDataCache to a reasonable fraction of Datanode Heap"
   },
   {
      "_id": "13246352",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-22 15:20:28",
      "description": "Right now, all write chunks use BufferedIO ie, sync flag is disabled by default. Also, Rocks Db metadata updates are done in Rocks DB cache first at Datanode. In case, there comes a situation where the buffered chunk data as well as the corresponding metadata update is lost as a part of datanode restart, it may lead to a situation where, it will not be possible to detect the corruption (not even with container scanner) of this nature in a reasonable time frame, until and unless there is a client IO failure or Recon server detects it over time. In order to atleast to detect the problem, Ratis snapshot on datanode should sync the rocks db file . In such a way, ContainerScanner will be able to detect this.We can also add a metric around sync to measure how much of a throughput loss it can incurr.\r\n\r\nThanks [~msingh] for suggesting this.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Undetectable corruption after restart of a datanode"
   },
   {
      "_id": "13246100",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-20 00:54:12",
      "description": "Implement S3\u00a0Abort MPU\u00a0request to use OM Cache, double buffer.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement S3 Abort MPU request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13246095",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-07-19 23:03:14",
      "description": "java.io.IOException: org.apache.hadoop.metrics2.MetricsException: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!\r\n\r\nat org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)\r\n at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:106)\r\n at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:169)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient.groupAdd(GrpcClientProtocolClient.java:138)\r\n at org.apache.ratis.grpc.client.GrpcClientRpc.sendRequest(GrpcClientRpc.java:94)\r\n at org.apache.ratis.client.impl.RaftClientImpl.sendRequest(RaftClientImpl.java:279)\r\n at org.apache.ratis.client.impl.RaftClientImpl.groupAdd(RaftClientImpl.java:206)\r\n at org.apache.hadoop.ozone.RatisTestHelper.initXceiverServerRatis(RatisTestHelper.java:135)\r\n at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.lambda$runTestClientServerRatis$4(TestSecureContainerServer.java:159)\r\n at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServer(TestSecureContainerServer.java:184)\r\n at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.runTestClientServerRatis(TestSecureContainerServer.java:155)\r\n at org.apache.hadoop.ozone.container.server.TestSecureContainerServer.testClientServerRatisGrpc(TestSecureContainerServer.java:131)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n at org.junit.runner.JUnitCore.run(JUnitCore.java:160)\r\n at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\r\n at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)\r\n at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\r\n at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\r\nCaused by: org.apache.hadoop.metrics2.MetricsException: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!\r\n at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n at org.apache.ratis.util.ReflectionUtils.instantiateException(ReflectionUtils.java:222)\r\n at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:104)\r\n ... 34 more\r\nCaused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: Metrics source CSMMetricsgroup-2947BAEC3941 already exists!\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)\r\n at org.apache.ratis.proto.grpc.AdminProtocolServiceGrpc$AdminProtocolServiceBlockingStub.groupManagement(AdminProtocolServiceGrpc.java:274)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient.lambda$groupAdd$3(GrpcClientProtocolClient.java:140)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:167)\r\n ... 32 more\r\n Suppressed: java.lang.IllegalStateException: Failed to cast the object to class java.io.IOException\r\n at org.apache.ratis.util.IOUtils.readObject(IOUtils.java:204)\r\n at org.apache.ratis.util.IOUtils.bytes2Object(IOUtils.java:195)\r\n at org.apache.ratis.grpc.GrpcUtil.tryUnwrapException(GrpcUtil.java:93)\r\n at org.apache.ratis.grpc.GrpcUtil.unwrapException(GrpcUtil.java:75)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient.blockingCall(GrpcClientProtocolClient.java:169)\r\n ... 32 more\r\n Caused by: java.lang.ClassCastException: Cannot cast org.apache.hadoop.metrics2.MetricsException to java.io.IOException\r\n at java.lang.Class.cast(Class.java:3369)\r\n at org.apache.ratis.util.IOUtils.readObject(IOUtils.java:200)\r\n ... 36 more",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestSecureContainerServer"
   },
   {
      "_id": "13246093",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-07-19 22:55:29",
      "description": "org.apache.hadoop.metrics2.MetricsException: Metrics source StorageContainerMetrics already exists!\r\n\r\nat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)\r\n at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)\r\n at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)\r\n at org.apache.hadoop.ozone.container.common.helpers.ContainerMetrics.create(ContainerMetrics.java:92)\r\n at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.<init>(OzoneContainer.java:93)\r\n at org.apache.hadoop.ozone.container.ozoneimpl.TestSecureOzoneContainer.testCreateOzoneContainer(TestSecureOzoneContainer.java:146)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestSecureOzoneContainer"
   },
   {
      "_id": "13245956",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-19 09:39:42",
      "description": "The default value of min leader election timeout currently is 5s(done with HDDS-1718) by default which is leading to leader election taking much longer time to timeout in case of network failures and leading to delayed creation of pipelines in the system. The idea is to change the default value to a lower value of \"2s\" for now.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Change the default value of ratis leader election min timeout to a lower value"
   },
   {
      "_id": "13245950",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-07-19 08:58:05",
      "description": "ozonesecure-ozonefs acceptance test is failing, because {{ozone fs -mkdir -p}} only creates key for the specific directory, not its parents.\r\n\r\n{noformat}\r\nozone fs -mkdir -p o3fs://bucket1.fstest/testdir/deep\r\n{noformat}\r\n\r\nPrevious result:\r\n\r\n{noformat:title=https://ci.anzix.net/job/ozone-nightly/176/artifact/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/result/log.html#s1-s16-t3-k2}\r\n$ ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.[].keyName'\r\ntestdir/\r\ntestdir/deep/\r\n{noformat}\r\n\r\nCurrent result:\r\n\r\n{noformat:title=https://ci.anzix.net/job/ozone-nightly/177/artifact/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/result/log.html#s1-s16-t3-k2}\r\n$ ozone sh key list o3://om/fstest/bucket1 | grep -v WARN | jq -r '.[].keyName'\r\ntestdir/deep/\r\n{noformat}\r\n\r\nThe failure happens on first operation that tries to use {{testdir/}} directly:\r\n\r\n{noformat}\r\n$ ozone fs -touch o3fs://bucket1.fstest/testdir/TOUCHFILE.txt\r\nls: `o3fs://bucket1.fstest/testdir': No such file or directory\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "parent directories not found in secure setup due to ACL check"
   },
   {
      "_id": "13245918",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-07-19 04:35:47",
      "description": "XceiverServerRatis should log the reason while sending the PipelineAction to the datanode.\r\nAlso on the PipelineActionHandler should also log the detailed reason for the action.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve logging for PipelineActions handling in SCM and datanode"
   },
   {
      "_id": "13245886",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-18 22:25:59",
      "description": "When Ratis server is starting it looks for the latest snapshot to load it. Even though OM does not save snapshots via Ratis, we need to load the saved snaphsot index into Ratis so that the LogAppender knows to not look for logs before the snapshot index. Otherwise, Ratis will replay the logs from beginning every time it starts up.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Load Snapshot info when OM Ratis server starts"
   },
   {
      "_id": "13245698",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-07-18 07:29:39",
      "description": "RatisPipelineProvider#initializePipeline does not logs the information about pipeline details and the failed nodes when initializePipeline fails. The debugging needs to be verbose to help in debugging.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "RatisPipelineProvider#initializePipeline logging needs to be verbose on failures/errors"
   },
   {
      "_id": "13245697",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-07-18 07:27:20",
      "description": "Exception is in SCMCommonPolicy.chooseDatanodes\r\n{code}\r\njava.lang.NullPointerException\r\n\tat java.util.Objects.requireNonNull(Objects.java:203)\r\n\tat java.util.ArrayList.removeAll(ArrayList.java:693)\r\n\tat org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMCommonPolicy.chooseDatanodes(SCMCommonPolicy.java:112)\r\n\tat org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom.chooseDatanodes(SCMContainerPlacementRandom.java:74)\r\n\tat org.apache.hadoop.hdds.scm.container.placement.algorithms.TestContainerPlacementFactory.testDefaultPolicy(TestContainerPlacementFactory.java:104)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{code}\r\n\r\ncc : [~xyao] [~Sammi]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "NPE in SCMCommonPolicy.chooseDatanodes"
   },
   {
      "_id": "13245696",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-07-18 07:24:39",
      "description": "When one of the datanode from the ratis pipeline is excluded by introducing network failure, the client write is failing with the following exception\r\n{noformat}\r\n2019-07-18 07:13:33 WARN  XceiverClientRatis:262 - 3 way commit failed on pipeline Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\nE         java.util.concurrent.ExecutionException: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 2 and log index 9 is not yet replicated to ALL_COMMITTED\r\nE         \tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395)\r\nE         \tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2022)\r\nE         \tat org.apache.hadoop.hdds.scm.XceiverClientRatis.watchForCommit(XceiverClientRatis.java:259)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:194)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchOnLastIndex(CommitWatcher.java:157)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:348)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)\r\nE         \tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)\r\nE         \tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)\r\nE         \tat org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)\r\nE         \tat org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)\r\nE         \tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\nE         \tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\nE         \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nE         \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nE         \tat java.base/java.lang.Thread.run(Thread.java:834)\r\nE         Caused by: org.apache.ratis.protocol.NotReplicatedException: Request with call Id 2 and log index 9 is not yet replicated to ALL_COMMITTED\r\nE         \tat org.apache.ratis.client.impl.ClientProtoUtils.toRaftClientReply(ClientProtoUtils.java:245)\r\nE         \tat org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:254)\r\nE         \tat org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:249)\r\nE         \tat org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)\r\nE         \tat org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)\r\nE         \tat org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)\r\nE         \tat org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)\r\nE         \tat org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\nE         \tat org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\nE         \t... 3 more\r\nE         2019-07-18 07:13:33 INFO  XceiverClientRatis:280 - Could not commit index 9 on pipeline Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN] to all the nodes. Server fa65a457-155d-4bf3-8d1b-b0e11ec157ae has failed. Committed by majority.\r\nE         2019-07-18 07:13:33 WARN  BlockOutputStream:354 - Failed to commit BlockId conID: 1 locID: 102461208643108865 bcsId: 9 on Pipeline[ Id: b338512c-1a3b-4ae6-b89c-7b7737d9bd93, Nodes: ce90cf89-0444-45bf-8c49-a126d8da5a5f{ip: 192.168.240.4, host: ozoneblockade_datanode_2.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: 192.168.240.6, host: ozoneblockade_datanode_3.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}c5785c99-7dc2-4afc-9054-2efa28a41e7e{ip: 192.168.240.2, host: ozoneblockade_datanode_1.ozoneblockade_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]. Failed nodes: [fa65a457-155d-4bf3-8d1b-b0e11ec157ae{ip: null, host: null, networkLocation: /default-rack, certSerialId: null}]\r\nE         2019-07-18 07:13:33 ERROR RandomKeyGenerator:730 - Exception while adding key: key-0-06904 in bucket: bucket-0-14179 of volume: vol-0-21379.\r\nE         java.lang.UnsupportedOperationException\r\nE         \tat java.base/java.util.AbstractList.add(AbstractList.java:153)\r\nE         \tat java.base/java.util.AbstractList.add(AbstractList.java:111)\r\nE         \tat java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:356)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)\r\nE         \tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)\r\nE         \tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)\r\nE         \tat org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)\r\nE         \tat org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)\r\nE         \tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\nE         \tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\nE         \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nE         \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nE         \tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\r\nE         java.lang.UnsupportedOperationException\r\nE         \tat java.base/java.util.AbstractList.add(AbstractList.java:153)\r\nE         \tat java.base/java.util.AbstractList.add(AbstractList.java:111)\r\nE         \tat java.base/java.util.AbstractCollection.addAll(AbstractCollection.java:352)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.watchForCommit(BlockOutputStream.java:356)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFlush(BlockOutputStream.java:480)\r\nE         \tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:494)\r\nE         \tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:143)\r\nE         \tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:434)\r\nE         \tat org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:472)\r\nE         \tat org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator.createKey(RandomKeyGenerator.java:706)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator.access$1100(RandomKeyGenerator.java:88)\r\nE         \tat org.apache.hadoop.ozone.freon.RandomKeyGenerator$ObjectCreator.run(RandomKeyGenerator.java:609)\r\nE         \tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\r\nE         \tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\nE         \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\nE         \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\nE         \tat java.base/java.lang.Thread.run(Thread.java:834)\r\nE\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "blockade"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "BlockOutputStream#watchForCommit fails with UnsupportedOperationException when one DN is down"
   },
   {
      "_id": "13245634",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-17 22:56:31",
      "description": "When we commit the key, we should increment numKeys in Ozone. This metrics shows the current count of keys in ozone. This is missed in OMKeyCommitRequest logic.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix numKeys metrics in OM HA"
   },
   {
      "_id": "13245610",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-17 19:21:44",
      "description": "Implement S3\u00a0Commit\u00a0MPU\u00a0request to use OM Cache, double buffer.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement S3 Commit MPU request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13245308",
      "assignee": "xyao",
      "components": [],
      "created": "2019-07-16 19:10:43",
      "description": "All acceptance passed but the results are marked failed due to the following warnings.\r\n\r\n[https://ci.anzix.net/job/ozone/17381/RobotTests/log.html]\r\n\r\n{code}\r\n\r\n[ WARN ] Collapsing consecutive whitespace during parsing is deprecated. Fix ' # Bucket already is created in Test Setup.' in file '/opt/hadoop/smoketest/s3/bucketcreate.robot' on line 31.\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix false warning from ozones3 acceptance test"
   },
   {
      "_id": "13245281",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-16 16:59:02",
      "description": "\r\n\r\n{code}\r\n2019-07-16 08:16:49,787 WARN org.apache.hadoop.fs.CachingGetSpaceUsed: Could not get disk usage information for path /data/3/ozone-0715\r\nExitCodeException exitCode=1: du: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/b113dd390e68e914d3ff405f3deec564_stream_60448f\r\n77-6349-48fa-ae86-b2d311730569_chunk_1.tmp.1.14118085': No such file or directory\r\ndu: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/37993af2849bdd0320d0f9d4a6ef4b92_stream_1f68be9f-e083-45e5-84a9-08809bc392ed\r\n_chunk_1.tmp.1.14118091': No such file or directory\r\ndu: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a38677def61389ec0be9105b1b4fddff_stream_9c3c3741-f710-4482-8423-7ac6695be96b\r\n_chunk_1.tmp.1.14118102': No such file or directory\r\ndu: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a689c89f71a75547471baf6182f3be01_stream_baf0f21d-2fb0-4cd8-84b0-eff1723019a0\r\n_chunk_1.tmp.1.14118105': No such file or directory\r\ndu: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/f58cf0fa5cb9360058ae25e8bc983e84_stream_d8d5ea61-995f-4ff5-88fb-4a9e97932f00\r\n_chunk_1.tmp.1.14118109': No such file or directory\r\ndu: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/a1d13ee6bbefd1f8156b1bd8db0d1b67_stream_db214bdd-a0c0-4f4a-8bc7-a3817e047e45_chunk_1.tmp.1.14118115': No such file or directory\r\ndu: cannot access '/data/3/ozone-0715/hdds/1b467d25-46cd-4de0-a4a1-e9405bde23ff/current/containerDir3/1724/chunks/8f8a4bd3f6c31161a70f82cb5ab8ee60_stream_d532d657-3d87-4332-baf8-effad9b3db23_chunk_1.tmp.1.14118127': No such file or directory\r\n\r\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)\r\n        at org.apache.hadoop.util.Shell.run(Shell.java:901)\r\n        at org.apache.hadoop.fs.DU$DUShell.startRefresh(DU.java:62)\r\n        at org.apache.hadoop.fs.DU.refresh(DU.java:53)\r\n        at org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:181)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Du while calculating used disk space reports that chunk files are file not found"
   },
   {
      "_id": "13245264",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-16 16:32:58",
      "description": "Datanodes can't be monitored with prometheus any more:\r\n\r\n{code}\r\nlevel=warn ts=2019-07-16T16:29:55.876Z caller=scrape.go:937 component=\"scrape manager\" scrape_pool=pods target=http://192.168.69.76:9882/prom msg=\"append failed\" err=\"invalid metric type \\\"apache.hadoop.ozone.container.common.transport.server.ratis._csm_metrics_delete_container_avg_time gauge\\\"\"\r\n{code}\r\n\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Prometheus metrics are broken for datanodes due to an invalid metric"
   },
   {
      "_id": "13245115",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-07-16 05:53:16",
      "description": "{code:java}\r\nError Message\r\ntest timed out after 30000 milliseconds\r\nStacktrace\r\njava.lang.Exception: test timed out after 30000 milliseconds\r\n\tat java.lang.Thread.sleep(Native Method)\r\n\tat org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:382)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineCreateAndDestory.waitForPipelines(TestRatisPipelineCreateAndDestory.java:126)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.TestRatisPipelineCreateAndDestory.testPipelineCreationOnNodeRestart(TestRatisPipelineCreateAndDestory.java:121)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestRatisPipelineCreateAndDestory#testPipelineCreationOnNodeRestart times out"
   },
   {
      "_id": "13245114",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-07-16 05:47:37",
      "description": "{code:java}\r\norg.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$407/213850519@1a8843a2 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=1000ms)\r\nStacktrace\r\njava.util.concurrent.ExecutionException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$407/213850519@1a8843a2 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=1000ms)\r\n\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\r\n\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestWatchForCommit.testWatchForCommitForRetryfailure(TestWatchForCommit.java:345)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{code}\r\nThe client here retries times with a delay of 1 sec between each retry but leader eleactiocouldnot complete.\r\n{code:java}\r\n2019-07-12 19:30:46,451 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46: receive RaftClientReply:client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 5931fd83-b899-480e-b15a-ecb8e7f7dd46 is not the leader (null). Request must be sent to leader., logIndex=0, commits[5931fd83-b899-480e-b15a-ecb8e7f7dd46:c-1]\r\n2019-07-12 19:30:47,469 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->d83929f1-c4db-499d-b67f-ad7f10dd7dde: receive RaftClientReply:client-6C83DC527A4C->d83929f1-c4db-499d-b67f-ad7f10dd7dde@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server d83929f1-c4db-499d-b67f-ad7f10dd7dde is not the leader (null). Request must be sent to leader., logIndex=0, commits[d83929f1-c4db-499d-b67f-ad7f10dd7dde:c-1]\r\n2019-07-12 19:30:48,504 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46: receive RaftClientReply:client-6C83DC527A4C->5931fd83-b899-480e-b15a-ecb8e7f7dd46@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 5931fd83-b899-480e-b15a-ecb8e7f7dd46 is not the leader (null). Request must be sent to leader., logIndex=0, commits[5931fd83-b899-480e-b15a-ecb8e7f7dd46:c-1]\r\n2019-07-12 19:30:49,540 INFO  client.GrpcClientProtocolClient (GrpcClientProtocolClient.java:onNext(255)) - client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509: receive RaftClientReply:client-6C83DC527A4C->73bdd98d-b003-44ff-a45b-bd12dfd50509@group-75C642DF7AE9, cid=55, FAILED org.apache.ratis.protocol.NotLeaderException: Server 73bdd98d-b003-44ff-a45b-bd12dfd50509 is not the leader (null). Request must be sent to leader., \r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestWatchForCommit#testWatchForCommitForRetryfailure fails as a result of no leader election for extended period of time "
   },
   {
      "_id": "13245099",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-16 05:03:20",
      "description": "Implement S3\u00a0Initiate MPU\u00a0request to use OM Cache, double buffer.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement S3 Initiate MPU request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13245087",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-16 03:05:06",
      "description": "# {{shellcheck.sh}} does not work on Mac\r\n{code}\r\nfind: -executable: unknown primary or operator\r\n{code}\r\n# {{$OUTPUT_FILE}} only contains problems from {{hadoop-ozone}}, not from {{hadoop-hdds}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "shellcheck.sh does not work on Mac"
   },
   {
      "_id": "13245077",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-16 00:19:31",
      "description": "In this Jira we will add eviction policy for table cache.\r\n\r\nIn this Jira, we will add 2 eviction policies for the cache.\r\n\r\nNEVER, // Cache will not be cleaned up. This mean's the table maintains\u00a0full cache.\r\nAFTERFLUSH // Cache will be cleaned up, once after flushing to DB.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add Eviction policy for table cache"
   },
   {
      "_id": "13245021",
      "assignee": "xyao",
      "components": [],
      "created": "2019-07-15 17:18:16",
      "description": "This helps stablize the ozone-0.4.1 release and fix HDDS-1705, HDDS-1751, HDDS-1713 and HDDS-1770 for 0.5.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make Topology Aware Replication/Read non-default for ozone 0.4.1   "
   },
   {
      "_id": "13244990",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-15 14:14:28",
      "description": "h2. What changes were proposed in this pull request?\r\nh2. \r\nFix:\r\n\r\n\u00a01. author check fails when no violations are found\r\n\u00a02. author check violations are duplicated in the output\r\n\r\nEg. https://ci.anzix.net/job/ozone-nightly/173/consoleText says that:\r\n\r\n\r\n{code:java}\r\nThe following tests are FAILED:\r\n\r\n[author]: author check is failed (https://ci.anzix.net/job/ozone-nightly/173//artifact/build/author.out/*view*/){code}\r\n\r\n\r\nbut no actual `@author` tags were found:\r\n\r\n{code}\r\n$ curl -s 'https://ci.anzix.net/job/ozone-nightly/173//artifact/build/author.out/*view*/' | wc\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\r\n{code}\r\n\r\nh2. How was this patch tested?\r\n\r\n{code}\r\n$ bash -o pipefail -c 'hadoop-ozone/dev-support/checks/author.sh | tee build/author.out'; echo $?\r\n0\r\n\r\n$ wc build/author.out\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0 build/author.out\r\n\r\n$ echo '// @author Tolkien' >> hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java\r\n\r\n$ bash -o pipefail -c 'hadoop-ozone/dev-support/checks/author.sh | tee build/author.out'; echo $?\r\n./hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/BucketManager.java:// @author Tolkien\r\n1\r\n\r\n$ wc build/author.out\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 3\u00a0\u00a0\u00a0\u00a0 108 build/author.out\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Result of author check is inverted"
   },
   {
      "_id": "13244955",
      "assignee": "elek",
      "components": [],
      "created": "2019-07-15 12:37:48",
      "description": "Goofys is a s3 fuse driver which is required for the ozone csi setup.\r\n\r\nAs of now it's installed in hadoop-ozone/dist/src/main/docker/Dockerfile from a non-standard location (because it couldn't be part of hadoop-runner earlier as it's ozone specific).\r\n\r\nIt should be installed to the ozone-runner from a canonical goffys release URL.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Add goofyfs to the ozone-runner docker image"
   },
   {
      "_id": "13244719",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-12 22:48:03",
      "description": "Implement S3 Bucket write requests to use OM Cache, double buffer.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement S3 Delete Bucket request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13244595",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-12 12:00:46",
      "description": "Since HDDS-1586 the smoketests of the ozone-topology compose file is broken:\r\n{code:java}\r\nOutput:  /tmp/smoketest/ozone-topology/result/robot-ozone-topology-ozone-topology-basic-scm.xml\r\nmust specify at least one container source\r\nStopping datanode_2 ... \r\nStopping datanode_3 ... \r\nStopping datanode_4 ... \r\nStopping scm        ... \r\nStopping om         ... \r\nStopping datanode_1 ... \r\n\u001b[6A\u001b[2K\r\nStopping datanode_2 ... \u001b[32mdone\u001b[0m\r\n\u001b[6B\u001b[4A\u001b[2K\r\nStopping datanode_4 ... \u001b[32mdone\u001b[0m\r\n\u001b[4B\u001b[1A\u001b[2K\r\nStopping datanode_1 ... \u001b[32mdone\u001b[0m\r\n\u001b[1B\u001b[5A\u001b[2K\r\nStopping datanode_3 ... \u001b[32mdone\u001b[0m\r\n\u001b[5B\u001b[3A\u001b[2K\r\nStopping scm        ... \u001b[32mdone\u001b[0m\r\n\u001b[3B\u001b[2A\u001b[2K\r\nStopping om         ... \u001b[32mdone\u001b[0m\r\n\u001b[2BRemoving datanode_2 ... \r\nRemoving datanode_3 ... \r\nRemoving datanode_4 ... \r\nRemoving scm        ... \r\nRemoving om         ... \r\nRemoving datanode_1 ... \r\n\u001b[1A\u001b[2K\r\nRemoving datanode_1 ... \u001b[32mdone\u001b[0m\r\n\u001b[1B\u001b[2A\u001b[2K\r\nRemoving om         ... \u001b[32mdone\u001b[0m\r\n\u001b[2B\u001b[5A\u001b[2K\r\nRemoving datanode_3 ... \u001b[32mdone\u001b[0m\r\n\u001b[5B\u001b[4A\u001b[2K\r\nRemoving datanode_4 ... \u001b[32mdone\u001b[0m\r\n\u001b[4B\u001b[6A\u001b[2K\r\nRemoving datanode_2 ... \u001b[32mdone\u001b[0m\r\n\u001b[6B\u001b[3A\u001b[2K\r\nRemoving scm        ... \u001b[32mdone\u001b[0m\r\n\u001b[3BRemoving network ozone-topology_net\r\n[ ERROR ] Reading XML source '/var/jenkins_home/workspace/ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-topology/result/robot-*.xml' failed: No such file or directory\r\n\r\nTry --help for usage information.\r\nERROR: Test execution of /var/jenkins_home/workspace/ozone/hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/ozone-topology is FAILED!!!!{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Acceptance test of ozone-topology cluster is failing"
   },
   {
      "_id": "13244480",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-07-12 00:26:43",
      "description": "Recon fails to startup in a kerberized cluster with the following error:\r\n\r\n\r\n{code:java}\r\nFailed startup of context o.e.j.w.WebAppContext@2009f9b0{/,file:///tmp/jetty-0.0.0.0-9888-recon-_-any-2565178148822292652.dir/webapp/,UNAVAILABLE}{/recon} javax.servlet.ServletException: javax.servlet.ServletException: Principal not defined in configuration at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:188) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194) at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180) at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139) at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873) at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:349) at org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1406) at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1368) at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:778) at org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:262) at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:522) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:113) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:131) at org.eclipse.jetty.server.Server.start(Server.java:427) at org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:105) at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:61) at org.eclipse.jetty.server.Server.doStart(Server.java:394) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1140) at org.apache.hadoop.hdds.server.BaseHttpServer.start(BaseHttpServer.java:175) at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:102) at org.apache.hadoop.ozone.recon.ReconServer.call(ReconServer.java:50) at picocli.CommandLine.execute(CommandLine.java:1173) at picocli.CommandLine.access$800(CommandLine.java:141) at picocli.CommandLine$RunLast.handle(CommandLine.java:1367) at picocli.CommandLine$RunLast.handle(CommandLine.java:1335) at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243) at picocli.CommandLine.parseWithHandlers(CommandLine.java:1526) at picocli.CommandLine.parseWithHandler(CommandLine.java:1465) at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65) at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56) at org.apache.hadoop.ozone.recon.ReconServer.main(ReconServer.java:61)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix kerberos principal error in Ozone Recon"
   },
   {
      "_id": "13244440",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-11 19:17:29",
      "description": "Right now, after after taking a new snapshot, the previous snapshot file is left in the raft log directory. When a new snapshot is taken, the previous snapshots should be deleted.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanodes takeSnapshot should delete previously created snapshots"
   },
   {
      "_id": "13244369",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-07-11 14:30:54",
      "description": "HDDS-1532 modified the concurrent framework usage of Freon (RandomKeyGenerator).\r\n\r\nThe new approach uses separated tasks (Runnable) to create the volumes/buckets/keys.\r\n\r\nUnfortunately it doesn't work very well in some cases.\r\n # When Freon starts it creates an executor with fixed number of threads (10)\r\n # The first loop submits numOfVolumes (10) VolumeProcessor tasks to the executor\r\n # The 10 threads starts to execute the 10 VolumeProcessor tasks\r\n # Each VolumeProcessor tasks creates numOfBuckets (1000) BucketProcessor tasks. All together 10000 tasks are submitted to the executor.\r\n # The 10 threads starts to execute the first 10 BucketProcessor tasks, they starts to create the KeyProcessor tasks: 500 000 * 10 tasks are submitted.\r\n # At this point of the time no keys are generated, but the next 10 BucketProcessor tasks are started to execute..\r\n # To execute the first key creation we should process all the BucketProcessor tasks which means that all the Key creation tasks (10 * 1000 * 500 000) are created and added to the executor\r\n # Which requires a huge amount of time and memory",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OOM error in Freon due to the concurrency handling"
   },
   {
      "_id": "13244213",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-11 03:56:47",
      "description": "applyTransaction is invoked from the Ratis pipeline and the ContainerStateMachine\r\n\r\nuses a async executor to complete the task.\r\n\r\n\u00a0\r\n\r\nWe require a latency metric to track the performance of log apply operations in the state machine. This will measure the end-to-end latency of apply which includes the queueing delay in the executor queues. Combined with the latency measurement in HddsDispatcher, this will be an indicator if the executors are overloaded.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Latency metric for applyTransaction in ContainerStateMachine"
   },
   {
      "_id": "13244108",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-07-10 13:09:25",
      "description": "The tests seem to fail bcoz , when the datanode goes down with stale node interval being set to a low value, containers may get closed early and client writes might fail with closed container exception rather than pipeline failure/Timeout exceptions as excepted in the tests. The fix made here is to tune the stale node interval.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestFailureHandlingByClient tests are flaky"
   },
   {
      "_id": "13244070",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-07-10 10:08:05",
      "description": "The tests have become flaky bcoz once\u00a0 nodes are shutdown inn Ratis pipeline, a watch request can either be received at server at a server and fail with NotReplicatedException or sometimes it fails with StatusRuntimeExceptions from grpc which both need to be accounted for in the tests. Other than that, HDDS-1384 also causes bind exception to e thrown intermittently which in turn shuts down the miniOzoneCluster. To overcome this, the test class has been refactored as well.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestWatchForCommit tests are flaky"
   },
   {
      "_id": "13243962",
      "assignee": "xyao",
      "components": [],
      "created": "2019-07-09 20:45:30",
      "description": "The docker compose file has invalid reference to scm images, which fails the docker-compose up with errors like below. This ticket is opened to fix them.\r\n\r\n\u00a0\r\n{code:java}\r\nERROR: no such image: apache/ozone-runner::20190617-2: invalid reference format}\r\n\r\nor \r\n\r\nERROR: no such image: apache/ozone-runner:latest:20190617-2: invalid reference format{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix image name in some ozone docker-compose files"
   },
   {
      "_id": "13243793",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-09 01:46:19",
      "description": "Currently OM KeyDeletingService directly deletes all the keys in DeletedTable after deleting the corresponding blocks through SCM. For HA compatibility, the key purging should happen through the OM Ratis server. This Jira introduces PurgeKeys request in OM protocol. This request will be submitted to OMs Ratis server after SCM deletes blocks corresponding to deleted keys.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make OM KeyDeletingService compatible with HA model"
   },
   {
      "_id": "13243234",
      "assignee": "elek",
      "components": [],
      "created": "2019-07-04 15:23:25",
      "description": "[~bharatviswa] pinged me offline with the problem that in some cases the smoketest is failing even if the reports are green:\r\n{code:java}\r\nAll smoke tests are passed, but CI is showing as Failed.\r\n\r\nhttps://ci.anzix.net/job/ozone/17284/RobotTests/log.html\r\nhttps://github.com/apache/hadoop/pull/1048{code}\r\nThe root cause is a few typo after HDDS-1698, which can be fixed with the uploaded PR.\r\n\r\n*What is the problem?*\r\n\r\nIn case of any error during the test execution the smoketest is failed. In this case because the typo in two docker-compose.yaml files two of the tests can't be started.\r\n\r\nBut there is no separated robot test report and the error is visible only in the console.\r\n\r\n*How did it happen?*\r\n\r\nThe ACL work improved some intermittency in the acceptance tests. HDDS-1698 is committed because the acceptance tests were failed with ACL errors which hide the real error (the test was red anyway).\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix hidden errors in acceptance tests"
   },
   {
      "_id": "13243231",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-07-04 15:12:48",
      "description": "In HDDS-1639 we restructured the ozone documentation and a new overview page is added to the main page.\r\n\r\nThis page contains an official aws logo, As [~bharatviswa] reported we are not sure about the exact condition to use logos / trademarks from Amazon. It's better to remain on the safe side and use a neutral S3 label.\r\n\r\nIn this patch the aws logo is replaced with an orange cloud + s3 text.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use vendor neutral s3 logo in ozone doc"
   },
   {
      "_id": "13243065",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-03 18:18:58",
      "description": "The patch looks mostly fine to me. A few minor comments.\u00a0-and one type error that needs to be fixed.-\r\n\r\nI would like to see the class hierarchy refactored in a follow up patch.\u00a0{{OMFileCreateRequest}}should not extend\u00a0{{OMKeyCreateRequest}}. Instead they should both extend an abstract class that encapsulates the common functionality.\r\n\r\nGenerally deriving from\u00a0_concrete_\u00a0classes is a bad idea.\r\n\r\n\u00a0\r\n\r\nThis Jira is created based on [~arp] comment during review of HDDS-1731",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix class hierarchy for KeyRequest and FileRequest classes."
   },
   {
      "_id": "13242825",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-07-02 18:41:37",
      "description": "In the current code in applyTransaction we have\u00a0\r\n\r\nCompletableFuture<Message> future = CompletableFuture\r\n .supplyAsync(() -> runCommand(request, trxLogIndex)); We are using ForkJoin#commonPool.\r\n\r\nWith the current approach we have 2 issues:\r\n # Thread exhausts when using this common pool.\r\n # Not a good practice of using common pool. Found some issues in our testing by using similarly in RatisPipelineUtils.\r\n # OM DB's across replica can be out of sync when the apply transactions are applied in out of order.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use ExecutorService in OzoneManagerStateMachine"
   },
   {
      "_id": "13242770",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-07-02 14:14:53",
      "description": "Leader datanode is unable to read chunk from the datanode while replicating data from leader to follower.\r\nPlease note that deletion of keys is also happening while the data is being replicated.\r\n\r\n{code}\r\n2019-07-02 19:39:22,604 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1\r\n014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#70:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782\r\n2019-07-02 19:39:22,605 ERROR impl.ChunkManagerImpl (ChunkUtils.java:readData(161)) - Unable to find the chunk file. chunk info : ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3\r\n-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048}\r\n2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(990)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: Failed appendEntries as latest snapshot (9770) already h\r\nas the append entries (first index: 1)\r\n2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1\r\n014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#71:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782\r\n2019-07-02 19:39:22,605 INFO  keyvalue.KeyValueHandler (ContainerUtils.java:logAndReturnError(146)) - Operation: ReadChunk : Trace ID: 4216d461a4679e17:4216d461a4679e17:0:0 : Message: Unable to find the c\r\nhunk file. chunk info ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048} : Result: UNABLE_TO_FIND_CHUNK\r\n2019-07-02 19:39:22,605 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(990)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: Failed appendEntries as latest snapshot (9770) already h\r\nas the append entries (first index: 2)\r\n2019-07-02 19:39:22,606 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(972)) - 5ac88709-a3a2-4c8f-91de-5e54b617f05e: inconsistency entries. Reply:76a3eb0f-d7cd-477b-8973-db1\r\n014feb398<-5ac88709-a3a2-4c8f-91de-5e54b617f05e#72:FAIL,INCONSISTENCY,nextIndex:9771,term:2,followerCommit:9782\r\n19:39:22.606 [pool-195-thread-19] ERROR DNAudit - user=null | ip=null | op=READ_CHUNK {blockData=conID: 3 locID: 102372189549953034 bcsId: 0} | ret=FAILURE\r\njava.lang.Exception: Unable to find the chunk file. chunk info ChunkInfo{chunkName='76ec669ae2cb6e10dd9f08c0789c5fdf_stream_a2850dce-def3-4d64-93d8-fa2ebafee933_chunk_1, offset=0, len=2048}\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:320) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:346) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.readStateMachineData(ContainerStateMachine.java:476) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$getCachedStateMachineData$2(ContainerStateMachine.java:495) ~[hadoop-hdds-container-service-0.5.0-SN\r\nAPSHOT.jar:?]\r\n        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4767) ~[guava-11.0.2.jar:?]\r\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568) ~[guava-11.0.2.jar:?]\r\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350) ~[guava-11.0.2.jar:?]\r\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313) ~[guava-11.0.2.jar:?]\r\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228) ~[guava-11.0.2.jar:?]\r\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3965) ~[guava-11.0.2.jar:?]\r\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4764) ~[guava-11.0.2.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.getCachedStateMachineData(ContainerStateMachine.java:494) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.ja\r\nr:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$readStateMachineData$4(ContainerStateMachine.java:542) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_171]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanode unable to find chunk while replication data using ratis."
   },
   {
      "_id": "13242682",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-07-02 08:35:27",
      "description": "Currently the list of nodes returned by SCM are static and are returned in the same order to all the clients. Ideally these should be sorted by the network topology and then returned to client.\r\n\r\nHowever even when network topology in not available, then SCM/client should randomly sort the nodes before choosing the replica's to connect.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone Client should randomize the list of nodes in pipeline for reads"
   },
   {
      "_id": "13242320",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-29 22:01:30",
      "description": "HDDS-1638 brought in HA code for Key operations like allocateBlock,createKey etc.,\u00a0\r\n\r\nOld code changes which are added as part of HDDS-1250 and HDDS-1262 for allocateBlock and openKey.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cleanup 2phase old HA code for Key requests."
   },
   {
      "_id": "13242256",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-28 23:51:33",
      "description": "hadoop-ozone/dev-support/checks directory contains multiple helper script to execute different type of testing (findbugs, rat, unit, build).\r\n\r\nThey easily define how tests should be executed, with the following contract:\r\n\r\n\u00a0* The problems should be printed out to the console\r\n\r\n\u00a0* in case of test failure a non zero exit code should be used\r\n\r\n\u00a0\r\n\r\nThe tests are working well (in fact I have some experiments with executing these scripts on k8s and argo where all the shell scripts are executed parallel) but we need some update:\r\n\r\n\u00a01. Most important: the unit tests and integration tests can be separated. Integration tests are more flaky and it's better to have a way to run only the normal unit tests\r\n\r\n\u00a02. As HDDS-1115 introduced a pom.ozone.xml it's better to use them instead of the magical \"am pl hadoop-ozone-dist\" trick--\r\n\r\n\u00a03. To make it possible to run blockade test in containers we should use - T flag with docker-compose\r\n\r\n\u00a04. checkstyle violations are printed out to the console",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create separated unit and integration test executor dev-support scripts"
   },
   {
      "_id": "13242059",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-27 23:53:25",
      "description": "In this Jira, we shall implement createFile\u00a0request according to the HA model, and use cache and double buffer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement File CreateFile Request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13242026",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-27 20:52:52",
      "description": "In this Jira, we shall implement createDirectory request according to the HA model, and use cache and double buffer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement File CreateDirectory Request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13241569",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-06-25 19:19:37",
      "description": "In this Jira, we shall use generate Resourcename from actual resource names like volume/bucket/user/key inside OzoneManagerLock. In this way, users using these locking API's no need to worry of calling these additional API of generateResourceName in OzoneManagerLockUtil. And this also reduces code during acquiring locks in OM operations.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use generation of resourceName for locks in OzoneManagerLock"
   },
   {
      "_id": "13241308",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-24 17:13:47",
      "description": "[~rmaruthiyodan] reported two problems regarding to the pv-test example in csi examples folder.\r\n\r\npv-test folder contains an example nginx deployment which can use an ozone PVC/PV to publish content of a folder via http.\r\n\r\nTwo problems are identified:\r\n * The label based matching filter of service doesn't point to the nginx deployment\r\n * The configmap mounting is missing from nginx deployment",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "pv-test example to test csi is not working"
   },
   {
      "_id": "13240985",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-22 00:05:04",
      "description": "This Jira is to use bit manipulation, instead of hashmap in OzoneManager lock logic. And also this Jira follows the locking order based on the document attached to HDDS-1672 jira.\r\n\r\nThis Jira is created based on [~anu] comment during review of HDDS-1672.\r\n\r\nNot a suggestion for this patch. But more of a question, should we just maintain a bitset here, and just flip that bit up and down to see if the lock is held. Or we can just maintain 32 bit integer, and we can easily find if a lock is held by Xoring with the correct mask. I feel that might be super efficient.\u00a0[@nandakumar131|https://github.com/nandakumar131]\u00a0. But as I said let us not do that in this patch.\r\n\r\n\u00a0\r\n\r\nThis Jira will add new class, integration of this new class into code will be done in a new jira.\u00a0\r\n\r\nClean up of old code also will be done in new jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create new OzoneManagerLock class"
   },
   {
      "_id": "13240949",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-06-21 20:18:33",
      "description": "Currently the table creation is done for each schema definition one by one. \r\n\r\nSetup sqlite DB and create Recon SQL tables.\r\ncc [~vivekratnavel], [~swagle]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use the bindings in ReconSchemaGenerationModule to create Recon SQL tables on startup"
   },
   {
      "_id": "13240940",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-06-21 19:12:05",
      "description": "Client Metrics are not being pushed to the configured sink while running a hadoop command to write to Ozone.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Client Metrics are not being pushed to the configured sink while running a hadoop command to write to Ozone."
   },
   {
      "_id": "13240939",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-06-21 19:10:49",
      "description": "While doing performance testing, it was seen that there was no way to get RocksDB logs for Ozone Manager. Along with Rocksdb metrics, this may be a useful mechanism to understand the health of Rocksdb while investigating large clusters. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ability to configure RocksDB logs for Ozone Manager"
   },
   {
      "_id": "13240936",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-06-21 19:02:23",
      "description": "While testing out ozone with long running clients which continuously write data, it was noted ratis logs were rolled 1-2 times every second. This adds unnecessary overhead to the pipeline thereby affecting write throughput. Increasing the size of the log segment to 1MB will decrease the overhead.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Increase ratis log segment size to 1MB."
   },
   {
      "_id": "13240851",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-06-21 11:58:20",
      "description": "Mapreduce Jobs are failing with exception ??Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient exception??\r\n\r\nOzone hadoop-ozone-filesystem-lib-current.jar copied to HDP cluster's hadoop and mapreduce classpath under :\r\n\r\n{code:java}\r\n/usr/hdp/3.1.0.0-78/hadoop/lib/hadoop-ozone-filesystem-lib-current-0.5.0-SNAPSHOT.jar\r\n/usr/hdp/3.1.0.0-78/hadoop-mapreduce/hadoop-ozone-filesystem-lib-current-0.5.0-SNAPSHOT.jar\r\n{code}\r\n\r\nExcerpt from exception :\r\n{code:java}\r\n2019-06-21 10:07:57,982 ERROR [main] org.apache.hadoop.ozone.client.OzoneClientFactory: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient exception:\r\njava.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)\r\n\tat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\r\n\tat org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\r\n\tat org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)\r\nCaused by: java.lang.VerifyError: Cannot inherit from final class\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)\r\n\tat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)\r\n\t... 33 more\r\n2019-06-21 10:07:57,985 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state INITED\r\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)\r\nCaused by: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:299)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)\r\n\tat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\r\n\tat org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\r\n\tat org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)\r\n\t... 11 more\r\nCaused by: java.lang.VerifyError: Cannot inherit from final class\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)\r\n\tat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)\r\n\t... 28 more\r\n2019-06-21 10:07:57,987 ERROR [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster\r\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:554)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:534)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.callWithJobClassLoader(MRAppMaster.java:1802)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.createOutputCommitter(MRAppMaster.java:534)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceInit(MRAppMaster.java:311)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6.run(MRAppMaster.java:1760)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1757)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1691)\r\nCaused by: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:299)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getRpcClient(OzoneClientFactory.java:169)\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneClientAdapterImpl.<init>(BasicOzoneClientAdapterImpl.java:134)\r\n\tat org.apache.hadoop.fs.ozone.OzoneClientAdapterImpl.<init>(OzoneClientAdapterImpl.java:50)\r\n\tat org.apache.hadoop.fs.ozone.OzoneFileSystem.createAdapter(OzoneFileSystem.java:103)\r\n\tat org.apache.hadoop.fs.ozone.BasicOzoneFileSystem.initialize(BasicOzoneFileSystem.java:143)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\r\n\tat org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory.createFileOutputCommitter(PathOutputCommitterFactory.java:134)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory.createOutputCommitter(FileOutputCommitterFactory.java:35)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.getOutputCommitter(FileOutputFormat.java:338)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$3.call(MRAppMaster.java:552)\r\n\t... 11 more\r\nCaused by: java.lang.VerifyError: Cannot inherit from final class\r\n\tat java.lang.ClassLoader.defineClass1(Native Method)\r\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\r\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.<init>(OzoneManagerProtocolClientSideTranslatorPB.java:190)\r\n\tat org.apache.hadoop.ozone.client.rpc.RpcClient.<init>(RpcClient.java:153)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientFactory.getClientProtocol(OzoneClientFactory.java:291)\r\n\t... 28 more\r\n2019-06-21 10:07:57,988 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Couldn't create protocol class org.apache.hadoop.ozone.client.rpc.RpcClient\r\n\r\nEnd of LogType:syslog\r\n***********************************************************************\r\n{code}\r\n\r\n\r\nPFA YARN application logs for detailed exception.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "MR Job fails as OMFailoverProxyProvider has dependency hadoop-3.2"
   },
   {
      "_id": "13240844",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-21 11:19:28",
      "description": "[~eyang] reported the problem in HDDS-1609 that the smoketest results are generated a user (the user inside the docker container) which can be different from the host user.\r\n\r\nThere is a minimal risk that the test results can be deleted/corrupted by an other users if the current user is different from uid=1000\r\n\r\nI opened this issue because [~eyang] said me during an offline discussion that HDDS-1609 is a more complex issue and not only about the ownership of the test results.\r\n\r\nI suggest to handle the two problems in different way. With this patch, the permission of the test result files can be fixed easily.\r\n\r\nIn HDDS-1609 we can discuss about general security problems and try to find generic solution for them.\r\n\r\nSteps to reproduce _this_ problem:\r\n # Use a user which is different from uid=1000\r\n # Create a new ozone build (mvn clean install -f pom.ozone.xml -DskipTests)\r\n # Go to a compose directory (cd hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT/compose/)\r\n # Execute tests (./test.sh)\r\n # check the ownership of the results (ls -lah ./results)\r\n\r\nCurrent result: the owner of the result files are the user uid=1000\r\n\r\nExpected result: the owner of the files should be always the current user (even if the current uid is different)\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Smoketest results are generated with an internal user"
   },
   {
      "_id": "13240801",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-21 07:34:37",
      "description": "the error stack:\r\n{code:java}\r\n2019-06-21 15:17:41,744 [main] INFO - Loaded 11 tokens\r\n2019-06-21 15:17:41,745 [main] INFO - Loading token state into token manager.\r\n2019-06-21 15:17:41,748 [main] ERROR - Failed to start the OzoneManager.\r\njava.lang.NullPointerException\r\nat org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.addPersistedDelegationToken(OzoneDelegationTokenSecretManager.java:371)\r\nat org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.loadTokenSecretState(OzoneDelegationTokenSecretManager.java:358)\r\nat org.apache.hadoop.ozone.security.OzoneDelegationTokenSecretManager.<init>(OzoneDelegationTokenSecretManager.java:96)\r\nat org.apache.hadoop.ozone.om.OzoneManager.createDelegationTokenSecretManager(OzoneManager.java:608)\r\nat org.apache.hadoop.ozone.om.OzoneManager.<init>(OzoneManager.java:332)\r\nat org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:941)\r\nat org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)\r\n2019-06-21 15:17:41,753 [pool-2-thread-1] INFO - SHUTDOWN_MSG:\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "When restart om with Kerberos, NPException happened at addPersistedDelegationToken "
   },
   {
      "_id": "13240607",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-06-20 08:27:24",
      "description": "In ozone metrics can be published with the help of hadoop metrics (for example via PrometheusMetricsSink)\r\n\r\nThe basic jvm metrics are not published by the metrics system (just with JMX)\r\n\r\nI am very interested about the basic JVM metrics (gc count, heap memory usage) to identify possible problems in the test environment.\r\n\r\nFortunately it's very easy to turn it on with the help of org.apache.hadoop.metrics2.source.JvmMetrics.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Publish JVM metrics via Hadoop metrics"
   },
   {
      "_id": "13240605",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-06-20 08:17:12",
      "description": "org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode is failed at last night with the following error:\r\n{code:java}\r\njava.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeMode(TestScmSafeMode.java:285) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}\r\nLocally it can be tested but it's very easy to reproduce by adding an additional sleep DataNodeSafeModeRule:\r\n{code:java}\r\n+++ b/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/safemode/DataNodeSafeModeRule.java\r\n@@ -63,7 +63,11 @@ protected boolean validate() {\r\n\u00a0\r\n\u00a0\u00a0 @Override\r\n\u00a0\u00a0 protected void process(NodeRegistrationContainerReport reportsProto) {\r\n-\r\n+\u00a0\u00a0\u00a0 try {\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0 Thread.sleep(3000);\r\n+\u00a0\u00a0\u00a0 } catch (InterruptedException e) {\r\n+\u00a0\u00a0\u00a0\u00a0\u00a0 e.printStackTrace();\r\n+\u00a0\u00a0\u00a0 }{code}\r\nThis is a clear race condition:\r\n\r\nDatanodeSafeModeRule and ContainerSafeModeRule are processing the same events but it can be possible (in case of an accidental sleep) that the container safe mode rule is done, but DatanodeSafeModeRule didn't process the new event (yet).\r\n\r\nAs a result the test execution will continue:\r\n{code:java}\r\nGenericTestUtils\r\n    .waitFor(() -> scm.getCurrentContainerThreshold() == 1.0, 100, 20000);\r\n{code}\r\n(This line is waiting ONLY for the ContainerSafeModeRule).\r\n\r\nThe fix is easy, let's wait for the processing of all the async events:\r\n{code:java}\r\nEventQueue eventQueue =\r\n    (EventQueue) cluster.getStorageContainerManager().getEventQueue();\r\neventQueue.processAll(5000L);{code}\r\nAs we are sure that the events are already sent to the EventQueue (because we have the previous waitFor), it should be enough.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "TestScmSafeNode is flaky"
   },
   {
      "_id": "13240059",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-06-17 20:48:38",
      "description": "Currently, hadoop-ozone-recon\u00a0node_modules folder is copied to target folder and this takes a lot of time while building hadoop-ozone project. Reduce the build time by excluding node_modules folder.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Optimize Ozone Recon build time "
   },
   {
      "_id": "13240009",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-06-17 17:40:13",
      "description": "Since HDDS-1634 we have an ozone specific runner image to run ozone with docker-compose based pseudo clusters.\r\n\r\nAs the new apache/ozone-runner image is uploaded to the dockerhub we can switch our scripts and use the new image.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Switch to use apache/ozone-runner in the compose/Dockerfile"
   },
   {
      "_id": "13240005",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-06-17 17:08:32",
      "description": "This will help on production systems where\u00a0WAL\u00a0logs and db actual data files will be in a different location. During compaction, it will not affect actual writes.\r\n\r\n\u00a0\r\n\r\nSuggested by [~msingh]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "RocksDB use separate Write-ahead-log location for RocksDB."
   },
   {
      "_id": "13239968",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-06-17 15:17:41",
      "description": "{code:java}\r\nFAILURE in ozone-unit-076618677d39x4h9/unit/hadoop-hdds/server-scm/org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.txt\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.hadoop.hdds.scm.node.TestNodeReportHandler\r\n-------------------------------------------------------------------------------\r\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.43 s <<< FAILURE! - in org.apache.hadoop.hdds.scm.node.TestNodeReportHandler\r\ntestNodeReport(org.apache.hadoop.hdds.scm.node.TestNodeReportHandler)\u00a0 Time elapsed: 0.288 s\u00a0 <<< ERROR!\r\njava.lang.NullPointerException\r\n\u00a0\u00a0 \u00a0at org.apache.hadoop.hdds.scm.node.SCMNodeManager.<init>(SCMNodeManager.java:122)\r\n\u00a0\u00a0 \u00a0at org.apache.hadoop.hdds.scm.node.TestNodeReportHandler.resetEventCollector(TestNodeReportHandler.java:53)\r\n\u00a0\u00a0 \u00a0at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u00a0\u00a0 \u00a0at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u00a0\u00a0 \u00a0at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u00a0\u00a0 \u00a0at java.lang.reflect.Method.invoke(Method.java:498)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\u00a0\u00a0 \u00a0at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\u00a0\u00a0 \u00a0at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\u00a0\u00a0 \u00a0at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\u00a0\u00a0 \u00a0at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n\r\n2019-06-16 23:52:29,345 INFO\u00a0 node.SCMNodeManager (SCMNodeManager.java:<init>(119)) - Entering startup safe mode.\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "TestNodeReportHandler is failing with NPE"
   },
   {
      "_id": "13239813",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-16 16:59:06",
      "description": "Iterator normally do a bulk load of the keys, this causes thrashing of the actual keys in the DB.\r\n\r\nThis option is documented here:-\r\nhttps://github.com/facebook/rocksdb/wiki/Basic-Operations#cache",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "RDBTable#iterator should disabled caching of the keys during iterator"
   },
   {
      "_id": "13239812",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-06-16 16:54:06",
      "description": "RDBTable#isExist can use Rocksdb#keyMayExist, this avoids the cost of reading the value for the key.\r\n\r\nPlease refer, \r\nhttps://github.com/facebook/rocksdb/blob/7a8d7358bb40b13a06c2c6adc62e80295d89ed05/java/src/main/java/org/rocksdb/RocksDB.java#L2184",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "RDBTable#isExist should use Rocksdb#keyMayExist"
   },
   {
      "_id": "13239655",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-14 22:31:50",
      "description": "Implement S3 Bucket write requests to use OM Cache, double buffer.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement S3 bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement S3 Create Bucket request to use Cache and DoubleBuffer"
   },
   {
      "_id": "13239388",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-06-13 22:33:30",
      "description": "* Support \"start\" query param to seek to the given key in RocksDB.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon: Add support for \"start\" query param to containers and containers/{id} endpoints"
   },
   {
      "_id": "13239387",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-06-13 22:33:10",
      "description": "In OM, Ratis related dirs (storage, snapshot etc.) should only be created if OM ratis is enabled.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM should create Ratis related dirs only if ratis is enabled"
   },
   {
      "_id": "13239295",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-06-13 14:25:37",
      "description": "TestEventWatcher is intermittent. (Failed twice out of 44 executions).\r\n\r\nError is:\r\n\r\n{code}\r\nTests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.764 s <<< FAILURE! - in org.apache.hadoop.hdds.server.events.TestEventWatcher\r\ntestMetrics(org.apache.hadoop.hdds.server.events.TestEventWatcher)  Time elapsed: 2.384 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<2> but was:<3>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat org.junit.Assert.assertEquals(Assert.java:555)\r\n\tat org.junit.Assert.assertEquals(Assert.java:542)\r\n\tat org.apache.hadoop.hdds.server.events.TestEventWatcher.testMetrics(TestEventWatcher.java:197)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{code}\r\n\r\nIn the test we do the following:\r\n\r\n 1. fire start-event1\r\n 2. fire start-event2\r\n 3. fire start-event3\r\n 4. fire end-event1\r\n 5. wait\r\n\r\nUsually the event2 and event3 are timed out and event1 is completed but in case of an accidental time between 3 and 4 (in fact between 1 and 4) the event1 also can be timed out.\r\n\r\nI improved the unit test and fixed the metrics calculation (completed message should be incremented only if it's not yet timed out).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestEventWatcher.testMetrics is flaky"
   },
   {
      "_id": "13239270",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-13 12:27:35",
      "description": "I started to execute all the unit tests continuously (in kubernetes with argo workflow).\r\n\r\nUntil now I got the following failures (number of failures / unit test name):\r\n\r\n```\r\n      1 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractMkdir\r\n      1 org.apache.hadoop.fs.ozone.contract.ITestOzoneContractRename\r\n      3 org.apache.hadoop.hdds.scm.container.placement.algorithms.TestSCMContainerPlacementRackAware\r\n     31 org.apache.hadoop.ozone.container.common.TestDatanodeStateMachine\r\n     31 org.apache.hadoop.ozone.container.common.volume.TestVolumeSet\r\n      1 org.apache.hadoop.ozone.freon.TestDataValidateWithSafeByteOperations\r\n```\r\n\r\nTestVolumeSet is also failed locally:\r\n\r\n{code}\r\n2019-06-13 14:23:18,637 ERROR volume.VolumeSet (VolumeSet.java:initializeVolumeSet(184)) - Failed to parse the storage location: /home/elek/projects/hadoop/hadoop-hdds/container-service/target/test-dir/dfs\r\njava.io.IOException: Cannot create directory /home/elek/projects/hadoop/hadoop-hdds/container-service/target/test-dir/dfs/hdds\r\n\tat org.apache.hadoop.ozone.container.common.volume.HddsVolume.initialize(HddsVolume.java:208)\r\n\tat org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:179)\r\n\tat org.apache.hadoop.ozone.container.common.volume.HddsVolume.<init>(HddsVolume.java:72)\r\n\tat org.apache.hadoop.ozone.container.common.volume.HddsVolume$Builder.build(HddsVolume.java:156)\r\n\tat org.apache.hadoop.ozone.container.common.volume.VolumeSet.createVolume(VolumeSet.java:311)\r\n\tat org.apache.hadoop.ozone.container.common.volume.VolumeSet.initializeVolumeSet(VolumeSet.java:165)\r\n\tat org.apache.hadoop.ozone.container.common.volume.VolumeSet.<init>(VolumeSet.java:130)\r\n\tat org.apache.hadoop.ozone.container.common.volume.VolumeSet.<init>(VolumeSet.java:109)\r\n\tat org.apache.hadoop.ozone.container.common.volume.TestVolumeSet.testFailVolumes(TestVolumeSet.java:232)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{code}\r\n\r\nThe problem here is that the parent directory of the volume dir is missing. I propose to use hddsRootDir.mkdirs() instead of hddsRootDir.mkdir() which creates the missing parent directories.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create missing parent directories during the creation of HddsVolume dirs"
   },
   {
      "_id": "13239202",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-06-13 08:49:14",
      "description": "During the build the kubernetes example files are adjusted to use a specific docker image name.\r\n\r\nBy default it should be the apache/ozone:${VERSION} to make it possible to use the examples without any build from the release artifact. With the examples of the release artifact the user can use the latest released apache/ozone:${VERSION} from docker hub.\r\n\r\nFor development build the image can be set with -Ddocker.image (or -Dozone.docker.image with HDDS-1667).\r\n\r\nUnfortunately -- due to a small typo -- apace/hadoop image is used by default instead of apache/ozone.  ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Default image name for kubernetes examples should be ozone and not hadoop"
   },
   {
      "_id": "13239198",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-06-13 08:16:17",
      "description": "When I tried to reproduce a problem which is reported by [~eyang], I found that the auditparser robot test uses the /opt/hadoop directory as a working directory to generate the audit.db export.\r\n\r\n/opt/hadoop is may or may not be writable, it's better to use /tmp instead.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Auditparser robot test shold use a world writable working directory"
   },
   {
      "_id": "13239152",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-13 01:05:10",
      "description": "This Jira is to clean up the old 2 phase HA code for Volume requests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Cleanup Volume Request 2 phase old code"
   },
   {
      "_id": "13238936",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-12 05:52:03",
      "description": "In this Jira, we shall follow the new lock ordering. In this way, in volume requests we can solve the issue of\u00a0acquire/release/reacquire problem. And few bugs in the current implementation of S3Bucket/Volume operations.\r\n\r\n\u00a0\r\n\r\nCurrently after acquiring volume lock, we cannot acquire user lock.\u00a0\r\n\r\nThis is causing an issue in Volume request implementation, acquire/release/reacquire volume lock.\r\n\r\n\u00a0\r\n\r\nCase of Delete Volume Request:\u00a0\r\n # Acquire volume lock.\r\n # Get Volume Info from DB\r\n # Release Volume lock. (We are releasing the lock, because while acquiring volume lock, we cannot acquire user lock0\r\n # Get owner from volume Info read from DB\r\n # Acquire owner lock\r\n # Acquire volume lock\r\n # Do delete logic\r\n # release volume lock\r\n # release user lock\r\n\r\n\u00a0\r\n\r\nWe can avoid this acquire/release/reacquire lock issue by making volume lock as low weight.\u00a0\r\n\r\n\u00a0\r\n\r\nIn this way, the above deleteVolume request will change as below\r\n # Acquire volume lock\r\n # Get Volume Info from DB\r\n # Get owner from volume Info read from DB\r\n # Acquire owner lock\r\n # Do delete logic\r\n # release\u00a0owner lock\r\n # release volume lock.\u00a0\r\n\r\nSame issue is seen with SetOwner for Volume request also.\r\n\r\nDuring HDDS-1620 [~arp] brought up this issue.\u00a0\r\n\r\nI am proposing the above solution to solve this issue. Any other idea/suggestions are welcome.\r\n\r\nThis also resolves a bug in setOwner for Volume request.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve locking in OzoneManager"
   },
   {
      "_id": "13238902",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-06-11 23:48:19",
      "description": "Add support for limit query param to limit the results of\u00a0/api/containers and /api/containers/\\{id} endpoints",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add limit support to /api/containers and /api/containers/{id} endpoints"
   },
   {
      "_id": "13238771",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-11 13:46:48",
      "description": "network-topology-default.xml can be loaded from file or classpath. But the NodeSchemaLoader assumes that the files on the classpath can be opened as a file. It's true if the file is in etc/hadoop (which is part of the classpath) but not true if the file is packaged to a jajr file:\r\n\r\n{code}\r\nscm_1         | 2019-06-11 13:18:03 INFO  NodeSchemaLoader:118 - Loading file from jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-0.5.0-SNAPSHOT.jar!/network-topology-default.xml\r\nscm_1         | 2019-06-11 13:18:03 ERROR NodeSchemaManager:74 - Failed to load schema file:network-topology-default.xml, error:\r\nscm_1         | java.lang.IllegalArgumentException: URI is not hierarchical\r\nscm_1         | \tat java.io.File.<init>(File.java:418)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.net.NodeSchemaLoader.loadSchemaFromFile(NodeSchemaLoader.java:119)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.net.NodeSchemaManager.init(NodeSchemaManager.java:67)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.net.NetworkTopologyImpl.<init>(NetworkTopologyImpl.java:63)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.initializeSystemManagers(StorageContainerManager.java:382)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:275)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.<init>(StorageContainerManager.java:208)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManager.createSCM(StorageContainerManager.java:586)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter$SCMStarterHelper.start(StorageContainerManagerStarter.java:139)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.startScm(StorageContainerManagerStarter.java:115)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:67)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.call(StorageContainerManagerStarter.java:42)\r\nscm_1         | \tat picocli.CommandLine.execute(CommandLine.java:1173)\r\nscm_1         | \tat picocli.CommandLine.access$800(CommandLine.java:141)\r\nscm_1         | \tat picocli.CommandLine$RunLast.handle(CommandLine.java:1367)\r\nscm_1         | \tat picocli.CommandLine$RunLast.handle(CommandLine.java:1335)\r\nscm_1         | \tat picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:1243)\r\nscm_1         | \tat picocli.CommandLine.parseWithHandlers(CommandLine.java:1526)\r\nscm_1         | \tat picocli.CommandLine.parseWithHandler(CommandLine.java:1465)\r\nscm_1         | \tat org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:65)\r\nscm_1         | \tat org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:56)\r\nscm_1         | \tat org.apache.hadoop.hdds.scm.server.StorageContainerManagerStarter.main(StorageContainerManagerStarter.java:56)\r\nscm_1         | Failed to load schema file:network-topology-default.xml, error:\r\n{code}\r\n\r\nThe quick fix is to keep the current behaviour but read the file from classloader.getResourceAsStream() instead of classloader.getResource().toURI()",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "SCM startup is failing if network-topology-default.xml is part of a jar"
   },
   {
      "_id": "13238720",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-11 09:37:57",
      "description": "In kubernetes resources we can define livebess probes which can help to detect any failure. If the define port is not available the pod will be rescheduled.\r\n\r\nWe need to add the liveness probes to our k8s resource files.\r\n\r\nNote: We shouldn't add readiness probes. Readiness probe is about the service availability. The service/dns can be available only after the service is restarted. This is not good for us as:\r\n\r\n * We need DNS resolution during the startup (See OzoneManager.loadOMHAConfigs)\r\n * We already implemented retry in case of missing DNS entries",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add liveness probe to the example k8s resources files"
   },
   {
      "_id": "13238619",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-06-10 19:57:52",
      "description": "We set size as below\r\n\r\n{code}\r\nfinal long size = args.getDataSize() >= 0 ?\r\n args.getDataSize() : scmBlockSize;\r\n{code}\r\n\u00a0\r\n\r\nand create OmKeyInfo with below size set. But when allocating Block for openKey, we use as below.\r\n\r\nallocateBlockInKey(keyInfo, args.getDataSize(), currentTime);\r\n\r\n\u00a0\r\n\r\nI feel here, we should use size which is set above so that we allocate at least a block when the openKey call happens.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve logic in openKey when allocating block"
   },
   {
      "_id": "13238135",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-07 07:26:48",
      "description": "We think that it would be more effective to collect all the design docs in one place and make it easier to review them by the community.\r\n\r\nWe propose to follow an approach where the proposals are committed to the hadoop-hdds/docs project and the review can be the same as a review of a PR",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Define the process to add proposal/design docs to the Ozone subproject"
   },
   {
      "_id": "13238098",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-06-07 03:27:19",
      "description": "This problem can be seen at https://builds.apache.org/job/hadoop-multibranch/job/PR-846/6/testReport/org.apache.hadoop.ozone.client.rpc/TestBCSID/testBCSID/.\r\n\r\nAs seen here, after a RaftRetryFailureException, the pipeline is excluded from the pipeline and that leads to SCM create a new pipeline. Creation of a new pipeline might not be possible in a test cluster because of limited number of nodes.\r\n\r\n{code}\r\n2019-06-06 22:29:23,311 WARN  KeyOutputStream - Encountered exception java.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-AD0A1CB44582->73f367e6-7f91-4409-b4d3-b831e0bfb585@group-31FAD62742D6, cid=1, seq=1*, RW, org.apache.hadoop.hdds.scm.XceiverClientRatis$$Lambda$313/1466662004@60d08041 for 180 attempts with RetryLimited(maxAttempts=180, sleepTime=1000ms) on the pipeline Pipeline[ Id: 27d23af1-7180-42f5-b3c7-31fad62742d6, Nodes: 73f367e6-7f91-4409-b4d3-b831e0bfb585{ip: 172.17.0.2, host: 5e847226af57, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN]. The last committed block length is 0, uncommitted data length is 5 retry count 0\r\n2019-06-06 22:29:23,343 WARN  BlockManagerImpl - Pipeline creation failed for type:RATIS factor:ONE. Retrying get pipelines call once.\r\norg.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 1 using 0 nodes.\r\n\tat org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:151)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:57)\r\n\tat org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:149)\r\n\tat org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:190)\r\n\tat org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)\r\n\tat org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)\r\n\tat \r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "RaftRetryFailureException & AlreadyClosedException should not exclude pipeline from client"
   },
   {
      "_id": "13237876",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-06-06 05:31:49",
      "description": "Currently, whenever there is a container state change, it updates the container but doesn't sync.\r\n\r\nThe idea is here to is to force sync the state to disk everytime there is a state change.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Ensure container state on datanode gets synced to disk whenever state change happens"
   },
   {
      "_id": "13237858",
      "assignee": "xyao",
      "components": [],
      "created": "2019-06-06 01:58:42",
      "description": "Add option to order the output acccording to topology layer.\r\nFor example, for /rack/node topolgy, we can show,\r\nState = HEALTHY\r\n/default-rack:\r\nozone_datanode_1.ozone_default/172.18.0.3\r\nozone_datanode_2.ozone_default/172.18.0.2\r\nozone_datanode_3.ozone_default/172.18.0.4\r\n/rack1:\r\nozone_datanode_4.ozone_default/172.18.0.5\r\nozone_datanode_5.ozone_default/172.18.0.6\r\nFor /dc/rack/node topology, we can either show\r\nState = HEALTHY\r\n/default-dc/default-rack:\r\nozone_datanode_1.ozone_default/172.18.0.3\r\nozone_datanode_2.ozone_default/172.18.0.2\r\nozone_datanode_3.ozone_default/172.18.0.4\r\n/dc1/rack1:\r\nozone_datanode_4.ozone_default/172.18.0.5\r\nozone_datanode_5.ozone_default/172.18.0.6\r\n\r\nor\r\n\r\nState = HEALTHY\r\ndefault-dc:\r\ndefault-rack:\r\nozone_datanode_1.ozone_default/172.18.0.3\r\nozone_datanode_2.ozone_default/172.18.0.2\r\nozone_datanode_3.ozone_default/172.18.0.4\r\ndc1:\r\nrack1:\r\nozone_datanode_4.ozone_default/172.18.0.5\r\nozone_datanode_5.ozone_default/172.18.0.6",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add option to \"ozone scmcli printTopology\" to order the output acccording to topology layer"
   },
   {
      "_id": "13237828",
      "assignee": "xyao",
      "components": [],
      "created": "2019-06-05 22:01:34",
      "description": "Currently both OzoneContainer#stop() and HddsDispatcher#stop() both invoke volumeSet.shutdown() explicitly to shutdown the same volume set.\r\n\r\n\u00a0\r\n\r\nIn addition, OzoneContainer#stop() will invoke\u00a0HddsDispatcher#stop(). Since the volume set object is created by OzoneContainer object, it should be the responsibility of OzoneContainer to shutdown. This ticket is opened to remove the volumeSet.shutdown() from HddsDispatcher#stop().\u00a0\r\n\r\n\u00a0\r\n\r\nThere are benchmark tools relies on HddsDispatcher#stop() to shutdown volumeSet object, that we could fix with explict volumeSet#shutdown call.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "HddsDispatcher should not shutdown volumeSet"
   },
   {
      "_id": "13237819",
      "assignee": "xyao",
      "components": [],
      "created": "2019-06-05 21:35:56",
      "description": "There are a few test leaking hdds volume checker thread. This ticket is opened to fix them.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Ozone tests leaking volume checker thread"
   },
   {
      "_id": "13237782",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-05 18:22:52",
      "description": "Installing a DB checkpoint on the OM involves following steps:\r\n 1. When an OM follower receives installSnapshot notification from OM leader, it should initiate a new checkpoint on the OM leader and download that checkpoint through Http. \r\n 2. After downloading the checkpoint, the StateMachine must be paused so that the\u00a0old OM DB can be replaced with the new\u00a0downloaded checkpoint. \r\n 3. The OM should be reloaded with the new state . All the services having a dependency on the OM DB (such as MetadataManager, KeyManager etc.) must be re-initialized/ restarted. \r\n 4. Once the OM is ready with the new state, the state machine must be unpaused to resume participating in the Ratis ring.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "On installSnapshot notification from OM leader, download checkpoint and reload OM state"
   },
   {
      "_id": "13237764",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-06-05 16:59:11",
      "description": "Recon tag does not show up on the list of tags on /conf page. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon config tag does not show up on Ozone UI."
   },
   {
      "_id": "13237720",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-05 12:33:01",
      "description": "Ozone release contains example k8s deployment files to make it easier to deploy Ozone to kubernetes. As of now we use emptyDir everywhere, we should support the configuration of host volumes (hostPath or Local Persistent volumes).\r\n\r\nThe big question here is the default:\r\n\r\n* Make the examples easy to start and ephemeral\r\n* Make the examples more safe, by default (but couldn't be started without additional administration).\r\n\r\n(Note this conversation is started in the review of HDDS-1508)\r\n\r\nXiaoyu:  Can we support mount hostVolume for datanode daemons?\r\n\r\nMarton: Yes, we can.\r\n\r\nAFAIK there are two options:\r\n * using [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)\r\n * or with [Local PersistentVolumes](https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/)\r\n\r\nThe first one requires the knowledge of directory names on the host.\r\nThe second one is recommended but it requires the creation of PersistentVolumes or install a PersistentVolume provider\r\n\r\nI am not sure what is the best approach, my current proposal is:\r\n\r\n * Use empty dir everywhere to make it easier to start simple ozone cluster\r\n * Provide simple option to turn on any of theses persistence (the kubernetes files are generated and the generation can be parametrized)\r\n * Document how to customize the kubernetes resources files\r\n\r\nSummary: it's question of the defaults:\r\n \r\n  1. Use a complex, but persistent solution, which may not work out of the box   \r\n  2. Use a simple, but ephemeral solution (as default)\r\n\r\nI started to use (2) but I am open to change.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Support real persistence in the k8s example files "
   },
   {
      "_id": "13237491",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-04 13:20:14",
      "description": "CSI server can't be started because an ClassNotFound exception.\r\n\r\nIt turned out that with using the new configuration api we got old netty jar files as transitive dependencies. (hdds-configuration depends on hadoop-common, hadoop-commons depends on the word)\r\n\r\nWe should exclude all the old netty version from the classpath of the CSI server.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Csi server fails because transitive Netty dependencies"
   },
   {
      "_id": "13237438",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-06-04 09:37:36",
      "description": "hadoop-ozone-recon-0.5.0-SNAPSHOT.jar is 73 MB, mainly because the node_modules are included (full typescript source, eslint, babel, etc.):\r\n\r\n{code}\r\nunzip -l hadoop-ozone-recon-0.5.0-SNAPSHOT.jar | grep node_modules | wc\r\n{code}\r\n\r\nFix me if I am wrong, but I think node_modules is not required in the distribution as the dependencies are already included in the compiled javascript files.\r\n\r\nI propose to remove the node_modules from the jar file.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Reduce the size of recon jar file"
   },
   {
      "_id": "13237425",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-06-04 08:41:54",
      "description": "Documentation page should be updated according to the recent changes:\r\n\r\nIn the uploaded PR I modified the following:\r\n\r\n #  Pages are restructured to use a similar structure what is intruced on the wiki by [~anu]. (Getting started guides are separated for different environments)\r\n # The width of the menu is increased (to make it more readable)\r\n # The logo is moved from the main page from the menu (to get more space for the menu items)\r\n # 'Requirements' section is added to each 'Getting started' page\r\n # Test tools / docker image / kubernetes pages are imported from the wiki. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Restructure documentation pages for better understanding"
   },
   {
      "_id": "13237383",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-06-04 02:33:34",
      "description": "Implement\u00a0Key write requests to use OM Cache, double buffer.\u00a0\r\n\r\nIn this Jira will add the changes to implement\u00a0key operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement Key Write Requests to use Cache and DoubleBuffer"
   },
   {
      "_id": "13237232",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 12:29:29",
      "description": "Recently a new exception become visible in the datanode logs, using standard freon (STANDLAONE)\r\n\r\n{code}\r\ndatanode_2  | 2019-06-03 12:18:21 WARN  PropagationRegistry$ExceptionCatchingExtractorDecorator:60 - Error when extracting SpanContext from carrier. Handling gracefully.\r\ndatanode_2  | io.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 7576cabf-37a4-4232-9729-939a3fdb68c4WriteChunk150a8a848a951784256ca0801f7d9cf8b_stream_ed583cee-9552-4f1a-8c77-63f7d07b755f_chunk_1\r\ndatanode_2  | \tat org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:49)\r\ndatanode_2  | \tat org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:34)\r\ndatanode_2  | \tat io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)\r\ndatanode_2  | \tat io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)\r\ndatanode_2  | \tat io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)\r\ndatanode_2  | \tat io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)\r\ndatanode_2  | \tat org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:102)\r\ndatanode_2  | \tat org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)\r\ndatanode_2  | \tat org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)\r\ndatanode_2  | \tat org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)\r\ndatanode_2  | \tat org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\ndatanode_2  | \tat org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\ndatanode_2  | \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\ndatanode_2  | \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n{code}\r\n\r\nIt turned out that the tracingId propagation between XCeiverClient and Server doesn't work very well (in case of Standalone and async commands)\r\n\r\n 1. there are many places (on the client side) where the traceId filled with  UUID.randomUUID().toString();  \r\n 2. This random id is propagated between the Output/InputStream and different part of the clients\r\n 3. It is unnecessary, because in the XceiverClientGrpc and XceiverClientGrpc the traceId field is overridden with the real opentracing id anyway (sendCommand/sendCommandAsync)\r\n 4. Except in the XceiverClientGrpc.sendCommandAsync where this part is accidentally missing.\r\n\r\nThings to fix:\r\n\r\n 1. fix XceiverClientGrpc.sendCommandAsync (replace any existing traceId with the good one)\r\n 2. remove the usage of the UUID based traceId (it's not used)\r\n 3. Improve the error logging in case of an invalid traceId on the server side.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Tracing id is not propagated via async datanode grpc call"
   },
   {
      "_id": "13237216",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 11:10:51",
      "description": "During an offline discussion with [~eyang] and [~arp], Eric suggested to maintain the source of the docker specific start images inside the main ozone branch (trunk) instead of the branch of the docker image.\r\n\r\nWith this approach the ozone-runner image can be a very lightweight image and the entrypoint logic can be versioned together with the ozone itself.\r\n\r\nAn other use case is a container creation script. Recently we [documented|https://cwiki.apache.org/confluence/display/HADOOP/Ozone+Docker+images] that hadoop-runner/ozone-runner/ozone images are not for production (for example because they contain development tools).\r\n\r\nWe can create a helper tool (similar what Spark provides) to create Ozone container images from any production ready base image. But this tool requires the existence of the scripts inside the distribution.\r\n\r\n(ps: I think sooner or later the functionality of envtoconf.py can be added to the OzoneConfiguration java class and we can parse the configuration values directly from environment variables.\r\n\r\nIn this patch I copied the required scripts to the ozone source tree and the new ozone-runner image (HDDS-1634) is designed to use it from this specific location.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Maintain docker entrypoint and envtoconf inside ozone project"
   },
   {
      "_id": "13237212",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 10:43:05",
      "description": "Ozone compose files use apache/hadoop-runner to provide a fixed environment to run any Ozone distribution.\r\n\r\n It can be better to use separated hadoop-runner and ozone-runner:\r\n\r\n 1. To make it easier to include Ozone specific behaviour (For example goofys install, scm/om initialization)\r\n 2. To make it clean which feature is required by all the subprojects of Hadoop and which one is Ozone specific (base on the comment from [~eyang] in HADOOP-16092)\r\n 3. for hadoop-runner we maintain two tags (jdk11/jdk8/latest). And it seems to be hard to maintain all of them. jdk8 is required only for hadoop and with separating hadoop-runner/ozone-runner we can use only one simple branch for ozone-runner development (and we can create incremental fixed tags very easily)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Introduce a new ozone specific runner image"
   },
   {
      "_id": "13237182",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 08:29:19",
      "description": "We have a new rat, the old one is not available. The url should be updated.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update rat from 0.12 to 0.13 in hadoop-runner build script"
   },
   {
      "_id": "13237179",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 08:19:45",
      "description": "[~eyang] reporeted in HDDS-1609 that the hadoop-runner image can be started *without* mounting a real hadoop (usually, it's ounted) AND using a different uid:\r\n\r\n{code}\r\ndocker run -it  -u $(id -u):$(id -g) apache/hadoop-runner bash\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"chdir to cwd (\\\"/opt/hadoop\\\") set in config.json failed: permission denied\": unknown.\r\n{code}\r\n\r\nThere are two blocking problems here:\r\n\r\n * the /opt/hadoop directory (which is the CWD inside the container) is 700 instead of 755\r\n * The usage of sudo in started scripts (sudo is not possible if the real user is not added to the /etc/passwd)\r\n\r\nBoth of them are addressed by this patch.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Make the hadoop home world readable and avoid sudo in hadoop-runner"
   },
   {
      "_id": "13237176",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-06-03 08:04:59",
      "description": "In HDDS-1518 we modified the location of the var and config files inside the container.\r\n\r\nThere are three problems with the current auditparser smokest:\r\n\r\n 1. The default audit log4j files are not part of the new config directory (fixed with HDDS-1630)\r\n 2. The smoketest is executed in scm container instead of om\r\n 3. The log directory is hard coded\r\n\r\nThe 2 and 3 will be fined in this patch. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix auditparser smoketests"
   },
   {
      "_id": "13237174",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 08:00:53",
      "description": "HDDS-1518 separates the read-only directories (/opt/ozone, /opt/hadoop) from the read-write directories (/etc/hadoop, /var/log/hadoop). \r\n\r\nThe configuration directory and log directory should be writeable and to make it easier to run the docker-compose based pseudo clusters with *different* host uid we started to use different config dir.\r\n\r\nBut we need all the defaults in the configuration dir. In this patch I add a small fragments to the hadoop-runner image to copy the default files (if available).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Copy default configuration files to the writeable directory"
   },
   {
      "_id": "13237161",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 06:27:34",
      "description": "Ozone tar file creation is a very time consuming step. I propose to make it optional and create the tar file only if the dist profile is enabled (-Pdist)\r\n\r\nThe tar file is not required to test ozone as the same content is available from hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT which is enough to run docker-compose pseudo clusters, smoketests. \r\n\r\nIf it's required, the tar file creation can be requested by the dist profile.\r\n \r\nOn my machine (ssd based) it can cause 5-10% time improvements as the tar size is ~500MB and it requires a lot of IO.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Tar file creation can be optional for non-dist builds"
   },
   {
      "_id": "13237158",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-06-03 06:06:24",
      "description": "Problem: Some of the smoketest executions were reported to green even if they contained failed tests.\r\n\r\nRoot cause: the legacy test executor (hadoop-ozone/dist/src/main/smoketest/test.sh) which just calls the new executor script (hadoop-ozone/dist/src/main/compose/test-all.sh) didn't handle the return code well (the failure of the smoketests should be signalled by the bash return code)\r\n\r\nThis patch:\r\n * Fixes the error code handling in smoketest/test.sh\r\n * Fixes the test execution in compose/test-all.sh (should work from any other directories)\r\n * Updates hadoop-ozone/dev-support/checks/acceptance.sh to use the newer test-all.sh executor instead of the old one.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix the execution and return code of smoketest executor shell script"
   },
   {
      "_id": "13237155",
      "assignee": "elek",
      "components": [],
      "created": "2019-06-03 05:54:45",
      "description": "During an offline discussion with [~arp] and [~eyang] we agreed that it could be more safe to fix the tag of the used hadoop-runner images during the releases.\r\n\r\nIt also requires fix tags from hadoop-runner, but after that it's possible to use the fixed tags.\r\n\r\nThis patch makes it possible to define the required version/tag in pom.xml\r\n\r\n 1. the default hadoop-runner.version is added to all .env files  during the build\r\n 2. If a variable is added to the .env, it can be used from docker-compose files AND can be overridden by environment variables (it makes it possible to define custom version during a local run) ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make the version of the used hadoop-runner configurable"
   },
   {
      "_id": "13236973",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-05-31 21:32:18",
      "description": "{{SCMBlockProtocolServer#allocateBlock}} throws {{ConcurrentModificationException}} when SCM has containers of different owners.\r\n\r\n{code}\r\n2019-05-31 13:53:16,808 WARN org.apache.hadoop.hdds.scm.container.SCMContainerManager: Container allocation failed for pipeline=Pipeline[ Id: 3e0eec4d-67d1-4582-a9e9-e68b0a340de6, Nodes: abaea3d2-a8c1-47de-8cdb-7cc5ed8f23a6{ip: 10.17.219.50, host: v\r\nc1340.halxg.cloudera.com, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN] requiredSize=268435456 {}\r\njava.util.ConcurrentModificationException\r\n        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1211)\r\n        at java.util.TreeMap$KeyIterator.next(TreeMap.java:1265)\r\n        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:473)\r\n        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:394)\r\n        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:203)\r\n        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "ConcurrentModificationException when SCM has containers of different owners"
   },
   {
      "_id": "13236952",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-31 19:09:05",
      "description": "There are a few steps that are done inside the OM bucket lock that are lock invariant and can be done outside the lock. This patch refactors those steps. It also adds an isExist API in the metadata store so that we dont need to deserialize the byte[] to Object while doing a simple _table.get(key) != null_ check. \r\n\r\nOn applying the patch, the OM + SCM (With dummy datanodes) write performance improves by around 3 - 6x based on number of existing keys in the OM RocksDB. \r\n\r\nThanks to [~nandakumar131] who helped with this patch. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Refactor operations inside the bucket lock in OM key write."
   },
   {
      "_id": "13236726",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-31 00:28:23",
      "description": "Implement\u00a0Volume write requests to use OM Cache, double buffer.\u00a0\r\n\r\nIn this Jira will add the changes to implement\u00a0volume operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement Volume Write Requests to use Cache and DoubleBuffer"
   },
   {
      "_id": "13236714",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-30 22:59:02",
      "description": "[HDDS-1539] adds 4 new api for Ozone rpc client. OM HA implementation needs to handle them.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support volume acl operations for OM HA."
   },
   {
      "_id": "13236710",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-30 22:11:13",
      "description": "In this Jira, we shall use the new code added in HDDS-1551 for Non-HA flow.\r\n\r\n\u00a0\r\n\r\nThis Jira modifies the\u00a0bucket requests only, further requests will be handled in further Jira's.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge code for HA and Non-HA OM requests for bucket"
   },
   {
      "_id": "13236449",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-05-30 09:59:24",
      "description": "Container missing on the datanode after a restart.\r\n\r\n{code}\r\n08:10:44.308 [pool-2131-thread-1] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 34 locID: 102182684750055212 bcsId: 6198} | ret=FAILURE\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 34 has been lost and and cannot be recreated on this DataNode\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:207) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:149) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$0(ContainerStateMachine.java:385) ~[hadoop-hdds-container-service-0.5.0-SNAPSHOT.jar:?]\r\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_171]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]\r\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Container Missing in the datanode after restart"
   },
   {
      "_id": "13236296",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-05-29 17:36:42",
      "description": "If the applyTransaction fails in the containerStateMachine, then the container should not accept new writes on restart,.\r\n\r\nThis can occur if\r\n# chunk write applyTransaction fails\r\n# container state update to UNHEALTHY also fails\r\n# Ratis snapshot is taken\r\n# Node restarts\r\n# container accepts new transactions",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "applyTransaction failure should not be lost on restart"
   },
   {
      "_id": "13236220",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-05-29 11:45:45",
      "description": "We had multiple problems earlier with the classpath separation and the internal ozonefs classloader. Before fixing all the issues I propose to create a smoketest to detect if the classpath separation is broken again .\r\n\r\nAs a first step I created an smoketest/ozone-mr environment (based on the  work of [~xyao], which is secure) and a smoketest \r\n\r\nPossible follow-up works:\r\n\r\n * Adapt the test.sh for the ozonesecure-mr\r\n * Include test runs with older hadoop versions ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create smoketest for non-secure mapreduce example"
   },
   {
      "_id": "13236073",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-28 18:46:59",
      "description": "In this Jira, we shall implement audit logging for OM HA Bucket write requests.\r\n\r\nAs now we cannot use\u00a0userName,\u00a0 IpAddress from Server API's as these will be null, because the requests are executed under GRPC context. So, in our AuditLogger API's we need to pass username and remoteAddress which we can get from OMRequest after HDDS-1600 and use these during audit logging.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement AuditLogging for OM HA Bucket write requests"
   },
   {
      "_id": "13236055",
      "assignee": "xyao",
      "components": [],
      "created": "2019-05-28 17:33:59",
      "description": "This was caught by the New ContainerCache with reference counting from HDDS-1449. The root cause is an unclosed\u00a0KeyValueBlockIterator from ContainerReader#initializeUsedBytes.\r\n\r\nI will post a patch shortly, which will fix some UT failures exposed by -HDDS-1449,-\u00a0such as\u00a0TestBCSID#testBCSID, etc.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ContainerReader#initializeUsedBytes leaks DB reference"
   },
   {
      "_id": "13235880",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-05-27 23:00:34",
      "description": "[https://ci.anzix.net/job/ozone/16899/testReport/org.apache.hadoop.ozone.container.common.impl/TestContainerPersistence/testDeleteBlockTwice/]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestContainerPersistence#testDeleteBlockTwice"
   },
   {
      "_id": "13235874",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-27 21:36:21",
      "description": "This Jira is to implement updating lastAppliedIndex in OzoneManagerStateMachine once after the buffer is flushed to OM DB.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement updating lastAppliedIndex after buffer flush to OM DB."
   },
   {
      "_id": "13235857",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-27 18:13:22",
      "description": "In OM HA, the actual execution of request happens under GRPC context, so UGI object which we retrieve from\u00a0ProtobufRpcEngine.Server.getRemoteUser(); will not be available.\r\n\r\nIn similar manner\u00a0ProtobufRpcEngine.Server.getRemoteIp().\r\n\r\n\u00a0\r\n\r\nSo, during preExecute(which happens under RPC context) extract userName and IPAddress and add it to the OMRequest, and then send the request to ratis server.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add userName and IPAddress as part of OMRequest."
   },
   {
      "_id": "13235852",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-05-27 17:24:56",
      "description": "When working on HDDS-1551, found some test failures which are not related to HDDS-1551.\r\n\r\nThis is caused by HDDS-700.\u00a0\r\n\r\n\u00a0\r\n\r\nThis has not caught by Jenkins run because our Jenkins run does not run UT's for all the sub-modules. In this case, it should have run UT's for hadoop-hdds-server-scm, as there are some changes in src/test files in that module, but still, it has not run for it. I think Jenkins run for ozone project is not properly setup.\r\n\r\n[https://ci.anzix.net/job/ozone/16895/testReport/]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestReplicationManager and checkstyle issues."
   },
   {
      "_id": "13235850",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-27 16:57:33",
      "description": "Some small checkstyle issues are accidentally committed with HDDS-700.\r\n\r\nTrivial fixes are coming here...\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix Ozone checkstyle issues on trunk"
   },
   {
      "_id": "13235840",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-27 15:18:53",
      "description": "I noticed that the hadoop-ozone/common project depends on hadoop-hdds-server-scm project.\r\n\r\nThe common projects are designed to be a shared artifacts between client and server side. Adding additional dependency to the common pom means that the dependency will be available for all the clients as well.\r\n\r\n(See the attached artifact about the current, desired structure).\r\n\r\nWe definitely don't need scm server dependency on the client side.\r\n\r\nThe code dependency is just one class (ScmUtils) and the shared code can be easily moved to the common.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hdds-server-scm dependency from ozone-common"
   },
   {
      "_id": "13234966",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-05-22 18:19:25",
      "description": "The test failures are caused bcoz the test relies on KeyoutputStream#getLocationList() to validate the no of preallocated blocks, but it has been changed recently to exclude the empty blocks. The fix is mostly to use KeyOutputStream#getStreamEntries() to get the no of preallocated blocks.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestFailureHandlingByClient tests"
   },
   {
      "_id": "13234741",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-21 22:40:00",
      "description": "This Jira is to implement OMDoubleBuffer metrics, to show metrics like.\r\n # flushIterations.\r\n # totalTransactionsflushed.\r\n\r\n\u00a0\r\n\r\nAny other related metrics. This Jira is created based on the comment by [~anu] during HDDS-1512 review.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create OMDoubleBuffer metrics"
   },
   {
      "_id": "13234739",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-05-21 22:28:04",
      "description": "libstdc++ is required for node install in alpine builds. Otherwise we get this error:\r\n\r\n\r\n{code:java}\r\n[ERROR] node: error while loading shared libraries: libstdc++.so.6: cannot open shared object file: No such file or directory{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add libstdc++ to ozone build docker image"
   },
   {
      "_id": "13234718",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-21 20:41:48",
      "description": "RocksDB statistics need to sinked to hadoop-metrics2 for Ozone Manager to understand how OM behaves under heavy load.\r\nExample: \"rocksdb.bytes.written\"\r\n\r\nhttps://github.com/facebook/rocksdb/wiki/Statistics\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add RocksDB metrics to OM"
   },
   {
      "_id": "13234652",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-21 15:54:39",
      "description": "Based on the feedback from [~eyang] I realized that the names of the k8s-dev and k8s-dev-push profiles are not expressive enough as the created containers can be used not only for kubernetes but can be used together with any other container orchestrator.\r\n\r\nI propose to rename them to docker/docker-push.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Rename k8s-dev and k8s-dev-push profiles to docker and docker-push"
   },
   {
      "_id": "13234180",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-05-19 17:46:00",
      "description": "IllegalArgumentException while processing container Reports\r\n\r\n{code}\r\n2019-05-19 23:15:04,137 ERROR events.SingleThreadExecutor (SingleThreadExecutor.java:lambda$onMessage$1(88)) - Error on execution message org.apache.hadoop.hdds.scm.server.SCMDatanodeHeartbeatDispatcher$ContainerReportFromDatanode@1a117ebc\r\njava.lang.IllegalArgumentException\r\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)\r\n        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:178)\r\n        at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:85)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:124)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:97)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:46)\r\n        at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "IllegalArgumentException while processing container Reports"
   },
   {
      "_id": "13234155",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-05-19 07:34:49",
      "description": "Datanode exits because Ratis fails to shutdown ratis server \r\n\r\n{code}\r\n2019-05-19 12:07:19,276 INFO  impl.RaftServerImpl (RaftServerImpl.java:checkInconsistentAppendEntries(965)) - 80747533-f47c-43de-85b8-e70db448c63f: inconsistency entries. Reply:99930d0a-72ab-4795-a3ac-f3c\r\nfb61ca1bb<-80747533-f47c-43de-85b8-e70db448c63f#3132:FAIL,INCONSISTENCY,nextIndex:9057,term:33,followerCommit:9057\r\n2019-05-19 12:07:19,276 WARN  impl.RaftServerProxy (RaftServerProxy.java:lambda$close$4(320)) - e143b976-ab35-4555-a800-7f05a2b1b738: Failed to close GRPC server\r\njava.io.InterruptedIOException: e143b976-ab35-4555-a800-7f05a2b1b738: shutdown server with port 64605 failed\r\n        at org.apache.ratis.util.IOUtils.toInterruptedIOException(IOUtils.java:48)\r\n        at org.apache.ratis.grpc.server.GrpcService.closeImpl(GrpcService.java:160)\r\n        at org.apache.ratis.server.impl.RaftServerRpcWithProxy.lambda$close$2(RaftServerRpcWithProxy.java:76)\r\n        at org.apache.ratis.util.LifeCycle.lambda$checkStateAndClose$2(LifeCycle.java:231)\r\n        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:251)\r\n        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:229)\r\n        at org.apache.ratis.server.impl.RaftServerRpcWithProxy.close(RaftServerRpcWithProxy.java:76)\r\n        at org.apache.ratis.server.impl.RaftServerProxy.lambda$close$4(RaftServerProxy.java:318)\r\n        at org.apache.ratis.util.LifeCycle.lambda$checkStateAndClose$2(LifeCycle.java:231)\r\n        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:251)\r\n        at org.apache.ratis.util.LifeCycle.checkStateAndClose(LifeCycle.java:229)\r\n        at org.apache.ratis.server.impl.RaftServerProxy.close(RaftServerProxy.java:313)\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.stop(XceiverServerRatis.java:432)\r\n        at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.stop(OzoneContainer.java:201)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.close(DatanodeStateMachine.java:270)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.stopDaemon(DatanodeStateMachine.java:394)\r\n        at org.apache.hadoop.ozone.HddsDatanodeService.stop(HddsDatanodeService.java:449)\r\n        at org.apache.hadoop.ozone.HddsDatanodeService.terminateDatanode(HddsDatanodeService.java:429)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:208)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:349)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.InterruptedException\r\n        at java.lang.Object.wait(Native Method)\r\n        at java.lang.Object.wait(Object.java:502)\r\n        at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.awaitTermination(ServerImpl.java:282)\r\n        at org.apache.ratis.grpc.server.GrpcService.closeImpl(GrpcService.java:158)\r\n        ... 19 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanode exits because Ratis fails to shutdown ratis server "
   },
   {
      "_id": "13233847",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-16 19:33:43",
      "description": "Implement Bucket write requests to use OM Cache, double buffer.\r\n\r\nAnd also in OM previously we used to Ratis client for communication to\u00a0Ratis server, instead of that use Ratis server API's.\r\n\r\n\u00a0\r\n\r\nIn this Jira will add the changes to implement bucket operations, and HA/Non-HA will have a different code path, but once all requests are implemented will\u00a0have a single code path.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement Bucket Write Requests to use Cache and DoubleBuffer"
   },
   {
      "_id": "13233541",
      "assignee": "xyao",
      "components": [],
      "created": "2019-05-15 17:37:32",
      "description": "Add dAcls for volume, bucket, keys and prefix",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support default Acls for volume, bucket, keys and prefix"
   },
   {
      "_id": "13233540",
      "assignee": "xyao",
      "components": [],
      "created": "2019-05-15 17:37:18",
      "description": "Implement addAcl,removeAcl,setAcl,getAcl  for Prefix",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement addAcl,removeAcl,setAcl,getAcl  for Prefix"
   },
   {
      "_id": "13233539",
      "assignee": "xyao",
      "components": [],
      "created": "2019-05-15 17:37:03",
      "description": "Create Radix tree to support ozone prefix ACLs.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create Radix tree to support ozone prefix ACLs "
   },
   {
      "_id": "13233513",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-05-15 15:48:35",
      "description": "The test is failing with the following stack trace.\r\n{code}\r\n[ERROR] testSCMSafeModeRestrictedOp(org.apache.hadoop.ozone.om.TestScmSafeMode)  Time elapsed: 9.79 s  <<< FAILURE!\r\njava.lang.AssertionError\r\n\tat org.junit.Assert.fail(Assert.java:86)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertFalse(Assert.java:64)\r\n\tat org.junit.Assert.assertFalse(Assert.java:74)\r\n\tat org.apache.hadoop.ozone.om.TestScmSafeMode.testSCMSafeModeRestrictedOp(TestScmSafeMode.java:304)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "testSCMSafeModeRestrictedOp is failing consistently"
   },
   {
      "_id": "13233378",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-05-15 05:45:38",
      "description": "Currently, by default while doing the chunk writes on datanodes, the sync flag is ON by default. This needs to be turned off by default.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable the sync flag by default during chunk writes in Datanode"
   },
   {
      "_id": "13233220",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-14 11:44:22",
      "description": "As 0.4.0-alpha is released we can publish the latest stable version on dockerhub.\r\n\r\nRequired steps:\r\n\r\n * Update ozone-latest branch in apache/hadoop-docker-ozone repository.\r\n * Create a new branch ozone-0.4.0 from the ozone-latest\r\n\r\nDockerhub is configured to create new tags from any branch which starts with ozone-:\r\n\r\nbranchname: ozone-XXXX --> docker image: apache/ozone:XXXX\r\n\r\nNote: this releasing is in sync with ASF policies (See LEGAL-270)\r\n\r\nbq. The main Docker Hub at hub.docker.com is a public-facing downstream\r\ndistribution channel \u2013 similar to Maven Central, PyPI, Debian package\r\nmanagement, etc.\r\n\r\nbq. It is appropriate to distribute official releases through downstream channels, but inappropriate to distribute unreleased materials through them. (That's\r\nwhy having `latest` on hub.docker.com point to git `master` is problematic.)\r\nSee Apache's formal Release Policy and Release Distribution Policy documents\r\n\r\n\r\nThis is a downstream distribution of the already voted and release ozone package. Please note that the Dockerfile points to the official download page. \r\n\r\nThe ozone version command can show that we have exactly the same, voted and released bits inside: \r\n\r\n{code}\r\ndocker run apache/ozone:latest ozone version\r\n                  //////////////                 \r\n               ////////////////////              \r\n            ////////     ////////////////        \r\n           //////      ////////////////          \r\n          /////      ////////////////  /         \r\n         /////            ////////   ///         \r\n         ////           ////////    /////        \r\n        /////         ////////////////           \r\n        /////       ////////////////   //        \r\n         ////     ///////////////   /////        \r\n         /////  ///////////////     ////         \r\n          /////       //////      /////          \r\n           //////   //////       /////           \r\n             ///////////     ////////            \r\n               //////  ////////////              \r\n               ///   //////////                  \r\n              /    0.4.0-alpha(Badlands)\r\n\r\nSource code repository https://github.com/apache/hadoop.git -r 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b\r\nCompiled by ajay.kumar on 2019-04-30T03:25Z\r\nCompiled with protoc 2.5.0\r\nFrom source with checksum 45e58ba9203a1b4470e183bf90281b20\r\n\r\nUsing HDDS 0.4.0-alpha\r\nSource code repository https://github.com/apache/hadoop.git -r 4ea602c1ee7b5e1a5560c6cbd096de4b140f776b\r\nCompiled by ajay.kumar on 2019-04-30T03:24Z\r\nCompiled with protoc 2.5.0\r\nFrom source with checksum 57412e0def0317aed91721fb7ef5\r\n{code}\r\n\r\nHow to test:\r\n\r\n1. Single image version\r\n\r\n{code}\r\n./build.sh\r\n\r\ndocker run -p 9878:9878 -p 9876:9876  apache/ozone:latest\r\n\r\nls > /tmp/testfile\r\naws s3api --endpoint http://localhost:9878/ create-bucket --bucket=bucket1\r\naws s3 --endpoint http://localhost:9878 cp --storage-class REDUCED_REDUNDANCY /tmp/testfile  s3://bucket1/testfile\r\n{code}\r\n\r\nYes, it's an ozone cluster in one line.\r\n\r\n2. pseudo-cluster version\r\n\r\n{code}\r\n./build.sh\r\ndocker tag apache/ozone:latest  apache/ozone:0.4.0\r\n\r\nmkdir /tmp/ozonetest\r\ncd /tmp/ozonetest\r\n#The required config files are inlined!!!\r\ndocker run apache/ozone:latest cat docker-config > docker-config\r\ndocker run apache/ozone:latest cat docker-config.yaml > docker-config.yaml\r\n\r\ndocker-compose up -d\r\ndocker-compose scale datanode=3\r\n\r\nls > /tmp/testfile\r\naws s3api --endpoint http://localhost:9878/ create-bucket --bucket=bucket1\r\naws s3 --endpoint http://localhost:9878 cp /tmp/testfile  s3://bucket1/testfile\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Publish ozone 0.4.0 to the dockerhub"
   },
   {
      "_id": "13233115",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-05-14 02:15:15",
      "description": "Integrate Ozone(0.4 branch) with Hadoop 2.7.5, \"hdfs dfs -ls /\" can pass, while teragen  failed. \r\n\r\nWhen add  -verbose:class to java options, it shows that class KeyProvider is loaded twice by different classloader while it is only loaded once when execute  \"hdfs dfs -ls /\" \r\n\r\nAll jars under share/ozone/lib are added into hadoop classpath except ozone file system current lib jar.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Mapreduce failure when using Hadoop 2.7.5"
   },
   {
      "_id": "13233018",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-13 16:24:42",
      "description": "Sometimes I need to start ozone cluster from intellij to debug issues. It's possible but it requires to create many runConfiguration object inside my IDE.\r\n\r\nI propose here to share the intellij specific runtimeConfigs to make it easy for anybody (who uses intellij) to run full ozone cluster from the IDE (1 datanode only).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide intellij runConfiguration for Ozone components"
   },
   {
      "_id": "13232697",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-10 15:54:33",
      "description": "As [~eyang] reported the docker-compose clusters write the config files with uid=1000. In case of the build is created with different user (eg id=401) the hadoop user inside the container (id=100) can't work to the ozone/etc/hadoop directory.\r\n\r\nI propose to generate the configuration file to /etc/hadoop (And add that directory to the classpath). In that case the volume mount of the ozone distribution folder can be read only.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use /etc/ozone for configuration inside docker-compose "
   },
   {
      "_id": "13232692",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-05-10 15:47:44",
      "description": "In allocateContainer call,\u00a0 the container is first added to pipelineStateMap and then added to container cache. If two allocate blocks execute concurrently, it might happen that one find the container to exist in the pipelineStateMap but the container is yet to be updated in the container cache, hence failing with CONTAINER_NOT_FOUND exception.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "AllocateBlock call fails with ContainerNotFoundException"
   },
   {
      "_id": "13232530",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-09 20:39:41",
      "description": "This Jira is created to implement DoubleBuffer in OzoneManager to flush transactions to OM DB.\r\n\r\n\u00a0\r\nh2. Flushing Transactions to RocksDB:\r\n\r\nWe propose using an implementation similar to the HDFS EditsDoubleBuffer.\u00a0 We\u00a0shall flush RocksDB transactions in batches, instead of current way of using rocksdb.put() after every operation. At a given time only one batch will be outstanding for flush while newer\u00a0transactions are accumulated in memory to be flushed later.\r\n\r\n\u00a0\r\n\r\nIn DoubleBuffer it will have 2 buffers one is currentBuffer, and the other is readyBuffer. We add entry to current buffer, and we check if another flush call is outstanding. If not, we flush to disk Otherwise we add entries to otherBuffer while sync is happening.\r\n\r\n\u00a0\r\n\r\nIn this if sync is happening, we shall add new requests to other buffer and when we can sync we use *RocksDB batch commit to sync to disk, instead of rocksdb put.*\r\n\r\n\u00a0\r\n\r\nNote: If flush to disk is failed on any OM, we shall terminate the OzoneManager, so that OM DB\u2019s will not diverge. Flush failure should be considered as catastrophic failure.\r\n\r\n\u00a0\r\n\r\nScope of this Jira is to add DoubleBuffer implementation, integrating to current OM will be done in further jira's.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement DoubleBuffer in OzoneManager"
   },
   {
      "_id": "13232456",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-05-09 12:00:31",
      "description": "1. Classpath files are plain text files which are generatede for each ozone projects. Classpath files are used to defined the classpath of a module (om, scm, etc) based on the maven classpath.\r\n\r\nExample classpath file:\r\n\r\n{code}\r\nclasspath=$HDDS_LIB_JARS_DIR/kerb-simplekdc-1.0.1.jar:$HDDS_LIB_JARS_DIR/hk2-utils-2.5.0.jar:$HDDS_LIB_JARS_DIR/jackson-core-2.9.5.jar:$HDDS_LIB_JARS_DIR/ratis-netty-0.4.0-fe2b15d-SNAPSHOT.jar:$HDDS_LIB_JARS_DIR/protobuf-java-2.5.0.jar:... \r\n{code}\r\n\r\nClasspath files are maven artifacts and copied to share/ozone/classpath in the distribution\r\n\r\n2. 0.4.0 was the first release when we deployed the artifacts to the apache nexus. [~ajayydv] reported the problem that the staging repository can't be closed: INFRA-18344\r\n\r\nIt turned out that the classpath files are uploaded with jar extension to the repository. We deleted all the classpath files manually and the repository became closable.\r\n\r\nTo avoid similar issues we need to fix this problem and make sure that the classpath files are not uploaded to the repository during a 'mvn deploy' or uploaded but with a good extension.\r\n\r\nps: I don't know the exact solution yet, but I can imagine that bumping the version of maven deploy plugin can help. Seems to be a bug in the plugin.\r\n\r\nps2: This is blocker as we need to fix it before the next release",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Classpath files are deployed to the maven repository as pom/jar files"
   },
   {
      "_id": "13232453",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-05-09 11:51:57",
      "description": "The test fails because, the test expects a exception after 2 datanodes failures to be of type RaftRetryFailureException. But it might happen that, the pipeline gets destroyed quickly then actual write executes over Ratis, hence it will fail with GroupMismatchhException in such case.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestBlockOutputStreamWithFailures#test2DatanodesFailure fails intermittently"
   },
   {
      "_id": "13232411",
      "assignee": "elek",
      "components": [],
      "created": "2019-05-09 09:00:10",
      "description": "Issue HDDS-1382 introduced a new internal CSI server. We should provide example deployment files to make it easy to deploy it to any kubernetes cluster.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide example k8s deployment files for the new CSI server"
   },
   {
      "_id": "13232175",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-05-08 08:25:29",
      "description": "This jira will add some metrics for Ratis pipeline performance\r\n1) number of bytes written\r\n2) number Read state Machine calls\r\n\r\n3) no of Read StateMachine Fails",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add metrics for Ozone Ratis performance"
   },
   {
      "_id": "13232076",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-05-07 17:58:08",
      "description": "The following error is seen intermittently in the Ozone client logs while writing large keys. We need to log the entire exception trace to find out more about the failure.\r\n\r\n{code}\r\n19/04/22 10:13:32 ERROR io.KeyOutputStream: Try to allocate more blocks for write failed, already allocated 0 blocks for this write.\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allocate block failures in client should print exception trace."
   },
   {
      "_id": "13232068",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-05-07 17:31:14",
      "description": "In this Jira, we shall implement a cache for Table.\r\n\r\nAs with OM HA, we are planning to implement double buffer implementation to flush transaction in a batch, instead of using rocksdb put() for every operation. When this comes in to place we need cache in OzoneManager HA to handle/server the requests for validation/returning responses.\r\n\r\n\u00a0\r\n\r\nThis Jira will implement Cache as an integral part of the table.\u00a0In this way users using this table does not need to handle like check cache/db. For this, we can update get API in the table to handle the cache.\r\n\r\n\u00a0\r\n\r\nThis Jira will implement:\r\n # Cache as a part of each Table.\r\n # Uses this cache in get().\r\n # Exposes api for cleanup, add entries to cache.\r\n\r\nUsage to add the entries in to cache will be done in further jira's.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager Cache"
   },
   {
      "_id": "13231920",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-05-07 04:04:42",
      "description": "BlockInputStream#readChunkFromContainer() reads the whole chunk from disk even if we need to read only a part of the chunk.\r\nThis Jira aims to improve readChunkFromContainer so that only that part of the chunk file is read which is needed by client plus the part of chunk file which is required to verify the checksum.\r\n\r\n\r\n\r\nFor example, lets say the client is reading from index 120 to 450 in the chunk. And let's say checksum is stored for every 100 bytes in the chunk i.e. the first checksum is for bytes from index 0 to 99, the next for bytes from index 100 to 199 and so on. To verify bytes from 120 to 450, we would need to read from bytes 100 to 499 so that checksum verification can be done.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support partial chunk reads and checksum verification"
   },
   {
      "_id": "13231824",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-05-06 16:56:28",
      "description": "Following exception is seen in SCM logs intermittently. \r\n\r\n{code}\r\njava.lang.RuntimeException: file name 'chunks/2a54b2a153f4a9c5da5f44e2c6f97c60_stream_9c6ac565-e2d4-469c-bd5c-47922a35e798_chunk_10.tmp.2.23115' is too long ( > 100 bytes)\r\n{code}\r\n\r\nWe may have to limit the name of the chunk to 100 bytes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Generated chunk size name too long."
   },
   {
      "_id": "13231680",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-05-05 22:08:18",
      "description": "KeyInputStream#seek() calls BlockInputStream#seek() to adjust the buffer position to the seeked position. As part of the seek operation, the whole chunk is read from the container and stored in the buffer so that the buffer position can be advanced to the seeked position.\u00a0\r\n\r\nWe should not read from disk on a seek() operation. Instead, for a read operation, when the chunk file is read and put in the buffer, at that time, we can advance the buffer position to the previously seeked position.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone KeyInputStream seek() should not read the chunk file"
   },
   {
      "_id": "13231347",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-05-02 20:09:02",
      "description": "Bootstrap React with Typescript, Ant,\u00a0LESS and other necessary libraries for Recon UI.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bootstrap React framework for Recon UI"
   },
   {
      "_id": "13231110",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-05-01 17:53:11",
      "description": "HDDS-864 added the implementation for Strongly typed codec implementation for the tables of OmMetadataManager.\r\n\r\n\u00a0\r\n\r\nTables which are added as part of S3 Implementation are not using this. This Jira is address to this.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use strongly typed codec implementations for the S3Table"
   },
   {
      "_id": "13230785",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-04-29 22:59:24",
      "description": "HDDS-791 implemented range get operation.\r\n\r\n\u00a0\r\n\r\nS3.md documentation has below line:\u00a0\r\n\r\nGET Object | implemented | Range headers are not supported\r\n\r\n\u00a0\r\n\r\nThis should be updated to remove the part `Range headers are not supported`.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update S3.md documentation"
   },
   {
      "_id": "13230702",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-29 13:45:43",
      "description": "Similar to HDDS-1412 we can further improve the available k8s resources with providing example resources to:\r\n\r\n1) install prometheus\r\n2) execute freon test and check the results.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide k8s resources files for prometheus and performance tests"
   },
   {
      "_id": "13230404",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-04-26 22:56:11",
      "description": "In OzoneContainer start() we have\u00a0\r\n{code:java}\r\nstartContainerScrub();\r\nwriteChannel.start();\r\nreadChannel.start();\r\nhddsDispatcher.init();\r\nhddsDispatcher.setScmId(scmId);{code}\r\n\u00a0\r\n\r\nSuppose here if readChannel.start() failed due to some reason, from VersionEndPointTask, we try to start OzoneContainer again. This\u00a0can cause an issue for writeChannel.start() if\u00a0it is already started.\u00a0\r\n\r\n\u00a0\r\n\r\nFix the logic such a way that if service is started, don't attempt to start the service again. Similar\u00a0changes needed to be done\u00a0for stop().",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix OzoneContainer start method"
   },
   {
      "_id": "13230403",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-04-26 22:53:17",
      "description": "Currently, the ozone config \"ozone.scm.datanode.id\" takes file path as its value. It should instead take dir path as its value and assume a standard filename \"datanode.id\"",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "\"ozone.scm.datanode.id\" config should take path for a dir and not a file"
   },
   {
      "_id": "13230095",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-25 13:26:53",
      "description": "See the design doc in the parent jira for more details.\r\n\r\nIn this jira I introduce a new annotation processor which can generate ozone-default.xml fragments based on the annotations which are introduced by HDDS-1468.\r\n\r\nThe ozone-default-generated.xml fragments can be used directly by the OzoneConfiguration as I added a small code to the constructor to check ALL the available ozone-default-generated.xml files and add them to the available resources.\r\n\r\nWith this approach we don't need to edit ozone-default.xml as all the configuration can be defined in java code.\r\n\r\nAs a side effect each service will see only the available configuration keys and values based on the classpath. (If the ozone-default-generated.xml file of OzoneManager is not on the classpath of the SCM, SCM doesn't see the available configs.) \r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Generate default configuration fragments based on annotations"
   },
   {
      "_id": "13230062",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-25 09:24:22",
      "description": "According to the design doc in the parent issue we would like to support java configuration objects which are simple POJO but the fields/setters are annotated. As a first step we can introduce the OzoneConfiguration.getConfigObject() api which can create the config object and inject configuration.\r\n\r\nLater we can improve it with annotation processor which can generate the ozone-default.xml.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Inject configuration values to Java objects"
   },
   {
      "_id": "13229902",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2019-04-24 15:17:44",
      "description": "During the review of HDDS-1457 I realized that the current documentation contains many outdated information regarding the usage of docker, build commands or s3 usage.\r\n\r\nThe security information is also rendered in an incorrect way.\r\n\r\nThe png files for the prometheus page are missing (were included in the patch of HDDS-846 but missing from the commit).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix content and format of Ozone documentation"
   },
   {
      "_id": "13229680",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-04-23 17:51:01",
      "description": "Recently we have seen an issue, in InitDatanodeState, there is error during create Path for volume. We set the state to shutdown and this has caused DatanodeStateMachine to stop, but datanode is still running. In this case we should stop Datanode, otherwise, user will know about this when running ozone commands or when user observed metrics like healthy nodes.\r\n\r\n\u00a0\r\n\r\ncc [~vivekratnavel]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Stop the datanode, when any datanode statemachine state is set to shutdown"
   },
   {
      "_id": "13229660",
      "assignee": "xyao",
      "components": [],
      "created": "2019-04-23 16:21:56",
      "description": "In SetupSecureOzone.md, the naming convention for keytab files are different from code.\r\n\r\n{code}\r\nhdds.scm.http.kerberos.keytab\r\nozone.om.http.kerberos.keytab\r\n{code}\r\n\r\nIn ozone-default.xml, it is looking for:\r\n\r\n{code}\r\nhdds.scm.http.kerberos.keytab\r\nozone.om.http.kerberos.keytab.file\r\n{code}\r\n\r\nFor the non http version of keytab, they are branded as:\r\n\r\n{code}\r\nhdds.scm.kerberos.keytab.file\r\nozone.om.kerberos.keytab.file\r\n{code}\r\n\r\nIt is best to shorten the name to remove .file suffix from the code to be consistent with Hadoop naming convention.  The second nitpick is hdds and ozone prefix.  Is there a good reason to have distinct prefix for both that work closely together?  How about hadoop.ozone prefix?  From usability point of view, the current prefix are very confusing.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Inconsistent naming convention with Ozone Kerberos configuration"
   },
   {
      "_id": "13229410",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-04-22 17:24:48",
      "description": "SCM BlockManager may try to allocate pipelines in the cases when it is not needed. This happens because BlockManagerImpl#allocateBlock is not lock protected, so multiple pipelines can be allocated from it. One of the pipeline allocation can fail even when one of the existing pipeline already exists.\r\n\r\n\r\n{code}\r\n2019-04-22 22:34:14,336 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 6f4bb2d7-d660-4f9f-bc06-72b10f9a738e, Nodes: 76e1a493-fd55-4d67-9f5\r\n5-c04fd6bd3a33{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}2b9850b2-aed3-4a40-91b5-2447dc5246bf{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}12248721-ea6a-453f-8dad-fc7fbe692f\r\nd2{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\n2019-04-22 22:34:14,386 INFO  impl.RoleInfo (RoleInfo.java:shutdownLeaderElection(134)) - e17b7852-4691-40c7-8791-ad0b0da5201f: shutdown LeaderElection\r\n2019-04-22 22:34:14,388 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 552e28f3-98d9-41f3-86e0-c1b9494838a5, Nodes: e17b7852-4691-40c7-879\r\n1-ad0b0da5201f{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}fd365bac-e26e-4b11-afd8-9d08cd1b0521{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}9583a007-7f02-4074-9e26-19bc18e29e\r\nc5{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\n2019-04-22 22:34:14,388 INFO  impl.RoleInfo (RoleInfo.java:updateAndGet(143)) - e17b7852-4691-40c7-8791-ad0b0da5201f: start FollowerState\r\n2019-04-22 22:34:14,388 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 5383151b-d625-4362-a7dd-c0d353acaf76, Nodes: 80f16ad6-3879-4a64-a3c\r\n7-7719813cc139{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}082ce481-7fb0-4f88-ac21-82609290a6a2{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}dd5f5a70-0217-4577-b7a2-c42aa139d1\r\n8a{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\n2019-04-22 22:34:14,389 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: be4854e5-7933-4caa-b32e-f482cf500247, Nodes: 6e2356f1-479d-498b-876\r\na-1c90623c498b{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}8ac46d94-9975-4eea-9448-2618c69d7bf3{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}a3ed36a1-44ca-47b2-b9b3-5aeef04595\r\n18{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\n2019-04-22 22:34:14,390 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: 21e368e2-f82a-4c61-9cc3-06e8de22ea6b, Nodes: 82632040-5754-4122-b187-331879586842{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}923c8537-b869-4085-adcb-0a9accdcd089{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}c6d790bf-e3a6-4064-acb5-f74796cd38a9{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\n2019-04-22 22:34:14,390 INFO  pipeline.RatisPipelineProvider (RatisPipelineProvider.java:lambda$create$1(103)) -  pipeline Pipeline[ Id: cccbc2ed-e0e2-4578-a8a2-94f4b645be52, Nodes: 91ae6848-a778-43be-a4a1-5855f7adc0d8{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}8f330a03-40e2-4bd1-9b43-5e05b13d89f0{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}4f3070dc-650b-48d7-87b5-d2076104e7b4{ip: 192.168.0.104, host: 192.168.0.104, certSerialId: null}, Type:RATIS, Factor:THREE, State:OPEN]\r\n2019-04-22 22:34:14,392 ERROR block.BlockManagerImpl (BlockManagerImpl.java:allocateBlock(192)) - Pipeline creation failed for type:RATIS factor:THREE\r\norg.apache.hadoop.hdds.scm.pipeline.InsufficientDatanodesException: Cannot create pipeline of factor 3 using 2 nodes 20 healthy nodes 20 all nodes.\r\n        at org.apache.hadoop.hdds.scm.pipeline.RatisPipelineProvider.create(RatisPipelineProvider.java:122)\r\n        at org.apache.hadoop.hdds.scm.pipeline.PipelineFactory.create(PipelineFactory.java:57)\r\n        at org.apache.hadoop.hdds.scm.pipeline.SCMPipelineManager.createPipeline(SCMPipelineManager.java:148)\r\n        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:190)\r\n        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)\r\n        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)\r\n        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:7533)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n2019-04-22 22:34:14,395 ERROR block.BlockManagerImpl (BlockManagerImpl.java:allocateBlock(213)) - Unable to allocate a block for the size: 16384, type: RATIS, factor: THREE\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCMBlockManager findPipeline and createPipeline are not lock protected"
   },
   {
      "_id": "13229398",
      "assignee": "xyao",
      "components": [],
      "created": "2019-04-22 16:11:23",
      "description": "[https://ci.anzix.net/job/ozone-nightly/72/testReport/]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix nightly run failures after HDDS-976"
   },
   {
      "_id": "13229384",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-04-22 14:20:33",
      "description": "While allocation pipelines, Ratis pipeline provider considers all the pipelines irrespective of the state of the pipeline. This can lead to case where all the datanodes are up but the pipelines are in closing state in SCM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "RatisPipelineProvider should only consider open pipeline while excluding dn for pipeline allocation"
   },
   {
      "_id": "13228497",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-04-17 05:00:39",
      "description": "In MiniOzoneChaosCluster some of the calls fail with NotReplicatedException. This Exception needs to be handled in OzoneClient\r\n\r\n{code}\r\n2019-04-17 10:13:47,254 INFO  client.GrpcClientProtocolService (GrpcClientProtocolService.java:lambda$processClientRequest$0(264)) - Failed RaftClientRequest:client-43B95E0E3BE0->1ebec547-8cf8-4466-bf43-ea9f19fb546b@group-1B28E0BF6CBC, cid=800, seq=0, Watch-ALL_COMMITTED(234), Message:<EMPTY>, reply=RaftClientReply:client-43B95E0E3BE0->1ebec547-8cf8-4466-bf43-ea9f19fb546b@group-1B28E0BF6CBC, cid=800, FAILED org.apache.ratis.protocol.NotReplicatedException: Request with call Id 800 and log index 234 is not yet replicated to ALL_COMMITTED, logIndex=234, commits[1ebec547-8cf8-4466-bf43-ea9f19fb546b:c267, 7b200ef5-7711-437d-a9bc-ad0e18fdf6bb:c267, ffbfb65f-a622-466d-b6e8-47038cc15e0b:c226]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add handling of NotReplicatedException in OzoneClient"
   },
   {
      "_id": "13227855",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-04-13 08:51:20",
      "description": "the test is failing with the following exception\r\n\r\n{code}\r\njava.lang.IllegalStateException\r\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:129)\r\n\tat org.apache.hadoop.hdds.scm.storage.CommitWatcher.watchForCommit(CommitWatcher.java:191)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestCommitWatcher.testReleaseBuffersOnException(TestCommitWatcher.java:277)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{code}\r\n\r\n\r\nhttps://ci.anzix.net/job/ozone-nightly/63/testReport/org.apache.hadoop.ozone.client.rpc/TestCommitWatcher/testReleaseBuffersOnException/\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "ozone-flaky-test"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestCommitWatcher#testReleaseBuffersOnException fails with IllegalStateException"
   },
   {
      "_id": "13227662",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-12 09:11:00",
      "description": "TestDatanodeStateMachine is flaky.\r\n It has failed in the following build\r\n [https://builds.apache.org/job/PreCommit-HDDS-Build/2650/artifact/out/patch-unit-hadoop-hdds.txt]\r\n [https://builds.apache.org/job/hadoop-multibranch/job/PR-661/6/artifact/out/patch-unit-hadoop-hdds_container-service.txt]\r\n [https://builds.apache.org/job/PreCommit-HDDS-Build/2635/artifact/out/patch-unit-hadoop-hdds.txt]\r\n\r\nStack trace:\r\n{noformat}\r\njava.lang.Thread.State: WAITING (on object monitor)\r\n        at sun.misc.Unsafe.park(Native Method)\r\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)\r\n        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)\r\n        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\n\tat org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389)\r\n\tat org.apache.hadoop.ozone.container.common.TestDatanodeStateMachine.testStartStopDatanodeStateMachine(TestDatanodeStateMachine.java:166)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n\r\n[INFO] \r\n[INFO] Results:\r\n[INFO] \r\n[ERROR] Errors: \r\n[ERROR]   TestDatanodeStateMachine.testStartStopDatanodeStateMachine:166 ? Timeout Timed...\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "ozone-flaky-test",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestDatanodeStateMachine is flaky"
   },
   {
      "_id": "13227467",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-11 14:05:16",
      "description": "I upgraded my docker-compose to the latest available one (1.24.0)\r\n\r\nBut after the upgrade I can't start the docker-compose based cluster any more:\r\n\r\n{code}\r\n./test.sh \r\n-------------------------------------------------\r\nExecuting test(s): [basic]\r\n\r\n  Cluster type:      ozone\r\n  Compose file:      /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/smoketest/../compose/ozone/docker-compose.yaml\r\n  Output dir:        /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/smoketest/result\r\n  Command to rerun:  ./test.sh --keep --env ozone basic\r\n-------------------------------------------------\r\nERROR: In file /home/elek/projects/hadoop-review/hadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozone/docker-config: environment variable name 'LOG4J2.PROPERTIES_appender.rolling.file \r\n{code}\r\n\r\nIt turned out that the line of LOG4J2.PROPERTIES_appender.rolling.file contains an unnecessary space which is not accepted by the latest docker-compose any more.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Ozone compose files are not compatible with the latest docker-compose"
   },
   {
      "_id": "13227465",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-11 13:50:14",
      "description": "The ./smoketest folder in the distribution package contains robotframework based test scripts to test the main behaviour of Ozone.\r\n\r\nThe tests have two layers:\r\n\r\n1. robot test definitions to execute commands and assert the results (on a given host machine)\r\n2. ./smoketest/test.sh which starts/stops the docker-compose based environments AND execute the selected robot tests inside the right hosts\r\n\r\nThe second one (test.sh) has some serious limitations:\r\n\r\n1. all the tests are executed inside the same container (om):\r\n\r\nhttps://github.com/apache/hadoop/blob/5f951ea2e39ae4dfe554942baeec05849cd7d3c2/hadoop-ozone/dist/src/main/smoketest/test.sh#L89\r\n\r\nSome of the tests (ozonesecure-mr, ozonefs) may require the flexibility to execute different robot tests in different containers.\r\n\r\n2. The definition of the global test set is complex and hard to understood. \r\n\r\nThe current code is:\r\n{code}\r\n   TESTS=(\"basic\")\r\n   execute_tests ozone \"${TESTS[@]}\"\r\n   TESTS=(\"auditparser\")\r\n   execute_tests ozone \"${TESTS[@]}\"\r\n   TESTS=(\"ozonefs\")\r\n   execute_tests ozonefs \"${TESTS[@]}\"\r\n   TESTS=(\"basic\")\r\n   execute_tests ozone-hdfs \"${TESTS[@]}\"\r\n   TESTS=(\"s3\")\r\n   execute_tests ozones3 \"${TESTS[@]}\"\r\n   TESTS=(\"security\")\r\n   execute_tests ozonesecure .\r\n{code} \r\n\r\nFor example for ozonesecure the TESTS is not used. And the usage of bash lists require additional complexity in the execute_tests function.\r\n\r\nI propose here a very lightweight refactor. Instead of including both the test definitions AND the helper methods in test.sh I would separate them.\r\n\r\nLet's put a test.sh to each of the compose directories. The separated test.sh can include common methods from a main shell script. For example:\r\n\r\n{code}\r\n\r\nsource \"$COMPOSE_DIR/../testlib.sh\"\r\n\r\nstart_docker_env\r\n\r\nexecute_robot_test scm basic/basic.robot\r\n\r\nexecute_robot_test scm s3\r\n\r\nstop_docker_env\r\n\r\ngenerate_report\r\n\r\n{code}\r\n\r\nThis is a more clean and more flexible definition. It's easy to execute just this test (as it's saved to the compose/ozones3 directory. And it's more flexible.\r\n\r\nOther example, where multiple containers are used to execute tests:\r\n\r\n{code}\r\n\r\nsource \"$COMPOSE_DIR/../testlib.sh\"\r\n\r\nstart_docker_env\r\n\r\nexecute_robot_test scm ozonefs/ozonefs.robot\r\n\r\n\r\n\r\nexport OZONE_HOME=/opt/ozone\r\n\r\nexecute_robot_test hadoop32 ozonefs/hadoopo3fs.robot\r\n\r\nexecute_robot_test hadoop31 ozonefs/hadoopo3fs.robot\r\n\r\nstop_docker_env\r\n\r\ngenerate_report\r\n{code}\r\n\r\nWith this separation the definition of the helper methods (eg. execute_robot_test or stop_docker_env) would also be simplified.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support multi-container robot test execution"
   },
   {
      "_id": "13227308",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-10 19:58:46",
      "description": "Fix the following shellcheck errors in start-chaos.sh:\r\n{code}\r\nhadoop-ozone/integration-test/src/test/bin/start-chaos.sh:18:6: note: Use $(..) instead of legacy `..`. [SC2006]\r\nhadoop-ozone/integration-test/src/test/bin/start-chaos.sh:27:19: note: Double quote to prevent globbing and word splitting. [SC2086]\r\nhadoop-ozone/integration-test/src/test/bin/start-chaos.sh:28:20: note: Double quote to prevent globbing and word splitting. [SC2086]\r\nhadoop-ozone/integration-test/src/test/bin/start-chaos.sh:31:33: note: Double quote to prevent globbing and word splitting. [SC2086]\r\nhadoop-ozone/integration-test/src/test/bin/start-chaos.sh:35:23: note: Double quote to prevent globbing and word splitting. [SC2086]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix shellcheck errors in start-chaos.sh"
   },
   {
      "_id": "13227261",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-10 17:23:23",
      "description": "[https://ci.anzix.net/job/ozone/16618/testReport/org.apache.hadoop.ozone.client.rpc/TestOzoneClientRetriesOnException/testGroupMismatchExceptionHandling/]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "ozone-flaky-test"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneClientRetriesOnException.testGroupMismatchExceptionHandling is flaky"
   },
   {
      "_id": "13227230",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-10 16:27:19",
      "description": "TestOzoneManagerHA.testMultipartUploadWithOneOmNodeDown is flaky, we get the below exception when it fails.\r\n\r\n{code}\r\norg.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client client-04649B8D5AF3->RAFT is closed.\r\n at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\r\n at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\r\n at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommand(OzoneManagerRatisClient.java:133)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestToRatis(OzoneManagerProtocolServerSideTranslatorPB.java:97)\r\n at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:83)\r\n at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\nCaused by: org.apache.ratis.protocol.AlreadyClosedException: SlidingWindow$Client client-04649B8D5AF3->RAFT is closed.\r\n at org.apache.ratis.util.SlidingWindow$Client.alreadyClosed(SlidingWindow.java:350)\r\n at org.apache.ratis.util.SlidingWindow$Client.submitNewRequest(SlidingWindow.java:224)\r\n at org.apache.ratis.client.impl.RaftClientImpl.sendAsync(RaftClientImpl.java:207)\r\n at org.apache.ratis.client.impl.RaftClientImpl.sendAsync(RaftClientImpl.java:174)\r\n at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendRequestAsync(OzoneManagerRatisClient.java:208)\r\n at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommandAsync(OzoneManagerRatisClient.java:168)\r\n at org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient.sendCommand(OzoneManagerRatisClient.java:132)\r\n ... 11 more\r\nCaused by: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-04649B8D5AF3->omNode-1@group-523986131536, cid=71, seq=1*, RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$396/1529424209@7ae5da75 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms)\r\n at org.apache.ratis.client.impl.RaftClientImpl.newRaftRetryFailureException(RaftClientImpl.java:383)\r\n at org.apache.ratis.client.impl.RaftClientImpl.handleAsyncRetryFailure(RaftClientImpl.java:388)\r\n at org.apache.ratis.client.impl.RaftClientImpl.lambda$sendRequestAsync$14(RaftClientImpl.java:370)\r\n at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:870)\r\n at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:852)\r\n at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\r\n at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.completeReplyExceptionally(GrpcClientProtocolClient.java:329)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers.access$000(GrpcClientProtocolClient.java:245)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:257)\r\n at org.apache.ratis.grpc.client.GrpcClientProtocolClient$AsyncStreamObservers$1.onNext(GrpcClientProtocolClient.java:248)\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls$StreamObserverToCallListenerAdapter.onMessage(ClientCalls.java:421)\r\n at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)\r\n at org.apache.ratis.thirdparty.io.grpc.ForwardingClientCallListener.onMessage(ForwardingClientCallListener.java:33)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1MessagesAvailable.runInContext(ClientCallImpl.java:519)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "ozone-flaky-test"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerHA.testMultipartUploadWithOneOmNodeDown is flaky"
   },
   {
      "_id": "13227228",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-04-10 16:22:23",
      "description": "TestCloseContainerCommandHandler.testCloseContainerViaStandalone is flaky, we get the below exception when it fails.\r\n\r\n{code}\r\norg.apache.ratis.protocol.NotLeaderException: Server a200dff7-f26d-4be3-addd-e8e0ca569ae0 is not the leader (null). Request must be sent to leader.\r\n\tat org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:448)\r\n\tat org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:419)\r\n\tat org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:514)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.lambda$submitClientRequestAsync$7(RaftServerProxy.java:333)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.lambda$null$5(RaftServerProxy.java:328)\r\n\tat org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:109)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.lambda$submitRequest$6(RaftServerProxy.java:328)\r\n\tat java.util.concurrent.CompletableFuture.uniComposeStage(CompletableFuture.java:981)\r\n\tat java.util.concurrent.CompletableFuture.thenCompose(CompletableFuture.java:2124)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.submitRequest(RaftServerProxy.java:327)\r\n\tat org.apache.ratis.server.impl.RaftServerProxy.submitClientRequestAsync(RaftServerProxy.java:333)\r\n\tat org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.submitRequest(XceiverServerRatis.java:485)\r\n\tat org.apache.hadoop.ozone.container.common.statemachine.commandhandler.TestCloseContainerCommandHandler.createContainer(TestCloseContainerCommandHandler.java:310)\r\n\tat org.apache.hadoop.ozone.container.common.statemachine.commandhandler.TestCloseContainerCommandHandler.testCloseContainerViaStandalone(TestCloseContainerCommandHandler.java:111)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{code}\r\n\r\nFull log is uploaded.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "ozone-flaky-test",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestCloseContainerCommandHandler is flaky"
   },
   {
      "_id": "13227222",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-10 16:04:40",
      "description": "In HDDS-872 we added Dockerfile and skaffold definition to run dev builds on kubernetes. But it would be great to include example k8s resource definitions helping the deployment of ozone to any kubernetes cluster.\r\n\r\nIn this patch I will\r\n\r\n1. Add k8s resources files to the release tar file to deploy basic ozone cluster\r\n2. Add Dockerfile to the release tar file to create custom ozone image any time\r\n3. Add additional maven profiles to build and push development docker images.\r\n4. We don't need skaffold any more as the maven based approach is more flexible (we can support multiple k8s definitions)\r\n\r\nTo easily support multiple type of configuration (simple ozone, minikube, csi) we need a basic set of k8s resources files and additional transformations to generate the ready-to-use files for each specific use-cases.\r\n\r\nThe easiest way to do this is adopting the existing structure from https://github.com/flokkr/k8s and use https://github.com/elek/flekszible tool. But the tool itself is not required at runtime as we generate all the required k8s resources files during the development and add the results to the version control. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide example k8s deployment files as part of the release package"
   },
   {
      "_id": "13227028",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-04-09 19:38:00",
      "description": "We use parallelStream in during createPipline, this internally uses commonPool. Use Our own\u00a0ForkJoinPool with parallelisim set with number of processors.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid usage of commonPool in RatisPipelineUtils"
   },
   {
      "_id": "13226761",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-04-08 18:22:26",
      "description": "Currently a Ozone Client retries a write operation 5 times. It is possible that the container being written to is already closed by the time it is written to. The key write will fail after retrying multiple times with this error. This needs to be fixed as this is an internal error.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "KeyOutputStream writes fails after max retries while writing to a closed container"
   },
   {
      "_id": "13226746",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-08 16:27:50",
      "description": "When I analyzed the usages of the available RPC protocols in Ozone I found that the ScmBlockLocationProtocol is not used in ObjectStore at all.\r\n\r\nI would propose to remove it...",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove unused ScmBlockLocationProtocol from ObjectStoreHandler"
   },
   {
      "_id": "13226462",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-04-05 22:29:54",
      "description": "In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.\r\n # StartTransaction, where we validate request and check for any errors and return the response.\r\n # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.\r\n\r\nIn this way, all requests which are failed with like volume not found or some conditions which\u00a0 are\u00a0not satisfied like when Key not found during rename, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Convert all OM Key related operations to HA model"
   },
   {
      "_id": "13226433",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-04-05 20:16:47",
      "description": "{{testStartStopDatanodeStateMachine}} is flaky, causing [occasional pre-commit build failures|https://builds.apache.org/job/hadoop-multibranch/job/PR-691/1/artifact/out/patch-unit-hadoop-hdds_container-service.txt].\u00a0 HDDS-1332 added some logging to find out more about the cause.\r\n\r\nI think the problem is not test-specific, and is caused by the following: {{SCMConnectionManager#scmMachines}} is a plain {{HashMap}}, guarded by a {{ReadWriteLock}} in most places where it's used, except {{getValues()}}.\u00a0 The method also returns the values collection without any write protection (though currently none of the callers modify it).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Concurrency issue in SCMConnectionManager#getValues"
   },
   {
      "_id": "13226328",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-05 09:13:41",
      "description": "[~arpaga] showed me a problem that TestQueryNode.testHealthyNodesCount is failed in the CI check of HDDS-1339.\r\n\r\nAccording to the logs the test is timed out because only 4 datanodes are started out of the 5.\r\n\r\nThe log also contained an exception from one datanode:\r\n\r\n{code}\r\n2019-04-04 00:26:33,583 WARN  ozone.HddsDatanodeService (LogAdapter.java:warn(59)) - failed to register any UNIX signal loggers: \r\njava.lang.IllegalStateException: Can't re-install the signal handlers.\r\n    at org.apache.hadoop.util.SignalLogger.register(SignalLogger.java:77)\r\n    at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:718)\r\n    at org.apache.hadoop.util.StringUtils.startupShutdownMessage(StringUtils.java:707)\r\n    at org.apache.hadoop.ozone.HddsDatanodeService.createHddsDatanodeService(HddsDatanodeService.java:126)\r\n    at org.apache.hadoop.ozone.HddsDatanodeService.createHddsDatanodeService(HddsDatanodeService.java:108)\r\n    at org.apache.hadoop.ozone.MiniOzoneClusterImpl$Builder.createHddsDatanodes(MiniOzoneClusterImpl.java:552)\r\n{code}\r\n\r\nThe code which requires the signal handler is the following (signal handler is registered in the startupShutdownMessage)\r\n\r\n{code}\r\n  /**\r\n   * Create an Datanode instance based on the supplied command-line arguments.\r\n   * <p>\r\n   * This method is intended for unit tests only. It suppresses the\r\n   * startup/shutdown message and skips registering Unix signal handlers.\r\n   *\r\n   * @param args        command line arguments.\r\n   * @param conf        HDDS configuration\r\n   * @param printBanner if true, then log a verbose startup message.\r\n   * @return Datanode instance\r\n   */\r\n  private static HddsDatanodeService createHddsDatanodeService(\r\n      String[] args, Configuration conf, boolean printBanner) {\r\n    if (args.length == 0 && printBanner) {\r\n      StringUtils\r\n          .startupShutdownMessage(HddsDatanodeService.class, args, LOG);\r\n      return new HddsDatanodeService(conf);\r\n    } else {\r\n      new HddsDatanodeService().run(args);\r\n      return null;\r\n   }\r\n{code}\r\n\r\nAs you can read from the comment it's expected to be called with printBanner=false to avoid the creation of the signal handler. \r\n\r\nNote: In the startupShutdownMessage method a new signal handler is registered and signal handlers can be registered only once:\r\n\r\n{code}\r\n//SignalLogger\r\n  void register(final LogAdapter LOG) {\r\n   if (registered) {\r\n      throw new IllegalStateException(\"Can't re-install the signal handlers.\");\r\n    }\r\n ....\r\n{code}\r\n\r\nWe have a dedicated method to create datanode service for the unit tests. The only thing what we need is to turn OFF the signal handler registration here. (The following code fragment shows the original state where the signal handler creation is requested with the true parameter value)\r\n\r\n{code}\r\n  @VisibleForTesting\r\n  public static HddsDatanodeService createHddsDatanodeService(\r\n      String[] args, Configuration conf) {\r\n    return createHddsDatanodeService(args, conf, true);\r\n  }\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid the usage of signal handlers in datanodes of the MiniOzoneClusters"
   },
   {
      "_id": "13226312",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-04-05 07:34:42",
      "description": "Recon Server start fails due to ClassNotFound exception when looking for org.apache.hadoop.ozone.recon.ReconServer. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon start fails due to changes in Aggregate Schema definition (HDDS-1189)."
   },
   {
      "_id": "13226304",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-04-05 06:15:51",
      "description": "Key write fails with BlockOutputStream has been closed\r\n\r\n{code}\r\n2019-04-05 11:24:47,770 ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(102)) - LOADGEN: Create key:pool-431-thread-9-2092651262 failed with exception, but skipping\r\njava.io.IOException: BlockOutputStream has been closed.\r\n        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.checkOpen(BlockOutputStream.java:662)\r\n        at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:245)\r\n        at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:131)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:325)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:287)\r\n        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)\r\n        at java.io.OutputStream.write(OutputStream.java:75)\r\n        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:100)\r\n        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:143)\r\n        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Key write fails with BlockOutputStream has been closed exception"
   },
   {
      "_id": "13226268",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-04-04 21:53:59",
      "description": "In this jira, we shall convert all OM Bucket related operations to OM HA model, which is a 2 step.\r\n # StartTransaction, where we validate request and check for any errors and return the response.\r\n # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.\r\n\r\nIn this way, all requests which are failed with like\u00a0bucket not found or some conditions which i have not satisfied like when deleting\u00a0bucket should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Convert all OM Bucket related operations to HA model"
   },
   {
      "_id": "13226212",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-04-04 18:06:46",
      "description": "Added an RPC end point to serve the set of updates in OM RocksDB from a given sequence number.\r\nThis will be used by Recon (HDDS-1105) to push the data to all the tasks that will keep their aggregate data up to date. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ability in OM to serve delta updates through an API."
   },
   {
      "_id": "13226206",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-04-04 17:43:06",
      "description": "Some hk2 transitive dependencies were mistakenly excluded in HDDS-1358 to solve maven enforcer plugin issues. This jira cleans that up. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix OzoneS3 Gateway server due to exclusion of hk2-api"
   },
   {
      "_id": "13226202",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-04 17:35:32",
      "description": "TestMiniChaosOzoneCluster is failing with the below exception\r\n{noformat}\r\n[ERROR] org.apache.hadoop.ozone.TestMiniChaosOzoneCluster  Time elapsed: 265.679 s  <<< ERROR!\r\njava.util.ConcurrentModificationException\r\n\tat java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909)\r\n\tat java.util.ArrayList$Itr.next(ArrayList.java:859)\r\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl.stop(MiniOzoneClusterImpl.java:350)\r\n\tat org.apache.hadoop.ozone.MiniOzoneClusterImpl.shutdown(MiniOzoneClusterImpl.java:325)\r\n\tat org.apache.hadoop.ozone.MiniOzoneChaosCluster.shutdown(MiniOzoneChaosCluster.java:130)\r\n\tat org.apache.hadoop.ozone.TestMiniChaosOzoneCluster.shutdown(TestMiniChaosOzoneCluster.java:92)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "ozone-flaky-test",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ConcurrentModificationException in TestMiniChaosOzoneCluster"
   },
   {
      "_id": "13226144",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-04 14:55:54",
      "description": "The MapReduce example project on branch ozone-0.4 contains 0.5.0-SNAPSHOT references in the dir:\r\n\r\nhadoop-ozone/dist/target/ozone-0.4.0-SNAPSHOT/compose/ozonesecure-mr\r\n\r\nAfter HDDS-1333 (which introduce filtering) it will be straightforward to always use the current version.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Make the ozonesecure-mr environment definition version independent"
   },
   {
      "_id": "13226129",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-04-04 13:41:26",
      "description": "TestBlockOutputStreamWithFailures is failing with the following error\r\n\r\n{noformat}\r\n2019-04-04 18:52:43,240 INFO  volume.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(140)) - Scheduling a check for org.apache.hadoop.ozone.container.common.volume.HddsVolume@1f6c0e8a\r\n2019-04-04 18:52:43,240 INFO  volume.HddsVolumeChecker (HddsVolumeChecker.java:checkAllVolumes(203)) - Scheduled health check for volume org.apache.hadoop.ozone.container.common.volume.HddsVolume@1f6c0e8a\r\n2019-04-04 18:52:43,241 ERROR server.GrpcService (ExitUtils.java:terminate(133)) - Terminating with exit status 1: Failed to start Grpc server\r\njava.io.IOException: Failed to bind\r\n  at org.apache.ratis.thirdparty.io.grpc.netty.NettyServer.start(NettyServer.java:253)\r\n  at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:166)\r\n  at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl.start(ServerImpl.java:81)\r\n  at org.apache.ratis.grpc.server.GrpcService.startImpl(GrpcService.java:144)\r\n  at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)\r\n  at org.apache.ratis.server.impl.RaftServerRpcWithProxy.start(RaftServerRpcWithProxy.java:69)\r\n  at org.apache.ratis.server.impl.RaftServerProxy.lambda$start$3(RaftServerProxy.java:300)\r\n  at org.apache.ratis.util.LifeCycle.startAndTransition(LifeCycle.java:202)\r\n  at org.apache.ratis.server.impl.RaftServerProxy.start(RaftServerProxy.java:298)\r\n  at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.start(XceiverServerRatis.java:419)\r\n  at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.start(OzoneContainer.java:186)\r\n  at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:169)\r\n  at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:338)\r\n  at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.BindException: Address already in use\r\n  at sun.nio.ch.Net.bind0(Native Method)\r\n  at sun.nio.ch.Net.bind(Net.java:433)\r\n  at sun.nio.ch.Net.bind(Net.java:425)\r\n  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:130)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1358)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:1019)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\r\n  at org.apache.ratis.thirdparty.io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:366)\r\n  at org.apache.ratis.thirdparty.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\r\n  at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)\r\n  at org.apache.ratis.thirdparty.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462)\r\n  at org.apache.ratis.thirdparty.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897)\r\n  at org.apache.ratis.thirdparty.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n  ... 1 more\r\n{noformat}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestBlockOutputStreamWithFailures is failing"
   },
   {
      "_id": "13226118",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335213",
            "id": "12335213",
            "name": "upgrade"
         }
      ],
      "created": "2019-04-04 13:13:09",
      "description": "Ozone In-Place upgrade tool is a tool to upgrade hdfs data to ozone data without data movement.\r\n\r\nIn this jira I will create a skeleton project with the cli interface without any business logic.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "[Ozone Upgrade] Create the project skeleton with CLI interface"
   },
   {
      "_id": "13226110",
      "assignee": "elek",
      "components": [],
      "created": "2019-04-04 12:39:45",
      "description": "CSI (Container Storage Interface) is a vendor neutral storage interface specification for container orchestrators. CSI support is implemented in YARN, Kubernetes and Mesos. Implementing a CSI server makes it easy to mount disk volumes for containers.\r\n\r\nSee https://github.com/container-storage-interface/spec for more details about the spec.\r\n\r\nUntil now we used https://github.com/CTrox/csi-s3 server to support CSI specification. Using an ozone specific CSI server would have the following advantages:\r\n\r\n * We can provide additional functionalities (as we have access to the internal Ozone API not just the very generic s3 api).\r\n * Security setup can be synchronized.\r\n * Increased stability\r\n * Simplified deployment (only the minimal set of the components are required to be installed)\r\n\r\nThe CSI specification itself is very simple (https://github.com/container-storage-interface/spec/blob/master/csi.proto) at least the part which is required for Ozone.\r\n\r\nWe can use various fuse s3 driver to mount the ozone buckets via s3.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create customized CSI server for Ozone"
   },
   {
      "_id": "13225883",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-04-03 18:26:48",
      "description": "In this jira, we shall convert all OM related operations to OM HA model, which is a 2 step.\r\n # StartTransaction, where we validate request and check for any errors and return the response.\r\n # ApplyTransaction, where original OM request will have a response which needs to be applied to OM DB. This step is just to apply response to Om DB.\r\n\r\nIn this way, all requests which are failed with like volume not found or some conditions which i have not satisfied like when deleting volume should be empty, these all will be executed during startTransaction, and if it fails these requests will not be written to raft log also.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Convert all OM Volume related operations to HA model"
   },
   {
      "_id": "13225852",
      "assignee": "xyao",
      "components": [],
      "created": "2019-04-03 16:31:32",
      "description": "OM failed to start after HDDS-1355.\r\n\r\n{code}\r\n\r\nom_1 | 2019-04-03 16:23:50 ERROR OzoneManager:865 - Failed to start the OzoneManager.\r\nom_1 | java.lang.IllegalArgumentException: IP Address is invalid\r\nom_1 | at org.bouncycastle.asn1.x509.GeneralName.<init>(Unknown Source)\r\nom_1 | at org.apache.hadoop.hdds.security.x509.certificates.utils.CertificateSignRequest$Builder.addAltName(CertificateSignRequest.java:205)\r\nom_1 | at org.apache.hadoop.hdds.security.x509.certificates.utils.CertificateSignRequest$Builder.addIpAddress(CertificateSignRequest.java:197)\r\nom_1 | at org.apache.hadoop.ozone.om.OzoneManager.getSCMSignedCert(OzoneManager.java:1387)\r\nom_1 | at org.apache.hadoop.ozone.om.OzoneManager.initializeSecurity(OzoneManager.java:1018)\r\nom_1 | at org.apache.hadoop.ozone.om.OzoneManager.omInit(OzoneManager.java:971)\r\nom_1 | at org.apache.hadoop.ozone.om.OzoneManager.createOm(OzoneManager.java:928)\r\nom_1 | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:859)\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OM failed to start with incorrect hostname set as ip address in CSR"
   },
   {
      "_id": "13225784",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-04-03 11:36:21",
      "description": "Ozone Datanode exits with the following error, this happens because DN hasn't received a scmID from the SCM after registration but is processing a client command.\r\n\r\n{code}\r\n2019-04-03 17:02:10,958 ERROR storage.RaftLogWorker (ExitUtils.java:terminate(133)) - Terminating with exit status 1: df6b578e-8d35-44f5-9b21-db7184dcc54e-RaftLogWorker failed.\r\njava.io.IOException: java.lang.NullPointerException: scmId cannot be null\r\n        at org.apache.ratis.util.IOUtils.asIOException(IOUtils.java:54)\r\n        at org.apache.ratis.util.IOUtils.toIOException(IOUtils.java:61)\r\n        at org.apache.ratis.util.IOUtils.getFromFuture(IOUtils.java:83)\r\n        at org.apache.ratis.server.storage.RaftLogWorker$StateMachineDataPolicy.getFromFuture(RaftLogWorker.java:76)\r\n        at org.apache.ratis.server.storage.RaftLogWorker$WriteLog.execute(RaftLogWorker.java:354)\r\n        at org.apache.ratis.server.storage.RaftLogWorker.run(RaftLogWorker.java:219)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NullPointerException: scmId cannot be null\r\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)\r\n        at org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer.create(KeyValueContainer.java:110)\r\n        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handleCreateContainer(KeyValueHandler.java:243)\r\n        at org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler.handle(KeyValueHandler.java:165)\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.createContainer(HddsDispatcher.java:350)\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatchRequest(HddsDispatcher.java:224)\r\n        at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:149)\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.dispatchCommand(ContainerStateMachine.java:347)\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.runCommand(ContainerStateMachine.java:354)\r\n        at org.apache.hadoop.ozone.container.common.transport.server.ratis.ContainerStateMachine.lambda$handleWriteChunk$0(ContainerStateMachine.java:385)\r\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run$$$capture(CompletableFuture.java:1590)\r\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        ... 1 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Datanode exits while executing client command when scmId is null"
   },
   {
      "_id": "13225726",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-04-03 07:54:31",
      "description": "ContainerStateMap cannot find container while allocating blocks.\r\n\r\n{code}\r\norg.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #14\r\n        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:542)\r\n        at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.getContainerInfo(ContainerStateMap.java:189)\r\n        at org.apache.hadoop.hdds.scm.container.ContainerStateManager.getContainer(ContainerStateManager.java:483)\r\n        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainer(SCMContainerManager.java:195)\r\n        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getContainersForOwner(SCMContainerManager.java:466)\r\n        at org.apache.hadoop.hdds.scm.container.SCMContainerManager.getMatchingContainer(SCMContainerManager.java:387)\r\n        at org.apache.hadoop.hdds.scm.block.BlockManagerImpl.allocateBlock(BlockManagerImpl.java:201)\r\n        at org.apache.hadoop.hdds.scm.server.SCMBlockProtocolServer.allocateBlock(SCMBlockProtocolServer.java:172)\r\n        at org.apache.hadoop.ozone.protocolPB.ScmBlockLocationProtocolServerSideTranslatorPB.allocateScmBlock(ScmBlockLocationProtocolServerSideTranslatorPB.java:82)\r\n        at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:7533)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ContainerStateMap cannot find container while allocating blocks."
   },
   {
      "_id": "13225623",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-04-02 20:03:07",
      "description": "In this code, the stream is closed via try with resource.\r\n\r\n{code}\r\n      try (OzoneOutputStream stream = ozoneBucket.createKey(keyName,\r\n          bufferCapacity, ReplicationType.RATIS, ReplicationFactor.THREE,\r\n          new HashMap<>())) {\r\n        stream.write(buffer.array());\r\n      } catch (Exception e) {\r\n        LOG.error(\"LOADGEN: Create key:{} failed with exception\", keyName, e);\r\n        break;\r\n      }\r\n{code}\r\n\r\nHere, the write call fails correctly as expected, However the close doesn't fail with the same exception.\r\n\r\nThe exception stack stack is as following\r\n\r\n{code}\r\n2019-04-03 00:52:54,116 ERROR ozone.MiniOzoneLoadGenerator (MiniOzoneLoadGenerator.java:load(101)) - LOADGEN: Create key:pool-431-thread-9-81262222 failed with exception\r\njava.io.IOException: Retry request failed. retries get failed due to exceeded maximum allowed retries number: 5\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:492)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleRetry(KeyOutputStream.java:514)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleException(KeyOutputStream.java:468)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:344)\r\n        at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:287)\r\n        at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)\r\n        at java.io.OutputStream.write(OutputStream.java:75)\r\n        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:99)\r\n        at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.lambda$startIO$0(MiniOzoneLoadGenerator.java:137)\r\n        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n        Suppressed: java.lang.IllegalArgumentException\r\n                at com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)\r\n                at org.apache.hadoop.ozone.client.io.KeyOutputStream.close(KeyOutputStream.java:643)\r\n                at org.apache.hadoop.ozone.client.io.OzoneOutputStream.close(OzoneOutputStream.java:60)\r\n                at org.apache.hadoop.ozone.MiniOzoneLoadGenerator.load(MiniOzoneLoadGenerator.java:100)\r\n                ... 5 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "KeyOutputStream, close after write request fails after retries, runs into IllegalArgumentException"
   },
   {
      "_id": "13225610",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-04-02 19:00:19",
      "description": "If a follower OM is lagging way behind the leader OM or in case of a restart or bootstrapping, a follower OM might need RocksDB checkpoint from the leader to catch up with it. This is because the leader might have purged its logs after taking a snapshot.\r\n This Jira aims to add support to download a RocksDB checkpoint from leader OM to follower OM\u00a0through a HTTP servlet. We\u00a0reuse the DBCheckpoint servlet used by Recon server.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Download RocksDB checkpoint from OM Leader to Follower"
   },
   {
      "_id": "13225602",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-04-02 18:31:34",
      "description": "The command execution on the datanode is failing with the following exception.\r\n\r\n{code}\r\n2019-04-02 23:56:30,434 ERROR statemachine.DatanodeStateMachine (DatanodeStateMachine.java:start(196)) - Unable to finish the execution.\r\njava.lang.NullPointerException\r\n        at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:179)\r\n        at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.execute(RunningDatanodeState.java:89)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:354)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:183)\r\n        at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:338)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Command Execution in Datanode fails becaue of NPE"
   },
   {
      "_id": "13225234",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-04-01 10:26:44",
      "description": "{{ozone.metadata.dirs}} doesn't pick comma(,) separated paths.\r\n It only picks one path as opposed to the property name _ozone.metadata.dir{color:#FF0000}s{color}_\r\n{code:java}\r\n   <property>\r\n      <name>ozone.metadata.dirs</name>\r\n      <value>/data/data1/meta,/home/hdfs/data/meta</value>\r\n   </property>\r\n{code}\r\n{code:java}\r\n2019-03-31 18:44:54,824 WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.\r\nSCM initialization succeeded.Current cluster id for sd=/data/data1/meta,/home/hdfs/data/meta/scm;cid=CID-1ad502d1-0104-4055-838b-1208ab78f35c\r\n2019-03-31 18:44:55,079 INFO server.StorageContainerManager: SHUTDOWN_MSG:\r\n{code}\r\n{code:java}\r\n[hdfs@localhost ozone-0.5.0-SNAPSHOT]$ ls //data/data1/meta,/home/hdfs/data/meta/scm/current/VERSION\r\nVERSION\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone.metadata.dirs doesn't pick multiple dirs"
   },
   {
      "_id": "13225219",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-04-01 09:40:28",
      "description": "Prometheus reports the following error ({{ozoneperf}} compose dir):\r\n\r\n{code}\r\nprometheus_1 | ... msg=\"append failed\" err=\"invalid metric type \\\"apache.hadoop.hdds.scm.server._scm_container_metrics_deleted_containers gauge\\\"\"\r\n{code}\r\n\r\nMetric name cannot contain dots.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Invalid metric type due to fully qualified class name"
   },
   {
      "_id": "13224878",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-03-29 17:05:07",
      "description": "Guice Jetty integration that is being used for Recon Server API layer is not working as expected. Fixing that in this JIRA.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Recon Server REST API not working as expected."
   },
   {
      "_id": "13224849",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-29 15:06:45",
      "description": "Let's check the potential subcommands of ozone sh:\r\n\r\n{code}\r\n[hadoop@om-0 keytabs]$ ozone sh\r\nIncomplete command\r\nUsage: ozone sh [-hV] [--verbose] [-D=<String=String>]... [COMMAND]\r\nShell for Ozone object store\r\n      --verbose   More verbose output. Show the stack trace of the errors.\r\n  -D, --set=<String=String>\r\n\r\n  -h, --help      Show this help message and exit.\r\n  -V, --version   Print version information and exit.\r\nCommands:\r\n  volume, vol  Volume specific operations\r\n  bucket       Bucket specific operations\r\n  key          Key specific operations\r\n  token        Token specific operations\r\n{code}\r\n\r\nThis is fine, but for ozone s3:\r\n\r\n{code}\r\n[hadoop@om-0 keytabs]$ ozone s3\r\nIncomplete command\r\nUsage: ozone s3 [-hV] [--verbose] [-D=<String=String>]... [COMMAND]\r\nShell for S3 specific operations\r\n      --verbose   More verbose output. Show the stack trace of the errors.\r\n  -D, --set=<String=String>\r\n\r\n  -h, --help      Show this help message and exit.\r\n  -V, --version   Print version information and exit.\r\nCommands:\r\n  getsecret    Returns s3 secret for current user\r\n  path         Returns the ozone path for S3Bucket\r\n  volume, vol  Volume specific operations\r\n  bucket       Bucket specific operations\r\n  key          Key specific operations\r\n  token        Token specific operations\r\n{code}\r\n\r\nThis list should contain only the getsecret/path commands and not the volume/bucket/key subcommands.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone s3 shell command has confusing subcommands"
   },
   {
      "_id": "13224799",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-03-29 11:40:32",
      "description": "There is a {{BackgroundPipelineCreator}} thread in SCM which runs in a fixed interval and tries to create pipelines. This BackgroundPipelineCreator uses {{IOException}} as exit criteria (no more pipelines can be created). In each run of BackgroundPipelineCreator we exit when we are not able to create any more pipelines, i.e. when we get IOException while trying to create the pipeline. This means that {{scm_pipeline_metrics_num_pipeline_creation_failed}} value will get incremented in each run of BackgroundPipelineCreator.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Metrics scm_pipeline_metrics_num_pipeline_creation_failed keeps increasing because of BackgroundPipelineCreator"
   },
   {
      "_id": "13224673",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-03-28 18:52:50",
      "description": "{{ozone genconf}} fails due to incomplete classpath.\r\n\r\nSteps to reproduce:\r\n\r\n# [build and run Ozone|https://cwiki.apache.org/confluence/display/HADOOP/Development+cluster+with+docker]\r\n# run {{ozone genconf}} in one of the containers:\r\n\r\n{code}\r\n$ ozone genconf /tmp\r\nException in thread \"main\" java.lang.NoClassDefFoundError: com/sun/xml/bind/v2/model/annotation/AnnotationReader\r\n\u00a0 at java.lang.ClassLoader.defineClass1(Native Method)\r\n...\r\n\u00a0 at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:242)\r\n\u00a0 at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:234)\r\n\u00a0 at javax.xml.bind.ContextFinder.find(ContextFinder.java:441)\r\n\u00a0 at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:641)\r\n\u00a0 at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:584)\r\n\u00a0 at org.apache.hadoop.hdds.conf.OzoneConfiguration.readPropertyFromXml(OzoneConfiguration.java:57)\r\n\u00a0 at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.generateConfigurations(GenerateOzoneRequiredConfigurations.java:103)\r\n\u00a0 at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.call(GenerateOzoneRequiredConfigurations.java:73)\r\n\u00a0 at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.call(GenerateOzoneRequiredConfigurations.java:50)\r\n\u00a0 at picocli.CommandLine.execute(CommandLine.java:919)\r\n...\r\n\u00a0 at org.apache.hadoop.ozone.genconf.GenerateOzoneRequiredConfigurations.main(GenerateOzoneRequiredConfigurations.java:68)\r\nCaused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.model.annotation.AnnotationReader\r\n\u00a0 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\u00a0 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\u00a0 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\r\n\u00a0 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\u00a0 ... 36 more\r\n{code}\r\n\r\n{{AnnotationReader}} is in {{jaxb-core}} jar, which is not in the {{hadoop-ozone-tools}} classpath.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "NoClassDefFoundError when running ozone genconf"
   },
   {
      "_id": "13224642",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-28 17:00:07",
      "description": "The following tests are FAILED:\r\n\u00a0\r\n[checkstyle]: checkstyle check is failed ([https://ci.anzix.net/job/ozone-nightly/44/checkstyle/])",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix checkstyle issue in TestDatanodeStateMachine"
   },
   {
      "_id": "13224467",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-03-28 01:45:46",
      "description": "In Om HA getS3Secret\u00a0 should happen only leader OM.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nThe reason is similar to initiateMultipartUpload. For more info refer HDDS-1319\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement GetS3Secret to use double buffer and cache."
   },
   {
      "_id": "13224466",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-28 01:31:33",
      "description": "As we are releasing ozone-0.4, we should not have hard-coded ozone-0.5 for trunk.\u00a0\r\n\r\nThe proposal is to use the following to replace it\r\n\r\n{{cd}}\u00a0{{$(git rev-parse --show-toplevel)}}{{/hadoop-ozone/dist/target/ozone-}}{{*-SNAPSHOT}}{{/compose/ozone}}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Remove hard-coded version ozone-0.5.0 from ReadMe of ozonesecure-mr docker-compose"
   },
   {
      "_id": "13224372",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-03-27 17:10:17",
      "description": "The test fails intermittently. The link to the test report can be found below.\r\n\r\n[https://builds.apache.org/job/PreCommit-HDDS-Build/2582/testReport/]\r\n{code:java}\r\njava.net.ConnectException: Call From ea902c1cb730/172.17.0.3 to localhost:10174 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\tat com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\r\n\tat sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\tat com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\r\n\tat com.sun.proxy.$Proxy34.submitRequest(Unknown Source)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.submitRequest(OzoneManagerProtocolClientSideTranslatorPB.java:310)\r\n\tat org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createVolume(OzoneManagerProtocolClientSideTranslatorPB.java:343)\r\n\tat org.apache.hadoop.ozone.client.rpc.RpcClient.createVolume(RpcClient.java:275)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)\r\n\tat com.sun.proxy.$Proxy86.createVolume(Unknown Source)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\r\n\tat com.sun.proxy.$Proxy86.createVolume(Unknown Source)\r\n\tat org.apache.hadoop.ozone.client.ObjectStore.createVolume(ObjectStore.java:100)\r\n\tat org.apache.hadoop.ozone.om.TestOzoneManagerHA.createVolumeTest(TestOzoneManagerHA.java:162)\r\n\tat org.apache.hadoop.ozone.om.TestOzoneManagerHA.testOMProxyProviderFailoverOnConnectionFailure(TestOzoneManagerHA.java:237)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\nCaused by: java.net.ConnectException: Connection refused\r\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\r\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\r\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1403)\r\n\t... 49 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerHA#testOMProxyProviderFailoverOnConnectionFailure fails intermittently"
   },
   {
      "_id": "13224370",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-03-27 17:02:50",
      "description": "The test fails intermittently. The link to the test report can be found below.\r\n\r\nhttps://builds.apache.org/job/PreCommit-HDDS-Build/2582/testReport/\r\n{code:java}\r\njava.lang.AssertionError: Container is not replicated to the destination datanode\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.assertTrue(Assert.java:41)\r\n\tat org.junit.Assert.assertNotNull(Assert.java:621)\r\n\tat org.apache.hadoop.ozone.container.TestContainerReplication.testContainerReplication(TestContainerReplication.java:139)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestContainerReplication#testContainerReplication fails intermittently"
   },
   {
      "_id": "13224156",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-03-26 21:59:38",
      "description": "Recon server should support \"/containers\" API that lists all the containers",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add List Containers API for Recon"
   },
   {
      "_id": "13224110",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-03-26 18:31:32",
      "description": "For bootstrapping and restarting OMs, we need to implement snapshots in OM. The OM state maintained by RocksDB will be checkpoint-ed on demand. Ratis snapshots will\u00a0only\u00a0preserve the last applied log index by the State Machine on disk. This index will be stored in file in the\u00a0OM metadata dir.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement Ratis Snapshots on OM"
   },
   {
      "_id": "13224001",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-03-26 10:23:42",
      "description": "If a pipeline gets destroyed in ozone client, ozone client may hit GroupMismatchException from Ratis. In cases as such, client should exclude the pipeline and retry write to a different block.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Blocker"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Handle GroupMismatchException in OzoneClient"
   },
   {
      "_id": "13223996",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-03-26 10:04:39",
      "description": "In case, a client hits close container exception and asks SCM to allocate block for the next write, SCM might allocate blocks from containers which are almost full and can get closed while the client write is still on. Ozone client by default tries 5 times on hitting an exception to write Data to a different block in case it hits CloseContainerException 5 times, it will give up.\r\n\r\nThe issue was found while testing with HDDS-1295.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Better block allocation policy in SCM"
   },
   {
      "_id": "13223732",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-25 10:18:17",
      "description": "The current ozonefs compatibility layer is broken by: HDDS-1299.\r\n\r\nThe spark jobs (including hadoop 2.7) can't be executed any more:\r\n\r\n{code}\r\n2019-03-25 09:50:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProviderTokenIssuer\r\n        at java.lang.ClassLoader.defineClass1(Native Method)\r\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\r\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\r\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)\r\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:74)\r\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:369)\r\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:363)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:362)\r\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n        at java.lang.Class.forName0(Native Method)\r\n        at java.lang.Class.forName(Class.java:348)\r\n        at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)\r\n        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)\r\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\r\n        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\r\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)\r\n        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\r\n        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n        at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)\r\n        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:757)\r\n        at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:724)\r\n        at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:45)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)\r\n        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)\r\n        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)\r\n        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\r\n        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)\r\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)\r\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer\r\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n        ... 43 more\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneFileSystem can't work with spark/hadoop2.7 because incompatible security classes"
   },
   {
      "_id": "13223462",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-22 23:32:05",
      "description": "[https://builds.apache.org/job/PreCommit-HDDS-Build/2565/testReport/org.apache.hadoop.ozone.scm.node/TestSCMNodeMetrics/testNodeReportProcessingFailure/]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "In DatanodeStateMachine join check for not null"
   },
   {
      "_id": "13223459",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-03-22 23:04:05",
      "description": "* Add a docker compose for Ozone deployment with Recon.\r\n* Test out Recon container key service. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a docker compose for Ozone deployment with Recon."
   },
   {
      "_id": "13223251",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-03-22 05:25:15",
      "description": "TestOzoneManagerHA failed once with the following error:\r\n{code}\r\n[ERROR] Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 105.931 s <<< FAILURE! - in org.apache.hadoop.ozone.om.TestOzoneManagerHA\r\n[ERROR] testOMRetryProxy(org.apache.hadoop.ozone.om.TestOzoneManagerHA)  Time elapsed: 21.781 s  <<< FAILURE!\r\njava.lang.AssertionError: expected:<30> but was:<10>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat org.junit.Assert.assertEquals(Assert.java:555)\r\n\tat org.junit.Assert.assertEquals(Assert.java:542)\r\n\tat org.apache.hadoop.ozone.om.TestOzoneManagerHA.testOMRetryProxy(TestOzoneManagerHA.java:305)\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerHA seems to be flaky"
   },
   {
      "_id": "13223198",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-03-21 21:34:42",
      "description": "I see some odd hugo errors when building Ozone, even though I am not building docs.\r\n{code}\r\n$ mvn -B -q clean compile install -DskipTests=true -Dmaven.javadoc.skip=true -Dmaven.site.skip=true -DskipShade -Phdds\r\n\r\nError: unknown command \"0.4.0-SNAPSHOT\" for \"hugo\"\r\nRun 'hugo --help' for usage.\r\n.../hadoop-hdds/docs/target\r\nError: unknown command \"0.4.0-SNAPSHOT\" for \"hugo\"\r\nRun 'hugo --help' for usage.\r\n.../hadoop-hdds/docs/target\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Hugo errors when building Ozone"
   },
   {
      "_id": "13222923",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-20 19:19:31",
      "description": "Have seen many warnings on DN logs. This ticket is opened to track the investigation and fix for this.\r\n\r\n{code}\r\n\r\n2019-03-20 19:01:33 WARN PropagationRegistry$ExceptionCatchingExtractorDecorator:60 - Error when extracting SpanContext from carrier. Handling gracefully.\r\nio.jaegertracing.internal.exceptions.MalformedTracerStateStringException: String does not match tracer state format: 2c919331-9a51-4bc4-acee-df57a8dcecf0\r\n at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:42)\r\n at org.apache.hadoop.hdds.tracing.StringCodec.extract(StringCodec.java:32)\r\n at io.jaegertracing.internal.PropagationRegistry$ExceptionCatchingExtractorDecorator.extract(PropagationRegistry.java:57)\r\n at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:208)\r\n at io.jaegertracing.internal.JaegerTracer.extract(JaegerTracer.java:61)\r\n at io.opentracing.util.GlobalTracer.extract(GlobalTracer.java:143)\r\n at org.apache.hadoop.hdds.tracing.TracingUtil.importAndCreateScope(TracingUtil.java:96)\r\n at org.apache.hadoop.ozone.container.common.impl.HddsDispatcher.dispatch(HddsDispatcher.java:148)\r\n at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:73)\r\n at org.apache.hadoop.ozone.container.common.transport.server.GrpcXceiverService$1.onNext(GrpcXceiverService.java:61)\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$StreamingServerCallHandler$StreamingServerCallListener.onMessage(ServerCalls.java:248)\r\n at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)\r\n at org.apache.ratis.thirdparty.io.grpc.Contexts$ContextualizedServerCallListener.onMessage(Contexts.java:76)\r\n at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onMessage(ForwardingServerCallListener.java:33)\r\n at org.apache.hadoop.hdds.tracing.GrpcServerInterceptor$1.onMessage(GrpcServerInterceptor.java:46)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.messagesAvailable(ServerCallImpl.java:263)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1MessagesAvailable.runInContext(ServerImpl.java:686)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\r\n at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix MalformedTracerStateStringException on DN logs"
   },
   {
      "_id": "13222921",
      "assignee": "shashikant",
      "components": [],
      "created": "2019-03-20 19:06:18",
      "description": "Repro steps:\r\n\r\n{code}\u00a0\r\n\r\njar $HADOOP_MAPRED_HOME/hadoop-mapreduce-examples-*.jar randomwriter -Dtest.randomwrite.total_bytes=10000000\u00a0 o3fs://bucket1.vol1/randomwrite.out\r\n\r\n{code}\r\n\r\n\u00a0\r\n\r\nError Stack:\r\n\r\n{code}\r\n\r\n2019-03-20 19:02:37 INFO Job:1686 - Task Id : attempt_1553108378906_0002_m_000000_0, Status : FAILED\r\nError: java.lang.ArrayIndexOutOfBoundsException: -5\r\n at java.util.ArrayList.elementData(ArrayList.java:422)\r\n at java.util.ArrayList.get(ArrayList.java:435)\r\n at org.apache.hadoop.hdds.scm.storage.BufferPool.getBuffer(BufferPool.java:45)\r\n at org.apache.hadoop.hdds.scm.storage.BufferPool.allocateBufferIfNeeded(BufferPool.java:59)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:215)\r\n at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:130)\r\n at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:311)\r\n at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:273)\r\n at org.apache.hadoop.fs.ozone.OzoneFSOutputStream.write(OzoneFSOutputStream.java:46)\r\n at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)\r\n at java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1444)\r\n at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:83)\r\n at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:670)\r\n at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\r\n at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)\r\n at org.apache.hadoop.examples.RandomWriter$RandomMapper.map(RandomWriter.java:199)\r\n at org.apache.hadoop.examples.RandomWriter$RandomMapper.map(RandomWriter.java:165)\r\n at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:799)\r\n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)\r\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "KeyOutputStream#write throws ArrayIndexOutOfBoundsException when running RandomWrite MR examples"
   },
   {
      "_id": "13222490",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-19 05:11:14",
      "description": "HDDS-1250 added a few new files. In few of them, it is missing adding asf license header.\r\n\r\n[https://github.com/apache/hadoop/pull/591]\r\n\r\n\u00a0\r\n\r\nYetus\u00a0has not reported about them. I think Yetus is broken in warning asf license errors.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix asf license errors"
   },
   {
      "_id": "13222489",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-19 04:58:54",
      "description": "[https://ci.anzix.net/job/ozone-nightly/35/testReport/junit/org.apache.hadoop.ozone.om/TestScmChillMode/testChillModeOperations/]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Test ScmChillMode testChillModeOperations failed"
   },
   {
      "_id": "13222357",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-03-18 16:16:40",
      "description": "In HDDS-1263 it is changed to handle the list containers with containerID 1 by changing the actual logic of listContainers\u00a0in ScmContainerManager.java. But now with this change, it is contradicting with the javadoc.\r\n\r\nFrom [~nandakumar131] comments\r\n\r\nhttps://issues.apache.org/jira/browse/HDDS-1263?focusedCommentId=16794865&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16794865\r\n\r\n\u00a0\r\n\r\nI agree this will be the way to fix it.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix SCM CLI does not list container with id 1"
   },
   {
      "_id": "13222242",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-18 06:54:48",
      "description": "This ticket is opened to add TokenIssuer interface support to OzoneFileSystem so that MR and Spark jobs can run with OzoneFileSystem in secure mode.\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Support TokenIssuer interface for running jobs with OzoneFileSystem"
   },
   {
      "_id": "13222221",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-18 05:14:09",
      "description": "[https://ci.anzix.net/job/ozone-nightly/32/checkstyle/moduleName.588460772/fileName.-1184872187/]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix checkstyle issue from Nightly run"
   },
   {
      "_id": "13222136",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-03-17 10:59:36",
      "description": "ExcludeList right now is a per BlockOutPutStream value, this can result in multiple keys created out of the same client to run into same exception",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ExcludeList shoud be a RPC Client config so that multiple streams can avoid the same error."
   },
   {
      "_id": "13222134",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-03-17 10:56:19",
      "description": "ExcludeList#getProtoBuf throws ArrayIndexOutOfBoundsException because getProtoBuf uses parallelStreams\r\n\r\n{code}\r\n2019-03-17 16:24:35,774 INFO  retry.RetryInvocationHandler (RetryInvocationHandler.java:log(411)) - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 3\r\n\tat java.util.ArrayList.add(ArrayList.java:463)\r\n\tat org.apache.hadoop.hdds.protocol.proto.HddsProtos$ExcludeListProto$Builder.addContainerIds(HddsProtos.java:12904)\r\n\tat org.apache.hadoop.hdds.scm.container.common.helpers.ExcludeList.lambda$getProtoBuf$3(ExcludeList.java:89)\r\n\tat java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\r\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\r\n\tat java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)\r\n\tat java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)\r\n\tat java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\r\n\tat java.util.concurrent.ForkJoinPool.helpComplete(ForkJoinPool.java:1870)\r\n\tat java.util.concurrent.ForkJoinPool.externalHelpComplete(ForkJoinPool.java:2467)\r\n\tat java.util.concurrent.ForkJoinTask.externalAwaitDone(ForkJoinTask.java:324)\r\n\tat java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:405)\r\n\tat java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)\r\n\tat java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)\r\n\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)\r\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\n\tat java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n\tat java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)\r\n\tat org.apache.hadoop.hdds.scm.container.common.helpers.ExcludeList.getProtoBuf(ExcludeList.java:89)\r\n\tat org.apache.hadoop.hdds.scm.protocolPB.ScmBlockLocationProtocolClientSideTranslatorPB.allocateBlock(ScmBlockLocationProtocolClientSideTranslatorPB.java:100)\r\n\tat sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.hdds.tracing.TraceAllMethod.invoke(TraceAllMethod.java:66)\r\n\tat com.sun.proxy.$Proxy22.allocateBlock(Unknown Source)\r\n\tat org.apache.hadoop.ozone.om.KeyManagerImpl.allocateBlock(KeyManagerImpl.java:275)\r\n\tat org.apache.hadoop.ozone.om.KeyManagerImpl.allocateBlock(KeyManagerImpl.java:246)\r\n\tat org.apache.hadoop.ozone.om.OzoneManager.allocateBlock(OzoneManager.java:2023)\r\n\tat org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.allocateBlock(OzoneManagerRequestHandler.java:631)\r\n\tat org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:231)\r\n\tat org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131)\r\n\tat org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86)\r\n\tat org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n, while invoking $Proxy28.submitRequest over null(localhost:59024). Trying to failover immediately.\r\n2019-03-17 16:24:35,783 INFO  om.KeyManagerImpl (KeyManagerImpl.java:allocateBlock(271)) - allocate block key:pool-9-thread-7-1581351327 exclude:datanodes:containers:#6#1#9#5pipelines:\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "MiniOzoneChaosCluster"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ExcludeList#getProtoBuf throws ArrayIndexOutOfBoundsException"
   },
   {
      "_id": "13221966",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-15 17:26:04",
      "description": "After HDDS-1138, the OM client will not talk to SCM directly to fetch the pipeline info. Instead the pipeline info is returned as part of the keyLocation cached by OM.\u00a0\r\n\r\n\u00a0\r\n\r\nIn case SCM pipeline is changed such as closed, the client may get invalid pipeline exception. In this case, the client need to getKeyLocation with\u00a0OmKeyArgs#refreshPipeline = true to force OM update its pipeline cache for this key.\u00a0\r\n\r\n\u00a0\r\n\r\nAn optimization could be queue a background task to update all the keyLocations that is affected when OM does a refreshPipeline. (This part can be done in 0.5)\r\n{code:java}\r\noldpipeline->newpipeline{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Set OmKeyArgs#refreshPipeline flag properly to avoid reading from stale pipeline"
   },
   {
      "_id": "13221718",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-03-14 16:36:02",
      "description": "# Destroy and close the pipelines\r\n # Close all the containers on the pipeline.\r\n # trigger for pipeline creation",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement actions need to be taken after chill mode exit wait time"
   },
   {
      "_id": "13221705",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-14 16:02:48",
      "description": "As of now we have a following algorithm to handle node failures:\r\n\r\n1. In case of a missing node the leader of the pipline or the scm can detected the missing heartbeats.\r\n2. SCM will start to close the pipeline (CLOSING state) and try to close the containers with the remaining nodes in the pipeline\r\n3. After 5 minutes the pipeline will be destroyed (CLOSED) and a new pipeline can be created from the healthy nodes (one node can be part only one pipwline in the same time).\r\n\r\nWhile this algorithm can work well with a big cluster it doesn't provide very good usability on small clusters:\r\n\r\nUse case1:\r\n\r\nGiven 3 nodes, in case of a service restart, if the restart takes more than 90s, the pipline will be moved to the CLOSING state. For the next 5 minutes (ozone.scm.pipeline.destroy.timeout) the container will remain in the CLOSING state. As there are no more nodes and we can't assign the same node to two different pipeline, the cluster will be unavailable for 5 minutes.\r\n\r\nUse case2:\r\n\r\nGiven 90 nodes and 30 pipelines where all the pipelines are spread across 3 racks. Let's stop one rack. As all the pipelines are affected, all the pipelines will be moved to the CLOSING state. We have no free nodes, therefore we need to wait for 5 minutes to write any data to the cluster.\r\n\r\nThese problems can be solved in multiple ways:\r\n\r\n1.) Instead of waiting 5 minutes, destroy the pipeline when all the containers are reported to be closed. (Most of the time it's enough, but some container report can be missing)\r\n2.) Support multi-raft and open a pipeline as soon as we have enough nodes (even if the nodes already have a CLOSING pipelines).\r\n\r\nBoth the options require more work on the pipeline management side. For 0.4.0 we can adjust the following parameters to get better user experience:\r\n\r\n{code}\r\n  <property>\r\n    <name>ozone.scm.pipeline.destroy.timeout</name>\r\n    <value>60s</value>\r\n    <tag>OZONE, SCM, PIPELINE</tag>\r\n    <description>\r\n      Once a pipeline is closed, SCM should wait for the above configured time\r\n      before destroying a pipeline.\r\n    </description>\r\n\r\n  <property>\r\n    <name>ozone.scm.stale.node.interval</name>\r\n    <value>90s</value>\r\n    <tag>OZONE, MANAGEMENT</tag>\r\n    <description>\r\n      The interval for stale node flagging. Please\r\n      see ozone.scm.heartbeat.thread.interval before changing this value.\r\n    </description>\r\n  </property>\r\n {code}\r\n\r\nFirst of all, we can be more optimistic and mark node to stale only after 5 mins instead of 90s. 5 mins should be enough most of the time to recover the nodes.\r\n\r\nSecond: we can decrease the time of ozone.scm.pipeline.destroy.timeout. Ideally the close command is sent by the scm to the datanode with a HB. Between two HB we have enough time to close all the containers via ratis. With the next HB, datanode can report the successful datanode. (If the containers can be closed the scm can manage the QUASI_CLOSED containers)\r\n\r\nWe need to wait 29 seconds (worst case) for the next HB, and 29+30 seconds for the confirmation. --> 66 seconds seems to be a safe choice (assuming that 6 seconds is enough to process the report about the successful closing)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Adjust default values of pipline recovery for more resilient service restart"
   },
   {
      "_id": "13221693",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-03-14 14:59:41",
      "description": "S3 gateway has a default web page to display a generic message if you open the endpoint in the browser:\r\n\r\nhttp://localhost:9878/static/\r\n\r\nIt also contains a simple example to use the endpoint:\r\n\r\n{code}\r\nThis is an endpoint of Apache Hadoop Ozone S3 gateway. Use it with any AWS S3 compatible tool with setting this url as an endpoint\r\n\r\nFor example with aws-cli:\r\n\r\naws s3api --endpoint http://localhost:9878/static/ create-bucket --bucket=wordcount\r\n\r\nFor more information, please check the documentation. \r\n{code}\r\n\r\nUnfortunately the endpoint is wrong here, the static should be removed from the url.\r\n\r\nThe trivial fix is to move the ) in the js code>  \r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix the dynamic documentation of basic s3 client usage"
   },
   {
      "_id": "13221592",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-03-14 05:38:51",
      "description": "https://ci.anzix.net/job/ozone-nightly/30/findbugs/",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix the findbug issue caused by HDDS-1163"
   },
   {
      "_id": "13221544",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2019-03-13 23:42:29",
      "description": "{code:java}\r\nhadoop@e72da2270499:~$ ozone sh s3 getsecret {code}\r\n{code:java}\r\n2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862). Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 1 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 2 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 3 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 4 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 5 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 6 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 7 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 8 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 INFO RetryInvocationHandler:411 - com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException at org.apache.hadoop.ozone.om.OzoneManager.getS3Secret(OzoneManager.java:2387) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getS3Secret(OzoneManagerRequestHandler.java:877) at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handle(OzoneManagerRequestHandler.java:308) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequestDirectlyToOM(OzoneManagerProtocolServerSideTranslatorPB.java:131) at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:86) at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682) , while invoking $Proxy14.submitRequest over null(om:9862) after 9 failover attempts. Trying to failover immediately. 2019-03-13 23:40:50 ERROR OMFailoverProxyProvider:235 - Failed to connect to OM. Attempted 10 retries and 10 failovers java.lang.NullPointerException\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "\"ozone sh s3 getsecret\" throws Null Pointer Exception for unsecured clusters"
   },
   {
      "_id": "13221530",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2019-03-13 22:06:30",
      "description": "HDDS-1068 removed RestClient from the TestOzoneShell.java.\r\n\r\nSo now we don't\u00a0need to be parameterized in the test anymore. We can directly test with RpcClient.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Remove Parametrized in TestOzoneShell"
   },
   {
      "_id": "13221527",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334605",
            "id": "12334605",
            "name": "Ozone CLI"
         }
      ],
      "created": "2019-03-13 21:44:12",
      "description": "Steps to reproduce\r\n # Create two containers\u00a0\r\n\r\n{code:java}\r\nozone scmcli create\r\nozone scmcli create{code}\r\n\r\n # Try to list containers\r\n\r\n{code:java}\r\nhadoop@7a73695402ae:~$ ozone scmcli list --start=0\r\n Container ID should be a positive long. 0\r\nhadoop@7a73695402ae:~$ ozone scmcli list --start=1 \r\n{ \r\n\"state\" : \"OPEN\",\r\n\"replicationFactor\" : \"ONE\",\r\n\"replicationType\" : \"STAND_ALONE\",\r\n\"usedBytes\" : 0,\r\n\"numberOfKeys\" : 0,\r\n\"lastUsed\" : 274660388,\r\n\"stateEnterTime\" : 274646481,\r\n\"owner\" : \"OZONE\",\r\n\"containerID\" : 2,\r\n\"deleteTransactionId\" : 0,\r\n\"sequenceId\" : 0,\r\n\"open\" : true \r\n}{code}\r\n\r\nThere is no way to list the container with containerID 1.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "SCM CLI does not list container with id 1"
   },
   {
      "_id": "13221524",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-03-13 21:39:47",
      "description": "In OM HA, currently, when\u00a0openKey is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB and also clientID will be generated by each OM node.\r\n\r\n\u00a0\r\n\r\nThe proposed approach is:\r\n\r\n1.\u00a0In startTransaction, call openKey\u00a0and the response returned should be used to create a new OmRequest\u00a0object and used in setting the transaction context. And also modify the ozoneManager and KeymanagerImpl to handle the code with and without ratis.\r\n\r\n\u00a0\r\n\r\nThis Jira also implements HDDS-1319.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "In OM HA OpenKey call Should happen only leader OM"
   },
   {
      "_id": "13221491",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-03-13 18:27:57",
      "description": "* Create the lifecycle scripts (start/stop) for Recon Server along with Shell interface like the other components.\r\n * Verify configurations are being picked up by Recon Server on startup.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Create Recon Server lifecyle integration with Ozone."
   },
   {
      "_id": "13221419",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-13 14:21:57",
      "description": "hadoop-ozone-filesystem-lib-legacy-0.4.0-SNAPSHOT.jar can't work any more together with older hadoop version, after the change of HDDS-1183.\r\n\r\n{code}\r\n2019-03-13 13:48:51 WARN  FileSystem:3170 - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.ozone.OzoneFileSystem could not be instantiated\r\n2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration\r\n2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration\r\n\tat java.lang.Class.forName0(Native Method)\r\n\tat java.lang.Class.forName(Class.java:348)\r\n\tat org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2332)\r\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2297)\r\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393)\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3208)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3240)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3291)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3259)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:470)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)\r\n\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)\r\n\tat org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:245)\r\n\tat org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:228)\r\n\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103)\r\n\tat org.apache.hadoop.fs.shell.Command.run(Command.java:175)\r\n\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:315)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\r\n\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:378)\r\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\t... 21 more\r\n{code}\r\n\r\nAnd Ozone file system current jar is compatible only with hadoop 3.2 after the latest HA change. It means that ozonefs is broken everywhere where the hadoop version is older than 3.2.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "OzoneFS classpath separation is broken by the token validation"
   },
   {
      "_id": "13221108",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-12 12:37:30",
      "description": "https://ci.anzix.net/job/ozone-nightly/28/checkstyle/",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix checkstyle issue from Nightly run"
   },
   {
      "_id": "13220954",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-03-11 22:23:03",
      "description": "In OM HA, currently when allocateBlock\u00a0is called, in applyTransaction() on all OM nodes, we make a call to SCM and write the allocateBlock information into OM DB. The problem with this is, every OM allocateBlock and appends new BlockInfo into OMKeyInfom and also this a correctness issue. (As all OM's should have the same block information for a key, even though eventually this might be changed during key commit)\r\n\r\n\u00a0\r\n\r\nThe proposed approach is:\r\n\r\n1. Calling SCM for allocation of block will happen outside of ratis, and this block information is passed and\u00a0writing to DB will happen via Ratis.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "In OM HA AllocateBlock call where connecting to SCM from OM should not happen on Ratis"
   },
   {
      "_id": "13220795",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-03-11 09:47:10",
      "description": "\u00a0\r\n\r\nTestSecureOzoneRpcClient fails intermittently with the following exception.\r\n{code:java}\r\njava.io.IOException: Unexpected Storage Container Exception: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:338)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:238)\r\n\tat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:131)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:310)\r\n\tat org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:271)\r\n\tat org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.uploadPart(TestOzoneRpcClientAbstract.java:2188)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.doMultipartUpload(TestOzoneRpcClientAbstract.java:2131)\r\n\tat org.apache.hadoop.ozone.client.rpc.TestOzoneRpcClientAbstract.testMultipartUpload(TestOzoneRpcClientAbstract.java:1721)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\nCaused by: java.util.concurrent.ExecutionException: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.\r\n\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\r\n\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.waitOnFlushFutures(BlockOutputStream.java:543)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.handleFullBuffer(BlockOutputStream.java:333)\r\n\t... 35 more\r\nCaused by: java.util.concurrent.CompletionException: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:417)\r\n\tat java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n\tat java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n\tat java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: Block token verification failed. Fail to find any token (empty or null.\r\n\tat org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:573)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:556)\r\n\tat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:415)\r\n\t... 6 more\r\n{code}\r\ncc : [~ajayydv] [~xyao]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestSecureOzoneRpcClient fails intermittently"
   },
   {
      "_id": "13220793",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-11 09:36:32",
      "description": "ozone-0.4 branch is working, we need to update the trunk version.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Bump trunk ozone version to 0.5.0"
   },
   {
      "_id": "13220753",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-11 06:34:44",
      "description": "This allow running dtutil with integration test and dev test for demo of Ozone security.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ozone delegation token utility subcmd for Ozone CLI"
   },
   {
      "_id": "13220749",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-11 06:18:10",
      "description": "Otherwise, we will\u00a0set incorrect\u00a0the exp date of OM delegation like below:\u00a0\r\n\r\n{code}\r\nozone dtutil print /tmp/om.dt\r\n\u00a0\r\nFile: /tmp/om.dt\r\nToken kind\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Service\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Renewer\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Exp date\u00a0\u00a0\u00a0\u00a0\u00a0URL enc token\r\n--------------------------------------------------------------------------------\r\nOzoneToken\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0om:9862\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0yarn\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0*1/8/70 12:03 PM*\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM delegation expiration time should use Time.now instead of Time.monotonicNow"
   },
   {
      "_id": "13220659",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-10 04:16:31",
      "description": "In S3 for a create bucket request, when bucket already exists it should just return the location.\r\n\r\nThis was broken by HDDS-1068.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "In s3 when bucket already exists, it should just return location "
   },
   {
      "_id": "13220506",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-08 17:27:01",
      "description": "Jenkins has not reported any issues, but found when working on another jira.\r\n\r\nhttps://github.com/apache/hadoop/pull/567",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix check style issues caused by HDDS-1196"
   },
   {
      "_id": "13220245",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-07 15:59:32",
      "description": "The serverPrincipal should be\u00a0OMConfigKeys.OZONE_OM_KERBEROS_PRINCIPAL_KEY instead of\u00a0ScmConfigKeys.HDDS_SCM_KERBEROS_PRINCIPAL_KEY",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix incorrect Ozone ClientProtocol KerberosInfo annotation"
   },
   {
      "_id": "13220243",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-07 15:55:21",
      "description": "This needs to be fixed when Ozone is running inside DN as plugin and DN is running using non-privilege HTTPS port.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "BaseHttpServer NPE is HTTP policy is HTTPS_ONLY"
   },
   {
      "_id": "13220097",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-03-07 05:12:55",
      "description": "* Define the Ozone Recon container DB service definition. \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Recon Container DB service definition"
   },
   {
      "_id": "13220061",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-06 23:55:08",
      "description": "This Jira is to add few of the chill mode metrics:\r\n # NumberofHealthyPipelinesThreshold\r\n # currentHealthyPipelinesCount\r\n # NumberofPipelinesWithAtleastOneReplicaThreshold\r\n # CurrentPipelinesWithAtleastOneReplicaCount\r\n # ChillModeContainerWithOneReplicaReportedCutoff\r\n # CurrentContainerCutoff\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add ChillMode metrics"
   },
   {
      "_id": "13219830",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-03-06 07:53:18",
      "description": "The code here does a buffer copy to to compute checksum. This needs to be avoided.\r\n{code:java}\r\n/**\r\n * Computes checksum for give data.\r\n * @param byteString input data in the form of ByteString.\r\n * @return ChecksumData computed for input data.\r\n */\r\npublic ChecksumData computeChecksum(ByteString byteString)\r\n    throws OzoneChecksumException {\r\n  return computeChecksum(byteString.toByteArray());\r\n}\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pushed-to-craterlake"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Avoid extra buffer copy during checksum computation in write Path"
   },
   {
      "_id": "13219689",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-03-05 22:59:02",
      "description": "hadoop-ozone-filesystem-lib-*.jar is missing in hadoop classpath.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ozone-filesystem jar missing in hadoop classpath"
   },
   {
      "_id": "13219661",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-03-05 21:35:41",
      "description": "**This Jira proposes to add docker-compose\u00a0file to run local pseudo cluster with OM HA (3 OM nodes).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide docker-compose for OM HA"
   },
   {
      "_id": "13219614",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-03-05 17:22:37",
      "description": "In the read path, the validation of the response while reading the data from the datanodes happen in XceiverClientGrpc as well as additional\u00a0\u00a0Checksum verification happens in Ozone client to verify the read chunk response. The aim of this Jira is to modify the function call to take a validator function as a part of reading data so all validation can happen in a single unified place.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Restructure code to validate the response from server in the Read path"
   },
   {
      "_id": "13219584",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-05 15:33:59",
      "description": "In HDDS-447 we removed the support the 'ozone noz' cli tool which was a rocksdb/leveldb to sql exporter.\r\n\r\nBut still we have the unit test for the tool (in fact only the skeleton of the unit test, as the main logic is removed). Even worse this unit test is failing as it calls System.exit:\r\n\r\n{code}\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-tools: There are test failures.\r\n[ERROR] \r\n[ERROR] Please refer to /testptch/hadoop/hadoop-ozone/tools/target/surefire-reports for the individual test results.\r\n[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.\r\n[ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\r\n{code}\r\n\r\nI think this test can be deleted.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove TestContainerSQLCli unit test stub"
   },
   {
      "_id": "13219550",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-05 12:44:36",
      "description": "It's failed multiple times during the CI builds:\r\n\r\n{code}\r\nError Message\r\n\r\nWanted but not invoked:\r\ncloseContainerEventHandler.onMessage(\r\n    #1,\r\n    org.apache.hadoop.hdds.server.events.EventQueue@3d3fcdb0\r\n);\r\n-> at org.apache.hadoop.hdds.scm.container.TestContainerActionsHandler.testCloseContainerAction(TestContainerActionsHandler.java:64)\r\nActually, there were zero interactions with this mock.\r\n{code}\r\n\r\nThe fix is easy: we should call queue.processAll(1000L) to wait for the processing of all the events.\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestContainerActionsHandler.testCloseContainerAction has an intermittent failure"
   },
   {
      "_id": "13219415",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-04 22:29:36",
      "description": "# Make the chillmodeExitRule abstract class and move common logic for all rules into this.\r\n # Update test's for chill mode accordingly.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor ChillMode rules and chillmode manager"
   },
   {
      "_id": "13219376",
      "assignee": "elek",
      "components": [],
      "created": "2019-03-04 19:51:21",
      "description": "{code}\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | Exception in thread \"main\" java.lang.NoClassDefFoundError: javax/activation/DataSource\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl.<clinit>(RuntimeBuiltinLeafInfoImpl.java:457)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.model.impl.RuntimeTypeInfoSetImpl.<init>(RuntimeTypeInfoSetImpl.java:65)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:133)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.createTypeInfoSet(RuntimeModelBuilder.java:85)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.model.impl.ModelBuilder.<init>(ModelBuilder.java:156)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.model.impl.RuntimeModelBuilder.<init>(RuntimeModelBuilder.java:93)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getTypeInfoSet(JAXBContextImpl.java:473)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.runtime.JAXBContextImpl.<init>(JAXBContextImpl.java:319)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.runtime.JAXBContextImpl$JAXBContextBuilder.build(JAXBContextImpl.java:1170)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:145)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.xml.bind.v2.ContextFactory.createContext(ContextFactory.java:236)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:186)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:146)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at javax.xml.bind.ContextFinder.find(ContextFinder.java:350)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:446)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:409)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.wadl.WadlApplicationContextImpl.<init>(WadlApplicationContextImpl.java:103)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.wadl.WadlFactory.init(WadlFactory.java:100)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.application.RootResourceUriRules.initWadl(RootResourceUriRules.java:169)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.application.RootResourceUriRules.<init>(RootResourceUriRules.java:106)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.application.WebApplicationImpl._initiate(WebApplicationImpl.java:1359)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.application.WebApplicationImpl.access$700(WebApplicationImpl.java:180)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:799)\r\n\r\nkms_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at com.sun.jersey.server.impl.application.WebApplicationImpl$13.f(WebApplicationImpl.java:795)\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Change hadoop-runner and apache/hadoop base image to use Java8"
   },
   {
      "_id": "13219300",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-03-04 14:23:10",
      "description": "HDDS-1150 introduced distributed for ozone components. But we have no trace context propagation between the clients and Ozone Datanodes.\r\n\r\nAs we use Grpc and Ratis on this RPC path the full tracing could be quite complex: we should propagate the trace id in Ratis and include it in all the log entries.\r\n\r\nI propose a simplified solution here: to trace only the StateMachine operations.\r\n\r\nAs Ratis is a library we provide the implementation of the appropriate Raft elements especially the StateMachine and the raft messages. We can add the tracing information to the raft messages (in fact, we already have this field) and we can restore the tracing context during the StateMachine operations.\r\n\r\nThis approach is very simple (only a few lines of codes) and can show the time of the real write/read operations, but can't see the internals of the Ratis operations.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable tracing for the datanode read/write path"
   },
   {
      "_id": "13219278",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2019-03-04 13:25:21",
      "description": "S3 Multi-Part-Upload (MPU) is implemented recently in the Ozone s3 gateway. We have extensive testing with using 'aws s3api' application which is passed.\r\n\r\nBut it turned out that the more simple `aws s3 cp` command fails with _405 Media type not supported error_ message\r\n\r\nThe root cause of this issue is the JAXRS implementation of the multipart upload method:\r\n\r\n{code}\r\n  @POST\r\n  @Produces(MediaType.APPLICATION_XML)\r\n  public Response multipartUpload(\r\n      @PathParam(\"bucket\") String bucket,\r\n      @PathParam(\"path\") String key,\r\n      @QueryParam(\"uploads\") String uploads,\r\n      @QueryParam(\"uploadId\") @DefaultValue(\"\") String uploadID,\r\n      CompleteMultipartUploadRequest request) throws IOException, OS3Exception {\r\n    if (!uploadID.equals(\"\")) {\r\n      //Complete Multipart upload request.\r\n      return completeMultipartUpload(bucket, key, uploadID, request);\r\n    } else {\r\n      // Initiate Multipart upload request.\r\n      return initiateMultipartUpload(bucket, key);\r\n    }\r\n  }\r\n{code}\r\n\r\nHere we have a CompleteMultipartUploadRequest parameter which is created by the JAXRS framework based on the media type and the request body. With _Content-Type: application/xml_ it's easy: the JAXRS framework uses the built-in JAXB serialization. But with plain/text content-type it's not possible as there is no serialization support for CompleteMultipartUploadRequest from plain/text.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Support plain text S3 MPU initialization request"
   },
   {
      "_id": "13218984",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-03-01 19:12:07",
      "description": "java.lang.Thread.State: TIMED_WAITING at sun.misc.Unsafe.park(Native Method) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460) at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362) at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1073) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) at org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:389) at org.apache.hadoop.ozone.om.TestScmChillMode.testSCMChillMode(TestScmChillMode.java:286) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available",
         "pushed-to-craterlake"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Test SCMChillMode failing randomly in Jenkins run"
   },
   {
      "_id": "13218850",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-03-01 09:41:22",
      "description": "steps taken :\r\n\r\n--------------------\r\n # create 40 datanode cluster.\r\n # one of the datanodes has less than 5 GB space.\r\n # Started writing key of size 600MB.\r\n\r\noperation failed:\r\n\r\nError on the client:\r\n\r\n----------------------------\r\n{noformat}\r\nFri Mar 1 09:05:28 UTC 2019 Ruuning /root/hadoop_trunk/ozone-0.4.0-SNAPSHOT/bin/ozone sh key put testvol172275910-1551431122-1/testbuck172275910-1551431122-1/test_file24 /root/test_files/test_file24\r\noriginal md5sum a6de00c9284708585f5a99b0490b0b23\r\n2019-03-01 09:05:39,142 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:39,578 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,368 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,450 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,457 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 does not exist\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:393)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,535 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,617 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,741 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,814 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 creation failed\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$writeChunkToContainer$5(BlockOutputStream.java:613)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-03-01 09:05:40,815 ERROR storage.BlockOutputStream: Unexpected Storage Container Exception:\r\norg.apache.hadoop.hdds.scm.container.common.helpers.StorageContainerException: ContainerID 79 does not exist\r\n at org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.validateContainerResponse(ContainerProtocolCalls.java:568)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.validateResponse(BlockOutputStream.java:535)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.lambda$handlePartialFlush$2(BlockOutputStream.java:393)\r\n at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:602)\r\n at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)\r\n at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\njava.nio.BufferOverflowException\r\n at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:189)\r\n at org.apache.hadoop.hdds.scm.storage.BlockOutputStream.write(BlockOutputStream.java:213)\r\n at org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.write(BlockOutputStreamEntry.java:128)\r\n at org.apache.hadoop.ozone.client.io.KeyOutputStream.handleWrite(KeyOutputStream.java:307)\r\n at org.apache.hadoop.ozone.client.io.KeyOutputStream.write(KeyOutputStream.java:268)\r\n at org.apache.hadoop.ozone.client.io.OzoneOutputStream.write(OzoneOutputStream.java:49)\r\n at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)\r\n at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:111)\r\n at org.apache.hadoop.ozone.web.ozShell.keys.PutKeyHandler.call(PutKeyHandler.java:53)\r\n at picocli.CommandLine.execute(CommandLine.java:919)\r\n at picocli.CommandLine.access$700(CommandLine.java:104)\r\n at picocli.CommandLine$RunLast.handle(CommandLine.java:1083)\r\n at picocli.CommandLine$RunLast.handle(CommandLine.java:1051)\r\n at picocli.CommandLine$AbstractParseResultHandler.handleParseResult(CommandLine.java:959)\r\n at picocli.CommandLine.parseWithHandlers(CommandLine.java:1242)\r\n at picocli.CommandLine.parseWithHandler(CommandLine.java:1181)\r\n at org.apache.hadoop.hdds.cli.GenericCli.execute(GenericCli.java:61)\r\n at org.apache.hadoop.ozone.web.ozShell.Shell.execute(Shell.java:84)\r\n at org.apache.hadoop.hdds.cli.GenericCli.run(GenericCli.java:52)\r\n at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:95){noformat}\r\n\u00a0\r\n\r\nozone.log\r\n\r\n-----------------\r\n\r\n\u00a0\r\n{noformat}\r\n2019-03-01 09:05:33,248 [IPC Server handler 17 on 9889] DEBUG (OzoneManagerRequestHandler.java:137) - Received OMRequest: cmdType: CreateKey\r\ntraceID: \"5f169cde0a4c8a4e:79f0b64c3329c0ba:5f169cde0a4c8a4e:0\"\r\nclientId: \"client-86810A76C95E\"\r\ncreateKeyRequest {\r\n keyArgs {\r\n volumeName: \"testvol172275910-1551431122-1\"\r\n bucketName: \"testbuck172275910-1551431122-1\"\r\n keyName: \"test_file24\"\r\n dataSize: 629145600\r\n type: RATIS\r\n factor: THREE\r\n isMultipartKey: false\r\n }\r\n}\r\n,\r\n2019-03-01 09:05:33,255 [IPC Server handler 17 on 9889] DEBUG (KeyManagerImpl.java:465) - Key test_file24 allocated in volume testvol172275910-1551431122-1 bucket testbuck172275910-1551431122-1\r\n2019-03-01 09:05:38,229 [IPC Server handler 8 on 9889] DEBUG (OzoneManagerRequestHandler.java:137) - Received OMRequest: cmdType: AllocateBlock\r\ntraceID: \"5f169cde0a4c8a4e:fe6c4bdb75978062:5f169cde0a4c8a4e:0\"\r\nclientId: \"client-86810A76C95E\"\r\nallocateBlockRequest {\r\n keyArgs {\r\n volumeName: \"testvol172275910-1551431122-1\"\r\n bucketName: \"testbuck172275910-1551431122-1\"\r\n keyName: \"test_file24\"\r\n dataSize: 629145600\r\n }\r\n clientID: 20622763490697872\r\n}\r\n,\r\n2019-03-01 09:05:38,739 [grpc-default-executor-17] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE\r\n2019-03-01 09:05:38,790 [grpc-default-executor-17] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE\r\n2019-03-01 09:05:38,800 [grpc-default-executor-17] DEBUG (ContainerStateMachine.java:358) - writeChunk writeStateMachineData : blockId containerID: 79\r\nlocalID: 101674591075108132\r\nblockCommitSequenceId: 0\r\n logIndex 3 chunkName f6508b585fbd0b834b2139939467ac03_stream_8101b9db-a724-4690-abe1-c7daa2630326_chunk_1\r\n2019-03-01 09:05:38,801 [grpc-default-executor-17] DEBUG (ContainerStateMachine.java:365) - writeChunk writeStateMachineData completed: blockId containerID: 79\r\nlocalID: 101674591075108132\r\nblockCommitSequenceId: 0\r\n logIndex 3 chunkName f6508b585fbd0b834b2139939467ac03_stream_8101b9db-a724-4690-abe1-c7daa2630326_chunk_1\r\n2019-03-01 09:05:38,978 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:0 detected, adding dummy entries\r\n2019-03-01 09:05:38,979 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:1 detected, adding dummy entries\r\n2019-03-01 09:05:38,980 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerStateMachine.java:573) - Gap in indexes at:2 detected, adding dummy entries\r\n2019-03-01 09:05:38,981 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE\r\n2019-03-01 09:05:38,981 [StateMachineUpdater-89770995-a80c-451d-a875-a0d384c2067f] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:f340a80dafdf68eb:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE\r\n2019-03-01 09:05:39,357 [grpc-default-executor-18] INFO (ContainerUtils.java:149) - Operation: CreateContainer : Trace ID: 5f169cde0a4c8a4e:896673a239485fcd:5f169cde0a4c8a4e:0 : Message: Container creation failed, due to disk out of space : Result: DISK_OUT_OF_SPACE\r\n2019-03-01 09:05:39,358 [grpc-default-executor-18] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 5f169cde0a4c8a4e:896673a239485fcd:5f169cde0a4c8a4e:0 : Message: ContainerID 79 creation failed : Result: DISK_OUT_OF_SPACE\r\n\u00a0\r\n{noformat}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle Datanode volume out of space"
   },
   {
      "_id": "13218808",
      "assignee": "xyao",
      "components": [],
      "created": "2019-03-01 05:12:30",
      "description": "The ozonesecure docker-compose has been changed to use hadoop-runner image based on Java 11. Several packages/classes have been removed from Java 8 such as\u00a0\r\n\r\njavax.xml.bind.DatatypeConverter.parseHexBinary\r\n\r\n\u00a0This ticket is opened to fix issues running ozonesecure docker-compose on java 11.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix ClassNotFound issue with javax.xml.bind.DatatypeConverter used by DefaultProfile"
   },
   {
      "_id": "13218790",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-03-01 01:23:38",
      "description": "In the current HealthyPipelineRule, we considered only pipeline type ratis and replication factor 3 pipelines for 10%.\r\n\r\n\u00a0\r\n\r\nThis Jira is to consider all the pipelines with all replication factor for the 10% threshold. (Means each pipeline-type with factor should meet 10%)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "In Healthy Pipeline rule consider pipelines with all replicationType and replicationFactor"
   },
   {
      "_id": "13218769",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-28 23:25:00",
      "description": "Condition for to Start Replication monitor thread.\r\n # Exit Chill mode\r\n # Time out (configurable value) default to 5 minutes. Additional time out is added to give some additional time for datanodes to report and make pipelines healthy.\r\n\r\nSo, once we are out of chillmode, we fire ChillModeStatus, this ReplicationTimer Class will listen to that event, and wait for a configurable time, and then emit replicationEnabled.\r\n\r\n\u00a0\r\n\r\nThe current code, when we are out of chill mode, we set replication enabled.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a ChillMode handler class "
   },
   {
      "_id": "13218720",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-28 18:50:50",
      "description": "The main intention of this Jira is to have all rules look in a similar way of handling events.\r\n\r\nIn this Jira, did the following changes:\r\n # Both DatanodeRule and ContainerRule\u00a0implements EventHandler and listen for NodeRegistrationContainerReport\r\n # Update ScmChillModeManager not to handle any events. (As each rule need to handle an event, and work on that rule)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Refactor ContainerChillModeRule and DatanodeChillMode rule"
   },
   {
      "_id": "13218566",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-02-28 08:28:56",
      "description": "As it's discussed in the the parent jira the rest support for Ozone Client protocol can be removed to use S3 Rest API instead of that.\r\n\r\nSome of the unit tests are already disabled, so it seems to be better to remove it from the documentation (and from the smoketests).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Replace Ozone Rest client with S3 client in smoketests and docs"
   },
   {
      "_id": "13218453",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-27 21:29:09",
      "description": "Few offline comments from [~nandakumar131]\r\n # We should not process pipeline report from datanode\u00a0again during calculations.\r\n # We should consider only replication factor 3 ratis\u00a0pipelines.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Healthy pipeline Chill Mode rule to consider only pipelines with replication factor three"
   },
   {
      "_id": "13218270",
      "assignee": "xyao",
      "components": [],
      "created": "2019-02-27 06:59:55",
      "description": "This includes addDelegationToken/renewDelegationToken/cancelDelegationToken so that MR jobs can collect tokens correctly upon job submission time.\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support getDelegationToken API for OzoneFileSystem"
   },
   {
      "_id": "13218198",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-26 23:44:49",
      "description": "h2. Pipeline Rule with configurable percentage of pipelines with at least one datanode reported:\r\n\r\nIn this rule we consider when at least \u00a090% of pipelines have at least one datanode reported. \r\n\r\n\u00a0\r\n\r\nThis rule satisfies, when we exit chill mode, Ozone clients will have at least one open replica for reads to succeed. (We can increase this threshold default from 90%, if we want to see fewer failures during reads after exit chill mode.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Pipeline Rule where atleast one datanode is reported in the pipeline"
   },
   {
      "_id": "13217960",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-26 01:14:14",
      "description": "This Jira is to implement one of the chill mode rule.\r\n\r\n*Pipeline Rule with configurable percentage of open pipelines with all datanodes reported*: In this rule, we consider when at least 10% of the pipelines have all 3 data node's reported (if it is a 3 node ratis ring), and one node reported if it is a one node ratis ring. (The percentage is configurable via ozone configuration parameter).\r\n\r\nThis rule satisfies, once the SCM is restarted, and after exiting chill mode, we have enough pipelines to give for clients for writes to succeed.\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Healthy pipeline Chill Mode Rule"
   },
   {
      "_id": "13217900",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-02-25 19:45:08",
      "description": "We can directly server read requests from the OM's RocksDB instead of going through the Ratis server. OM should first check its role and only if it is the leader can it server read requests.\u00a0\r\n\r\nThere can be a scenario where an OM can lose its Leader status but not know about the new election in the ring. This OM could server stale reads for the duration of the heartbeat timeout but this should be acceptable (similar to how Standby Namenode could possibly server stale reads till it figures out the new status).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Serve read requests directly from RocksDB"
   },
   {
      "_id": "13217188",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-02-21 12:42:03",
      "description": "As I wrote in the description of the parent Jira I propose to disable (@Ignore) the unit tests which are failing while we are fixing them to get clean Jira response from the PreCommit builds.\r\n\r\nAll the tests are tracked in a separated jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable failing test which are tracked by a separated jira"
   },
   {
      "_id": "13217185",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2019-02-21 12:32:24",
      "description": "h3. Error Message\r\n{code:java}\r\norg.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-4D77D2A8F653->omNode-3@group-523986131536, cid=9, seq=0 RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$373/2067504307@6afa0221 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms){code}\r\nStacktrace\r\n{code:java}\r\nINTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: org.apache.ratis.protocol.RaftRetryFailureException: Failed RaftClientRequest:client-4D77D2A8F653->omNode-3@group-523986131536, cid=9, seq=0 RW, org.apache.hadoop.ozone.om.ratis.OzoneManagerRatisClient$$Lambda$373/2067504307@6afa0221 for 10 attempts with RetryLimited(maxAttempts=10, sleepTime=100ms) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:586) at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.createVolume(OzoneManagerProtocolClientSideTranslatorPB.java:230) at org.apache.hadoop.ozone.web.storage.DistributedStorageHandler.createVolume(DistributedStorageHandler.java:179) at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testCreateVolume(TestOzoneManagerHA.java:153) at org.apache.hadoop.ozone.om.TestOzoneManagerHA.testTwoOMNodesDown(TestOzoneManagerHA.java:138) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168) at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestOzoneManagerHA.testTwoOMNodesDown is failing with ratis error"
   },
   {
      "_id": "13217143",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-21 09:28:20",
      "description": "The best way to display the network performance in the tracing server is to start a new span (=save tracing information) on both of the client and and server side.\r\n\r\nThis issue is about improving the client side tracing of OzoneManager and StorageLocationManager.\r\n\r\nThe easiest way to turn on tracing for a class which implements an interface is to create a java proxy with TracingUtil.createProxy. With this utility we can execute the required tracing methods (create span/close span) around the original methods without adding any boilerplate code.\r\n\r\nThanks to the current design hadoop rpc calls are create by the *ClientSideTranslatorPB classes which implements the original protocol interface (eg. StorageContainerLocationProtocolClientSideTranslatorPB implements StorageContainerLocationProtocol) which means that it can easily instrumented by TracingUtil.createProxy.\r\n\r\nThe only thing what we need is to use the interface everywhere (StorageContainerLocationProtocol) instead of the implementation (ClientSideTranslator) as a client.\r\n\r\nThe only required method which is not published in the ClientSideTranslator is the close method. With adding the close method to the interface (extends Closable) we are able to use the interface everywhere which can be instrumented to send the tracing information.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add tracing to the client side of StorageContainerLocationProtocol and OzoneManagerProtocol"
   },
   {
      "_id": "13217127",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-21 08:32:18",
      "description": "The XCeiverClients can be traced on the client side to get some information about the time of chunk writes / block puts.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add trace information for the client side of the datanode writes"
   },
   {
      "_id": "13217126",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-21 08:29:38",
      "description": "The tracing propagation is not yet enabled for the ScmBlockLocationProtocol. We can't see the internal calls between OM and SCM. We need to propagate it at least for the allocateBlock call.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Propagate the tracing id in ScmBlockLocationProtocol"
   },
   {
      "_id": "13217050",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-21 01:04:01",
      "description": "If we don't\u00a0do that, we get an error when handling container report for open containers.\r\n\r\nAs they don't exist in container DB.\r\n\r\n\u00a0\r\n{code:java}\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.lang.Thread.run(Thread.java:748)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 2019-02-21 00:00:32 ERROR ContainerReportHandler:173 - Received container report for an unknown container 1 from datanode e2733c00-162b-4993-a986-f6104f5008d8{ip: 172.18.0.2, host: 4f4e683d86c3} {}\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | org.apache.hadoop.hdds.scm.container.ContainerNotFoundException: #1\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.checkIfContainerExist(ContainerStateMap.java:543)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.states.ContainerStateMap.updateContainerReplica(ContainerStateMap.java:230)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerStateManager.updateContainerReplica(ContainerStateManager.java:565)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.SCMContainerManager.updateContainerReplica(SCMContainerManager.java:393)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ReportHandlerHelper.processContainerReplica(ReportHandlerHelper.java:74)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.processContainerReplicas(ContainerReportHandler.java:159)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:110)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.scm.container.ContainerReportHandler.onMessage(ContainerReportHandler.java:51)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nscm_1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n{code}\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "After allocating container, we are not adding to container DB."
   },
   {
      "_id": "13217011",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-20 19:25:12",
      "description": "This jira aims to add more container related metrics to SCM.\r\n Following metrics will be added as part of this jira:\r\n * Number of containers\r\n * Number of open containers\r\n * Number of closed containers\r\n * Number of quasi closed containers\r\n * Number of closing containers\r\n\r\nAbove are already handled in HDDS-918.\r\n * Number of successful create container calls\r\n * Number of failed create container calls\r\n * Number of successful delete container calls\r\n * Number of failed delete container calls\r\n\r\nHandled in HDDS-2193.\r\n * Number of successful container report processing\r\n * Number of failed container report processing\r\n * Number of successful incremental container report processing\r\n * Number of failed incremental container report processing\r\n\r\nThese will be handled in this jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Adding container related metrics in SCM"
   },
   {
      "_id": "13216977",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333912",
            "id": "12333912",
            "name": "Tools",
            "description": "ozone CLI, SCM CLI, Genesis, Freon etc."
         }
      ],
      "created": "2019-02-20 16:49:29",
      "description": "Recently we improved the default HttpServer to support prometheus monitoring and java profiling.\r\n\r\nIt would be very useful to enable the same options for freon testing:\r\n\r\n\u00a01. We need a simple way to profile freon and check the problems\r\n\r\n\u00a02. Long running freons should be monitored\r\n\r\nWe can create a new optional FreonHttpServer which includes all the required servlets by default.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add optional web server to the Ozone freon test tool"
   },
   {
      "_id": "13216932",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-02-20 13:57:07",
      "description": "Currently, when we write StateMachineData, we first write to cache followed by write to disk. The entry in the cache can get evicted while the actual write is happening in case write is very slow. The purpose of this Jira is to ensure the cache eviction only after writeChunk completes.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ensure stateMachineData to be evicted only after writeStateMachineData completes in ContainerStateMachine cache"
   },
   {
      "_id": "13216742",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-02-19 20:19:05",
      "description": "Instead of DBCheckpointSnapshot to DBCheckpoint, as internally we used RocksDBCheckpoint. It is little confusing to have both checkpoint and snapshot. And update similarly in other places.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Update DBCheckpointSnapshot to DBCheckpoint"
   },
   {
      "_id": "13216737",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-02-19 20:07:09",
      "description": "The list of issues can be found here - https://ci.anzix.net/job/ozone/106/findbugs/.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix findbugs issues caused by HDDS-1085."
   },
   {
      "_id": "13216728",
      "assignee": "xyao",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-02-19 19:13:19",
      "description": "Currently, while a block is allocated from OM, the request is forwarded to SCM. However, even though the pipeline information is present with the OM for block allocation, this information is passed through to the client.\r\n\r\nThis optimization will help in reducing the number of hops for the client by reducing 1 RPC round trip for each block allocated.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager should return the pipeline info of the allocated block along with block info"
   },
   {
      "_id": "13216724",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2019-02-19 19:09:13",
      "description": "As per the discussion with [~anu] on HDDS-1085, this JIRA tracks the effort to add metric counters to capture ROcksDB checkpointing performance. \r\n\r\nFrom [~anu]'s comments, it might be interesting to have 3 counters \u2013 or a map of counters.\r\n* How much time are we taking for each CheckPoint\r\n* How much time are we taking for each Tar operation \u2013 along with sizes\r\n* How much time are we taking for the transfer.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add metric counters to capture the RocksDB checkpointing statistics."
   },
   {
      "_id": "13216571",
      "assignee": "xyao",
      "components": [],
      "created": "2019-02-19 09:39:11",
      "description": "HDDS-1041 implemented TDE for Ozone and added the KMS server to the compose/ozonesecure cluster definition.\r\n\r\nWe need a simple robot framework based test to try out TDE from command line.\r\n\r\nThis task requires a working ozonesecure docker-compose cluster first.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "beta1",
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create robot test for Ozone TDE support"
   },
   {
      "_id": "13216440",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-18 17:39:16",
      "description": "HDDS-1114 fixed all the findbug/checkstyle problems but in the mean time new patches are committed with newer error.\r\n\r\nHere I would like to cleanup the projects again.\r\n\r\n(Except the static field in RatisPipelineProvider which will be ignored in this patch and tracked in HDDS-1128)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix findbug/checkstyle errors in hadoop-hdds projects"
   },
   {
      "_id": "13216066",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2019-02-15 19:57:20",
      "description": "Currently, if the checksum is computed during data write and persisted in the disk, we will always end up verifying it while reading. This Jira aims to selectively disable checksum verification during reads even though checksum info is present in the data stored.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add a config to disable checksum verification during read even though checksum data is present in the persisted data"
   },
   {
      "_id": "13216050",
      "assignee": "xyao",
      "components": [],
      "created": "2019-02-15 18:40:00",
      "description": "This is needed when the OM received\u00a0delegation token signed by other OM instances and it does not have the certificate for foreign OM.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM get the certificate from SCM CA for token validation"
   },
   {
      "_id": "13216000",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334817",
            "id": "12334817",
            "name": "docker"
         }
      ],
      "created": "2019-02-15 14:45:40",
      "description": "HDDS-1116 provides a simple servlet to execute async profiler (https://github.com/jvm-profiling-tools/async-profiler) thanks to the Hive developers.\r\n\r\nTo run it in the docker-composed based example environments we should add it to the apache/hadoop-runner base image. \r\n\r\nNote: The size is not significant, the downloadable package is 102k.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add async profiler to the hadoop-runner base container image"
   },
   {
      "_id": "13215998",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-15 14:40:03",
      "description": "Thanks to [~gopalv] we learned that [~prasanth_j] implemented a helper servlet in Hive to initialize new [async profiler|https://github.com/jvm-profiling-tools/async-profiler] sessions and provide the svg based flame graph over HTTP. (see HIVE-20202)\r\n\r\nIt seems to very useful as with this approach the profiling could be very easy.\r\n\r\nThis patch imports the servlet from the Hive code base to the Ozone code base with minor modification (to make it work with our servlet containers)\r\n\r\n * The two servlets are unified to one\r\n * Streaming the svg to the browser based on IOUtils.copy \r\n * Output message is improved\r\n\r\nBy default the profile servlet is turned off, but you can enable it with 'hdds.profiler.endpoint.enabled=true' ozone-site.xml settings. In that case you can access the /prof endpoint from scm,om,s3g. \r\n\r\nYou should upload the async profiler first (https://github.com/jvm-profiling-tools/async-profiler) and set the ASYNC_PROFILER_HOME environment variable to find it. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add java profiler servlet to the Ozone web servers"
   },
   {
      "_id": "13215974",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-02-15 12:35:25",
      "description": "Ozone build process doesn't require the pom.xml in the top level hadoop directory as we use hadoop 3.2 artifacts as parents of hadoop-ozone and hadoop-hdds. The ./pom.xml is used only to include the hadoop-ozone/hadoop-hdds projects in the maven reactor.\r\n\r\nFrom command line, it's easy to build only the ozone artifacts:\r\n\r\n{code}\r\nmvn clean install -Phdds  -am -pl :hadoop-ozone-dist  -Danimal.sniffer.skip=true  -Denforcer.skip=true\r\n{code}\r\n\r\nWhere: '-pl' defines the build of the hadoop-ozone-dist project\r\nand '-am' defines to build all of the dependencies from the source tree (hadoop-ozone-common, hadoop-hdds-common, etc.)\r\n\r\nBut this filtering is available only from the command line.\r\n\r\nWith providing a lightweight pom.ozone.xml we can achieve the same:\r\n\r\n * We can open only hdds/ozone projects in the IDE/intellij. It makes the development faster as IDE doesn't need to reindex all the sources all the time + it's easy to execute checkstyle/findbugs plugins of the intellij to the whole project.\r\n * Longer term we should create an ozone specific source artifact (currently the source artifact for hadoop and ozone releases are the same) which also requires a simplified pom.\r\n\r\nIn this patch I also added the .mvn directory to the .gitignore file.\r\n\r\nWith \r\n{code}\r\nmkdir -p .mvn && echo \"-f ozone.pom.xml\" > .mvn/maven.config\" you can persist the usage of the ozone.pom.xml for all the subsequent builds (in the same dir)\r\n\r\nHow to test?\r\n\r\nJust do a 'mvn -f ozonze.pom.xml clean install -DskipTests'",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide ozone specific top-level pom.xml"
   },
   {
      "_id": "13215973",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-15 12:25:07",
      "description": "Unfortunately as the previous two big commits (error handling HDDS-1068, checkstyle HDDS-1103) are committed in the same time a few new errors are introduced during the rebase.\r\n\r\nThis patch will fix the remaining 5 issues (+ a type in the acceptance test executor) ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix findbugs/checkstyle/accepteance errors in Ozone"
   },
   {
      "_id": "13215917",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335013",
            "id": "12335013",
            "name": "build"
         }
      ],
      "created": "2019-02-15 08:57:06",
      "description": "There are two ways to define common dependencies with maven:\r\n\r\n  1.) put all the dependencies to the parent project and inherit them\r\n  2.) get all the dependencies via transitive dependencies\r\n\r\nTLDR; I would like to switch from 1 to 2 in hadoop-ozone\r\n\r\nMy main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.\r\n\r\nUsing transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove default dependencies from hadoop-ozone project"
   },
   {
      "_id": "13215851",
      "assignee": "xyao",
      "components": [],
      "created": "2019-02-14 23:26:15",
      "description": "{code}\r\n\r\nozoneManager_1\u00a0 | 2019-02-14 23:21:51 ERROR OzoneManager:596 - Unable to read key pair for OM.\r\n\r\nozoneManager_1\u00a0 | org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:638)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:594)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManagerIfNecessary(OzoneManager.java:1216)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.start(OzoneManager.java:1007)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:768)\r\n\r\nozoneManager_1\u00a0 | Caused by: java.lang.NullPointerException\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:635)\r\n\r\nozoneManager_1\u00a0 | ... 4 more\r\n\r\nozoneManager_1\u00a0 | 2019-02-14 23:21:51 ERROR OzoneManager:772 - Failed to start the OzoneManager.\r\n\r\nozoneManager_1\u00a0 | java.lang.RuntimeException: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:597)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManagerIfNecessary(OzoneManager.java:1216)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.start(OzoneManager.java:1007)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.main(OzoneManager.java:768)\r\n\r\nozoneManager_1\u00a0 | Caused by: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:638)\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.startSecretManager(OzoneManager.java:594)\r\n\r\nozoneManager_1\u00a0 | ... 3 more\r\n\r\nozoneManager_1\u00a0 | Caused by: java.lang.NullPointerException\r\n\r\nozoneManager_1\u00a0 | at org.apache.hadoop.ozone.om.OzoneManager.readKeyPair(OzoneManager.java:635)\r\n\r\nozoneManager_1\u00a0 | ... 4 more\r\n\r\nozoneManager_1\u00a0 | 2019-02-14 23:21:51 INFO\u00a0 ExitUtil:210 - Exiting with status 1: java.lang.RuntimeException: org.apache.hadoop.ozone.security.OzoneSecurityException: Error reading private file for OzoneManager\r\n\r\nozoneManager_1\u00a0 | 2019-02-14 23:21:51 INFO\u00a0 OzoneManager:51 - SHUTDOWN_MSG: \r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "test-badlands"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager NPE reading private key file."
   },
   {
      "_id": "13215751",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-02-14 14:21:35",
      "description": "*Some context*\r\n\r\nThe FSCK server will periodically invoke this OM API passing in the most recent sequence number of its own RocksDB instance. The OM will use the RockDB getUpdateSince() API to answer this query. Since the getUpdateSince API only works against the RocksDB WAL, we have to configure OM RocksDB WAL (https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log) with sufficient max size to make this API useful. If the OM cannot get all transactions since the given sequence number (due to WAL flushing), it can error out. In that case the FSCK server can fall back to getting the entire checkpoint snapshot implemented in HDDS-1085.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add mechanism in Recon to obtain DB snapshot 'delta' updates from Ozone Manager."
   },
   {
      "_id": "13215734",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-14 12:57:53",
      "description": "Due to the partial Yetus checks (see HDDS-891) recent patches and merge introduced many new checkstyle/rat/findbugs errors.\r\n\r\nI would like to fix them all.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix rat/findbug/checkstyle errors in ozone/hdds projects"
   },
   {
      "_id": "13215716",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2019-02-14 10:32:29",
      "description": "steps taken:\r\n\r\n--------------------\r\n # created 5 datanode cluster.\r\n # shutdown 2 datanodes\r\n # started the datanodes again.\r\n\r\nOne of the datanodes was shut down.\r\n\r\nexception seen :\r\n\r\n\u00a0\r\n{noformat}\r\n2019-02-14 07:37:26 INFO LeaderElection:230 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8 got exception when requesting votes: {}\r\njava.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.\r\n at java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n at java.util.concurrent.FutureTask.get(FutureTask.java:192)\r\n at org.apache.ratis.server.impl.LeaderElection.waitForResults(LeaderElection.java:214)\r\n at org.apache.ratis.server.impl.LeaderElection.askForVotes(LeaderElection.java:146)\r\n at org.apache.ratis.server.impl.LeaderElection.run(LeaderElection.java:102)\r\nCaused by: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:233)\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:214)\r\n at org.apache.ratis.thirdparty.io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:139)\r\n at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$RaftServerProtocolServiceBlockingStub.requestVote(RaftServerProtocolServiceGrpc.java:265)\r\n at org.apache.ratis.grpc.server.GrpcServerProtocolClient.requestVote(GrpcServerProtocolClient.java:83)\r\n at org.apache.ratis.grpc.server.GrpcService.requestVote(GrpcService.java:187)\r\n at org.apache.ratis.server.impl.LeaderElection.lambda$submitRequests$0(LeaderElection.java:188)\r\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n2019-02-14 07:37:26 INFO LeaderElection:46 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: Election PASSED; received 1 response(s) [6a0522ba-019e-4b77-ac1f-a9322cd525b8<-61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5#0:OK-t7] and 1 exception(s); 6a0522ba-019e-4b77-ac1f-a9322cd525b8:t7, leader=null, voted=6a0522ba-019e-4b77-ac1f-a9322cd525b8, raftlog=6a0522ba-019e-4b77-ac1f-a9322cd525b8-SegmentedRaftLog:OPENED, conf=3: [61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5:172.20.0.8:9858, 6a0522ba-019e-4b77-ac1f-a9322cd525b8:172.20.0.6:9858, 0f377918-aafa-4d8a-972a-6ead54048fba:172.20.0.3:9858], old=null\r\n2019-02-14 07:37:26 INFO LeaderElection:52 - 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: a3d1dd2d-554e-4e87-a2cf-076a229af352: group-FD6FA533F1FB not found.\r\n2019-02-14 07:37:26 INFO RoleInfo:130 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: shutdown LeaderElection\r\n2019-02-14 07:37:26 INFO RaftServerImpl:161 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8 changes role from CANDIDATE to LEADER at term 7 for changeToLeader\r\n2019-02-14 07:37:26 INFO RaftServerImpl:258 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: change Leader from null to 6a0522ba-019e-4b77-ac1f-a9322cd525b8 at term 7 for becomeLeader, leader elected after 1066ms\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.staging.catchup.gap = 1000 (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.sleep.time = 25ms (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.watch.timeout = 10s (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.watch.timeout.denomination = 1s (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)\r\n2019-02-14 07:37:26 INFO GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.log.appender.buffer.element-limit = 1 (custom)\r\n2019-02-14 07:37:26 INFO GrpcConfigKeys$Server:43 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)\r\n2019-02-14 07:37:26 INFO RaftServerConfigKeys:43 - raft.server.rpc.request.timeout = 3000ms (default)\r\n2019-02-14 07:37:26 INFO RoleInfo:139 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: start LeaderState\r\n2019-02-14 07:37:26 INFO RaftLogWorker:303 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8-RaftLogWorker: Rolling segment log-3_4 to index:4\r\n2019-02-14 07:37:26 INFO RaftLogWorker:403 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8-RaftLogWorker: Rolled log segment from /data/metadata/ratis/134f574e-c1b0-4556-a206-fd6fa533f1fb/current/log_inprogress_3 to /data/metadata/ratis/134f574e-c1b0-4556-a206-fd6fa533f1fb/current/log_3-4\r\n2019-02-14 07:37:26 INFO RaftServerImpl:354 - 6a0522ba-019e-4b77-ac1f-a9322cd525b8: set configuration 5: [61ad3bf3-e9b1-48e5-90e3-3b78c8b5bba5:172.20.0.8:9858, 6a0522ba-019e-4b77-ac1f-a9322cd525b8:172.20.0.6:9858, 0f377918-aafa-4d8a-972a-6ead54048fba:172.20.0.3:9858], old=null at 5\r\n/opt/starter.sh: line 162: 13 Killed $@\r\n\u00a0\r\n\u00a0\r\n{noformat}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending",
         "newbie",
         "pushed-to-craterlake",
         "test-badlands"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Confusing error log when datanode tries to connect to a destroyed pipeline"
   },
   {
      "_id": "13215258",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-12 14:19:31",
      "description": "Configuration tab in OM/SCM ui is not displaying the correct/configured values, rather it is displaying the default values.\r\n\r\n!image-2019-02-12-19-47-18-332.png!\r\n{code:java}\r\n[hdfs@freonnode10 hadoop]$ curl -s http://freonnode10:9874/conf | grep ozone.om.handler.count.key\r\n<property><name>ozone.om.handler.count.key</name><value>40</value><final>false</final><source>ozone-site.xml</source></property>\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
         "name": "Critical",
         "id": "2"
      },
      "projectname": "HDDS",
      "summary": "Configuration tab in OM/SCM ui is not displaying the correct values"
   },
   {
      "_id": "13215248",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-12 13:23:14",
      "description": "As of now we use opendk 1.8.0 in the Ozone containers.\r\n\r\nJava 9 and Java 10 introduces advanced support for the resource management of the containers and not all of them are available from the latest release of 1.8.0. (see this blog for more details: https://medium.com/adorsys/jvm-memory-settings-in-a-container-environment-64b0840e1d9e)\r\n\r\nI propose to switch to use Java 11 in the containers and test everything with Java 11 at runtime.\r\n\r\nNote: this issue is just about the runtime jdk not about the compile time JDK.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use Java 11 JRE to run Ozone in containers"
   },
   {
      "_id": "13215227",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2019-02-12 12:35:26",
      "description": "HDDS-1033 introduced OzoneFSStorageStatistics for OzoneFileSystem. It uses the StorageStatistics which is introduced in HADOOP-13065 (available from the hadoop2.8/3.0).\r\n\r\nUsing older hadoop (for example hadoop-2.7 which is included in the spark distributions) is not possible any more even with using the isolated class loader (introduced in HDDS-922).\r\n\r\nFortunately it can be fixed:\r\n # We can support null in storageStatistics field with checking everywhere before call it.\r\n # We can create a new constructor of OzoneClientAdapterImpl without using OzoneFSStorageStatistics): If OzoneFSStorageStatistics is not in the method/constructor signature we don't need to load it.\r\n # We can check the availability of HADOOP-13065 and if the classes are not in the classpath we can skip the initialization of the OzoneFSStorageStatistics",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Disable OzoneFSStorageStatistics for hadoop versions older than 2.8"
   },
   {
      "_id": "13215141",
      "assignee": "xyao",
      "components": [],
      "created": "2019-02-12 03:05:59",
      "description": "Investigate failure of TestDefaultCertificateClient#testSignDataStream in jenkins run. \r\n\r\nhttps://builds.apache.org/job/PreCommit-HDDS-Build/2217/testReport/org.apache.hadoop.hdds.security.x509.certificate.client/TestDefaultCertificateClient/testSignDataStream/\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "blocker",
         "pull-request-available",
         "security"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix TestDefaultCertificateClient#testSignDataStream"
   },
   {
      "_id": "13215120",
      "assignee": "avijayan",
      "components": [],
      "created": "2019-02-11 23:45:23",
      "description": "We need to add an API to OM so that we can serve snapshots from the OM server.\r\n - The snapshot should be streamed to fsck server with the ability to throttle network utilization (like TransferFsImage)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create an OM API to serve snapshots to Recon server"
   },
   {
      "_id": "13214494",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-02-08 00:06:52",
      "description": "RPC Client should implement a retry and failover proxy provider to failover between OM Ratis clients. The failover should occur in two scenarios:\r\n# When the client is unable to connect to the OM (either because of network issues or because the OM is down). The client retry proxy provider should failover to next OM in the cluster.\r\n# When OM Ratis Client receives a response from the Ratis server for its request, it also gets the LeaderId of server which processed this request (the current Leader OM nodeId). This information should be propagated back to the client. The Client failover Proxy provider should failover to the leader OM node. This helps avoid an extra hop from Follower OM Ratis Client to Leader OM Ratis server for every request.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement RetryProxy and FailoverProxy for OM client"
   },
   {
      "_id": "13214365",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333910",
            "id": "12333910",
            "name": "Security",
            "description": "Kerberos, X.509, ACLs, SASL and Any other Security issues"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2019-02-07 13:19:31",
      "description": "Because HDDS-1019 and HDDS-1018 (and HDDS-1038?) the security acceptance tests are not working. \r\n\r\nI propose to remove them from daily tests as currently we can't get any meaningful results from Jenkins.\r\n\r\nSecurity tests can be run manually with the test.sh script.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Temporarily disable the security acceptance tests by default in Ozone"
   },
   {
      "_id": "13214364",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2019-02-07 13:11:21",
      "description": "As of now the server side (om, scm) errors are not propagated to the client.\r\n\r\nFor example if ozone is started with one single datanode:\r\n\r\n{code}\r\ndocker-compose exec ozoneManager ozone sh key  put -r THREE /vol1/bucket1/test2 NOTICE.txt             \r\nCreate key failed, error:KEY_ALLOCATION_ERROR\r\n{code}\r\n\r\nThere is no information here about the missing datanodes, or missing pipelines.\r\n\r\nThere are multiple problems which should be fixed:\r\n\r\n1. type safety\r\n\r\nIn ScmBlockLocationProtocolClientSideTranslatorPB the server (om) side exceptions are transformed to IOException where the original status is added to the message: \r\n\r\nFor example:\r\n\r\n{code}\r\n throw new IOException(\"Volume quota change failed, error:\" + resp.getStatus());\r\n{code}\r\n\r\nIn s3 gateway it's very hard to handle the different errors in a proper way. The current code:\r\n\r\n{code}\r\nif (!ex.getMessage().contains(\"KEY_NOT_FOUND\")) {\r\n            result.addError(\r\n                new Error(keyToDelete.getKey(), \"InternalError\",\r\n                    ex.getMessage()));\r\n{code}\r\n\r\n2. message\r\n\r\nThe exception message is not propagated in the om response just the status code\r\n\r\n3. status code and error message are handled in a different way\r\n\r\nTo propagate error code and status code to the client we need to handle them in the same way.  But the Status field is part of the specific response objects (CreateVolumeRequest) and not the OMRequest. I propose to put both StatusCode and error message to the OMRequest.\r\n\r\n4. The status codes in OzoneManagerProtocol.proto/Status enum is not in sync with OmException.ResultCodes.\r\n\r\nIt would be easy to use the same strings for both enums. With a unit test we can ensure that they have the same names in the same order.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Improve the error propagation for ozone sh"
   },
   {
      "_id": "13214063",
      "assignee": "elek",
      "components": [],
      "created": "2019-02-06 02:39:09",
      "description": "This Jira is to implement in ozone to list of in-progress\u00a0multipart uploads in a bucket.\r\n\r\n[https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "List Multipart uploads in a bucket"
   },
   {
      "_id": "13213646",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2019-02-04 09:57:07",
      "description": "\u00a0\r\n{code:java}\r\njava.lang.StackOverflowError\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.getSubject(Subject.java:297)\r\nat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:569)\r\nat org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.getEncodedBlockToken(ContainerProtocolCalls.java:578)\r\nat org.apache.hadoop.hdds.scm.storage.ContainerProtocolCalls.writeChunkAsync(ContainerProtocolCalls.java:318)\r\nat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunkToContainer(BlockOutputStream.java:602)\r\nat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.writeChunk(BlockOutputStream.java:464)\r\nat org.apache.hadoop.hdds.scm.storage.BlockOutputStream.close(BlockOutputStream.java:480)\r\nat org.apache.hadoop.ozone.client.io.BlockOutputStreamEntry.close(BlockOutputStreamEntry.java:137)\r\nat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:489)\r\nat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:501)\r\nat org.apache.hadoop.ozone.client.io.KeyOutputStream.handleFlushOrClose(KeyOutputStream.java:501)\r\n{code}\r\nThe failure is happening because, ozone client receives a CONTAINER_NOT+OPEN exception from daranode, and it allocates a new and retries to write. But every allocate block call to SCM allocates a block on the same quasi closed container and hence client retries indefinitely and ultimately runs out of stack space.\r\n\r\nLogs below indicate 3 successive block allocations from SCM in quasi closed container.\r\n{code:java}\r\n15:15:26.812 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189852894070 bcId: 0} | ret=FAILURE\r\norg.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state\r\n\r\n15:15:26.818 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189853352823 bcId: 0} | ret=FAILURE\r\norg.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state\r\n\r\n15:15:26.825 [grpc-default-executor-3] ERROR DNAudit - user=null | ip=null | op=WRITE_CHUNK {blockData=conID: 2 locID: 101533189853746040 bcId: 0} | ret=FAILURE\r\norg.apache.hadoop.hdds.scm.container.common.helpers.ContainerNotOpenException: Container 2 in QUASI_CLOSED state\r\n{code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "TestCloseContainerByPipeline#testIfCloseContainerCommandHandlerIsInvoked fails intermittently"
   },
   {
      "_id": "13213168",
      "assignee": "xyao",
      "components": [],
      "created": "2019-01-31 20:04:43",
      "description": "In a secure Ozone cluster. Datanodes fail to connect to SCM on {{StorageContainerDatanodeProtocol}}. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "Security"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support Service Level Authorization for Ozone"
   },
   {
      "_id": "13212686",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-01-30 00:56:16",
      "description": "Right now, we check container state if it is not open, and then\u00a0we delete container.\r\n\r\nWe need a way to delete the containers which are open, so adding a force flag will allow deleting a container without any state checks. (This is required for delete replica's when SCM detects over-replicated, and that container to delete can be in open state)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Allow option for force in DeleteContainerCommand"
   },
   {
      "_id": "13212367",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-01-29 01:11:29",
      "description": "This Jira is to do one of the TODO mentioned in the DeadNodeHandler\r\n\r\n// TODO: Check replica count and call replication manager.\r\n\r\n\u00a0\r\n\r\nAs Right now, when a node is dead, replication for the closed containers is not triggered.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle replication of closed containers in DeadNodeHanlder"
   },
   {
      "_id": "13212335",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-01-28 21:50:39",
      "description": "Right now, in the SCMDatanodeProtocolServer getCommandResponse() deleteContainerCommand is not handled, so deleteContainerCommand is not sent to Datanode.\r\n\r\n\u00a0\r\n\r\nThe deletecontainercommand request is sent for over replicated containers, so this over replication is currently broken because of this.\r\n\r\n\u00a0\r\n\r\nBecause of this we see below error:\r\n\r\n\u00a0\r\n{code:java}\r\njava.lang.IllegalArgumentException: Not implemented\r\n at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.getCommandResponse(SCMDatanodeProtocolServer.java:345)\r\n at org.apache.hadoop.hdds.scm.server.SCMDatanodeProtocolServer.sendHeartbeat(SCMDatanodeProtocolServer.java:272)\r\n at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolServerSideTranslatorPB.sendHeartbeat(StorageContainerDatanodeProtocolServerSideTranslatorPB.java:88)\r\n at org.apache.hadoop.hdds.protocol.proto.StorageContainerDatanodeProtocolProtos$StorageContainerDatanodeProtocolService$2.callBlockingMethod(StorageContainerDatanodeProtocolProtos.java:27753)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "SCM"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Handle DeleteContainerCommand in the SCMDatanodeProtocolServer"
   },
   {
      "_id": "13212235",
      "assignee": "xyao",
      "components": [],
      "created": "2019-01-28 13:14:59",
      "description": "The krb5 image based on a base image which is no longer available from dockerhub\r\n\r\nSee: hadoop-ozone/dist/src/main/compose/ozonesecure/docker-image/docker-krb5/Dockerfile-krb5\r\n\r\nDockerfile:\r\nFROM frolvlad/alpine-oraclejdk8:slim\r\n\r\nhttps://hub.docker.com/r/frolvlad/alpine-oraclejdk8\r\n\r\nI propose to switch to openjdk:8u191-jdk-alpine3.8 but it should be tested with the acceptance test suite (A java base image is required as the issuer application creates keystores/truststores on the fly with the help of java keytool.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update the base image of krb5 container for the secure ozone cluster "
   },
   {
      "_id": "13212232",
      "assignee": "elek",
      "components": [],
      "created": "2019-01-28 12:51:22",
      "description": "In the recent months multiple performance issues are resolved in OM/SCM and datanode sides. To identify the remaining problems a distributed tracing framework would help a lot.\r\n\r\nIn HADOOP-15566 there is an ongoing discussion to remove the discontinued HTrace and use something else instead. Until now without any conclusion, but\r\n\r\n 1). There is one existing poc in the jira which uses opentracing\r\n 2). It was suggested to \"evaluating all the options\" before a final decision \r\n\r\nAs an evaluation step we would like to investigate the performance of ozone components with opentracing. This patch can help us to find the performance problem but can be reverted when we will have a final solution in HADOOP-15566 about the common tracing library.\r\n\r\nTo make it lightweight we can use the ozone message level tracing identifier for context propagation instead of modifying the existing hadoop rpc framework. \r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Use distributed tracing to indentify performance problems in Ozone"
   },
   {
      "_id": "13210490",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-01-18 22:52:40",
      "description": "https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "MultipartUpload: S3API for list parts of a object"
   },
   {
      "_id": "13208957",
      "assignee": "elek",
      "components": [],
      "created": "2019-01-11 09:32:59",
      "description": "As of now we have one (false positive) rat violation:\r\n\r\n\u00a0\r\n{code:java}\r\nhadoop-ozone/ozonefs/target/rat.txt: !????? /home/elek/projects/hadoop/hadoop-ozone/ozonefs/dependency-reduced-pom.xml\r\n{code}\r\nAs it's generated during the build, it could be safely ignored.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/5",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/trivial.svg",
         "name": "Trivial",
         "id": "5"
      },
      "projectname": "HDDS",
      "summary": "Exclude dependency-reduced-pom.xml from ozone rat check"
   },
   {
      "_id": "13208874",
      "assignee": "adoroszlai",
      "components": [],
      "created": "2019-01-10 22:12:25",
      "description": "This jira has been filed based on [~ajayydv]'s [review comment |https://issues.apache.org/jira/browse/HDDS-960?focusedCommentId=16739807&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16739807]on HDDS-960\r\n\r\n1. Add a method getServiceAddress(ServicePort port) in ServiceInfo\r\n2. Use this method in TestOzoneShell in place of following snippet:\r\n\r\n{code:java}\r\nString omHostName = services.stream().filter(\r\n        a -> a.getNodeType().equals(HddsProtos.NodeType.OM))\r\n        .collect(Collectors.toList()).get(0).getHostname();\r\n{code}\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add getServiceAddress method to ServiceInfo and use it in TestOzoneShell"
   },
   {
      "_id": "13207970",
      "assignee": "elek",
      "components": [],
      "created": "2019-01-07 08:36:16",
      "description": "Experimental scripts to test github pr capabilities after the github url move. The provided scripts are easier to use locally and provides more strict/focused checks then the existing pre-commit scripts. But this is not a replacements of the existing yetus build as it adds additional (more strict) checks. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone: checkstyle improvements and code quality scripts"
   },
   {
      "_id": "13207339",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2019-01-02 19:54:24",
      "description": "This Jira is to implement backend to support API in S3 for list parts for an object.\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "MultipartUpload: List Parts for a Multipart upload key"
   },
   {
      "_id": "13204046",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2018-12-12 17:05:11",
      "description": "As of now we create a shaded ozonefs artifact which includes all the required class files to use ozonefs (Hadoop compatible file system for Ozone)\r\n\r\nBut the shading process of this artifact is very easy, it includes all the class files but no relocation rules (package name renaming) are configured. With this approach ozonefs can be used from the compatible hadoop version (this is hadoop 3.1 only, I guess) but can't be used with any older hadoop version as it requires the newer version of hadoop-common.\r\n\r\nI tried to configure a full shading (with relocation) but it's not a simple task. For example a pure (non-relocated) Configuration is required by the ozonefs itself, but an other, newer Configuration class is required by the ozone client code which is a dependency of OzoneFileSystem So we need a relocated and a non-relocated class in the same time.\r\n\r\nI tried out a different approach: I moved out all of the ozone specific classes from the OzoneFileSystem to an adapter class (OzoneClientAdapter). In case of an older hadoop version the adapter class itself can be loaded with an isolated classloader. The isolated classloader can load all the required classes from the jar file from a specific path. It doesn't require any specific package relocation as the default class loader doesn't load these classes. \r\n\r\nThe OzoneFileSystem (in case of older hadoop version) can load the adapter with the isolated classloader and only a few classes should be shared between the normal and isolated classloader (the interface of the adapter and the types in the method signatures). All of the other ozone classes and the newer hadoop dependencies will be hidden by the isolated classloader.\r\n\r\nThis patch is more like a proof of concept, I would like to start a discussion about this approach. I successfully used the generated artifact to use ozonefs from spark 2.4 default distribution (which includes hadoop 2.7). \r\n\r\nFor a final patch I would add some check to use the ozonefs without any classpath separation by default. (could be configured or chosen by automatically)\r\n\r\n\r\nFor using spark (+ hadoop 2.7 + kubernetes scheduler) together with ozone, you can check this screencast: https://www.youtube.com/watch?v=cpRJcSHIEdM&t=8s\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create isolated classloder to use ozonefs with any older hadoop versions"
   },
   {
      "_id": "13203964",
      "assignee": "avijayan",
      "components": [],
      "created": "2018-12-12 10:57:28",
      "description": "HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.\r\n\r\nUnfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone S3G daemon in the default web server. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable prometheus endpoint for Ozone S3 gateway"
   },
   {
      "_id": "13203962",
      "assignee": "elek",
      "components": [],
      "created": "2018-12-12 10:56:19",
      "description": "HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.\r\n\r\nUnfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone/HDDS datanodes on the web server of the HDDS Rest endpoint. ",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Enable prometheus endpoints for Ozone datanodes"
   },
   {
      "_id": "13202780",
      "assignee": "adoroszlai",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2018-12-06 11:32:46",
      "description": "SCM and OM web uis (http://localhost:9876 and http://localhost:9874) display the actual version but the displayed version is the version of the hadoop dependencies:\r\n\r\nThis is provided by the org.apache.hadoop.hdds.server.ServiceRuntimeInfoImpl which is a default implementation of ServiceRuntimeInfo. (Both OzoneManager and StorageContainerManager extend this class).\r\n\r\nWe need to use OzoneVersionInfo and HddsVersionInfo classes to display the actual version instead of org.apache.hadoop.util.VersionInfo.\r\n\r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Display the ozone version on SCM/OM web ui instead of Hadoop version"
   },
   {
      "_id": "13202769",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2018-12-06 11:23:20",
      "description": "{color:#FF0000}{color}\r\n\r\nAs of now the main s3g endpoint (such as [http://localhost:9878|http://localhost:9878/]) returns with HTTP 500 if it's opened from the browser.\r\n\r\nThe main endpoint is used to get all the available buckets, but amazon returns with a redirect IF the Authorization header is missing:\r\n{code:java}\r\n curl -v s3.us-east-2.amazonaws.com\r\n*   Trying 52.219.88.59...\r\n* TCP_NODELAY set\r\n* Connected to s3.us-east-2.amazonaws.com (52.219.88.59) port 80 (#0)\r\n> GET / HTTP/1.1\r\n> Host: s3.us-east-2.amazonaws.com\r\n> User-Agent: curl/7.62.0\r\n> Accept: */*\r\n> \r\n< HTTP/1.1 307 Temporary Redirect\r\n< x-amz-id-2: fq8RXJdSlVo8PqidHaP8XXczMfLSEAt5Tm4JP98atilWRjalMvqtPa6mwq6rEIXz4cCPrPqJkO4=\r\n< x-amz-request-id: 5C6ACE6D6FC273B9\r\n< Date: Thu, 06 Dec 2018 11:16:36 GMT\r\n< Location: https://aws.amazon.com/s3/\r\n< Content-Length: 0\r\n{code}\r\nI propose to do the same for Ozone:\r\n\r\n1.) If the authorization header is missing on the root URL, redirect to an internal page.\r\n 2.) Create an internal landing page at [http://localhost:9878/_ozone] with the following content:\r\n a) A very short introduction to use the endpoint (with aws client)\r\n b) The actual documentation of ozone (which is also included in the scm/om ui)\r\n\r\nNote: we need an url schema which is not conflicting with the real REST requests. As the bucket and volume names should not contain underscore in ozone, we can use it to prefix all the urls:\r\n * [http://localhost:9878/_ozone] --> landing page\r\n * [http://localhost:9878/_ozone/(css]|js) --> required resources\r\n * [http://localhost:9878/_ozone/docs] --> Documentation with the required resources.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create informative landing page for Ozone S3 gateway "
   },
   {
      "_id": "13199715",
      "assignee": "xyao",
      "components": [],
      "created": "2018-11-20 23:47:39",
      "description": "This can be reproed with \"mvn test\" under hadoop-ozone project but not with individual test run under IntelliJ.\r\n\r\n\u00a0\r\n{code:java}\r\nTests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.33 s <<< FAILURE! - in org.apache.hadoop.ozone.TestOmUtils\r\n\r\ntestNoOmDbDirConfigured(org.apache.hadoop.ozone.TestOmUtils)\u00a0 Time elapsed: 0.028 s\u00a0 <<< FAILURE!\r\n\r\njava.lang.AssertionError:\r\n\r\n\u00a0\r\n\r\nExpected: an instance of java.lang.IllegalArgumentException\r\n\r\n\u00a0\u00a0 \u00a0 but: <java.lang.NullPointerException> is a java.lang.NullPointerException\r\n\r\nStacktrace was: java.lang.NullPointerException\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdds.server.ServerUtils.getOzoneMetaDirPath(ServerUtils.java:130)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.OmUtils.getOmDbDir(OmUtils.java:141)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ozone.TestOmUtils.testNoOmDbDirConfigured(TestOmUtils.java:89)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:498)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\r\n\r\n\u00a0\r\n\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "test-badlands"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Fix NPE ServerUtils#getOzoneMetaDirPath"
   },
   {
      "_id": "13199305",
      "assignee": "elek",
      "components": [],
      "created": "2018-11-19 11:47:52",
      "description": "Similar to the apache/hadoop:2 and apache/hadoop:3 images I propose to provide apache/ozone docker images which includes the voted release binaries.\r\n\r\nThe image can follow all the conventions from HADOOP-14898\r\n\r\n1. BRANCHING\r\n\r\nI propose to create new docker branches:\r\n\r\ndocker-ozone-0.3.0-alpha\r\ndocker-ozone-latest\r\n\r\nAnd ask INFRA to register docker-ozone-(.*) in the dockerhub to create apache/ozone: images\r\n\r\n2. RUNNING\r\n\r\nI propose to create a default runner script which starts om + scm + datanode + s3g all together. With this approach you can start a full ozone cluster as easy as\r\n\r\n{code}\r\ndocker run -p 9878:9878 -p 9876:9876 -p 9874:9874 -d apache/ozone\r\n{code}\r\n\r\nThat's all. This is an all-in-one docker image which is ready to try out.\r\n\r\n3. RUNNING with compose\r\n\r\nI propose to include a default docker-compose + config file in the image. To start a multi-node pseudo cluster it will be enough to execute:\r\n\r\n{code}\r\ndocker run apache/ozone cat docker-compose.yaml > docker-compose.yaml\r\ndocker run apache/ozone cat docker-config > docker-config\r\ndocker-compose up -d\r\n{code}\r\n\r\nThat's all, and you have a multi-(pseudo)node ozone cluster which could be scaled up and down with ozone.\r\n\r\n4. k8s\r\n\r\nLater we can also provide k8s resource files with the same approach:\r\n\r\n{code}\r\ndocker run apache/ozone cat k8s.yaml | kubectl apply -f -\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Provide official apache docker image for Ozone"
   },
   {
      "_id": "13195865",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2018-11-02 10:22:55",
      "description": "I propose to add a custom Map<String,String> annotation field to objects/buckets and keys in Ozone Manager.\r\n\r\nIt would enable to build any extended functionality on top of the OM's generic interface. For example:\r\n\r\n * Support tags in Ozone S3 gateway (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html)\r\n * Support md5 based ETags in s3g\r\n * Store s3 related authorization data (ACLs, policies) together with the parent objects\r\n\r\nAs an optional feature (could be implemented later) the client can defined the exposed annotations. For example s3g can defined which annotations should be read from rocksdb on OM side and sent the the client (s3g)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Support custom key/value annotations on volume/bucket/key"
   },
   {
      "_id": "13195193",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334212",
            "id": "12334212",
            "name": "test"
         }
      ],
      "created": "2018-10-30 18:25:46",
      "description": "This Jira is created from the comment from [~elek]\r\n\r\n1. I think sooner or later we need to run ozone tests with real replication. We can add a 'scale up' to the hadoop-ozone/dist/src/main/smoketest/test.sh\r\n{code:java}\r\ndocker-compose -f \"$COMPOSE_FILE\" down\r\ndocker-compose -f \"$COMPOSE_FILE\" up -d\r\ndocker-compose -f \"$COMPOSE_FILE\" scale datanode=3\r\n{code}\r\nAnd with this modification we don't need the '--storage-class REDUCED_REDUNDANCY'. (But we can do it in separated jira)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie",
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Run S3 smoke tests with replication STANDARD."
   },
   {
      "_id": "13195020",
      "assignee": "vivekratnavel",
      "components": [],
      "created": "2018-10-30 03:21:28",
      "description": "This Jira is added to create S3 subcommand, which will be used for all S3 related operations.\r\nUnder this jira, move the command ozone sh bucket <<bucketname>> to ozone s3 bucket <<bucketname>>",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create S3 subcommand to run S3 related operations"
   },
   {
      "_id": "13194160",
      "assignee": "elek",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2018-10-25 14:46:06",
      "description": "Since we have functional {{S3Gateway}} for Ozone which works on REST protocol, having REST protocol support in OzoneClient feels redundant and it will take a lot of effort to maintain it up to date.\r\nAs S3Gateway is in a functional state now, I propose to remove REST protocol support from OzoneClient.\r\n\r\nOnce we remove REST support from OzoneClient, the following will be the interface to access Ozone cluster\r\n * OzoneClient (RPC Protocol)\r\n * OzoneFS (RPC Protocol)\r\n * S3Gateway (REST Protocol)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Removing REST protocol support from OzoneClient"
   },
   {
      "_id": "13192396",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333905",
            "id": "12333905",
            "name": "Ozone Filesystem",
            "description": "Hadoop compatible file system - OzoneFS"
         }
      ],
      "created": "2018-10-17 23:13:45",
      "description": "Files created by o3fs show creation timestamp as unix epoch.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "app-compat"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Incorrect creation time for files created by o3fs."
   },
   {
      "_id": "13192046",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2018-10-16 21:19:36",
      "description": "\u00a0\r\n\r\nRenaming a directory within the same parent directory fails with the exception:\r\n{code:java}\r\nUnable to move: o3://bucket2.volume2/testo3/.hive-staging_hive_2018-10-16_21-09-35_130_1001829123585250245-1/_tmp.-ext-10000 to: o3://bucket2.volume2/testo3/.hive-staging_hive_2018-10-16_21-09-35_130_1001829123585250245-1/_tmp.-ext-10000.moved\r\n{code}\r\nDetailed exception in comment below.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "app-compat"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Fix OzoneFS directory rename"
   },
   {
      "_id": "13191737",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2018-10-15 21:50:45",
      "description": "Modified\u00a0HIVE_AUX_JARS_PATH to include Ozone jars. Tried creating Hive external table on Ozone. It fails with \"Error: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)\"\r\n{code:java}\r\n-bash-4.2$ beeline\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/hdp/3.0.3.0-63/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\r\nConnecting to jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\r\nEnter username for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:\r\nEnter password for jdbc:hive2://ctr-e138-1518143905142-510793-01-000011.hwx.site:2181,ctr-e138-1518143905142-510793-01-000006.hwx.site:2181,ctr-e138-1518143905142-510793-01-000008.hwx.site:2181,ctr-e138-1518143905142-510793-01-000010.hwx.site:2181,ctr-e138-1518143905142-510793-01-000007.hwx.site:2181/default:\r\n18/10/15 21:36:55 [main]: INFO jdbc.HiveConnection: Connected to ctr-e138-1518143905142-510793-01-000004.hwx.site:10000\r\nConnected to: Apache Hive (version 3.1.0.3.0.3.0-63)\r\nDriver: Hive JDBC (version 3.1.0.3.0.3.0-63)\r\nTransaction isolation: TRANSACTION_REPEATABLE_READ\r\nBeeline version 3.1.0.3.0.3.0-63 by Apache Hive\r\n0: jdbc:hive2://ctr-e138-1518143905142-510793> create external table testo3 ( i int, s string, d float) location \"o3://bucket2.volume2/testo3\";\r\nError: Error while compiling statement: FAILED: HiveAuthzPluginException Error getting permissions for o3://bucket2.volume2/testo3: User: hive is not allowed to impersonate anonymous (state=42000,code=40000)\r\n0: jdbc:hive2://ctr-e138-1518143905142-510793> {code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "Triaged",
         "app-compat"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Creating hive table on Ozone fails"
   },
   {
      "_id": "13191261",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-10-12 17:35:27",
      "description": "public static final String DFS_CONTAINER_RATIS_NUM_CONTAINER_OP_EXECUTORS_KEY\r\n = \"dfs.container.ratis.num.container.op.threads\";\r\n\r\nThis should be changed to\u00a0dfs.container.ratis.num.container.op.executors\r\n\r\n\u00a0\r\n\r\nHDDS-550 has added this in OzoneConfigKeys.java, but they have named differently in ozone-default.xml and ScmConfigKeys.java\r\n\r\n\u00a0\r\n\r\nBecause of this TestOzoneConfigurationFields.java is failing\r\n\r\n[https://builds.apache.org/job/PreCommit-HDDS-Build/1378/testReport/]\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Rename dfs.container.ratis.num.container.op.threads"
   },
   {
      "_id": "13190786",
      "assignee": "elek",
      "components": [],
      "created": "2018-10-10 23:22:45",
      "description": "Fields like below are empty\r\n\r\nNode Manager: Minimum chill mode nodes \r\nNode Manager: Out-of-node chill mode \r\nNode Manager: Chill mode status \r\nNode Manager: Manual chill mode\r\n\r\nPlease see attached screenshot !Screen Shot 2018-10-10 at 4.19.59 PM.png!",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "pull-request-available"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "On SCM UI, Node Manager info is empty"
   },
   {
      "_id": "13190304",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334617",
            "id": "12334617",
            "name": "documentation"
         }
      ],
      "created": "2018-10-09 07:25:25",
      "description": "Set up a hadoop cluster where ozone is also installed. Ozone can be referenced via o3://xx.xx.xx.xx:9889\r\n{code:java}\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]# ozone sh bucket list o3://xx.xx.xx.xx:9889/volume1/\r\n2018-10-09 07:21:24,624 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n[ {\r\n\"volumeName\" : \"volume1\",\r\n\"bucketName\" : \"bucket1\",\r\n\"createdOn\" : \"Tue, 09 Oct 2018 06:48:02 GMT\",\r\n\"acls\" : [ {\r\n\"type\" : \"USER\",\r\n\"name\" : \"root\",\r\n\"rights\" : \"READ_WRITE\"\r\n}, {\r\n\"type\" : \"GROUP\",\r\n\"name\" : \"root\",\r\n\"rights\" : \"READ_WRITE\"\r\n} ],\r\n\"versioning\" : \"DISABLED\",\r\n\"storageType\" : \"DISK\"\r\n} ]\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]# ozone sh key list o3://xx.xx.xx.xx:9889/volume1/bucket1\r\n2018-10-09 07:21:54,500 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n[ {\r\n\"version\" : 0,\r\n\"md5hash\" : null,\r\n\"createdOn\" : \"Tue, 09 Oct 2018 06:58:32 GMT\",\r\n\"modifiedOn\" : \"Tue, 09 Oct 2018 06:58:32 GMT\",\r\n\"size\" : 0,\r\n\"keyName\" : \"mr_job_dir\"\r\n} ]\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]#{code}\r\nHdfs is also set fine as below\r\n{code:java}\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]# hdfs dfs -ls /tmp/mr_jobs/input/\r\nFound 1 items\r\n-rw-r--r-- 3 root hdfs 215755 2018-10-09 06:37 /tmp/mr_jobs/input/wordcount_input_1.txt\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]#{code}\r\nNow try to run Mapreduce example job against ozone o3:\r\n{code:java}\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]# /usr/hdp/current/hadoop-client/bin/hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /tmp/mr_jobs/input/ o3://xx.xx.xx.xx:9889/volume1/bucket1/mr_job_dir/output\r\n18/10/09 07:15:38 INFO conf.Configuration: Removed undeclared tags:\r\njava.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character : :\r\nat org.apache.hadoop.hdds.scm.client.HddsClientUtils.verifyResourceName(HddsClientUtils.java:143)\r\nat org.apache.hadoop.ozone.client.rpc.RpcClient.getVolumeDetails(RpcClient.java:231)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)\r\nat com.sun.proxy.$Proxy16.getVolumeDetails(Unknown Source)\r\nat org.apache.hadoop.ozone.client.ObjectStore.getVolume(ObjectStore.java:92)\r\nat org.apache.hadoop.fs.ozone.OzoneFileSystem.initialize(OzoneFileSystem.java:121)\r\nat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3354)\r\nat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\r\nat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)\r\nat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)\r\nat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)\r\nat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\r\nat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(FileOutputFormat.java:178)\r\nat org.apache.hadoop.examples.WordCount.main(WordCount.java:85)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\r\nat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\r\nat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.hadoop.util.RunJar.run(RunJar.java:318)\r\nat org.apache.hadoop.util.RunJar.main(RunJar.java:232)\r\n18/10/09 07:15:39 INFO conf.Configuration: Removed undeclared tags:\r\n[root@ctr-e138-1518143905142-510793-01-000002 ~]#\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "app-compat",
         "test-badlands"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Mapreduce example fails with java.lang.IllegalArgumentException: Bucket or Volume name has an unsupported character"
   },
   {
      "_id": "13189703",
      "assignee": "shashikant",
      "components": [],
      "created": "2018-10-05 12:32:59",
      "description": "ContainerStateMachine will keep of track of the last successfully applied transaction index and on restart inform Ratis the index, so that the subsequent transactions can be reapplied from here.\r\n\r\nMoreover, in case one transaction fails, all the subsequent transactions on the container should fail in the containerStateMachine and a container close action to SCM needs to be initiated to close the container.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "recovery"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "ContainerStateMachine should fail subsequent transactions per container in case one fails"
   },
   {
      "_id": "13189331",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-10-04 00:05:14",
      "description": "This Jira is the starter Jira to make changes required for copy key\u00a0request in S3 to support copy key across the bucket. In ozone world, this is just a metadata change. This Jira is created to just change .proto file for copy key\u00a0request support.\r\n\r\n\u00a0\r\n\r\nThis Jira is created to provide a copy object API in RpcClient to copy an object which already exists in ozone to same/another bucket.\r\n\r\n[https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html]",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "S3"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add proto changes required for CopyKey support in ozone"
   },
   {
      "_id": "13188594",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2018-10-01 17:19:31",
      "description": "UPDATE:  Since the creation of this jira the volume name is removed from the url. So the original goal is simplified: the only task is to remove the volume from the VirtualHostStyleFilter parsing part.\r\n\r\n\u00a0ORIGINAL description:\r\n\r\nIn VirtualHost Style filter parse bucketname from the HttpHeader Host if it is virtual host style request. Remove the volume parsing code.\r\n\r\nThis Jira is created from [~elek] comments on HDDS-525 jira.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove volume name parsing from VirtualHostStyleFilter"
   },
   {
      "_id": "13187717",
      "assignee": "xyao",
      "components": [],
      "created": "2018-09-26 20:28:49",
      "description": "This is found during the release verification of 0.2.1. In the \"Building from Souces\" page:\r\n\r\n\"\u00a0please follow the instructions in the\u00a0*README.md*\u00a0in the\u00a0\r\n\r\n{{$hadoop_src/hadoop-ozone/acceptance-test\"}}\r\n\r\n\u00a0\r\n\r\n{{The correct location should be }}\r\n\r\n{{\"$hadoop_src/}}hadoop-ozone/dist/target/$hdds_version/smoketest\"",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Update the acceptance test location mentioned in ozone document"
   },
   {
      "_id": "13186260",
      "assignee": "elek",
      "components": [],
      "created": "2018-09-20 07:44:43",
      "description": "Simple delete Object call.\r\n\r\nImplemented by HDDS-444 without the acceptance tests.\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement DeleteObject REST endpoint"
   },
   {
      "_id": "13186258",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-09-20 07:38:32",
      "description": "The create bucket creates a bucket using createS3Bucket which has been added as part of HDDS-577.\r\n\r\n[https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html]\r\n\r\nStub implementation is created as part of HDDS-444. Need to finalize, check the missing headers, add acceptance tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement PutBucket REST endpoint"
   },
   {
      "_id": "13186257",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-09-20 07:35:16",
      "description": "The delete bucket will do the opposite of the create bucket call. It will locate the volume via the username in the delete call.\r\n\r\nReference is here:\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html\r\n\r\nThis is implemented as part of HDDS-444 but we need the double check the headers and add acceptance tests.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement DeleteBucket REST endpoint"
   },
   {
      "_id": "13186256",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-09-20 07:34:05",
      "description": "This operation is useful to determine if a bucket exists and you have permission to access it. The operation returns a 200 OK if the bucket exists and you have permission to access it. Otherwise, the operation might return responses such as 404 Not Found and 403 Forbidden.  \r\n\r\nSee the reference here:\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement HeadBucket REST endpoint"
   },
   {
      "_id": "13186250",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12334701",
            "id": "12334701",
            "name": "S3"
         }
      ],
      "created": "2018-09-20 07:24:10",
      "description": "The Copy object is a simple call to Ozone Manager.  This API can only be done after the PUT OBJECT Call.\r\n\r\nThis implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3. A PUT copy operation is the same as performing a GET and then a PUT. Adding the request header, x-amz-copy-source, makes the PUT operation copy the source object into the destination bucket.\r\n\r\nIf the Put Object call has this header, then Put Object call will issue a rename. \r\n\r\nWork Items or JIRAs\r\nDetect the presence of the extra header - x-amz-copy-source\r\nMake sure that destination bucket exists.\r\n\r\nThe AWS reference is here:\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\r\n\r\n(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody would be interested, I can be more specific, explain what we need or help).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement CopyObject REST endpoint"
   },
   {
      "_id": "13186248",
      "assignee": "elek",
      "components": [],
      "created": "2018-09-20 07:21:40",
      "description": "The goal is implement the GetObject endpoint for the S3 Gateway:\r\n\r\nThere reference doc is here:\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html\r\n\r\nThis is mostly just a simple stream copy between the OzoneClient and the output, but we also need to:\r\n\r\n1. support rest-response-* headers\r\n2. Support Range headers (Jetty may support it out-of the box)\r\n\r\n(This jira is marked as newbie as it requires only basic Ozone knowledge. If somebody interested I can be more specific).",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement GetObject REST endpoint"
   },
   {
      "_id": "13185918",
      "assignee": "hanishakoneru",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12337244",
            "id": "12337244",
            "name": "OM HA"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         }
      ],
      "created": "2018-09-18 22:08:32",
      "description": "OzoneManager can be a single point of failure in an Ozone cluster. We propose an HA implementation for OM using Ratis (Raft protocol).\r\n\r\nAttached the design document for the proposed implementation.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
         "id": "2",
         "description": "A new feature of the product, which has yet to be developed.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
         "name": "New Feature",
         "subtask": false,
         "avatarId": 21141
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OzoneManager HA"
   },
   {
      "_id": "13184931",
      "assignee": "avijayan",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333906",
            "id": "12333906",
            "name": "Ozone Manager",
            "description": "Ozone Metadata server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2018-09-13 14:52:09",
      "description": "SCM and OM can use the picocli to parse command-line arguments.\r\n\r\nSuggested in HDDS-415 by [~anu].",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "alpha2",
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "OM and SCM should use picocli to parse arguments"
   },
   {
      "_id": "13184885",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2018-09-13 12:35:29",
      "description": "steps taken :\r\n\r\n------------------\r\n # Ran Put Key command to write 50GB data. Put Key client operation failed after 17 mins.\r\n\r\nerror seen\u00a0 ozone.log :\r\n\r\n------------------------------------\r\n\r\n\u00a0\r\n{code}\r\n2018-09-13 12:11:53,734 [ForkJoinPool.commonPool-worker-20] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_1 chunk stage:COMMIT_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_1 tmp chunk file\r\n2018-09-13 12:11:56,576 [pool-3-thread-60] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 chunk stage:WRITE_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 tmp chunk file\r\n2018-09-13 12:11:56,739 [ForkJoinPool.commonPool-worker-20] DEBUG (ChunkManagerImpl.java:85) - writing chunk:bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 chunk stage:COMMIT_DATA chunk file:/tmp/hadoop-root/dfs/data/hdds/de0a9e01-4a12-40e3-b567-51b9bd83248e/current/containerDir0/16/chunks/bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2 tmp chunk file\r\n2018-09-13 12:12:21,410 [Datanode State Machine Thread - 0] DEBUG (DatanodeStateMachine.java:148) - Executing cycle Number : 206\r\n2018-09-13 12:12:51,411 [Datanode State Machine Thread - 0] DEBUG (DatanodeStateMachine.java:148) - Executing cycle Number : 207\r\n2018-09-13 12:12:53,525 [BlockDeletingService#1] DEBUG (TopNOrderedContainerDeletionChoosingPolicy.java:79) - Stop looking for next container, there is no pending deletion block contained in remaining containers.\r\n2018-09-13 12:12:55,048 [Datanode ReportManager Thread - 1] DEBUG (ContainerSet.java:191) - Starting container report iteration.\r\n2018-09-13 12:13:02,626 [pool-3-thread-1] ERROR (ChunkUtils.java:244) - Rejecting write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216}\r\n2018-09-13 12:13:03,035 [pool-3-thread-1] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 54834b29-603d-4ba9-9d68-0885215759d8 : Message: Rejecting write chunk request. OverWrite flag required.ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216} : Result: OVERWRITE_FLAG_REQUIRED\r\n2018-09-13 12:13:03,037 [ForkJoinPool.commonPool-worker-11] ERROR (ChunkUtils.java:244) - Rejecting write chunk request. Chunk overwrite without explicit request. ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216}\r\n2018-09-13 12:13:03,037 [ForkJoinPool.commonPool-worker-11] INFO (ContainerUtils.java:149) - Operation: WriteChunk : Trace ID: 54834b29-603d-4ba9-9d68-0885215759d8 : Message: Rejecting write chunk request. OverWrite flag required.ChunkInfo{chunkName='bd80b58a5eba888200a4832a0f2aafb3_stream_5f3b2505-6964-45c9-a7ad-827388a1e6a0_chunk_2, offset=0, len=16777216} : Result: OVERWRITE_FLAG_REQUIRED\r\n\u00a0\r\n{code}\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "alpha2"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "PutKey failed due to error \"Rejecting write chunk request. Chunk overwrite without explicit request\""
   },
   {
      "_id": "13184843",
      "assignee": "elek",
      "components": [],
      "created": "2018-09-13 09:22:42",
      "description": "The next step is after HDDS-441 is to add a rest server to the s3gateway service.\r\n\r\nFor the http server the obvious choice is to use org.apache.hadoop.http.HttpServer2. We also have a org.apache.hadoop.hdds.server.BaseHttpServer which helps to create the HttpServer2.\r\n\r\nIn hadoop usually the jersey 1.19 is used. I prefer to exclude jersey dependency from the s3gateway and add the latest jersey2. Hopefully it also could be initialized easily, similar to HttpServer2.addJerseyResourcePackage\r\n\r\nThe trickiest part is the resource handling. By default the input parameter of the jersey is the JAX-RS resource class and jersey creates new instances from the specified resource classes.\r\n\r\nBut with this approach we can't inject other components (such as the OzoneClient) to the resource classes. In Hadoop usually a singleton is used or the reference object is injected to the ServletContext. Both of these are just workaround and make the testing harder.\r\n\r\nI propose to use some lightweight managed dependency injection:\r\n # If we can use and JettyApi to instantiate the resource classes, that would be the easiest one.\r\n # Using a simple CDI framework like dagger, also would help. Dagger is very lightweight, it doesn't support request scoped objects just simple @Inject annotations, but hopefully we won't need fancy new features.\r\n # The most complex solution would be to use CDI or Guice. CDI seems to be more nature choice for the JAX-RS. It can be checked how easy is to integrate Weld to the Jetty + Jersey combo.\r\n\r\nThe expected end result of this task is a new HttpServer subcomponent inside the s3gateway which could be started/stopped. We need an example simple service (for exampe a /health endpoint which returns with an 'OK' string) which can demonstrate how our own utilitites (such as OzoneClient) could be injected to the REST resources.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add rest service to the s3gateway"
   },
   {
      "_id": "13184803",
      "assignee": "elek",
      "components": [],
      "created": "2018-09-13 06:56:32",
      "description": "The first element what we need is a new command line application to start the s3 gateway.\r\n\r\n1. A new project should be introduced: hadoop-ozone/s3-gateway\r\n\r\n2. A new command line application (eg. org.apache.hadoop.ozone.s3.Gateway should be added with a simple main and start/stop method which just prints out a starting/stopping log message\r\n\r\n3. hadoop-ozone/common/src/main/bin/ozone should be modified to manage the new service (eg. ozone s3g start, ozone s3g stop)\r\n\r\n4. to make it easier to test a new docker-compose based test cluster should be added to the hadoop-dist/src/main/compose (the normal ./ozone could be copied but we need to add the new s3g component)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Create new s3gateway daemon"
   },
   {
      "_id": "13184761",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         }
      ],
      "created": "2018-09-13 00:32:35",
      "description": "Datanode starts but runs in a tight loop forever if it cannot create the DataNode ID directory e.g. due to permissions issues. I encountered this by having a typo in my ozone-site.xml for {{ozone.scm.datanode.id}}.\r\n\r\nIn\u00a0just a few minutes the DataNode had generated over 20GB of log+out files with the following exception:\r\n{code:java}\r\n2018-09-12 17:28:20,649 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 2\r\n63:\r\njava.io.IOException: Unable to create datanode ID directories.\r\nat org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)\r\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)\r\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)\r\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\r\n2018-09-12 17:28:20,648 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Execution exception when running task in Datanode State Mach\r\nine Thread - 160\r\n2018-09-12 17:28:20,650 WARN org.apache.hadoop.util.concurrent.ExecutorHelper: Caught exception in thread Datanode State Machine Thread - 1\r\n60:\r\njava.io.IOException: Unable to create datanode ID directories.\r\nat org.apache.hadoop.ozone.container.common.helpers.ContainerUtils.writeDatanodeDetailsTo(ContainerUtils.java:211)\r\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.persistContainerDatanodeDetails(InitDatanodeState.java:131)\r\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:111)\r\nat org.apache.hadoop.ozone.container.common.states.datanode.InitDatanodeState.call(InitDatanodeState.java:50)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748){code}\r\n\r\nWe should just exit since this is a fatal issue.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Datanode loops forever if it cannot create directories"
   },
   {
      "_id": "13181326",
      "assignee": "elek",
      "components": [],
      "created": "2018-08-27 12:25:59",
      "description": "It would be great to make the hdds/ozone proto files independent from hdfs proto files. It would help as to start ozone with multiple version of hadoop version.\r\n\r\nAlso helps to make artifacts from the hdds protos:  HDDS-220\r\n\r\n Currently we have a few unused \"hdfs.proto\" import in the proto files and we use the StorageTypeProto from hdfs:\r\n\r\n{code}\r\ncd hadoop-hdds\r\ngrep -r \"hdfs\" --include=\"*.proto\"\r\ncommon/src/main/proto/ScmBlockLocationProtocol.proto:import \"hdfs.proto\";\r\ncommon/src/main/proto/StorageContainerLocationProtocol.proto:import \"hdfs.proto\";\r\n\r\n cd ../hadoop-ozone\r\ngrep -r \"hdfs\" --include=\"*.proto\"\r\ncommon/src/main/proto/OzoneManagerProtocol.proto:import \"hdfs.proto\";\r\ncommon/src/main/proto/OzoneManagerProtocol.proto:    required hadoop.hdfs.StorageTypeProto storageType = 5 [default = DISK];\r\ncommon/src/main/proto/OzoneManagerProtocol.proto:    optional hadoop.hdfs.StorageTypeProto storageType = 6;\r\n{code}\r\n\r\nI propose to \r\n\r\n1.) remove the hdfs import statements from the proto files\r\n2.) Copy the StorageTypeProto and create a Hdds version from it (without PROVIDED)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove dependencies between hdds/ozone and hdfs proto files"
   },
   {
      "_id": "13180787",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333904",
            "id": "12333904",
            "name": "Ozone Client",
            "description": "Ozone Client Library"
         }
      ],
      "created": "2018-08-23 17:58:28",
      "description": "Certain Exception thrown by a server can be because server is in a state\r\nwhere request cannot be processed temporarily.\r\n\u00a0Ozone Client may retry the request. If the service is up, the server may be able to\r\n\u00a0process a retried request. This Jira aims to introduce notion of\u00a0RetriableException in Ozone.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add RetriableException class in Ozone"
   },
   {
      "_id": "13179303",
      "assignee": "elek",
      "components": [],
      "created": "2018-08-16 08:57:13",
      "description": "In the current acceptance tests (hadoop-ozone/acceptance-test) the robot files contain two kind of commands:\r\n\r\n1) starting and stopping clusters\r\n2) testing the basic behaviour with client calls\r\n\r\nIt would be great to separate the two functionality and include only the testing part in the robot files.\r\n\r\n1. Ideally the tests could be executed in any environment. After a kubernetes install I would like to do a smoke test. It could be a different environment but I would like to execute most of the tests (check ozone cli, rest api, etc.)\r\n\r\n2. There could be multiple ozone environment (standlaone ozone cluster, hdfs + ozone cluster, etc.). We need to test all of them with all the tests.\r\n\r\n3. With this approach we can collect the docker-compose files just in one place (hadoop-dist project). After a docker-compose up there should be a way to execute the tests with an existing cluster. Something like this:\r\n\r\n{code}\r\ndocker run -it apache/hadoop-runner -v ./acceptance-test:/opt/acceptance-test -e SCM_URL=http://scm:9876 --network=composenetwork start-all-tests.sh\r\n{code}\r\n\r\n4. It also means that we need to execute the tests from a separated container instance. We need a configuration parameter to define the cluster topology. Ideally it could be just one environment variables with the url of the scm and the scm could be used to discovery all of the required components + download the configuration files from there.\r\n\r\n5. Until now we used the log output of the docker-compose files to do some readiness probes. They should be converted to poll the jmx endpoints and check if the cluster is up and running. If we need the log files for additional testing we can create multiple implementations for different type of environments (docker-compose/kubernetes) and include the right set of functions based on an external parameters.\r\n\r\n6. Still we need a generic script under the ozone-acceptance test project to run all the tests (starting the docker-compose clusters, execute tests in a different container, stop the cluster) \r\n",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "test"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Separate install and testing phases in acceptance tests."
   },
   {
      "_id": "13175286",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-07-27 22:58:09",
      "description": "This Jira is to add IntefaceAudience for\u00a0datanode code in the container-service module.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "Triaged"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Add InterfaceAudience/InterfaceStability annotations"
   },
   {
      "_id": "13173770",
      "assignee": "vivekratnavel",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12335108",
            "id": "12335108",
            "name": "Ozone Recon",
            "description": "Ozone Recon Server (previously fsck server)"
         }
      ],
      "created": "2018-07-23 10:42:26",
      "description": "It would be good if we have some metric/histogram in OzoneManager UI indicating the different container size range and corresponding percentages for the same created in the cluster.\r\n\r\nFor example :\r\n\r\n0-2 GB\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a010%\r\n\r\n2-4 GB .\u00a0 \u00a0 \u00a0 \u00a0 \u00a020%\r\n\r\n4-5 GB\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a070%\r\n\r\n5+ GB\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0%\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "TriagePending"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "Recon: Add container size distribution visualization"
   },
   {
      "_id": "13171796",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-07-12 18:25:06",
      "description": "This Jira is to remove the datanodeID file. After ContainerIO\u00a0 work\u00a0(HDDS-48 branch) is merged, we have a version file in each Volume which stores datanodeUuid and some additional fields in that file.\r\n\r\nAnd also if this disk containing datanodeId\u00a0path is removed, that\u00a0DN will now be unusable with current code.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "alpha2"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Eliminate the datanode ID file"
   },
   {
      "_id": "13170081",
      "assignee": "elek",
      "components": [],
      "created": "2018-07-04 13:54:13",
      "description": "As the ozone release artifact doesn't contain a stable namenode/datanode code the hdfs command should be removed from the ozone artifact.\r\n\r\nozone-dist-layout-stitching also could be simplified to copy only the required jar files (we don't need to copy the namenode/datanode server side jars, just the common artifacts",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Remove hdfs command line from ozone distribution."
   },
   {
      "_id": "13170075",
      "assignee": "elek",
      "components": [],
      "created": "2018-07-04 13:43:23",
      "description": "Currently we use docker-compose files to run ozone pseudo cluster locally. After a full build, they can be found under hadoop-dist/target/compose.\r\n\r\nAs they are very useful, I propose to make them part of the ozone release to make it easier to try out ozone locally. \r\n\r\nI propose to create a new folder (docker/) in the ozone.tar.gz which contains all the docker-compose subdirectories + some basic README how they could be used.\r\n\r\nWe should explain in the README that the docker-compose files are not for production just for local experiments.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/4",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/minor.svg",
         "name": "Minor",
         "id": "4"
      },
      "projectname": "HDDS",
      "summary": "add existing docker-compose files to the ozone release artifact"
   },
   {
      "_id": "13167767",
      "assignee": "elek",
      "components": [],
      "created": "2018-06-22 22:52:00",
      "description": "The error message with an unrecognized option is unfriendly. E.g.\r\n{code}\r\n$ ozone oz -badOption\r\nUnrecognized option: -badOptionERROR: null\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
         "id": "4",
         "description": "An improvement or enhancement to an existing feature or task.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
         "name": "Improvement",
         "subtask": false,
         "avatarId": 21140
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "Improve shell error message for unrecognized option"
   },
   {
      "_id": "13163311",
      "assignee": "shashikant",
      "components": [],
      "created": "2018-05-31 23:39:00",
      "description": "When createVolume is invoked for a non-existent user, it logs a verbose warning for\u00a0{{PartialGroupNameException}}.\r\n{code:java}\r\n    hadoop@9a70d9aa6bf9:~$ ozone oz volume create --user=nosuchuser vol4\r\n    2018-05-31 20:40:17 WARN  ShellBasedUnixGroupsMapping:210 - unable to return groups for user nosuchuser\r\n    PartialGroupNameException The user name 'nosuchuser' is not found. id: \u2018nosuchuser\u2019: no such user\r\n    id: \u2018nosuchuser\u2019: no such user\r\n\r\n      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:294)\r\n      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:207)\r\n      at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\r\n      at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:51)\r\n      at org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupList(Groups.java:384)\r\n      at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:319)\r\n      at org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:269)\r\n      at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)\r\n      at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)\r\n      at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)\r\n      at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)\r\n      at com.google.common.cache.LocalCache.get(LocalCache.java:3965)\r\n      at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)\r\n      at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)\r\n      at org.apache.hadoop.security.Groups.getGroups(Groups.java:227)\r\n      at org.apache.hadoop.security.UserGroupInformation.getGroups(UserGroupInformation.java:1545)\r\n      at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1533)\r\n      at org.apache.hadoop.ozone.client.rpc.RpcClient.createVolume(RpcClient.java:190)\r\n      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n      at java.lang.reflect.Method.invoke(Method.java:498)\r\n      at org.apache.hadoop.ozone.client.OzoneClientInvocationHandler.invoke(OzoneClientInvocationHandler.java:54)\r\n      at com.sun.proxy.$Proxy11.createVolume(Unknown Source)\r\n      at org.apache.hadoop.ozone.client.ObjectStore.createVolume(ObjectStore.java:77)\r\n      at org.apache.hadoop.ozone.web.ozShell.volume.CreateVolumeHandler.execute(CreateVolumeHandler.java:98)\r\n      at org.apache.hadoop.ozone.web.ozShell.Shell.dispatch(Shell.java:395)\r\n      at org.apache.hadoop.ozone.web.ozShell.Shell.run(Shell.java:135)\r\n      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n      at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\r\n      at org.apache.hadoop.ozone.web.ozShell.Shell.main(Shell.java:114)\r\n    2018-05-31 20:40:17 INFO  RpcClient:210 - Creating Volume: vol4, with nosuchuser as owner and quota set to 1152921504606846976 bytes.\r\n{code}\r\nHowever the volume is\u00a0created:\r\n{code}\r\n$ ozone oz volume list --user=nosuchuser\r\n[ {\r\n  \"owner\" : {\r\n    \"name\" : \"nosuchuser\"\r\n  },\r\n  \"quota\" : {\r\n    \"unit\" : \"TB\",\r\n    \"size\" : 1048576\r\n  },\r\n  \"volumeName\" : \"vol4\",\r\n  \"createdOn\" : \"Thu, 31 May 2018 20:40:17 GMT\",\r\n  \"createdBy\" : \"nosuchuser\"\r\n} ]\r\n{code}",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie",
         "usability"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/1",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/blocker.svg",
         "name": "Blocker",
         "id": "1"
      },
      "projectname": "HDDS",
      "summary": "createVolume verbose warning with non-existent user"
   },
   {
      "_id": "13161589",
      "assignee": "hanishakoneru",
      "components": [],
      "created": "2018-05-23 21:09:05",
      "description": "VolumeSet would be responsible for managing volumes in the Datanode. Some of its functions are:\r\n # Initialize volumes on startup\r\n # Provide APIs to add/ remove volumes\r\n # Choose and return volume to calling service based on the volume choosing\u00a0policy (currently implemented Round Robin choosing policy)",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "ContainerIO"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Implement VolumeSet to manage disk volumes"
   },
   {
      "_id": "13159933",
      "assignee": "bharatviswa",
      "components": [],
      "created": "2018-05-17 05:56:09",
      "description": "According to refactoring of containerIO, ContainerData has common fields for different kinds of containerTypes, and each Container will extend ConatinerData to add its fields. So, for this merging ContainerStatus fields to ConatinerData.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
         "id": "7",
         "description": "The sub-task of the issue",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
         "name": "Sub-task",
         "subtask": true,
         "avatarId": 21146
      },
      "labels": [
         "reviewed"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Merge ContainerData and ContainerStatus classes"
   },
   {
      "_id": "13152822",
      "assignee": "xyao",
      "components": [],
      "created": "2018-04-16 20:49:54",
      "description": "Because Hdds\u00a0profile https://issues.apache.org/jira/browse/HDDS-61#is off by default, the links in the generated site will be invalid without specifying maven profile -Phdds.\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "newbie"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Ozone related doc links in hadoop-project/src/site/site.xml"
   },
   {
      "_id": "13096955",
      "assignee": "shashikant",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333902",
            "id": "12333902",
            "name": "Native",
            "description": "Native code components"
         }
      ],
      "created": "2017-08-23 08:35:12",
      "description": "This Jira is introduced for implementation of ozone client in C/C++ using curl library.\n\nAll these calls will make use of HTTP protocol and would require libcurl. The libcurl API are referenced from here:\nhttps://curl.haxx.se/libcurl/\n\nAdditional details would be posted along with the patches.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/1",
         "id": "1",
         "description": "A problem which impairs or prevents the functions of the product.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype",
         "name": "Bug",
         "subtask": false,
         "avatarId": 21133
      },
      "labels": [
         "OzonePostMerge"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Ozone: C/C++ implementation of ozone client using curl"
   },
   {
      "_id": "13157736",
      "assignee": "xyao",
      "components": [],
      "created": "2018-05-07 23:08:29",
      "description": "This is an umbrellas JIRA to fix unit test failures related or unrelated HDDS-1.",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/6",
         "id": "6",
         "description": "A new unit, integration or system test.",
         "iconUrl": "https://issues.apache.org/jira/images/icons/issuetypes/requirement.png",
         "name": "Test",
         "subtask": false
      },
      "labels": [
         "alpha2"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Fix Ozone Unit Test Failures"
   },
   {
      "_id": "13151737",
      "assignee": "bharatviswa",
      "components": [
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333903",
            "id": "12333903",
            "name": "Ozone Datanode",
            "description": "Datanode Code - Plugin and Rest Server"
         },
         {
            "self": "https://issues.apache.org/jira/rest/api/2/component/12333908",
            "id": "12333908",
            "name": "SCM",
            "description": "HDDS block Manager"
         }
      ],
      "created": "2018-04-11 19:04:30",
      "description": "From chillmode\u00a0Deisgn Notes:\r\n\r\nAs part of this Jira, will update register to send NodeReport and ContaineReport.\r\n\r\nCurrent Datanodes, send one heartbeat per 30 seconds. That means that even if the datanode is ready it will take around a 1 min or longer before the SCM sees the datanode container reports. We can address this partially\u00a0by making sure that Register call contains both NodeReport and ContainerReport.\r\n\r\n\u00a0\r\n\r\n\u00a0",
      "issuetype": {
         "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
         "id": "3",
         "description": "A task that needs to be done.",
         "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
         "name": "Task",
         "subtask": false,
         "avatarId": 21148
      },
      "labels": [
         "reviewed"
      ],
      "priority": {
         "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
         "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
         "name": "Major",
         "id": "3"
      },
      "projectname": "HDDS",
      "summary": "Send NodeReport and ContainerReport when datanodes register"
   }
]