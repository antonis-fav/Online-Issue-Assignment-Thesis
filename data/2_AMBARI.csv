id,summary,description,labels,components_name,project_name,issue_type_name,priority_name,created_date,assignee
13282971,After node reboot autostart of components takes too much time.,"The reason for this is because ambari-agent does not know start order. And so
if INFRA_SOLR with timeout of 10 hours starts before ZOOKEEPER, which it
requires, it’s a big problem. As well as it blocks agent queue for any other
commands. The solution is to run the commands in parallel. Also for fast
autostart users might want to disable retry_gap: {noformat}/var/lib/ambari-
server/resources/scripts/configs.py -a set -l localhost -n cluster_name -u
admin -p admin -c cluster-env -k retry_gap_in_sec -v 0{noformat}

",pull-request-available,[],AMBARI,Bug,Major,2020-02-03 12:16:34,0
13281670,Components autostart does not work sometimes after ambari-agent restart,"If configurations where cached and didn't change during restart of agent (for some people they always change, they will not see the issue). Recovery didn't get enabled. Until some changes configs/topology changes where done.",pull-request-available,[],AMBARI,Bug,Major,2020-01-27 09:41:53,0
13280160,Ambari doesn't show versions page after invalid repo was added,"Steps to reproduce:
 # Deploy cluster.
 # Add repository version with ""****:****"" as credentials (set skip validation before saving).
 # Try to open versions page.

ambari-server log:
{code:java}
2020-01-18 00:45:14,915 ERROR [ambari-client-thread-388] ReadHandler:99 - Bad request:
 java.util.regex.PatternSyntaxException: Dangling meta character '*' near index 0
 *****:*****
 ^
     at java.util.regex.Pattern.error(Pattern.java:1955)
     at java.util.regex.Pattern.sequence(Pattern.java:2123)
     at java.util.regex.Pattern.expr(Pattern.java:1996)
     at java.util.regex.Pattern.compile(Pattern.java:1696)
     at java.util.regex.Pattern.<init>(Pattern.java:1351)
     at java.util.regex.Pattern.compile(Pattern.java:1028)
     at java.lang.String.replaceFirst(String.java:2178)
     at org.apache.ambari.server.utils.URLCredentialsHider.hideCredentials(URLCredentialsHider.java:48)
     at org.apache.ambari.server.controller.internal.RepositoryResourceProvider.getResources(RepositoryResourceProvider.java:182)
     at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java: 965)
     at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:139)
     at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:529)
     at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:482)
     at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:503)
     at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:503)
     at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:454)
     at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:222)
     at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:77)
     at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
     at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:164)
     at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:128)
     at org.apache.ambari.server.api.services.ClusterStackVersionService.getClusterStackVersions(ClusterStackVersionService.java:68)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2020-01-17 14:57:20,1
13274731,[ubuntu16] HDP install failed for upgrade from  HDP-3.0.1.0-187 to HDP-3.1.5.0-139,"Package Manager failed to install packages: No package found for
sqoop-$\\{stack_version}(expected name: sqoop-3-1-5-0-139) Traceback (most
recent call last): File ""/var/lib/ambari-
agent/cache/custom_actions/scripts/install_packages.py"", line 493, in
InstallPackages().execute() File ""/usr/lib/ambari-
agent/lib/resource_management/libraries/script/script.py"", line 352, in
execute method(env) File ""/var/lib/ambari-
agent/cache/custom_actions/scripts/install_packages.py"", line 156, in
actionexecute raise Fail(""Failed to distribute repositories/install packages"")
resource_management.core.exceptions.Fail: Failed to distribute
repositories/install packages Live Cluster :
[+http://172.27.120.193:8080/+|http://172.27.120.193:8080/] ambari-server
version : 2.7.5.0-67

",pull-request-available,[],AMBARI,Bug,Major,2019-12-16 10:49:10,0
13273864,Credentials should not be shown on cleartext on Ambari UI,"Please see screenshot attached. Ambari UI - Stack and Versions page shows
username and password in cleartext . We should atleast hide the password Also
in Review page for UI install cc [~accountid:557058:af856db7-a0ad-
4e2b-b848-f11f481bf96f] / [~accountid:5dc59258b6e6b50c58af136b] /
[~accountid:557058:e797d548-8a74-4f63-a68d-616111201b1c]

",pull-request-available,[],AMBARI,Bug,Major,2019-12-11 12:16:50,0
13273682,VDF registration fails with SunCertPathBuilderException: unable to find valid certification path to requested target' on HTTPS cluster ,"Retrying deploy with latest build after BUG-122455 was fixed Here Stack
registration is failing with {code} An internal system exception occurred:
Could not load url from https://1055c7c3-1b7b-
43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-test.cloudera.com/p/HDP-
GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml.
sun.security.validator.ValidatorException: PKIX path building failed:
sun.security.provider.certpath.SunCertPathBuilderException: unable to find
valid certification path to requested target {code} 172.27.74.1 is a live node
where issue has occurred.

",pull-request-available,[],AMBARI,Bug,Major,2019-12-10 18:24:38,0
13273376,Deploy fails with 401:Unauthorized on HDP-GPL,"We are trying to deploy with paywalled repos of HDP and Ambari (ones supplied
in RELENG-7654) but deploy is failing at client install with {code}
RuntimeError: Failed to execute command '/usr/bin/yum -y install hdp-select',
exited with code '1', message: 'https://****:****@archive-test.cloudera.com/p
/HDP-GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml: [Errno 14] HTTPS
Error 401 - Unauthorized {code} This url is accessible with credentials RE has
supplied ie; https://1055c7c3-1b7b-
43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-test.cloudera.com/p/HDP-
GPL/centos7/3.x/BUILDS/3.1.5.0-139/repodata/repomd.xml So not sure which
credential is Ambari picking as from the error it looks like the credentials
Ambari is using on this repo is incorrect. Could you please take a look Live
cluster : http://172.27.136.132:8080/ Repo urls used here for deploy: Ambari :
https://1055c7c3-1b7b-43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-
test.cloudera.com/p/ambari/2.7.5.0-64/centos7/ambaribn.repo HDP:
https://1055c7c3-1b7b-43f6-80a7-3e72cb0f8912:Cloudera_TechPreview2@archive-
test.cloudera.com/p/HDP/centos7/3.x/BUILDS/3.1.5.0-139 cc
[~accountid:557058:af856db7-a0ad-
4e2b-b848-f11f481bf96f]/[~accountid:5dc59258b6e6b50c58af136b]

",pull-request-available,[],AMBARI,Bug,Major,2019-12-09 13:21:41,0
13271936,Server sets blueprint_provisioning_state for component to finished before start command execution,"Server sends host level parameters update with blueprint_provisioning_state=""FINISHED"" right after start execution command generation. This may led to performing of autostart command on the agent side before execution of start command.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-12-03 12:40:00,1
13271267,No Validation error on UI for an 'Unauthorized' repo url,UI should show error for URLs if credentials are required but missed there.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-11-28 17:12:06,1
13271187,Ambari should add login and password to urls populated from VDF,"This was pointed out in https://cloudera.slack.com/archives/CQYUPAV7A chat and
we decided that it would be un-secure for RelEng to store login/password
inside url's in VDF's, hence ambari should add that to url's itself.

",pull-request-available,[],AMBARI,Bug,Major,2019-11-28 11:03:40,0
13271181,Ambari should add login and password to urls populated from VDF,"This was pointed out in https://cloudera.slack.com/archives/CQYUPAV7A chat and
we decided that it would be un-secure for RelEng to store login/password
inside url's in VDF's, hence ambari should add that to url's itself.

",pull-request-available,[],AMBARI,Bug,Major,2019-11-28 10:36:47,0
13268296,Configure heartbeat timeout,Make heartbeat timeout configurable to prevent it loosing on slow or overloaded instances.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-11-14 12:29:57,1
13267160,Cannot use HTTPS repourl and VDF url,"Using VDF https://<user>:<pass>@<pathToXML>
results in:
{noformat}
An internal system exception occurred: Could not load url from https://<user>:<pass>@<pathToXML>. Can't get secure connection to https://****:****@<pathToXML>. Truststore path or password is not set.{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-11-08 18:07:59,1
13265790,Support basic auth for repositories,"User should be able to provide credentials for basic authorization inside repo base URL:

{code}

http://user:password@<host>:<port>/...

{code}

Also server should mask the credentials in URLs from API responses and logs.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2019-11-01 16:13:56,1
13261131,Backport: Performance Tune Hosts and service Configs Pages to 2.7.5,Backport for AMBARI-24842 and [AMBARI-24876|https://hortonworks.jira.com/issues/?jql=project+in+%2810320%2C+11620%2C+11320%2C+10520%29+AND+cf%5B11018%5D+%3D+AMBARI-24876] into ambari-2.7.5.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-10-08 14:40:22,1
13260554,Ambari is indexing all subdirectories contents under /resources folder via API.,"GET /resources gives back the directory content of /var/lib/ambari-server/resources. The directory doesn't contain any sensitive information, only files which are already visible on github. But it might freak out security guys therefore it's best to disable the listing.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-10-04 11:03:28,0
13259633,dfs_ha_initial_* properties should be removed during upgrade,"NameNode HA cluster was created via blueprint with specifying 'dfs_ha_initial_namenode_active' and 'dfs_ha_initial_namenode_standby' properties for hadoop-env with invalid values 'None'.
Afterwards ambari upgrade from 2.6.* to 2.7.5 was failed with following stacktrace:
{code}
ERROR [main] SchemaUpgradeHelper:240 - Upgrade failed. 
java.lang.IllegalArgumentException: NAMENODE HA hosts mapped incorrectly for properties 'dfs_ha_initial_namenode_active' and 'dfs_ha_initial_namenode_standby'. Expected hosts are: [<nnHAHost1>, <nnHAHost2>]
	at org.apache.ambari.server.topology.ClusterTopologyImpl.validateTopology(ClusterTopologyImpl.java:221)
	at org.apache.ambari.server.topology.ClusterTopologyImpl.<init>(ClusterTopologyImpl.java:79)
	at org.apache.ambari.server.topology.PersistedStateImpl.getAllRequests(PersistedStateImpl.java:217)
	at org.apache.ambari.server.topology.TopologyManager.ensureInitialized(TopologyManager.java:205)
	at org.apache.ambari.server.topology.TopologyManager.getPendingHostComponents(TopologyManager.java:819)
	at org.apache.ambari.server.utils.StageUtils.getClusterHostInfo(StageUtils.java:306)
	at org.apache.ambari.server.controller.KerberosHelperImpl.addAdditionalConfigurations(KerberosHelperImpl.java:2961)
	at org.apache.ambari.server.controller.KerberosHelperImpl.calculateConfigurations(KerberosHelperImpl.java:1723)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1630)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1060)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:238)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:458)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-09-30 10:17:12,1
13258779,Reduce cluster creation request processing time,"Identify where time is spent during processing of blueprint-based cluster creation request.
{noformat:title=HDInsight cluster with 7 nodes (14 seconds)}
19 Dec 2016 09:41:58,225  INFO [ambari-client-thread-45] AbstractResourceProvider:518 - Creating Cluster 'hdfs-toader-1219-dev' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 09:42:12,015  INFO [pool-18-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.
{noformat}
{noformat:title=OpenStack cluster with 8 nodes (9 seconds)}
19 Dec 2016 11:17:30,640  INFO [ambari-client-thread-23] AbstractResourceProvider:518 - Creating Cluster 'TEST' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 11:17:39,875  INFO [pool-17-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-09-25 11:40:56,1
13256939,UI does not reflect/update task logs ,"Steps to reproduce

--------------------
 # Install Ambari 2.7.x and install a cluster
 # start/install any service and navigate to any of the task before starting.
 # Wait till the task is finished ( showing completed icon on top) 
 # observe there is no task logs getting updated but it is completed refer: !Screen Shot 2019-09-16 at 11.01.39 AM.png!
 # now go back one step up and come back again.. now it shows task logs:
 # !Screen Shot 2019-09-16 at 11.01.49 AM.png!",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-09-16 18:08:43,1
13253919,SSTI in Ambari config,"Hello, i found SSTI to RCE vulnerability in Apache ambari, and i send three mails with technical details to  [private@ambari.apache.org, |mailto:private@ambari.apache.org] [security@apache.org and |mailto:private@ambari.apache.org] [root@apache.org|mailto:private@ambari.apache.org]

In dates: 4 jul 2019, 8 aug 2019, and 24 aug 2019, but you not responce, bug is critical, and i want cve approve, help me?

best regards",vulnerability,['ambari-admin'],AMBARI,Bug,Critical,2019-08-30 12:09:05,1
13253304,"CLONE - Ambari audit log shows ""null"" user when executing an API call as admin - Ambari 2.6.2","When running a simple REST API call from CLI, I could see two entries in ambari-audit.log file.

 

Following is my API call:

{{curl -k -i -u admin:<passwd> -H ""X-Requested-By: ambari"" -X GET [http://<ambari-host>:8080/api/v1/clusters|http://saurabh-ambari:8080/api/v1/clusters]}}

 

Following are the 2 entries in ambari-audit.log:
{quote}2019-04-08T10:19:04.991Z, User(null), RemoteIp(x.x.x.x), Operation(User login), Roles(
 ), Status(Failed), Reason(Authentication required), Consecutive failures(UNKNOWN USER)
 2019-04-08T10:19:04.999Z, User(admin), RemoteIp(x.x.x.x), Operation(User login), Roles(
     Ambari: Ambari Administrator
 ), Status(Success)
{quote}
 

The second line seems to be valid. However, the first line (with the null user) shouldn't be there.

Note: I'm not sure if it helps, but the cluster is Kerberized and Knox isn't involved.

 

Edit: This issue could be seen on both Ambari 2.5.2 and 2.7.3. Also, 2.5.2 version cluster is Kerberized, the 2.7.3 version is NOT Kerberized. ",newbie pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2019-08-27 17:08:08,1
13251844,Daily Namenode Heap Usage alert does not work,"Daily NameNode Heap Usage alert has *Unknown* status with *There are not enough data points to calculate the standard deviation (0 sampled)* response.
It is caused by invalid case of *appId* alert property in alert definition.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-08-20 13:43:06,1
13250764,Hive Service Check fails during Rolling Upgrade from HDP-3.1.0.0 to HDP-3.1.4.0,"Hive Service Check fails during Rolling Upgrade from HDP-3.1.0.0 to
HDP-3.1.4.0-301 {code} 2019-08-11 17:24:29,680 - Using hadoop conf dir:
/usr/hdp/3.1.4.0-301/hadoop/conf 2019-08-11 17:24:29,698 - call['ambari-
python-wrap /usr/bin/hdp-select status hive-server2'] {'timeout': 20}
2019-08-11 17:24:29,732 - call returned (0, 'hive-server2 - 3.1.0.0-78')
2019-08-11 17:24:29,734 - Stack Feature Version Info: Cluster Stack=3.1,
Command Stack=None, Command Version=3.1.4.0-301, Upgrade Direction=upgrade ->
3.1.4.0-301 2019-08-11 17:24:29,775 - File['/var/lib/ambari-
agent/cred/lib/CredentialUtil.jar'] {'content':
DownloadSource('http://ctr-e141-1563959304486-21229-01-000007.hwx.site:8080/resources/CredentialUtil.jar'),
'mode': 0755} 2019-08-11 17:24:29,777 - Not downloading the file from
http://ctr-e141-1563959304486-21229-01-000007.hwx.site:8080/resources/CredentialUtil.jar,
because /var/lib/ambari-agent/tmp/CredentialUtil.jar already exists 2019-08-11
17:24:30,749 - Running Hive Server checks 2019-08-11 17:24:30,749 -
-------------------------- 2019-08-11 17:24:30,752 - Server Address List :
[u'ctr-e141-1563959304486-21229-01-000002.hwx.site',
u'ctr-e141-1563959304486-21229-01-000004.hwx.site'], Port : 10000, SSL
KeyStore : None 2019-08-11 17:24:30,752 - Waiting for the Hive Server to
start... 2019-08-11 17:24:30,753 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:30,863 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:36,712 - Successfully connected to
ctr-e141-1563959304486-21229-01-000002.hwx.site on port 10000 2019-08-11
17:24:36,713 - Successfully stayed connected to 'Hive Server' on host:
ctr-e141-1563959304486-21229-01-000007.hwx.site and port 10000 after
5.96049404144 seconds 2019-08-11 17:24:36,713 - Running Hive Server2 checks
2019-08-11 17:24:36,713 - -------------------------- 2019-08-11 17:24:36,719 -
Server Address List : [u'ctr-e141-1563959304486-21229-01-000002.hwx.site'],
Port : 10500, SSL KeyStore : None 2019-08-11 17:24:36,719 - Waiting for the
Hive Server2 to start... 2019-08-11 17:24:36,719 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:36,834 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:42,100 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:24:47,106 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:47,225 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:24:52,456 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:24:57,462 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:24:57,582 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:02,793 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:07,798 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:07,919 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:13,260 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:18,265 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:18,360 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:23,658 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:28,664 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:28,772 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:34,144 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:39,146 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:39,264 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:44,573 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:49,579 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:25:49,694 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:25:54,972 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:25:59,977 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:00,094 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:05,416 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:10,422 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:10,539 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:15,973 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:20,979 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:21,090 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:26,397 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:31,401 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:31,516 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:36,865 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:41,866 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:41,983 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:47,237 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:26:52,243 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:26:52,362 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:26:57,704 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:02,710 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:02,805 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:08,094 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:13,099 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:13,215 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:18,498 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:23,504 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:23,622 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:28,949 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:33,953 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:34,071 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:39,277 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:44,283 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:44,399 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:27:49,720 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:27:54,726 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:27:54,846 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:00,106 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:05,112 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:05,229 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:10,652 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:15,658 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:15,778 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:21,134 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:26,140 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:26,266 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:31,610 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:36,616 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:36,734 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:42,040 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:47,045 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:47,161 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:28:52,510 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:28:57,516 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:28:57,633 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:02,939 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:29:07,945 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:29:08,064 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:13,369 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed 2019-08-11 17:29:18,374 - Execute['/usr/bin/kinit -kt
/etc/security/keytabs/smokeuser.headless.keytab ambari-qa@EXAMPLE.COM; ']
{'user': 'ambari-qa'} 2019-08-11 17:29:18,475 - Execute['beeline -u
'jdbc:hive2://ctr-e141-1563959304486-21229-01-000002.hwx.site:10500/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM'
-n cstm-hive -e ';' 2>&1 | awk '{print}' | grep -i -e 'Connected to:' -e
'Transaction isolation:' -e 'inactive HS2 instance; use service discovery'']
{'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'],
'timeout_kill_strategy': 2, 'timeout': 30, 'user': 'ambari-qa'} 2019-08-11
17:29:23,822 - Connection to ctr-e141-1563959304486-21229-01-000002.hwx.site
on port 10500 failed Command failed after 1 tries {code} Ambari was upgraded
from 2.7.3.0 to 2.7.4.0

",pull-request-available,[],AMBARI,Bug,Major,2019-08-14 11:04:52,0
13244312,Reduce cluster creation request processing time,"Identify where time is spent during processing of blueprint-based cluster creation request.
{noformat:title=HDInsight cluster with 7 nodes (14 seconds)}
19 Dec 2016 09:41:58,225  INFO [ambari-client-thread-45] AbstractResourceProvider:518 - Creating Cluster 'hdfs-toader-1219-dev' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 09:42:12,015  INFO [pool-18-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.
{noformat}
{noformat:title=OpenStack cluster with 8 nodes (9 seconds)}
19 Dec 2016 11:17:30,640  INFO [ambari-client-thread-23] AbstractResourceProvider:518 - Creating Cluster 'TEST' based on blueprint 'hadoop_ha'.
...
19 Dec 2016 11:17:39,875  INFO [pool-17-thread-1] AmbariContext:441 - All required configuration types are in the TOPOLOGY_RESOLVED state.  Blueprint deployment can now continue.{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-07-11 13:02:31,1
13243232,[HA] RESOURCEMANAGER is not starting after adding and removing journal nodes,"Adding and removing the Journal nodes, Resource manager is not starting",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-07-04 15:19:25,2
13239192,Request configurations when needed during server-side actions rather than rely on configuration data from the execution command,"Request configurations when needed during server-side actions rather than rely on configuration data from the execution command.

Due to a recent change, which appeared to remove configuration data from the execution command JSON document, data needed for Kerberos-related service-side actions is missing. This data may be requested when needed from the cluster data at the time of execution rather than when setting up the stages.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-06-13 07:14:48,3
13239030,Cluster Information Page URL is broken,"Cluster Information Page URL is broken 
Before install HDP, login to ambari. The first page you will be navigated to is Cluster Information Page. The URL here is not what is expected

Expected: views/ADMIN_VIEW/2.7.4.0/INSTANCE/#/clusterInformation
Actual: views/ADMIN_VIEW/2.7.4.0/INSTANCE/#!/clusterInformation#%2F

",pull-request-available,['ambari-admin'],AMBARI,Bug,Blocker,2019-06-12 13:24:35,2
13237800,"Druid requires HDFS_CLIENT to be co-hosted, doesn't recognize ONEFS_CLIENT","While deploying Druid with OneFS using the Ambari OneFS mpack, an error is emitted while assigning slaves and clients. If Druid Historical or Druid MiddleManager is assigned, the error reads ""Druid [component] requires HDFS_CLIENT to be co-hosted"". If either is NOT assigned, the error reads ""You have selected 0 Druid [component] components. Please consider that at least 1 Druid [component] components should be installed in cluster.""

 

See the attached images.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-06-05 19:42:19,3
13235737,Persistent Cross Site Scripting (XSS) in Ambari,"Below is the HTTP Request and Response issued when a user submits a note containing a JavaScript
after modifying some configuration in ""Tez"" service.
HTTP Request:
PUT /api/v1/clusters/<env> HTTP/1.1
Host: xyz601:8080
Content-Length: 199
Accept: application/json, text/javascript, /; q=0.01
Origin: http://xyz601:8080
X-Requested-With: XMLHttpRequest
X-Requested-By: X-Requested-By
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/70.0.3538.102 Safari/537.36
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
Referer: http://xyz:8080/
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.9
Cookie: AMBARISESSIONID=vfiy4336mxwl1k5ehd6jrz43i
Connection: close
{""Clusters"":{""desired_service_config_versions"":

{""service_config_version"":4,""service_name"":""TEZ"",""service_config_version_note"":""Creat ed from service config version V4\n<img src=x onerror=alert(1)>""}
}}

Remediation Recommendations
Restrict all input passed to the application to valid, whitelisted content, and ensure that all
response/output sent by the server is HTML/URL/JavaScript encoded, depending on the context in
which the data is used by the application.
The remediation should not attempt to blacklist content and remove, filter, or sanitize it. There are
too many types of encoding it to get around filters for such content.
We strongly recommend a positive security policy that specifies what is allowed.
Negative or attack signature based policies are difficult to maintain and are likely to be incomplete.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-05-27 07:45:37,4
13234567,"Ambari UI evaluates Javascript embedded in user input when adding hosts, adding remote clusters, and renaming the cluster","Ambari's UI evaluates Javascript blocks embedded in user input when adding hosts, adding remote clusters, and renaming the cluster.

The script evaluation appears to occur before the data is submitted and saved to the Ambari database (if save at all).  Therefore, no XSS vulnerability needs to be reported since the scope of the threat is only to the interactive user at the instance the data is evaluated.

*Add remote cluster steps to reproduce:*
# Log into ambari and navigate to admin > Manage Ambari> Cluster Management>  Remote Cluster > Register Remote Cluster
# Enter malicious script in Ambari Cluster URL textbox and click on save. The output of XSS is reflected. 

*Add hosts steps to reproduce:*
# Log into ambari and navigate to Hosts> Actions>  Add New Hosts
# Enter malicious script in Target Hosts textbox and click on save. The output of XSS is reflected

*Edit cluster name steps to reproduce:*
# Log into ambari and navigate to admin > Manage Ambari> Cluster Management>  Cluster Information
# Enter malicious script in Cluster Name textbox. The output of XSS is reflected",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2019-05-21 10:36:37,2
13234519,Hive View API response contains plain-text password,Getting response from http://c7401:8080/api/v1/views/HIVE/versions/2.0.0/instances/AUTO_HIVE20_INSTANCE contains plain text password (hive.ranger.password). This only effects auto created views and default passwords.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-05-21 07:47:53,3
13231723,Ambari-server 2.7.3 uninstall removes ambari-python-wrap even when agent is still installed,"There is a erroneous equal sign in the following expression:

{code}
AMBARI_AGENT_ROOT_DIR==""${ROOT}/usr/lib/ambari-agent""
{code}

in /var/lib/ambari-server/install-helper.sh. The double equal sign causes the AMBARI_AGENT_ROOT_DIR to be evaluated to ""=/usr/lib/ambari-agent"" (note the leading =).",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-05-06 08:58:42,3
13231484,"Admin View build fails due to unavailable npm package ""ecstatic""","{{ambari-admin}} build started failing with:

{noformat:title=https://builds.apache.org/job/Ambari-branch-2.7/494/consoleText}
[ERROR] npm ERR! notarget No compatible version found: ecstatic@'>=0.4.0 <0.5.0'
[ERROR] npm ERR! notarget Valid install targets:
[ERROR] npm ERR! notarget [""4.1.2""]
...
[ERROR] npm ERR! notarget It was specified as a dependency of 'http-server'
{noformat}

It seems old versions of [ecstatic|https://www.npmjs.com/package/ecstatic?activeTab=versions] have been [removed|https://github.com/jfhbrook/node-ecstatic/issues/255] due to a security issue.",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2019-05-03 16:58:04,5
13231418,Hive Server Interactive process alert triggered in HA setup,"With 2 HSI instances deployed in HA setup the _HiveServer2 Interactive Process_ alert is triggered for the inactive one:

{noformat}
Connecting to jdbc:hive2://...:10501/;transportMode=http;httpPath=cliservice
19/05/02 09:13:11 [main]: WARN jdbc.HiveConnection: Failed to connect to ...:10501
Error: Could not open client transport with JDBC Uri: jdbc:hive2://...:10501/;transportMode=http;httpPath=cliservice: Cannot open sessions on an inactive HS2 instance; use service discovery to connect (state=08S01,code=0)
Cannot run commands specified using -e. No current connection
{noformat}",pull-request-available,[],AMBARI,Bug,Major,2019-05-03 09:10:27,5
13229346,Service checks mentioned in HOU upgrade plan are run on the same node that was upgraded,"Now service checks are performed for any appropriate host during HOU. We should tune behaviour without API change to add some host selection restrictions.
{code:java}
[

{ ""hosts"": [""host1""], ""service_checks"": [""KAFKA"", ""ZOOKEEPER""] }
,

{ ""hosts"": [""host2""], ""service_checks"": [""ZOOKEEPER""] }
,

{ ""hosts"": [""host3"", ""host4""], ""service_checks"": [""CUSTOMSERVICE""] }
]
{code}
 - Each service check will be performed for all hosts from hosts section. e.g. CUSTOMSERVICE check will be run on both host3 and host4
 - In case there is no service component on proposed host the upgrade request will be failed. This means that user should change upgrade plan and post it again.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-04-22 09:24:58,1
13228838,upgrade moment.js to v2.22.2,The application is running a vulnerable version of Moment.js.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-04-18 11:19:32,2
13228348,Missed support for abfs protocol,"There are still present couple of places that require extending default protocol set with abfs protocol:
 - Pig and File views check *webhdfs.url* property, which contains protocol name, but only for non-local view's configuration. List of allowed protocols is loaded from ambari.properties.
 - ranger hive plugin property *ranger.plugin.hive.urlauth.filesystem.schemes*. We set default value during stack upgrade to HDP 2.6 stack.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-04-16 13:36:40,1
13228316,Disable directory Indexing at /resources,"GET /resources gives back the directory content of /var/lib/ambari-server/resources. The directory doesn't contain any sensitive information, only files which are already visible on github. But it might freak out security guys therefore it's best to disable the listing.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-04-16 10:49:11,3
13228253,Blueprint processor should support multiple ZooKeeper nodes for livy.server.recovery.state-store.url,"Livy can store its state in ZooKeeper for recovery ({{""livy.server.recovery.state-store"": ""zookeeper""}}).  In this case {{livy.server.recovery.state-store.url}} should point to the ZooKeeper quorum.

Setting {{""livy.server.recovery.state-store.url"": ""%HOSTGROUP::quorum%:2181""}} should be enough to specify ZK servers in a host group named {{quorum}}.  However, since it is not explicitly handled in {{BlueprintConfigurationProcessor}}, the placeholder is replaced only with a single address.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2019-04-16 06:19:53,5
13228123,Hive service check is failing after moving Hive Metastore from node to another using system tests.,hive-interactive-site/hive.metastore.uris is not updated after move of Hive Metastore via wizard,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2019-04-15 14:48:19,2
13227694,Tez/MR service check fails with ClassNotFoundException LzoCodec during host ordered upgrade,"ClassPath used by on the client side is: /usr/hdp/<NEW.HDP.VERSION>/hadoop/lib/hadoop-lzo-0.6.0.${NEW.HDP.VERSION}.jar

LzoPackage is installed lazily therefore it's not available at the server side.",pull-request-available,[],AMBARI,Task,Major,2019-04-12 12:02:35,3
13226921,Host Ordered Upgrade: Pre Upgrade check of fs.defaultFS fails for ABFS,"# Install cluster with Ambari- 2.6.2.26 + HDP-2.6.5.3006
 # Set abfs protocol for mapreduce.application.framework.path/fs.defaultFS/tez.lib.uris properties.
 # Disable Service Auto Start
 # Register and Install stack HDP-2.6.5.3009
 # Run preupgrade checks. They fail with
{code:java}
MapReduce should reference Hadoop libraries from the distributed cache in HDFS
Reason: The mapred-site.xml property mapreduce.application.framework.path or the core-site.xml property fs.defaultFS should point to *dfs:/ url.
Failed on: MAPREDUCE2 Tez should reference Hadoop libraries from the distributed cache in HDFS
Reason: The tez-site.xml property tez.lib.uris or the core-site.xml property fs.defaultFS should point to *dfs:/ url.
Failed on: TEZ
{code}
For ABFS, fs.defaultFS is
{code:java}
abfs://<hostname>
{code}
Not seeing this issue for WASB where the value was
{code:java}
wasb://<hostname>
{code}
Also, we need to check if there are other places where we have this check, that will break too",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2019-04-09 11:30:16,1
13226920,Add a sysprep configurations to run conf-selects only a single time	,"Background:

On the cluster with 5-6 nodes conf-select cycles takes 3 seconds. Trying to create conf-dir for every single components. Which makes install stages like ~25-30seconds for hosts with a lot of clients.

Usually we run conf-select during every task. Since with every single install new packages might appear and new conf-select links will be required.

Solution:

If host_sysprep is true, run conf-select cycle only during first task and than create a file indicating this was done, so other tasks skip.

Since all packages are preinstalled this should work fine as during first single run it will be possible to create all links.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-04-09 11:20:53,1
13225573,Unable to move Hive metastore from one node to another ,"While moving Hive Metastore from one node to another, it is failing at test DB connection step in configure Component page.Seeing following error in ambari-agent logs:

{noformat}19-03-22 11:15:38,527 - DB connection check started.
2019-03-22 11:15:38,527 - There was an unknown error while checking database connectivity: Configuration parameter 'db_name' was not found in configurations dictionary!
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py"", line 145, in actionexecute
    db_connection_check_structured_output = self.execute_db_connection_check(config, tmp_dir)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/check_host.py"", line 281, in execute_db_connection_check
    if db_name == DB_MYSQL:
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'db_name' was not found in configurations dictionary!
2019-03-22 11:15:38,528 - Host checks completed.
2019-03-22 11:15:38,529 - Check db_connection_check was unsuccessful. Exit code: 1. Message: Configuration parameter 'db_name' was not found in configurations dictionary!

Command failed after 1 tries
{noformat}

Reproduce steps:
# Goto Hive Service.
# Click on Actions tab and select Move Hive Metastore.
# Fill appropriate values in Move Wizard.
# Before going to configure component page from Review page, please run following ambari-server command on ambari server host:
{{ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar}}
# Click next on review page, it will fail at test DB connection step in Configure component page.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2019-04-02 15:59:30,2
13225463,tproxy-enabled flag is not used by ambari-server setup-trusted-proxy,"When running

ambari-server setup-trusted-proxy --tproxy-enabled=true

ambari still asks the user interactively about enabling/disabling tproxy

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-04-02 10:40:21,3
13223693,ONEFS installation via blueprint fails,"error message comes from SingleHostTopologyUpdater

{code}
Logical Request: Provision Cluster 'hadoop5832' FAILED: Unable to update configuration property 'fs.defaultFS' with topology information. Component 'NAMENODE' is mapped to an invalid number of host groups '0'.
{code}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-03-25 08:19:59,3
13222987,hdfs_to_onefs_convert.py script missing HDP3.1 + upgrade logic,"hdfs to onefs convert script used to upgrade HDP2.6.5 to HDP3.0.1 stack is missing HDP3.1.* stack support. Please add HDP3.1.* support as well make it generic for all future stacks.

!hdfs to onefs issue.jpg!",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-21 02:57:35,3
13222277,"Ambari does not update ""dfs.namenode.lifeline.rpc-address"" during Namenode move operation","While performing Namenode (NN) move from one host to another. And if ""dfs.namenode.lifeline.rpc-address"" property is set., during the NN move, we see that the value associated with the property ""dfs.namenode.lifeline.rpc-address"" that is being moved is not updated. Hence, NN would fail to start during restart stage of the NN Move operation.

This can be reproduced as below,

1. Have a HDFS HA cluster. 
2. Make sure ""dfs.namenode.lifeline.rpc-address"" is set for both the NNs.
3. Perform the NN move operation from any NN to new host.
4. Observe NN failure to startup during restart phase",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-03-18 10:41:21,3
13222274,MYSQL connector exception while upgrading ambari to ambari-2.7.3,"h2. Description
Problem statement : ambari-server upgrade command is failing with below exception in ambari-server.log If using mysqlconnector-8.0.1.x.jar
{code:java}
2018-12-28 13:59:07,062  INFO [main] DBAccessorImpl:869 - Executing query: CREATE TABLE ambari_configuration (category_name VARCHAR(100) NOT NULL, property_name VARCHAR(100) NOT NULL, property_value VARCHAR(2048)) ENGINE=INNODB
2018-12-28 13:59:07,087 ERROR [main] SchemaUpgradeHelper:209 - Upgrade failed.
java.sql.SQLSyntaxErrorException: Unknown table 'ambari_configuration' in information_schema
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
  at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
  at com.mysql.cj.jdbc.StatementImpl.executeQuery(StatementImpl.java:1218)
  at com.mysql.cj.jdbc.DatabaseMetaData$7.forEach(DatabaseMetaData.java:2965)
  at com.mysql.cj.jdbc.DatabaseMetaData$7.forEach(DatabaseMetaData.java:2953)
  at com.mysql.cj.jdbc.IterateBlock.doForAll(IterateBlock.java:56)
  at com.mysql.cj.jdbc.DatabaseMetaData.getPrimaryKeys(DatabaseMetaData.java:3006)
  at org.apache.ambari.server.orm.DBAccessorImpl.tableHasPrimaryKey(DBAccessorImpl.java:1086)
  at org.apache.ambari.server.orm.DBAccessorImpl.addPKConstraint(DBAccessorImpl.java:577)
  at org.apache.ambari.server.orm.DBAccessorImpl.addPKConstraint(DBAccessorImpl.java:588)
  at org.apache.ambari.server.upgrade.UpgradeCatalog270.addAmbariConfigurationTable(UpgradeCatalog270.java:989)
  at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:319)
  at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:970)
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:207)
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
2018-12-28 13:59:07,093 ERROR [main] SchemaUpgradeHelper:475 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: Unknown table 'ambari_configuration' in information_schema
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:210)
  at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:450)
Caused by: java.sql.SQLSyntaxErrorException: Unknown table 'ambari_configuration' in information_schema
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)
  at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)
  at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)
{code}
Supportmatrix says ambari supports mysql-5.7 version. but its not mentioning anything about connector jar or connector.jar version.

Currently there is this exception in Ambari upgrade is mysqlConnector jar version is 8.0.15 .

If ambari doesnt support mysqlConnector.jar version 8.x this should be documented in hortonworks docs and also in supportmatrix website.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-03-18 10:34:18,1
13221909,HDFS Service Checking with an ClassNotFound Error in Ambari WFManager view,"Below is the error that is coming with the API Call

[/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFD/resources/proxy/hdfsCheck|https://ambviews-celws-igtc-143841.southeastasia.cloudapp.azure.com/api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFD/resources/proxy/hdfsCheck]
{code:java}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /api/v1/views/WORKFLOW_MANAGER/versions/1.0.0/instances/WFD/resources/proxy/hdfsCheck. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>javax.servlet.ServletException: java.lang.NoClassDefFoundError: org/eclipse/jetty/util/ajax/JSON$Convertor
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:420)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:291)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilterInternal(BasicAuthenticationFilter.java:215)
	at org.apache.ambari.server.security.authentication.AmbariBasicAuthenticationFilter.doFilterInternal(AmbariBasicAuthenticationFilter.java:139)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:123)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
{code}",pull-request-available,['ambari-views'],AMBARI,Bug,Blocker,2019-03-15 13:07:08,1
13221085,Ambari server setup failed with postgres connectivity error when using postgres 9.3,"INFO 2019-03-04 17:45:49,694 serverSetup.py:1088 - Setup ambari-server.
ERROR 2019-03-04 17:45:50,498 ambari-server.py:901 - 'Fatal exception: Unable to start PostgreSQL server. Exiting, exit code 4'
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 884, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1145, in setup
    _setup_database(options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 958, in _setup_database
    dbmsAmbari.setup_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 150, in setup_database
    self._setup_local_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 474, in _setup_local_database
    raise FatalException(retcode, err)
FatalException: 'Fatal exception: Unable to start PostgreSQL server. Exiting, exit code 4'
INFO 2019-03-04 17:45:51,001 ambari-server.py:798 - loglevel=logging.INFO
INFO 2019-03-04 17:45:51,003 serverSetup.py:1208 - Reset ambari-server.
ERROR 2019-03-04 17:45:51,036 ambari-server.py:901 - 'Fatal exception: could not change directory to ""/root"": Permission denied\npsql: could not connect to server: No such file or directory\n\tIs the server running locally and accepting\n\tconnections on Unix domain socket ""/tmp/.s.PGSQL.5432""?\n, exit code 1'
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 884, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1236, in reset
    _reset_database(options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 999, in _reset_database
    dbmsAmbari.reset_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 157, in reset_database
    self._reset_local_database()
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 513, in _reset_local_database
    raise FatalException(1, drop_errdata)
FatalException: 'Fatal exception: could not change directory to ""/root"": Permission denied\npsql: could not connect to server: No such file or directory\n\tIs the server running locally and accepting\n\tconnections on Unix domain socket ""/tmp/.s.PGSQL.5432""?\n, exit code 1'
INFO 2019-03-04 17:45:53,854 ambari-server.py:798 - loglevel=logging.INFO
INFO 2019-03-04 17:45:53,856 ambari-server.py:161 - Stopping ambari-server.
INFO 2019-03-04 17:45:53,857 ambari-server.py:190 - Ambari Server is not running",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-03-12 10:59:45,3
13220810,Chrome and Firefox browsers are crashing while opening Ambari UI,Ambari UI is consuming almost 1.4 GB of browser memory.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-03-11 11:04:54,2
13219880,"Host registration through Ambari UI failed with an error ""Unsupported Media Type""","Host registration through Ambari UI failed with an error ""Unsupported Media Type""",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-03-06 11:08:14,3
13219594,[Knox TP] Config Group selection is empty while Adding Service in Ambari,"The following HTTP requests failed in the console:

GET https://ctr-e139-1542663976389-81666-01-000003.hwx.site:8443/gateway/default/ambari/api/v1/clusters/cl1/configurations/service_config_versions?is_current=true&group_id%3E0&fields=*&_=1551793258100

{""status"":400,""message"":""Unable to compile query predicate: Invalid Query Token: token='&', previous token type=PROPERTY_OPERAND""}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-03-05 16:03:35,3
13218836,"Start, Stop, Service Check and other request actions using PUT/POST from Ambari UI do not respond when tried via Knox TP","Getting the error message below when trying to start/stop a service using Ambari UI through Knox.

{code}
{
  ""status"" : 400,
  ""message"" : ""Invalid Request: Malformed Request Body.  An exception occurred parsing the request body: Unexpected character ('%' (code 37)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n at [Source: java.io.StringReader@5e9b8d28; line: 1, column: 3]""
}
{code}

",pull-request-available,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2019-03-01 08:24:11,3
13218285,Backport Knox Trusted Proxy changes into 2.7.4,Please backport all necessary changes from  Knox Trusted Proxy support into Ambari 2.7.4.,pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2019-02-27 08:22:00,3
13217149,"Rack ""Config Refresh"" behaviour is different in Ambari 2.6 and 2.7.3","In the new ambari-2.7.3 version, there is no way (via web ui) to generally refresh configs on a host . There is a way to refresh HDFS_CLIENT config on the Namenode host, but refreshing HDFS_CLIENT config *does not* refresh the topology_mappings.data file. 


*In Ambari 2.6.2: (During the ""Refresh Configs"")*
When we refresh ""Refresh Configs"" then it refreshes all client configs which also includes HDFS and MapReduce2 config refresh. During the ""Restart MapReduce2 Client"" step it actually performs the ""/etc/hadoop/conf/topology_mappings.data"" file refresh with correct rack info which is updated via ambari ui.
Example : 
{code}
2019-02-09 21:31:05,642 - File['/usr/hdp/2.6.5.0-292/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2019-02-09 21:31:05,647 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
2019-02-09 21:31:05,655 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('P@.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
{code}


*In Ambari 2.7.3:  (During the ""Refresh Configs"")*
Even after refreshing HDFS Client (AND) MapReduce Clients individually using ""Refresh Configs"" iwe do not see above kind of message. Also no changes happens inside the ""/etc/hadoop/conf/topology_mappings.data"" is not updated until we restart daemon components like DataNode. So the behavior of client config refresh seems to be slightly changed from ambari side.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-02-21 09:57:27,4
13216706,http.strict-transport-security change does not take affect in 2.7.x,"Updating the below configurations does not take affect in Ambari 2.7.x version


{noformat}
http.strict-transport-security=max-age=0
views.http.strict-transport-security=max-age=0
{noformat}

After setting the above configurations still API response gives below max-age headers.


{noformat}
Strict-Transport-Security: max-age=31536000 ; includeSubDomains
{noformat}

I see AmbariServerSecurityHeaderFilter.java setting the correctly defined params but later somehow it is going to default value.

This works fine in 2.6.x versions.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-02-19 18:12:06,1
13216658,"Dashboard is unable to load . Common console error : ""SEVERE TypeError: widgetGroups is undefined""","I found a corner scenario where the dashboard never loads.
If we click on Dashboard link on landing page before dashboard completely loads upon login, dashboard never loads.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-02-19 14:30:58,4
13216621,ClientComponentHasNoStatus exception clutters Operating System's /var/log/messages,"As part of the status check, exception ClientComponentHasNoStatus throws ""detected unhandled Python exception"" in /var/log/messages every 20 seconds for each client installed.",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2019-02-19 12:41:38,0
13214585,Implement additional error reporting for ambari-web unit tests,"If ambari-web unit tests fail because of JS error thrown in test files outside the callbacks passed to {{it}}/{{before}}/{{beforeEach}}/{{after}}/{{afterEach}} methods, the execution is just stopped without reporting the cause. This might be confusing because failure isn't reported if none of the previous tests failed.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-08 11:17:33,2
13214345,Cover views of Widget Create wizard with unit tests,"Following files to cover:
* app/views/main/service/widgets/create/expression_view.js
* app/views/main/service/widgets/create/step2_view.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-07 10:58:10,4
13213890,Remove unused models from ambari-web,"Remove the following models:
- {{App.TargetCluster}}
- {{App.RootService}}
- {{App.RootServiceComponents}}
- {{App.Rack}}
- {{App.Authentication}}
- {{App.BackgroundOperation}}
- {{App.BackgroundOperationEvent}}
- {{App.Form}}
- {{App.FormField}}
- {{App.CreateUserForm}}
- {{App.ServiceAudit}}",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-05 12:33:05,2
13213697,LDAP password in cleartext in ldap-password.dat file after encrypting passwords,"In 2.7.x we store LDAP password within its own file; however the content of that file is not encrypted even if password encryption is on. To approach this issue the following should be done:
 - in case password encryption is enabled we will encrypt the LDAP password in the credential store and write the corresponding CS alias in the LDAP password file (just like we do with other passwords in {{ambari.properties}})
 - in case the password encryption is disabled we will write the raw password in the LDAP password file

In both cases an additional level of security can be achieved by setting the appropriate user/group access on the file system to the LDAP password file.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-02-04 14:29:34,6
13213631,Cover widget mixin with unit tests,"Following files to cover:
* app/mixins/common/widgets/widget_mixin.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-04 07:39:02,4
13213376,Cover mappers files with unit tests,"Cover following files:
- {{ambari-web/app/mappers/alert_notification_mapper.js}}
- {{ambari-web/app/mappers/cluster_mapper.js}}
- {{ambari-web/app/mappers/stack_version_mapper.js}}
- {{ambari-web/app/mappers/widget_mapper.js}}
",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-02-01 18:23:45,2
13213024,Scale hosts ignores rack_info,"The following request is accepted by Ambari, but the new host is assigned to {{/default-rack}}:

{noformat}
$ curl -X POST -d @- ""http://${AMBARI_SERVER}:8080/api/v1/clusters/TEST/hosts"" <<EOF
[
    {
        ""blueprint"": ""blue"",
        ""host_group"": ""node"",
        ""host_name"": ""c7402.ambari.apache.org"",
        ""rack_info"": ""/rack/a""
    }
]
EOF
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-01-31 09:22:29,5
13212611,StackAdvisorAdapterTest result depends on method execution order,"{{StackAdvisorAdapterTest}} result depends on method execution order, {{getLayoutRecommendationInfo}} fails if executed after {{recommendConfigurations_alwaysApply}}.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2019-01-29 17:38:41,5
13211812,Cover widget mixin with unit tests,"Following files to cover app/mixins/common/widgets/widget_section.js.

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-25 12:03:18,4
13211473,/var/lib/ambari-agent/cache not updating (Ambari 2.7),"As of Ambari 2.7, any changes to /var/lib/ambari-server/resources are not
pushed to ambari-agents cache (/var/lib/ambari-agent/cache/. This is not an
issue in Ambari 2.6.

Also, if you remove files or need to wipe ambari-agent cache(s), the files
will never repopulate.

Another example, is custom ambari host_scripts not being pushed to the ambari-
agents as should be done based on this article, and works in Ambari 2.6:
<https://community.hortonworks.com/articles/38149/how-to-create-and-register-
custom-ambari-alerts.html>

To reproduce:

  * 1\. deploy hosts with Ambari 2.7.3 and have them connected to an ambari-server
  * 2\. turn on DEBUG logging in ambari-agent
  * 3\. go to an ambari-agent. Notice that there is no /var/lib/ambari-agent/cache/host_scripts/.hash but there are .hash files for the stack directories.
  * 4\. on the ambari-server, add files into /var/lib/ambari-server/resources/host_scripts
  * 5\. restart ambari-agent and notice that their host_scripts directory is never updated and .hash is never generated
  * 6\. check ambari-agent.log and notice that other paths were checked (by FileCache.py) but host_scripts were not

",pull-request-available,[],AMBARI,Bug,Major,2019-01-24 07:09:58,0
13211337,UI unit tests are sometimes failing,"Error is thrown:
{noformat}
Ambari Web Unit tests ""after each"" hook:
Uncaught Error: assertion failed: You must pass at least an object and event name to Ember.addListener
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2019-01-23 17:31:11,2
13210959,getAsPost requests don't work when ambari is used behind knox proxy,"getting:

https://c7401.ambari.apache.org:8443/gateway/default/ambari/api/v1/clusters/cc/hosts?fields=Hosts/rack_info,Hosts/host_name,Hosts/maintenance_state,Hosts/public_host_name,Hosts/cpu_count,Hosts/ph_cpu_count,alerts_summary,Hosts/host_status,Hosts/host_state,Hosts/last_heartbeat_time,Hosts/ip,host_components/HostRoles/state,host_components/HostRoles/maintenance_state,host_components/HostRoles/stale_configs,host_components/HostRoles/service_name,host_components/HostRoles/display_name,host_components/HostRoles/desired_admin_state,host_components/metrics/dfs/namenode/ClusterId,host_components/metrics/dfs/FSNamesystem/HAState,metrics/disk,metrics/load/load_one,Hosts/total_mem,Hosts/os_arch,Hosts/os_type,metrics/cpu/cpu_system,metrics/cpu/cpu_user,metrics/memory/mem_total,metrics/memory/mem_free,stack_versions/HostStackVersions,stack_versions/repository_versions/RepositoryVersions/repository_version,stack_versions/repository_versions/RepositoryVersions/id,stack_versions/repository_versions/RepositoryVersions/display_name&minimal_response=true,host_components/logging&page_size=25&from=0&_=1546003790663


gives:

{""status"":400,""message"":""Invalid Request: Malformed Request Body.  An exception occurred parsing the request body: Unrecognized token 'from': was expecting \n at [Source: java.io.StringReader@9ba0d3a; line: 1, column: 6]""}

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-22 11:49:47,3
13210428,Cover errors utils with unit tests,"Cover the following files:
- {{utils/errors/assertions.js}}
- {{utils/errors/definitions.js}}",pull-request-available,[],AMBARI,Task,Major,2019-01-18 16:40:00,2
13210380,Intermittent ConcurrentModificationException exception during STOMP message emitting,"During 2k cluster deploying a lot of exceptions were found in the ambari-server log:
{code}
2019-01-15 16:25:33,567 ERROR [stomp-agent-bus-0] agent-update-bus:232 - Exception thrown by subscriber method onUpdateEvent(org.apache.ambari.server.events.STOMPEvent) on subscriber org.apache.ambari.server.events.listeners.requests.STOMPUpdateListener@5ee973ee when dispatching event: org.apache.ambari.server.events.TopologyAgentUpdateEvent@f4bf3c68
org.springframework.messaging.MessageDeliveryException: Failed to handle GenericMessage [payload=byte[577], headers={simpMessageType=MESSAGE, contentType=application/json;charset=UTF-8, simpDestination=/events/topologies}] to org.springframework.messaging.support.ExecutorSubscribableChannel$SendTask@13106eff in SimpleBrokerMessageHandler [DefaultSubscriptionRegistry[cache[5480 destination(s)], registry[2392 sessions]]]; nested exception is java.util.ConcurrentModificationException, failedMessage=GenericMessage [payload=byte[577], headers={simpMessageType=MESSAGE, contentType=application/json;charset=UTF-8, simpDestination=/events/topologies}]
        at org.springframework.messaging.support.ExecutorSubscribableChannel$SendTask.run(ExecutorSubscribableChannel.java:153)
        at org.springframework.messaging.support.ExecutorSubscribableChannel.sendInternal(ExecutorSubscribableChannel.java:100)
        at org.springframework.messaging.support.AbstractMessageChannel.send(AbstractMessageChannel.java:136)
        at org.springframework.messaging.support.AbstractMessageChannel.send(AbstractMessageChannel.java:122)
        at org.springframework.messaging.simp.SimpMessagingTemplate.sendInternal(SimpMessagingTemplate.java:187)
        at org.springframework.messaging.simp.SimpMessagingTemplate.doSend(SimpMessagingTemplate.java:162)
        at org.springframework.messaging.simp.SimpMessagingTemplate.doSend(SimpMessagingTemplate.java:48)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:109)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.convertAndSend(AbstractMessageSendingTemplate.java:151)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.convertAndSend(AbstractMessageSendingTemplate.java:129)
        at org.springframework.messaging.core.AbstractMessageSendingTemplate.convertAndSend(AbstractMessageSendingTemplate.java:122)
        at org.apache.ambari.server.events.MessageEmitter.emitMessageToAll(MessageEmitter.java:159)
        at org.apache.ambari.server.events.DefaultMessageEmitter.emitMessage(DefaultMessageEmitter.java:102)
        at org.apache.ambari.server.events.listeners.requests.STOMPUpdateListener.onUpdateEvent(STOMPUpdateListener.java:53)
        at sun.reflect.GeneratedMethodAccessor174.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)
        at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.ConcurrentModificationException
        at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)
        at java.util.LinkedHashMap$LinkedEntryIterator.next(LinkedHashMap.java:752)
        at java.util.LinkedHashMap$LinkedEntryIterator.next(LinkedHashMap.java:750)
        at java.util.Map.forEach(Map.java:620)
        at org.springframework.messaging.simp.broker.SimpleBrokerMessageHandler.sendMessageToSubscribers(SimpleBrokerMessageHandler.java:388)
        at org.springframework.messaging.simp.broker.SimpleBrokerMessageHandler.handleMessageInternal(SimpleBrokerMessageHandler.java:304)
        at org.springframework.messaging.simp.broker.AbstractBrokerMessageHandler.handleMessage(AbstractBrokerMessageHandler.java:256)
        at org.springframework.messaging.support.ExecutorSubscribableChannel$SendTask.run(ExecutorSubscribableChannel.java:144)
        ... 21 more
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-01-18 13:23:37,1
13210150,Cover views of the modals with unit tests,"Following files to cover app/views/common/modal_popups/*.

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-17 13:05:28,4
13209921,"Ambari is not respecting host component maintenance mode when performing ""Restart All Required"" at the cluster level","* Put HSI in maintenance mode.
* Changed auth to local mapping in core-site.
* A bunch of services got a restart indicator.
* Triggering ""Restart All Required"" at the cluster level schedules HSI to be restarted.  HSI restart fails so the entire operation fails.  This is cumbersome because now the user has to trigger ""restart affected"" for individual services.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-01-16 15:16:33,2
13209455,Duplicate title on YARN summary page,'Components' sub-title is displayed twice,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-01-14 16:21:00,2
13209424,Dasboard metrics will not load for ambari user which has dot in their username.,"Ambari users which as dot in their username will not be able to see the dashboard metrics. 

It looks that js scripts are truncating the username to use only first part before dot and fails with below error.
{code:java}
{\""file\"":\""http://c116-node1.raghav.com:8080/javascripts/vendor.js\"",\""line\"":13510,\""col\"":12,\""error\"":\""Uncaught Error: Object in path user-pref-test1 could not be found or was destroyed.\"",\""stackTrace\"":\""Error: Object in path user-pref-test1 could not be found or was destroyed.\\n    at setPath (/vendor.js:13510:18)\\n    at Object.set (/vendor.js:13387:12)\\n    at Object.App.db.set (/app.js:200972:6)\\n    at Class.setDBProperty (/app.js:72577:12)\\n    at Class.saveWidgetsSettings (/app.js:238244:10)\\n    at Class.getUserPrefErrorCallback (/app.js:238258:10)\\n    at Class.newFunc [as getUserPrefErrorCallback] (/vendor.js:12954:16)\\n    at Class.opt.error (/app.js:193035:36)\\n    at fire (/vendor.js:1141:36)\\n    at Object.fireWith [as rejectWith] (/vendor.js:1252:15)\""},\""1541550308010\
{code}

Tried manually setting the persist data for the user which dint help. 

Steps to reproduce: (on Ambari 2.7.1)
-->Create user test.user1 in ambari: 
-->Assign privileges to see the dashboard
-->Login as test.user1, which should show loading of metric but never completes.

All other features work fine exception dashboard metrics

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2019-01-14 14:10:51,4
13208789,Cover views of reassign wizard with unit tests,"Following files to cover:
* app/views/main/service/reassign/step1_view.js
* app/views/main/service/reassign/step3_view.js
* app/views/main/service/reassign/step5_view.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-10 13:48:13,4
13208700,deploy-gce-perf-cluster.py fails after upgrade on gce controller,"'gce fqdn' command was removed. While the script relied on it.

",pull-request-available,[],AMBARI,Bug,Major,2019-01-10 06:20:07,0
13208603,"Remove Flume Live widget from Ambari, alongside the Flume service during upgrade to HDP3. ","During the ambari-managed upgrade from HDP 2.6 -> HDP 3.0, the flume service is removed (as it is no longer supported in the new stack). 

The ""Flume Live"" widget is still displayed on the Ambari home page.  

Let's remove this widget as well, when the flume service is removed. ",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-09 18:35:06,2
13208286,Cover flume metric graphs with unit tests,"Following files to cover:
* app/views/main/service/info/metrics/flume/flume_metric_graph.js
* app/views/main/service/info/metrics/flume/flume_metric_graphs.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-08 11:24:44,4
13207537,Enable Kerberos fails when Ambari server is not on a registered host,"Enable Kerberos fails when Ambari server is not on a registered host.  

The following error is seen in /var/log/ambari-server.log

{noformat}
2019-01-03 15:28:34,238  WARN [Server Action Executor Worker 39] ServerActionExecutor:471 - Task #39 failed to complete execution due to thrown exception: org.apache.ambari.server.HostNotFoundException:Host not found, hostname=c7401.ambari.apache.org
org.apache.ambari.server.HostNotFoundException: Host not found, hostname=c7401.ambari.apache.org
        at org.apache.ambari.server.state.cluster.ClustersImpl.getHost(ClustersImpl.java:456)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:190)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:174)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.findConfigurationTagsWithOverrides(AmbariManagementControllerImpl.java:2431)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.google.inject.internal.DelegatingInvocationHandler.invoke(DelegatingInvocationHandler.java:50)
        at com.sun.proxy.$Proxy134.findConfigurationTagsWithOverrides(Unknown Source)
        at org.apache.ambari.server.state.ConfigHelper.calculateExistingConfigurations(ConfigHelper.java:2158)
        at org.apache.ambari.server.controller.KerberosHelperImpl.calculateConfigurations(KerberosHelperImpl.java:1722)
        at org.apache.ambari.server.controller.KerberosHelperImpl.getActiveIdentities(KerberosHelperImpl.java:1797)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.calculateServiceIdentities(KerberosServerAction.java:512)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:456)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.execute(CreatePrincipalsServerAction.java:92)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

This is caused when Ambari tried to find the host-specific configuration values when processing the Kerberos identities and the host is not registered for the relevant cluster. This can happen when the Ambari server Kerberos identity is being processed when the Ambari server host is not registered with the cluster. 

To solve this, host specific configuration values should not be obtained for the non-registered Ambari server host. 
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2019-01-03 19:49:40,7
13207480,Delete identities fails when removing service in reverse order,"(reverse order = host components first, then components, then service)

STR:

Install ZooKeeper + Kafka + Kerberos
Stop Kafka
curl -X DELETE http://c7401.ambari.apache.org:8080/api/v1/clusters/TEST/hosts/c7401.ambari.apache.org/host_components/KAFKA_BROKER
curl -X DELETE http://c7401.ambari.apache.org:8080/api/v1/clusters/TEST/services/KAFKA/components/KAFKA_BROKER
=> identities are removed
curl -X DELETE http://c7401.ambari.apache.org:8080/api/v1/clusters/TEST/services/KAFKA
=> identity removal fails with PersistenceException

{code}
Internal Exception: org.postgresql.util.PSQLException: ERROR: syntax error at or near "")""
  Position: 210
Error Code: 0
Call: SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ()) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))
        bind => [4 parameters bound]
Query: ReadAllQuery(name=""ClusterConfigEntity.findLatestConfigsByStackWithTypes"" referenceClass=ClusterConfigEntity sql=""SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ?) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))"")
FetchGroup(){serviceConfigEntities, stack, selectedTimestamp, clusterId, type, version, unmapped, configId, configGroupConfigMappingEntities, tag, selected, timestamp, clusterEntity}
javax.persistence.PersistenceException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: syntax error at or near "")""
  Position: 210
Error Code: 0
Call: SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ()) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))
        bind => [4 parameters bound]
Query: ReadAllQuery(name=""ClusterConfigEntity.findLatestConfigsByStackWithTypes"" referenceClass=ClusterConfigEntity sql=""SELECT t0.config_id, t0.cluster_id, t0.selected, t0.selected_timestamp, t0.version_tag, t0.create_timestamp, t0.type_name, t0.unmapped, t0.version, t0.stack_id FROM clusterconfig t0 WHERE ((((t0.type_name IN ?) AND (t0.cluster_id = ?)) AND (t0.stack_id = ?)) AND (t0.selected_timestamp = (SELECT MAX(t1.selected_timestamp) FROM clusterconfig t1 WHERE (((t1.cluster_id = ?) AND (t1.stack_id = ?)) AND (t1.type_name = t0.type_name)))))"")
FetchGroup(){serviceConfigEntities, stack, selectedTimestamp, clusterId, type, version, unmapped, configId, configGroupConfigMappingEntities, tag, selected, timestamp, clusterEntity}
        at org.eclipse.persistence.internal.jpa.QueryImpl.getDetailedException(QueryImpl.java:382)
        at org.eclipse.persistence.internal.jpa.QueryImpl.executeReadQuery(QueryImpl.java:260)
        at org.eclipse.persistence.internal.jpa.QueryImpl.getResultList(QueryImpl.java:473)
        at org.apache.ambari.server.orm.dao.DaoUtils.selectList(DaoUtils.java:53)
        at org.apache.ambari.server.orm.dao.ClusterDAO.getLatestConfigurationsWithTypes(ClusterDAO.java:226)
        at org.apache.ambari.server.orm.AmbariLocalSessionInterceptor.invoke(AmbariLocalSessionInterceptor.java:44)
        at org.apache.ambari.server.state.cluster.ClusterImpl.getLatestConfigsWithTypes(ClusterImpl.java:1185)
        at org.apache.ambari.server.controller.DeleteIdentityHandler$PrepareDeleteIdentityServerAction.extendWithDeletedConfigOfService(DeleteIdentityHandler.java:304)
        at org.apache.ambari.server.controller.DeleteIdentityHandler$PrepareDeleteIdentityServerAction.calculateConfig(DeleteIdentityHandler.java:296)
        at org.apache.ambari.server.controller.DeleteIdentityHandler$PrepareDeleteIdentityServerAction.execute(DeleteIdentityHandler.java:273)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:745)
{code}
",pull-request-available,['ambari-server'],AMBARI,Task,Major,2019-01-03 14:34:43,3
13207459,Cover HA wizard controller with unit tests,"Following files to cover:
* app/controllers/main/admin/highAvailability/hawq/addStandby/step3_controller.js
* app/controllers/main/admin/highAvailability/hawq/addStandby/wizard_controller.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-03 11:35:11,4
13207246,Select Service page: Show Yarn and MR2 as separate services for selection instead of single selection (Yarn+MapReduce2),"This needs to be done so that user can select YARN without MR2 using UI installer wizard

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2019-01-02 09:29:48,4
13205957,Update AngularJS version to 1.7.5 due to known vulnerabilities,"Update AngularJS version to 1.7.5 due to known cross site scripting vulnerabilities.

See https://snyk.io/vuln/npm:angular

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-21 12:34:28,4
13205779,Restarting Ambari Server Fails Due to Recursive Injection of STOMPUpdatePublisher,"In some cases during a restart of Ambari Server, the server fails to load due to:
{code:java}
Caused by: java.lang.IllegalStateException: Recursive load of: org.apache.ambari.server.events.publishers.STOMPUpdatePublisher.<init>()
	at com.google.common.base.Preconditions.checkState(Preconditions.java:197)
	at com.google.common.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2299)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2191)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)
	at com.google.inject.internal.FailableCache.get(FailableCache.java:48)
	at com.google.inject.internal.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
	at com.google.inject.internal.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:138)
	at com.google.inject.internal.InjectorImpl.initializeJitBinding(InjectorImpl.java:550)
	at com.google.inject.internal.InjectorImpl.createJustInTimeBinding(InjectorImpl.java:887)
	at com.google.inject.internal.InjectorImpl.createJustInTimeBindingRecursive(InjectorImpl.java:808)
	at com.google.inject.internal.InjectorImpl.getJustInTimeBinding(InjectorImpl.java:285)
	at com.google.inject.internal.InjectorImpl.getBindingOrThrow(InjectorImpl.java:217)
	at com.google.inject.internal.SingleFieldInjector.<init>(SingleFieldInjector.java:42)
	at com.google.inject.internal.MembersInjectorStore.getInjectors(MembersInjectorStore.java:131)
	at com.google.inject.internal.MembersInjectorStore.createWithListeners(MembersInjectorStore.java:98)
	at com.google.inject.internal.MembersInjectorStore.access$000(MembersInjectorStore.java:37)
	at com.google.inject.internal.MembersInjectorStore$1.create(MembersInjectorStore.java:45)
	at com.google.inject.internal.MembersInjectorStore$1.create(MembersInjectorStore.java:42)
	at com.google.inject.internal.FailableCache$1.load(FailableCache.java:37)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
	... 378 more{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-12-20 16:33:10,1
13205774,Optionally execute the post user creation hook on existing users during LDAP sync,"Optionally execute the post user creation hook on existing users during LDAP sync. 

The post user creation hook is executed on users when created or imported into Ambari.  This hook is executed given the following criteria is met:
# The post user creation hook is enabled (ambari.properties - {{ambari.post.user.creation.hook.enabled = true}}, default: {{false}})
# The post user creation hook is set and available (ambari.properties - {{ambari.post.user.creation.hook = <path to script>}}, default: {{/var/lib/ambari-server/resources/scripts/post-user-creation-hook.sh}})
# HDFS is installed and running.

It is possible to have executed the LDAP sync process before all of the criteria has been met.  Therefore, it would be beneficial to trigger the post user creation hook to be executed on these users when the criteria has been met. 

To do this, an optional property should be set on the LDAP sync request - {{post_process_existing_users}}.  The {{post_process_existing_users}} property is part of a ""spec"" object and should be set to either ""true"" or ""false"", if set at all.  If set to ""true"", the post user creation hook will be executed on all user's that come back from the LDAP query that also exist in the Ambari database as LDAP users. 

Example REST API calls:
{noformat:title=Sync All Users and Groups}
POST /api/v1/ldap_sync_events
[
  {
    ""Event"": {
      ""specs"": [
        {
          ""principal_type"": ""users"",
          ""sync_type"": ""all"",
          ""post_process_existing_users"" : ""true""
        },
        {
          ""principal_type"": ""groups"",
          ""sync_type"": ""all"",
          ""post_process_existing_users"" : ""true""
        }
      ]
    }
  }
]
{noformat}

{noformat:title=Sync Specific Users}
POST /api/v1/ldap_sync_events
[
  {
    ""Event"": {
      ""specs"": [
        {
          ""principal_type"": ""users"",
          ""sync_type"": ""specific"",
          ""names"" : ""user1, user2, user3"",
          ""post_process_existing_users"" : ""true""
        }
      ]
    }
  }
]
{noformat}

{noformat:title=Sync Specific Groups}
POST /api/v1/ldap_sync_events
[
  {
    ""Event"": {
      ""specs"": [
        {
          ""principal_type"": ""groups"",
          ""sync_type"": ""specific"",
          ""names"" : ""hadoop_users, hadoop_admins"",
          ""post_process_existing_users"" : ""true""
        }
      ]
    }
  }
]
{noformat}

Using the Ambari sync-ldap CLI, an optional argument named ""--post-process-existing-users"" may be added to enable this feature.

Example CLI calls:
{noformat:title=Sync All Users and Groups}
ambari-server sync-ldap --all --post-process-existing-users
{noformat}

{noformat:title=Sync Specific Users}
ambari-server sync-ldap --users users.txt --post-process-existing-users
{noformat}

{noformat:title=Sync Specific Groups}
ambari-server sync-ldap --groups groups.txt --post-process-existing-users
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-20 16:16:42,7
13205748,Refactor AddServiceInfo to use a builder,"{{AddServiceInfo}} constructor could be improved by the Builder pattern.

https://github.com/apache/ambari/blob/1431ab44887c2ff7f9e94f8fcb42e24e3ce33800/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java#L45-L83",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-20 15:01:32,5
13205714,Ambari Admin doesn't redirect user to login page if auth session becomes invalidated,"When auth session becomes invalid, nothing happens (instead of redirection to login page)",pull-request-available,['ambari-admin'],AMBARI,Bug,Major,2018-12-20 12:45:55,2
13205373,JS error after starting stack downgrade,"{code:java}
Uncaught TypeError: Cannot convert undefined or null to object
    at Function.keys (<anonymous>)
    at Object.<anonymous> (app.js:17261)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at Object.deferred.(:8080/anonymous function) [as resolve] (http://104.196.93.139:8080/javascripts/vendor.js:1341:40)
    at Class.complete (app.js:17012)
    at app.js:206411
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-12-19 13:36:37,4
13205322,Validate SQL schema creation scripts,Verify that SQL schema creation scripts are valid using different DBs in Docker container.,pull-request-available,['ambari-server'],AMBARI,New Feature,Major,2018-12-19 09:50:32,5
13205307,Unable to Add Services Due to toMapByProperty(),"STR:
- Install a simple cluster with ZK using a blueprint.
- Go to the web client and add HDFS as a service.
- You will not be able to progress past the master assignment page after clicking ""Next""

{code}
app.js:28589 Uncaught TypeError: App.HostComponent.find(...).toMapByProperty is not a function
    at Class.saveMasterComponentHosts (app.js:28589)
    at Class.newFunc [as saveMasterComponentHosts] (vendor.js:12954)
    at Class.next (app.js:94607)
    at Class.newFunc [as next] (vendor.js:12954)
    at Class.sendRecursively (vendor.js:27675)
    at Class.send (vendor.js:27660)
    at Class._goNextStepIfValid (app.js:42517)
    at Class.newFunc [as _goNextStepIfValid] (vendor.js:12954)
    at Class.showValidationIssuesAcceptBox (app.js:81684)
    at app.js:81665
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-12-19 08:35:25,4
13205121,Update momentum.js version to 2.22.2 due to known vulnerabilities,"Update momentum.js version to 2.22.2 due to known vulnerabilities.

See https://snyk.io/vuln/npm:Moment.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-12-18 12:27:13,4
13204902,UI logic for component dependencies needs to be modified to use the type field,"The component dependency now has the type field. If the value of that field is ""exclusive"" the dependent component should never be co-hosted.
{code}
{
  ""href"" : ""http://c7301:8080/api/v1/stacks/HDP/versions/3.0/services/HDFS/components/DATANODE/dependencies/OZONE_DATANODE"",
  ""Dependencies"" : {
    ""component_name"" : ""OZONE_DATANODE"",
    ""conditions"" : [ ],
    ""dependent_component_name"" : ""DATANODE"",
    ""dependent_service_name"" : ""HDFS"",
    ""scope"" : ""host"",
    ""service_name"" : ""OZONE"",
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""3.0"",
    ""type"" : ""exclusive""
  }
}{code}
Currently this dependency causes UI to force dependent component being add on the host
 !Screenshot from 2018-11-29 16-10-38.png|thumbnail! ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-17 14:24:09,4
13204252,"Sensitive Ambari configuration values should be encrypted in the Ambari server DB, if enabled","Sensitive Ambari configuration values should be encrypted in the Ambari server DB, if enabled.

Ambari configuration value types are defined in {{org.apache.ambari.server.configuration.AmbariServerConfigurationKey}}. Sensitive properties have property type of {{org.apache.ambari.server.configuration.ConfigurationPropertyType#PASSWORD}}.

Using this information, _if this feature is enabled_, the Ambari server should encrypt sensitive values before storing them in the {{ambari_configuration}} table in the Ambari DB.

The Ambari server should encrypt sensitive configuration values if the following has been met:
 * A master key has been setup using the ""ambari-server setup-security"" CLI (using option #2 - Encrypt passwords stored in ambari.properties file)
 * The Ambari server configuration property named ""{{security.server.encrypt_sensitive_data}}"" is set to ""true""

If encrypting sensitive data:
 * the value should be encrypted using a secure symmetric key encryption algorithm. For example AES - [https://aesencryption.net/].
 * the encryption key should be the previously set master key, or some reproducible encoding of it.
 * the encrypted bytes should be converted to a hex string
 * the value should be stored in the relevant field such that the value is declared as encrypted.
 ** for example:
{noformat}
""password"" : ""${enc=aes256_base64, value=5248...303d}""{noformat}

 ** this is needed in the event {{server.security.encrypt_sensitive_data}} is changed to false, but there are still encrypted values in the database.

Encrypted data needs to be decrypted before being used or returned via the REST API. The data may be re-encrypted depending on use.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-13 14:36:18,6
13204219,Cover host component view with unit tests,"Following files to cover:
app/views/main/host/details/host_component_view.js

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-12-13 12:11:00,4
13204100,Scale hosts does not install component if service is in maintenance mode,"STR:

# Create cluster via blueprint
# Turn on maintenance mode for some service
# Add host to cluster (scale up)

Result: components of the service in maintenance mode are not installed (install pending state) on the new host

Possible workaround: manually install and start such components once the host request is complete",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-12 21:11:36,5
13204025,Handle blueprint/VDF stack version mismatch,"If a repository version is explicitly specified in the cluster creation request (using {{repository_version_id}} or {{repository_version}}), then Ambari should reject the cluster creation request if the blueprint and the VDF stacks are different (eg. HDP 2.6 vs HDP 3.0).  If it allowed deployment, the cluster would run into various errors due to the mismatch (eg. UI glitches, attempting to install the wrong package, etc.)",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-12 15:25:59,5
13203982,Parse error should be reported as Bad Request,"Any JSON errors encountered while parsing Add Service request should be reported as HTTP 400 Bad Request.  Currently such errors result in HTTP 500 Server Error.

{noformat}
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
<title>Error 500 Server Error</title>
</head>
<body><h2>HTTP ERROR 500</h2>
<p>Problem accessing /api/v1/clusters/TEST/services. Reason:
<pre>    Server Error</pre></p><h3>Caused by:</h3><pre>java.io.UncheckedIOException: com.fasterxml.jackson.databind.exc.MismatchedInputException: Cannot deserialize instance of `java.util.HashSet` out of VALUE_STRING token
 at [Source: (String)&quot;{  &quot;operation_type&quot;: &quot;ADD_SERVICE&quot;,  &quot;components&quot;: &quot;asdf&quot;}&quot;; line: 1, column: 52] (through reference chain: org.apache.ambari.server.controller.AddServiceRequest[&quot;components&quot;])
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-12-12 12:25:53,5
13203829,Allow skipping parts of Add Service request validation,"Provide ability to disable parts of the validation for the Add Service request, eg. configuration validation and topology validation.  Some parts still need to be validated (eg. it probably makes no sense to add unknown services).",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-11 21:50:14,5
13203736,Layout recommendation adds unwanted components,"STR:

# Install ZooKeeper
# Try to add Metrics Collector and Metrics Monitor using Add Service request

{noformat}
{
  ""operation_type"": ""ADD_SERVICE"",
  ""components"": [
    { ""name"": ""METRICS_COLLECTOR"", ""hosts"": [ { ""fqdn"": ""c7401.ambari.apache.org"" } ] },
    { ""name"": ""METRICS_MONITOR"", ""hosts"": [ { ""fqdn"": ""c7401.ambari.apache.org"" } ] }
  ]
}
{noformat}

Result: Metrics Grafana is also installed.  It is being added by the layout recommendation, which should not be invoked if full layout is explicitly specified in the request (ie. no {{""services""}} part is present).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-11 14:26:40,5
13203693,Cover Service controllers with unit tests,"Following files to cover:
* app/controllers/main/service/add_controller.js
* app/controllers/main/service/item.js",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-12-11 12:06:59,4
13203657,Add better host-matching syntax to Add Service request,"Change syntax of the Add Service request:

# allow multiple hosts per component
# simplify {{component_name}} to {{name}} to be more consistent with blueprints

{noformat}
   ""components"" : [
      {
	  ""name"" : ""DATANODE"",
          “hosts”: [
             { ""fqdn"" : ""c7401.ambari.apache.org"" },
             { ""fqdn"" : ""c7402.ambari.apache.org"" },
             ...
          ]
      },
      {
         ""name"" : ""NODEMANAGER"",
         “hosts”: [
             { ""fqdn"" : ""c7401.ambari.apache.org"" },
             { ""fqdn"" : ""c7402.ambari.apache.org"" },
             ...
          ]
      }
  ],
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-12-11 09:28:19,5
13203295,Duplicate kerberos_descriptor name reported as HTTP 500,"Duplicate {{kerberos_descriptor}} name is reported as HTTP 500 Server Error.  It should result in HTTP 409 Conflict.

{noformat}
$ curl -X POST -d @metrics_descriptor.json ""http://${AMBARI_SERVER}:8080/api/v1/kerberos_descriptors/metrics_descriptor""
HTTP/1.1 201 Created
$ curl -X POST -d @metrics_descriptor.json ""http://${AMBARI_SERVER}:8080/api/v1/kerberos_descriptors/metrics_descriptor""
HTTP/1.1 500 Server Error
...
  Detail: Key (kerberos_descriptor_name)=(metrics_descriptor) already exists.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-12-09 18:25:22,5
13203121,Allow unattended mpack install with purge,"The question that pops up if {{purge}} is specified prevents installing mpacks without user input.  The default answer is ""no"", which gets automatically selected in {{silent}} mode, causing the installation to be cancelled.

{noformat:title=ambari-server install-mpack --purge --silent --verbose --mpack=...}
...
CAUTION: You have specified the --purge option with --purge-list=['stack-definitions', 'mpacks']. This will replace all existing stack definitions, management packs currently installed.
Are you absolutely sure you want to perform the purge [yes/no]? (no)
ERROR: Exiting with exit code 1.
REASON: Management pack installation cancelled by user
{noformat}

The default answer should be ""no"" for regular mode, but ""yes"" for silent mode, to make unattended install possible.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-07 18:41:39,5
13203054,setup-ldap can not be executed non-interactively when using SSL without custom TrustStore,"We should provide a way to our end user to execute {{ambari-server setup-ldap}} in a non-interactive way (i.e. all answers are provided by command line options).

This is not the case when we would like to setup a secure LDAP (SSL is set to true) but we do not want to use a custom trust store. In this case the following question(s) are being asked:
1. Do you want to provide custom TrustStore for Ambari?
2. Optionally: if custom trust store was set previously the tool displays the earlier configuration and asks the following: Do you want to remove these properties?

Sample run:
{code:java}
[root@c7401 ~]# ambari-server setup-ldap --ambari-admin-username=admin --ambari-admin-password=admin --ldap-url=ad-nano.qe.hortonworks.com:636 --ldap-secondary-url=: --ldap-user-class=user --ldap-user-
attr=sAMAccountName --ldap-group-class=group --ldap-group-attr=cn --ldap-member-attr=member --ldap-dn=distinguishedName --ldap-base-dn=CN=Users,DC=hwqe,DC=hortonworks,DC=com --ldap-bind-anonym=false --ldap-manager-dn=cn=manager,cn=Users,dc=hwqe,dc=hortonworks,dc=com --ldap-manager-password=TestUser123 --ldap-referral=follow --ldap-sync-username-collisions-behavior=skip --ldap-force-lowercase-usernames=false --ldap-pagination-enabled=false --ldap-ssl=true --ldap-sync-disable-endpoint-identification=true --ldap-force-setup --ldap-save-settings --ldap-enabled-ambari=true --ldap-manage-services=true --ldap-enabled-services=* --ldap-user-group-member-attr=myMemberOf
Using python  /usr/bin/python

Fetching LDAP configuration from DB.
Primary LDAP Host (ad-nano.qe.hortonworks.com): 
Primary LDAP Port (636): 
Secondary LDAP Host <Optional>: 
Secondary LDAP Port <Optional>: 
Use SSL [true/false] (true): 
Disable endpoint identification during SSL handshake [true/false] (true): 
Do you want to provide custom TrustStore for Ambari [y/n] (y)?n
The TrustStore is already configured: 
  ssl.trustStore.type = jks
  ssl.trustStore.path = /tmp/ambari-server-truststore
  ssl.trustStore.password = keystore
Do you want to remove these properties [y/n] (y)? y
User object class (user): 
User ID attribute (sAMAccountName): 
User group member attribute (myMemberOf): 
Group object class (group): 
Group name attribute (cn): 
Group member attribute (member): 
Distinguished name attribute (distinguishedName): 
Search Base (CN=Users,DC=hwqe,DC=hortonworks,DC=com): 
Referral method [follow/ignore] (follow): 
Bind anonymously [true/false] (false): 
Bind DN (cn=manager,cn=Users,dc=hwqe,dc=hortonworks,dc=com): 
Enter Bind DN Password: 
Confirm Bind DN Password: 
Handling behavior for username collisions [convert/skip] for LDAP sync (skip): 
Force lower-case user names [true/false] (false):
Results from LDAP are paginated when requested [true/false] (false):
====================
Review Settings
====================
Primary LDAP Host (ad-nano.qe.hortonworks.com):  ad-nano.qe.hortonworks.com
Primary LDAP Port (636):  636
Use SSL [true/false] (true):  true
User object class (user):  user
User ID attribute (sAMAccountName):  sAMAccountName
User group member attribute (myMemberOf):  myMemberOf
Group object class (group):  group
Group name attribute (cn):  cn
Group member attribute (member):  member
Distinguished name attribute (distinguishedName):  distinguishedName
Search Base (CN=Users,DC=hwqe,DC=hortonworks,DC=com):  CN=Users,DC=hwqe,DC=hortonworks,DC=com
Referral method [follow/ignore] (follow):  follow
Bind anonymously [true/false] (false):  false
Handling behavior for username collisions [convert/skip] for LDAP sync (skip):  skip
Force lower-case user names [true/false] (false): false
Results from LDAP are paginated when requested [true/false] (false): false
ambari.ldap.connectivity.bind_dn: cn=manager,cn=Users,dc=hwqe,dc=hortonworks,dc=com
ambari.ldap.connectivity.bind_password: *****
ambari.ldap.advanced.disable_endpoint_identification: true
ambari.ldap.manage_services: true
ambari.ldap.enabled_services: *
Saving LDAP properties...
Saving LDAP properties finished
Ambari Server 'setup-ldap' completed successfully.{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-07 14:22:19,6
13202950,Ambari should optionally generate auth-to-local rules for the Kerberos identities of all components of installed services,"Ambari should optionally generate auth-to-local rules for the Kerberos identities of all components of installed services.  

Currently Ambari will generate auth-to-local rules for the installed components of installed services.  This is generally the accepted behavior. However, there may be cases where identities from remote clusters (using the same Kerberos realm) need to be translated to local names.  

A use case may be that some slave component for a service is installed on a remote cluster, but that component is not installed on the local cluster.  However a master component of that service is installed on the local cluster and the slave component from the remote cluster needs to communicate with it. 

The solution is to add a new property to {{kerberos-env}}, maybe named something like {{include_all_components_in_auth_to_local_rules}}, where the default value is {{false}}.  If set to {{true}}, when building the auth-to-local rules, Ambari should add the rules for all components of installed services, not just the installed components (which is what it does today).  

The relevant code to change is in {{org.apache.ambari.server.controller.KerberosHelperImpl#setAuthToLocalRules}}. 


",kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-07 02:34:56,6
13202801,Set unique configuration version tag,"Add Service does not work for add-delete-add scenario, because configuration tag is constant {{ADD_SERVICE}}.  It should include some changing part, eg. timestamp suffix.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-12-06 13:24:25,5
13202791,Process Kerberos descriptor for Add Service request,"The goal of this task is to process the Kerberos descriptor provided in the Add Service request, and merge it with the cluster's existing one.  Only settings for newly added services should be accepted.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-06 12:35:10,5
13202787,Support component-level provision_action in Add Service request,"AMBARI-24988 added support for request-level {{provision_action}} to the Add Service request.  The goal of this task is to add support at the host component-level, similar to blueprints.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-06 11:53:50,5
13202756,Ambari hides information about cred_store generation failures. Resulting in confusing errors at later stages,"Component was failing to install due to:

    
    
    Caught an exception while executing custom service command: <type 'exceptions.OSError'>: [Errno 2] No such file or directory: '/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks'; [Errno 2] No such file or directory: '/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks'
    
    Command failed after 1 tries
    

The reason was an empty password provided in blueprint. However it took lots
of time to debug this. Since ambari won't show any information regarding
failures during cred_store generation.

The goal is too fail earlier and show output of failed generation command.  
So with the patch it looks like this:

    
    
    Caught an exception while executing custom service command: <class 'resource_management.core.exceptions.ExecutionFailed'>: Execution of '/usr/lib/jvm/java-openjdk/bin/java -cp '/var/lib/ambari-agent/cred/lib/*' org.apache.hadoop.security.alias.CredentialShell create dpprofiler.spnego.signature.secret -value '[PROTECTED]' -provider jceks://file/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks' returned 1. SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
    Dec 06, 2018 11:05:45 AM org.apache.hadoop.util.NativeCodeLoader <clinit>
    WARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    java.lang.IllegalArgumentException: Empty key
    	at javax.crypto.spec.SecretKeySpec.<init>(SecretKeySpec.java:96)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.innerSetCredential(AbstractJavaKeyStoreProvider.java:304)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.createCredentialEntry(AbstractJavaKeyStoreProvider.java:269)
    	at org.apache.hadoop.security.alias.CredentialShell$CreateCommand.execute(CredentialShell.java:365)
    	at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:68)
    	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    	at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:442); Execution of '/usr/lib/jvm/java-openjdk/bin/java -cp '/var/lib/ambari-agent/cred/lib/*' org.apache.hadoop.security.alias.CredentialShell create dpprofiler.spnego.signature.secret -value '[PROTECTED]' -provider jceks://file/var/lib/ambari-agent/cred/conf/dp_profiler_agent/dpprofiler-config.jceks' returned 1. SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
    SLF4J: Defaulting to no-operation (NOP) logger implementation
    SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
    Dec 06, 2018 11:05:45 AM org.apache.hadoop.util.NativeCodeLoader <clinit>
    WARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
    java.lang.IllegalArgumentException: Empty key
    	at javax.crypto.spec.SecretKeySpec.<init>(SecretKeySpec.java:96)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.innerSetCredential(AbstractJavaKeyStoreProvider.java:304)
    	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.createCredentialEntry(AbstractJavaKeyStoreProvider.java:269)
    	at org.apache.hadoop.security.alias.CredentialShell$CreateCommand.execute(CredentialShell.java:365)
    	at org.apache.hadoop.security.alias.CredentialShell.run(CredentialShell.java:68)
    	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
    	at org.apache.hadoop.security.alias.CredentialShell.main(CredentialShell.java:442)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-12-06 11:13:08,0
13202729,Directory/File creation hangs if relative path is supplied with cd_access,"
    Directory('hadoofs/fs1/ams-hbase-wal', owner='root', group='root', create_parents=True, recursive_ownership=True, cd_access='a')
    

Hangs as seen on s3a clusters when one of paths was set to relative by an
accident.

",pull-request-available,[],AMBARI,Bug,Major,2018-12-06 08:56:36,0
13202670,Starting JPA persistence service sometimes throws IllegalStateException,"Starting JPA persistence service sometimes throws IllegalStateException.  

For example:
{noformat}
Exception in thread ""main"" java.lang.IllegalStateException: Persistence service was already initialized.
	at com.google.common.base.Preconditions.checkState(Preconditions.java:173)
	at com.google.inject.persist.jpa.JpaPersistService.start(JpaPersistService.java:104)
	at com.google.inject.persist.jpa.AmbariJpaPersistService.start(AmbariJpaPersistService.java:27)
	at 
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-06 01:22:57,7
13202506,START_ONLY provision action may be applied to the wrong component,"If component XY is provisioned with {{START_ONLY}}, and component X is a prefix of XY, then the provision action is applied to both X and XY.

{noformat:title=blueprint}
{
  ""host_groups"": [
    {
      ""components"": [
        { ""name"": ""HIVE_SERVER"" },
        { ""name"": ""HIVE_SERVER_INTERACTIVE"", ""provision_action"": ""START_ONLY"" },
...
{noformat}

{noformat:title=ambari-server.log}
AmbariManagementControllerImpl:3149 - Skipping create of INSTALL task for HIVE_SERVER on c7401.ambari.apache.org.
AmbariManagementControllerImpl:3149 - Skipping create of INSTALL task for HIVE_SERVER_INTERACTIVE on c7401.ambari.apache.org.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-12-05 11:29:54,5
13202505,Disallow changing Kerberos-related configs in Add Service request,"Ambari should not allow a user to set configurations in the {{kerberos-env}} and {{krb5-conf}} configuration types within the complex Add Service request.  Generally, setting these configurations on an existing Kerberized cluster could be problematic, since existing services already depend upon the existing configuration to interact with the KDC.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-05 11:29:41,5
13202494,Not supported actions(like Start/Stop) should not be shown for HDFS/YARN clients in WL cluster with externalized HDFS/YARN,"Start/Stop operations should not be shown/should be disabled for HDFS/YARN clients.
But as of Ambari 2.7, they are visible in Service actions. 

Also there are multiple other actions which are currently enabled and are not applicable for the clients(listed below). These should be masked in the UI and not be shown to the user. Please clarify , if there is any discrepancy in understanding.

*HDFS Client :*
Manage Journal Nodes
Add new HDFS Namespace
Rebalance HDFS
restart all

*YARN Client:*
Refresh YARN Capacity Scheduler.
Enable resourceManager HA
restart All",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-05 11:17:24,4
13202253,Commands timeout if stdout has non-unicode symbols.,"
    
    ERROR 2018-12-03 18:08:08,694 ActionQueue.py:198 - Exception while processing EXECUTION_COMMAND command
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 191, in process_command
        self.execute_command(command)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 379, in execute_command
        self.commandStatuses.put_command_status(command, role_result)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CommandStatusDict.py"", line 77, in put_command_status
        is_sent, correlation_id = self.force_update_to_server({command['clusterId']: [report]})
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CommandStatusDict.py"", line 95, in force_update_to_server
        correlation_id = self.initializer_module.connection.send(message={'clusters':reports_dict}, destination=Constants.COMMANDS_STATUS_REPORTS_ENDPOINT, log_message_function=CommandStatusDict.log_sending)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/security.py"", line 137, in send
        body = json.dumps(message)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/__init__.py"", line 230, in dumps
        return _default_encoder.encode(obj)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 202, in encode
        chunks = list(chunks)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 426, in _iterencode
        for chunk in iterencode_dict(o, current_indent_level):
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 400, in _iterencode_dict
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 400, in _iterencode_dict
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 323, in _iterencode_list
        for chunk in chunks:
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 382, in _iterencode_dict
        yield _encoder(value)
      File ""/usr/lib/ambari-agent/lib/ambari_simplejson/encoder.py"", line 48, in py_encode_basestring_ascii
        s = s.decode('utf-8')
      File ""/usr/lib64/python2.7/encodings/utf_8.py"", line 16, in decode
        return codecs.utf_8_decode(input, errors, True)
    UnicodeDecodeError: 'utf8' codec can't decode byte 0xea in position 90211: invalid continuation byte
    

",pull-request-available,[],AMBARI,Bug,Major,2018-12-04 12:24:11,0
13202245,Knox: Reorder of query parameters breaks Ambari logic for retrieving configs,"Knox proxy sometimes modifies service config version request during transferring.
I extended logging on the ambari server to track actual requests. 
Direct API request:
{noformat}
https://172.27.16.232:8443/api/v1/clusters/cl1/configurations/service_config_versions?
service_name=HBASE&service_config_version.in(12)%7C
service_name.in(ATLAS,YARN,RANGER,HIVE,HDFS,MAPREDUCE2,SPARK2,TEZ,DRUID,ZOOKEEPER,AMBARI_METRICS,KAFKA,RANGER_KMS)&is_current=true&_=1542046206583
{noformat}
Via Knox:
{noformat}
https://ctr-e138-1518143905142-579999-01-000002.hwx.site/api/v1/clusters/cl1/configurations/service_config_versions?
service_name=HBASE&is_current=true&service_config_version.in(12)%7C
service_name.in(ATLAS,YARN,RANGER,HIVE,HDFS,MAPREDUCE2,SPARK2,TEZ,DRUID,ZOOKEEPER,AMBARI_METRICS,KAFKA,RANGER_KMS)&_=1542046206583
{noformat}

In both cases same request was used on client side:
{noformat}
/api/v1/clusters/cl1/configurations/service_config_versions?
service_name=HBASE&service_config_version.in(12)|
service_name.in(ATLAS,YARN,RANGER,HIVE,HDFS,MAPREDUCE2,SPARK2,TEZ,DRUID,ZOOKEEPER,AMBARI_METRICS,KAFKA,RANGER_KMS)&is_current=true&_=1542046206583
{noformat}

As we can see Knox proxy reordered request parameters (is_current=true was moved) and completely changed request logic.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-12-04 11:44:29,4
13202203,Support provision_action in complex Add Service request,"Support {{provision_action}} in complex Add Service request, similarly to blueprints.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-12-04 08:56:37,5
13201939,Use Ambari CLI to enable and disable trusted proxy support in Ambari,"Use Ambari CLI to enable and disable trusted proxy support in Ambari.

Information needed to be collected:
 * Enable/Disable trusted proxy support
 ** {{ambari.tproxy.authentication.enabled}} : ""true""|""false""
 * Trusted proxy user (the authenticated user allowed to declare a proxied user) details - One or more may be specified
 ** hosts from which the proxy user can connect
 *** {{ambari.tproxy.proxyuser.PROXY_USER.hosts}} : * or a comma-delimited list of hostname, ip address, CIDR Notation (intermixed is ok)
 ** users allowed to be specified as proxied users by the proxy user
 *** {{ambari.tproxy.proxyuser.PROXY_USER.users}} : *, or a comma-delimited list of usernames
 ** group for which users to be proxied are members
 *** {{ambari.tproxy.proxyuser.PROXY_USER.groups}} : *, or a comma-delimited list of group names

{noformat}
[root@c7402 ~]# ambari-server  setup-trusted-proxy
Using python  /usr/bin/python
Enter Ambari Admin login: admin
Enter Ambari Admin password:

Fetching Trusted Proxy configuration from DB.
Trusted Proxy support is currently disabled
Do you want to configure Trusted Proxy support [y/n] (y)?  y
The proxy user's (local) username? knox  
Allowed hosts for knox (*)? knox.ambari.apache.org
Allowed users for knox (*)? *
Allowed groups for knox (*)? users
Add another proxy user [y/n]?  y
The proxy user's (local) username? admin  
Allowed hosts for admin (*)? 192.168.74.0/24 
Allowed users for admin (*)? tom, sam, admin
Allowed groups for admin (*)? admin_users
Add another proxy user [y/n]?  n
Save settings [y/n] (y)? y
Saving Trusted Proxy configuration...
Saving Trusted Proxy configuration finished
Ambari Server ' setup-trusted-proxy' completed successfully.
{noformat}
The REST API calls to get and set the trusted proxy configurations are
{noformat:title=GET request}
GET /api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration
{noformat}
{noformat:title=GET example response}
{
  ""href"" : ""http://c7401.ambari.apache.org:8080/api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/tproxy-configuration"",
  ""Configuration"" : {
    ""category"" : ""tproxy-configuration"",
    ""component_name"" : ""AMBARI_SERVER"",
    ""service_name"" : ""AMBARI"",
    ""properties"" : {
      ""ambari.tproxy.authentication.enabled"" : ""true"",
      ""ambari.tproxy.proxyuser.admin.groups"" : ""admin_users"",
      ""ambari.tproxy.proxyuser.admin.hosts"" : ""192.168.74.0/24"",
      ""ambari.tproxy.proxyuser.admin.users"" : ""sam, tom, admin"",
      ""ambari.tproxy.proxyuser.knox.groups"" : ""users"",
      ""ambari.tproxy.proxyuser.knox.hosts"" : ""c7401.ambari.apache.org"",
      ""ambari.tproxy.proxyuser.knox.users"" : ""*""
    },
    ""property_types"" : {
      ""ambari.tproxy.authentication.enabled"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.admin.groups"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.admin.hosts"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.admin.users"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.knox.groups"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.knox.hosts"" : ""PLAINTEXT"",
      ""ambari.tproxy.proxyuser.knox.users"" : ""PLAINTEXT""
    }
  }
}
{noformat}
{noformat:title=POST request}
POST /api/v1/services/AMBARI/components/AMBARI_SERVER/configurations
{
  ""Configuration"": {    
    ""category"" : ""tproxy-configuration"",
    ""properties"": {
      ""ambari.tproxy.authentication.enabled"" : ""true"",
      ""ambari.tproxy.proxyuser.knox.hosts"": ""c7401.ambari.apache.org"",
      ""ambari.tproxy.proxyuser.knox.users"": ""*"",
      ""ambari.tproxy.proxyuser.knox.groups"": ""users"",
      ""ambari.tproxy.proxyuser.admin.hosts"": ""192.168.74.0/24"",
      ""ambari.tproxy.proxyuser.admin.users"": ""sam, tom, admin"",
      ""ambari.tproxy.proxyuser.admin.groups"": ""admin_users""
    }
  }
}{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-12-03 09:27:41,6
13201894,Handle requests from a configured trusted proxy to identify a proxied user using Kerberos,"Handle requests from a configured trusted proxy to identify a proxied user using Kerberos.

Upon receiving a request where that caller is identified using Kerberos, check to see of the request was from a (trusted) proxy.  If so, validate the trusted proxy and set the authenticated user to the proxied user specified in the ""{{doAs}}"" query parameter. 

After receiving a request where the user is to be authenticated using Kerberos, perform the following steps:
# Determine if a proxied user is specified using a ""{{doAs}}"" query parameter.  
# Using the following Ambari configuration property, determine if a proxied user can be specified from the requesting host:
** {{ambari.tproxy.proxyuser.$username.hosts}}, where $username is the username of the authenticated user (not the user specified in the doAs query parameter)
# Obtain the proxied username from the {{doAs}} query parameter
# Using the following Ambari configuration property, determine if the proxied user can be specified based on the user's username:
** {{ambari.tproxy.proxyuser.$username.users}}, where $username is the username of the authenticated user 
# Using the following Ambari configuration property, determine if the proxied user can be specified based on the groups the proxied user belong to:
** {{ambari.tproxy.proxyuser.$username.groups}}, where $username is the username of the authenticated user t",pull-request-available tproxy,['ambari-server'],AMBARI,Task,Major,2018-12-02 21:29:38,7
13201811,APT/DPKG existence check broken for packages with long names,"AMBARI-24632 addressed package existence check for system packages.  However, it may still not work correctly for packages with longer names, depending on the environment:

{noformat}
output-2.txt:2018-12-01 19:33:54,576 - Installing package unzip ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install unzip')
output-2.txt:2018-12-01 19:34:02,873 - Installing package hdp-select ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install hdp-select')
output-2.txt:2018-12-01 19:34:05,443 - Installing package zookeeper-3-0-1-0-187 ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187')
output-2.txt:2018-12-01 19:34:08,832 - Installing package zookeeper-3-0-1-0-187-server ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187-server')
output-3.txt:2018-12-01 19:34:14,450 - Installing package zookeeper-3-0-1-0-187 ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187')
output-3.txt:2018-12-01 19:34:16,253 - Installing package zookeeper-3-0-1-0-187-server ('/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install zookeeper-3-0-1-0-187-server')
{noformat}

This was reproduced with non-root agent user.",pull-request-available,[],AMBARI,Bug,Minor,2018-12-01 19:39:33,5
13201805,JournalNode may fail to start due to unreadable config file,"During startup, JournalNode reads config files that belong to YARN, not HDFS (eg. {{yarn-site.xml}}), if they are present in the Hadoop config dir.

Ambari's {{File()}} provider creates files at the final location, then sets metadata (ownership and permissions).  This makes the file visible to other processes with possibly the wrong metadata.

If agent-side parallel execution is enabled, JournalNode may be started concurrently with some YARN component.  JournalNode may encounter a config file that it cannot read, because the file's permissions and owner are still 640 and root, respectively, which causes it to shutdown:

{noformat}
ERROR conf.Configuration (Configuration.java:loadResource(2999)) - error parsing conf yarn-site.xml
java.io.FileNotFoundException: /etc/hadoop/conf/yarn-site.xml (Permission denied)
{noformat}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-12-01 16:53:00,5
13201661,Improve message order preserving,"We need to update spring to 5.x version and use message order preserving option.
[https://docs.spring.io/spring/docs/5.1.0.BUILD-SNAPSHOT/spring-framework-reference/web.html#websocket-stomp-ordered-messages]",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-30 16:04:56,1
13201652,Loading on Dashboard stuck,"Dashboard doesn't load, spinner keeps spinning infinitely due to incorrect work of App.get('isHaEnabled') property in a cluster which has only HDFS_CLIENT from HDFS service.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-30 15:23:18,4
13201378,Sometimes Task Log is not refreshed in UI after operation completes,"Task Log in Background Operations popup is sometimes not refreshed after the operation completes. The log may be completely empty or may just show a previous 10-line snippet. The breadcrumb at the top is updated with the status, though. Navigating back and forth in the popup fixes it.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-29 16:08:02,1
13201324,Improve Add Service request validation,Improve request validation for Add Service request (eg. reject request to add existing service).,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-11-29 11:51:37,5
13201259,Document enabling LDAP configuration management,"Document enabling LDAP configuration management in {{.../ambari-server/docs/security/ldap}}

The document should be similar to [https://github.com/apache/ambari/blob/trunk/ambari-server/docs/security/sso/index.md].",pull-request-available,['ambari-server'],AMBARI,Documentation,Major,2018-11-29 07:02:24,6
13201189,Start Namenode failing during Move master NN wizard on non-HA cluster with custom hdfs service user,"Start Namenode failing during Move master NN wizard.

From NN logs:

{code:java}
2018-11-21 19:43:57,126 WARN Encountered exception loading fsimage java.io.IOException: NameNode is not formatted. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:237) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1090) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2018-11-21 19:43:57,131 INFO Stopped o.e.j.w.WebAppContext@58359ebd{/,null,UNAVAILABLE}{/hdfs}
2018-11-21 19:43:57,135 INFO Stopped ServerConnector@75e91545{HTTP/1.1,[http/1.1]}{ctr-e139-1542663976389-4877-02-000004.hwx.site:50070}
2018-11-21 19:43:57,136 INFO Stopped o.e.j.s.ServletContextHandler@2f4205be{/static,file:///usr/hdp/3.1.0.0-13/hadoop-hdfs/webapps/static/,UNAVAILABLE}
2018-11-21 19:43:57,136 INFO Stopped o.e.j.s.ServletContextHandler@319bc845{/logs,file:///grid/0/log/hdfs/cstm-hdfs/,UNAVAILABLE}
2018-11-21 19:43:57,138 INFO Stopping NameNode metrics system...
2018-11-21 19:43:57,139 INFO timeline thread interrupted.
2018-11-21 19:43:57,140 INFO NameNode metrics system stopped.
2018-11-21 19:43:57,141 INFO NameNode metrics system shutdown complete.
2018-11-21 19:43:57,141 ERROR Failed to start namenode. java.io.IOException: NameNode is not formatted. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:237) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1090) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:632) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:694) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937) at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:910) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1643) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
2018-11-21 19:43:57,143 INFO No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.
2018-11-21 19:43:57,143 INFO Exiting with status 1: java.io.IOException: NameNode is not formatted.
2018-11-21 19:43:57,145 INFO SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at ctr-e139-1542663976389-4877-02-000004.hwx.site/172.27.25.135 ************************************************************/
{code}

Ambari task logs

{code:java}
2018-11-22 03:31:30,588 - The NameNode is still in Safemode. Please be careful with commands that need Safemode OFF.
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 408, in <module>
    NameNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 352, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 138, in start
    upgrade_suspended=params.upgrade_suspended, env=env)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 264, in namenode
    create_hdfs_directories(name_service)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 336, in create_hdfs_directories
    nameservices=name_services
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 677, in action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 674, in action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 373, in action_delayed
    self.action_delayed_for_nameservice(None, action_name, main_resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 395, in action_delayed_for_nameservice
    self._assert_valid()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 334, in _assert_valid
    self.target_status = self._get_file_status(target)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 497, in _get_file_status
    list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 214, in run_command
    return self._run_command(*args, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 282, in _run_command
    _, out, err = get_user_call_output(cmd, user=self.run_user, logoutput=self.logoutput, quiet=False)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/get_user_call_output.py"", line 62, in get_user_call_output
    raise ExecutionFailed(err_msg, code, files_output[0], files_output[1])
resource_management.core.exceptions.ExecutionFailed: Execution of 'curl -sS -L -w '%{http_code}' -X GET -d '' -H 'Content-Length: 0' --negotiate -u : 'http://ctr-e139-1542663976389-4877-02-000004.hwx.site:50070/webhdfs/v1/tmp?op=GETFILESTATUS' 1>/tmp/tmpC3TR3n 2>/tmp/tmpTlrH_1' returned 7. curl: (7) Failed to connect to ctr-e139-1542663976389-4877-02-000004.hwx.site port 50070: Connection refused
000
{code}

Customized service users and ambari agent user is enabled in the test.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-11-28 20:41:28,4
13201082,Summary panel should not have any information if only client component is installed in a cluster,"# [^HDFS_client_installed.png] does not list number of HDFS clients installed
# Other data in summary panel fetched from ambari metrics api should be made hidden (like Disk usage etc)
# Same needs to be done for Yarn client service
",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-28 12:30:42,4
13200888,Remove warning about requirement for IPA password policy without expiration in Ambari kerberos wizard,"The Ambari kerberos wizard for Existing FreeIPA displays a warning about setting up a password policy without expiration for the kerberos principals.

As these (user and service) principals are not created with a password, the password expiration policy does not apply to them. I verified this by maintaining a cluster by maintaining a kerberized cluster for 120+ days, where the password for my ldapbind (and other accounts that do have passwords) expired in 90 days per default policy, without any impact to my kerberos principals or cluster operations.

Unless we've seen contradictory information, let's please remove this warning from the wizard to avoid confusing users on what is needed here.",pull-request-available,['ambari-web'],AMBARI,Task,Minor,2018-11-27 15:56:38,6
13200655,Create Tproxy configuration provider and supporting infrastructure,"Create Tproxy configuration provider and supporting infrastructure.

Also create a common configuration provider infrastructure for Ambari Server Configuration data. 
",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-26 19:15:58,7
13200605,Counter installedClients for AMBARI_INFRA_SOLR is null,"App.Service.find('AMBARI_INFRA_SOLR').get('installedClients') = null,
Should return number of installed clients.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-26 15:40:17,4
13200517,Use Ambari CLI to specify which services should be setup for LDAP integration,"Use Ambari CLI to specify which services should be setup for LDAP integration.
{noformat:title=Example}
[root@c7402 ~]# ambari-server setup-ldap
Using python  /usr/bin/python
Currently 'no auth method' is configured, do you wish to use LDAP instead [y/n] (y)? y
Enter Ambari Admin login: admin
Enter Ambari Admin password:

Fetching LDAP configuration from DB. No configuration.
Please select the type of LDAP you want to use [AD/IPA/Generic](Generic):
Primary LDAP Host (ldap.ambari.apache.org): c7401.ambari.apache.org
Primary LDAP Port (389):
Secondary LDAP Host <Optional>:
Secondary LDAP Port <Optional>:
Use SSL [true/false] (false):
User object class (posixUser):
User ID attribute (uid):
User group member attribute (memberOf): 
Group object class (posixGroup):
Group name attribute (cn):
Group member attribute (memberUid):
Distinguished name attribute (dn):
Search Base (dc=ambari,dc=apache,dc=org):
Referral method [follow/ignore] (follow):
Bind anonymously [true/false] (false):
Bind DN (uid=ldapbind,cn=users,dc=ambari,dc=apache,dc=org): uid=admin,cn=users,dc=ambari,dc=apache,dc=org
Enter Bind DN Password:
Confirm Bind DN Password:
Handling behavior for username collisions [convert/skip] for LDAP sync (skip):
Force lower-case user names [true/false]:true
Results from LDAP are paginated when requested [true/false]:true
Use LDAP authentication for Ambari [y/n] (n)?
Manage LDAP configurations for eligible services [y/n] (n)? y
 Manage LDAP for all services [y/n] (n)?
    Manage LDAP for HDFS [y/n] (y)? y
    Manage LDAP for YARN [y/n] (y)? y
    ...
Save settings [y/n] (y)? y
Saving LDAP properties...
Saving LDAP properties finished
Ambari Server 'setup-ldap' completed successfully.
{noformat}
NOTE: this will require obtaining an Ambari administrator username and password to GET, PUT, and POST to the Ambari REST API.

Note: ""User group member attribute (memberOf)"" is to be added to populate the existing {{ambari.ldap.attributes.user.group_member_attr}} Ambari configuration property (See {{org.apache.ambari.server.configuration.AmbariServerConfigurationKey#USER_GROUP_MEMBER_ATTRIBUTE}})",pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-11-26 10:02:26,6
13200285,Test ordering issue in ExecutionCommandWrapperTest,"Success/failure of test cases in {{ExecutionCommandWrapperTest}} depends on the execution order.  The two interdependent tests are {{testExecutionCommandNoRepositoryFile}} and {{testGetExecutionCommand}}, executing the former first makes the latter fail.

{noformat:title=order 1}
  @Test
  public void testExecutionCommandNoRepositoryFile_first() throws Exception {
    testExecutionCommandNoRepositoryFile();
    testGetExecutionCommand();
  }
{noformat}

{noformat:title=result 1}
[INFO] Running org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 11.002 s <<< FAILURE! - in org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
[ERROR] testExecutionCommandNoRepositoryFile_first(org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest)  Time elapsed: 0.031 s  <<< FAILURE!
java.lang.AssertionError
	at org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest.testGetExecutionCommand(ExecutionCommandWrapperTest.java:202)
	at org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest.testExecutionCommandNoRepositoryFile_first(ExecutionCommandWrapperTest.java:320)
{noformat}

{noformat:title=order 2}
  @Test
  public void testGetExecutionCommand_first() throws Exception {
    testGetExecutionCommand();
    testExecutionCommandNoRepositoryFile();
  }
{noformat}

{noformat:title=result 2}
[INFO] Running org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.304 s - in org.apache.ambari.server.actionmanager.ExecutionCommandWrapperTest
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-23 16:18:51,5
13200186,Support for complex Add Service request in secure cluster,Support adding services using the complex Add Service request in clusters that have Kerberos enabled.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-23 06:28:49,5
13200038,Dir creation fails if webhdfs is enabled,"The previous clusters had webhdfs disabled as a workaround.

With webHDFS turned back on, dir creation continues to fail.

    
    
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 90, in <module>
        AmsCollector().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 345, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 48, in start
        self.configure(env, action = 'start') # for security
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py"", line 43, in configure
        hbase('master', action)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/hbase.py"", line 228, in hbase
        dfs_type=params.dfs_type
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 712, in action_create_on_execute
        self.action_delayed(""create"")
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 709, in action_delayed
        self.get_hdfs_resource_executor().action_delayed(action_name, self)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 385, in action_delayed
        self.action_delayed_for_nameservice(None, action_name, main_resource)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 407, in action_delayed_for_nameservice
        self._assert_valid()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 346, in _assert_valid
        self.target_status = self._get_file_status(target)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 509, in _get_file_status
        list_status = self.util.run_command(target, 'GETFILESTATUS', method='GET', ignore_status_codes=['404'], assertable_result=False)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 226, in run_command
        return self._run_command(*args, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 294, in _run_command
        _, out, err = get_user_call_output(cmd, user=self.run_user, logoutput=self.logoutput, quiet=False)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/get_user_call_output.py"", line 62, in get_user_call_output
        raise ExecutionFailed(err_msg, code, files_output[0], files_output[1])
    resource_management.core.exceptions.ExecutionFailed: Execution of 'curl -sS -L -w '%{http_code}' -X GET -d '' -H 'Content-Length: 0' 'http://fakelocalhost:50070/webhdfs/v1s3a:/cloudhdp-dl-s3-2/c01/amshbase?op=GETFILESTATUS&user.name=hdfs' 1>/tmp/tmpsSkYRy 2>/tmp/tmpYQM8tv' returned 6. curl: (6) Could not resolve host: fakelocalhost; Unknown error
    000
    

",pull-request-available,[],AMBARI,Bug,Major,2018-11-22 10:38:38,0
13199817,Accept legacy JSON configuration in Add Service request,"Cluster creation via blueprint accepts configuration in the following format, where the {{""properties""}} level is omitted:

{noformat}
  ""configurations"": [
    {
      ""cluster-env"": {
        ""custom-property"": ""whatever""
      }
    },
    {
      ""zoo.cfg"": {
        ""syncLimit"": ""7""
      }
    },
...
{noformat}

""Add Service"" request should accept the same format, too, but currently it results in:

{noformat}
<h3>Caused by:</h3><pre>java.lang.IllegalArgumentException: Invalid fields in cluster-env configuration: [custom-property]
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:145)
	at org.apache.ambari.server.topology.ConfigurableHelper.lambda$parseConfigs$1(ConfigurableHelper.java:102)
	at java.util.ArrayList.forEach(ArrayList.java:1249)
	at org.apache.ambari.server.topology.ConfigurableHelper.parseConfigs(ConfigurableHelper.java:88)
	at org.apache.ambari.server.controller.AddServiceRequest.&lt;init&gt;(AddServiceRequest.java:88)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:124)
	at com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:283)
	at com.fasterxml.jackson.databind.deser.ValueInstantiator.createFromObjectWith(ValueInstantiator.java:229)
	at com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:195)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:488)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:159)
	at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4001)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2992)
	at org.apache.ambari.server.controller.AddServiceRequest.of(AddServiceRequest.java:115)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.createAddServiceRequest(ServiceResourceProvider.java:1242)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.processAddServiceRequest(ServiceResourceProvider.java:1232)
	at org.apache.ambari.server.controller.internal.ServiceResourceProvider.createResourcesAuthorized(ServiceResourceProvider.java:257)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-11-21 12:07:55,5
13199678,Bug at MetricsCollectorHAManager class,"Here org/apache/ambari/server/controller/metrics/MetricsCollectorHAManager.java:80
the second return statement is inside a loop without any condition. That breaks an execution flow
{code:java}
    if (externalMetricCollectorsState.containsKey(clusterName)) {
      for (String externalCollectorHost : externalMetricCollectorsState.get(clusterName).keySet()) {
        if (externalMetricCollectorsState.get(clusterName).get(externalCollectorHost)) {
          return externalCollectorHost;
        }
        return refreshAndReturnRandomExternalCollectorHost(clusterName);
      }
    }{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-11-20 19:35:23,1
13199620,Recommendation configs request was failed with custom config group,"Recommendation configs request was failed with following issue:
{code:java}
Request body is not correct, error: org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse$ConfigGroup cannot be cast to java.lang.Comparable{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-11-20 15:05:19,1
13199568,Add LDAP integration support information to service information via Ambari's REST API,"Add LDAP integration support information to service information via Ambari's REST API. This information should be usable by Ambari's search predicate feature.

New _read-only_ properties for (stack) services should be:
 * *{{ldap_integration_supported}}* - Indicates whether the service supports LDAP integration or not
 ** Information is expected to be determined by service's meta info (see AMBARI-24907)

New _read-only_ properties for installed services should be:
 * *{{ldap_integration_supported}}* - Indicates whether the service supports LDAP integration or not
 ** Information is expected to be determined by service's meta info (see AMBARI-24907 )
 * *{{ldap_integration_enabled}}* - Indicates whether the service is configured for LDAP integration or not
 ** Information is expected to be determined by a value indicated in the service's meta info (see AMBARI-24907)
 * *{{ldap_integration_desired}}* - Indicates whether the service is chosen for LDAP integration or not (see AMBARI-24913)
 ** Information is expected to be in the Ambari configurations with the property name {{ldap_enabled_services}}

Examples:
{noformat:title=Get stack service details}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME"",
{
  ""href"" : "":URL"",
  ""StackServices"" : {
     ...
     ""ldap_integration_supported"": ""false"",
     ...
  },
  ...
{noformat}
{noformat:title=Get installed service information}
GET /api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME
{
  ""href"" : "":URL"",
  ""ServiceInfo"" : {
    ""cluster_name"" : "":CLUSTER_NAME"",
    ...
    ""ldap_integration_supported"": ""true"",
    ""ldap_integration_enabled"": ""false"",
    ""ldap_integration_desired"": ""false"",
     ...
    },
    ...
{noformat}
{noformat:title=List installed services that support LDAP integration}
GET /api/v1/clusters/:CLUSTER_NAME/services?ServiceInfo/ldap_integration_supported=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}
{noformat:title=List stack services that support LDAP integration}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services?StackServices/ldap_integration_supported=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Blocker,2018-11-20 10:13:57,6
13199537,Apply user-defined configuration for Add Service request,"Continuing AMBARI-24917, apply any configuration specified in the request to override the stack defaults.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-11-20 07:46:05,5
13199424,Create tproxy-configuration category in Ambari Configurations data	,"Create tproxy-configuration category in Ambari Configurations data with the following properties:

* {{ambari.tproxy.authentication.enabled}}
** Determines whether to allow trusted proxy authentication when logging into Ambari
** {{true}} | {{false}}
* {{ambari.tproxy.proxyuser.$username.hosts}}
** List of hosts from which trusted-proxy user ‘$username’ can connect from
** {{\*}} | {{c7401.ambari.apache.org}} | {{10.42.80.64,10.42.80.65}} | {{10.222.0.0/16,10.113.221.221}}
* {{ambari.tproxy.proxyuser.$username.users}}
** List of users which the trusted-proxy user ‘$username’ can proxy for
** {{\*}} | {{user1,user2}}
* {{ambari.tproxy.proxyuser.$username.groups}}
** List of user-groups which trusted-proxy user ‘$username’ can proxy for
** {{\*}} | {{group1,group2}}

Note: {{$username}} is variable, declaring the values for a particular proxy user. For example ""knox"".",pull-request-available tproxy,['ambari-server'],AMBARI,Task,Major,2018-11-19 20:11:36,7
13199320,No need to create test jar if tests are skipped,"Some test-related tasks ({{create-sample-upgrade-check-jar}}, {{generate-test-oozie2-checks-dir}}, {{generate-test-oozie2-server-actions-dir}}) can be skipped during build if tests are skipped.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-11-19 12:22:32,5
13199299,"If service does not have client service component created then ""Run Service Check"" option should be made hidden","If service does not have client service component created then ""Run Service Check"" option should be made hidden",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-19 11:19:49,4
13199246,Implement complex Add Service request using default configs,"Continuing AMBARI-24901, create components and host components for the Add Service request.  Also create stack default configs, necessary for install and start tasks.

User-defined and advisor-recommended configs to be implemented separately.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-19 07:29:15,5
13199242,Ambari Server setup for non embedded DB miss out on default configuration,"If we setup Ambari Server with it's own embedded postgresql DB, we get all other configurations correctly e.g. setup command updates home.java, server.os_type, server.os_family etc in ambari.properties file that can be used during starting Ambari Server.

 

However, if we provide server setup command with arguments to use non-embedded standalone DB, server setup does not update ambari.properties and does not perform any further setup utility that can be leveraged during starting Ambari Server.

Example:

1) *ambari-server setup -j \{JDK_PATH} -s*

This command updates ambari.properties for home.java, server.os_type, server.os_family etc.

 

2) *ambari-server setup --java-home=\{JDK_PATH} --jdbc-db=postgres --jdbc-driver=\{JDBC_DRIVER_PATH} --databasehost=\{IP_ADDR} --databaseport=5432 --databasename=ambari --postgresschema=ambari --databaseusername=\{DB_USER} --databasepassword=\{DB_PASSWD} --database=postgres -s*

This command does not update ambari.properties and we need to update it manually to bring up Ambari Server. Hence, automation of server setup with server startup is blocked due to this bug.",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-11-19 06:59:11,5
13198906,Update ldap-configuration category in Ambari Configurations data to contain properties to aid in automated LDAP configuration,"Update {{ldap-configuration}} category in Ambari Configurations data to contain properties to aid in automated LDAP configuration:
 * {{ambari.ldap.manage_services}}
 ** This property is used to indicate whether Ambari is to manage relevant services' LDAP configurations or not (""true"" | ""false"")
 * {{ambari.ldap.enabled_services}}
 ** This property is used to declare what services are expected to be configured for LDAP and is expected to be a comma-delimited list of services or ""{{*}}"" to indicate all services.

Examples:
{code:java|title=All services}
""ambari.ldap.manage_services"":""true"",
""ambari.ldap.enabled_services"":""*""
{code}
{code:java|title=Only Ranger}
""ambari.ldap.manage_services"":""true"",
""ambari.ldap.enabled_services"":""Ranger""
{code}
{code:java|title=Ranger, and Atlas}
""ambari.ldap.manage_services"":""true"",
""ambari.ldap.enabled_services"":""Ranger, Atlas""
{code}
{code:java|title=Do not manage services}
""ambari.ldap.manage_services"":""false"",
""ambari.ldap.enabled_services"":""""
{code}
Each service in the set of services should have indicated it supports LDAP (see BUG-114409) else it will silently be ignored.

This value should be set via Ambari's REST API or a Blueprint.

Upon setting this value via the Ambari REST API, it is expected that internal logic will be triggered to ensure the relevant services in the list are configured for LDAP or not configured for LDAP as the case may be.
 ",pull-request-available,"['ambari-server', 'security']",AMBARI,Task,Blocker,2018-11-16 12:55:11,6
13198654,Update service metainfo to declare LDAP integration support,"Update service metainfo to declare LDAP integration support. The following tag may be optionally set in a service's metainfo.xml file:
{noformat}
<ldap>
 <supported>true</supported>
 <ldapEnabledTest>
    {
      ""equals"": [
        ""ranger-admin-site/ranger.authentication.method"",
        ""LDAP""
      ]
    }
 <ldapEnabledTest>
</ldap>{noformat}",pull-request-available,['ambari-sever'],AMBARI,Task,Blocker,2018-11-15 15:29:21,6
13198604,"Service display name on left navigation bar should be suffixed with ""Client"" if only client service component is present for a service","When a service only contains client service component then check service's displayname. If service's displayname does not and with ""Client"" then suffix service's displayname  with ""Client""

Current behavior:   !Dsplayname HDFS.png! 
Expected behavior:   !Displayname HDFS Client.png! ",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-15 11:59:33,4
13198530,"Handle complex ""Add Service"" request in ServiceResourceProvider","Related to AMBARI-24881, change {{ServiceResourceProvider}} to handle the complex ""Add Service"" request.  So far only stub logic is needed, details of the service creation will be filled later.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-15 06:33:02,5
13198258,"Sensitive service configuration values should be decrypted when processing the Ambari agent command script, if enabled","Sensitive service configuration values should be decrypted when processing the Ambari agent command script, if enabled.

During the processing of resource_management.libraries.script.script.Script#execute, the command data file is to be read in and the encrypted values in the JSON document are to be decrypted before executing the command.

Each encrypted value will be in the form of

${enc=<algorithm_encoding>, value=<value>}
For example:

${enc=aes265_hex, value=5248...303d}",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2018-11-14 08:34:29,3
13198055,Restart option should not be shown if service components are not created in a cluster,"In the attached screenshot ""Restart Nodemanager"" should not be shown because NodeManager service component is not present in the cluster",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-13 13:12:02,4
13197837,The kadmin service principal name should be configurable for MIT KDC interactions,"The kadmin service principal name should be configurable for MIT KDC interactions. The current process assumes the kadmin service principal is {{kadmin/FQDN_KADMIN_HOST}}, but this could be different on some installations. For example, {{kadmin/admin}}.

A new kerberos-env property should be added to allow a user to change the kadmin principal name - {{kerberos-env/kadmin_principal_name}}

The default value for the new property should be ""{{kadmin/$\{admin_server_host|stripPort()}}}"".  To be able to do this, we have to create a new variable replacement _function_. For example, {{stripPort}}.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-12 15:14:06,6
13197800,"Heatmap tab, metrics tab and QuickLinks section in summary tab should be hidden if service has only client service component","HDFS client and YARN client service component is created but other HDFS/YARN components are not created via blueprint deploy

*Expected behavior:*
Heatmap tab, metrics tab and QuickLinks section should be hidden if a service does not have any non-client servicecomponent created in the cluster

*Actual behavior:*
Heatmap and metrics tab are shown with empty metrics and no graphs",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-11-12 13:09:23,4
13197798,BE: Performance Tune service Configs Pages,"Need to be able to show configuration in less than 2 seconds. The heaviest part of configurations loading is recommendations request. Now for 2k cluster recommendations request is taking about 12 seconds:
{code:java}
/api/v1/stacks/HDP/versions/3.0/recommendations

{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-12 13:07:18,1
13197144,Request configurations when needed during server-side actions rather than rely on configuration data from the execution command,"Request configurations when needed during server-side actions rather than rely on configuration data from the execution command.

Due to a recent change, which appeared to remove configuration data from the execution command JSON document, data needed for Kerberos-related service-side actions is missing. This data may be requested when needed from the cluster data at the time of execution rather than when setting up the stages.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-08 14:45:16,7
13197088,JS error when changing service auto-start toggle,"# Go to Admin / Service Auto Start page
# Toggle Auto Start Settings
# Click Save button

JS error appears:
{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
    at app.js:15024
    at Array.forEach (<anonymous>)
    at Class.syncStatus (app.js:15022)
    at Array.<anonymous> (app.js:15092)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at Class.<anonymous> (vendor.js:1379)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-08 10:58:14,4
13196814,The stack/service advisor should be able to return LDAP-related recommendations upon request,"The stack/service advisor should be able to return LDAP-related recommendations upon request.

A new \{StackAdvisorRequestType}} is to be created - 'ldap-configurations' - along with the infrastructure to call request LDAP-specific configuration recommendations.

It is expected that stack owners will implement the optional {{recommendConfigurationsForLDAP}} function to provide the service-specific data.

This stack advisor request should be invoked upon changing the LDAP configuration values and Ambari should automatically apply the recommended changes - potentially causing service configurations to be updated and the need to restart affected services. For example, see {{org.apache.ambari.server.controller.internal.AmbariServerSSOConfigurationHandler#updateComponentCategory}}.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-11-07 12:39:50,6
13196744,Build error at Findbugs with Maven 3.6,"{noformat:title=mvn -am -pl ambari-server clean verify}
...
[INFO] Ambari Server 3.0.0.0-SNAPSHOT ..................... FAILURE [01:16 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
...
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:findbugs-maven-plugin:3.0.3:findbugs (findbugs) on project ambari-server: Unable to parse configuration of mojo org.codehaus.mojo:findbugs-maven-plugin:3.0.3:findbugs for parameter pluginArtifacts: Cannot assign configuration entry 'pluginArtifacts' with value '${plugin.artifacts}' of type java.util.Collections.UnmodifiableRandomAccessList to property of type java.util.ArrayList -> [Help 1]
{noformat}

{noformat:title=mvn -version}
Apache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T20:41:47+02:00)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-07 06:05:52,5
13196630,Delete host confirm popup does not contain all master components,"# Navigate to host with one or more masters
# Click Delete Host from the Host Actions menu

Observed: All the masters are not mentioned in the popup. This is a frontend bug
 !Screen Shot 2018-11-05 at 2.47.04 PM.png! 
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-06 16:46:40,4
13196434,Provide wrapper class for LDAP-related data for use in service advisors,Provide wrapper class for LDAP-related data for use in service advisors like the wrapper class for SSO-related data - {{stacks.ambari_configuration.AmbariSSODetails}}.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-11-06 08:52:53,6
13195645,Cluster user can't modify shared widgets,"If a user with role ""Cluster User"" tries to ""Edit Shared"" widgets, the server will return an error:
{noformat}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: Only cluster operator can create widgets with cluster scope""
}
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-11-01 11:55:44,4
13195477,NPE in default host group replacement,"Cluster deployment fails if some property value is null:

{noformat}
2018-10-31 17:38:44,579  INFO [pool-3-thread-1] AsyncCallableService:100 - Task ConfigureClusterTask exception during execution
java.lang.NullPointerException
	at java.util.regex.Matcher.getTextLength(Matcher.java:1283)
	at java.util.regex.Matcher.reset(Matcher.java:309)
	at java.util.regex.Matcher.<init>(Matcher.java:229)
	at java.util.regex.Pattern.matcher(Pattern.java:1093)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor$HostGroupUpdater.updateForClusterCreate(BlueprintConfigurationProcessor.java:1754)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.updateValue(BlueprintConfigurationProcessor.java:739)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.lambda$null$7(BlueprintConfigurationProcessor.java:705)
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1044)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.lambda$applyDefaultUpdater$8(BlueprintConfigurationProcessor.java:700)
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1044)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.applyDefaultUpdater(BlueprintConfigurationProcessor.java:697)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doGeneralPropertyUpdatesForClusterCreate(BlueprintConfigurationProcessor.java:664)
	at org.apache.ambari.server.controller.internal.BlueprintConfigurationProcessor.doUpdateForClusterCreate(BlueprintConfigurationProcessor.java:443)
	at org.apache.ambari.server.topology.ClusterConfigurationRequest.process(ClusterConfigurationRequest.java:152)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-31 20:20:20,5
13195108,Ambari-agent stop hangs if ambari-server is stopped.,"
    [root@gc7001 ~]# ambari-agent  stop
    Verifying Python version compatibility...
    Using python  /usr/bin/python
    Found ambari-agent PID: 7391
    Stopping ambari-agent
    ^C^C^X^Z
    [2]+  Stopped                 ambari-agent stop
    [root@gc7001 ~]# ^C
    [root@gc7001 ~]# kill -9 7391
    

",pull-request-available,[],AMBARI,Bug,Major,2018-10-30 12:43:08,0
13195094,Sometimes host status still in heartbeat lost after agent become heartbeating,Sometimes initial component states update does not fire host status recheck and host still in heartbeat lost.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-30 11:16:08,1
13194957,Improve copy of atlas.war,"Atlas Metadata Server's start script in Ambari includes a step to copy {{atlas.war}} from the default location to the {{expanded_webapp_dir}}.  This is performed using {{File(content=StaticFile)}}, which reads and writes the contents of the war file to/from memory.  It may take 1-3 seconds to perform, and is done even if the source and target files are the same, because there is no check on the paths.

This could be improved:

# Use {{cp}} to copy
# Skip the operation if source and target are the same",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-10-29 20:12:49,5
13194905,Make Ambaripreupload.py more configurable,"When testing {{Ambaripreupload.py}} locally, one needs to work around some hardcoded values (JDBC driver, DFS type). The goal of this change is to make the script more configurable.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-10-29 16:51:29,5
13194812,BE: Performance Tune Hosts Pages,"Hosts List - The initial host page and filter operations should all take less than 2 seconds. Today on the 5k node cluster all operations are taking 4 seconds.
Request URL for filtering:
{noformat}
/api/v1/clusters/c1/hosts?fields=Hosts/rack_info,Hosts/host_name,Hosts/maintenance_state,Hosts/public_host_name,Hosts/cpu_count,Hosts/ph_cpu_count,alerts_summary,Hosts/host_status,Hosts/host_state,Hosts/last_heartbeat_time,Hosts/ip,host_components/HostRoles/state,host_components/HostRoles/maintenance_state,host_components/HostRoles/stale_configs,host_components/HostRoles/service_name,host_components/HostRoles/display_name,host_components/HostRoles/desired_admin_state,host_components/metrics/dfs/namenode/ClusterId,host_components/metrics/dfs/FSNamesystem/HAState,Hosts/total_mem,stack_versions/HostStackVersions,stack_versions/repository_versions/RepositoryVersions/repository_version,stack_versions/repository_versions/RepositoryVersions/id,stack_versions/repository_versions/RepositoryVersions/display_name&minimal_response=true,host_components/logging&page_size=10&from=0&sortBy=Hosts/host_name.asc
{noformat}
with various parameters:
{noformat}
{""RequestInfo"":{""query"":""page_size:100
from:0
Hosts/host_status:HEALTHY""}}{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-29 11:48:23,1
13194769,Ambari is trying to create hbase.rootdir using s3 url,"Please look at the related Jira for blueprint.

For shared services cluster, Hbase needs to use S3 Object store for rootdir.
Ambari is trying to create hbase.rootdir using s3 url, hence failing with
below error.

    
    
    2018-10-24 03:12:45,424 - get_user_call_output returned (0, u'<html>\n<head>\n<meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>\n<title>Error 404 Not Found</title>\n</head>\n<body><h2>HTTP ERROR 404</h2>\n<p>Problem accessing /webhdfs/v1s3a:/san-s3-ohio/hbase. Reason:\n<pre>    Not Found</pre></p>\n</body>\n</html>\n404', u'')
    out: <html>
    <head>
    <meta http-equiv=""Content-Type"" content=""text/html;charset=utf-8""/>
    <title>Error 404 Not Found</title>
    </head>
    <body><h2>HTTP ERROR 404</h2>
    <p>Problem accessing /webhdfs/v1s3a:/san-s3-ohio/hbase. Reason:
    <pre>    Not Found</pre></p>
    </body>
    </html>
    404​
    

",pull-request-available,[],AMBARI,Bug,Major,2018-10-29 08:32:35,0
13194526,Make Grafana connection attempts and retry delay configurable,"Metrics Grafana HTTP requests are attempted at most 15 times in case of failure, with 20 seconds delay each time.  The goal of this change is to make both the number of attempts and length of the delay configurable.  Shorter retry delay could be useful to reduce overall startup time.",pull-request-available,['ambari-metrics'],AMBARI,Improvement,Minor,2018-10-26 22:06:21,5
13194475,Fix javadoc errors in ambari-utility,"Fix the following javadoc errors in {{ambari-utility}}:

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:jar (attach-javadocs) on project ambari-utility: MavenReportException: Error while generating Javadoc:
[ERROR] Exit code: 1 - ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:28: error: reference not found
[ERROR]  * {@link org.apache.ambari.swagger.NestedApiRecord} when {@link org.apache.ambari.swagger.AmbariSwaggerReader}
[ERROR]           ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:42: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:48: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:54: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerOverwriteNestedAPI.java:60: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
[ERROR] ambari-utility/src/main/java/org/apache/ambari/annotations/SwaggerPreferredParent.java:37: warning: no description for @return
[ERROR]      * @return
[ERROR]        ^
{noformat}",pull-request-available,[],AMBARI,Bug,Minor,2018-10-26 17:47:20,5
13194220,LDAP users fail to authenticate using LDAPS due to 'No subject alternative DNS name' exception,"LDAP users fail to authenticate using LDAPS due to `No subject alternative DNS name` exception:

{noformat}
2018-10-26 14:49:45,716  WARN [ambari-client-thread-37] AmbariLdapAuthenticationProvider:126 - Failed to communicate with the LDAP server: simple bind failed: ad.example.com:636; nested exception is javax.naming.CommunicationException: simple bind failed: ad.example.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching ad.example.com found.]
{noformat}

This is the other half of the issue from AMBARI-24533 (which was related to the LDAP sync process).  

Note:  If LDAP sync is performed before a user attempts to log in, then the issue will not be seen since the system property, {{com.sun.jndi.ldap.object.disableEndpointIdentification}}, would have already been set to ""true"".   However, the logic path setting this value is not reached for an authentication attempt. 

Note: This occurs with OpenJDK 1.8.0.191 and maybe some earlier versions.
{noformat}
openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
{noformat}

This does not occur with Oracle JDK 1.8.0.112
{noformat}
java version ""1.8.0_112""
Java(TM) SE Runtime Environment (build 1.8.0_112-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-25 18:51:28,7
13194086,Set path encoding for GCS,Add {{fs.gs.path.encoding}} to {{core-site}} with default value of {{uri-path}}.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-25 08:19:52,5
13193860,TrimmingStrategy implementations should be singletons,"{{BlueprintConfigurationProcessor}} trims properties during deployment using various implementations of {{TrimmingStrategy}}.  A new strategy object is created for each property.  This is unnecessary, since all current implementations are stateless.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-10-24 12:42:21,5
13193773,Set cloud storage tracking property for GCS,"Similarly to AMBARI-23329, set {{fs.gs.application.name.suffix}} for GCS.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-24 07:20:38,5
13193502,Add column to show which service recommended changes during upgrade refer to,"During the stack upgrade from HDP 2.6 -> HDP 3.x, the list of changes Ambari wants to make to each service is very difficult to interpret. Below is an example of the config changes Ambari recommends when attempting to upgrade fresh-install HDP2.6.2 cluster. 

Here's an example of how these configs look like when the user presses the ""Open"" button:
open_recommended_config_changes.png 
Note the highlighting is not intentional - it is automatically done by the browser (chrome) due to the special characters in the configs.

A small step to improve the user experience would be to add a column that shows which service each config applies.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-10-23 09:22:50,4
13192830,Swagger spec for Ambari REST API generation is failing,"Running the following command failed with the following error:

 
{noformat}
mvn -Del.log=WARN -Dcheckstyle.skip -Dfindbugs.skip -Drat.skip -DskipTests -DfailIfNoTests=false -am -pl ambari-server clean install
...
[ERROR] Failed to execute goal com.github.kongchen:swagger-maven-plugin:3.1.4:generate (default) on project ambari-server: Conflicting setter definitions for property ""requestBody"": org.apache.ambari.server.orm.entities.RequestScheduleBatchRequestEntity#setRequestBody(1 params) vs org.apache.ambari.server.orm.entities.RequestScheduleBatchRequestEntity#setRequestBody(1 params) -> [Help 1]
{noformat}
 

According to this thread t[his is a known issue|https://github.com/FasterXML/jackson-databind/issues/1251] in swagger

Upgrading the swagger plugin to 3.1.5 solves the issue.",pull-request-available,[],AMBARI,Bug,Blocker,2018-10-19 12:30:23,6
13192295,Intermittent CredentialStoreTest test failure,"Stacktrace
{code}
java.lang.NullPointerException
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.getExpiredCredentialTest(CredentialStoreTest.java:169)
	at org.apache.ambari.server.security.encryption.CredentialStoreTest.testInMemoryCredentialStoreService_CredentialExpired(CredentialStoreTest.java:90)
{code}
Standard Output
{code}
2018-10-12 14:06:26,230 INFO  [main] encryption.MasterKeyServiceImpl (MasterKeyServiceImpl.java:initializeFromFile(277)) - Loading from persistent master: #1.0# Fri, Oct 12 2018 14:06:26.207
2018-10-12 14:06:28,371 INFO  [main] encryption.MasterKeyServiceImpl (MasterKeyServiceImpl.java:initializeFromFile(277)) - Loading from persistent master: #1.0# Fri, Oct 12 2018 14:06:28.370{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-17 16:20:25,1
13192257,Allow skipping Python unit tests,{{ambari-server}} Python tests can be skipped by passing {{-DskipPythonTests}} to Maven.  The goal of this task is to allow the same for {{ambari-agent}}.,pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-10-17 14:17:01,5
13191917,Eliminating duplicated sudo calls when copying files in Ambari with non-root configuration,"During NiFi start Ambari invokes ambari-common/src/main/python/resource_management/core/sudo.copy(src, dest) (line 306) in case of non-root configuration. Here the command we pass to the command wrapper within shell.py starts with 'sudo'. However the sudo flag is also set to True.

As a result we have a double 'sudo' invocation which may cause issues in ambari with non-root configuration.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-16 13:17:36,6
13191894,Ambari-agent cannot register sometimes,"
    ERROR 2018-10-11 13:45:57,401 HeartbeatThread.py:108 - Exception in HeartbeatThread. Re-running the registration
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 95, in run
        self.register()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 163, in register
        self.force_component_status_update()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 173, in force_component_status_update
        self.component_status_executor.force_send_component_statuses()
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ComponentStatusExecutor.py"", line 206, in force_send_component_statuses
        service_name, component_name = service_and_component_name.split(""/"")
    ValueError: need more than 1 value to unpack
    

",pull-request-available,[],AMBARI,Bug,Major,2018-10-16 12:04:17,0
13191555,Fix CVE issues in ambari server,"The following 3rd party dependencies have to be eliminated/upgraded to a secure version based on the latest BlackDuck scan:
||Current version||Upgrade to||CVE issue(s)||
|org.springframework:spring-web:jar:4.3.17.RELEASE|org.springframework:spring-web:jar:4.3.18.RELEASE or the latest|CVE-2018-11039, CVE-2018-11040|
|jquery-1.8.3.min.js|1.9.0rc1 or the latest|CVE-2011-4969, CVE-2015-9251, CVE-2012-6708|
|org.eclipse.jetty:jetty-server:jar:9.4.11.v20180605|9.4.12.v20180830 or the latest|CVE-2017-9735, CVE-2018-12536|",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-15 11:06:31,6
13191524,Allow the agent's SSL certificate data to be accessible by heartbeat handlers,"If 2-way SSL is enabled, allow the agent's SSL certificate data to be accessible by heartbeat handers. The agent's SSL certificate contains the agent's public key, which may be used to encrypt data for that specific agent.

For example, the agent's SSL certificate data may be needed by org.apache.ambari.server.agent.rest.AgentResource#heartbeat and org.apache.ambari.server.agent.stomp.HeartbeatController#heartbeat, among other handlers used to communicate with the Ambari agent.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-10-15 08:56:09,3
13191468,Wrong settings in exported blueprint,"Cluster blueprint export is broken wrt. the {{settings}} section.

# Only one component per service is included.
# Default setting value assumed to be {{false}}, ignoring component-level stack definition

STR:

# Disable ""Auto Start"" for Metrics Collector, for which it is enabled by default per stack definition
# Enable ""Auto Start"" for both DataNode and NameNode
# Export blueprint

Result: 

{noformat:title=http://$AMBARI_SERVER:8080/api/v1/clusters/TEST?format=blueprint}
  ""settings"" : [
    {
      ""recovery_settings"" : [
        {
          ""recovery_enabled"" : ""true""
        }
      ]
    },
    {
      ""service_settings"" : [
        {
          ""recovery_enabled"" : ""true"",
          ""name"" : ""HDFS""
        }
      ]
    },
    {
      ""component_settings"" : [
        {
          ""recovery_enabled"" : ""true"",
          ""name"" : ""NAMENODE""
        }
      ]
    }
{noformat}

Problem: creating a cluster using the exported blueprint results in different settings than the original cluster.  ""Auto Start"" would be disabled for DataNode and enabled for Metrics Collector per stack defaults.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-14 19:10:47,5
13191169,Error while starting Timeline v2 Reader during Move operation,"# Deployed HDP-3.0.3 HA cluster with Ambari-2.7.3 (Active RM and Timeline v2 Reader are on the same host)
# Under Yarn --> Service Actions - choose to Move Timeline Service V2.0 Reader. Pick a new host and let the move operation complete

{code}
2018-10-11 13:04:33,887 ERROR reader.TimelineReaderServer (TimelineReaderServer.java:startTimelineReaderServer(236)) - Error starting TimelineReaderWebServer
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login from keytab
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.serviceInit(TimelineReaderServer.java:88)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.startTimelineReaderServer(TimelineReaderServer.java:233)
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.main(TimelineReaderServer.java:246)
Caused by: org.apache.hadoop.security.KerberosAuthException: failure to login: for principal: yarn/ctr-e138-1518143905142-517945-01-000006.hwx.site@EXAMPLE.COM from keytab /etc/security/keytabs/yarn.service.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1847)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytabAndReturnUGI(UserGroupInformation.java:1215)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1008)
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:313)
	at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer.serviceInit(TimelineReaderServer.java:85)
	... 3 more
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:897)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
	at org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext.login(UserGroupInformation.java:1926)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1837)
	... 7 more
2018-10-11 13:04:33,890 INFO  util.ExitUtil (ExitUtil.java:terminate(210)) - Exiting with status -1: Error starting TimelineReaderWebServer
2018-10-11 13:04:33,894 INFO  reader.TimelineReaderServer (LogAdapter.java:info(51)) - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down TimelineReaderServer at ctr-e138-1518143905142-517945-01-000002.hwx.site/172.27.74.4
************************************************************/
{code}",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-10-12 10:45:50,3
13190975,Ambari server continues to send request updates after all commands were completed.,After start/stop all request was completed server continues to send request updates. This causes UI shows invalid request/tasks progress statuses.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-11 16:18:27,1
13190622,Ambari-agent takes up too many cpu on perf,"   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    14129 1426.122    0.101 1426.122    0.101 {time.sleep}
        1    0.337    0.337 1426.769 1426.769 main.py:358(run_threads)
      331    0.219    0.001    0.219    0.001 {method 'acquire' of 'thread.lock' objects}
       11    0.181    0.016    0.181    0.016 {built-in method poll}
        1    0.108    0.108    0.108    0.108 {method 'do_handshake' of '_ssl._SSLSocket' objects}
    14151    0.042    0.000    0.042    0.000 threading.py:571(isSet)
      125    0.028    0.000    0.028    0.000 {method 'flush' of 'file' objects}
     5078    0.027    0.000    0.052    0.000 decoder.py:65(py_scanstring)
       15    0.020    0.001    0.020    0.001 {posix.read}
     5093    0.020    0.000    0.024    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}
        1    0.019    0.019    0.019    0.019 {method 'connect' of '_socket.socket' objects}
    15365    0.018    0.000    0.018    0.000 {method 'match' of '_sre.SRE_Pattern' objects}
55424/13131    0.016    0.000    0.057    0.000 encoder.py:332(_iterencode_dict)
    38241    0.014    0.000    0.014    0.000 {isinstance}
       21    0.013    0.001    0.022    0.001 collections.py:282(namedtuple)
    473/5    0.012    0.000    0.073    0.015 decoder.py:148(JSONObject)
     5078    0.009    0.000    0.034    0.000 encoder.py:43(py_encode_basestring_ascii)
        5    0.006    0.001    0.070    0.014 __init__.py:122(dump)
    13251    0.005    0.000    0.005    0.000 {method 'write' of 'file' objects}
        7    0.004    0.001    0.004    0.001 {ambari_commons.libs.x86_64._posixsubprocess.fork_exec}
     5638    0.004    0.000    0.004    0.000 encoder.py:49(replace)
   3167/5    0.003    0.000    0.073    0.015 scanner.py:27(_scan_once)
13353/9909    0.003    0.000    0.030    0.000 encoder.py:279(_iterencode_list)
       75    0.003    0.000    0.003    0.000 {method 'read' of '_ssl._SSLSocket' objects}
   3177/8    0.003    0.000    0.008    0.001 Utils.py:124(make_immutable)
    13131    0.003    0.000    0.060    0.000 encoder.py:409(_iterencode)
    11128    0.003    0.000    0.003    0.000 {method 'groups' of '_sre.SRE_Match' objects}
  2202/45    0.003    0.000    0.004    0.000 Utils.py:135(get_mutable_copy)
   474/10    0.002    0.000    0.008    0.001 Utils.py:170(__init__)
      238    0.002    0.000    0.002    0.000 {time.localtime}
      119    0.002    0.000    0.004    0.000 __init__.py:242(__init__)
     3759    0.002    0.000    0.002    0.000 {method 'isalnum' of 'str' objects}
    14752    0.002    0.000    0.002    0.000 {method 'end' of '_sre.SRE_Match' objects}
    16854    0.002    0.000    0.002    0.000 {method 'append' of 'list' objects}
     5098    0.002    0.000    0.002    0.000 {method 'join' of 'unicode' objects}
      238    0.002    0.000    0.007    0.000 __init__.py:451(format)
       13    0.002    0.000    0.004    0.000 metric_alert.py:286(__init__)
        7    0.001    0.000    0.026    0.004 subprocess32.py:1153(_execute_child)
       41    0.001    0.000    0.001    0.000 {open}
      616    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
      238    0.001    0.000    0.004    0.000 __init__.py:404(formatTime)
       18    0.001    0.000    0.001    0.000 {posix.listdir}
        5    0.001    0.000    0.072    0.014 ClusterCache.py:131(persist_cache)
     4032    0.001    0.000    0.003    0.000 collections.py:323(<genexpr>)
       18    0.001    0.000    0.001    0.000 {method 'sort' of 'list' objects}
       90    0.001    0.000    0.001    0.000 {built-in method now}
      238    0.001    0.000    0.001    0.000 {time.strftime}
      119    0.001    0.000    0.001    0.000 __init__.py:1215(findCaller)
     5706    0.001    0.000    0.001    0.000 {method 'group' of '_sre.SRE_Match' objects}
      281    0.001    0.000    0.001    0.000 threading.py:146(acquire)
      281    0.001    0.000    0.001    0.000 threading.py:186(release)
      119    0.001    0.000    0.001    0.000 {method 'seek' of 'file' objects}
        1    0.001    0.001    0.001    0.001 ClusterTopologyCache.py:58(on_cache_update)
      119    0.001    0.000    0.005    0.000 handlers.py:144(shouldRollover)
     50/5    0.001    0.000    0.035    0.007 decoder.py:223(JSONArray)
Major cpu cosumers:
1) Regexp operation: 
As we can see a lot of time is took for regexp operations. This happens because we use non-compiled regular expressions.
2) Json operations:
Another major cpu consumer is json module, because _speedups.so is not compiled for python2.7 currently. We have this situation. This is tackled by other issue
3) Main thread waking up/sleeping too often.
This seems to create quite a bit cpu usage.
The approach was implemented so agent can check for SIGTERM (ambari-agent stop). A proper solution should be a usage signal.pause() instead of sleep/wakeup.",pull-request-available,[],AMBARI,Bug,Major,2018-10-10 12:30:29,0
13189709,"Sensitive service configuration values should be encrypted in the Ambari server DB, if enabled","Sensitive service configuration values should be encrypted in the Ambari server DB, if enabled.

Sensitive service configuration values are defined by a service's configuration metadata. Properties are defined in XML files under the service's definition directory and contain attributes that Ambari may use to determine whether they should be encrypted or not.

Currently, Ambari uses the {{property-type}} attribute to determine the type of property. If the value of this attribute is ""PASSWORD"", than the value is considered sensitive and should be encrypted.
{code:java|title=Example: This password field is to be encrypted, implicitly}
  <property>
    <name>ssl.server.truststore.password</name>
    <value>bigdata</value>
    <property-type>PASSWORD</property-type>
    <description>Password to open the trust store file.</description>
    <value-attributes>
      <type>password</type>
    </value-attributes>
    <on-ambari-upgrade add=""false""/>
  </property>
{code}
Using this information, _if this feature is enabled_, the Ambari server should encrypt sensitive values before storing them in the database. Values should be encrypted within the container they are stored. For example, Ambari stores configurations as JSON documents. Before writing these JSON documents to the database, the Ambari server should process each name/value pair and encrypt only those that are deemed sensitive.

The Ambari server should encrypt sensitive configuration values if the following has been met:
 * A master key has been setup using the ""ambari-server setup-security"" CLI (using option #2 - Encrypt passwords stored in ambari.properties file)
 * The Ambari server configuration property named ""{{server.security.encrypt_sensitive_data}}"" is set to ""true""

If encrypting sensitive data:
 * the value should be encrypted using a secure symmetric key encryption algorithm. For example AES - [https://aesencryption.net/].
 * the encryption key should be the previously set master key, or some reproducible encoding of it.
 * the encrypted bytes should be converted to a hex string
 * the value should be stored in the relevant JSON document suck that that the value is declared as encrypted.
 ** for example:
{noformat}
""password"" : ""${enc=aes265_hex, value=5248...303d}""{noformat}

 ** this is needed in the event {{server.security.encrypt_sensitive_data}} is changed to false, but there are still encrypted values in the database.

Encrypted data needs to be decrypted before being used or returned via the REST API. The data may be re-encrypted depending on use. For example, when being sent to an Ambari agent.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-05 13:02:07,6
13189173,Ambari Server stops with Java 9 due to Guice error,"Ambari Server stopped a few seconds after start with the following error:

 
{code:title=/var/log/ambari-server/ambari-server.out}
Exception in thread ""main"" com.google.inject.CreationException: Unable to create injector, see the following errors:

1) No scope is bound to org.apache.ambari.server.AmbariService.
at org.apache.ambari.server.state.services.MetricsRetrievalService.class(MetricsRetrievalService.java:85)
while locating org.apache.ambari.server.state.services.MetricsRetrievalService
for field at org.apache.ambari.server.controller.jmx.JMXPropertyProvider.metricsRetrievalService(JMXPropertyProvider.java:88)
at org.apache.ambari.server.controller.metrics.MetricPropertyProviderFactory.createJMXPropertyProvider(MetricPropertyProviderFactory.java:1)
at com.google.inject.assistedinject.FactoryProvider2.initialize(FactoryProvider2.java:666)
at com.google.inject.assistedinject.FactoryModuleBuilder$1.configure(FactoryModuleBuilder.java:335) (via modules: org.apache.ambari.server.controller.ControllerModule -> com.google.inject.assistedinject.FactoryModuleBuilder$1)

1 error
at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)
at com.google.inject.internal.InternalInjectorCreator.injectDynamically(InternalInjectorCreator.java:176)
at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:110)
at com.google.inject.Guice.createInjector(Guice.java:99)
at com.google.inject.Guice.createInjector(Guice.java:73)
at com.google.inject.Guice.createInjector(Guice.java:62)
at org.apache.ambari.server.controller.AmbariServer.main(AmbariServer.java:1079)
~
~
""/var/log/ambari-server/ambari-server.out"" 41L, 6558C{code}

{code}
cat /etc/ambari-server/conf/ambari.properties | grep 'java\.home'
java.home=/usr/java/jdk-9.0.4/
stack.java.home=/usr/java/jdk-9.0.4/
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-10-03 14:02:22,5
13188795,Failed to force_non_member_install a stack version on hosts,"The ability to pre-install packages on hosts (before being added to the cluster) using the following API request is broken:

{noformat}
$ curl -X POST -d '\{ ""HostStackVersions"": { ""repository_version"": <repository_version>, ""stack"": ""HDP"", ""version"": ""2.6"", ""cluster_name"": ""TEST"", ""force_non_member_install"": true, ""components"": [ { ""name"" : ""ZOOKEEPER_SERVER"" }, \{ ""name"": ""ZOOKEEPER_CLIENT"" } ] } }' http://localhost:8080/api/v1/hosts/${hostname}/stack_versions
{noformat}

{noformat:title=ambari-agent.log}
ERROR 2018-06-19 08:54:08,222 CustomServiceOrchestrator.py:448 - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: 'Host_Level_Params for cluster_id=2 is missing. Check if server sent it.'; 'Host_Level_Params for cluster_id=2 is missing. Check if server sent it.'
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 322, in runCommand
    command = self.generate_command(command_header)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 487, in generate_command
    command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 38, in get_configuration
    host_level_params_cache = self.host_level_params_cache[cluster_id]
  File ""/usr/lib/ambari-agent/lib/ambari_agent/ClusterCache.py"", line 155, in __getitem__
    raise KeyError(""{0} for cluster_id={1} is missing. Check if server sent it."".format(self.get_cache_name().title(), key))
KeyError: 'Host_Level_Params for cluster_id=2 is missing. Check if server sent it.'
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-10-02 10:00:48,1
13188490,Ambari-agent does for save data hashes correctly,"This causes all the data to be re-send during registration. Which can be very
dramatic for perf clusters.

",pull-request-available,[],AMBARI,Bug,Major,2018-10-01 11:32:00,0
13188454,User aborted task reported as FAILED in Bgoperations,"User aborted task reported as FAILED in Bgoperations.

1. Install cluster with all services ( Including Druid)
2. Click on Stop all services
3. When BGoperations window is loaded, click on the abort button for the same task
4. The operation should be aborted and marked as Aborted with ""Aborted by user"" message in the sub tasks logs
5. But instead the operation is marked as failure.

Failed task is Druid Coordinator Stop
{code:java}
""href"": ""https://ctr-e138-1518143905142-466827-01-000002.hwx.site:8443/api/v1/clusters/cl1/requests/59/stages/0/tasks/756"",
""Tasks"": {
""attempt_cnt"": 1,
""cluster_name"": ""cl1"",
""command"": ""STOP"",
""command_detail"": ""DRUID_COORDINATOR STOP"",
""end_time"": 1536363316602,
""error_log"": ""/var/lib/ambari-agent/data/errors-756.txt"",
""exit_code"": 0,
""host_name"": ""ctr-e138-1518143905142-466827-01-000005.hwx.site"",
""id"": 756,
""ops_display_name"": null,
""output_log"": ""/var/lib/ambari-agent/data/output-756.txt"",
""request_id"": 59,
""role"": ""DRUID_COORDINATOR"",
""stage_id"": 0,
""start_time"": 1536363307548,
""status"": ""FAILED"",
""stderr"": ""\nCommand aborted. Reason: 'Aborted by user'"",
""stdout"": ""\nCommand aborted. Reason: 'Aborted by user'\n\nCommand failed after 1 tries\n"",
""structured_out"": {}
}{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-10-01 07:28:22,6
13188144,Implement support for Minimal Blueprint Export,"Implement a new mode for Blueprint exports, a ""minimal"" mode in which only modified configuration is included in the exported Blueprint. The original ""full"" Blueprint export should still be possible, in order to maintain backwards compatibility.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-09-28 12:34:35,5
13188138,Automation script for upgrade old style isilon cluster to the new mpack based structure,"A Python that can convert old style isilon onefs clusters to the new mpack based cluster. The script should replace all of the HDFS_CLIENTs to ONEFS_CLIENT and the HDFS_SERVICE to ONEFS_SERVICE. Config types like core-site, hdfs-site, hadoop-env should be preserved.",pull-request-available,['contrib'],AMBARI,Task,Major,2018-09-28 11:46:33,3
13188020,Build fails due to missing ambari-utility,"{noformat:title=mvn -Del.log=WARN -am -pl ambari-server -DskipTests clean test}
...
[ERROR] Failed to execute goal on project ambari-server: Could not resolve dependencies for project org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT: Could not find artifact org.apache.ambari:ambari-utility:jar:1.0.0.0-SNAPSHOT in oss.sonatype.org (https://oss.sonatype.org/content/groups/staging) -> [Help 1]
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-27 20:18:22,5
13187564,Ambari-server setup-ldap throws an error when the OU has spaces,"Ambari-server setup-ldap throws an error when the OU has spaces

{code}
ambari-server setup-ldap \
 --ldap-url=ldap.example.com:636 \
 --ldap-secondary-url=ldap.example.com:636 \
 --ldap-ssl=true \
 --ldap-user-class=person \
 --ldap-user-attr=sAMAccountName \
 --ldap-group-class=group \
 --ldap-group-attr=cn \
 --ldap-member-attr=member \
 --ldap-dn=distinguishName \
 --ldap-base-dn=""OU=Group Users,DC=example,DC=com"" \
 --ldap-referral=follow \
 --ldap-bind-anonym=false \
 --ldap-manager-dn=bind@example.com \
 --ldap-manager-password=**** \
 --ldap-sync-username-collisions-behavior=convert \
 --ldap-save-settings
{code}

error message:

{code}
Using python  /usr/bin/python
Usage: ambari-server.py action [options]

Options:
  -h, --help            show this help message and exit
...
  --truststore-reconfigure
                        Force to reconfigure TrustStore if exits
None
Usage: ambari-server.py action [options]

ambari-server.py: error: Invalid number of arguments. Entered: 2, required: 1
{code}",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-09-26 10:13:31,3
13187182,Cannot deploy cluster without HDFS_CLIENT,"The attached blueprint cannot be deployed, because {{hadoop-env.sh}} is not saved by Ambari, because it requires {{HDFS_CLIENT}} component on at least one host:

https://github.com/apache/ambari/blob/8d145e0c04917866fd76690688826cf44065370e/ambari-server/src/main/resources/stack-hooks/before-ANY/scripts/hook.py#L32-L33

Datanode start fails with:

{noformat}
ExecutionFailed: Execution of 'ambari-sudo.sh su hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/3.0.0.0-1634/hadoop/bin/hdfs --config /usr/hdp/3.0.0.0-1634/hadoop/conf --daemon start datanode'' returned 1. ERROR: JAVA_HOME is not set and could not be found.
{noformat}

This appears in output from {{after-INSTALL}} hook:

{noformat}
Parameter hadoop_conf_dir is missing or directory does not exist. This is expected if this host does not have any Hadoop components.
{noformat}

Additionally, host groups without {{HDFS_CLIENT}} cannot be scaled up.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-25 04:47:07,5
13186644,Upgrade checkstyle version to 8.9,"Currently we use checkstyle 6.19 which is very old.

We should upgrade to 8.9 release.",pull-request-available,[],AMBARI,Task,Major,2018-09-21 15:53:05,5
13186607,The operation 'Install HDP-{$version} version' change name after ending and page reloading,"STR:
1) Deploy ambari 2.7.1 cluster
2) Make registration of new version HDP 

Actual result: The operation 'Install HDP-{$version} version' change name (from 'Install HDP-{$version} version' on 'Install Version') after ending of operation and page reloading. 
Note: take a look on screenshots

Expected result: The operation for installing of new HDP version must have the constant name.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-21 12:22:21,4
13186596, Regenerating keytabs for HDFS only does not re-create headless keytabs on all hosts where needed,"+*STR*+
 # install a cluster with HDFS and Tez (+other dependecies)
 # Kerberize the cluster
 # remove the HDFS client from the host where Tez client is installed
 # regenerate keytabs for HDFS

+*Actual results*+

  the headless keytab for HDFS has not been regenerated

+*Expected results*+

  the headless keytab is regenerated where it's necessary",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-09-21 11:51:37,6
13186544,Workaround for non-atomic directory creation,"{{before-*}} hooks create a few directories.  If parallel agent execution is enabled tasks may need to be retried, because directory creation is not atomic (see AMBARI-24670).  This causes delays during cluster deployment.  While the underlying problem is being fixed, the goal of this task is to provide a workaround by creating the directory during sysprep phase.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-09-21 05:29:29,5
13186542,Directory creation should be atomic,"If parallel execution is enabled on Ambari Agent, concurrent directory creation may fail with:

{noformat:title=errors-62.txt}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 31, in hook
    setup_hadoop_env()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py"", line 203, in setup_hadoop_env
    mode=01777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 191, in action_create
    sudo.makedir(path, self.resource.mode or 0755)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 121, in makedir
    os.mkdir(path)
OSError: [Errno 17] File exists: '/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'
{noformat}

or

{noformat:title=errors-63.txt}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 375, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/hook.py"", line 31, in hook
    setup_hadoop_env()
  File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-ANY/scripts/shared_initialization.py"", line 203, in setup_hadoop_env
    mode=01777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 179, in action_create
    path = sudo.readlink(path)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 161, in readlink
    return os.readlink(path)
OSError: [Errno 22] Invalid argument: '/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'
{noformat}

The failed tasks need to be retried to succeed, causing delays.",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-09-21 05:21:16,0
13186362,Component status can stuck in starting/stopping status on heartbeat lost,"Now server sets components statuses to UNKNOWN and saves last actual state on heartbeat lost. After agent re-registering server restores saved state, but in case ""in progress"" state request is already aborted and agent status updates can not be applied. Agent should always send component statuses after registering, not only on status changes. Also last actual state logic should be removed. ",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-09-20 14:10:54,1
13186332,JS errors during adding hosts,"Steps:
# Deploy any cluster.
# Do not refresh the page.
# Go through add hosts wizard to deploy step.

Result: JS errors in console after deploy button click. Also some persist requests with errors were sent to the server. ""App.get('clusterId')"" returns null. For perf cluster deploy the number of errors increases to several hundred.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-20 12:48:01,4
13186309,Implement data visualization color palette,"Colors in data visualization should be used in the following order of priority:
# #41bfae
# #79e3d1
# #63c2e5
# #c4aeff
# #b991d9
# #ffb9bf
# #ffae65
# #f6d151
# #a7cf82
# #abdfd5
# #3aac9c
# #6dccbc
# #59aece
# #b09ce5
# #a682c3
# #e5a6ac
# #e59c5b
# #ddbc49
# #96ba75
# #9ac8bf
# #83d5ca
# #a8ede1
# #99d7ee
# #d9caff
# #d1b7e6
# #ffd1d5
# #ffca9b
# #f9e18e
# #c6e0ae
# #c8eae4",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-09-20 11:29:31,2
13185736,Tasks fail on ambari-agent intermittently under cpu load due to race condition in ambari-agent,"
    INFO 2018-09-13 18:30:15,817 ClusterCache.py:125 - Rewriting cache ClusterTopologyCache for cluster 2
    ERROR 2018-09-13 18:30:16,386 CustomServiceOrchestrator.py:456 - Caught an exception while executing custom service command: <type 'exceptions.TypeError'>: 'NoneType' object has no attribute '__getitem__'; 'NoneType' object has no attribute '__getitem__'
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 324, in runCommand
        command = self.generate_command(command_header)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 504, in generate_command
        command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 46, in get_configuration
        'agentLevelParams': {'hostname': self.topology_cache.get_current_host_info(cluster_id)['hostName']},
    TypeError: 'NoneType' object has no attribute '__getitem__'
    

This was found while perf testing.  
However can occur under any circumstances if ambari-agent node if slow enough.

",pull-request-available,[],AMBARI,Bug,Major,2018-09-18 08:01:07,0
13185599,Add cluster and stack settings properties to agent STOMP updates,Server should post cluster settings and stack settings to agent with STOMP updates.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-17 18:13:55,1
13185183,Issues with tooltip containing custom time range for charts,"- The tooltip content is not formatted
- Tooltip isn't displayed on some pages",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-14 16:10:18,2
13185115,"Stackadvisor error while trying to add atlas service. Error - ""if mountPoints[mountPoint] < reqiuredDiskSpace: KeyError: None""","Stack advisor error: 

{code}
Error details: None
2018-09-11 06:40:09,738  INFO [ambari-client-thread-24150] StackAdvisorRunner:167 -     Advisor script stderr: Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 184, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 138, in main
    result = stackAdvisor.validateConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1079, in validateConfigurations
 validationItems = self.getConfigurationsValidationItems(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1468, in getConfigurationsValidationItems
    items.extend(self.getConfigurationsValidationItemsForService(configurations, recommendedDefaults, service, services, hosts))
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1521, in getConfigurationsValidationItemsForService
    items.extend(serviceAdvisor.getServiceConfigurationsValidationItems(configurations, recommendedDefaults, services, hosts))
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/AMBARI_METRICS/service_advisor.py"", line 218, in getServiceConfigurationsValidationItems
    return validator.validateListOfConfigUsingMethod(configurations, recommendedDefaults, services, hosts, validator.validators)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1491, in validateListOfConfigUsingMethod
    validationItems = method(siteProperties, siteRecommendations, configurations, services, hosts)
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/AMBARI_METRICS/service_advisor.py"", line 556, in validateAmsHbaseSiteConfigurationsFromHDP206
    validationItems.extend([{""config-name"": 'hbase.rootdir', ""item"": self.validatorEnoughDiskSpace(properties, 'hbase.rootdir', host[""Hosts""], recommendedDiskSpace)}])
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 2998, in validatorEnoughDiskSpace
    if mountPoints[mountPoint] < reqiuredDiskSpace:
KeyError: None
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-09-14 11:23:34,3
13185090,Not able to turn off maintenance mode for a host.,"In the cluster , while trying to turn off maintenance mode on host , nothing happens and below error is seen in the console.

Initial analysis indicate that this could be because some host components are out of sync. But this may or may not be RC. Kindly check.

{code:java}
Uncaught TypeError: Cannot read property 'get' of undefined
    at Class.onOffPassiveModeForHost (app.js:26825)
    at Class.doAction (app.js:26800)
    at handler (vendor.js:31554)
    at HTMLAnchorElement.<anonymous> (vendor.js:23346)
    at HTMLDivElement.dispatch (vendor.js:3178)
    at HTMLDivElement.elemData.handle (vendor.js:2854)
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-14 08:52:55,4
13185077,Ambari-agent process memory leak,"There was one process which started using memory rapidly at certain point and grew up to ~27GB of RSS used until eventually we restarted it. Which happened after a month of running of 10 ambari-agent nodes.

    [root@andrew2-1n01 ~]# ps aux | grep ambari_agent
    root     39955  0.0  0.0  47580  6024 ?        S    Aug17   0:00 /usr/bin/python /usr/lib/ambari-agent/lib/ambari_agent/AmbariAgent.py start
    root     39959 20.4 10.2 31623096 27154348 ?   Sl   Aug17 7645:55 /usr/bin/python /usr/lib/ambari-agent/lib/ambari_agent/main.py start

Just before the growth in memory usage is seen. This exception pops out:

ERROR 2018-09-11 10:56:59,716 websocket.py:552 - Websocket connection was closed with an exception
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 549, in run
    if not self.once():
  File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 428, in once
    if not self.process(self.buf[:requested]):
  File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 483, in process
    self.reading_buffer_size = s.parser.send(bytes) or DEFAULT_READING_SIZE
ValueError: generator already executing

This exception is not seen on all other nodes or on this one at any other period (during 1 month). So I suggest it can be the root cause.
Basically this error means that generator is being used by multiple threads. So I will upload the fix to thread-lock this place.

This is just a guess solution which might work and might not. No way to test really. But definitely we should try this.
    
This is noticed in ambari-2.7.1.0-73 version as well.  

",pull-request-available,[],AMBARI,Bug,Major,2018-09-14 08:07:08,0
13184596,"""Host is in Maintenance mode"" text is not displayed in UI after maintenance mode is turned on","Behavior in ambari-2.7.0
1. login
2. navigate host details page.
3. click host action
4. click ""Turn On Maintenance Mode""
after the first confirm window. we can see the text which displays ""Host is in Maintenance mode""
 
But in ambari-2.7.1
The UI Tab which display ""Turn On Maintenance Mode"" 
It is not getting displayed.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-12 13:08:53,4
13184542,hdfsResource fails to using nameservices for filesystems which does not support that,"Some filesystems does not support nameservices:

    
    
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hadoop --config /usr/hdp/3.0.1.0-179/hadoop/conf jar /var/lib/ambari-agent/lib/fast-hdfs-resource.jar /var/lib/ambari-agent/tmp/hdfs_resources_1536226305.38.json' returned 1. Exception occurred, Reason: abfs://mycluster has invalid authority.
    abfs://mycluster has invalid authority.
    	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:732)
    	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:129)
    	at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:97)
    	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3354)
    	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
    	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3403)
    	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3371)
    	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:477)
    	at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:83)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
    	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-09-12 08:22:44,0
13184530,Allow skipping package operations for LZO on sysprepped hosts,"LZO packages may be pre-installed in sysprepped environments, but Ambari still manages the repo and checks for existence of the packages, which takes time.  The goal of this change is to allow users who pre-install packages to skip package manager operations for LZO packages, too.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-09-12 05:45:37,5
13184361,Duplicate view of configurations in Add Service wizard,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-09-11 14:08:00,4
13184340,Horizontal scroll bar on assign slaves and clients page is not convenient for deploy with numerous hosts,It is not convenient to configure slaves and clients for deploy with numerous hosts because horizontal scroll bar is placed in the bottom of hosts list.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-09-11 12:55:00,2
13184100,Disable Kerberos from Ambari UI didn't clean up keytab directories,"Disable Kerberos from Ambari UI didn't clean up keytab directories,

stderr:

{code:java}
2018-09-08 05:27:19,276 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,298 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,325 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,348 - Failed to remove identity for amsmon/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,465 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,491 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,515 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,539 - Failed to remove identity for dn/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,671 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,696 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,723 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,744 - Failed to remove identity for hbase/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,959 - Failed to remove identity for nm/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:19,987 - Failed to remove identity for nm/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,049 - Failed to remove identity for nn/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,376 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,399 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,420 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,441 - Failed to remove identity for HTTP/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,590 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000003.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,617 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000002.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,647 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,677 - Failed to remove identity for yarn-ats-hbase/ctr-e138-1518143905142-467151-01-000005.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,768 - Failed to remove identity for zookeeper/ctr-e138-1518143905142-467151-01-000006.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type.
2018-09-08 05:27:20,798 - Failed to remove identity for zookeeper/ctr-e138-1518143905142-467151-01-000004.hwx.site@EXAMPLE.COM from the Ambari database - Object: null is not a known Entity type
{code}


{code:java}
[root@ctr-e138-1518143905142-467151-01-000002 ~]# ls -lrt /etc/security/keytabs/
total 56
-rw-r----- 1 ambari-qa  hadoop 318 Sep  7 23:00 kerberos.service_check.090718.keytab
-r-------- 1 slava      slava  353 Sep  7 23:05 ambari.server.keytab
-r--r----- 1 root       hadoop 538 Sep  7 23:05 spnego.service.keytab
-r-------- 1 cstm-ams   hadoop 548 Sep  7 23:05 ams-monitor.keytab
-r-------- 1 cstm-hdfs  hadoop 533 Sep  7 23:05 nfs.service.keytab
-r--r----- 1 cstm-hbase hadoop 338 Sep  7 23:05 hbase.headless.keytab
-r-------- 1 yarn-ats   hadoop 328 Sep  7 23:05 yarn-ats.hbase-client.headless.keytab
-r-------- 1 cstm-hdfs  hadoop 528 Sep  7 23:05 dn.service.keytab
-r-------- 1 yarn-ats   hadoop 588 Sep  7 23:05 yarn-ats.hbase-regionserver.service.keytab
-r--r----- 1 ambari-qa  hadoop 333 Sep  7 23:05 smokeuser.headless.keytab
-r-------- 1 cstm-hbase hadoop 543 Sep  7 23:05 hbase.service.keytab
-r-------- 1 cstm-hdfs  hadoop 528 Sep  7 23:05 nn.service.keytab
-r-------- 1 yarn-ats   hadoop 588 Sep  7 23:05 yarn-ats.hbase-master.service.keytab
-r-------- 1 cstm-hdfs  hadoop 333 Sep  7 23:05 hdfs.headless.keytab
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-10 17:08:19,7
13184008,Host Level Maintenance mode is not working through Ambari UI,"Steps to reproduce:-
1. Login
2.Open host page.
3. select on host.
4. click on ""Host-Action"" Tab and select ""Turn On Maintenance mode ""

The message is showing 
{code}
Are you sure you want to Turn Off Maintenance Mode for ctr-e138-1518143905142-471677-01-000006.hwx.site?
{code}

I am trying to Turn On Maintenance Mode.

after that 
 !Screen Shot 2018-09-10 at 2.31.42 PM.png|thumbnail! 
{code}
Maintenance Mode has been turned off. It may take a few minutes for the alerts to be enabled.
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-09-10 11:05:34,4
13184001,Disabling an Alert Does Not Clear It in the Web UI,"When you disable an alert, the web client shows that the instance of the alert was removed immediately, however, the red badges indicating a critical alert do not disappear.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-10 10:28:23,1
13183651,ambari-server exception if any user with cluster operator role is trying to edit the widget.,"*STR*
1. login using widgetuser(username),widgetpass(password) the role is cluster operator
2. go to hdfs-> metrics tab
3. EDIT shared the ""Under Replicated Blocks"" widgets. 
4. the error can be reproduced

*Actual Result:*

!errorWhileEditingWidgetsAsClusterOperator.png!

*Expected Result:*

According to our Role Chart a user with Cluster Operator role should be able to create/edit widgets

!RoleChart.png!

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-09-07 14:39:28,6
13183275,Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'onefs' ,"Below error message pop-up when make and save any configuration changes to the service running on HDP301 cluster.

 

Error

500 status code received on POST method for API: /api/v1/stacks/HDP/versions/3.0/validations 
 

*Error message:* Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'onefs'

StdOut file: /var/run/ambari-server/stack-recommendations/20/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/20/stackadvisor.err

 

!image-2018-09-06-09-12-32-080.png!

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-06 01:23:16,3
13183089,Download client config fails if user running Ambari server has UID>2097151	,"When ambari is running with a user that has UID/GID >2097151 download client configs fails with the following error:

{code}
{ 
""status"" : 500, 
""message"" : ""org.apache.ambari.server.controller.spi.SystemException: group id '19600006' is too big ( > 2097151 )"" 
} 
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-09-05 10:34:01,3
13182692,HDI Livy2 fails to restart,"Livy2 restart fails from Ambari due to Ambari could not fetch some Hadoop
configs?

StdErr:
{code}    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 148, in <module>
        LivyServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 351, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 62, in start
        self.configure(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/livy2_server.py"", line 52, in configure
        setup_livy(env, 'server', upgrade_type=upgrade_type, action = 'config')
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/SPARK2/package/scripts/setup_livy2.py"", line 53, in setup_livy
        params.HdfsResource(None, action=""execute"")
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 681, in action_execute
        self.get_hdfs_resource_executor().action_execute(self)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 164, in action_execute
        logoutput=logoutput,
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
        returns=self.resource.returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hadoop --config /usr/hdp/3.0.1.0-175/hadoop/conf jar /var/lib/ambari-agent/lib/fast-hdfs-resource.jar /var/lib/ambari-agent/tmp/hdfs_resources_1535895647.58.json' returned 1. Initializing filesystem uri: hdfs://mycluster
    Creating: Resource [source=null, target=wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, type=directory, action=create, owner=livy, group=null, mode=700, recursiveChown=false, recursiveChmod=false, changePermissionforParents=false, manageIfExists=true] in hdfs://mycluster
    Exception occurred, Reason: Wrong FS: wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, expected: hdfs://mycluster
    java.lang.IllegalArgumentException: Wrong FS: wasb://spark2l-at30wu-livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-69e01f25-3b7b-4af4-a787-37664ab45f0c, expected: hdfs://mycluster
    	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:781)
    	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:240)
    	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1583)
    	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1580)
    	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1595)
    	at org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1768)
    	at org.apache.ambari.fast_hdfs_resource.Resource.checkResourceParameters(Resource.java:193)
    	at org.apache.ambari.fast_hdfs_resource.Runner.main(Runner.java:112)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at org.apache.hadoop.util.RunJar.run(RunJar.java:318)
    	at org.apache.hadoop.util.RunJar.main(RunJar.java:232)
    
{code}

Please note that the system test sets the

property | from | to  
---|---|---  
`livy.server.recovery.state-store` | `zookeeper` | `filesystem`  
`livy.server.recovery.state-store.url` |
`zk1-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181,zk2-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181,zk5-b24996.zu2zfpuge4su5ceknrmkpsq3ra.ix.internal.cloudapp.net:2181`
| `wasb://spark2l-at30wu-
livy2-121012@humbtesting5wua2.blob.core.windows.net/user/livy/recovery-
69e01f25-3b7b-4af4-a787-37664ab45f0c`  
  
and then the restart fails.

",pull-request-available,[],AMBARI,Bug,Major,2018-09-03 21:49:19,0
13182670,'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' is not equal to the value of yarn_hierarchy in UI Deploy,"The property 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' does not show up in the UI install wizard even when {{yarn_cgroups_enabled}} and {{gpu_module_enabled}} are enabled in YARN configs. But this property is available after deployment of the cluster.  

 !Screen Shot 2018-09-02 at 10.20.43 PM.png|thumbnail! 
 !Screen Shot 2018-09-02 at 10.22.33 PM.png|thumbnail! 
When trying to set a value for the config 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy', Stack advisor displays the following:
{code}
yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value
yarn.nodemanager.linux-container-executor.cgroups.hierarchy and yarn_hierarchy should always have same value
Name of the Cgroups hierarchy under which all YARN jobs will be launched
{code}
 !Screen Shot 2018-09-02 at 10.25.08 PM.png|thumbnail! 
From the stack advisor description, looks like 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' property needs to be same as yarn_hierarchy. As part of UI Deploy, the property {{yarn_hierarchy}} is set to {{hadoop-yarn-tmp-ctr-e138-1518143905142-461959-01-000002.hwx.site}}. But after deploy, the {{yarn.nodemanager.linux-container-executor.cgroups.hierarchy}} is set to {{/yarn}} and not the property set while UI Deploy. This is causing Deploy failures due to Nodemanagers not getting started. When this property is set correctly, Nodemanagers start successfully. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-09-03 16:19:01,4
13182337,Quicklinks URL overflow outside the UI box,"Quicklinks in HDI environment are overflowing in UI. See attached.

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-31 10:59:36,4
13182161,Ambari Metrics should handle a customized Zookeeper service principal name,"AMS should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be zookeeper/_HOST as opposed to something like zookeeper-mycluster/_HOST.",pull-request-available,['stacks'],AMBARI,Bug,Major,2018-08-30 15:43:00,3
13182116,Atlas should handle a customized Zookeeper service principal name,"Atlas should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be zookeeper/_HOST as opposed to something like zookeeper-mycluster/_HOST.
",pull-request-available,['stacks'],AMBARI,Bug,Major,2018-08-30 12:53:08,3
13182104,Cannot deploy Hive Metastore with Kerberos without HDFS,"In order to enable Kerberos for Hive MetaStore we need a property in {{core-site}}, which is ignored by Ambari if HDFS is not present.  Hive Metastore start fails due to empty {{core-site}} with:

{noformat}
KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL for /hive/cluster/delegationMETASTORE/keys
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-30 11:37:25,5
13181969,global name 'VERIFY_DEPENDENCY_CMD' is not defined',"STR:-
1. deploy the cluster with HDP-3.0.1.0-164 + ambari_version : 2.7.1.0-147
2. added a patch targetVDF-29-08-2018-00-53-55.xml 
3. install version

{code}
2018-08-29 16:47:27,225 - Package Manager failed to install packages: Failed to execute command '/usr/bin/apt-get -o Dpkg::Options::=--force-confdef --allow-unauthenticated --assume-yes install -o Dir::Etc::SourceList=/dev/null -o Dir::Etc::SourceParts= hdp-select', exited with code '100', message: 'E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. 
'
2018-08-29 16:47:27,250 - Could not install packages. Error: global name 'VERIFY_DEPENDENCY_CMD' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 136, in actionexecute
    ret_code = self.install_packages(package_list)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 447, in install_packages
    if not self.repo_mgr.verify_dependencies():
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/apt_manager.py"", line 218, in verify_dependencies
    err_msg = Logger.filter_text(""Failed to verify package dependencies. Execution of '%s' returned %s. %s"" % (VERIFY_DEPENDENCY_CMD, code, out))
NameError: global name 'VERIFY_DEPENDENCY_CMD' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 136, in actionexecute
    ret_code = self.install_packages(package_list)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 447, in install_packages
    if not self.repo_mgr.verify_dependencies():
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/apt_manager.py"", line 218, in verify_dependencies
    err_msg = Logger.filter_text(""Failed to verify package dependencies. Execution of '%s' returned %s. %s"" % (VERIFY_DEPENDENCY_CMD, code, out))
NameError: global name 'VERIFY_DEPENDENCY_CMD' is not defined

The above exception was the cause of the following exception:

Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 486, in <module>
    InstallPackages().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 351, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 149, in actionexecute
    raise Fail(""Failed to distribute repositories/install packages"")
resource_management.core.exceptions.Fail: Failed to distribute repositories/install packages
{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-08-29 18:29:45,3
13181955,Protect the ClusterConfig resource so that only authorized users may have read-only access the data,"Protect the ClientConfig resource so that only authorized users may have read-only access the data.

Users with the following permission should have read-only access:
* {{CLUSTER.VIEW_CONFIGS}}
* {{SERVICE.VIEW_CONFIGS}}
* {{HOST.VIEW_CONFIGS}}

These permissions should be allow for the following roles:
* {{AMBARI.ADMINISTRATOR}}
* {{CLUSTER.ADMINISTRATOR}}
* {{CLUSTER.OPERATOR}}
* {{SERVICE.ADMINISTRATOR}}
* {{SERVICE.OPERATOR}}
* {{CLUSTER.USER}}

Users with no role related to the cluster may not view the data.

Example REST API entry point:
{noformat}
GET /api/v1/clusters/cl1/services/HDFS/components/HDFS_CLIENT?format=client_config_tar
{noformat}
",pull-request-available rbac,['ambari-server'],AMBARI,Bug,Major,2018-08-29 17:27:30,7
13181905,Update styles for pre-upgrade modals,"- Move 'Preparing the Upgrade' text from the corresponding modal to its header
- Upgrade Options popup: add some vertical padding between peragraphs
- Upgrade Options popup: center the contents of upgrade type blocks and add more padding to them
- Make pre-upgrade checks popup wider
- Pre-upgrade checks popup: display warnings as regular text instead of well blocks",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-08-29 12:31:05,2
13181875,Better debugging for agent start failure due to stuck fuser call,"While debugging with we encountered an issue where call to `fuser 8670/tcp` it got blocked for minutes
- in fact we did not ever see the agent start and had to comment the call. Two
improvements:

  * Need timeout for such calls (lets also research and see why the call may get stuck). We checked that the port was free.
  * Need more debug logging in the initial start up path to help debug what command is getting stuck - in main.py, and PingPortListener.py lets see if more debug logging can be added


",pull-request-available,[],AMBARI,Bug,Major,2018-08-29 10:38:52,0
13181872,"Diff in Downloaded client config: Host file has Stack info where as downloaded file has 'None' in ""user.agent.prefix"" properties","  * Download client config for HDFS from UI
  * Compare the contents of core-site.xml between downloaded config and the one in /etc/hadoop/conf/core-site.xml

Following properties have a difference:

  * fs.azure.user.agent.prefix
  * fs.s3a.user.agent.prefix

Both of them have value **User-Agent: APN/1.0 Hortonworks/1.0 HDP/None** in
downloaded file where as in host config file it is **User-Agent: APN/1.0
Hortonworks/1.0 HDP/3.0.0.0-1453**

Could you please check

",pull-request-available,[],AMBARI,Bug,Major,2018-08-29 10:32:44,0
13181626,Nifi Registry install fails,"Facing issue installing Nifi Registry on HDP_HDF cluster. The create keytab step in Ambari is failing during installation. Below exception is seen in ambari logs.
 
{code}
2018-08-21 13:11:03,401 ERROR [Server Action Executor Worker 1305] CreatePrincipalsServerAction:309 - Failed to create principal,  - Failed to create new principal - no principal specified
org.apache.ambari.server.serveraction.kerberos.KerberosOperationException: Failed to create new principal - no principal specified
        at org.apache.ambari.server.serveraction.kerberos.MITKerberosOperationHandler.createPrincipal(MITKerberosOperationHandler.java:159)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.createPrincipal(CreatePrincipalsServerAction.java:268)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.processIdentity(CreatePrincipalsServerAction.java:157)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:460)
        at org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction.execute(CreatePrincipalsServerAction.java:92)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
        at java.lang.Thread.run(Thread.java:748)
2018-08-21 13:11:03,401  INFO [Server Action Executor Worker 1305] KerberosServerAction:481 - Processing identities completed.
2018-08-21 13:11:04,191 ERROR [ambari-action-scheduler] ActionScheduler:482 - Operation completely failed, aborting request id: 117
 {code}

The Ambari UI should not display any properties from Kerberos identity blocks that indicate they are referencing another Kerberos identity. There are 2 ways we know this:

- The new/preferred way: the identity block has a non-empty/non-null ""reference"" attribute
- The old (backwards compatible way): the identity block has a ""name"" attribute the starts with a '/'.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-28 14:44:35,2
13181605,Cannot start Hive Metastore without HDFS,"Starting Hive Metastore fails if HDFS is not present in the cluster with the error: {{JAVA_HOME is not set and could not be found.}}

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 211, in <module>
    HiveMetastore().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive_metastore.py"", line 61, in start
    create_metastore_schema() # execute without config lock
  File ""/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 374, in create_metastore_schema
    user = params.hive_user
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'export HIVE_CONF_DIR=/usr/hdp/current/hive-metastore/conf/conf.server ; /usr/hdp/current/hive-server2-hive2/bin/schematool -initSchema -dbType mysql -userName hive -passWord [PROTECTED] -verbose' returned 1. Error: JAVA_HOME is not set and could not be found.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-28 13:31:40,5
13181577,Yarn Timeline Service V2 Reader goes down after Ambari Upgrade from 2.7.0.0 to 2.7.1.0,"STR:

1) Install cluster with Ambari2.7.0.0 + HDP-3.0.0.0
2) Upgrade Ambari to 2.7.1.0

Yarn Timeline Service V2 Reader goes down after some time.

Reason: the placeholders in yarn.timeline-service.reader.webapp.address and yarn.timeline-service.reader.webapp.https.address are no longer replaced by the stack code so these values become empty. In this case the timeline reader will use the default ports which may conflict with   other ports used by other components.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-28 10:46:13,3
13181568,Move blueprint provisioning state property to host component level,Move blueprint provisioning state property from cluster level to host component level.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-28 09:46:08,1
13181544,Allow skipping Hive Metastore schema creation for sysprepped cluster,"Similar to AMBARI-24540, Hive Metastore DB schema may be manually pre-created to save time during initial service start. However, {{schematool}} could still take quite some time to confirm that the schema exists. The goal of this change is to allow users who pre-create Hive Metastore DB schema to make Ambari skip managing the DB (create or check existence).",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-08-28 08:32:17,5
13181385,Protect the Request resource so that only authorized users may have read-only access the data,"Protect the Request resource so that only authorized users may have read-only access the data.

Users with the following roles should have read-only access:
* {{AMBARI.ADMINISTRATOR}}
* {{CLUSTER.ADMINISTRATOR}}
* {{CLUSTER.OPERATOR}}
* {{SERVICE.ADMINISTRATOR}}
* {{SERVICE.OPERATOR}}
* {{CLUSTER.USER}}

Users with no role related to the cluster may not view the data.",pull-request-available rbac,['ambari-server'],AMBARI,Bug,Major,2018-08-27 14:36:04,6
13181286,Implement Notifications/Coasters from Fluid Design,See screenshot,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-27 10:37:50,4
13181244,Rename LDAP configuration ambari.ldap.advance.collision_behavior,In Ambari 2.7 we moved LDAP configuration into the Ambari DB and introduced common naming pattern. However a typo has been made in _'ambari.ldap.*advance*.collision_behavior_'. This should be renamed to _'ambari.ldap.*advanced*.collision_behavior_',pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-27 06:23:40,6
13181152,Allow skipping Oozie DB schema creation for sysprepped cluster,"Oozie DB schema may be manually pre-created to save time during initial service start.  However, {{ooziedb.sh}} could still take quite some time to confirm that the schema exists.  The goal of this change is to allow users who pre-create Oozie DB schema to make Ambari skip managing the DB (create or check existence).",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-08-25 20:09:31,5
13181092,OneFS mpack should not include webhdfs enable setting,"webhdfs.dfs.enabled is included in the configurations for OneFS management pack. That is not needed because OneFS 8.1.2.0 (used with Ambari 2.7 and the mpack) does not require the disablement of webhdfs in Ambari to support Ambari Views.

webhdfs.dfs.enabled property should be removed entirely from configurations/hdfs-site.xml ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-25 00:27:07,3
13180962,Ambari SPNEGO breaks SSO redirect,"When SPNEGO is enabled (`ambari-server setup-kerberos`), the SSO (`ambari-server setup-sso`) redirect no longer works.

How to reproduce:
# Enable SSO `ambari-server setup-sso`
# `ambari-server restart`
# Visit Ambari and notice that you are redirected to the SSO system (i.e. Knox)
# Enable SPNEGO `ambari-server setup-kerberos`
# `ambari-server restart`
# Visit Ambari and notice that you are *NOT redirected* to the SSO system (i.e. Knox)",kerberos pull-request-available security spnego sso,"['ambari-server', 'security']",AMBARI,Bug,Major,2018-08-24 12:03:46,7
13180886,Ambari Server Ldap Sync Failed upon subject alternative DNS name check,"STR:
 1. Install Ambari
 2. Get certificate for secure LDAP (LDAPS) connection to your AD server.
 3. Generate ambari truststore with LDAPS certificate.
 4. Setup Ambari to use LDAPS with providing truststore.
{code:java}
2018-08-20 18:38:04,763 DEBUG com.hw.commonuifrm.impl.commands.CommandExecutorImpl.executeCommand(): Sending command [(echo ""admin"" ; echo ""admin"") | ambari-server sync-ldap --users /tmp/users.txt --groups /tmp/groups.txt]


2018-08-20 18:38:05,666 DEBUG com.hw.commonuifrm.impl.commands.ProcessDataImpl.buildOutputAndErrorStreamData(): /usr/lib64/python2.7/getpass.py:83: GetPassWarning: Can not control echo on the terminal.
  passwd = fallback_getpass(prompt, stream)
Warning: Password input may be echoed.
Enter Ambari Admin password:


2018-08-20 18:38:07,169 INFO com.hw.ambari.ui.util.cluster_managers.LDAPClusterManager.ambariServerSyncLDAPWithAD(): Result: Using python  /usr/bin/python
Syncing with LDAP...
Enter Ambari Admin login: 
Fetching LDAP configuration from DB.
Syncing specified users and groups...ERROR: Exiting with exit code 1. 
REASON: Caught exception running LDAP sync. ***.com:636; nested exception is javax.naming.CommunicationException: ***.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching ***.com found.]

2018-08-20 18:38:07,170 INFO com.hw.ambari.ui.tests.console.ldap.TestLDAPSOnAD.test010_AmbariSynchronizeWithADThroughLDAPS(): AMBARI LDAPS synchronization result: Using python  /usr/bin/python
Syncing with LDAP...
Enter Ambari Admin login: 
Fetching LDAP configuration from DB.
Syncing specified users and groups...ERROR: Exiting with exit code 1. 
REASON: Caught exception running LDAP sync. ***.com:636; nested exception is javax.naming.CommunicationException: ***.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching ***.com found.]{code}
The issue is that the AD server's certificate contains a section:
{noformat}
X509v3 Subject Alternative Name: othername:<unsupported>, DNS:***-2.COM{noformat}
As you can see this is not the same that we use to connect to the AD server (***.com:636). Even if this is a certificate issue the connection could be open and we should be able to sync LDAP users/groups.

*Important note*: it's reproducible only with OpenJDK (I used openjdk-1.8.0.181-3.b13.el7_5.x86_64); working properly with Oracle's JDK.

+*Recommended solution*+

We can disable endpoint identification when the client is negotiating with the server during SSL handshake by setting _com.sun.jndi.ldap.object.disableEndpointIdentification_ to _true_ (see [https://github.com/ojdkbuild/lookaside_java-1.8.0-openjdk/blob/master/jdk/src/share/classes/com/sun/jndi/ldap/Connection.java#L386]). By default this should not be the case but end users may set this up when configuring LDAP if they face this issue.

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-24 07:10:15,6
13180705,"Kerberos ""Additional Realms"" should not require keytab re-generation and cluster restart","""Admin -> Kerberos -> Additonal Realms""
* Currently requires keytab re-generation which in turn requires restarting the cluster. *But it is completely unrelated to keytabs*.

Fix:
* Move ""Additional Realms"" to the ""Kerberos"" service configs where it belongs, along with the ""auth_to_local"" setting which is what it is used for.
* When it is changed:
   ** No keytab re-generation is then required.
   ** Instead of silently altering ""auth_to_local"" rules, they should come up as ""Recommendations"".",auth_to_local kerberos,"['ambari-admin', 'security']",AMBARI,Bug,Major,2018-08-23 12:41:46,7
13180484,Accumulo does not startup in Federated Cluster,"In a manually setup federated cluster (not through deployNG) --

Accumulo was installed and when trying to start, below error thrown --

{noformat}
2018-08-16 07:33:31,748 [start.Main] ERROR: Thread 'org.apache.accumulo.master.state.SetGoalState' died.
java.lang.IllegalArgumentException: Expected fully qualified URI for instance.volumes got ns2/apps/accumulo/data
	at org.apache.accumulo.core.volume.VolumeConfiguration.getVolumeUris(VolumeConfiguration.java:107)
	at org.apache.accumulo.server.fs.VolumeManagerImpl.get(VolumeManagerImpl.java:334)
{noformat}

Caused by incorrect config value:
{{instance.volumes = hdfs://ns1,ns2/apps/accumulo/data}}
where ns1 and ns2 are namespaces

Expected is --
{{instance.volumes = hdfs://ns1/apps/accumulo/data,hdfs://ns2/apps/accumulo/data}}

according to --
https://accumulo.apache.org/docs/2.0/administration/multivolume
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-22 14:25:09,2
13180444,"Unable to validate password complexity for properties rangertagsync_user_password, rangerusersync_user_password","STR

- perform UI deploy with selecting Ranger & Ranger KMS

Used password: {{78C%4KwS5J$&}}   or any other, created by password generator with defined complexity 


Additionally, check on glitches picture 1 & 2. Additionally, I believe it is best to put special characters to grayed/dashed box to make user attention to them.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-08-22 10:46:06,4
13180268,Cannot connect to MIT KDC admin server when port is specified in kerberos-env/admin_server_host,"Cannot connect to MIT KDC admin server when port is specified in {{kerberos-env/admin_server_host}}.  The following error is seen when validating the KDC admin credentials:

{noformat}
kinit: Server not found in Kerberos database while getting initial credentials
{noformat}

The reason for this is due to how the credentials are created for accessing the MIT KDC administration server. 

{noformat}
kinit -c <path> -S kadmin/<kerberos-env/admin_server_host>  <principal>
{noformat}

If a port was added to the {{kerberos-env/admin_server_host}} value then the server principal will be generated like {{kadmin/kdc.example.com:4749}} rather than {{kadmin/kdc.example.com}}. Therefore the server principal is not found.
",kerberos pull-request-available regression,['ambari-server'],AMBARI,Bug,Critical,2018-08-21 18:14:21,7
13180258,CLONE - Requests STOMP topic sent updates for host check request,"Server should post only cluster related request updates to ""/events/requests"" STOMP topic.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-21 17:39:56,1
13180249,Fix set KDC admin credentials section in enable Kerberos doc,"Fix set KDC admin credentials section in enable Kerberos doc

The current documentation at https://github.com/apache/ambari/blob/branch-2.7/ambari-server/docs/security/kerberos/enabling_kerberos.md#set-the-kdc-administrator-credentials indicates an incorrect URL for the API call. 

It should read:

{noformat}
http://AMBARI_SERVER:8080/api/v1/clusters/CLUSTER_NAME/credentials/kdc.admin.credential
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-08-21 16:49:52,7
13180217,Requests STOMP topic sent updates for host check request,"Server should post only cluster related request updates to ""/events/requests"" STOMP topic.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-21 14:00:11,1
13180160,Remove dependency on JQuery 1.8.0 for Ambari Server UI,"Remove dependency on JQuery 1.8.0 for Ambari Server UI due to security concerns. See 
* CVE-2012-6708 - https://nvd.nist.gov/vuln/detail/CVE-2012-6708
* CVE-2011-4969 - https://nvd.nist.gov/vuln/detail/CVE-2011-4969
* CVE-2015-9251 - https://nvd.nist.gov/vuln/detail/CVE-2015-9251

It is recommended that JQuery is updated to 1.8.3+1

Path to offending file:
{noformat}
ambari
|- ambari-server-2.7.1.0-119.x86_64.rpm
|  |- usr
|  |  |- lib
|  |  |  |- ambari-server
|  |  |  |  |- web
|  |  |  |  |  |- api-docs
|  |  |  |  |  |  |- lib
|  |  |  |  |  |  |  |- jquery-1.8.0.min.js
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-21 08:54:57,4
13180142,NPE when migrating users table during upgrade to Ambari 2.7.0 with Oracle DB,"NPE when migrating users table during upgrade to Ambari 2.7.0 with Oracle DB:
{noformat}
2018-08-20 11:36:46,395 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed. 
java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.convertUserCreationTimeToLong(UpgradeCatalog270.java:595)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:342)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:318)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:970)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
2018-08-20 11:36:46,395 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.convertUserCreationTimeToLong(UpgradeCatalog270.java:595)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:342)
	at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:318)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:970)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
	... 1 more
{noformat}
*Cause*
This is caused by one or more records with a {{NULL}} value in the {{create_time}} field.

For example:
||user_id||user_name||user_type||create_time||
|1|admin|LOCAL|NULL|

*Workaround* 
Update the relevant records to not have a {{NULL}} in the {{create_time}} column.

For example:
{noformat}
UPDATE users SET create_time=systimestamp WHERE create_time IS NULL;
{noformat}
*Solution*
During upgrade, protect against a {{null}} value for {{currentUserCreateTime.getValue()}} at:
{code:java|title=org/apache/ambari/server/upgrade/UpgradeCatalog270.java:595}
          dbAccessor.updateTable(USERS_TABLE, temporaryColumnName, currentUserCreateTime.getValue().getTime(),
              ""WHERE "" + USERS_USER_ID_COLUMN + ""="" + currentUserCreateTime.getKey());
{code}
If {{currentUserCreateTime.getValue()}} is null, the current timestamp should be used.

*Note:* This may be a reoccurring issue since there is no provision to ensure that {{create_time}} is not {{NULL}} when initializing the Ambari database:
{code:java}
insert into users(user_id, principal_id, user_name, user_password)
select 1,1,'admin','538916f8943ec225d97a9a86a2c6ec0818c1cd400e09e03b660fdaaec4af29ddbb6f2b1033b81b00' from dual;{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-21 07:38:53,6
13180069,Remove dependency on org.bouncycastle bcprov-jdk15on before version 1.6.0 for Ambari Server,"Remove dependency on org.bouncycastle bcprov-jdk15on before version 1.6.0 for Ambari Server security concerns. See 
* CVE-2018-1000180 - https://nvd.nist.gov/vuln/detail/CVE-2018-1000180

This dependency is compiled into the apacheds-all.jar from 

{code}
    <dependency>
      <groupId>org.apache.directory.server</groupId>
      <artifactId>apacheds-all</artifactId>
      <version>2.0.0-M24</version>
    </dependency>
{code}

The relevant parts of this need to be broken out and the offending bouncy castle JAR needs to be excluded as needed. 
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-20 21:19:23,7
13179943,host component out of sync error message on version screen,"Below is the screenshot on versions screen of ambari where below error message is displayed.

""Host component out of sync HDP-2.2.9.0-3393
One of more host components did not report the version that Ambari expected. Please re-install the failed host component, or remove it.""

Please note these hosts have gone through an CentOS6 to CentOS7 OS upgrade and then ambari-server is upgraded to 2.7..1#73.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-20 11:36:09,4
13179642,Stack and Versions page style fixes,"- Change text in 'Manage versions' modal
- Fix alignment issues on Versions tab
- Wrap services column on Versions tab into white block
- Remove redundant horizontal space in services column
- Make shadow of version column softer",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-08-17 17:36:26,2
13179544,Ambari updates lates_url info for HDF stacks with HDP link,Ambari updates the repoinfo.xml files in each stack and changes the latest value to the json.url which is found in the repo file (for example /etc/yum.d/ambari.repo). In a non-HDP stack this causes the wrong url to be added to repoinfo.xml,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-17 09:19:20,3
13179508,ambari-server setup fails with postgresql >= 9.3 ,"ambari-server setup with default postgres has been failing with

    
    
    OSError: [Errno 2] No such file or directory: '/usr/bin/postgresql-setup'
    

Looks like postgres packages/dependencies are not installed along with
installing ambari server install.  
below is output from ambari-server install from ambari-2.7.1 cluster

    
    
    2018-08-07 11:17:56,106|executor.py.167|INFO|14260|MainThread|172.27.25.203|executing the command='yum -y install ambari-server'
    2018-08-07 11:17:56,432|executor.py.167|INFO|14260|Thread-191|stdout: Loaded plugins: ovl, priorities
    2018-08-07 11:17:57,971|executor.py.167|INFO|14260|Thread-191|stdout: 12711 packages excluded due to repository priority protections
    2018-08-07 11:17:58,754|executor.py.167|INFO|14260|Thread-191|stdout: Resolving Dependencies
    2018-08-07 11:17:58,754|executor.py.167|INFO|14260|Thread-191|stdout: --> Running transaction check
    2018-08-07 11:17:58,755|executor.py.167|INFO|14260|Thread-191|stdout: ---> Package ambari-server.x86_64 0:2.7.1.0-63 will be installed
    2018-08-07 11:17:58,974|executor.py.167|INFO|14260|Thread-191|stdout: --> Finished Dependency Resolution
    2018-08-07 11:17:59,146|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,146|executor.py.167|INFO|14260|Thread-191|stdout: Dependencies Resolved
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout:  Package            Arch        Version            Repository              Size
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,147|executor.py.167|INFO|14260|Thread-191|stdout: Installing:
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout:  ambari-server      x86_64      2.7.1.0-63         ambari-2.7.1.0-63      366 M
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: Transaction Summary
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: ================================================================================
    2018-08-07 11:17:59,148|executor.py.167|INFO|14260|Thread-191|stdout: Install  1 Package
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: Total download size: 366 M
    2018-08-07 11:17:59,149|executor.py.167|INFO|14260|Thread-191|stdout: Installed size: 434 M
    2018-08-07 11:17:59,152|executor.py.167|INFO|14260|Thread-191|stdout: Downloading packages:
    2018-08-07 11:18:03,735|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction check
    2018-08-07 11:18:03,736|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction test
    2018-08-07 11:18:04,048|executor.py.167|INFO|14260|Thread-191|stdout: Transaction test succeeded
    2018-08-07 11:18:04,049|executor.py.167|INFO|14260|Thread-191|stdout: Running transaction
    2018-08-07 11:18:21,843|executor.py.167|INFO|14260|Thread-191|stdout:   Installing : ambari-server-2.7.1.0-63.x86_64                              1/1
    2018-08-07 11:18:22,568|executor.py.167|INFO|14260|Thread-191|stdout:  
    2018-08-07 11:18:22,569|executor.py.167|INFO|14260|Thread-191|stdout:   Verifying  : ambari-server-2.7.1.0-63.x86_64                              1/1
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout:  
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout: Installed:
    2018-08-07 11:18:22,676|executor.py.167|INFO|14260|Thread-191|stdout:   ambari-server.x86_64 0:2.7.1.0-63                                             
    2018-08-07 11:18:22,677|executor.py.167|INFO|14260|Thread-191|stdout: 
    2018-08-07 11:18:22,677|executor.py.167|INFO|14260|Thread-191|stdout: Complete!
    

where as on an ambari-2.7.0 cluster we do have postgres server packages and
lib and is being installed with ambari-server install

    
    
    [root@c7401 ~]# yum install -y ambari-server
    Loaded plugins: fastestmirror
    ambari-2.7.0.0                                                                                                                                             | 2.9 kB  00:00:00     
    ambari-2.7.0.0/primary_db                                                                                                                                  |  25 kB  00:00:00     
    Loading mirror speeds from cached hostfile
     * base: repos.lax.quadranet.com
     * extras: linux.mirrors.es.net
     * updates: mirror.keystealth.org
    Resolving Dependencies
    --> Running transaction check
    ---> Package ambari-server.x86_64 0:2.7.0.0-897 will be installed
    --> Processing Dependency: postgresql-server >= 8.1 for package: ambari-server-2.7.0.0-897.x86_64
    --> Running transaction check
    ---> Package postgresql-server.x86_64 0:9.2.23-3.el7_4 will be installed
    --> Processing Dependency: postgresql-libs(x86-64) = 9.2.23-3.el7_4 for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Processing Dependency: postgresql(x86-64) = 9.2.23-3.el7_4 for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Processing Dependency: libpq.so.5()(64bit) for package: postgresql-server-9.2.23-3.el7_4.x86_64
    --> Running transaction check
    ---> Package postgresql.x86_64 0:9.2.23-3.el7_4 will be installed
    ---> Package postgresql-libs.x86_64 0:9.2.23-3.el7_4 will be installed
    --> Finished Dependency Resolution
    
    Dependencies Resolved
    
    ==================================================================================================================================================================================
     Package                                        Arch                                Version                                     Repository                                   Size
    ==================================================================================================================================================================================
    Installing:
     ambari-server                                  x86_64                              2.7.0.0-897                                 ambari-2.7.0.0                              367 M
    Installing for dependencies:
     postgresql                                     x86_64                              9.2.23-3.el7_4                              base                                        3.0 M
     postgresql-libs                                x86_64                              9.2.23-3.el7_4                              base                                        234 k
     postgresql-server                              x86_64                              9.2.23-3.el7_4                              base                                        3.8 M
    
    Transaction Summary
    ==================================================================================================================================================================================
    Install  1 Package (+3 Dependent packages)
    
    Total download size: 374 M
    Installed size: 468 M
    Downloading packages:
    (1/4): postgresql-libs-9.2.23-3.el7_4.x86_64.rpm                                                                                                           | 234 kB  00:00:00     
    (2/4): postgresql-server-9.2.23-3.el7_4.x86_64.rpm                                                                                                         | 3.8 MB  00:00:00     
    (3/4): postgresql-9.2.23-3.el7_4.x86_64.rpm                                                                                                                | 3.0 MB  00:00:04     
    warning: /var/cache/yum/x86_64/7/ambari-2.7.0.0/packages/ambari-server-2.7.0.0-897.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID 07513cad: NOKEY17 MB/s | 371 MB  00:00:00 ETA 
    Public key for ambari-server-2.7.0.0-897.x86_64.rpm is not installed
    (4/4): ambari-server-2.7.0.0-897.x86_64.rpm                                                                                                                | 367 MB  00:00:23     
    ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    Total                                                                                                                                              16 MB/s | 374 MB  00:00:23     
    Retrieving key from http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.0.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    Importing GPG key 0x07513CAD:
     Userid     : ""Jenkins (HDP Builds) <jenkin@hortonworks.com>""
     Fingerprint: df52 ed4f 7a3a 5882 c099 4c66 b973 3a7a 0751 3cad
     From       : http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.7.0.0/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    Running transaction check
    Running transaction test
    Transaction test succeeded
    Running transaction
      Installing : postgresql-libs-9.2.23-3.el7_4.x86_64                                                                                                                          1/4 
      Installing : postgresql-9.2.23-3.el7_4.x86_64                                                                                                                               2/4 
      Installing : postgresql-server-9.2.23-3.el7_4.x86_64                                                                                                                        3/4 
      Installing : ambari-server-2.7.0.0-897.x86_64                                                                                                                               4/4 
      Verifying  : postgresql-9.2.23-3.el7_4.x86_64                                                                                                                               1/4 
      Verifying  : ambari-server-2.7.0.0-897.x86_64                                                                                                                               2/4 
      Verifying  : postgresql-libs-9.2.23-3.el7_4.x86_64                                                                                                                          3/4 
      Verifying  : postgresql-server-9.2.23-3.el7_4.x86_64                                                                                                                        4/4 
    
    Installed:
      ambari-server.x86_64 0:2.7.0.0-897                                                                                                                                              
    
    Dependency Installed:
      postgresql.x86_64 0:9.2.23-3.el7_4                     postgresql-libs.x86_64 0:9.2.23-3.el7_4                     postgresql-server.x86_64 0:9.2.23-3.el7_4                    
    
    Complete!
    


",pull-request-available,[],AMBARI,Bug,Major,2018-08-17 05:55:04,0
13179272,Yarn Timeline Service V2 Reader is found down after EU (Atlantic to AtlanticM05) With error - Address already in use,{{{{timeline_reader_address_http}}}} and {{{timeline_reader_address_https}}}} properties are not replaced in yarn-site by the FixTimelineReaderAddress upgrade task.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-16 07:17:29,3
13179113,HDP-GPL repo's shouldn't be pushed to hosts when GPL license was not applied,Only push the HDP-GPL repo to hosts when user applied GPL license during ambari-server setup.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-15 15:49:27,1
13179065,Overlapping text in Recommendations in Configurations page while UI installer,Overlapping text in Centralized Configurations page: [^config-validation-overlap.png],pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-08-15 12:45:31,2
13178883,ambair-agent floods data directory with files created for status commands,"
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-001de461-9c2f-417a-b323-6233ed86eb37.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00209852-ad33-4026-90a0-4da2aac71403.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00333009-87b2-4649-80d1-c97b639a31f0.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0042da27-dae6-4e9b-9ffb-336cd17787c5.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-005b6b80-baf7-43a5-aa7f-2e4695b3f478.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0066dcd4-9406-49c7-a2b3-effd9a3a4f2b.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-006a1999-d33d-4d48-a74f-8a7830b449ec.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0094b368-c5e1-496e-ba5b-b5080767dbc0.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-00b2a497-0088-464c-a4e4-855534d082ed.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00bbd469-370f-4eca-aee9-109db61edd75.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00bbe2f2-aeb2-4eb7-9bfb-6ce5fe9950b0.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00c341b7-5c4c-4dc5-a5ba-a550b0cabf67.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00d80541-e6a5-4bc4-9020-c018a928c6b9.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00da8bca-ac68-4d47-b492-2a978f368a1c.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00ed37ff-a9b8-4fe6-a46d-7d792586ac62.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00f49f50-cd1e-498f-ac97-5cc9724adf56.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-00f74fcb-8cc1-46f8-b74a-6ea3089e19d9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-010174eb-dee9-41a7-9afb-25c5b16e185e.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01024c7d-beb5-40b2-914c-aece83011cea.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0106b69f-9343-40d1-a225-65112470f884.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-0109e024-4cbf-49a8-8496-9d05787ed58d.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-011e9c15-5531-43c9-aabf-a9efa5123d2f.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-012f2630-46ad-4608-b7ac-971d3ee35f59.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0130258e-5c15-4147-a348-c86fa0ae5a1c.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01357110-1577-4d4f-908d-7956232ecd73.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-013a04e5-d2e8-4d9e-8cb6-3fe3ac6f75ca.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01456ca3-f135-4fd8-8b58-83ea513f7c06.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01473538-7a02-4df9-92c4-728d158d3e0f.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-014bdd5a-899d-43f6-a375-d65e30e0a2c3.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-014e0a58-fdc5-4d0a-8b0c-ac62129ba0ce.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-015c87f3-cdaa-4cf5-91ee-bf9af09e4262.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-015c972a-9210-4854-a49b-a1d591f799e6.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-016d1956-8076-4722-829d-e10748420432.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-017e22f1-561a-41ca-a59e-1473f3ec3065.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-017fe18b-5dc7-4c97-891a-737810ec5416.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-01d63c54-9033-4932-974f-f6c756a2d1d4.json
    2018-08-13T21:27:31.000Z        0.1 kB         status_structured-out-01ff5f2e-4572-4f52-b77c-12e3aea1c6fa.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02216bb7-9bac-4c5b-a074-f3e2b33c7a86.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02239432-9ba1-4899-923f-2f94c5facb89.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0225d7ba-22c5-412e-bea1-931732bf98f2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-023d005d-d253-4445-951b-22b903e9ff85.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-023e4124-bd5d-4e0d-8f1e-b146b5f9bb03.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-024c5c89-56c0-4470-8d64-2ab9e48707fa.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-025bb933-f6a5-4495-8df4-342e0c695845.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-026dcffa-015b-4ac0-be4e-70c2092a091a.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0272568c-5492-46be-8c6d-26edebb06f92.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0283f8dd-0d90-4978-9b44-c982a0ee78e9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0291a3b2-9b11-4e60-9f36-d63e11d48803.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02a4a54a-fc9c-4839-94b3-fd29cdbaeda6.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02a781a3-4db4-468b-a180-964d4fcdf176.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02bdfe4c-a2ce-4e38-990b-12c889a06a32.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02c873ea-60ce-444a-8b67-05c648d4c8d5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02d706ec-4b9d-44ad-baf6-67ec62dbdcdd.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02de1f1d-d3e8-4c74-b5dc-550540f4277e.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-02e34a4b-8f2d-489b-acd3-3c77b07e4de9.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-02e42e6d-8756-45f6-8097-16430aebb4e3.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02e48587-00ff-48e4-81d4-a8535b3d6cd7.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-02eebc26-43ed-4b36-99df-091e66ed6317.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0304d392-7471-422c-9229-2222ca7be62a.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-031091e8-b6cd-47d8-804c-29947ad34c87.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03140883-e4d2-4cc3-9731-7406762199da.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03203cad-edfc-4293-90db-8edd305f470b.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-032c1a82-386b-43de-9e17-d6228ce4ca40.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-03351bb9-9cc3-4d80-9bbb-4edf49b93b9c.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0348d610-18bf-42cc-8f3b-e16671f0f7b8.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-034d698d-7946-4503-b64b-20c9592c363b.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03640ef9-54f3-45c9-b9de-a0a211524093.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-036ad39f-9ef1-4967-b023-316e3429af34.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0377c628-708b-41f5-b3cf-1dda26686fda.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-037b5ae4-966c-4815-82fb-d10d7e7947c5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-037ce4fe-4c96-4614-bca1-ddb4d702ce03.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-039472ba-ea55-4a68-922b-dd0d799c1c07.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03990281-da3a-4d79-8998-5ae573d62fae.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03f2239b-f8d9-4bb4-a043-a675b91dd2e2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-03f70e9b-549f-4019-9bd6-19a511ce67e1.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0403a633-8967-4408-bd94-e8a5d1e1b4b2.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-0416dae8-52da-48c6-9451-e3bd10f8a0c7.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-041eebf1-8908-4484-aa5b-67c22101942c.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0452e168-012a-4b77-b73e-89f900fd9ce5.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-045c0c2a-4a93-4ccb-8b02-e31f32f297c4.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-046d50dd-f5de-4ef9-a0eb-36b6946a3ce9.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0490547f-88dd-446c-aa64-1897521e2eba.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-049a2eca-90a1-4ca4-9216-6065f90bf855.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-049f27b7-f073-41a3-a415-1d8975b2d608.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04a989b1-2d78-4f35-bef9-9be4ab6c13e0.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04b2d2ee-f5d4-4c98-8011-db57ec5c28da.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-04b3c7f8-5c80-418c-a4f6-c6148e0c8a92.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04be6118-ae18-4d2d-80ca-5fc79dacf86d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04bedda7-53ab-40b4-8c7d-33242b660f06.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04bfc4e2-aa8f-4a24-a79f-b483bb80d210.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04c7e1dc-3846-453e-be10-75955445088c.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04dd82ce-25da-4dea-8b20-cda14df347fd.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-04f65567-6f8c-4da8-8505-80664a4ee439.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-050f7e3c-6460-4045-9bdb-1eb3b602136d.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-051e0ccb-d6b8-4158-a1a9-9d7d0d52224d.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-052e32da-f71b-4f09-b62b-b3f30b25ec32.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-056e9da7-5a96-4646-bae6-2d080d0b2a82.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-056f3673-aa47-4efc-b0ad-e0ab96812654.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05708b0e-0df8-4b16-8bd9-874a72607186.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0577ca36-f1f2-49ce-852a-43b56a6656ca.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-059f1d92-1419-42c4-a7fd-d0f85a85f240.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05af50fb-e48f-40a9-9877-962d1718bbff.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05b2dcef-6aea-400d-992d-f0443d4c56d2.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05b706f4-cd06-49b1-8d9c-c356f38c99d3.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05f829b6-8340-4673-97a2-94eaf6290e23.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-05ff36a6-c39a-412e-b6c4-58f0f50cbab0.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0609de51-1254-4385-98f8-0bc027510908.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06123cae-b89d-496b-8a26-13c3e208b087.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0625192e-495d-4a2f-8226-54befd87daea.json
    2018-08-13T21:47:09.000Z        0.1 kB         status_structured-out-062ab0e0-9854-46ff-91a0-47e6d920e055.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-062bef42-9ef9-4808-896d-4b77a124458d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06510548-2629-480b-99e0-9769a4a3e254.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-066265b2-56e9-4629-8c13-309fd5b9bedb.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-066f2f1e-337e-4dca-a7aa-51b4efcb2b4d.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-067d3e69-333a-424f-a846-c6411d8e8e87.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068226fb-f9d8-46bb-8bd7-3a13056bb983.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-0688025a-d92d-4a8d-b14d-f7486a3f0d59.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068c77f3-ad4f-4986-836f-f4b9f44b53bf.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-068f3c75-a176-49c2-96ba-48a4a6cc76ed.json
    2018-08-13T21:27:32.000Z        0.1 kB         status_structured-out-06adba4c-9a60-432a-80f4-780602f476a7.json
    ...
    

",pull-request-available,[],AMBARI,Bug,Major,2018-08-14 15:36:16,0
13178857,Disable autostart during blueprint deploy,Ambari should disable components' autostart during blueprint deploy.,pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-08-14 13:40:39,1
13178549,ambari-server upgrade stuck with NPE,"====================================
Ambari 2.6.2
HDP 2.6.5.0-292
Cluster Details : https://github.com/hortonworks/HCube#stanley-hotel-hdp-26
Please login with okta credential to these cluster machines
https://hcube1-1n01.eng.hortonworks.com:8443/
====================================

This is cluster is being upgraded to Ambari 2.7.1 and the schema upgrade step is hung.
{code}
root@hcube1-1n01 ~]# ambari-server upgrade
Using python  /usr/bin/python
Upgrading ambari-server
INFO: Upgrade Ambari Server
INFO: Updating Ambari Server properties in ambari.properties ...
INFO: Updating Ambari Server properties in ambari-env.sh ...
INFO: Original file ambari-env.sh kept
WARNING: Original file krb5JAASLogin.conf kept
INFO: File krb5JAASLogin.conf updated.
INFO: Fixing database objects owner
Ambari Server configured for Postgres. Confirm you have made a backup of the Ambari Server database [y/n] (n)? y
INFO: Upgrading database schema



{code}


Below exception is noticed in ambari-server.log

{code}
2018-08-10 06:57:50,795 ERROR [main] AbstractUpgradeCatalog:375 - Error in transaction
java.lang.NullPointerException
        at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1617)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:184)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:94)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270$3.run(UpgradeCatalog270.java:1637)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.executeInTransaction(AbstractUpgradeCatalog.java:367)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1633)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1060)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:457)
2018-08-10 06:57:50,799 ERROR [main] SchemaUpgradeHelper:239 - Upgrade failed.
org.apache.ambari.server.AmbariException: Failed to upgrade kerberos tables
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1644)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDMLUpdates(UpgradeCatalog270.java:1060)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:985)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:237)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:457)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.executeInTransaction(AbstractUpgradeCatalog.java:379)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateKerberosConfigurations(UpgradeCatalog270.java:1633)
        ... 4 more
Caused by: java.lang.NullPointerException
        at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1617)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:184)
        at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:94)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270$3.run(UpgradeCatalog270.java:1637)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.executeInTransaction(AbstractUpgradeCatalog.java:367)
        ... 5 more
{code}

Attached is the complete ambari-server logs


Below is the jstack dump of schemaupgradehelper

{code}
[root@hcube1-1n01 ambari-server]# /opt/java/jdk1.8.0_152/bin/jstack -l 8239
2018-08-10 07:10:00
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.152-b16 mixed mode):

""Attach Listener"" #40 daemon prio=9 os_prio=0 tid=0x00007f9a6c001000 nid=0x3afa waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""DestroyJavaVM"" #39 prio=5 os_prio=0 tid=0x00007f9b6c00d000 nid=0x2030 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""FileWatchdog:ambari.properties"" #38 daemon prio=5 os_prio=0 tid=0x00007f9b6d684000 nid=0x2c19 waiting on condition [0x00007f9b2e9d4000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.log4j.helpers.FileWatchdog.run(FileWatchdog.java:104)

   Locked ownable synchronizers:
	- None

""pool-6-thread-1"" #37 prio=5 os_prio=0 tid=0x00007f9a10006800 nid=0x210f waiting on condition [0x00007f9b2eed9000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080dc3270> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""pool-5-thread-1"" #36 prio=5 os_prio=0 tid=0x00007f9b6d497800 nid=0x2108 waiting on condition [0x00007f9b2edd8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080dc2fe8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""pool-3-thread-1"" #35 prio=5 os_prio=0 tid=0x00007f9b6d0cb000 nid=0x2107 waiting on condition [0x00007f9b2f2dd000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080dc2b38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""server-action-executor-cache-timer"" #23 daemon prio=5 os_prio=0 tid=0x00007f9b6da29000 nid=0x20d8 in Object.wait() [0x00007f9b34120000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000080f0b6e0> (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x0000000080f0b6e0> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

   Locked ownable synchronizers:
	- None

""InMemoryCredentialStore active cleanup timer"" #22 daemon prio=5 os_prio=0 tid=0x00007f9b6d541800 nid=0x20cd waiting on condition [0x00007f9b35021000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000080f2c750> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

""Service Thread"" #20 daemon prio=9 os_prio=0 tid=0x00007f9b6c107800 nid=0x205c runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread14"" #19 daemon prio=9 os_prio=0 tid=0x00007f9b6c104800 nid=0x205b waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread13"" #18 daemon prio=9 os_prio=0 tid=0x00007f9b6c102800 nid=0x205a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread12"" #17 daemon prio=9 os_prio=0 tid=0x00007f9b6c100000 nid=0x2059 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007f9b6c0fe000 nid=0x2058 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007f9b6c0fb800 nid=0x2057 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f9800 nid=0x2056 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f7800 nid=0x2054 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread7"" #12 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f5000 nid=0x2053 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread6"" #11 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f3000 nid=0x2052 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread5"" #10 daemon prio=9 os_prio=0 tid=0x00007f9b6c0f0800 nid=0x2051 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread4"" #9 daemon prio=9 os_prio=0 tid=0x00007f9b6c0ee800 nid=0x2050 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007f9b6c0ec000 nid=0x204f waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007f9b6c0ea000 nid=0x204e waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007f9b6c0e8000 nid=0x204d waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007f9b6c0e5000 nid=0x204c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007f9b6c0e3800 nid=0x204b runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007f9b6c0af800 nid=0x204a in Object.wait() [0x00007f9b3d0e2000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008020b148> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
	- locked <0x000000008020b148> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

   Locked ownable synchronizers:
	- None

""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f9b6c0ab000 nid=0x2049 in Object.wait() [0x00007f9b3d1e3000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x000000008020c698> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x000000008020c698> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

""VM Thread"" os_prio=0 tid=0x00007f9b6c0a3800 nid=0x2048 runnable 

""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c022800 nid=0x2031 runnable 

""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c024000 nid=0x2032 runnable 

""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c026000 nid=0x2033 runnable 

""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c028000 nid=0x2034 runnable 

""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c029800 nid=0x2035 runnable 

""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c02b800 nid=0x2036 runnable 

""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c02d000 nid=0x2037 runnable 

""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c02f000 nid=0x2038 runnable 

""GC task thread#8 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c031000 nid=0x2039 runnable 

""GC task thread#9 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c032800 nid=0x203a runnable 

""GC task thread#10 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c034800 nid=0x203b runnable 

""GC task thread#11 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c036800 nid=0x203c runnable 

""GC task thread#12 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c038000 nid=0x203d runnable 

""GC task thread#13 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03a000 nid=0x203e runnable 

""GC task thread#14 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03c000 nid=0x203f runnable 

""GC task thread#15 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03d800 nid=0x2040 runnable 

""GC task thread#16 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c03f800 nid=0x2041 runnable 

""GC task thread#17 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c041800 nid=0x2042 runnable 

""GC task thread#18 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c043000 nid=0x2043 runnable 

""GC task thread#19 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c045000 nid=0x2044 runnable 

""GC task thread#20 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c046800 nid=0x2045 runnable 

""GC task thread#21 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c048800 nid=0x2046 runnable 

""GC task thread#22 (ParallelGC)"" os_prio=0 tid=0x00007f9b6c04a800 nid=0x2047 runnable 

""VM Periodic Task Thread"" os_prio=0 tid=0x00007f9b6c10c800 nid=0x205d waiting on condition 

JNI global references: 328
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-08-13 09:53:14,3
13178263,Hiveserver2 can't connect to metastore when using OneFS,"{code}
2018-07-12T07:56:27,148 ERROR [pool-6-thread-3]: metastore.HiveMetaStore (HiveMetaStore.java:get_current_notificationEventId(7617)) - Not authorized to make the get_current_notificationEventId call. You can try to disable metastore.metastore.event.db.notification.api.auth
org.apache.hadoop.hive.metastore.api.MetaException: User hive is not allowed to perform this API call
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7655) ~[hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7615) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at com.sun.proxy.$Proxy34.get_current_notificationEventId(Unknown Source) [?:?]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_112]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_112]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) [hadoop-common-3.1.0.3.0.0.0-1628.jar:?]
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.0.0.0-1628.jar:3.1.0.3.0.0.0-1628]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-12T07:56:27,153 ERROR [pool-6-thread-3]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(201)) - org.apache.thrift.TException: MetaException(message:User hive is not allowed to perform this API call)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7619)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy34.get_current_notificationEventId(Unknown Source)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688)
        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:User hive is not allowed to perform this API call)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7655)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7615)
        ... 20 more
{code}

hadoop.proxyuser.hive.hosts and hadoop.proxyuser.hive.groups is not set when OneFS is used in place of HDFS",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-10 12:21:47,3
13178213,Ambari server can not start on latest Amazon Linux 2,"On latest Amazon Linux 2 ambari-server cannot start with error message:

    
    
    [root@ip-10-0-222-155 cloudbreak]# ambari-server start
    Using python  /usr/bin/python
    Starting ambari-server
    Traceback (most recent call last):
      File ""/usr/sbin/ambari-server.py"", line 37, in <module>
        from ambari_server.dbConfiguration import DATABASE_NAMES, LINUX_DBMS_KEYS_LIST
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 30, in <module>
        from ambari_server.serverConfiguration import decrypt_password_for_alias, get_ambari_properties, get_is_secure, \
      File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 46, in <module>
        OS_VERSION = OSCheck().get_os_major_version()
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 322, in get_os_major_version
        return OSCheck.get_os_version().split('.')[0]
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 301, in get_os_version
        return OSCheck.get_alias(OSCheck._get_os_type(), OSCheck._get_os_version())[1]
      File ""/usr/lib/ambari-server/lib/ambari_commons/os_check.py"", line 313, in _get_os_version
        raise Exception(""Cannot detect os version. Exiting..."")
    Exception: Cannot detect os version. Exiting...
    

Additional infos from the machine:

    
    
    [root@ip-10-0-222-155 cloudbreak]# cat /etc/system-release
    Amazon Linux 2
    [root@ip-10-0-222-155 cloudbreak]# cat /etc/*release
    NAME=""Amazon Linux""
    VERSION=""2""
    ID=""amzn""
    ID_LIKE=""centos rhel fedora""
    VERSION_ID=""2""
    PRETTY_NAME=""Amazon Linux 2""
    ANSI_COLOR=""0;33""
    CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2""
    HOME_URL=""https://amazonlinux.com/""
    Amazon Linux 2
    [root@ip-10-0-222-155 cloudbreak]#
    

",pull-request-available,[],AMBARI,Bug,Major,2018-08-10 08:17:46,0
13178076,"ambari.ldap.advanced.group_mapping_rules does not work, LDAP sync does not add admin roles for configured group(s)","If I configure ambari.ldap.advanced.group_mapping_rules, it does not add ambari admin roles to the configured role. I did some debug and the AmbariLdapConfiguration is uninitialized in Users.java class, so the adminGroupMappings value is the default ""Ambari Administrators"" not the one I configured before. 

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-09 17:40:42,7
13178050,No subject alternative DNS name exception encountered when Enabling Kerberos against an Active Directory even when SSL verification is off,"No subject alternative DNS name exception encountered when Enabling Kerberos against an Active Directory even when SSL verification is off.

{noformat}
2018-08-09 14:48:28,275  WARN [ambari-client-thread-35] ADKerberosOperationHandler:471 - Failed to communicate with the Active Directory at ldaps://adserver.example.com:636: adserver.example.com:636
javax.naming.CommunicationException: adserver.example.com:636 [Root exception is javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching adserver.example.com found.]
        at com.sun.jndi.ldap.Connection.<init>(Connection.java:238)
        at com.sun.jndi.ldap.LdapClient.<init>(LdapClient.java:137)
...
Caused by: javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative DNS name matching adserver.example.com found.
        at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
        at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1964)
...
Caused by: java.security.cert.CertificateException: No subject alternative DNS name matching adserver.example.com found.
        at sun.security.util.HostnameChecker.matchDNS(HostnameChecker.java:214)
        at sun.security.util.HostnameChecker.match(HostnameChecker.java:96)
        at sun.security.ssl.X509TrustManagerImpl.checkIdentity(X509TrustManagerImpl.java:459)
        at sun.security.ssl.AbstractTrustManagerWrapper.checkAdditionalTrust(SSLContextImpl.java:1026)
        at sun.security.ssl.AbstractTrustManagerWrapper.checkServerTrusted(SSLContextImpl.java:993)
        at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1596)
{noformat}

Note: This occurs when the hostname embedded in the SSL certificate does not match the hostname of the Active Directory host and Open JDK 1.8.181-b13 is used.  This is not seen when Oracle JDK is used. 

{noformat:title=Observed with this version of JDK}
openjdk version ""1.8.0_181""
OpenJDK Runtime Environment (build 1.8.0_181-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
{noformat}


{noformat:title=Not observed with this version of JDK}
java version ""1.8.0_112""
Java(TM) SE Runtime Environment (build 1.8.0_112-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)
{noformat}

*Solution*
The {{org.apache.ambari.server.security.InternalSSLSocketFactory.LenientTrustManager}} class needs to extend {{javax.net.ssl.X509ExtendedTrustManager}} and do nothing in the additional overridden methods. 


",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-08-09 16:31:17,7
13177899,Cannot deploy HBase without HDFS,"HBase can store data on filesystems other than HDFS, but Ambari cannot deploy it without HDFS.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-09 08:31:48,5
13177749,Remove dependencies with CVE issues from Ambari Agent,"Remove dependencies with CVE issues from Ambari Agent

* org.apache.commons:commons-collections4:jar before version 4.0-alpha1-RC1
** CVE-2015-6420 - https://nvd.nist.gov/vuln/detail/CVE-2015-6420

{noformat}
[INFO] org.apache.ambari.contrib.views:ambari-views-utils:jar:2.0.0.0-SNAPSHOT
[INFO] \- org.apache.commons:commons-collections4:jar:4.0:compile
{noformat}
",cleanup pull-request-available,['ambari-agent'],AMBARI,Task,Critical,2018-08-08 17:51:17,7
13177496,Component Versions Are Not Reported On Initial Status Commands Anymore,"In Ambari 2.6, some status commands were able to supply the version of the
component they were running for. This was needed especially during upgrades to
correct situations where components would fail to start but the processes were
actually running and reporting.

  * During heartbeat registration, Ambari asked the agent to get the version in the next status command:  
<https://github.com/apache/ambari/blob/branch-2.6/ambari-
server/src/main/java/org/apache/ambari/server/agent/HeartBeatHandler.java#L416-L419>

  * The status command would place the version information from the structured out into the ""extra"" mapping, which would then be handled here via an event:  
<https://github.com/apache/ambari/blob/trunk/ambari-
server/src/main/java/org/apache/ambari/server/agent/HeartbeatProcessor.java#L599-L603>

We need to replace this functionality so that component versions are reported
on registration of agents.

",pull-request-available,[],AMBARI,Bug,Major,2018-08-07 19:27:21,0
13177485,Remove dependencies with CVE issues from Ambari Server,"Remove dependencies with CVE issues from Ambari Server

* org.springframework:spring-beans:jar before 4.3.17.RELEASE 
** CVE-2018-1270 - https://nvd.nist.gov/vuln/detail/CVE-2018-1270
** CVE-2018-1275 - https://nvd.nist.gov/vuln/detail/CVE-2018-1275
** CVE-2018-1199 - https://nvd.nist.gov/vuln/detail/CVE-2018-1199
** CVE-2018-1271 - https://nvd.nist.gov/vuln/detail/CVE-2018-1271
** CVE-2018-1257 - https://nvd.nist.gov/vuln/detail/CVE-2018-1257
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.springframework.security:spring-security-core:jar:4.2.4.RELEASE:compile
[INFO]    \- org.springframework:spring-beans:jar:4.3.12.RELEASE:compile
{noformat}

* org.kohsuke:libpam4j:jar before version 1.9
** CVE-2017-12197 - https://nvd.nist.gov/vuln/detail/CVE-2017-12197
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.kohsuke:libpam4j:jar:1.8:compile
{noformat}

* org.springframework:spring-context before version 4.3.17.RELEASE
** CVE-2018-1257 - https://nvd.nist.gov/vuln/detail/CVE-2018-1257
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.springframework:spring-context:jar:4.3.16.RELEASE:compile
{noformat}

* org.springframework.security:spring-security-ldap:jar before version 4.1.5.RELEASE 
** CVE-2018-1199 - https://nvd.nist.gov/vuln/detail/CVE-2018-1199
** CVE-2016-9879 - https://nvd.nist.gov/vuln/detail/CVE-2016-9879
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- org.springframework.security:spring-security-ldap:jar:4.1.1.RELEASE:compile
{noformat}

* com.jcraft:jsch:jar before version 1.54 
** CVE-2016-5725 - https://nvd.nist.gov/vuln/detail/CVE-2016-5725
{noformat}
[INFO] org.apache.ambari:ambari-server:jar:2.7.0.0.0
[INFO] \- com.jcraft:jsch:jar:0.1.45:compile
{noformat}
",cleanup pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-08-07 18:38:23,7
13177146,Update org.eclipse.jetty version to 9.4.11.v20180605 to avoid CVE issues,"Update org.eclipse.jetty version to 9.4.11.v20180605 to avoid CVE issues.

See https://dev.eclipse.org/mhonarc/lists/jetty-announce/msg00123.html for reported issues. ",cleanup pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-06 15:53:03,7
13176727,Fix validate single sign-in support information to look for ssoEnabledTest,"Fix validate single sign-in support information to look for {{ssoEnabledTest}}.

The current test looks for the old {{enabledConfiguration}} value only. However {{ssoEnabledTest}} is the new way, leaving {{enabledConfiguration}} for backward compatibility. ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-08-03 14:47:18,7
13176252,Components start failing with 'Holder DFSClient_NONMAPREDUCE does not have any open files' while adding Namespace ,"STR: 
Add a namespace from UI. In the last step restart required services, hiveserver2 restart fails. Although on retrying it comes back up

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.status(env)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 79, in status
 check_process_status(status_params.hive_pid)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/check_process_status.py"", line 43, in check_process_status
 raise ComponentIsNotRunning()
ComponentIsNotRunning

The above exception was the cause of the following exception:

Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 137, in <module>
 HiveServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 993, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 50, in start
 self.configure(env) # FOR SECURITY
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 45, in configure
 hive(name='hiveserver2')
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive.py"", line 119, in hive
 setup_hiveserver2()
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive.py"", line 167, in setup_hiveserver2
 skip=params.sysprep_skip_copy_tarballs_hdfs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 516, in copy_to_hdfs
 replace_existing_files=replace_existing_files,
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 654, in action_create_on_execute
 self.action_delayed(""create"")
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 651, in action_delayed
 self.get_hdfs_resource_executor().action_delayed(action_name, self)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 354, in action_delayed
 self.action_delayed_for_nameservice(nameservice, action_name, main_resource)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 380, in action_delayed_for_nameservice
 self._create_resource()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 396, in _create_resource
 self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 511, in _create_file
 self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 199, in run_command
 return self._run_command(*args, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 272, in _run_command
 raise WebHDFSCallException(err_msg, result_dict)
resource_management.libraries.providers.hdfs_resource.WebHDFSCallException: Execution of 'curl -sS -L -w '%\{http_code}' -X PUT --data-binary @/usr/hdp/3.0.1.0-30/hive/hive.tar.gz -H 'Content-Type: application/octet-stream' --negotiate -u : -k 'https://<HOST-FQDN>:50470/webhdfs/v1/hdp/apps/3.0.1.0-30/hive/hive.tar.gz?op=CREATE&overwrite=True&permission=444'' returned status_code=404. 
{
 ""RemoteException"": {
 ""exception"": ""FileNotFoundException"", 
 ""javaClassName"": ""java.io.FileNotFoundException"", 
 ""message"": ""File does not exist: /hdp/apps/3.0.1.0-30/hive/hive.tar.gz (inode 16450) Holder DFSClient_NONMAPREDUCE_-1764810327_120 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2800)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:597)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:172)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2679)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:875)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:561)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)\n""
 }
}
{code}

 ",pull-request-available,[],AMBARI,Bug,Major,2018-08-01 22:03:23,0
13175854,Filter services eligible for Ambari Single Sign-on Configuration if Kerberos is required but not enabled,"Filter services from Ambari CLI when setting up SSO if not eligible when Kerberos is not enabled.  

In Ambari 2.7, services that are eligible for Ambari to manage their SSO configurations specify this in their metainfo file using like:

{code}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>application-properties/atlas.sso.knox.enabled</enabledConfiguration>
      </sso>
{code}

See AMBARI-23253
See [Ambari Single Sign-on Configuration|https://github.com/apache/ambari/blob/branch-2.7/ambari-server/docs/security/sso/index.md] documentation

However some services require Kerberos to be enabled for SSO to work.  For example, HDFS, Yarn, and Oozie.  For this case, the metadata is enhanced allowing for the metadata to indicate whether Kerberos is required (AMBARI-24335) and whether Kerberos is enabled (AMBARI-24384) for that service.

This information can be found in the service resource data

{code:title=GET /api/v1/clusters/CLUSTERNAME/services/OOZIE}
{
  ""href"" : ""http://ambari_host:8080/api/v1/clusters/CLUSTERNAME/services/OOZIE"",
  ""ServiceInfo"" : {
    ...
    ""kerberos_enabled"" : true,
    ...
   ""sso_integration_desired"": false,
   ""sso_integration_enabled"": false,
   ""sso_integration_requires_kerberos"": true,
   ""sso_integration_supported"": true,
   ...
   },
   ...
}
{code}

Using this information, services may be included in or excluded from the list of services a user can choose for enabling SSO integration. 

For example
||sso_integration_supported||sso_integration_requires_kerberos||kerberos_enabled||Can Enable SSO||
|true|true|true|yes
|true|true|false|no
|true|false|true|yes
|true|false|false|yes
|false|true|true|no
|false|true|false|no
|false|false|true|no
|false|false|false|no

  ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-31 14:35:43,3
13175846,Update Ambari Single Sign-on Configuration documentation to include Kerberos options,"Update Ambari Single Sign-on Configuration documentation to include Kerberos options.

See https://github.com/apache/ambari/blob/branch-2.7/ambari-server/docs/security/sso/index.md.",documentation pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-31 14:08:21,7
13175691,Unit test error in ambari-metrics-timelineservice: metric_blacklist.dat (No such file or directory),"Many branch-2.7 test runs from Github are failing with 

{noformat}
2018-07-30 21:33:23,838 ERROR [main] timeline.TimelineMetricsFilter (TimelineMetricsFilter.java:readMetricWhitelistFromFile(132)) - Unable to parse metric file
java.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder%402/ambari-metrics/ambari-metrics-timelineservice/target/test-classes/test_data/metric_blacklist.dat (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:93)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilter.readMetricWhitelistFromFile(TimelineMetricsFilter.java:117)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilter.initializeMetricFilter(TimelineMetricsFilter.java:85)
	at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testMetricBlacklisting(TimelineMetricsFilterTest.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:344)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:269)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:240)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:184)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:286)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:240)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
2018-07-30 21:33:23,842 INFO  [main] timeline.TimelineMetricsFilter (TimelineMetricsFilter.java:initializeMetricFilter(86)) - Blacklisting 0 metrics
{noformat}
",unit-test,['ambari-metrics'],AMBARI,Bug,Critical,2018-07-30 22:22:09,5
13175604,Logic and declaration used to determine if Kerberos is enabled for a service,"Add logic and declaration used to determine if Kerberos is enabled for a service.

To support a robust method to determine whether Kerberos is enabled or not, a new attribute should be added - {{kerberosEnabledTest}}.  

The {{kerberosEnabledTest}} attribute is to contain a JSON document that can be _compiled_ into a {{org.apache.commons.collections.Predicate}} (ideally using {{org.apache.ambari.server.collections.PredicateUtils#fromJSON}}).  For example

{code}
<sso>
  <supported>true</supported>
  <kerberosRequired>true</kerberosRequired>
  ...
</sso>
<kerberosEnabledTest>
    {
      ""equals"": [
        ""service-properties/kerberos.enabled"",
        ""true""
      ]
    }
</kerberosEnabledTest>

{code}

{code}
<sso>
  <supported>true</supported>
  <kerberosRequired>true</kerberosRequired>
  ...
</sso>
<kerberosEnabledTest>
    {
      ""or"": [
        {
          ""equals"": [
            ""oozie-site/oozie.authentication.type"",
            ""kerberos""
          ]
        },
        {
          ""equals"": [
            ""oozie-site/oozie.authentication.type"",
            ""org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler""
          ]
        }
      ]
    }
  </kerberosEnabledTest>
{code}
The result of the test, is to be available via the services REST API:

{code:title=GET /api/v1/clusters/CLUSTERNAME/services/OOZIE}
{
  ""href"" : ""http://ambari_host:8080/api/v1/clusters/CLUSTERNAME/services/OOZIE"",
  ""ServiceInfo"" : {
    ...
    ""kerberos_enabled"" : true,
    ...
   },
   ...
}
{code}


",pull-request-available,[],AMBARI,Bug,Major,2018-07-30 16:25:41,7
13175355,Adding services when Kerberos is enabled incorrectly changes unrelated service configurations,"Adding services when Kerberos is enabled incorrectly changes unrelated service configurations.  For example, {{kerberos-env/service_check_principal_name}} is changed from ""{{$\{cluster_name|toLower()\}-$\{short_date\}}}"" to a concrete value like ""{{c1-072818}}"".

This is a regression created with the resolution of [AMBARI-23292|https://issues.apache.org/jira/browse/AMBARI-23292].

",kerberos pull-request-available regresion,['ambari-server'],AMBARI,Bug,Critical,2018-07-28 18:43:05,7
13175233,Check Keytabs Kerberos Client step failing while re-dding a service via UI,"*STR*

install ambari-2.7.0.1-11

deploy a kerberized cluster (i.e. with ZK and Atlas)

remove one of the services (I removed Atlas)

add that service again in via UI

*Result*

The following error occurred at the Check Keytabs Kerberos Client task:
{code:java}
stderr: 
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KERBEROS/package/scripts/kerberos_client.py"", line 91, in 
    KerberosClient().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KERBEROS/package/scripts/kerberos_client.py"", line 87, in check_keytabs
    find_missing_keytabs(params, output_hook)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/kerberos/kerberos_common.py"", line 166, in find_missing_keytabs
    missing_keytabs = MissingKeytabs.from_kerberos_records(params.kerberos_command_params, params.hostname)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/kerberos/kerberos_common.py"", line 55, in from_kerberos_records
    with_missing_keytab = (each for each in kerberos_record \
TypeError: 'NoneType' object is not iterable
 stdout:


Command failed after 1 tries{code}",pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2018-07-27 17:22:29,6
13175179,How alert count is presented for config errors/warnings/suggestions confusing and misleading,See screenshots,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-07-27 14:41:45,4
13174918,Fixes for modal with config validations and dependent properties,"Dependent configs table:
- Checkboxes should be located on the left side
- Properties names hyperlinks are broken; remove the links and add the tooltip with property description on hover instead
- Make modal wider
- Move modal closer to the top

Config validations results table:
- Limit the width and wrap the contents of 'Current Value' column
- Make modal wider",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-26 16:39:48,2
13174876,Make upgrade progress counter more readable,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-26 14:51:22,4
13174809,Back and forth on All Configurations Page has multiple tabs marked as active,"In ambari-2.7.1.0-11
# Navigating to All configurations page in installer, 
# selecting multiple services
# Navigate to Directories section
# Navigate back to All configs page.

Multiple services are marked as active in All Configuration page leading to confusion
 !Screen Shot 2018-07-25 at 11.19.37 PM.png|thumbnail! ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-26 11:42:27,4
13174280,Logic and declaration used to determine if SSO is enabled for a service needs to be able to handle more than a boolean property,"Logic and declaration used to determine if SSO is enabled for a service needs to be able to handle more than a boolean property.

The current way Ambari determines whether SSO is enabled for a service or not is by getting the value of the property indicated in {{sso/enabledConfiguration}} property in the service's metadata:
{code:java|title=Example}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>service-properties/sso.knox.enabled</enabledConfiguration>
      </sso>
{code}
Using the above example, the {{service-properties/sso.knox.enabled}} is checked to see if its value is ""true"" or ""false"".

This method works for a few services, but other services require more elaborate checks. For example Oozie relies on the value of {{oozie-site/oozie.authentication.type}}. If the value is ""org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler"", then SSO is enabled; otherwise it is not. This is different that just a Boolean value.

*Solution*

To support a more robust method to determine whether SSO is enabled or not, a new attribute should be added - {{ssoEnabledTest}}. The existing attribute, {{enabledConfiguration}}, should be available for backward compatibility - but converted on the backend.

The {{ssoEnabledTest}} attribute is to contain a JSON document that can be _compiled_ into a {{org.apache.commons.collections.Predicate}} (ideally using {{org.apache.ambari.server.collections.PredicateUtils#fromJSON}}). For example
{code:java}
      <sso>
        <supported>true</supported>
        <ssoEnabledTest>
          {
            ""equals"": [
              ""service-properties/sso.knox.enabled"",
              ""true""
            ]
          }
      </sso>
{code}
{code:java}
      <sso>
        <supported>true</supported>
        <ssoEnabledTest>
          {
            ""equals"": [
              ""oozie-site/oozie.authentication.type"",
              ""org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler""
            ]
          }
        </ssoEnabledTest>
      </sso>{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-25 06:32:49,6
13174129,Rescheduled and canceled tasks stay in progress forever,"1\. Ambari-server reschedules task (timeout #1)  
2\. Task did yet got rescheduled (due to something else being in queue)  
3\. but, ambari-server cancels it (timeout #2)  
4\. The tasks keeps reprorting that's it's in progess forever, however being
canceled

",pull-request-available,[],AMBARI,Bug,Major,2018-07-24 15:32:25,0
13174103,Host Details page: alignment issues,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-24 13:27:12,4
13173827,Background Operations: minor UX changes,"See screenshots

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-07-23 14:40:55,4
13173777,Update service metainfo to declare Kerberos is required for SSO integration support,"Update service metainfo to declare Kerberos is required for SSO integration support.

The metainfo file allow for a service to indicate that is supports SSO integration.
{code:java|title=Example}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>service-site/knox.sso.enabled</enabledConfiguration>
      </sso>
{code}
However, some services required that Kerberos is enabled to support SSO. This needs to be indicated in the metainfo so that Ambari knows how to behave properly.
{code:java|title=Example}
      <sso>
        <supported>true</supported>
        <enabledConfiguration>service-site/knox.sso.enabled</enabledConfiguration>
        <kerberosRequired>true</kerberosRequired>
      </sso>
{code}
Along with this change, the following API request needs to be updated to supply the relevant data:
{noformat:title=Get stack service details}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME"",
{
  ""href"" : "":URL"",
  ""StackServices"" : {
     ...
     ""sso_integration_supported"": ""true"",
     ""sso_integration_requires_kerberos"": ""true"",
     ...
  },
  ...
{noformat}
{noformat:title=Get installed service information}
GET /api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME
{
  ""href"" : "":URL"",
  ""ServiceInfo"" : {
    ""cluster_name"" : "":CLUSTER_NAME"",
    ...
    ""sso_integration_supported"": ""true"",
    ""sso_integration_requires_kerberos"": ""true"",
    ""sso_integration_enabled"": ""false"",
    ""sso_integration_desired"": ""false"",
     ...
    },
    ...
{noformat}
{noformat:title=List installed services that support SSO integration only when Kerberos to be enabled}
GET /api/v1/clusters/:CLUSTER_NAME/services?ServiceInfo/sso_integration_supported=true&ServiceInfo/sso_integration_requires_kerberos=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}
{noformat:title=List stack services that support SSO integration only when Kerberos is enabed}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services?StackServices/sso_integration_supported=true&StackServices/sso_integration_requires_kerberos=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-23 11:23:46,6
13173774,User is able to set the same short URLs for different view instances,"*STR*
 # Let's say the user has CAPACITY-SCHEDULER view instance with {{/main/view/CAPACITY-SCHEDULER/auto_cs_instance}} short URL set
 # Clone this instance or create another CAPACITY-SCHEDULER instance manually
 # Set the same short URL as for the first instance

*Result*
 - If 'Name' property is the same, request for adding new URL fails
request:
{noformat}
POST api/v1/view/urls/auto_cs_instance

{
    ""ViewUrlInfo"": {
        ""url_name"": ""auto_cs_instance"",
        ""url_suffix"": ""auto_cs_instance"",
        ""view_instance_version"": ""1.0.0"",
        ""view_instance_name"": ""AUTO_CS_INSTANCE_Copy"",
        ""view_instance_common_name"": ""CAPACITY-SCHEDULER""
    }
}
{noformat}
response:
{noformat}
{
  ""status"" : 500,
  ""message"" : ""An internal system exception occurred: This view URL name exists, URL names should be unique""
}
{noformat}

 - If name is different, the REST call succeeds even if URL itself is duplicate:
{noformat}
POST api/v1/view/urls/auto_cs_instance1

{
    ""ViewUrlInfo"": {
        ""url_name"": ""auto_cs_instance1"",
        ""url_suffix"": ""auto_cs_instance"",
        ""view_instance_version"": ""1.0.0"",
        ""view_instance_name"": ""AUTO_CS_INSTANCE_Copy"",
        ""view_instance_common_name"": ""CAPACITY-SCHEDULER""
    }
}
{noformat}
response is 201 in this case; URL is created

 - As result there are two view instances with the same URL, and the first one is no longer accessible by this URL
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-23 11:14:24,6
13173437,install tasks sometimes fail,"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ZOOKEEPER/package/scripts/zookeeper_client.py"", line 81, in <module>
        ZookeeperClient().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ZOOKEEPER/package/scripts/zookeeper_client.py"", line 61, in install
        self.install_packages(env)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 836, in install_packages
        retry_count=agent_stack_retry_count)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 30, in action_install
        self._pkg_manager.install_package(package_name, self.__create_context())
      File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/yum_manager.py"", line 219, in install_package
        shell.repository_manager_executor(cmd, self.properties, context)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 742, in repository_manager_executor
        call_result = subprocess_executor(cmd, timeout=-1, env=env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 446, in subprocess_executor
        lines = [line for line in output]
      File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
        self.gen.next()
      File ""/usr/lib/ambari-agent/lib/ambari_commons/shell.py"", line 528, in process_executor
        kill_timer.join()
      File ""/usr/lib64/python2.7/threading.py"", line 940, in join
        raise RuntimeError(""cannot join thread before it is started"")
    RuntimeError: cannot join thread before it is started
    

",pull-request-available,[],AMBARI,Bug,Major,2018-07-20 12:02:03,0
13173412,Fix background colors on pages,"The title should have a white background on Stack and Versions -> Stack page and Service Accounts page.

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-20 09:30:49,4
13173398,SET_KEYTABS is preformed twice per host during Kerberos operations,"*STR*
* Enable Kerberos and check the DB after the 'Test Kerberos' step, or
* Regenerate keytabs on a secure cluster with the option 'Only regenerate keytabs for missing hosts and components' enabled (this is important, otherwise the issue is not reproducible)

*Result:*

{noformat}
SELECT hrc.task_id, hrc.request_id, hrc.host_id, hrc.stage_id, st.request_context, hrc.status, hrc.role, hrc.role_command, hrc.custom_command_name FROM host_role_command hrc, stage st WHERE hrc.stage_id = st.stage_id AND hrc.request_id = st.request_id AND hrc.custom_command_name = 'SET_KEYTAB' ORDER BY request_id, host_id, stage_id;{noformat}

||task_id||request_id||host_id||stage_id||request_context||status||role||role_command||custom_command_name||
|63|17|1|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|68|17|1|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|64|17|2|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|69|17|2|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|65|17|3|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|70|17|3|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|83|19|1|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|88|19|1|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|84|19|2|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|89|19|2|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|85|19|3|1|Checking keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|
|90|19|3|4|Distribute Keytabs|COMPLETED|KERBEROS_CLIENT|CUSTOM_COMMAND|SET_KEYTAB|

*Expected result:*
only one SET_KEYTAB command should be recorded",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-20 08:28:40,6
13173394,"Multiple alerts after HDFS service only regenerate keytabs, as keytabs are out of sync","*STR*
 # Install Ambari 2.7.0
 # Deploy a cluster with some services
 # Execute to HDFS / Actions / Regenerate Keytabs
 # Restart all required components

*Result*

Multiple alerts have been shown saying they receive 403 when trying to connect to different web UIs

*Expected result*

No alerts; everything should work as expected",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-20 08:22:07,7
13173220,Inconsistent Ambari warnings,"STR:

#  Go to Hive/Configs
#  Enable Interactive Query, select the host, Save
#  6 Warnings are coming up, though the REST API call returned only 2, the other 4 are produced by Ambari Web
#  Click on cancel, discard the changes.
#  Go to the Summary, tab, then back to the Configs tab, and do steps #1-#3 again
#  This time only the two warnings are displayed, Ambari Web doesn't generates those extra 4 (desired outcome)
#  Log out and log in again, and the same starts again, for the first modification you'll receive 4 extra warnings.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 15:28:19,4
13173204,UI Elements in Incorrect layout at Customize Services Page,"- Please see screenshots attached
- After a config group is created, message 'You are changing not default config group,' is towards the right side with no formatting
- Switch to Misc tab, dfs.permissions.superusergroup has a longer textbox
- Undo icon in Misc tab is below the textbox for some elements where as it is to the right for others 
- closing bracket for Services should be shrinked if there are less number of services

Screenshot attached for all 4",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 14:15:49,4
13173194,"Service disappear from the UI when Going back to ""Customize Services"" page or doing page refresh","Here is the STR:
- install isilon mpack
- go through the cluster creation wizard and select onefs service
- on the configuration page check that there are onefs related properties (for example smart connect zone name)
- either hit f5 or go to the next page then go back
- on the configuration page there will be no onefs related properties any more
- if you continue the deployment this way it will fail at the end
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-19 13:49:49,4
13173189,Host and Alerts page style fixes,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-19 13:33:27,4
13173183,UI sends host check request for custom jdk with invalid hosts,"UI sends ""java_home_check"" host check with all populated hosts instead hosts with passed registration only. This can cause 500 server exception due HostNotFoundException.
{code}
2018-07-05 14:10:30,930 ERROR [ambari-client-thread-573] AbstractResourceProvider:295 - Caught AmbariException when creating a resource
org.apache.ambari.server.HostNotFoundException: Host not found, hostname=test-0000
        at org.apache.ambari.server.state.cluster.ClustersImpl.getHost(ClustersImpl.java:456)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:189)
        at org.apache.ambari.server.state.ConfigHelper.getEffectiveDesiredTags(ConfigHelper.java:173)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.findConfigurationTagsWithOverrides(AmbariManagementControllerImpl.java:2353)
{code}
{code}
POST http://host:8080/api/v1/requests 500 (Internal Server Error)
send @ vendor.js:8630
jQuery.extend.ajax @ vendor.js:8082
send @ app.js:192735
doCheckJDK @ app.js:41241
(anonymous function) @ app.js:41318
fire @ vendor.js:1141
self.fireWith @ vendor.js:1252
done @ vendor.js:8178
callback @ vendor.js:8702
{code}
Body of request:
{code}
{""RequestInfo"":{""context"":""Check hosts"",""action"":""check_host"",""parameters"":{""threshold"":""60"",""java_home"":""/tmp/jdk1.8.0_171/"",""jdk_location"":""http://host:8080/resources"",""check_execute_list"":""java_home_check""}},""Requests/resource_filters"":[{""hosts"":""test-0000,test-0001,test-0002,...
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 12:51:39,2
13173170,'Changed properties' filter for configs is displayed even in non-compare mode,"Filtering configs by changed properties should be available in versions comparison mode only, but as of now it's displayed always, not filtering out anything in regular mode.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-19 12:00:07,2
13172365,Start All Services on 100-nodes cluster timed out after 1 hour,"ambari-server with 16 core CPU, 32GB RAM, fake agents on 2 hosts (16GB RAM, 2 core CPU) - 50 agents on each host, Mysql db on last host
 # Deploy a PERF cluster with 100 fake agents
 # Initiate Stop All components
 # Initiate Start All components

Observations:
 # As part of initial deploy and start services, the Start All Services call took about 6 minutes.
 # The first call to Stop All Services took about 23 minutes.
 # The subsequent call to Start All services timed out after 1 hour 1 minute.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-07-16 10:51:45,1
13172063,Remove org.apache.directory.api:api-ldap-model from Ambari server's dependencies due to security concerns,"Remove {{org.apache.directory.api:api-ldap-model}} from Ambari server's dependencies due to security concerns regarding the following CVE:

* CVE-2018-1337: Plaintext Password Disclosure in Secured Channel

See https://cve.mitre.org/cgi-bin/cvename.cgi?name=2018-1337

Though Ambari server includes {{api-ldap-model-1.0.0.jar}} in {{/usr/lib/ambari-server}}, the library is not used.  Therefore, the vulnerability is not exposed and the library may be excluded from Ambari's package. 
",cleanup pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-13 15:19:20,7
13171941,Enabling Hive Server Interactive doesn't work with ONEFS,"The following popup prevents enabling HSI:

HDFS service should be added to the cluster to enable interactive query (requires yarn pre-emption)

The check is located in assign_master_controller.js/showPopup.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-13 08:35:24,3
13171459,Fix ambari-admin UI unit tests,"As of now UTs are failing:
{noformat}
#Editablelist directive Editing Updates permissions after save FAILED

#Editablelist directive Editing Show dialog window if user trying to leave page without save FAILED

#Editablelist directive Editing Saves current user in editing window if user click ""save"" FAILED

Executed 61 of 61 (3 FAILED) (0.387 secs / 0.378 secs)
{noformat}",pull-request-available,['ambari-admin'],AMBARI,Bug,Critical,2018-07-11 16:29:52,2
13171123,hadoop-env is not regenerated when OneFS is used as a FileSystem,"The before-ANY/shared_initialization.py only regenerates hadop_env if there is a namenode or dfs_type is set to HCFS

{code}
  def hook(self, env):
    import params
    env.set_params(params)

    setup_users()
    if params.has_namenode or params.dfs_type == 'HCFS':
      setup_hadoop_env()
    setup_java()
{code}

This is no longer true because in the latest ambari-server we set dfs_type as follows:

{code}
    Map<String, ServiceInfo> serviceInfos = ambariMetaInfo.getServices(stackId.getStackName(), stackId.getStackVersion());
    for (ServiceInfo serviceInfoInstance : serviceInfos.values()) {
      if (serviceInfoInstance.getServiceType() != null) {
        LOG.debug(""Adding {} to command parameters for {}"", serviceInfoInstance.getServiceType(),
            serviceInfoInstance.getName());

        clusterLevelParams.put(DFS_TYPE, serviceInfoInstance.getServiceType());
        break;
      }
    }
{code}

This iterates over all of the stack service which will find HDFS first, so that the dfs_type will be HDFS instead of HCFS.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-10 12:04:47,3
13171090,OneFS service check fails with 'dfs_type' is not set.,"Following error comes when running a OneFS service check.

{code}
resource_management.core.exceptions.Fail: Resource parameter 'dfs_type' is not set.


stderr:
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ONEFS/package/scripts/service_check.py"", line 59, in <module>
    HdfsServiceCheck().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ONEFS/package/scripts/service_check.py"", line 43, in service_check
    mode=0777
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 119, in run_action
    provider = provider_class(resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 594, in __init__
    self.assert_parameter_is_set('dfs_type')
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 673, in assert_parameter_is_set
    raise Fail(""Resource parameter '{0}' is not set."".format(parameter_name))
resource_management.core.exceptions.Fail: Resource parameter 'dfs_type' is not set.
stdout:
2018-07-09 17:29:05,706 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1485/hadoop/conf
2018-07-09 17:29:05,715 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1485/hadoop/conf
2018-07-09 17:29:05,717 - checked_call['hostid'] {}
2018-07-09 17:29:05,721 - checked_call returned (0, 'f20a5572')
2018-07-09 17:29:05,722 - HdfsResource['/tmp'] {'security_enabled': False, 'hadoop_bin_dir': '/usr/hdp/3.0.0.0-1485/hadoop/bin', 'keytab': [EMPTY], 'dfs_type': '', 'default_fs': 'hdfs://ah-onefs-hdp3.west.isilon.com:8020', 'hdfs_resource_ignore_file': '/var/lib/ambari-agent/data/.hdfs_resource_ignore', 'hdfs_site': ..., 'kinit_path_local': 'kinit', 'principal_name': None, 'user': 'hdfs', 'action': ['create_on_execute'], 'hadoop_conf_dir': '/usr/hdp/3.0.0.0-1485/hadoop/conf', 'type': 'directory', 'immutable_paths': [u'/apps/hive/warehouse', u'/mr-history/done', u'/app-logs', u'/tmp'], 'mode': 0777}

Command failed after 1 tries
{code}


In params_linux.py, dfs_type should come from clusterLevelParams instead of commandParams.
 
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-10 08:29:40,3
13170872,All input fields are disabled after validation - cancel - filter for something on advanced tab,"STR: (see attached video)

1. try to do some property modifications, and save it
2. when some warnings appear, click cancel
3. filter for a variable that is not on the Advanced tab
4. everything is disabled on all tabs, even after discarding the modifications",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-07-09 14:28:13,4
13170827,Ambari Alerts are not triggered,"Even when Namenodes, Datanodes/ entire services are down, there are no alerts in the Ambari UI.

Most of the summary information in HDFS, YARN show as n/a. There are no errors in the JS console too.
",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-09 10:52:42,3
13170730,Suppress log messages from the credential_store_helper,Suppress log messages from the credential_store_helper since they are not necessary.,pull-request-available,[],AMBARI,Bug,Major,2018-07-08 16:04:18,7
13170706,"Restart services just before stack upgrade fails due to AMS package incompatibility errors, causing EU not to be started","*STR*
 # Deploy HDP-2.6.5 cluster with Ambari-2.6.2 (Atlas is installed on the cluster)
 # Upgrade Ambari server to 2.7
 # Upgrade AMS, Smartsense
 # Register HDP-3.0 version and install the packages
 # Try to start Express Upgrade. One of the pre-check fails: ""The property atlas.migration.data.filename is not found in application-properties, need to add the property before upgrade.""
 # Add the atlas property and save the config
 # Now Atlas and Hive are marked as stale config services and need restart
 # Restart Hive, it fails with compatibility error
 # Try to start Upgrade - if fails with below error:

Reason: The following Service Components should be in a started state.  Please invoke a service Stop and full Start and try again. HIVE: HIVE_SERVER (in INSTALLED on host dk-upgradetest-2.openstacklocal), HIVE: HIVE_METASTORE (in INSTALLED on host dk-upgradetest-2.openstacklocal)
Failed on: HIVE

As a result, Upgrade cannot be started",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-08 13:23:50,1
13170579,Ambari shows success when HBase Decommission/Recommission operations fail,"If HBase Decommission/Recommission operation fails with non zero exit code, that component still gets decommissioned/recommissioned. We need to handle this failure and not transition the state to Decommissioned/Recommissioned when the respective operation fails",pull-request-available,[],AMBARI,Bug,Major,2018-07-06 20:46:34,2
13170571,ambari-server sync-ldap with users and groups fails with NoSuchAlgorithmException,"Setup secure ldap went through fine but later on sync-ldap with users and groups has failed with below error
{code:java}
[root@ctr-e138-1518143905142-391100-01-000006 ~]# ambari-server sync-ldap --users /tmp/users.txt --groups /tmp/groups.txt
Using python  /usr/bin/python
Syncing with LDAP...
Enter Ambari Admin login: admin
Enter Ambari Admin password: 

Fetching LDAP configuration from DB.
Syncing specified users and groups..ERROR: Exiting with exit code 1. 
REASON: Caught exception running LDAP sync. ad-nano.qe.hortonworks.com:636; nested exception is javax.naming.CommunicationException: ad-nano.qe.hortonworks.com:636 [Root exception is java.net.SocketException: java.security.NoSuchAlgorithmException: Error constructing implementation (algorithm: Default, provider: SunJSSE, class: sun.security.ssl.SSLContextImpl$DefaultSSLContext)]
[root@ctr-e138-1518143905142-391100-01-000006 ~]# cat /tmp/users.txt {code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-07-06 20:30:15,6
13170502,Tasks fail from time to time due error in file download,"
    ERROR 2018-06-27 11:29:49,955 CustomServiceOrchestrator.py:448 - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: u'/var/lib/ambari-agent/cache/stack-hooks'; u'/var/lib/ambari-agent/cache/stack-hooks'
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 351, in runCommand
        hook_dir = self.file_cache.get_hook_base_dir(command, server_url_prefix)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/FileCache.py"", line 100, in get_hook_base_dir
        server_url_prefix)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/FileCache.py"", line 163, in provide_directory
        wait_for_another_execution_event = self.currently_providing[full_path]
    KeyError: u'/var/lib/ambari-agent/cache/stack-hooks'
    

",pull-request-available,[],AMBARI,Bug,Major,2018-07-06 14:41:10,0
13170460,"Can't register HDP 3.0.0.0 version and ""Use Local Repository""","When trying to register a VDF for HDP-3.0.0 and click on ""Use Local Repository"" the ""Save"" button is inactive and you can't register the version.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-07-06 10:26:30,4
13170294,Create Checkpoint page stuck while Enabling HA on Namenode,"Ambari UI stuck at Manual Steps Required: Create Checkpoint on NameNode page while Enabling HA after performing the manual steps.

{code}
[root@ctr-e138-1518143905142-378399-01-000004 ~]# sudo su cstm-hdfs -l -c 'hdfs dfsadmin -safemode enter'
######## Hortonworks #############
This is MOTD message, added for testing in qe infra
Safe mode is ON
[root@ctr-e138-1518143905142-378399-01-000004 ~]# sudo su cstm-hdfs -l -c 'hdfs dfsadmin -saveNamespace'
######## Hortonworks #############
This is MOTD message, added for testing in qe infra
Save namespace successful
{code}


The stat result of file /etc/security/keytabs/ambari.server.keytab, looks like the final change in the file occured at 2018-07-04 05:53:52.

{code:java}
[root@ctr-e138-1518143905142-395357-01-000002 ~]# stat /etc/security/keytabs/ambari.server.keytab
  File: ‘/etc/security/keytabs/ambari.server.keytab’
  Size: 333       	Blocks: 8          IO Block: 4096   regular file
Device: fd12h/64786d	Inode: 16390172    Links: 1
Access: (0400/-r--------)  Uid: ( 1803/agentslava)   Gid: ( 1803/agentslava)
Access: 2018-07-04 03:01:39.282093801 +0000
Modify: 2018-07-04 03:01:39.282093801 +0000
Change: 2018-07-04 05:53:52.333709142 +0000
 Birth: -
{code}

From the ambari server logs at the same time stamp:

{code:java}
2018-07-04 05:53:52,054  INFO [Server Action Executor Worker 586] KerberosServerAction:411 - Processing identities...
2018-07-04 05:53:52,322  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:111 - Updated the owner of the keytab file at /etc/security/keytabs/ambari.server.keytab to null
2018-07-04 05:53:52,323  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:125 - Updated the group of the keytab file at /etc/security/keytabs/ambari.server.keytab to null
2018-07-04 05:53:52,337  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:142 - Updated the access mode of the keytab file at /etc/security/keytabs/ambari.server.keytab to owner:'r' and group:'null'
2018-07-04 05:53:52,352  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:111 - Updated the owner of the keytab file at /etc/security/keytabs/ams-monitor.keytab to cstm-ams
2018-07-04 05:53:52,371  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:125 - Updated the group of the keytab file at /etc/security/keytabs/ams-monitor.keytab to hadoop
2018-07-04 05:53:52,387  INFO [Server Action Executor Worker 586] FinalizeKerberosServerAction:142 - Updated the access mode of the keytab file at /etc/security/keytabs/ams-monitor.keytab to owner:'r' and group:''
{code}

",pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2018-07-05 15:15:32,7
13170247,Kafka failed to stop,"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka_broker.py"", line 145, in <module>
        KafkaBroker().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka_broker.py"", line 99, in stop
        ensure_base_directories()
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/KAFKA/package/scripts/kafka.py"", line 266, in ensure_base_directories
        recursive_ownership = True,
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 125, in __new__
        cls(names_list.pop(0), env, provider, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 199, in action_create
        recursion_follow_links=self.resource.recursion_follow_links, safemode_folders=self.resource.safemode_folders)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 73, in _ensure_metadata
        sudo.chown_recursive(path, _user_entity, _group_entity, recursion_follow_links)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/sudo.py"", line 55, in chown_recursive
        os.lchown(os.path.join(root, name), uid, gid)
    OSError: [Errno 2] No such file or directory: '/grid/0/log/kafka/controller.log'
    

",pull-request-available,[],AMBARI,Bug,Major,2018-07-05 11:35:08,0
13170160,Fix FindBugs warnings,"{noformat}
[INFO] --- findbugs-maven-plugin:3.0.3:check (default) @ ambari-server ---
...
[INFO] Total bugs: 1700
{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-07-05 04:24:15,5
13170049,data-qa attribute present not for all properties,"data-qa attribute present not for all properties

Example with the attribute:
{code}
<input id=""ember38283"" class=""ember-view ember-text-field form-control col-md-9 long-input"" data-qa=""manage.include.files"" type=""text"" value=""false"" data-original-title="""" title="""">
{code}

Example without attribute:
{code}
<input id=""ember39059"" class=""ember-view ember-text-field form-control"" placeholder=""Type password"" type=""password"" value="""">
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-04 11:08:32,4
13170029,Host details page: remove duplicate title,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-04 09:28:23,4
13170025,Admin View: Dashboard link leads to login page,"Steps to reproduce:

* Go to Admin View
* Click Dashboard link

Actual result:
Ambari web firstly show login page and then redirects to the Dashboard page.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-07-04 09:22:00,4
13169577,"Ambari UI ""NEXT"" button is disabled for add service step of Spectrum Scale","Hi ,
     I was trying to install/add service the Spectrum Scale service to Ambari UI on IBM Power machine , in order to achieve that I followed below steps :

1. Stop all services 
2. Run Spectrum Scale installer from command line at Ambari host
2.At Ambari UI:  Click on Services --> Add Service
3. Selected the service ""Spectrum Scale"", then clicked on ""NEXT"" button
4. At next page , there will be drop down box for selecting the host for ""GPFS Master"" role, 

The issue comes at step 4 after selecting host for ""GPFS Master"" , ""NEXT"" button is not enabled and doesn't gets enabled even after variating the options too from drop down box.

Relevant screen shots are attached along with this bug.


Details for version are as below :

Ambari - 2.7.0.0-765",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-07-02 17:47:20,4
13169572,Alerts Are Running For Components Which Are Not Installed,"I recently noticed on a cluster that there were alerts being received and discarded for JournalNode on a cluster which had no JournalNodes installed. I did a quick grep of my own logs, and found a massive amount of alerts that are running for components which are:

* Not installed on those hosts
* Not installed anywhere in the cluster

{code}
2018-06-26 19:03:31,054  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:03:31,054  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:03:31,997  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:04:25,730  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:04:27,011  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:04:30,731  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:04:31,069  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:05:30,765  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:05:30,767  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:05:31,090  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:05:31,091  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:05:32,036  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:06:25,775  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:06:27,047  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:06:30,779  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:06:31,104  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:07:30,815  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:30,816  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_rpc_latency for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:30,821  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:30,821  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_cpu for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:07:31,136  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:07:31,136  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:07:32,064  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:08:25,835  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:08:27,092  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:08:30,835  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:08:31,150  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:09:25,862  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:09:25,863  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:09:31,174  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:09:31,174  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:09:32,097  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:10:25,883  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:10:25,883  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:10:27,115  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:10:31,199  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:11:25,897  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:11:25,897  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:11:27,136  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:11:31,229  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:11:31,229  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:16:27,291  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:16:30,970  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:30,971  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_rpc_latency for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:30,974  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:30,992  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_cpu for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:16:31,310  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:16:31,310  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:17:25,985  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:30,987  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_rpc_latency for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:30,990  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:30,990  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_cpu for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:17:31,324  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:17:31,324  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:17:32,274  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:18:26,002  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:18:26,004  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:18:27,294  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:18:31,332  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:19:26,020  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:19:26,021  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:19:27,327  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:19:31,354  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert namenode_rpc_latency for an invalid service HDFS and component NAMENODE on host c7402.ambari.apache.org
2018-06-26 19:19:31,354  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
2018-06-26 19:20:26,051  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert yarn_resourcemanager_webui for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:20:26,052  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert nodemanager_health_summary for an invalid service YARN and component RESOURCEMANAGER on host c7403.ambari.apache.org
2018-06-26 19:20:27,354  WARN [alert-event-bus-2] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7401.ambari.apache.org
2018-06-26 19:20:31,370  WARN [alert-event-bus-1] AlertReceivedListener:576 - Unable to process alert journalnode_process for an invalid service HDFS and component JOURNALNODE on host c7402.ambari.apache.org
{code}

STR:
* Install a simple cluster with ZK, HDFS, and YARN from HDP 2.6
* Monitor the logs and observe entries for components which are not installed",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-07-02 17:19:26,3
13169301,Prevent Configuration Changes During Keytab Regeneration in an Upgrade,"Certain configuration changes should be avoided when regenerating keytab files during different scenarios.  

For example, existing non-Kerberos configurations should not be changed during the regenerate keytabs operation performed during an upgrade. However it is necessary for Kerberos identity-related configurations (such as keytab file paths and principal names) to be added and updated; as well as allow for new Kerberos-related configurations to be added. 

To allow for this, a new _update configuration policy_ value has been added to the set of directives (*_config_update_policy_*) allowed when issuing a call to regenerate keytab files. This directive replaces the less flexible *_ignore_config_updates_* directive which only allows a user to enable or disable the ability for the operation to change configurations. The values allowed for *_config_update_policy_* are as follows:
* {{none}} - No configurations will be updated
* {{identities_only}} - New and updated configurations related to Kerberos identity information - principal, keytab file, and auth-to-local rule properties
* {{new_and_identities}} - Only new configurations declared by the Kerberos descriptor and stack advisor as well as the identity-related changes
* {{all}} - All configuration changes

During an upgrade, the _update configuration policy_ is set to {{new_and_identities}}.
",pull-request-available,[],AMBARI,Bug,Blocker,2018-06-30 01:01:23,7
13169247,Agent-side command-*.json files should optionally be deleted when no longer needed by the command,"Agent-side _command JSON_ files ({{command-*.json}}, {{status_command.json}}) should optionally be deleted when no longer needed by the command.  One reason for this is to reduce the risk of leaking sensitive data stored at plaintext in the _command JSON_ files. 

Currently the _command JSON_ files are stored on disk in /var/lib/ambari-agent/data.  These files may be cleared out over time, but there is a need to have them removed as soon as they are no longer needed.

To do this, a retention policy may be defined so that the Ambari agent behaves accordingly:

* {{keep}}
** No automatic removal is performed
**  This is the default behavior  
* {{remove}}
** The _command JSON_ file are removed as soon as the command completes
* {{remove_on_success}} 
** The _command JSON_ files are removed as soon as the command *successfully* completes
** The _command JSON_ files are not removed on failure conditions

This value is to be set in the {{ambari-agent.ini}} file, typically found at {{/etc/ambari-agent/conf/ambari-agent.ini}} using the *{{command_file_retention_policy}}* property.  After setting this property, the agent needs to be restarted. ",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-06-29 20:31:23,7
13169223,Ambari ldap integration has inconsistent behavior in Group Object Class definition after upgrade,"The behavior of the ambari ldap integration seems inconsistent across 2.6.2 -> 2.7.0 upgrade.

In Ambari 2.6.2, the Group Object Class property is exemplified as taking what is commonly known as the group object class attribute.

Group object class* (posixGroup):
In Ambari 2.7.0, the Group Object Class property is exemplified as taking what is commonly known as the group search base.

Group object class* (ou=groups,dc=ambari,dc=apache,dc=org):

The first one is the correct.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 18:12:46,3
13169205,Error in persisting web client state at ambari-server in editing a widget of HDFS ,"STR
 # Login as admin to Ambari
 # Install a cluster with HDFS, ZK and AMS
 # Go to Services / HDFS / Metrics and edit one of the widgets (for instance change the name of the widget)
 # Create a new user with 'Cluster Operator' role
 # Logout and login with the newly created user
 # Try to edit the previously edit widget

The action will fail due to an authorization issue (403 is thrown).",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-29 16:53:52,6
13169196,Component with recovery Enabled are not coming up when autostart is enabled ,"STR:-  
1\. test enable autostart for all component  
2\. test restart all the host at once.  
3\. test expect within 45 min. each and every component should be up.  
4\. test failed with the following exception

    
    
     ||Host:- nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal|| 
    
    
     |ComponentName : RANGER_TAGSYNC Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : DRUID_ROUTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
     ComponentName : HIVE_SERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : SPARK2_THRIFTSERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : HISTORYSERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
     |ComponentName : DRUID_BROKER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
      |ComponentName : HBASE_MASTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : HIVE_METASTORE Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : ZEPPELIN_MASTER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : ACTIVITY_ANALYZER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : DRUID_OVERLORD Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : TIMELINE_READER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED
      |ComponentName : RANGER_ADMIN Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : OOZIE_SERVER Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
      ComponentName : METRICS_COLLECTOR Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED| 
      |ComponentName : METRICS_GRAFANA Expected Status: Started on host: nat-yc-r7-irrs-ambari-autostart-1-re-re-3.openstacklocal Found: INSTALLED|
    

That means these components are not up untill the waiting time completes.

I checked the agent logs:-  
attching the ambari-agent logs generated from  
cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py  
[agent-
autostart.log![](/images/icons/link_attachment_7.gif)](/secure/attachment/159734
/159734_agent-autostart.log ""agent-autostart.log attached to BUG-106407"")  
the relevent part if

    
    
    cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py
    
    INFO 2018-06-28 11:14:31,282 RecoveryManager.py:454 - RecoverConfig = {u'components': ()}
    INFO 2018-06-28 11:14:34,208 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'METRICS_COLLECTOR',
    INFO 2018-06-28 11:14:34,208 RecoveryManager.py:178 - New status, desired status is set to INIT for METRICS_COLLECTOR
    INFO 2018-06-28 11:14:50,695 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HBASE_REGIONSERVER
    INFO 2018-06-28 11:14:51,404 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SUPERVISOR
    INFO 2018-06-28 11:14:51,954 RecoveryManager.py:157 - New status, current status is set to INSTALLED for TEZ_CLIENT
    INFO 2018-06-28 11:14:52,672 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_CLIENT
    INFO 2018-06-28 11:14:53,467 RecoveryManager.py:157 - New status, current status is set to INSTALLED for PIG
    INFO 2018-06-28 11:14:54,241 RecoveryManager.py:157 - New status, current status is set to INSTALLED for KERBEROS_CLIENT
    INFO 2018-06-28 11:14:54,683 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HST_AGENT
    INFO 2018-06-28 11:14:55,367 RecoveryManager.py:157 - New status, current status is set to INSTALLED for METRICS_MONITOR
    INFO 2018-06-28 11:14:55,920 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ZOOKEEPER_CLIENT
    INFO 2018-06-28 11:14:56,458 RecoveryManager.py:157 - New status, current status is set to INSTALLED for LOGSEARCH_LOGFEEDER
    INFO 2018-06-28 11:14:57,017 RecoveryManager.py:157 - New status, current status is set to INSTALLED for INFRA_SOLR_CLIENT
    INFO 2018-06-28 11:14:57,823 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DATANODE
    INFO 2018-06-28 11:14:58,443 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HST_SERVER
    INFO 2018-06-28 11:14:59,205 RecoveryManager.py:157 - New status, current status is set to INSTALLED for TIMELINE_READER
    INFO 2018-06-28 11:15:00,015 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_ROUTER
    INFO 2018-06-28 11:15:00,617 RecoveryManager.py:157 - New status, current status is set to INSTALLED for NAMENODE
    INFO 2018-06-28 11:15:01,151 RecoveryManager.py:157 - New status, current status is set to INSTALLED for YARN_REGISTRY_DNS
    INFO 2018-06-28 11:15:01,745 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRPC_SERVER
    INFO 2018-06-28 11:15:02,250 RecoveryManager.py:157 - New status, current status is set to INSTALLED for STORM_UI_SERVER
    INFO 2018-06-28 11:15:02,812 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HBASE_MASTER
    INFO 2018-06-28 11:15:03,353 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_USERSYNC
    INFO 2018-06-28 11:15:03,825 RecoveryManager.py:157 - New status, current status is set to INSTALLED for NIMBUS
    INFO 2018-06-28 11:15:04,413 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SUPERSET
    INFO 2018-06-28 11:15:04,996 RecoveryManager.py:157 - New status, current status is set to INSTALLED for APP_TIMELINE_SERVER
    INFO 2018-06-28 11:15:05,552 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ACTIVITY_EXPLORER
    INFO 2018-06-28 11:15:06,202 RecoveryManager.py:157 - New status, current status is set to INSTALLED for LOGSEARCH_SERVER
    INFO 2018-06-28 11:15:07,436 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SPARK2_JOBHISTORYSERVER
    INFO 2018-06-28 11:15:07,962 RecoveryManager.py:157 - New status, current status is set to INSTALLED for INFRA_SOLR
    INFO 2018-06-28 11:15:08,494 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_METASTORE
    INFO 2018-06-28 11:15:09,508 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RESOURCEMANAGER
    INFO 2018-06-28 11:15:10,197 RecoveryManager.py:157 - New status, current status is set to INSTALLED for OOZIE_SERVER
    INFO 2018-06-28 11:15:10,840 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 11:15:11,517 RecoveryManager.py:157 - New status, current status is set to INSTALLED for SPARK2_THRIFTSERVER
    INFO 2018-06-28 11:15:12,028 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_OVERLORD
    INFO 2018-06-28 11:15:12,998 RecoveryManager.py:157 - New status, current status is set to INSTALLED for DRUID_BROKER
    INFO 2018-06-28 11:15:13,650 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HIVE_SERVER
    INFO 2018-06-28 11:15:14,321 RecoveryManager.py:157 - New status, current status is set to INSTALLED for RANGER_TAGSYNC
    INFO 2018-06-28 11:15:14,852 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ACTIVITY_ANALYZER
    INFO 2018-06-28 11:15:14,956 RecoveryManager.py:163 - current status is set to INSTALLED for METRICS_COLLECTOR
    INFO 2018-06-28 11:15:15,655 RecoveryManager.py:157 - New status, current status is set to INSTALLED for ZEPPELIN_MASTER
    INFO 2018-06-28 11:15:16,199 RecoveryManager.py:157 - New status, current status is set to INSTALLED for METRICS_GRAFANA
    INFO 2018-06-28 11:15:16,859 RecoveryManager.py:157 - New status, current status is set to INSTALLED for HISTORYSERVER
    INFO 2018-06-28 11:17:36,038 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'METRICS_COLLECTOR',
    INFO 2018-06-28 11:17:36,042 RecoveryManager.py:183 - desired status is set to STARTED for METRICS_COLLECTOR
    INFO 2018-06-28 11:27:33,451 RecoveryManager.py:183 - desired status is set to INSTALLED for METRICS_COLLECTOR
    INFO 2018-06-28 11:27:33,451 RecoveryManager.py:620 - Received EXECUTION_COMMAND (STOP/INSTALL), desired state of METRICS_COLLECTOR to INSTALLED
    INFO 2018-06-28 11:29:10,328 RecoveryManager.py:588 - After EXECUTION_COMMAND (STOP/INSTALL), with taskId=63, current state of METRICS_COLLECTOR to INSTALLED
    INFO 2018-06-28 11:45:03,661 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 11:45:12,168 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 11:45:20,469 RecoveryManager.py:163 - current status is set to STARTED for HST_SERVER
    INFO 2018-06-28 11:45:26,682 RecoveryManager.py:163 - current status is set to STARTED for RANGER_TAGSYNC
    INFO 2018-06-28 11:45:46,474 RecoveryManager.py:163 - current status is set to STARTED for YARN_REGISTRY_DNS
    INFO 2018-06-28 11:45:54,622 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    INFO 2018-06-28 11:46:27,459 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_SERVER
    INFO 2018-06-28 11:48:53,946 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 14:34:17,142 RecoveryManager.py:583 - After EXECUTION_COMMAND (START), with taskId=1530194603, current state of ACTIVITY_EXPLORER to STARTED
    INFO 2018-06-28 14:34:18,039 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,105 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,170 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,259 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,303 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,409 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,478 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,570 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,616 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,644 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,674 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,730 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,764 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,780 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,831 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,875 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,888 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,960 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:18,982 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,015 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,051 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,090 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,108 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,125 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,220 RecoveryManager.py:454 - RecoverConfig = {u'components': ({u'component_name': u'SUPERSET',
    INFO 2018-06-28 14:34:19,290 RecoveryManager.py:454 - RecoverConfig = {u'components': ()}
    INFO 2018-06-28 14:35:36,027 RecoveryManager.py:163 - current status is set to INSTALLED for SUPERVISOR
    INFO 2018-06-28 14:36:05,838 RecoveryManager.py:163 - current status is set to STARTED for SUPERSET
    INFO 2018-06-28 14:36:45,585 RecoveryManager.py:163 - current status is set to STARTED for YARN_REGISTRY_DNS
    INFO 2018-06-28 14:37:18,681 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 14:37:55,603 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 14:38:14,872 RecoveryManager.py:163 - current status is set to STARTED for RANGER_TAGSYNC
    INFO 2018-06-28 14:38:29,806 RecoveryManager.py:163 - current status is set to STARTED for DATANODE
    INFO 2018-06-28 14:38:43,789 RecoveryManager.py:163 - current status is set to STARTED for RANGER_USERSYNC
    INFO 2018-06-28 14:38:54,727 RecoveryManager.py:163 - current status is set to INSTALLED for METRICS_MONITOR
    INFO 2018-06-28 14:39:40,572 RecoveryManager.py:163 - current status is set to STARTED for STORM_UI_SERVER
    INFO 2018-06-28 14:39:42,898 RecoveryManager.py:163 - current status is set to STARTED for METRICS_MONITOR
    INFO 2018-06-28 14:40:49,742 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 14:41:13,508 RecoveryManager.py:163 - current status is set to STARTED for HBASE_REGIONSERVER
    INFO 2018-06-28 14:44:02,873 RecoveryManager.py:163 - current status is set to STARTED for NIMBUS
    INFO 2018-06-28 14:45:17,040 RecoveryManager.py:163 - current status is set to STARTED for METRICS_COLLECTOR
    INFO 2018-06-28 14:45:20,928 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_LOGFEEDER
    INFO 2018-06-28 14:46:24,130 RecoveryManager.py:163 - current status is set to INSTALLED for INFRA_SOLR
    INFO 2018-06-28 14:46:24,598 RecoveryManager.py:163 - current status is set to STARTED for ACTIVITY_ANALYZER
    INFO 2018-06-28 14:47:14,171 RecoveryManager.py:163 - current status is set to STARTED for INFRA_SOLR
    INFO 2018-06-28 14:49:22,384 RecoveryManager.py:163 - current status is set to STARTED for NAMENODE
    INFO 2018-06-28 14:49:31,552 RecoveryManager.py:163 - current status is set to STARTED for DRPC_SERVER
    INFO 2018-06-28 15:10:55,994 RecoveryManager.py:163 - current status is set to STARTED for LOGSEARCH_SERVER
    INFO 2018-06-28 15:11:01,316 RecoveryManager.py:163 - current status is set to STARTED for SPARK2_JOBHISTORYSERVER
    INFO 2018-06-28 15:11:11,251 RecoveryManager.py:163 - current status is set to STARTED for METRICS_GRAFANA
    INFO 2018-06-28 15:11:37,007 RecoveryManager.py:163 - current status is set to STARTED for HST_SERVER
    INFO 2018-06-28 15:11:57,521 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    INFO 2018-06-28 15:12:03,321 RecoveryManager.py:163 - current status is set to STARTED for OOZIE_SERVER
    INFO 2018-06-28 15:13:24,798 RecoveryManager.py:163 - current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 15:14:13,744 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 15:15:03,889 RecoveryManager.py:163 - current status is set to INSTALLED for RANGER_ADMIN
    INFO 2018-06-28 15:15:50,119 RecoveryManager.py:163 - current status is set to INSTALLED for OOZIE_SERVER
    INFO 2018-06-28 15:15:50,451 RecoveryManager.py:163 - current status is set to INSTALLED for NAMENODE
    INFO 2018-06-28 15:15:51,459 RecoveryManager.py:163 - current status is set to STARTED for RANGER_ADMIN
    INFO 2018-06-28 15:15:57,678 RecoveryManager.py:163 - current status is set to STARTED for NAMENODE
    INFO 2018-06-28 15:16:22,782 RecoveryManager.py:163 - current status is set to STARTED for HBASE_MASTER
    INFO 2018-06-28 15:16:40,512 RecoveryManager.py:163 - current status is set to STARTED for APP_TIMELINE_SERVER
    INFO 2018-06-28 15:16:43,582 RecoveryManager.py:163 - current status is set to STARTED for OOZIE_SERVER
    INFO 2018-06-28 15:17:45,952 RecoveryManager.py:163 - current status is set to STARTED for HISTORYSERVER
    INFO 2018-06-28 15:37:57,495 RecoveryManager.py:163 - current status is set to INSTALLED for HST_AGENT
    INFO 2018-06-28 15:38:47,195 RecoveryManager.py:163 - current status is set to STARTED for HST_AGENT
    

I checked is this because of memory issue. but

    
    
    [root@nat-yc-r7-irrs-ambari-autostart-1-re-re-3 data]# df -h
    Filesystem             Size  Used Avail Use% Mounted on
    /dev/mapper/rhel-root   17G   11G  6.0G  65% /
    devtmpfs               7.8G     0  7.8G   0% /dev
    tmpfs                  7.8G     0  7.8G   0% /dev/shm
    tmpfs                  7.8G  361M  7.5G   5% /run
    tmpfs                  7.8G     0  7.8G   0% /sys/fs/cgroup
    /dev/vda1             1014M  172M  843M  17% /boot
    /dev/vdb               246G   19G  216G   8% /grid/0
    tmpfs                  1.6G     0  1.6G   0% /run/user/1037
    tmpfs                  1.6G     0  1.6G   0% /run/user/0
    

I checked some of auto_errors

    
    
    cat auto_errors-1530190498.txt
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 132, in <module>
        HistoryServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 100, in start
        skip=params.sysprep_skip_copy_tarballs_hdfs) or resource_created
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 497, in copy_to_hdfs
        source_file = prepare_function()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 97, in _prepare_tez_tarball
        hadoop_lib_native_lzo_dir = os.path.join(stack_root, service_version, ""hadoop"", ""lib"", ""native"")
      File ""/usr/lib64/python2.7/posixpath.py"", line 75, in join
        if b.startswith('/'):
    AttributeError: 'NoneType' object has no attribute 'startswith'
    

attching the ambari-agent logs generated from  
cat /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py  
[agent-
autostart.log![](/images/icons/link_attachment_7.gif)](/secure/attachment/159734
/159734_agent-autostart.log ""agent-autostart.log attached to BUG-106407"")

Repro cluster:- <http://linux-jenkins.qe.hortonworks.com:8080/job/Nightly-
Start-EC2-Run-HDP/985907/>  
lifetime extended to 72 hours <http://linux-
jenkins.qe.hortonworks.com:8080/job/update-openstack-lifetime/12085/>

",pull-request-available,[],AMBARI,Bug,Major,2018-06-29 16:07:59,0
13169165,"Ambari setup-ldap fails with ""internal server error"" after upgrade","Steps to reproduce:

# integrate ambari 2.6.0 with ldap
# upgrade to ambari 2.7.0
# integrate ambari 2.7.0 with same ldap

Result:

{code}
ERROR: Unexpected HTTPError: HTTP Error 500: Internal Server Error
For more info run ambari-server with -v or --verbose option
{code}

Cause:

{code}
2018-06-29T09:43:32.624Z, User(admin), RemoteIp(127.0.0.1), RequestType(PUT), url(http://127.0.0.1:8080/api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/ldap-configuration), ResultStatus(500 Internal Server Error), Reason(org.apache.ambari.server.controller.spi.SystemException: 
Invalid Ambari server configuration key: ldap-configuration:ssl.trustStore.path)
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 13:49:03,3
13169155,Add Service Wizard: Next Button is not enabled while adding Ranger after fixing an erroneous property,"Following are the steps performed
- On an already deployed cluster (without Ranger) setup ldap
- navigate to Add Service wizard to add ranger
- Use Authentication method as LDAP
- At customize services property populate all required fields
- Set one of the password to not meet the requirement (say set it as rangeradmin)
- Click Next. There will be a warning popup which says properties did not meet the requirements and user has to change it before proceeding
- Close the popup and find the property which has to be changed (It would be better if user is navigated to the property upon clicking on the warning popup itself)
- Fix this property, notice that there are no other properties which needs attention
- Still Next button is not enabled",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-29 13:10:02,2
13169091,Blueprint deployment with custom service name,"Blueprint deployment with custom service name fails, because some parts of Ambari mix and match service name (eg. zk1) and service type (eg. ZOOKEEPER).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-29 07:31:51,5
13168836,Python unit test failure on 2.7.6,"Python unit tests fail on Python 2.7.6, because [SSLContext|https://docs.python.org/2/library/ssl.html#ssl-contexts] was only added in 2.7.9.

{noformat:title=https://builds.apache.org/job/Ambari-trunk-Commit/9543/consoleText}
ERROR: test_ldap_sync_ssl (TestAmbariServer.TestAmbariServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/TestAmbariServer.py"", line 7804, in test_ldap_sync_ssl
    sync_ldap(options)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/main/python/ambari_server/setupSecurity.py"", line 405, in sync_ldap
    raise FatalException(1, err)
FatalException: ""Fatal exception: Sync event creation failed. Error details: 'module' object has no attribute 'SSLContext', exit code 1""

ERROR: test_get_ssl_context (TestServerUtils.TestServerUtils)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/TestServerUtils.py"", line 124, in test_get_ssl_context
    context = get_ssl_context(properties)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/main/python/ambari_server/serverUtils.py"", line 274, in get_ssl_context
    context = ssl.SSLContext(protocol)
AttributeError: 'module' object has no attribute 'SSLContext'
{noformat}

CC [~rlevas]",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-28 11:03:12,7
13168610,Unable to filter on the alerts in dashboard page,"WIth the new Alerts shown in the Ambari dashboard, unable to filter the various alerts seen. When filter is selected, the dropdown immediately disappears and doesn't allow the user to select the alert level to filter.
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-27 12:13:17,4
13168421,Ranger Storm plugin toggle behavior changes under different scenarios,"Here is a related issue which only occurs for fresh HDP-3.0 installs where the cluster is setup manually and is a bit strange.

There are two scenarios:
 1. When a cluster is kerberos enabled with Ranger installed, Now using Add service wizard Add Storm service and on the customize configuration page, we see that Ranger Storm plugin is enabled and [service-advisor|https://github.com/hortonworks/hdp_ambari_definitions/blob/AMBARI-2.7.0.0/src/main/resources/stacks/HDP/3.0/services/STORM/service_advisor.py#L216-L222] provides appropriate recommendations by showing:
{noformat}
* ranger-storm-plugin-enabled with recommended value Yes.
* nimbus.authorizer with recommended value org.apache.ranger.authorization.storm.authorizer.RangerStormAuthorizer.
{noformat}
2. When a cluster is kerberos enabled with Ranger and Storm installed and we then enable Ranger Storm plugin the service-advisor recommendations differ this time by showing:
{noformat}
* ranger-storm-plugin-enabled with recommended value Yes.
* nimbus.authorizer with recommended value Property removed.
{noformat}
Looks like in the first case the {{security_enabled}} is {{True}} whereas in the second case the flag is {{False}} while the cluster is actually kerberized.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-26 17:56:02,6
13168344,WebSockets traffic does not work between Ambari Web UI and Ambari Server when it is accessed via Knox Proxy (UI side changes),When connected via Knox proxy ambari server web socket URL should be changed from <ambari-server protocol>://<ambari-server host>:<ambari-server port>/api/stomp/v1/websocket to <knox protocol>://<knox host>:<knox port>/gateway/default/ambari/websocket,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-26 13:07:22,4
13168277,Remove reference to JDK 1.7 in ambari-server setup,"We need to remove the reference to JDK 1.7, as it's not supported with HDP 3.0. We still see the following in ambari-server
setup:
    
    
    [2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-26 07:30:04,0
13168058,Database tab disabled if Setup Database and Database User,"Although 'Database' tab doesn't allow to go to the next step if the required properties are empty it is not allowing even if there are no required properties left on the screen.

STR
1. Go to Customize Services --> Databases --> Ranger.
2. Enter the Ranger DB host.
3. Set/Toggle 'Setup Database and Database Use' to No.

Although no required properties are present but the 'Next' button is disabled.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-25 14:03:24,4
13168012,Ambari gets stuck at host checks (after successful registration) when installing more number of nodes via UI,"While installing 22 node cluster via ambari, it gets stuck at checking hosts for potential warnings after successful registration.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-25 11:06:04,1
13168010,Deleting a service fails to remove keytabs,"while deleting Atlas service, faced this issue:

{code}
stderr: 
Caught an exception while executing custom service command: : Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969; Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969
 stdout:
Caught an exception while executing custom service command: : Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969; Command requires configs with timestamp=1529647351404 but configs on agent have timestamp=1529647350969

Command failed after 1 tries
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-25 10:48:18,3
13167998,Error processing agent reports due to wrong stack usage,"{noformat}
22 May 2018 15:33:05,830 ERROR [agent-report-processor-0] AgentReportsProcessor:90 - Error processing agent reports
org.apache.ambari.server.StackAccessException: Stack data, stackName=HDPCORE, stackVersion=1.0.0-b368, stackServiceName=HBASE
        at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:604)
        at org.apache.ambari.server.api.services.AmbariMetaInfo.getComponent(AmbariMetaInfo.java:357)
        at org.apache.ambari.server.state.host.HostImpl.calculateHostStatus(HostImpl.java:1247)
        at org.apache.ambari.server.agent.HeartbeatProcessor.processHostStatus(HeartbeatProcessor.java:300)
        at org.apache.ambari.server.agent.HeartBeatHandler.handleCommandReportStatus(HeartBeatHandler.java:275)
        at org.apache.ambari.server.agent.AgentReportsProcessor$AgentReportProcessingTask.run(AgentReportsProcessor.java:83)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-25 09:40:36,5
13167806,Agent failed to process execution command,"Some execution commands were failed during blueprint deploy:

    
    
    
    ERROR 2018-06-19 22:29:28,058 ActionQueue.py:221 - Exception while processing EXECUTION_COMMAND command
     Traceback (most recent call last):
       File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 214, in process_command
         self.execute_command(command)
       File ""/usr/lib/ambari-agent/lib/ambari_agent/ActionQueue.py"", line 352, in execute_command
         commandresult['stdout'] += '\n\nCommand completed successfully!\n' if status == self.COMPLETED_STATUS else '\n\nCommand failed after ' + str(numAttempts) + ' tries\n'
     UnboundLocalError: local variable 'commandresult' referenced before assignment
     INFO 2018-06-19 22:29:28,100 ActionQueue.py:238 - Executing command with id = 4-0, taskId = 5 for role = MAPREDUCE2_CLIENT of cluster_id 2.
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-23 09:17:58,0
13167664,Test Connection button not working in Database Tab,"Before running below command, 'Test Connection' button was showing the excepted output to run below command:
{noformat}
ambari-server setup --jdbc-db=mysql --jdbc-driver=/root/mysql-connector-java.jar
{noformat}

After running the command, 'Test Connection'  keeps on spinning the loader and browser console shows below error
{noformat}
Uncaught TypeError: Cannot read property 'filter' of undefined
    at Class.getConnectionProperty (app.js:216783)
    at Class.<anonymous> (app.js:216763)
    at ComputedPropertyPrototype.get (vendor.js:14954)
    at get (vendor.js:13360)
    at Class.get (vendor.js:19791)
    at Class.getTaskInfoSuccess (app.js:216816)
    at Class.opt.success (app.js:192713)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
{noformat}

This issue is not seen on 'All Configurations' tab under services smart config tab.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-22 13:54:13,4
13167637,Express Upgrade Blocked on Missing OS in repo_version,"We should add a prereq check that for EU, the SOURCE version has an entry for each OS type in the cluster, and the TARGET version also has an entry (this better be true, or distribution would fail).",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-22 11:58:09,3
13167576,Metric Collector goes down after HDFS restart post EU,"
**STR**

  1. Deployed cluster with Ambari version: 2.6.1.5-3 and HDP version: 2.6.1.0-129
  2. Upgrade Ambari to Target Version: 2.7.0.0-709
  3. Upgrade AMS and Smartsense (keeping them stopped)
  4. Perform EU to HDP-3.0 and let it complete
  5. Restart HDFS
  6. Observe state of Metrics Collectors (AMS is configured in distributed mode)

**Result**  
Both metrics collectors are down (auto start is enabled for Metrics Collector)

From logs:

    
    
    
    2018-06-13 16:45:05,620 ERROR org.apache.ambari.metrics.core.timeline.discovery.TimelineMetricMetadataManager: TimelineMetricMetadataKey is null for : [-8, 31, -72, 32, 88, -8, -51, -88, -104, 12, -123, 99, 55, -90, 45, -12, 115, 0, -6, 13]
    2018-06-13 16:45:05,622 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR
    java.lang.NullPointerException
            at org.apache.ambari.metrics.core.timeline.aggregators.TimelineMetricReadHelper.getTimelineMetricCommonsFromResultSet(TimelineMetricReadHelper.java:116)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getLastTimelineMetricFromResultSet(PhoenixHBaseAccessor.java:446)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getLatestMetricRecords(PhoenixHBaseAccessor.java:1134)
            at org.apache.ambari.metrics.core.timeline.PhoenixHBaseAccessor.getMetricRecords(PhoenixHBaseAccessor.java:953)
            at org.apache.ambari.metrics.core.timeline.HBaseTimelineMetricsService.getTimelineMetrics(HBaseTimelineMetricsService.java:288)
            at org.apache.ambari.metrics.webapp.TimelineWebServices.getTimelineMetrics(TimelineWebServices.java:261)
            at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
            at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
            at java.lang.reflect.Method.invoke(Method.java:498)
    
    2018-06-13 16:45:07,887 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=ctr-e138-1518143905142-361872-01-000005.hwx.site:2181,ctr-e138-1518143905142-361872-01-000006.hwx.site:2181,ctr-e138-1518143905142-361872-01-000003.hwx.site:2181 sessionTimeout=120000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$13/572967831@60474c94
    2018-06-13 16:45:07,889 INFO org.apache.zookeeper.client.ZooKeeperSaslClient: Client will use GSSAPI as SASL mechanism.
    2018-06-13 16:45:07,891 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
    2018-06-13 16:45:07,891 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181, initiating session
    2018-06-13 16:45:07,894 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server ctr-e138-1518143905142-361872-01-000006.hwx.site/172.27.73.151:2181, sessionid = 0x363f94c8d6d0059, negotiated timeout = 90000
    2018-06-13 16:45:11,938 INFO org.apache.hadoop.hbase.client.RpcRetryingCallerImpl: Call exception, tries=6, retries=6, started=4153 ms ago, cancelled=false, msg=Call to ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320 failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320, details=row 'SYSTEM.CATALOG' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=ctr-e138-1518143905142-361872-01-000007.hwx.site,61320,1528896330963, seqNum=-1
    2018-06-13 16:45:15,954 INFO org.apache.hadoop.hbase.client.RpcRetryingCallerImpl: Call exception, tries=7, retries=7, started=8169 ms ago, cancelled=false, msg=Call to ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320 failed on local exception: org.apache.hadoop.hbase.ipc.FailedServerException: This server is in the failed servers list: ctr-e138-1518143905142-361872-01-000007.hwx.site/172.27.74.131:61320, details=row 'SYSTEM.CATALOG' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=ctr-e138-1518143905142-361872-01-000007.hwx.site,61320,1528896330963, seqNum=-1
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-22 07:19:21,0
13167435,Cannot distinguish components on Host Details page due to shortened display name (Ambari should show full component name on mouse over),"Yarn has 2 version of timeline service .
1) TIMELINE SERVICE V1.5
2) TIMELINE SERVICE V2.0 READER
When both of these services are installed on one host, go to Component page . Component Page only shows first few chars ""Timeline Service... "".
Ambari UI shows ""Timeline Service.."" for both Timeline service 1.5 and 2.0 . Thus, user can not identify which is Timeline service 1.5 or 2.0
ambari should show the full component name when user brings mouse pointer on the component name.
",pull-request-available,[],AMBARI,Bug,Major,2018-06-21 16:03:54,2
13167364,Provide a way to disable topology validation in cluster creation request,"The topology validation that takes place during blueprint creation request can be disabled by passing {{validate_topology=false}} as query param.  Validation also has a side-effect of adding any auto-deployable components/dependencies to the topology.

For mpack-based deployment most of the validation is moved to the cluster creation request, since mpacks may not be available prior to that.  We need to provide a way to disable validation in this request, too.  Using the same {{validate_topology=false}} flag may be the best for this purpose.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-06-21 10:33:15,5
13167191,Ranger and KMS tab still present in Customize ServicesPage even after going back and deselecting them from Choose ServicesPage,"- Navigate to Customize ServicePage of UI install wizard by selecting all services from ChooseServicesPage. 
- Populate credentials tab
- Switch to Databases tab, Noticed that Ranger and Ranger KMS have multiple properties to be filled in
- Now go back to Choose Services page and deselect Ranger and Ranger KMS
- Proceed through wizard to reach Customize ServicesPage
- Ranger and KMS sub tabs are still present with errors at Databases tab. 
(None of the ranger/KMS credential properties are present in Credential tabs)

- Now even if we go back and choose Ranger and KMS - update all necessary properties for Ranger - Test Connection is hung with error",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-20 15:27:56,2
13167147,Customize Service step issues,"There are some issues in Step 7 of the Installer Wizard
1. Even if there are some *required* changes in the 'Database' tab it allows to go to the next step. Next should only be enabled if the required property has been provided with an input
2. If there are some CRITICAL errors, the top notification bell should be 'Red' (similar to what we see if there is an empty required property) 
3. If I click on any of the above critical error properties it takes me to a wrong page. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-20 12:06:15,4
13167069,Agent fails to auto-start HDFS when using LZO,"Service auto-start for HDFS fails with the following error when using LZO libraries:
{noformat:title=auto_errors-...txt}
...
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs.py"", line 137, in hdfs
    install_lzo_if_needed()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/lzo_utils.py"", line 87, in install_lzo_if_needed
    Script.repository_util.create_repo_files()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/repository_util.py"", line 53, in create_repo_files
    if self.command_repository.version_id is None:
AttributeError: RepositoryUtil instance has no attribute 'command_repository'
{noformat}
{{repositoryFile}} entry is missing from auto-start commands.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-20 05:51:06,0
13166972,Make STOMP updates immutable,STOMP event update object can be corrupted before emitting to subscribers by other update handling.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-19 16:20:18,1
13166835,Rolling restarts not working when Kerberos Auto sign in is enabled for Ambari,"Rolling restarts not working when Kerberos Auto sign in is enabled for Ambari.  This is due to a missing user ID value in the {{requestschedule}} table, resulting in the Rolling Restart operation aborting with no deals in the ambari-server.log file.

||schedule_id||cluster_id||authenticated_user_id||create_user||update_user||last_execution_status||
|2|2|-1|rlevas|rlevas|IN_PROGRESS|

This occurs because the {{org.springframework.security.core.Authentication}} class stored in Ambari's SecurityContext for the authenticated user does not contain the authenticated user's ID data. Adjustments need to be made to the infrastructure to capture this data upon login and then retrieve it later, when needed. 

",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-19 03:00:24,7
13166709,Failed to force_non_member_install a stack version on hosts,"The ability to pre-install packages on hosts using the following API request is broken since 2.6.0:

{noformat}
$ curl -X POST -d '{ ""HostStackVersions"": { ""repository_version"": ""2.6.1.0-129"", ""stack"": ""HDP"", ""version"": ""2.6"", ""cluster_name"": ""TEST"", ""force_non_member_install"": true, ""components"": [ { ""name"" : ""ZOOKEEPER_SERVER"" }, { ""name"": ""ZOOKEEPER_CLIENT"" } ] } }' http://localhost:8080/api/v1/hosts/${hostname}/stack_versions
{noformat}

The request is accepted, but:

* on 2.6.0: package installation (1st task) fails due to missing {{stack_name}} and {{stack_version}}
* on 2.6.1: ""set all"" (2nd task) fails for the same reason",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-18 13:51:17,5
13166705,Customer cannot back from step 4 to step 3 during adding a new host to cluster,"STR:
1) Deploy cluster
2) try to add a new host (only with client) to the cluster
3) On step 4 try to get back on step 3 (using button back)

Actual result: Customer cannot back from step 4 to step 3 during adding a new host to cluster.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-18 13:39:22,4
13166702,"Components Filter in ""Manage ConfigGroup at Customize Services page"" doesn't list hosts correctly","- At Assign Slaves and Client page chose to have clients on all nodes
- At Customize Services Page create a config group by navigating to ManageConfigGroups
- create a new group
- While adding hosts use filter COMPONENTS to find hosts with particular component
- It doesn't list any host for HDFS client. It should actually list host based on what user selected in Assign slaves page",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-18 13:19:21,2
13166677,Failure of component install leave ambari ui with inactive Customize Services,"Tried installing hive on a cluster where hive client install failed with following error

{noformat}
resource_management.core.exceptions.ExecutionFailed: Execution of 'cp --remove-destination /var/lib/ambari-agent/tmp/mysql-connector-java.jar /usr/hdp/current/hive-client/lib/mysql-connector-java.jar' returned 1. cp: cannot create regular file '/usr/hdp/current/hive-client/lib/mysql-connector-java.jar': No such file or directory
{noformat}

Post that it is unable to customize any configs since all links except Assign Masters was inactive",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-18 10:20:11,4
13166540,Add notification to the Alert Groups not working,"Add notification to the Alert Groups not working.

STR:
1. Navigate to alerts page
2. Open Manage Alert Groups
3. Select an Alert Group
4. Add Notification by clicking Add and selecting a 'pre created' Alert Notification
5. Click Save and close the popup, the alert group should be saved with selected notification type, but only an empty String is being saved.

E.g API being invoked:

{code:java}
PUT https://<host>:<port>/api/v1/clusters/<cluster>/alert_groups/4

{""AlertGroup"":{""name"":""AMBARI_INFRA_SOLR"",""definitions"":[14],""targets"":[4]}}: 
{code}

Notes:
1. Observed that the auto fill for the notification is case sensitive, which is not ideal.
2. Also observed trying to add another notification after performing the steps in STR, a 500 server error is thrown.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-16 14:10:39,2
13166527,"Ambari setup-ldap does not prompt for ldaps cert path, even when use-ssl is set to true","Running  ambari-server setup-ldap with use SSL option gives the following error:

{code}
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 1056, in <module>
    mainBody()
  File ""/usr/sbin/ambari-server.py"", line 1026, in mainBody
    main(options, args, parser)
  File ""/usr/sbin/ambari-server.py"", line 976, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/setupSecurity.py"", line 799, in setup_ldap
    if get_YN_input(""Do you want to remove these properties [y/n] (y)? "", True, options.trust_store_reconfigure):
AttributeError: Values instance has no attribute 'trust_store_reconfigure'
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-16 10:17:40,3
13166435,Duration for a failed BG Operation is incorrect,"There are several failed install components and they report the incorrect duration.

Example: Install Nodemanager failed and was executed only for a few seconds, whereas in the Duration column, it shows 16 hours.
 !Screen Shot 2018-06-15 at 12.12.38 PM.png! 

Primarily because the output from the server has the end time as -1:
{noformat}
{
  ""itemTotal"" : ""10"",
  ""items"" : [
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 31,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install NFSGateway"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1528983750924,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 32,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install NodeManager"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1528983763421,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 33,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install YARN Client"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1528983776062,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1528985439608,
        ""id"" : 34,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Reinstall Failed Components"",
        ""request_status"" : ""COMPLETED"",
        ""start_time"" : 1528985380177,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1528985625102,
        ""id"" : 35,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Reinstall Failed Components"",
        ""request_status"" : ""COMPLETED"",
        ""start_time"" : 1528985594981,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1528986079631,
        ""id"" : 36,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""put services into STARTED"",
        ""request_status"" : ""COMPLETED"",
        ""start_time"" : 1528986013070,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : 1529043756536,
        ""id"" : 37,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Start Knox Gateway"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529043756451,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 38,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install MapReduce2 Client"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529044241508,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 39,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install Hive Client"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529044679827,
        ""user_name"" : ""admin""
      }
    },
    {
      ""Requests"" : {
        ""cluster_name"" : ""cl1"",
        ""end_time"" : -1,
        ""id"" : 40,
        ""progress_percent"" : 100.0,
        ""request_context"" : ""Install Metrics Collector"",
        ""request_status"" : ""FAILED"",
        ""start_time"" : 1529045027390,
        ""user_name"" : ""admin""
      }
    }
  ]
}
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-15 17:47:03,6
13166430,cluster version is in invalid state,"Steps to reproduce:

1. Make sure you have stable cluster

2. add a new host - and make sure it fails with the installation at the end step.

3. Now go to ""stack and versions"" screen - you will see the cluster in kind of messed up state.

Usually customers may not see the 3rd step right after adding the host and if they observe after few days then it gives very wrong impression that cluster is it messed up state though it is not.

Output for host_version


{noformat}
select * from host_version where host_id in ( 301, 551) and repo_version_id = 102

'901','102','301','CURRENT'
'1052','102','551','OUT_OF_SYNC'

2nd row is problematic one.

{noformat}

Workaround:

Remove the host and add it again and make sure installation is not failing
or
change the status of host_version to 'CURRENT'",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-06-15 17:04:09,4
13166380,Web Client Pulls Back Too Much Information in Upgrade Wizard,"During an upgrade, the upgrade wizard is very inefficient at the data that it pulls back. On larger upgrades, expanding groups in progress is practically unusable and can actually cause the browser to crash.

I recently observed this on a small upgrade but quickly opening and closing the active group (see screenshot). The expansion of groups/tasks just sat there spinning.

I think that the upgrade wizard needs to pull back a lot less data and possibly cache the majority of what it has in order to make it usable during a larger upgrade.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-15 14:33:40,4
13166361,Ranger server password checks are not performed during Cluster Install Wizard,"While installing a new cluster, if the Ranger-specific passwords do not meet the (non-obvious) requirements, failures will occur during the install Ranger task. Once this happens, there is no way for the user to go back and fix the issue. So the issue needs to be caught sooner.

STR
# Create new Ambari 2.7.0 cluster
# Include Ranger
# Set simple passwords when prompted - for example: hadoop
# Proceed to install
# See failure",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-15 13:47:14,2
13166353,Requests failed after Ambari upgrade with exception while executing custom service command,"*STR*
 # Deployed cluster with Ambari version: 2.6.2
 # Upgrade Ambari to Target Version: 2.7.0
 # Restart all services with stale configs

*Result*
Some of the requests failed with below error
{code:java}
stderr: 
Caught an exception while executing custom service command: : 'Metadata for cluster_id=2 is missing. Check if server sent it.'; 'Metadata for cluster_id=2 is missing. Check if server sent it.'
 stdout:
Caught an exception while executing custom service command: : 'Metadata for cluster_id=2 is missing. Check if server sent it.'; 'Metadata for cluster_id=2 is missing. Check if server sent it.'

Command failed after 1 tries{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-15 13:39:36,1
13166328,Add Ranger Kms Server doesn't not trigger Install request while enabling HA,"Steps to reproduce:
- Install Cluster with Ambari-2.7.0.0
- Add Ranger service +  Ranger KMS service
- Go to Action option of Ranger KMS service > Click on Add Ranger KMS server > Confirmation Pop up shows up, after clicking Confirm ADD option Install request is not triggered.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-15 11:19:07,4
13166086,Update the Manage JN wizard step for multiple nameservice scenario,Update the Save namespace commands based on available nameservices.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-14 12:19:36,2
13166050,Canceling task during blueprint install results in agent not responding to any other tasks,"If failing operation which is in retry cycle is canceled during blueprint
install. The agent will still continue to execute the canceled task. Resulting
in ActionQueue being stuck and no other task being able to start execution

",pull-request-available,[],AMBARI,Bug,Major,2018-06-14 09:33:24,0
13165871,Delete Button not enabled on Druid Router component,"facing issue while trying to delete Druid Router, where the delete button is disabled. 

The Test performs the following:
* Select component from list (after clicking on add button)
* Start component
* Restart all component with stale config
* Run Service check against service
* Stop component
* Delete component
* Restart all component with stale config
* Run Service check
* Repeat the above steps for all possible components.

Noticed that the delete button was not enabled for Druid Router only. When I accessed the cluster later, the delete button was enabled. This looks like an intermittent issue. Only once I could reproduce this on my local after doing various actions related to add/delete component.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-13 15:23:39,4
13165859,Blueprint should add service dependencies from specific mpack,"Currently client components need to be specified in the blueprint once per mpack, and {{mpack_instance}} needs to be explicitly declared, which is a bit cumbersome.

{{DependencyAndCardinalityValidator}} should be mpack-aware, and add client components from each mpack as required.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-06-13 14:41:15,5
13165817,Cannot add component to 25+ hosts,"STR:
1. On 26+ nodes cluster go to Hosts -> All Hosts
2. Add any component to all hosts:
3. Obverse this error from UI:
{noformat}
app.js:23508 Uncaught TypeError: Cannot read property 'get' of undefined
    at app.js:23508
    at Array.forEach (<anonymous>)
    at Class._getComponentsFromServerForHostComponentsAddCallback (app.js:23507)
    at Object.callback (app.js:23498)
    at Object.getComponentsFromServerSuccessCallback (app.js:181958)
    at Class.opt.success (app.js:181325)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{noformat}

As per discussion with UI guys. This is because model is only available for first 25 hosts, and so for others it's not possible to check if heartbeat is not lost.

Possibly the solution could be as simple as UI asking heartbeat state in scope of the request along with other info.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-13 11:41:53,4
13165762,"The alert ""DataNode Unmounted Data Dir"" did not appear","str:


1) execute commands on hosts:

dd if=/dev/zero of=/grid/0/fs bs=1 count=0 seek=10G
mkfs.ext3 -F /grid/0/fs
mkdir /grid/1/
mount -o loop,rw /grid/0/fs /grid/1/
chown cstm-hdfs:hadoop /grid/1/
2) set DN dir property (dfs.datanode.data.dir) to ""/grid/0/hadoop/hdfs/data,/newdndir,/grid/1/hadoop/hdfs"" and restart needed services 
3) check that alert is not present ""DataNode Unmounted Data Dir"" (after 2 min)
4) Stop DN(s)
5) umount /grid/1/
6) Start DN(s)
7) check that alert is present",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-06-13 07:59:01,3
13165684,setup-sso in Ambari fails when SSL is enabled,"When SSL is enabled and the python version is 2.7.14, accessing the Ambari server via the {{ambari-server}} CLI fails with CERTIFICATE_VERIFY_FAILED.

 
{noformat:title=Example}
-bash-4.2# ambari-server setup-sso -v
Using python /usr/bin/python
Setting up SSO authentication properties...
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Setup SSO.
INFO: about to run command: ps -p 33705
INFO:
process_pid=107113
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Enter Ambari Admin login: admin
Enter Ambari Admin password:
INFO: Fetching SSO configuration from DB
INFO: Fetching information from Ambari's REST API
Traceback (most recent call last):
 File ""/usr/sbin/ambari-server.py"", line 1056, in <module>
 mainBody()
 File ""/usr/sbin/ambari-server.py"", line 1026, in mainBody
 main(options, args, parser)
 File ""/usr/sbin/ambari-server.py"", line 976, in main
 action_obj.execute()
 File ""/usr/sbin/ambari-server.py"", line 90, in execute
 self.need_restart = self.fn(*self.args, **self.kwargs)
 File ""/usr/lib/ambari-server/lib/ambari_server/setupSso.py"", line 266, in setup_sso
 properties = get_sso_properties(ambari_properties, admin_login, admin_password)
 File ""/usr/lib/ambari-server/lib/ambari_server/setupSso.py"", line 221, in get_sso_properties
 response_code, json_data = get_json_via_rest_api(properties, admin_login, admin_password, SSO_CONFIG_API_ENTRYPOINT)
 File ""/usr/lib/ambari-server/lib/ambari_server/serverUtils.py"", line 206, in get_json_via_rest_api
 with closing(urllib2.urlopen(request)) as response:
 File ""/usr/lib64/python2.7/urllib2.py"", line 154, in urlopen
 return opener.open(url, data, timeout)
 File ""/usr/lib64/python2.7/urllib2.py"", line 429, in open
 response = self._open(req, data)
 File ""/usr/lib64/python2.7/urllib2.py"", line 447, in _open
 '_open', req)
 File ""/usr/lib64/python2.7/urllib2.py"", line 407, in _call_chain
 result = func(*args)
 File ""/usr/lib64/python2.7/urllib2.py"", line 1243, in https_open
 context=self._context)
 File ""/usr/lib64/python2.7/urllib2.py"", line 1200, in do_open
 raise URLError(err)
urllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)>
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-06-12 22:07:01,7
13165592,Reordering of dashboard widgets doesn't work after enabling NN federation,"*STR*
# Enable NameNode federation
# Go to dashboard
# Try to reorder some widgets (common or namespace-specific ones)

*Expected result*
New widgets order is persisted

*Actual result*
- JS error is thrown: {{app.js:237946 Uncaught TypeError: Cannot read property 'getAttribute' of undefined}}
- New order isn't persisted",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-12 15:05:12,2
13165544,Ambari UI isn't loading in IE,"After submitting login credentials, UI is stuck on loading data.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-12 12:10:53,2
13165369,Guarantee execution command retrying task completion,Server should guarantee execution command retry task will not stuck.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-11 16:46:20,1
13165324,Guarantee STOMP update contains not null hash/timestamp if exists,Server should not populate STOMP update with null hash/timestamp.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-11 13:18:58,1
13165320,Adding alerts to alert groups not working,"STR:
1. Navigate to Alerts> Manage Alert groups >  Create alert group e.g Test_group
2. Add alerts to the alert groups and click Save button
3. Navigate to Alerts> Manage Alert groups > Select Test_group and the alerts selected should be listed in the alert group, but only empty entry is available.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-11 12:52:43,2
13165306, NN cannot start due do not have permission for creation of the folde,"STR:  
1) Install ambari cluster with custom user configuration via BP  
Cluster: <http://172.27.14.154:8080>

Actual result: NN cannot start due do not have permission for creation of the
folder ""/var/run/hadoop/cstm-hdfs""  
Looks like some script changed permission for

    
    
    
    [root@ctr-e138-1518143905142-357962-01-000006 ~]# ls -la /var/run/ | grep ""hadoop""
    drwxr-xr-x  2 cstm-ams       hadoop   4096 Jun 11 01:56 ambari-metrics-monitor
    drwxr-xr-x  6 root           root     4096 Jun 11 01:56 hadoop
    

NN Logs:

    
    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 414, in 
        NameNode().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 138, in start
        upgrade_suspended=params.upgrade_suspended, env=env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 115, in namenode
        format_namenode()
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/hdfs_namenode.py"", line 369, in format_namenode
        logoutput=True
      File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
        returns=self.resource.returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 314, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'hdfs --config /usr/hdp/3.0.0.0-1469/hadoop/conf namenode -format -nonInteractive' returned 1. ######## Hortonworks #############
    This is MOTD message, added for testing in qe infra
    WARNING: /var/run/hadoop/cstm-hdfs does not exist. Creating.
    mkdir: cannot create directory ‘/var/run/hadoop/cstm-hdfs’: Permission denied
    ERROR: Unable to create /var/run/hadoop/cstm-hdfs. Aborting.
     stdout:
    2018-06-11 10:00:42,196 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:42,308 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:43,323 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:43,353 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:43,358 - Group['cstm-users'] {}
    2018-06-11 10:00:43,364 - Group['cstm-ranger'] {}
    2018-06-11 10:00:43,364 - Group['cstm-zeppelin'] {}
    2018-06-11 10:00:43,365 - Group['hdfs'] {}
    2018-06-11 10:00:43,365 - Group['cstm-livy'] {}
    2018-06-11 10:00:43,365 - Group['hadoop'] {}
    2018-06-11 10:00:43,366 - Group['cstm-knox'] {}
    2018-06-11 10:00:43,366 - Group['cstm-spark'] {}
    2018-06-11 10:00:43,368 - User['yarn-ats'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,371 - User['cstm-ranger'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-ranger', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,373 - User['cstm-hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,376 - User['cstm-sqoop'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,378 - User['cstm-ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,381 - User['cstm-yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,384 - User['cstm-tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,386 - User['cstm-atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,389 - User['cstm-storm'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,391 - User['cstm-knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'cstm-knox'], 'uid': None}
    2018-06-11 10:00:43,394 - User['cstm-kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,397 - User['cstm-logsearch'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,399 - User['cstm-infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,402 - User['cstm-hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,404 - User['cstm-hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,407 - User['cstm-mr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,409 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,412 - User['cstm-zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-zeppelin', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,414 - User['cstm-zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-06-11 10:00:43,417 - User['cstm-livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-livy', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,419 - User['cstm-oozie'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['cstm-users', 'hadoop'], 'uid': None}
    2018-06-11 10:00:43,422 - User['cstm-spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'cstm-spark'], 'uid': None}
    2018-06-11 10:00:43,424 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,549 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-06-11 10:00:43,558 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-06-11 10:00:43,558 - Directory['/tmp/hbase-hbase'] {'owner': 'cstm-hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:43,696 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,819 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-06-11 10:00:43,942 - call['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase'] {}
    2018-06-11 10:00:43,953 - call returned (0, '1817')
    2018-06-11 10:00:43,954 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase /home/cstm-hbase,/tmp/cstm-hbase,/usr/bin/cstm-hbase,/var/log/cstm-hbase,/tmp/hbase-hbase 1817'] {'not_if': '(test $(id -u cstm-hbase) -gt 1000) || (false)'}
    2018-06-11 10:00:43,961 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh cstm-hbase /home/cstm-hbase,/tmp/cstm-hbase,/usr/bin/cstm-hbase,/var/log/cstm-hbase,/tmp/hbase-hbase 1817'] due to not_if
    2018-06-11 10:00:43,962 - Group['hdfs'] {}
    2018-06-11 10:00:43,963 - User['cstm-hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-06-11 10:00:43,964 - FS Type: HDFS
    2018-06-11 10:00:43,964 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-06-11 10:00:44,026 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'root', 'group': 'hadoop'}
    2018-06-11 10:00:44,116 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-06-11 10:00:44,225 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}
    2018-06-11 10:00:44,235 - Skipping Execute[('setenforce', '0')] due to not_if
    2018-06-11 10:00:44,236 - Directory['/grid/0/log/hdfs'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:44,436 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}
    2018-06-11 10:00:44,595 - Directory['/tmp/hadoop-cstm-hdfs'] {'owner': 'cstm-hdfs', 'create_parents': True, 'cd_access': 'a'}
    2018-06-11 10:00:44,719 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'root'}
    2018-06-11 10:00:44,807 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'root'}
    2018-06-11 10:00:44,897 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:45,020 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-metrics2.properties'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:45,112 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
    2018-06-11 10:00:45,233 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/configuration.xsl'] {'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:45,306 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'cstm-hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:45,409 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
    2018-06-11 10:00:45,545 - Skipping unlimited key JCE policy check and setup since the Java VM is not managed by Ambari
    2018-06-11 10:00:46,488 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:46,490 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:46,614 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1469/hadoop/conf
    2018-06-11 10:00:46,658 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}
    2018-06-11 10:00:46,721 - File['/etc/security/limits.d/hdfs.conf'] {'content': Template('hdfs.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}
    2018-06-11 10:00:46,836 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs_dn_jaas.conf'] {'content': Template('hdfs_dn_jaas.conf.j2'), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:46,925 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs_nn_jaas.conf'] {'content': Template('hdfs_nn_jaas.conf.j2'), 'owner': 'cstm-hdfs', 'group': 'hadoop'}
    2018-06-11 10:00:47,011 - XmlConfig['hadoop-policy.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,026 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-policy.xml
    2018-06-11 10:00:47,026 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hadoop-policy.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,118 - XmlConfig['ssl-client.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,132 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-client.xml
    2018-06-11 10:00:47,133 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-client.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,234 - Directory['/usr/hdp/3.0.0.0-1469/hadoop/conf/secure'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'cd_access': 'a'}
    2018-06-11 10:00:47,489 - XmlConfig['ssl-client.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf/secure', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,503 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/secure/ssl-client.xml
    2018-06-11 10:00:47,504 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/secure/ssl-client.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,602 - XmlConfig['ssl-server.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {}, 'configurations': ...}
    2018-06-11 10:00:47,615 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-server.xml
    2018-06-11 10:00:47,615 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ssl-server.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,712 - XmlConfig['hdfs-site.xml'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'configuration_attributes': {u'final': {u'dfs.datanode.failed.volumes.tolerated': u'true', u'dfs.datanode.data.dir': u'true', u'dfs.namenode.http-address': u'true', u'dfs.namenode.name.dir': u'true', u'dfs.webhdfs.enabled': u'true'}}, 'configurations': ...}
    2018-06-11 10:00:47,725 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs-site.xml
    2018-06-11 10:00:47,725 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/hdfs-site.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
    2018-06-11 10:00:47,889 - XmlConfig['core-site.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'xml_include_file': None, 'mode': 0644, 'configuration_attributes': {u'final': {u'fs.defaultFS': u'true'}}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:47,906 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml
    2018-06-11 10:00:47,906 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0644, 'encoding': 'UTF-8'}
    2018-06-11 10:00:48,038 - Writing File['/usr/hdp/3.0.0.0-1469/hadoop/conf/core-site.xml'] because contents don't match
    2018-06-11 10:00:48,093 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/slaves'] {'content': Template('slaves.j2'), 'owner': 'root'}
    2018-06-11 10:00:48,181 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,214 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,287 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,304 - Repository['HDP-3.0-GPL-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469', 'action': ['create'], 'components': [u'HDP-GPL', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,329 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-1]\nname=HDP-3.0-GPL-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,397 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,417 - Repository['HDP-UTILS-1.1.0.22-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-06-11 10:00:48,446 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-3.0-GPL-repo-1]\nname=HDP-3.0-GPL-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-1469\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-1]\nname=HDP-UTILS-1.1.0.22-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-06-11 10:00:48,518 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-06-11 10:00:48,540 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-1469 -> 3.0.0.0-1469
    2018-06-11 10:00:48,551 - Package['lzo'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,196 - Skipping installation of existing package lzo
    2018-06-11 10:00:49,196 - Package['hadooplzo_3_0_0_0_1469'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,402 - Skipping installation of existing package hadooplzo_3_0_0_0_1469
    2018-06-11 10:00:49,402 - Package['hadooplzo_3_0_0_0_1469-native'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-06-11 10:00:49,857 - Skipping installation of existing package hadooplzo_3_0_0_0_1469-native
    2018-06-11 10:00:49,861 - Directory['/grid/0/hadoop/hdfs/namenode'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
    2018-06-11 10:00:50,128 - Directory['/usr/lib/ambari-logsearch-logfeeder/conf'] {'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
    2018-06-11 10:00:50,315 - Generate Log Feeder config file: /usr/lib/ambari-logsearch-logfeeder/conf/input.config-hdfs.json
    2018-06-11 10:00:50,315 - File['/usr/lib/ambari-logsearch-logfeeder/conf/input.config-hdfs.json'] {'content': Template('input.config-hdfs.json.j2'), 'mode': 0644}
    2018-06-11 10:00:50,410 - Skipping setting up secure ZNode ACL for HFDS as it's supported only for NameNode HA mode.
    2018-06-11 10:00:50,415 - Called service start with upgrade_type: None
    2018-06-11 10:00:50,415 - HDFS: Setup ranger: command retry not enabled thus skipping if ranger admin is down !
    2018-06-11 10:00:50,417 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-client'] {'timeout': 20}
    2018-06-11 10:00:50,450 - call returned (0, 'hadoop-client - 3.0.0.0-1469')
    2018-06-11 10:00:50,451 - RangeradminV2: Skip ranger admin if it's down !
    2018-06-11 10:00:50,493 - checked_call['/usr/bin/kinit -c /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147 -kt /etc/security/keytabs/nn.service.keytab nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM > /dev/null'] {'user': 'cstm-hdfs'}
    2018-06-11 10:00:50,611 - checked_call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,612 - call['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'curl --location-trusted -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/b6b261de-4ab4-4c87-a271-bbaa9fc306f4 -c /var/lib/ambari-agent/tmp/cookies/b6b261de-4ab4-4c87-a271-bbaa9fc306f4 -w '""'""'%{http_code}'""'""' http://ctr-e138-1518143905142-357962-01-000006.hwx.site:6080/login.jsp --connect-timeout 10 --max-time 12 -o /dev/null 1>/tmp/tmpzBm4Ow 2>/tmp/tmprJvRDV''] {'quiet': False, 'env': {'KRB5CCNAME': '/var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'}}
    2018-06-11 10:00:50,729 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,729 - get_user_call_output returned (0, u'200', u'  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3630  100  3630    0     0  1954k      0 --:--:-- --:--:-- --:--:-- 3544k')
    2018-06-11 10:00:50,731 - call['/usr/bin/klist -s /var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'] {'user': 'cstm-hdfs'}
    2018-06-11 10:00:50,843 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,844 - call['ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'curl --location-trusted -k --negotiate -u : -b /var/lib/ambari-agent/tmp/cookies/49ac670c-d187-4b4c-8c13-32bc9f8ac060 -c /var/lib/ambari-agent/tmp/cookies/49ac670c-d187-4b4c-8c13-32bc9f8ac060 '""'""'http://ctr-e138-1518143905142-357962-01-000006.hwx.site:6080/service/public/v2/api/service?serviceName=cl1_hadoop&serviceType=hdfs&isEnabled=true'""'""' --connect-timeout 10 --max-time 12 -X GET 1>/tmp/tmpskeV7D 2>/tmp/tmpIUYUU7''] {'quiet': False, 'env': {'KRB5CCNAME': '/var/lib/ambari-agent/tmp/curl_krb_cache/ranger_admin_calls_cstm-hdfs_cc_dadca887d91334850d23f3a2088dac346b6b85f0706813c3f2212147'}}
    2018-06-11 10:00:50,984 - call returned (0, '######## Hortonworks #############\nThis is MOTD message, added for testing in qe infra')
    2018-06-11 10:00:50,985 - get_user_call_output returned (0, u'[{""id"":2,""guid"":""92620a51-bd3f-44f1-aed6-29a7c80809ec"",""isEnabled"":true,""createdBy"":""cstm-hdfs"",""updatedBy"":""cstm-hdfs"",""createTime"":1528682426000,""updateTime"":1528682426000,""version"":1,""type"":""hdfs"",""name"":""cl1_hadoop"",""description"":""hdfs repo"",""configs"":{""commonNameForCertificate"":""-"",""dfs.secondary.namenode.kerberos.principal"":""nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""hadoop.security.authentication"":""kerberos"",""hadoop.security.auth_to_local"":""RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/\\nRULE:[1:$1@$0](cstm-hbase@EXAMPLE.COM)s/.*/cstm-hbase/\\nRULE:[1:$1@$0](cstm-hdfs@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[1:$1@$0](cstm-spark@EXAMPLE.COM)s/.*/cstm-spark/\\nRULE:[1:$1@$0](cstm-zeppelin@EXAMPLE.COM)s/.*/cstm-zeppelin/\\nRULE:[1:$1@$0](yarn-ats@EXAMPLE.COM)s/.*/yarn-ats/\\nRULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//\\nRULE:[2:$1@$0](activity_analyzer@EXAMPLE.COM)s/.*/activity_analyzer/\\nRULE:[2:$1@$0](activity_explorer@EXAMPLE.COM)s/.*/activity_explorer/\\nRULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](amsmon@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/cstm-ams/\\nRULE:[2:$1@$0](atlas@EXAMPLE.COM)s/.*/cstm-atlas/\\nRULE:[2:$1@$0](ats-hbase@EXAMPLE.COM)s/.*/yarn-ats/\\nRULE:[2:$1@$0](cstm-knox@EXAMPLE.COM)s/.*/cstm-knox/\\nRULE:[2:$1@$0](cstm-livy@EXAMPLE.COM)s/.*/cstm-livy/\\nRULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/cstm-hbase/\\nRULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/cstm-hive/\\nRULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/cstm-mr/\\nRULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/cstm-yarn/\\nRULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/cstm-hdfs/\\nRULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/cstm-oozie/\\nRULE:[2:$1@$0](rangeradmin@EXAMPLE.COM)s/.*/cstm-ranger/\\nRULE:[2:$1@$0](rangertagsync@EXAMPLE.COM)s/.*/rangertagsync/\\nRULE:[2:$1@$0](rangerusersync@EXAMPLE.COM)s/.*/rangerusersync/\\nRULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/cstm-yarn/\\nRULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/cstm-yarn/\\nDEFAULT"",""dfs.datanode.kerberos.principal"":""dn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""tag.download.auth.users"":""cstm-hdfs"",""password"":""*****"",""policy.download.auth.users"":""cstm-hdfs"",""hadoop.rpc.protection"":""authentication"",""dfs.namenode.kerberos.principal"":""nn/ctr-e138-1518143905142-357962-01-000006.hwx.site@EXAMPLE.COM"",""fs.default.name"":""hdfs://ctr-e138-1518143905142-357962-01-000006.hwx.site:8020"",""hadoop.security.authorization"":""true"",""username"":""hadoop""},""policyVersion"":3,""policyUpdateTime"":1528682427000,""tagVersion"":1,""tagUpdateTime"":1528682426000}]', u'  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r  0     0    0  2603    0     0  92268      0 --:--:-- --:--:-- --:--:-- 92268')
    2018-06-11 10:00:50,986 - Hdfs Repository cl1_hadoop exist
    2018-06-11 10:00:50,989 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-security.xml'] {'content': InlineTemplate(...), 'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:51,064 - Writing File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-security.xml'] because contents don't match
    2018-06-11 10:00:51,124 - Directory['/etc/ranger/cl1_hadoop'] {'owner': 'cstm-hdfs', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:51,306 - Directory['/etc/ranger/cl1_hadoop/policycache'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-06-11 10:00:51,515 - File['/etc/ranger/cl1_hadoop/policycache/hdfs_cl1_hadoop.json'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0644}
    2018-06-11 10:00:51,605 - XmlConfig['ranger-hdfs-audit.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,617 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-audit.xml
    2018-06-11 10:00:51,618 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-audit.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:51,750 - XmlConfig['ranger-hdfs-security.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,767 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-security.xml
    2018-06-11 10:00:51,768 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-hdfs-security.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:51,892 - XmlConfig['ranger-policymgr-ssl.xml'] {'group': 'hadoop', 'conf_dir': '/usr/hdp/3.0.0.0-1469/hadoop/conf', 'mode': 0744, 'configuration_attributes': {}, 'owner': 'cstm-hdfs', 'configurations': ...}
    2018-06-11 10:00:51,905 - Generating config: /usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-policymgr-ssl.xml
    2018-06-11 10:00:51,905 - File['/usr/hdp/3.0.0.0-1469/hadoop/conf/ranger-policymgr-ssl.xml'] {'owner': 'cstm-hdfs', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': 0744, 'encoding': 'UTF-8'}
    2018-06-11 10:00:52,023 - Execute[(u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/ranger_credential_helper.py', '-l', u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/install/lib/*', '-f', '/etc/ranger/cl1_hadoop/cred.jceks', '-k', 'sslKeyStore', '-v', [PROTECTED], '-c', '1')] {'logoutput': True, 'environment': {'JAVA_HOME': u'/usr/lib/jvm/java-openjdk'}, 'sudo': True}
    Using Java:/usr/lib/jvm/java-openjdk/bin/java
    Alias sslKeyStore created successfully!
    2018-06-11 10:00:53,259 - Execute[(u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/ranger_credential_helper.py', '-l', u'/usr/hdp/3.0.0.0-1469/ranger-hdfs-plugin/install/lib/*', '-f', '/etc/ranger/cl1_hadoop/cred.jceks', '-k', 'sslTrustStore', '-v', [PROTECTED], '-c', '1')] {'logoutput': True, 'environment': {'JAVA_HOME': u'/usr/lib/jvm/java-openjdk'}, 'sudo': True}
    Using Java:/usr/lib/jvm/java-openjdk/bin/java
    Alias sslTrustStore created successfully!
    2018-06-11 10:00:54,461 - File['/etc/ranger/cl1_hadoop/cred.jceks'] {'owner': 'cstm-hdfs', 'group': 'hadoop', 'mode': 0640}
    2018-06-11 10:00:54,556 - File['/etc/ranger/cl1_hadoop/.cred.jceks.crc'] {'owner': 'cstm-hdfs', 'only_if': 'test -e /etc/ranger/cl1_hadoop/.cred.jceks.crc', 'group': 'hadoop', 'mode': 0640}
    2018-06-11 10:00:54,652 - File['/etc/hadoop/conf/dfs.exclude'] {'owner': 'cstm-hdfs', 'content': Template('exclude_hosts_list.j2'), 'group': 'hadoop'}
    2018-06-11 10:00:54,749 - call[('ls', u'/grid/0/hadoop/hdfs/namenode')] {}
    2018-06-11 10:00:54,756 - call returned (0, '')
    2018-06-11 10:00:54,757 - Execute['ls /grid/0/hadoop/hdfs/namenode | wc -l  | grep -q ^0$'] {}
    2018-06-11 10:00:54,765 - Execute['hdfs --config /usr/hdp/3.0.0.0-1469/hadoop/conf namenode -format -nonInteractive'] {'logoutput': True, 'path': ['/usr/hdp/3.0.0.0-1469/hadoop/bin'], 'user': 'cstm-hdfs'}
    ######## Hortonworks #############
    This is MOTD message, added for testing in qe infra
    WARNING: /var/run/hadoop/cstm-hdfs does not exist. Creating.
    mkdir: cannot create directory ‘/var/run/hadoop/cstm-hdfs’: Permission denied
    ERROR: Unable to create /var/run/hadoop/cstm-hdfs. Aborting.
    
    Command failed after 1 tries
    

Artifacts:

    
    
    
    http://testqelog.s3.amazonaws.com/qelogs/nat/107592/ambari-blueprints/split-6/nat-yc-r7-gfgs-ambari-blueprints-6/log_tree/index.html
    

",pull-request-available,[],AMBARI,Bug,Major,2018-06-11 11:45:30,0
13165295,Final flag (blue icon) is not set for Final properties in Settings tab,"Please see screenshot attached
- We have final property set in Settings tab as well as Advanced tab.
- The one in Settings tab doesn't have blue icon where as it should be
- While setting a property to final we should be seeing blue icon which is not happening in Settings tab. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-11 10:54:43,4
13164958,Command retry on server when agent-server connection drops,Server should try to re-send execution command message when agent does not confirmed successful receiving.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-08 15:55:20,1
13164883,Autostart is not working for TIMELINE_READER. ,"Steps to reproduce:-  
1\. Enable Autostart for all components.  
2.restart one host which contain TIMELINE_READER component.  
3\. wait for 5-10 minute for autostart to take an effect

verify from the logs  
tail -f /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py

    
    
    
    INFO 2018-06-07 08:32:32,487 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:34,488 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:36,489 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:38,490 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:40,491 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:42,496 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:44,501 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:46,505 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:48,518 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:50,519 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:52,521 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:54,533 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:56,534 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:32:58,535 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:33:00,537 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    INFO 2018-06-07 08:33:02,538 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
    

repro cluster:-<http://172.22.107.230:8080/>  
repro job:- <http://linux-jenkins.qe.hortonworks.com:8080/job/Nightly-Start-
EC2-Run-HDP/965535/>

",pull-request-available,[],AMBARI,Bug,Major,2018-06-08 09:15:52,0
13164752,Test Kerberos Client fail after reenter of right realm value,"Test Kerberos Client fail after reenter of right realm value during kerberos enabling

STR:
1)Start of kerberos enabling
2)Select existing MIT
3)Go to Configure Kerberos page
4)Set wrong ""Realm name"" value. As an example: ""example.com"" (expected EXAMPLE.COM)
5)Go to Install and Test Kerberos Client step
6)Wait of Test Kerberos Client fail
7)Back to Configure Kerberos step
8)Enter right ""Realm name"" value 
9)Go to Install and Test Kerberos Client step

Expected:
Test Kerberos Client pass successfully

Actual:
Test Kerberos Client fail

The supposed main reason is using old realm value for principals creation:
{code:java}
2018-05-30 11:13:38,895 - Failed to create principal, cl1-053018@example.com - Failed to create service principal for cl1-053018@example.com
STDOUT: Authenticating as principal admin/admin@EXAMPLE.COM with existing credentials.

STDERR: WARNING: no policy specified for cl1-053018@example.com; defaulting to no policy
add_principal: No such entry in the database while creating ""cl1-053018@example.com"".

Administration credentials NOT DESTROYED.
{code}
This is happening only using MySQL.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-07 18:07:10,6
13164723,Add Kerberos-related configuration recommendations to the stack advisor,"Add Kerberos-related configuration recommendations to the stack advisor.
* Add a new action, {{recommend-configurations-for-kerberos}}, to query services for only Kerberos-related configuration changes
",pull-request-available stackadvisor,['ambari-server'],AMBARI,Bug,Major,2018-06-07 15:50:52,7
13164668,Save button is disabled after adding the custom property,"Save button is disabled after adding the custom property

STR:
# Navigate to Ranger page
# Go to advanced tab of config page
# Add custom property

Expected:
Save button is enabled

Actual:
Save button is disabled",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-06-07 11:40:19,2
13164448,Increase outbound message buffer size,Make websocket outbound message buffer size configurable to prevent limit exceeding for large messages (agent configurations stomp updates).,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-06 12:48:24,1
13164434,Kafka Service Check failed on https enabled/WE cluster after Ambari Upgrade. Error : Failed to check that topic exists,"When delete.topic.enable is set to false, ambari service check is not able to delete the topic and fails.",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-06-06 11:38:48,3
13164246,HDFS Metrics shows all blocks as 'Corrupt Replicas',"*STR*
# Deployed a cluster with HDP-3.0 and Ambari-2.7 (or upgrade from HDP-2.6 to HDP-3.0)
# Go to HDFS Summary page
# Observe the count of blocks under 'Corrupt Replica'

*Result*
All blocks show as corrupt",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-05 18:21:16,2
13164230,Alerts label isn't clickable at the service page after turning on mm for service,"Alerts label isn't clickable at the service page after turning on mm for service

STR:
# Go to the service page
# Enable mm for service
# Try to click alerts(bell) icon for the current service

Expected:
Will open alerts window

Actual
Nothing happens",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-05 17:29:31,2
13163873,Display Mpack Info on Service Summary Page,"At present service summary page displays ""Components Status Summary"" and ""Service Metrics"". We should also display Mpack Info in the Summary Page as follows

{code}
Summary:
    Mpack:   HDPCORE (1.0.0-b123)
    Components:    
                              Started           Started
                              NAMENODE          SNAMENODE
    Service Metrics:
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-04 14:48:48,4
13163814,AutoStart Is not working for some of the components in the cluster,"Autostart is not working for the following components:-

TIMELINE_READER
 HIVE_SERVER
 SPARK2_THRIFTSERVER
 HISTORYSERVER
 HBASE_MASTER
 ZEPPELIN_MASTER
 OOZIE_SERVER
 APP_TIMELINE_SERVER
 SPARK2_JOBHISTORYSERVER

 {code}

[root@re-nat-yc-r7-nubs-ambari-autostart-1-1 ~]# tail -f /var/log/ambari-agent/ambari-agent.log |grep -i RecoveryManager.py
 INFO 2018-06-04 11:31:51,233 RecoveryManager.py:213 - TIMELINE_READER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,236 RecoveryManager.py:213 - SPARK2_THRIFTSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,238 RecoveryManager.py:213 - ZEPPELIN_MASTER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,250 RecoveryManager.py:213 - HISTORYSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,251 RecoveryManager.py:213 - HIVE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,254 RecoveryManager.py:213 - HBASE_MASTER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,255 RecoveryManager.py:213 - APP_TIMELINE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,257 RecoveryManager.py:213 - HIVE_METASTORE needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,258 RecoveryManager.py:213 - SPARK2_JOBHISTORYSERVER needs recovery, desired = STARTED, and current = INSTALLED.
 INFO 2018-06-04 11:31:51,261 RecoveryManager.py:213 - OOZIE_SERVER needs recovery, desired = STARTED, and current = INSTALLED.

 {code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-06-04 12:11:57,0
13163811,Atlas fails to start in an unsecure cluster after Ambari Upgrade . Error - Unable to run the custom hook script,"1) Deployed cluster with Ambari version: 2.6.2.0-155 and HDP version: 2.6.5.0-292 in an unsecure cluster.
2) Upgrade Ambari to 2.7.0.0-630

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py"", line 35, in <module>
    BeforeAnyHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py"", line 29, in hook
    setup_users()
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/shared_initialization.py"", line 50, in setup_users
    groups = params.user_to_groups_dict[user],
KeyError: u'livy'
Error: Error: Unable to run the custom hook script ['/usr/bin/python', '/var/lib/ambari-agent/cache/stack-hooks/before-ANY/scripts/hook.py', 'ANY', '/var/lib/ambari-agent/data/command-232.json', '/var/lib/ambari-agent/cache/stack-hooks/before-ANY', '/var/lib/ambari-agent/data/structured-out-232.json', 'INFO', '/var/lib/ambari-agent/tmp', 'PROTOCOL_TLSv1_2', '']
{code}",pull-request-available,[],AMBARI,Improvement,Major,2018-06-04 11:57:04,3
13163806,Remove HDFS Disk Usage widget from host summary page,"There are NameNode widgets on host summary page which are specific for the namespace containing this host. Since HDFS DIsk Usage metrics is an aggregate from all the namespaces, the corresponding widget on host page is confusing and should be removed from there.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-04 11:36:50,2
13163803,Upgrade Task details is not loading in Upgrade History Details Window.,"On Clicking Upgrade on upgrade history tab under Stacks and Versions , details (logs etc) are not loading for upgrade tasks. JS error is seen in the console. Attaching screenshot .",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-06-04 11:20:42,4
13163243,Duplicate data is showing up when enabling NN Federation,"We need to make a few changes to the HDFS Summary screen when NN Federation is enabled.  We're seeing the HDFS fs usage data duplicated in each namespace, when it should be separated out in a new section.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-31 18:42:40,2
13163125,Server metadata cache does not have valid hash after updates with no any registered agent,Server metadata cache should always have actual hash.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-31 12:02:54,1
13163100,Ambari server installation failing on debian cluster with mysql db,"Ambari server install is failing at db create phase
{noformat}
2018-05-28 09:49:40,565|executor.py.165|DEBUG|13917|MainThread|172.27.17.22|executing the command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql'
2018-05-28 09:49:40,822|executor.py.165|DEBUG|13917|Thread-165|stdout: mysql: [Warning] Using a password on the command line interface can be insecure.
2018-05-28 09:49:40,822|executor.py.165|DEBUG|13917|Thread-165|stdout: ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
2018-05-28 09:49:40,822|executor.py.128|ERROR|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution failed with exitcode=1
2018-05-28 09:49:40,823|executor.py.116|INFO|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution re-try #1
2018-05-28 09:49:41,250|executor.py.165|DEBUG|13917|Thread-166|stdout: mysql: [Warning] Using a password on the command line interface can be insecure.
2018-05-28 09:49:41,250|executor.py.165|DEBUG|13917|Thread-166|stdout: ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
2018-05-28 09:49:41,251|executor.py.128|ERROR|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution failed with exitcode=1
2018-05-28 09:49:41,252|executor.py.116|INFO|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution re-try #2
2018-05-28 09:49:41,561|executor.py.165|DEBUG|13917|Thread-167|stdout: mysql: [Warning] Using a password on the command line interface can be insecure.
2018-05-28 09:49:41,561|executor.py.165|DEBUG|13917|Thread-167|stdout: ERROR 1067 (42000) at line 330 in file: '/tmp/Ambari-DDL-MySQL-CREATE.sql': Invalid default value for 'update_time'
2018-05-28 09:49:41,561|executor.py.128|ERROR|13917|MainThread|command='mysql --user=ambaricustomuser --password=bigdatacustom < /tmp/createDB.sql' execution failed with exitcode=1
{noformat}
Ambari server version - 2.7.0.0-568
Mysql version - 5.7.21

The root cause is (per documentation):
{noformat}
NOW() (or its synonyms) can be used as the default value for TIMESTAMP columns as well as,
since MariaDB 10.0.1, DATETIME columns. Before MariaDB 10.0.1, it was only possible for a single TIMESTAMP 
column per table to contain the CURRENT_TIMESTAMP as its default.
{noformat}
In _user_authentication_ there is another TIMESTAMP column with default value of CURRENT_TIMESTAMP called '_create_time_' and this causes the issue in older versions (< v10).

For the sake of consistency the following columns' type should be changed to BIGINT (defaulting to 0):
 * users.create_time
 * user_authentication.create_time
 * user_authentication.update_time

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-31 08:50:43,6
13163098,stack advisor error : UnboundLocalError: local variable 'host' referenced before assignment,"running hive stack advisor when tez + druid is installed gives

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 178, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 132, in main
    result = stackAdvisor.validateConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1059, in validateConfigurations
    validationItems = self.getConfigurationsValidationItems(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1446, in getConfigurationsValidationItems
    recommendations = self.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1608, in recommendConfigurations
    servicesList)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1549, in recommendConfigGroupsConfigurations
    serviceAdvisor.getServiceConfigurationRecommendations(configurations, cgClusterSummary, cgServices, cgHosts)
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/HIVE/service_advisor.py"", line 127, in getServiceConfigurationRecommendations
    recommender.recommendHiveConfigurationsFromHDP30(configurations, clusterData, services, hosts)
  File ""/var/lib/ambari-server/resources/stacks/HDP/3.0/services/HIVE/service_advisor.py"", line 196, in recommendHiveConfigurationsFromHDP30
    druid_coordinator_host_port = str(host['Hosts']['host_name']) + "":"" + str(
UnboundLocalError: local variable 'host' referenced before assignment
{code}",pull-request-available,['stacks'],AMBARI,Improvement,Major,2018-05-31 08:45:40,3
13163074,"""Ambari persisted credential store"" pre check appearing in Patch Upgrade.",We should add logic to the pre req to see if the regen keytab task is present in the upgrade pack,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-05-31 07:01:56,3
13163009,credential_store_helper.get_password_from_credential_store does not return the correct password string,"credential_store_helper.get_password_from_credential_store does not return the correct password string. 

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-30 21:35:14,7
13162971,Host components API call doesn't return all host components,"Some host components are missing from the list response, but can be queried individually:

{noformat}
$ curl ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components"" | jq -r '.items[].href'
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/1
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/2
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/3
http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/4
$ curl --head ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/5""
HTTP/1.1 200 OK
$ curl --head ""http://c7401:8080/api/v1/clusters/TEST/hosts/c7401/host_components/6""
HTTP/1.1 200 OK
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-30 18:49:17,5
13162898,Agent can send too large websocket message with command output.,Agent should properly limit report size instead of limiting line count.,pull-request-available,['ambari-agent'],AMBARI,Bug,Blocker,2018-05-30 13:32:41,1
13162861,Express Upgrade: Clicking on upgrade item shows no tasks on large clusters,"There are two issues over here.
*1st issue:*
This seems to be a functional bug. As it seems from the attached screenshot that even after API call was completed content was not rendered.

*2nd issue:*
There seems to be a performance issue. UI asks for all tasks fields instead of only ones that are needed making api call really expensive. Expected behavior is that only status should be asked for and when user clicks on a task then  log for that task should be requested. ",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-30 10:17:33,4
13162208,Spark2 Thrift Server fails to start if LLAP and parallel execution are enabled,"STR:

# Enable parallel execution in Ambari Agent
# Deploy cluster via blueprint, placing Spark2 Thrift Server on the same node as Hive Server Interactive

Result: Spark2 Thrift Server runs into the following error during startup:

{noformat}
ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper
...
KeeperErrorCode = NoNode for /hiveserver2-hive2
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-26 06:01:20,5
13162111,YARN start failed during EU with IllegalArgumentException,"*STR*
# Deployed cluster with Ambari version: 2.6.0.0-267 and HDP version: 2.6.0.3-8 (UI based install and cluster was kerberized)
# Upgrade Ambari to Target Version: 2.7.0.0-588
# Start EU to HDP-3.0.0.0-1390

*Result*
Observed following error at Timeline Reader v2 start:
{code}
2018-05-25 11:47:44,940 INFO  timeline.RollingLevelDBTimelineStore (RollingLevelDBTimelineStore.java:run(404)) - Deletion thread received interrupt, exiting
2018-05-25 11:47:44,941 ERROR applicationhistoryservice.ApplicationHistoryServer (ApplicationHistoryServer.java:launchAppHistoryServer(180)) - Error starting ApplicationHistoryServer
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: AHSWebApp failed to start.
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.startWebApp(ApplicationHistoryServer.java:322)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceStart(ApplicationHistoryServer.java:121)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:178)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:187)
Caused by: java.io.IOException: Problem starting http server
        at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1165)
        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.startWebApp(ApplicationHistoryServer.java:313)
        ... 4 more
Caused by: java.lang.IllegalArgumentException: Could not parse [${clusterHostInfo/rm_host}]
        at org.apache.commons.net.util.SubnetUtils.calculate(SubnetUtils.java:275)
        at org.apache.commons.net.util.SubnetUtils.<init>(SubnetUtils.java:51)
        at org.apache.hadoop.util.MachineList.<init>(MachineList.java:108)
        at org.apache.hadoop.util.MachineList.<init>(MachineList.java:82)
        at org.apache.hadoop.util.MachineList.<init>(MachineList.java:74)
        at org.apache.hadoop.security.authorize.DefaultImpersonationProvider.init(DefaultImpersonationProvider.java:98)
        at org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration(ProxyUsers.java:75)
        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:202)
        at org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter.init(TimelineAuthenticationFilter.java:47)
        at org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:139)
        at org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:873)
{code}
",kerberos pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Major,2018-05-25 15:36:47,7
13162061,Selection information disappears when a filter is applied,"As part of AMBARI-23911, selection information was added back to Ambari. But when a filter is applied, the selected hosts section is removed and refreshed. In previous versions, the selection information persisted even when a filter was applied.
Another issue faced while automating is that there is no unique xpath to the selection information. It would be great if a class/data-qa attribute is added to the anchor tag.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-25 12:12:42,2
13162053,Next button enabled when invalid values entered,"STR:
Enter invalid value on Configs page for any property.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-25 11:44:03,2
13161698,Hiveserver2 fails to start on viewFS enabled cluster: {hive_server2_zookeeper_namespace} is not ready yet,"  * Tried to deploy a cluster with viewFS enabled via blueprint
  * HiveServer2 is failing to start with below error 
    
        
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 137, in 
        HiveServer().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_server.py"", line 53, in start
        hive_service('hiveserver2', action = 'start', upgrade_type=upgrade_type)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_service.py"", line 102, in hive_service
        wait_for_znode()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 62, in wrapper
        return function(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HIVE/package/scripts/hive_service.py"", line 191, in wait_for_znode
        raise Fail(format(""ZooKeeper node /{hive_server2_zookeeper_namespace} is not ready yet""))
    resource_management.core.exceptions.Fail: ZooKeeper node /hiveserver2 is not ready yet
    

out log has a reference to hdfs:// Filesystem

    
        
    2018-05-17 23:54:17,398 - Skipping fs root check as fs_root does not start with hdfs://
    2018-05-17 23:54:17,399 - Execute['/var/lib/ambari-agent/tmp/start_hiveserver2_script /grid/0/log/hive/hive-server2.out /grid/0/log/hive/hive-server2.err /var/run/hive/hive-server.pid /usr/hdp/current/hive-server2/conf/ /grid/0/log/hive'] {'environment': {'HIVE_BIN': 'hive', 'JAVA_HOME': u'/usr/lib/jvm/java-openjdk', 'HADOOP_HOME': u'/usr/hdp/current/hadoop-client'}, 'not_if': 'ls /var/run/hive/hive-server.pid >/dev/null 2>&1 && ps -p  >/dev/null 2>&1', 'user': 'hive', 'path': [u'/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/var/lib/ambari-agent:/usr/hdp/current/hive-server2/bin:/usr/hdp/3.0.0.0-1345/hadoop/bin']}
    2018-05-17 23:54:17,507 - Execute['/usr/lib/jvm/java-openjdk/bin/java -cp /usr/lib/ambari-agent/DBConnectionVerification.jar:/usr/hdp/current/hive-server2/lib/mysql-connector-java.jar org.apache.ambari.server.DBConnectionVerification 'jdbc:mysql://ctr-e138-1518143905142-317960-01-000006.hwx.site/hivedb?createDatabaseIfNotExist=true' hiveuser [PROTECTED] com.mysql.jdbc.Driver'] {'path': ['/usr/sbin:/sbin:/usr/local/bin:/bin:/usr/bin'], 'tries': 5, 'try_sleep': 10}
    2018-05-17 23:54:18,156 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server ctr-e138-1518143905142-317960-01-000005.hwx.site:2181,ctr-e138-1518143905142-317960-01-000006.hwx.site:2181,ctr-e138-1518143905142-317960-01-000007.hwx.site:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
    2018-05-17 23:54:19,032 - call returned (1, 'Node does not exist: /hiveserver2')
    2018-05-17 23:54:19,033 - Will retry 29 time(s), caught exception: ZooKeeper node /hiveserver2 is not ready yet. Sleeping for 5 sec(s)
    2018-05-17 23:54:24,038 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server ctr-e138-1518143905142-317960-01-000005.hwx.site:2181,ctr-e138-1518143905142-317960-01-000006.hwx.site:2181,ctr-e138-1518143905142-317960-01-000007.hwx.site:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
    2018-05-17 23:54:24,842 - call returned (1, 'Node does not exist: /hiveserver2')
    


",pull-request-available,[],AMBARI,Bug,Major,2018-05-24 08:51:16,0
13161607,Failure during re-installation of services,"Steps to reproduce :

1) Create a healthy cluster
2) Stop a service and delete it(say Kafka or Zookeeper)
3) Try to reinstall the same service and the action fails.

Seeing below error in ambari-agent.log
{code:java}
INFO 2018-05-21 10:17:56,852 ActionQueue.py:276 - Command execution metadata - taskId = 300, retry enabled = False, max retry duration (sec) = 0, log_output = True
ERROR 2018-05-21 10:17:56,852 CustomServiceOrchestrator.py:448 - Caught an exception while executing custom service command: <type 'exceptions.KeyError'>: u'KAFKA'; u'KAFKA'
Traceback (most recent call last):
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 322, in runCommand
    command = self.generate_command(command_header)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/CustomServiceOrchestrator.py"", line 487, in generate_command
    command_dict = self.configuration_builder.get_configuration(cluster_id, service_name, component_name, required_config_timestamp)
  File ""/usr/lib/ambari-agent/lib/ambari_agent/ConfigurationBuilder.py"", line 50, in get_configuration
    command_dict['serviceLevelParams'] = metadata_cache.serviceLevelParams[service_name]
KeyError: u'KAFKA'
INFO 2018-05-21 10:17:56,853 ActionQueue.py:324 - Quit retrying for command with taskId = 300. Status: FAILED, retryAble: False, retryDuration (sec): -1, last delay (sec): 1
INFO 2018-05-21 10:17:56,853 ActionQueue.py:339 - Command with taskId = 300 failed after 1 tries
{code}
Attaching the screenshot.

I have tried the same action for Zookeeper and it fails with similar exception.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-23 22:30:40,6
13161521,Reassign Master Wizard issues,"- On step 1, namespace ids are displayed next to hostname selects. When user selects hostname from other namespace, the displayed id isn't changed
- After completing the last step, page URL remains {{#/main/service/reassign/step6}}, with JS error thrown: {{app.js:212117 Uncaught TypeError: Cannot read property 'modal' of undefined}}. After page refresh user is broutght to the final step of wizard again, containing no relevant data",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-23 17:09:02,2
13161380,ambari ui fails to load due to js error,"Ambari UI is failing to load completely due to below js error

{code}
app.js:16513 Uncaught TypeError: Cannot read property 'get' of undefined
    at app.js:16513
    at Array.forEach (<anonymous>)
    at Class.getServiceVersionFromRepo (app.js:16494)
    at invokeAction (vendor.js:4833)
    at iterateSet (vendor.js:4815)
    at Object.sendEvent (vendor.js:4932)
    at notifyObservers (vendor.js:3524)
    at Object.Ember.notifyObservers (vendor.js:3639)
    at Object.propertyDidChange (vendor.js:4272)
    at ChainNodePrototype.chainDidChange (vendor.js:3987)
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-23 11:03:58,4
13161037,"Disabled alert status isn't ""NONE""","Disabled alert status isn't ""NONE""

STR:
1)Stop Hbase
2)Navigate to alerts page
3)Move state of first CRITICALalert to ""Disabled""

Expected:
Alert status is NONE

Actual:
Alert status is CRITICAL
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-22 11:59:52,4
13161029,Phantom null request shows up on the bg ops for Stop All SCH,Server sends STOMP request updates with null host_name for tasks.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-22 11:05:05,1
13160936,"If Kerberos is enabled, then stack upgrade prerequisite check should ensure the KDC admin credential is persisted","If Kerberos is enabled and Ambari is managing Kerberos identities, then the stack upgrade prerequisite check should ensure that the KDC admin credential is stored in the Ambari persisted credential store.

# The Ambari credential store must be set up (ambari-server setup-security, option #2)
# The KDC administrator credential is stored in the Ambari credential store (persisted)",kerberos pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-05-22 02:10:30,7
13160850,Ambari 2way SSL does not work if CA signed certs are used,"Enable 2 way SSL between Ambari server and agent using CA Signed certificates.  Communication fails with below error/Exception

{noformat}
ERROR 2018-05-21 15:57:35,357 Controller.py:226 - Unable to connect to: https://apappu4.hdp.com:8441/agent/v1/register/apappu4.hdp.com
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ambari_agent/Controller.py"", line 175, in registerWithServer
    ret = self.sendRequest(self.registerUrl, data)
  File ""/usr/lib/python2.6/site-packages/ambari_agent/Controller.py"", line 549, in sendRequest
    raise IOError('Request to {0} failed due to {1}'.format(url, str(exception)))
IOError: Request to https://apappu4.hdp.com:8441/agent/v1/register/apappu4.hdp.com failed due to [Errno 1] _ssl.c:492: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
ERROR 2018-05-21 15:57:35,357 Controller.py:227 - Error:Request to https://apappu4.hdp.com:8441/agent/v1/register/apappu4.hdp.com failed due to [Errno 1] _ssl.c:492: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
{noformat}


Root cause: As part of the setup - CA Root and CA Cert chains are imported to PKCS file. but Ambari server is not pushing these root/chain to Ambari agents and Agents are unable to trust the server certs.

*+Workaround:+*

Combine certs, Chains, root and then copy to agent hosts.

{noformat}

cat certchain.pem  servercert.pem root.pem  > caroot.pem
{noformat}

then copy this file to


{noformat}
cp caroot.pem /var/lib/ambari-agent/keys/ca.crt
{noformat}

Restarting agent should resolve the issue.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-21 23:06:04,7
13160682,"New UI Makes it hard to see that a host is in a ""Decommissioning"" state","When decommissioning a host in Ambari 2.6 - the button indicating the component state clearly shows ""Decommissioning"". With the change to the ... menu in Ambari 2.7, it's hard to see that the component is in a decommissioning state because the tooltip for the status indicator only shows ""Started"".

We need to change the tooltip from ""Started"" to ""Decommissioning"" when the host is in a decommissioning state.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-21 12:20:31,4
13160671,Selection information not present in Hosts page filtering,"In the new Ambari web UI, when a host is selected by selecting the checkbox, the number of items selected is not shown. This was shown in previous releases.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-21 11:19:12,2
13160399,ZKFC fails to start while moving Namenode on a cluster with multiple namespaces,"STR: 
- Deploy a cluster with 2 namespaces using blueprint.
- Use the UI wizard to move Active Namenode for namespace NS2
- Perform manual operations when prompted in the wizard (FormatZkfc on other 3 hosts, perform bootstrapStandby on the new NN)
- In the final step, start all services, ZKFC fails to start on the host where we moved the NN 

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py"", line 192, in <module>
    ZkfcSlave().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py"", line 71, in start
    ZkfcSlaveDefault.start_static(env, upgrade_type)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py"", line 96, in start_static
    create_log_dir=True
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/utils.py"", line 258, in service
    Execute(daemon_cmd, not_if=process_id_exists_command, environment=hadoop_env_exports)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/system.py"", line 263, in action_run
    returns=self.resource.returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy, returns=returns)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 308, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'ambari-sudo.sh su cstm-hdfs -l -s /bin/bash -c 'ulimit -c unlimited ;  /usr/hdp/3.0.0.0-1316/hadoop/bin/hdfs --config /usr/hdp/3.0.0.0-1316/hadoop/conf --daemon start zkfc'' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
WARNING: HADOOP_ZKFC_OPTS has been replaced by HDFS_ZKFC_OPTS. Using value of HADOOP_ZKFC_OPTS.
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 17:54:00,2
13160361,Heatmap Data Issues,"""Data Not Available"" in Dashboard / Heatmap for:
* Host Disk Space Used %
* Host Memory Used %

""Invalid Data"" in Dashboard / Heatmap for:
* DataNode Process Disk I/O Utilization   
* DataNode Process Network I/O Utilization

These should all be reporting data.

STR:
- Install ZooKeeper, HDFS, AMS, SmartSense
- Wait a 10 minutes to ensure data is coming into AMS
- Click on Dashboard / Heatmap and view these metrics",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 15:07:53,4
13160355,Server posts numerous redundant metadata updates to agent,Server should send metadata updates only when metadata was really changed.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-18 14:55:39,1
13160315,Configuration Validation is throwing errors on correct configuration,"During installation if you change configuration there are a lot of configuration validation errors that appear to be using > and not >=.  See screenshot.  This results in 20+ things for the user to review, but most of them are correct because the value is 19 and we're asking them to set it to 19.

Steps to Reproduce:
# Install Ambari 2.7
# Choose ZK, HDFS, YARN, Hive
# Double the *Minimum Container Size (Memory)* during the *All Configurations* step
# Observe that the configuration validations are mostly asking us to change values to what they already are",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-18 10:46:00,4
13160310,ZooKeepers Show As Down After EU to HDP 3.0 But They Are Not,"STR:

  * Perform an EU from HDP 2.6 to HDP 3.0

After, 2 of my 3 ZKs are shown as being down. However, they are actually alive
on my boxes:

    
    
    
    [root@c7402 ~]$ ps aux | grep [z]oo.cfg
    zookeep+ 22463  0.2  2.8 3064236 53728 ?       Sl   20:41   0:01 /usr/jdk64/jdk1.8.0_144/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.log.file=zookeeper-zookeeper-server-c7402.ambari.apache.org.log -Dzookeeper.root.logger=INFO,ROLLINGFILE -cp /usr/hdp/current/zookeeper-server/bin/../build/classes:/usr/hdp/current/zookeeper-server/bin/../build/lib/*.jar:/usr/hdp/current/zookeeper-server/bin/../lib/xercesMinimal-1.9.6.2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-provider-api-2.4.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-shared4-2.4.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-shared-1.0-beta-6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-lightweight-1.0-beta-6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-http-2.4.jar:/usr/hdp/current/zookeeper-server/bin/../lib/wagon-file-1.0-beta-6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/slf4j-api-1.6.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/plexus-utils-3.0.8.jar:/usr/hdp/current/zookeeper-server/bin/../lib/plexus-interpolation-1.11.jar:/usr/hdp/current/zookeeper-server/bin/../lib/plexus-container-default-1.0-alpha-9-stable-1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/netty-3.10.5.Final.jar:/usr/hdp/current/zookeeper-server/bin/../lib/nekohtml-1.9.6.2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-settings-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-repository-metadata-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-project-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-profile-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-plugin-registry-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-model-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-error-diagnostics-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-artifact-manager-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-artifact-2.2.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/maven-ant-tasks-2.1.3.jar:/usr/hdp/current/zookeeper-server/bin/../lib/log4j-1.2.16.jar:/usr/hdp/current/zookeeper-server/bin/../lib/jsoup-1.7.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/jline-0.9.94.jar:/usr/hdp/current/zookeeper-server/bin/../lib/commons-logging-1.1.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/commons-io-2.2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/commons-codec-1.6.jar:/usr/hdp/current/zookeeper-server/bin/../lib/classworlds-1.1-alpha-2.jar:/usr/hdp/current/zookeeper-server/bin/../lib/backport-util-concurrent-3.1.jar:/usr/hdp/current/zookeeper-server/bin/../lib/ant-launcher-1.8.0.jar:/usr/hdp/current/zookeeper-server/bin/../lib/ant-1.8.0.jar:/usr/hdp/current/zookeeper-server/bin/../zookeeper-3.4.6.3.0.0.0-1250.jar:/usr/hdp/current/zookeeper-server/bin/../src/java/lib/*.jar:/usr/hdp/current/zookeeper-server/conf::/usr/share/zookeeper/*:/usr/share/zookeeper/* -Xmx1024m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/hdp/current/zookeeper-server/conf/zoo.cfg
    
    [root@c7402 ~]$ telnet localhost 2181
    Trying ::1...
    Connected to localhost.
    Escape character is '^]'.
    ^CConnection closed by foreign host.
    

But you can see that we clearly think it's down on c7402:

    
    
    
    {
      ""href"" : ""http://localhost:8080/api/v1/clusters/c1/hosts/c7402.ambari.apache.org/host_components/ZOOKEEPER_SERVER"",
      ""HostRoles"" : {
        ""cluster_name"" : ""c1"",
        ""component_name"" : ""ZOOKEEPER_SERVER"",
        ""desired_repository_version"" : ""3.0.0.0-1250"",
        ""desired_stack_id"" : ""HDP-3.0"",
        ""desired_state"" : ""STARTED"",
        ""display_name"" : ""ZooKeeper Server"",
        ""host_name"" : ""c7402.ambari.apache.org"",
        ""maintenance_state"" : ""OFF"",
        ""public_host_name"" : ""c7402.ambari.apache.org"",
        ""reload_configs"" : false,
        ""service_name"" : ""ZOOKEEPER"",
        ""stale_configs"" : false,
        ""state"" : ""INSTALLED"",
        ""upgrade_state"" : ""NONE"",
        ""version"" : ""3.0.0.0-1250"",
        ""actual_configs"" : { }
      },
      ""host"" : {
        ""href"" : ""http://localhost:8080/api/v1/clusters/c1/hosts/c7402.ambari.apache.org""
      },
      ""component"" : [
        {
          ""href"" : ""http://localhost:8080/api/v1/clusters/c1/services/ZOOKEEPER/components/ZOOKEEPER_SERVER"",
          ""ServiceComponentInfo"" : {
            ""cluster_name"" : ""c1"",
            ""component_name"" : ""ZOOKEEPER_SERVER"",
            ""service_name"" : ""ZOOKEEPER""
          }
        }
      ],
      ""processes"" : [ ]
    }
    

The PID file looks correct:

    
    
    
    [root@c7402 zookeeper]$ cat /var/run/zookeeper/zookeeper_server.pid
    22463
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-18 10:30:38,0
13160077,Service stop/start/restart times out when agent registration was not completed,Server should ensure that registration was successfully finished before applying registering/initial heartbeat timestamps.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-17 16:34:58,1
13160055,Remove dependency on marked.js 0.3.2 in Ambari Web,"Remove dependency on marked.js 0.3.2 in Ambari Web due to security concerns. See 
* https://nvd.nist.gov/vuln/detail/CVE-2015-1370
* https://nvd.nist.gov/vuln/detail/CVE-2017-1000427

{noformat}
[root@host ~]# ambari-server --version
2.7.0.0-519
{noformat}

{noformat}
[root@host ~]# find /usr/lib -name marked.js
/usr/lib/ambari-server/web/api-docs/lib/marked.js
{noformat}

Recommendation is to remove the dependency or upgrade to version 0.3.2-1 or the latest version, if possible. 
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-17 15:34:18,2
13160038,Install Wizard: fix markup issues,"Fix small markup issues on Install Wizard:
* decrease space after checkboxes on step3
* increase table width on step6
* change logic for bell animation on configs
* use scrollable tabs for service selection on All Configurations tab
* update warning color for bell and labels
* add title for deploy popup on step8
* change progress bar width on step9",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-17 14:22:45,4
13160014,Redo the Manage Journalnodes wizard in the context of federation changes,"STR: Deploy a cluster with multiple namespaces via Blueprint
Add JournalNode using the wizard
In the format JN step {code}sudo su cstm-hdfs -l -c 'hdfs namenode -initializeSharedEdits'{code} fails with 

{code}
Re-format filesystem in QJM to [<ip0>:8485, <ip1>:8485, <ip2>:8485] ? (Y or N) y
18/05/09 18:43:02 ERROR namenode.NameNode: Could not initialize shared edits dir
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Could not format one or more JournalNodes. 1 exceptions thrown:
<ip0>:8485: Directory /hadoop/hdfs/journal/ns1 is in an inconsistent state: Can't format the storage directory because the current directory is not empty.
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.checkEmptyCurrent(Storage.java:600)
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:683)
	at org.apache.hadoop.hdfs.qjournal.server.JNStorage.format(JNStorage.java:210)
	at org.apache.hadoop.hdfs.qjournal.server.Journal.format(Journal.java:235)
	at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.format(JournalNodeRpcServer.java:181)
	at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.format(QJournalProtocolServerSideTranslatorPB.java:148)
	at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:27399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:286)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.format(QuorumJournalManager.java:228)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.formatNonFileJournals(FSEditLog.java:426)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:1262)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1619)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1710)
18/05/09 18:43:02 INFO util.ExitUtil: Exiting with status 1: ExitException
18/05/09 18:43:02 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at <host0>/<ip0>
************************************************************/
{code}

Solution
Reorganize wizard steps:
- Assign Journalnodes
- Save Namespace
- Add/Remove Journalnodes (no start JN)
- Copy Journalnode directories
- Start Journalnodes
- Start All Services",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-17 12:43:31,2
13159971,Move user-related info to stack-level params,"User-related information (users, groups, user-group mapping) is currently sent to agents in {{clusterLevelParams}}.  The problem is that the data reflects only a single stack, so users etc. from other stacks are not created, causing deployment failures.  In addition, {{stack_version}} should be moved, too, similar to how {{stack_name}} was moved in AMBARI-23746.",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Critical,2018-05-17 08:58:03,5
13159945,New Alert JSON Is Invalid When Sent To Agents,"STR:

  * Set a simple cluster with HDFS
  * Attempt to create a new Alert:

    
    
    
    POST http://{{ambari-server}}:8080/api/v1/clusters/c1/alert_definitions
    
    {
      ""AlertDefinition"": {
        ""component_name"": ""NAMENODE"",
        ""description"": ""This service-level alert is triggered if the total number of volume failures across the cluster is greater than the configured critical threshold."",
        ""enabled"": true,
        ""help_url"": null,
        ""ignore_host"": false,
        ""interval"": 2,
        ""label"": ""NameNode Volume Failures"",
        ""name"": ""namenode_volume_failures"",
        ""scope"": ""ANY"",
        ""service_name"": ""HDFS"",
        ""source"": {
          ""jmx"": {
            ""property_list"": [
              ""Hadoop:service=NameNode,name=FSNamesystemState/VolumeFailuresTotal""
            ],
            ""value"": ""{0}""
          },
          ""reporting"": {
            ""ok"": {
              ""text"": ""There are {0} volume failures""
            },
            ""warning"": {
              ""text"": ""There are {0} volume failures"",
              ""value"": 1
            },
            ""critical"": {
              ""text"": ""There are {0} volume failures"",
              ""value"": 1
            },
            ""units"": ""Volume(s)""
          },
          ""type"": ""METRIC"",
          ""uri"": {
            ""http"": ""{{hdfs-site/dfs.namenode.http-address}}"",
            ""https"": ""{{hdfs-site/dfs.namenode.https-address}}"",
            ""https_property"": ""{{hdfs-site/dfs.http.policy}}"",
            ""https_property_value"": ""HTTPS_ONLY"",
            ""kerberos_keytab"": ""{{hdfs-site/dfs.web.authentication.kerberos.keytab}}"",
            ""kerberos_principal"": ""{{hdfs-site/dfs.web.authentication.kerberos.principal}}"",
            ""default_port"": 0,
            ""connection_timeout"": 5,
            ""high_availability"": {
              ""nameservice"": ""{{hdfs-site/dfs.internal.nameservices}}"",
              ""alias_key"": ""{{hdfs-site/dfs.ha.namenodes.{{ha-nameservice}}}}"",
              ""http_pattern"": ""{{hdfs-site/dfs.namenode.http-address.{{ha-nameservice}}.{{alias}}}}"",
              ""https_pattern"": ""{{hdfs-site/dfs.namenode.https-address.{{ha-nameservice}}.{{alias}}}}""
            }
          }
        }
      }
    }
    

This alert will not be scheduled on the agent correctly:

    
    
    
    ERROR 2018-05-16 20:11:55,186 AlertSchedulerHandler.py:307 - [AlertScheduler] Unable to load an invalid alert definition. It will be skipped.
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/AlertSchedulerHandler.py"", line 287, in __json_to_callable
        alert = MetricAlert(json_definition, source, self.config)
      File ""/usr/lib/ambari-agent/lib/ambari_agent/alerts/metric_alert.py"", line 52, in __init__
        self.metric_info = JmxMetric(alert_source_meta['jmx'])
      File ""/usr/lib/ambari-agent/lib/ambari_agent/alerts/metric_alert.py"", line 288, in __init__
        self.property_list = jmx_info['property_list']
    KeyError: 'property_list'
    

Looking at `/var/lib/ambari-agent/cache/cluster_cache/alerts.json`, we can see
that `property_list` was changed into `propertyList`.

    
    
    
            ""name"": ""namenode_volume_failures"",
            ""componentName"": ""NAMENODE"",
            ""description"": ""This service-level alert is triggered if the total number of volume failures across the cluster is greater than the configured critical threshold."",
            ""interval"": 2,
            ""clusterId"": 2,
            ""label"": ""NameNode Volume Failures"",
            ""ignore_host"": false,
            ""source"": {
              ""jmx"": {
                ""urlSuffix"": ""/jmx"",
                ""propertyList"": [
                  ""Hadoop:service=NameNode,name=FSNamesystemState/VolumeFailuresTotal""
                ],
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-17 07:00:28,0
13159894,Remove insecure dependencies from Ambari Server,"Remove insecure dependencies from Ambari Server

*Jetty: Java based HTTP, Servlet, SPDY, WebSocket Server 6.1.26*
* https://nvd.nist.gov/vuln/detail/CVE-2017-9735
* https://nvd.nist.gov/vuln/detail/CVE-2011-4461
* https://nvd.nist.gov/vuln/detail/CVE-2009-1523

{noformat}
[INFO] ------------------------------------------------------------------------
[INFO] Building Ambari Server 2.0.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
[INFO] org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT
[INFO] +- org.mortbay.jetty:jsp-api-2.1-glassfish:jar:2.1.v20100127:compile
[INFO] +- org.mortbay.jetty:jsp-2.1-glassfish:jar:2.1.v20100127:compile
[INFO] \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
[INFO]    \- org.mortbay.jetty:jetty:jar:6.1.26:compile{noformat}

Recommendation is to remove the dependency or upgrade to version 6.1.26.hwx or the latest version, if possible. 
",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2018-05-16 23:43:21,7
13159780,STOMP alert update was not sent when maintenance mode change,"Steps:
 - Shutdown a service, like AMS
 - Wait for alerts (about 10 showed up)
 - Turn in MM for AMS

Alerts don't clear until you hard refresh, even though the MM cleared them immediately.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-16 16:50:29,1
13159724,Service status is difficult to understand in the left nav of the new UI,"In previous versions of Ambari:
* A red dot that appears to the left of the service name meant that the service is down.  In the new UI, this red dot means there's an alert associated with the service, and if the service name turns red, then the service is down.  The semantics of the red dot has changed, and this is really confusing to the end user who is used to the old UI.  <- We need the red dot to mean the service is down.  
* We used to show a green dot to show that the service is up.  This gave assurance to the end user and this behavior has come to be expected.   <- We need to bring back the green dot.
* When there are associated service alerts, we used to show a red badge and the alert count to the right of the service name.  As mentioned above, a red dot shows up in the new UI and this is not easily understood as meaning there's an alert.  <- We need to bring back the alert badge w/ a count.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-16 13:02:07,4
13159525,Topology  cache on agent side is not actual after unsupported services removal during stack upgrade,Server should notify agent about unsupported services removal during stack upgrade.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-05-15 20:42:47,1
13159470,NN Federation wizard is stuck on step 3,"NameNode Federation wizard is stuck on Review step. Infinite spinner is displayed instead of config properties to be changed, and also JS error is thrown: {{app.js:5361 Uncaught TypeError: Cannot read property 'properties' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 17:04:36,2
13159451,No relationship between generic parameter and method argument,"Call to a generic collection method contains an argument with an incompatible class from that of the collection's parameter (i.e., the type of the argument is neither a supertype nor a subtype of the corresponding generic type argument). Therefore, it is unlikely that the collection contains any objects that are equal to the method argument used here. Most likely, the wrong value is being passed to the method.

* {{String}} is incompatible with expected argument type {{Long}} in {{onHostRemoved(String)}}
* {{Long}} is incompatible with expected argument type {{AlertDefinitionEntity}} in {{AlertGroupsUpdateListener.onAlertDefinitionDeleted(AlertDefinitionDeleteEvent)}}
* {{String}} is incompatible with expected argument type {{Long}} in {{HostConfigMappingDAO.removeByClusterAndHostName(long, String)}}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-15 15:47:51,5
13159424,Customise Services - All Configurations Tab - Issue with default dimensions of text box and toggle buttons colour,"The toggle buttons don't look like a button is present there. need to change the color of the toggle button base so that it is distinguishable from the background panel color. ( See Anonymous access allowed toggle button for Smartsense )

The issue is applicable to the web elements across all services under All Configurations Tab",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 14:02:08,4
13159397,Manage Ambari UI style fixes,"* Add a tooltip on config version compare button: ""Compare this version with current""
* User edit page: remove extra space between columns
* Group edit: Group Access select alignment",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 12:09:00,4
13159388,unable to differentiate b/w new added service and existing service,"While adding new service, both existing components and new components on Assign Masters step are displayed in green.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-15 11:33:41,2
13159374,"Timeline service is shown as stopped, while being started.","
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 108, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 87, in status
        for pid_file in self.get_pid_files():
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/timelinereader.py"", line 99, in get_pid_files
        import params
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/params.py"", line 29, in <module>
        from params_linux import *
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/params_linux.py"", line 473, in <module>
        repo_name = str(config['clusterName']) + '_yarn'
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'clusterName' was not found in configurations dictionary!
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-15 10:45:39,0
13159359,"ambari-server setup throwing Error ""/usr/lib/ambari-server/lib/ambari_commons/subprocess32.py:153: RuntimeWarning: The _posixsubprocess module is not being used""","
    
    [root@nat-r7-bjks-man-5 ~]# ambari-server setup --java-home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.ppc64le/
    Using python  /usr/bin/python
    Setup ambari-server
    /usr/lib/ambari-server/lib/ambari_commons/subprocess32.py:153: RuntimeWarning: The _posixsubprocess module is not being used. Child process reliability may suffer if your program uses threads.
      ""program uses threads."", RuntimeWarning)
    Checking SELinux...
    SELinux status is 'enabled'
    SELinux mode is 'permissive'
    WARNING: SELinux is set to 'permissive' mode and temporarily disabled.
    OK to continue [y/n] (y)?
    Customize user account for ambari-server daemon [y/n] (n)?
    Adjusting ambari-server permissions and ownership...
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-15 10:03:54,0
13158713,Save button is inactive for changed filtered property after configs comparing	," # Go to HIVE configs.
 # Filter configs by hive.metastore.client.socket.timeout.
 # Compare current version with some previous.
 # Close comparing panel.
 # Change filtered property.

Result: ""Save"" button is inactive. Was reproduced not for any service/property.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-11 12:19:57,2
13158669,Ambari Agent registration task is failing ,"Logs:

    
    
    
    INFO 2018-05-10 10:04:59,174 NetUtil.py:61 - Connecting to https://os-mv-07-test-3.openstacklocal:8440/ca
    WARNING 2018-05-10 10:04:59,174 NetUtil.py:92 - Failed to connect to https://os-mv-07-test-3.openstacklocal:8440/ca due to 'module' object has no attribute '_create_unverified_context'  
    WARNING 2018-05-10 10:04:59,174 NetUtil.py:115 - Server at https://os-mv-07-test-3.openstacklocal:8440 is not reachable, sleeping for 10 seconds...
    

",pull-request-available,[],AMBARI,Bug,Major,2018-05-11 07:49:09,0
13158569,NN Federation wizard: Restart Required Services should not restart JN and ZKFC,"STR:
Add namespace from UI. In the Restart All Services operation RM fails to start

{code}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 54, in wrapper
 return function(*args, **kwargs)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 244, in wait_for_dfs_directory_created
 raise Fail(""DFS directory '"" + dir_path + ""' does not exist !"")
Fail: DFS directory '/ats/done/' does not exist !
{code}

The above exception was the cause of the following exception:

{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 261, in <module>
 Resourcemanager().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 142, in start
 self.wait_for_dfs_directories_created(params.entity_groupfs_store_dir, params.entity_groupfs_active_dir)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 211, in wait_for_dfs_directories_created
 self.wait_for_dfs_directory_created(dir_path, ignored_dfs_dirs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/decorator.py"", line 62, in wrapper
 return function(*args, **kwargs)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/resourcemanager.py"", line 244, in wait_for_dfs_directory_created
 raise Fail(""DFS directory '"" + dir_path + ""' does not exist !"")
resource_management.core.exceptions.Fail: DFS directory '/ats/done/' does not exist !
{code}

But after closing the wizard and starting all the services it started.

Also restarting history server failed on another host

{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 134, in <module>
 HistoryServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 982, in restart
 self.start(env, upgrade_type=upgrade_type)
 File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/YARN/package/scripts/historyserver.py"", line 95, in start
 skip=params.sysprep_skip_copy_tarballs_hdfs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/copy_tarball.py"", line 502, in copy_to_hdfs
 replace_existing_files=replace_existing_files,
 File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
 self.env.run()
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
 self.run_action(resource, action)
 File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 124, in run_action
 provider_action()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 627, in action_create_on_execute
 self.action_delayed(""create"")
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 624, in action_delayed
 self.get_hdfs_resource_executor().action_delayed(action_name, self)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 333, in action_delayed
 self.action_delayed_for_nameservice(nameservice, action_name, main_resource)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 359, in action_delayed_for_nameservice
 self._create_resource()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 375, in _create_resource
 self._create_file(self.main_resource.resource.target, source=self.main_resource.resource.source, mode=self.mode)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 490, in _create_file
 self.util.run_command(target, 'CREATE', method='PUT', overwrite=True, assertable_result=False, file_to_put=source, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 178, in run_command
 return self._run_command(*args, **kwargs)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 251, in _run_command
 raise WebHDFSCallException(err_msg, result_dict)
resource_management.libraries.providers.hdfs_resource.WebHDFSCallException: Execution of 'curl -sS -L -w '%\{http_code}' -X PUT --data-binary @/var/lib/ambari-agent/tmp/mapreduce-native-tarball-staging/mapreduce-native.tar.gz -H 'Content-Type: application/octet-stream' --negotiate -u : -k 'https://<Host>:50470/webhdfs/v1/hdp/apps/3.0.0.0-1309/mapreduce/mapreduce.tar.gz?op=CREATE&overwrite=True&permission=444'' returned status_code=403. 
{
 ""RemoteException"": {
 ""exception"": ""IOException"", 
 ""javaClassName"": ""java.io.IOException"", 
 ""message"": ""Failed to find datanode, suggest to check cluster health. excludeDatanodes=null""
 }
}
{code}

 ",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-05-10 20:51:50,2
13158536,"Service actions (Stop, Start) not enabled when individual components are stopped","Stop all components from the host detail page and navigate to the service actions. The start/stop button is not enabled. 

Only on refresh, the Start button is enabled.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-10 18:10:40,4
13158510,TimelineMetricsFilterTest failure if dir name contains @,"[PullRequest Builder|https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/] is occasionally running into failure in the following 2 test cases:

{noformat}
testHybridFilter(org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest)  Time elapsed: 0.725 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testHybridFilter(TimelineMetricsFilterTest.java:221)

testMetricWhitelisting(org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest)  Time elapsed: 0.035 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at org.apache.ambari.metrics.core.timeline.TimelineMetricsFilterTest.testMetricWhitelisting(TimelineMetricsFilterTest.java:78)
{noformat}

Looking at the logs, it only happens if two builds are running on the same Jenkins node concurrently (workspace dir has suffix {{@2}}), but timing doesn't matter, the two concurrent builds can be at completely different stages.  The problem is caused by not finding the whitelist file due to URL escaping ({{@}} is converted to {{%40}}):

{noformat}
FileNotFoundException: Ambari-Github-PullRequest-Builder%402/ambari-metrics/ambari-metrics-timelineservice/target/test-classes/test_data/metric_whitelist.dat (No such file or directory)
{noformat}

https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2225/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2220/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2214/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/2195/",pull-request-available,['ambari-metrics'],AMBARI,Bug,Major,2018-05-10 16:03:16,5
13158499,Alerts icon is absent in service page if no alerts present,Grayed-out bell should be displayed like in the top nav.,pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-10 15:06:38,2
13158476,Identical sets of capacity scheduler properties are displayed as unequal ones in configs comparison view,"While comparing different YARN config versions, capacity scheduler values are shown as different ones. The only displayed difference is the order of properties in those sets, though even the order is the same for both versions.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-10 13:55:09,2
13158264,Action not shown immediately in the BG Operations window,"In ambari-2.7.0.0-472, When an action is selected via the Web UI (From service/host), the action is not shown in the BG Operations window that pops up. 

It is shown only after a refresh.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-09 18:09:47,4
13158263,Unable to add Hive Metastore from Host detail Page,"In ambari-2.7.0.0-472, 

Adding Hive Metastore from Host detail page leads to the modal window being stuck and javascript errors:

Javascript errors: 
{code}
Uncaught TypeError: Cannot read property 'tag' of undefined
    at Class.loadHiveConfigs (app.js:24588)
    at Class.opt.success (app.js:181624)
    at fire (vendor.js:1141)
    at Object.fireWith [as resolveWith] (vendor.js:1252)
    at done (vendor.js:8178)
    at XMLHttpRequest.callback (vendor.js:8702)
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-05-09 18:06:40,2
13158192,Regenerate Keytabs/Reenable security After Ambari Upgrade modifies hadoop.proxyuser.HTTP.hosts to an incorrect value,"value of the property hadoop.proxyuser.HTTP.hosts in HDFS is set to an incorrect value after regenerate keytabs operation or disable and then enable security is done post Ambari Upgrade to 2.7.0.0

The value contains hostnames repeated twice with ""\"" at the end.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-05-09 13:26:26,6
13157929,MySQL Connector JAR distribution is broken,"When attempting to use Oozie, Hive, Superset, and Druid with an external MySQL database the jar is not properly placed in each service so startup fails. Even after running the ambari-server setup pointing to a MySQL connector jar prior to install, we still see exceptions where the jar is not in the class path.

The reason is that  db connection properties in ambariLevelParams are not updated after modifying ambari.properties.",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2018-05-08 15:37:50,3
13157868,Agent is not notified about cluster delete in runtime,Server should notify agent about cluster removal for appropriate updating topology/configs/metadata/hostlevelparams caches.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-08 11:09:28,1
13157866,Manage Ambari UI issues,"See screenshots

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-08 11:07:37,4
13157865,Regenerate keytabs on single host should be an experimental feature,Regenerating keytabs on songle host should be available with experimental flag checked only ({{regenerateKeytabsOnSingleHost}}),pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-08 11:03:48,2
13157848,Make server/agent connection with no cert verification possible with agent python 2.7.5,"By reading <https://bugzilla.redhat.com/show_bug.cgi?id=1173041> and also the
last update of AMBARI-14149, I think the python fix is also backported into
python 2.7.5  
If so, could you change ""(2, 7, 9)"" to a lower version please?

Editing /etc/python/cert-verification.cfg wouldn't be ideal workaround as it
would affect to all other python applications in the system.  
And today I had a case which system didn't have this file (SUSE and Anaconda2
python)

Thank you

",pull-request-available,[],AMBARI,Bug,Major,2018-05-08 09:59:22,0
13157681,Remove dependency on com.fasterxml.jackson.core:jackson-databind 2.7.8 in Ambari Metrics Collector,"Remove dependency on com.fasterxml.jackson.core:jackson-databind 2.7.8 in Ambari Metrics Collector due to security concerns. See
 * [https://nvd.nist.gov/vuln/detail/CVE-2018-5968]
 * [https://nvd.nist.gov/vuln/detail/CVE-2018-7489]
 * [https://nvd.nist.gov/vuln/detail/CVE-2017-7525]
 * [https://nvd.nist.gov/vuln/detail/CVE-2017-17485]
 * [https://nvd.nist.gov/vuln/detail/CVE-2017-15095]

 
{noformat}
HW15069:ambari-metrics-timelineservice smolnar$ mvn dependency:tree -Dincludes=com.fasterxml.jackson.core:jackson-databind -Dverbose=true
[INFO] Scanning for projects...
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] Building Ambari Metrics Collector 2.0.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
 
[INFO] 
[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-metrics-timelineservice ---
[INFO] org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT
[INFO] +- org.apache.phoenix:phoenix-core:jar:5.0.0.3.0.0.0-1181:compile
[INFO] |  +- org.apache.hbase:hbase-mapreduce:jar:2.0.0.3.0.0.0-1181:compile
[INFO] |  |  +- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] |  |  \- org.apache.hadoop:hadoop-hdfs:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |  |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for conflict with 2.7.8)
[INFO] |  +- org.apache.hbase:hbase-common:jar:2.0.0.3.0.0.0-1181:compile
[INFO] |  |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:compile - omitted for duplicate)
[INFO] |  +- org.apache.hbase:hbase-client:jar:2.0.0.3.0.0.0-1181:compile
[INFO] |  |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:compile - omitted for duplicate)
[INFO] |  \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |     +- org.apache.hadoop:hadoop-hdfs-client:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |     |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for conflict with 2.7.8)
[INFO] |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for conflict with 2.7.8)
[INFO] +- org.apache.hadoop:hadoop-common:jar:3.0.0.3.0.0.0-1181:provided (scope not updated to compile)
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - scope updated from provided; omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-common:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - scope updated from test; omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-yarn-common:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - scope updated from test; omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-yarn-common:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |  +- com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile
[INFO] |  +- com.fasterxml.jackson.module:jackson-module-jaxb-annotations:jar:2.7.8:compile
[INFO] |  |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] |  \- com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider:jar:2.7.8:compile
[INFO] |     +- com.fasterxml.jackson.jaxrs:jackson-jaxrs-base:jar:2.7.8:compile
[INFO] |     |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] +- org.apache.hadoop:hadoop-yarn-server-common:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |  \- org.apache.hadoop:hadoop-yarn-registry:jar:3.0.0.3.0.0.0-1181:compile
[INFO] |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:compile - omitted for duplicate)
[INFO] +- org.apache.hbase:hbase-it:jar:tests:2.0.0.3.0.0.0-1181:test
[INFO] |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:test - omitted for conflict with 2.7.8)
[INFO] \- org.apache.hbase:hbase-testing-util:jar:2.0.0.3.0.0.0-1181:test
[INFO]    +- org.apache.hbase:hbase-common:test-jar:tests:2.0.0.3.0.0.0-1181:test
[INFO]    |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.9.2:test - omitted for conflict with 2.7.8)
[INFO]    +- org.apache.hadoop:hadoop-hdfs:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO]    |  \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:test - omitted for duplicate)
[INFO]    \- org.apache.hadoop:hadoop-minicluster:jar:3.0.0.3.0.0.0-1181:test
[INFO]       +- org.apache.hadoop:hadoop-yarn-server-tests:test-jar:tests:3.0.0.3.0.0.0-1181:test
[INFO]       |  \- org.apache.hadoop:hadoop-yarn-server-timelineservice:jar:3.0.0.3.0.0.0-1181:test
[INFO]       |     \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:test - omitted for duplicate)
[INFO]       \- org.apache.hadoop:hadoop-mapreduce-client-app:jar:3.0.0.3.0.0.0-1181:test
[INFO]          \- (com.fasterxml.jackson.core:jackson-databind:jar:2.7.8:test - omitted for duplicate)
{noformat}
Recommendation is to remove the dependency or upgrade to version 2.8.11.1 or the latest, if possible.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-05-07 19:15:30,6
13157559,Service summary style tweaks,See screenshot,pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-05-07 11:01:07,4
13157526,Remove unsecure dependencies from ambari-agent,"Remove - or upgrade to a recommended version - the following libraries in ambari-agent due to security concerns:
 * Remove dependency on com.jcraft:jsch 0.1.42 (or upgrade to version is 0.1.45 or greater)
 * Remove dependency on org.mortbay.jetty:jetty-util 6.1.26 (or upgrade to version is 6.1.26.hwx or greater)
 * Remove dependency on org.apache.zookeeper:zookeeper 3.4.9 (or upgrade to version 3.4.10, 3.5.3, and later. pre [CVE-2017-5637|https://nvd.nist.gov/vuln/detail/CVE-2017-5637])
 * Remove dependency on commons-httpclient:commons-httpclient 3.1 (or upgrade to version  5.0-alpha2-RC1)
 * Remove dependency on commons-beanutils:commons-beanutils-core 1.8.0 (or upgrade to version 1.9.2 or 1.9.3)

 ",black-duck pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-05-07 08:11:33,6
13157378,Suppress FindBugs warnings for EclipseLink-generated code,"FindBugs produces 1428 warnings for String comparisons in Ambari Server's statically weaved entities.  This amounts to almost 400KB in Jenkins build logs.

{noformat:title=}
[INFO] Comparison of String parameter using == or != in org.apache.ambari.server.orm.entities.AlertCurrentEntity._persistence_get(String)  [org.apache.ambari.server.orm.entities.AlertCurrentEntity] In AlertCurrentEntity.java ES_COMPARING_PARAMETER_STRING_WITH_EQ
[INFO] Comparison of String parameter using == or != in org.apache.ambari.server.orm.entities.AlertCurrentEntity._persistence_set(String, Object)  [org.apache.ambari.server.orm.entities.AlertCurrentEntity] In AlertCurrentEntity.java ES_COMPARING_PARAMETER_STRING_WITH_EQ
{noformat}

As we have no control over this kind of generated code, it should be suppressed, both to focus on more real problems pointed out by FindBugs, and to reduce size of our build logs.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-05-05 18:45:43,5
13157244,"Icon for ""start demo ldap"" is missing","Icon for ""Start Demo Ldap"" is missing from knox action list in ambari.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-04 16:13:42,2
13157173,Corrupt mapreduce/tez tar.gz may be uploaded to HDFS if parallel execution is enabled,"If parallel_execution on Ambari Agent is enabled, two components (History Server and Hive Server) may create and upload the MapReduce and Tez archives concurrently.  This could result in a corrupt uploaded file, preventing Hive Server from starting.

{noformat:title=output-32.txt}
2018-05-04 07:24:40,421 - Creating a new Tez tarball at /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz
...
2018-05-04 07:24:49,751 - Creating new file /hdp/apps/2.6.4.5-2/tez/tez.tar.gz in DFS
2018-05-04 07:24:49,753 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl -sS -L -w '""'""'%{http_code}'""'""' -X PUT --data-binary @/var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz -H '""'""'Content-Type: application/octet-stream'""'""' '""'""'http://localhost:50070/webhdfs/v1/hdp/apps/2.6.4.5-2/tez/tez.tar.gz?op=CREATE&user.name=hdfs&overwrite=True&permission=444'""'""' 1>/tmp/tmpoEINHo 2>/tmp/tmpkjlNxP''] {'logoutput': None, 'quiet': False}
2018-05-04 07:24:51,629 - call returned (0, '')
{noformat}

{noformat:title=output-30.txt}
2018-05-04 07:24:46,818 - Creating a new Tez tarball at /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz
...
2018-05-04 07:24:54,117 - DFS file /hdp/apps/2.6.4.5-2/tez/tez.tar.gz is identical to /var/lib/ambari-agent/tmp/tez-native-tarball-staging/tez-native.tar.gz, skipping the copying
{noformat}",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-05-04 14:18:19,5
13157168,Remove topology info from non-blueprint commands,Server should sent topology info as a part of command only during blueprint commands posting.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-05-04 13:56:45,1
13157130,Subscriptions should be bound to WebSocket connection,"Stomp client subscription should happen after WebSocket connected.
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-04 12:19:59,4
13157112,Dropdowns in Host Details Page do not conform to the new UI,"In ambari-2.7, the dropdowns in the following screens do not conform to the rest of the UI.

# Host Details Page -> Configs -> Change Config Group -> Groups
# Host Details Page -> Alerts -> Filters (Service, Status) 
# Host Details Page -> Versions -> Filters (Stack, Name, Status) 
# Host Details Page -> Logs -> Filters (Service, Component, Extension) 

Whereas dropdowns in  other pages, e.g. HDFS -> Configs -> Config Group have a different design.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-04 10:49:32,4
13156944,Ambari Infra Solr Service Check fails after Ambari Upgrade,"*STR*
1) Upgrade Ambari from 2.6.X to 2.7.0.0-435 ( Unkerberized cluster)
2) Upgrade Non Stack Services, like Infra Solr.
3) Restart all required services (which might have stale configs)
4) Run Service check on Ambari Infra Solr . It fails with below error

{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/service_check.py"", line 48, in <module>
    InfraServiceCheck().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/service_check.py"", line 27, in service_check
    import params
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_INFRA_SOLR/0.1.0/package/scripts/params.py"", line 109, in <module>
    infra_solr_java_stack_size = format(config['configurations']['infra-solr-env']['infra_solr_java_stack_size'])
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/format.py"", line 95, in format
    return ConfigurationFormatter().format(format_string, args, **result)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/format.py"", line 59, in format
    result_protected = self.vformat(format_string, args, all_params)
  File ""/usr/lib64/python2.7/string.py"", line 549, in vformat
    result = self._vformat(format_string, args, kwargs, used_args, 2)
  File ""/usr/lib64/python2.7/string.py"", line 558, in _vformat
    self.parse(format_string):
  File ""/usr/lib64/python2.7/string.py"", line 621, in parse
    return format_string._formatter_parser()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'infra_solr_java_stack_size' was not found in configurations dictionary!
{code}

*Cause*
While upgrading the name of the AMBARI_INFRA service is changed to AMBARI_INFRA_SOLR, but the old name is cached in the JPA entities cause a mismatch on services names at some point. 

*Solution*
Clear the JPA entity cache after changing the AMBARI_INFRA service name. 
",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-05-03 19:22:43,7
13156873,Hide SmartSense master components in HDF stack,"Our assumption is that ambari should be able to allow zero node deployment of this master component. But unfortunately, UI does not have any provision to remove master components (which has cardinality 0-3). This causes dependency issues in HDF environment where the dependent component does not exists.
For the Activity Analyzer and Activity Explorer. We don’t want those to show up in assign masters for the HDF stack.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-03 13:47:12,4
13156631,Hive Service Summary View Links Need to be removed,"The ""Hive View 2.0"" and ""Debug Hive Query"" links need to be removed from the Hive Summary section as they are being removed from Ambari 2.7/HDP 3.0.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-02 15:05:22,4
13156618,JS error after installing Ranger from Install Wizard - Smart Configs are broken - page refresh fixed the issue,"See attached.
Modifed FE code (doNotShowAndInstall computed property) so that Ranger can be installed from Install Wizard.  Installation succeeded.  However, post-Install, there were some JS errors (see attached) that prevented the Smart Config pages from loading.  See attached. !Screen Shot 2018-04-27 at 2.17.22 PM.png|thumbnail! 
After page refresh, the problem went away and Smart Configs can be viewed just fine.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-02 14:31:41,4
13156604,Namespace names are converted to uppercase after selecting from dropdown,Selected namespace names are shown in uppercase where as they are actually created in lowercase and in dropdown it's in lowercase,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-05-02 13:18:51,2
13156117,"When Spark2 is selected in left pane, Spark also gets selected along with it","When Spark2 is selected in services in left pane, Spark gets selected along with it.
The pointer should only point to Spark2 (which is the selected service).",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-30 14:30:20,2
13156082,Group HDFS components into the components section of the ambari service summary page,The HDFS Service Page in the new Ambari UI page displays useful information about the service components. This information would be easier to digest if all the HDFS components were grouped together at the top of the page (in the components section) as they are for the rest of the services.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-30 09:46:48,2
13155760,Adding and deleting widgets of NameNode section on dashboard isn't persisted,"*STR*
# Enable NameNode federation
# Add or remove some NameNode widgets on dashboard
# Go to other page and retirn to dashboard, or stay on dashboard and refresh the page

*Expected result*
NameNode widgets are displayed/hidden according to the adding/removing actions described above

*Actual result*
Default set of NameNode widgets is displayed",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 18:14:38,2
13155735,Ambari UI trying to create GPL repo with empty base url for the rest repos,"On cluster deploy, Ambari UI sending such request body: 

{code}
{""operating_systems"":[{""OperatingSystems"":{""os_type"":""redhat7"",""ambari_managed_repositories"":true},""repositories"":[{""Repositories"":{""base_url"":"""",""repo_id"":""HDP-3.0"",""repo_name"":""HDP"",""components"":null,""tags"":[],""distribution"":null}},{""Repositories"":{""base_url"":"""",""repo_id"":""HDP-3.0-GPL"",""repo_name"":""HDP-GPL"",""components"":null,""tags"":[""GPL""],""distribution"":null}},{""Repositories"":{""base_url"":"""",""repo_id"":""HDP-UTILS-1.1.0.21"",""repo_name"":""HDP-UTILS"",""components"":null,""tags"":[],""distribution"":null}}]}]}: 
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-27 15:42:14,4
13155704,hbase.zookeeper.property.clientPort config of AMS is empty after Ambari Upgrade,"STR

1). Install Ambari-2.6.0.0-267
2) Upgrade Ambari to 2.7.0.0-308 
3) Check for hbase.zookeeper.property.clientPort property under Advanced ams-hbase-site in UI. It is empty and is shown as required. But it has a value before Upgrade . 
",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-27 13:16:20,3
13155678,Kafka Quick Links Shows all Views links,"The Kafka Service Summary screen shows all Ambari View instances under the ""Views"" Quick Link.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 10:51:16,2
13155677,Config Version Comparison tool shows diff text merging into each other if config values are long.,"While comparing two config versions , if config values are long , than the diff shows values merging into each other and text is not wrapped.

See Attached Screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-27 10:51:16,4
13155653,Ambari Schema Upgrade Failing source 2.6.2.0 target 2.7.0.0,"When upgrading Ambari from 2.6.x to 2.7.0 an error occured in SchemaUpgradeHelper:
{code:java}
15 Apr 2018 17:12:06,307 INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE repo_tags (repo_definition_id BIGINT NOT NULL, tag VARCHAR(255) NOT NULL) ENGINE=INNODB
15 Apr 2018 17:12:06,333 INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE repo_tags ADD CONSTRAINT FK_repo_tag_definition_id FOREIGN KEY (repo_definition_id) REFERENCES repo_definition (id)
15 Apr 2018 17:12:06,349 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.lang.NullPointerException
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.migrateRepoData(UpgradeCatalog270.java:443)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeRepoTables(UpgradeCatalog270.java:323)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:291)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
15 Apr 2018 17:12:06,350 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.lang.NullPointerException
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.migrateRepoData(UpgradeCatalog270.java:443)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeRepoTables(UpgradeCatalog270.java:323)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:291)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 ... 1 more{code}
This issue occurred only when we had data in _repo_version_ and the _repositories_ JSON contained missing data (_OperatingSystems/ambari_managed_repositories_ was missing).

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-27 08:26:18,6
13155460,Agent uses compressed topology upon retry,"HiveServer 2 fails to start due to:

{noformat}
2018-04-26 07:54:04,971 - call['/usr/hdp/current/zookeeper-client/bin/zkCli.sh -server 0:2181,2:2181,4:2181 ls /hiveserver2 | grep '\[serverUri=''] {}
2018-04-26 07:54:05,693 - call returned (1, 'Exception in thread ""main"" org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
{noformat}

ZooKeeper connection string contains host indexes instead of hostnames.  This only happens upon command retry, initial run uses hostnames.",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-04-26 16:45:59,5
13155421,NameNode HA wizard style changes,"We should use the standard gray in the background. rgba(233, 233, 233, 0.5).
See screenshots",pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-04-26 14:11:17,4
13155413,Need better xpaths for quicklinks when multiple namespaces are present,Need better xpaths for quicklinks when multiple namespaces are present. It would be better if quicklinks are grouped by namespaces,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-26 13:42:40,2
13155355,Adding new namespace fails at Reconfigure Services for HDFS 2,"Add New HDFS Namespace wizard fails at Reconfigure Services step when trying to add new namespace ""ns2"" (existing namespace is ""TEST"") in a HDP 2.6 cluster.

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_client.py"", line 78, in <module>
    HdfsClient().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_client.py"", line 35, in install
    import params
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py"", line 330, in <module>
    if hostname.lower() in nn_host.lower() or public_hostname.lower() in nn_host.lower():
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.rpc-address.TEST,ns2.nn1' was not found in configurations dictionary!
{noformat}",pull-request-available,['stacks'],AMBARI,Bug,Critical,2018-04-26 09:52:58,5
13155342,Stack scripts should only try to reach corresponding Namenode pair in the context rather than all NNs,"A good example is when we start up a specific Namenode, say NN1. In that case,
we try to get the HAState of all the NNs (even NN3 and NN4) to find out the
active namenode. This is not really needed since starting up NN1 should not
care about NN3 and NN4's state.

I see usages in following places.

  * ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py
  * ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/namenode_ha_state.py
  * ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py

Some of the above might not need any change.

",pull-request-available,[],AMBARI,Bug,Major,2018-04-26 08:52:22,0
13155263,Remove unsecure dependencies from ambari-utility,"# Remove dependency on com.fasterxml.jackson.dataformat:jackson-dataformat-xml 2.4.5 in ambari-utility due to security concerns. Recommendation is to remove the dependency or upgrade to version 2.6.3.redhat-3 or the latest version, if possible.
 # Remove dependency on com.fasterxml.jackson.core:jackson-databind 2.9.4 in ambari-utility due to security concerns. Recommendation is to remove the dependency or upgrade to version 2.9.5 or the latest, if possible.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-25 22:27:29,6
13155235,Symlinks are not followed when requesting resources from Ambari's resources entry point,"Symlinks are not followed when requesting resources from Ambari's resources entry point.

For example if the Beacon mpack is installed, a symlink is made from /var/lib/ambari-server/resources/mpacks/beacon-engine.mpack-1.1.0.0/addon-services/BEACON/1.1.0 to /var/lib/ambari-server/resources/stacks/HDP/2.6/services/BEACON:
{noformat}
# ls -ltr /var/lib/ambari-server/resources/stacks/HDP/2.6/services/BEACON
lrwxrwxrwx 1 root root 95 Apr 25 07:03 /var/lib/ambari-server/resources/stacks/HDP/2.6/services/BEACON -> /var/lib/ambari-server/resources/mpacks/beacon-engine.mpack-1.1.0.0/addon-services/BEACON/1.1.0
{noformat}

{noformat}
# curl -i -X GET http://c7401.ambari.apache.org:8080/resources/stacks/HDP/2.6/services/BEACON/metainfo.xml
HTTP/1.1 404 Not Found
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
X-Content-Type-Options: nosniff
Pragma: no-cache
Content-Type: text/plain;charset=ISO-8859-1
Content-Length: 45

{
  ""status"": 404,
  ""message"": ""Not Found""
}
{noformat}

When this occurs, the stack definitions from BEACON are not able to be cached by the Ambari agents. 
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-25 19:53:26,7
13155170,The UI should ignore Kerberos identity references when setting the user-supplied Kerberos descriptor,"The UI should ignore Kerberos identity references when setting the user-supplied Kerberos descriptor.  

For example, any Kerberos identity entry that contains a ""reference"" attribute, should not be added to the JSON data stored in the cluster artifact table, which represents the _user-supplied_ Kerberos descriptor. 

Currently, there are a few errors in the current process:

*Variables are being replaced*
{code}
{
  ""keytab"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-keytab-file"",
    ""file"": ""/etc/security/keytabs/spnego.service.keytab""
  },
  ""name"": ""mapreduce2_historyserver_spnego"",
  ""principal"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-principal"",
    ""local_username"": null,
    ""type"": null,
    ""value"": ""HTTP/_HOST@EXAMPLE.COM""
  },
  ""reference"": ""/spnego""
}
{code}

This should be 

{code}
{
  ""keytab"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-keytab-file"",
    ""file"": ""${keytab_dir}/spnego.service.keytab""
  },
  ""name"": ""mapreduce2_historyserver_spnego"",
  ""principal"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-principal"",
    ""local_username"": null,
    ""type"": null,
    ""value"": ""HTTP/_HOST@EXAMPLE.COM""
  },
  ""reference"": ""/spnego""
}
{code}

But really should be 

{code}
{
  ""keytab"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-keytab-file""
  },
  ""name"": ""mapreduce2_historyserver_spnego"",
  ""principal"": {
    ""configuration"": ""mapred-site/mapreduce.jobhistory.webapp.spnego-principal""
  },
  ""reference"": ""/spnego""
}
{code}

*Incorrect variable replacement*
Some replacement issue has occurred where the keytab _file_ and the principal _name_ values have been swapped:

{code}
{
  ""keytab"": {
    ""configuration"": ""hive-site/hive.server2.authentication.spnego.keytab"",
    ""file"": ""HTTP/_HOST@EXAMPLE.COM""
  },
  ""name"": ""hive_hive_server_spnego"",
  ""principal"": {
    ""configuration"": ""hive-site/hive.server2.authentication.spnego.principal"",
    ""local_username"": null,
    ""type"": null,
    ""value"": ""/etc/security/keytabs/spnego.service.keytab""
  },
  ""reference"": ""/spnego""
},
{code}

And error that has resulted from this occurred while installing Hive into a cluster where Kerberos was enabled:
{noformat}
2018-04-18 19:30:24,557 - Failed to create principal, /etc/security/keytabs/spnego.service.keytab - Failed to create service principal for /etc/security/keytabs/spnego.service.keytab
STDOUT: Authenticating as principal admin/admin@EXAMPLE.COM with existing credentials.
Principal ""/etc/security/keytabs/spnego.service.keytab@EXAMPLE.COM"" created.

STDERR: WARNING: no policy specified for /etc/security/keytabs/spnego.service.keytab@EXAMPLE.COM; defaulting to no policy

Administration credentials NOT DESTROYED.
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-25 16:20:30,4
13155141,Remove unsecure dependencies from ambari-server,"* Remove dependency on org.springframework.security:spring-security-core 4.2.2.RELEASE in Ambari Server
* Remove dependency on org.springframework.ldap:spring-ldap-core 2.0.4.RELEASE in Ambari Server	
* Remove dependency on org.springframework.security:spring-security-config 4.2.2.RELEASE in Ambari Server
* Remove dependency on com.nimbusds:nimbus-jose-jwt 3.9 in Ambari Server
* Remove dependency on org.apache.ant:ant-launcher 1.7.1 in Ambari Server
* Remove dependency on org.springframework.security:spring-security-ldap 4.0.4.RELEASE in Ambari Server
* Remove dependency on org.springframework.security:spring-security-web 4.2.2.RELEASE in Ambari Server
* Remove dependency on com.jcraft:jsch 0.1.42 in Ambari Server
* Remove dependency on com.fasterxml.jackson.dataformat:jackson-dataformat-xml 2.4.5 in Ambari Server",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-25 14:20:25,3
13155109,HDFS metrics page: list of namespaces is misaligned,"After enabling NameNode federation, the section with namespace-scoped widgets on HDFS metric page appears. Namespaces list is misaligned relatively to select control.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-25 12:59:37,2
13155094,Deselect NFS Gateway (and Phoenix Query Server) by default in Ambari UI deployment,UI deployment via Ambari-2.6 has components NFSGateway & 'Phoenix Query Server' disabled (i.e. the checkbox is not checked) on 'Assign Slaves and Clients' page. In Ambari 2.7 these check boxes are active but they should not be.,pull-request-available,[],AMBARI,Bug,Critical,2018-04-25 11:55:21,6
13155076,dfs_ha_initial_* properties should be removed after installation,{{dfs_ha_initial_namenode_active}} and {{dfs_ha_initial_namenode_standby}} properties are not removed from {{hadoop-env}} config after installing a cluster with NameNode HA using blueprints.,pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-25 09:57:25,5
13155068,Ambari Metrics Service Check Fails Post Ambari Upgarde with error : Configuration parameter not found in configurations dictionary!,"In an unkerberized cluster , after ambari upgrade from 2.6.X to 2.7.0.0 , the
service check for Ambari Metrics fail with below error.

    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/service_check.py"", line 304, in <module>
        AMSServiceCheck().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
        method(env)
      File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
        return fn(*args, **kwargs)
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/service_check.py"", line 170, in service_check
        import params
      File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/params.py"", line 119, in <module>
        'files', 'grafana-dashboards', stack_name))
      File ""/usr/lib64/python2.7/posixpath.py"", line 75, in join
        if b.startswith('/'):
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'service_package_folder' was not found in configurations dictionary!
",pull-request-available,[],AMBARI,Bug,Major,2018-04-25 09:15:09,0
13154845,"In Select Target Host page as part of Move Master wizard, the host holding the component is not present in list of options","In Select Target Hosts page, the current host for the component is not shown in the list of available options for the component. This leads to a blank being shown in the combo box.

The current host was part of the options in earlier ambari versions.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-24 15:58:56,4
13154811,UI Styling Is Incorrect On Upgrade Repositories Page,"Some of the following areas of the UI look a bit off WRT styling. 

- Upgrade repositories
-- Button size
-- Repository placement / size

- Upgrade History
- General styling and alignment",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-24 13:59:09,2
13154789,issue with service auto restart service categorization,"The service and component categorization are incorrect for e.x.

NameNode, Nodemanager and and Nimbus are under ""Ambari Metrics"".",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-24 12:38:49,4
13154765,Heatmaps page style changes,"See screenshots

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-24 11:14:11,4
13154552,Ensure URLs to Ambari server resources are valid,"Ensure URLs to Ambari server resources are valid.  For example, fix URLs like 
{noformat}
http://ambari_server_host:8080/resources//DBConnectionVerification.jar
{noformat}
To
{noformat}
http://ambari_server_host:8080/resources/DBConnectionVerification.jar
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-23 18:08:10,7
13154503,Hosts topology can be incomplete for components install during blueprint deploy,Execution command should contain actual cluster topology information. Temporary fix.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-23 14:53:05,1
13154441,No widgets displayed on dashboard after adding third NameNode namespace,"After enabling NameNode federation and adding two additional namespaces, dashboards has no widgets displayed, with infinite spinner instead of them. Also, JS error is thrown: {{Uncaught TypeError: Cannot read property '1' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-23 10:38:11,2
13154434,Ambari-agent should handle delivery status responses from server,"This is required so that if connection between agent and server is lost, agent
knows what was lost and can resend it later.

",pull-request-available,[],AMBARI,Bug,Major,2018-04-23 10:05:35,0
13154433,NameNode namespaces aren't sorted by name,Namespaces should be displayed in alphabetical order.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-23 09:54:40,2
13154334,Ensure URLs to Ambari server resources are valid,"Ensure URLs to Ambari server resources are valid.  For example, fix URLs like 
{noformat}
http://ambari_server_host:8080/resources//DBConnectionVerification.jar
{noformat}
To
{noformat}
http://ambari_server_host:8080/resources/DBConnectionVerification.jar
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-22 18:44:01,7
13154004,Missing tooltip/indicator for unsupported services in Versions page when HDP-3.0 VDF is registered,"*STR*
# Deploy HDP-2.6.1.0 cluster with Ambari-2.6.x
# Upgrade Ambari to 2.7.0.0
# Register HDP-3.0 VDF
# Go to Version page and observe the small icon against services that are unsupported in 3.0 like Falcon, Flume, Spark, Mahout, Slider
# Hover mouse over them - it does not indicate any message

*Expected Result*
It would be good to say something like: This service is unsupported in the current version of the stack",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-20 11:31:40,2
13154002,Compatibility repository version wrong,"GET http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions

{code}

  ""href"" : ""http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions"",
  ""items"" : [
    {
      ""href"" : ""http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions/51"",
      ""CompatibleRepositoryVersions"" : {
        ""id"" : 51,
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""2.6"",
        ""upgrade_types"" : [
          ""NON_ROLLING"",
          ""HOST_ORDERED"",
          ""ROLLING""
        ]
      }
    },
    {
      ""href"" : ""http://c7401:8080/api/v1/stacks/HDP/versions/2.6/compatible_repository_versions/1"",
      ""CompatibleRepositoryVersions"" : {
        ""id"" : 1,
        ""stack_name"" : ""HDP"",
        ""stack_version"" : ""3.0"",
        ""upgrade_types"" : [
          ""NON_ROLLING""
        ]
      }
    }
  ]
}
{code}

The version in the second href is incorrect.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-20 11:22:40,3
13153844,Enable Ambari SSO to be enabled without impacting other service sso configs,"*Scenario*
Ranger and Atlas are SSO enabled via BP deploys and Ambari is not SSO enabled. 
Later Ambari SSO has to be enabled without changing existing configs(so restart will not be required) for Atlas and Ranger. 
Now this is not possible with ""Enable for the selected services"" option. 
This was possible in previous versions but with the latest changes from AMBARI-23253, even if SSO was enabled for services earlier we still have to opt SSO for Ranger and Atlas in the list. When services are specified in the list, this would prompt for service restart.
So,
---If we enable SSO for Ambari and not the other services via the CLI, then any previous SSO setting for those services will be cleared
---If we enable SSO for Ambari and the other services via the CLI, then any previous SSO setting for those services will be potentially updated and this cause services to need to restart. But since data is the same no restart should be needed for those services

*Solution*
Add new prompts to separate Ambari's SSO configuration from the managed service's SSO configs so they can be managed separately:
* Use SSO for Ambari ({{--sso-enabled-ambari}})
* Manage SSO configurations for eligible services ({{--sso-manage-services}})

{noformat}
[root@c7401 ~]# ambari-server setup-sso --help
Using python  /usr/bin/python
Setting up SSO authentication properties...
Usage: ambari-server.py action [options]

Options:
  -h, --help            show this help message and exit
  -v, --verbose         Print verbose status messages
  -s, --silent          Silently accepts default prompt values. For db-cleanup
                        command, silent mode will stop ambari server.
  --sso-enabled=SSO_ENABLED
                        Indicates whether to enable/disable SSO
  --sso-enabled-ambari=SSO_ENABLED_AMBARI
                        Indicates whether to enable/disable SSO authentication
                        for Ambari, itself
  --sso-manage-services=SSO_MANAGE_SERVICES
                        Indicates whether Ambari should manage the SSO
                        configurations for specified services
  --sso-enabled-services=SSO_ENABLED_SERVICES
                        A comma separated list of services that are expected
                        to be configured for SSO (you are allowed to use '*'
                        to indicate ALL services)
  --sso-provider-url=SSO_PROVIDER_URL
                        The URL of SSO provider; this must be provided when
                        --sso-enabled is set to 'true'
  --sso-public-cert-file=SSO_PUBLIC_CERT_FILE
                        The path where the public certificate PEM is located;
                        this must be provided when --sso-enabled is set to
                        'true'
  --sso-jwt-cookie-name=SSO_JWT_COOKIE_NAME
                        The name of the JWT cookie
  --sso-jwt-audience-list=SSO_JWT_AUDIENCE_LIST
                        A comma separated list of JWT audience(s)
  --ambari-admin-username=AMBARI_ADMIN_USERNAME
                        Ambari administrator username for accessing Ambari's REST API
  --ambari-admin-password=AMBARI_ADMIN_PASSWORD
                        Ambari administrator password for accessing Ambari's REST API
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-19 21:16:58,7
13153780,Update Spring dependencies to fix CVE-2018-1270,"Spring dependencies should be updated to fix remote code execution vulnerabilities:
[https://pivotal.io/security/cve-2018-1270]
[https://pivotal.io/security/cve-2018-1275]",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-19 17:37:05,1
13153708,Additional realm config change not working,"1) go to https://<host_name>/#/main/admin/kerberos
2) click edit
3) add Additional Realms config in ambari kerberos config tab
4) click save
5) Click OK from the popup which ask for the restart of the components
In network tab we got this error:
Request URL:https://<host_name>/api/v1/clusters/<cluster_name>/artifacts/kerberos_descriptor
Request Method:PUT
Status Code:404 Not Found
{ ""status"" : 404, ""message"" : ""org.apache.ambari.server.controller.spi.NoSuchResourceException: The requested resource doesn't exist: Artifact not found, Artifacts/cluster_name=<cluster_name> AND Artifacts/artifact_name=kerberos_descriptor"" }",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-19 13:56:27,2
13153679,Enable NameNode HA fails if Ranger is set up with Ambari-managed Hive MySQL DB instance,"To repro:
* Install a cluster with Hive's database type ""New MySQL""
* Add Ranger via Add Service Wizard, and have its database point to the Hive's Ambari-managed MySQL database
* Attempt to enable HA via ""Enable NameNode HA Wizard""
This fails when Ranger is being started up, because the Ambari-managed MySQL server is down

To fix this, we can start the Ambari-managed MySQL server before starting Ranger",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-19 12:40:28,4
13153668,JWT cookie name and audiences not queried for during ambari-server setup-sso,"JWT cookie name and audiences not queried for during ambari-server setup-sso

Example:
{noformat}
[root@c7401 ~]# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Enter Ambari Admin login: admin
Enter Ambari Admin password:

SSO is currently not configured
Do you want to configure SSO authentication [y/n] (y)? y
Provider URL (https://knox.example.com:8443/gateway/knoxsso/api/v1/websso):https://c7401.ambari.apache.org:8443/gateway/knoxsso/api/v1/websso
Public Certificate PEM (empty line to finish input):
MIICVTCCAb6gAwIBAgIILf1Tx+q3QEMwDQYJKoZIhvcNAQEFBQAwbTELMAkGA1UE
...
6crsjbE33yYbJ1mZCpLGtM7mCj0liitItA==

Use SSO for all services [y/n] (n):
Use SSO for AMBARI [y/n] (y):
Use SSO for ATLAS [y/n] (y):
Use SSO for RANGER [y/n] (y): n
Ambari Server 'setup-sso' completed successfully.
{noformat}

The following queries are expected before setup is complete:
{noformat}
JWT Cookie name (hadoop-jwt):
JWT audiences list (comma-separated), empty for any ():
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-19 12:10:42,7
13153612,Ambari-server setup fails on Amazonlinux2,"Ambari server setup fails with the following error:

    
    
    
    Traceback (most recent call last):
      File ""/usr/sbin/ambari-server.py"", line 1054, in <module>
        mainBody()
      File ""/usr/sbin/ambari-server.py"", line 1024, in mainBody
        main(options, args, parser)
      File ""/usr/sbin/ambari-server.py"", line 974, in main
        action_obj.execute()
      File ""/usr/sbin/ambari-server.py"", line 79, in execute
        self.fn(*self.args, **self.kwargs)
      File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1199, in setup
        _setup_database(options)
      File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1005, in _setup_database
        dbmsAmbari = factory.create(options, properties, ""Ambari"")
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 511, in create
        dbmsConfig = desc.create_config(options, properties, dbId)
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 81, in create_config
        return self.fn_create_config(options, properties, self.storage_key, dbId)
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 884, in createPGConfig
        return PGConfig(options, properties, storage_type)
      File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 421, in __init__
        PGConfig.PG_HBA_DIR = get_postgre_hba_dir(OS_FAMILY)
      File ""/usr/lib/ambari-server/lib/ambari_server/utils.py"", line 268, in get_postgre_hba_dir
        get_pg_hba_init_files())
    OSError: [Errno 17] File exists
    

As I checked this function /usr/lib/ambari-server/lib/ambari_server/utils.py
get_postgre_hba_dir determines incorrectly the postgres data dir. If I change
it to return ""/var/lib/pgsql/9.5/data/"" the setup finishes successfully.
Verbose log attached. Used Ambari version: 2.7.0.0-330

",pull-request-available,[],AMBARI,Bug,Major,2018-04-19 07:28:37,0
13153600,Oozie - JDBC Driver class does not change when the Database type is changed,"In ambari 2.7.0.0-330, when the database type is changed for Oozie service, the JDBC server class does not change. This happens in the install wizard as well as post-install.

The behavior is different in Hive, where the JDBC server class changes when the database type changes
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-19 06:39:08,6
13153486,Header missing in Step2 Select hosts page in NN Federation Wizard,Header missing in Step2 Select hosts page in NN Federation Wizard. All other pages have proper headers,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-18 21:12:05,2
13153412,Move NameNode wizard is stuck on Review step after enabling federation,"- Infinite spinner is displayed
- 'Next' button is disabled
- JS error is thrown: {{app.js:31501 Uncaught TypeError: Cannot read property 'indexOf' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-18 16:52:50,2
13153397,View instances not created by default,"In ambari-2.7.0, the view instances are not created by default. 

In previous versions, the instances of YARN, Files, Hive, Hive 2.0, Smartsense and Tez  were created automatically. ",pull-request-available,['ambari-views'],AMBARI,Bug,Blocker,2018-04-18 16:23:38,1
13153361,Enable NameNode HA fails after Ambari Upgrade due to AttributeError,"STR

# Install a Ambari Cluster with Ambari 2.6.1
# Upgrade to Ambari 2.7
# Try to enable NameNode HA

Result:

{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/journalnode.py"", line 143, in <module>
    JournalNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/journalnode.py"", line 39, in install
    import params
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py"", line 330, in <module>
    if hostname.lower() in nn_host.lower() or public_hostname.lower() in nn_host.lower():
AttributeError: 'NoneType' object has no attribute 'lower'
{noformat}

Problem:

{noformat:title=command*json}
""public_hostname"": null
{noformat}",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-04-18 14:21:44,5
13153314,Unable to update credentials of a remotely registered cluster,"*STR*
# Register a remote cluster with Ambari server
# Go to Admin - Remote Cluster page and select the cluster
# Hit 'Update Credentials' and supply new/existing admin credentials of remote cluster
# Hit Update

Result: Credentials are not updated and UI shows an error",pull-request-available,['ambari-admin'],AMBARI,Bug,Critical,2018-04-18 10:40:53,2
13153051,NN Federation Lower priority changes,"- The add metric in the HDFS Metrics needs more padding around it
- Create metric - because of menu depth the name of the metric is hard to see, we should wrap them and do anything to make sure the metric names are easily readable
- In the move master wizard we need to add the namespace before the word ""NameNode"", like ""ns1 NameNode""",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-17 16:02:26,2
13153018,Config's tags should be cached,"UI often make calls to fetch current tags, which can be cached and updated on WebSocket event.

",pull-request-available,['ambari-web'],AMBARI,Task,Critical,2018-04-17 13:58:03,4
13152988,Ambari web should escape slash in config names,"Should escape property's name ""a/b"" => ""a\u002Fb"".

",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-17 12:34:18,4
13152744,Ambari Schema Upgrade Failing source 2.6.1.0 target 2.7.0.0,"{code:title=ambari-server.log}
16 Apr 2018 11:04:47,676 INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD status VARCHAR2(255) NULL
16 Apr 2018 11:04:47,695 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.sql.SQLSyntaxErrorException: ORA-00904: ""PENDING"": invalid identifier

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)
 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)
 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)
 at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:227)
 at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531)
 at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:195)
 at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1036)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1336)
 at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1845)
 at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1810)
 at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:294)
 at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:829)
 at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:632)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateStageTable(UpgradeCatalog270.java:842)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:283)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
16 Apr 2018 11:04:47,695 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: ORA-00904: ""PENDING"": invalid identifier

at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.sql.SQLSyntaxErrorException: ORA-00904: ""PENDING"": invalid identifier

at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:447)
 at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
 at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:951)
 at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:513)
 at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:227)
 at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:531)
 at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:195)
 at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:1036)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1336)
 at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1845)
 at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1810)
 at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:294)
 at org.apache.ambari.server.orm.DBAccessorImpl.updateTable(DBAccessorImpl.java:829)
 at org.apache.ambari.server.orm.DBAccessorImpl.addColumn(DBAccessorImpl.java:632)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.updateStageTable(UpgradeCatalog270.java:842)
 at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:283)
 at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
 at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
 ... 1 more
{code}",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-04-16 14:48:05,5
13152679,Register VDF failing for AmazonLinux2 ,"
    
    -bash-4.2# curl -v -k -u admin:admin -H ""X-Requested-By:ambari"" -X POST http://ctr-e138-1518143905142-226218-01-000002.hwx.site:8080/api/v1/version_definitions -d '{ ""VersionDefinition"": { ""version_url"": ""http://s3.amazonaws.com/dev.hortonworks.com/HDP/amazonlinux2/3.x/BUILDS/3.0.0.0-1189/HDP-3.0.0.0-1189.xml"" } }'
    Note: Unnecessary use of -X or --request, POST is already inferred.
    *   Trying 172.27.80.4...
    * TCP_NODELAY set
    * Connected to ctr-e138-1518143905142-226218-01-000002.hwx.site (172.27.80.4) port 8080 (#0)
    * Server auth using Basic with user 'admin'
    > POST /api/v1/version_definitions HTTP/1.1
    > Host: ctr-e138-1518143905142-226218-01-000002.hwx.site:8080
    > Authorization: Basic YWRtaW46YWRtaW4=
    > User-Agent: curl/7.55.1
    > Accept: */*
    > X-Requested-By:ambari
    > Content-Length: 151
    > Content-Type: application/x-www-form-urlencoded
    >
    * upload completely sent off: 151 out of 151 bytes
    < HTTP/1.1 500 Internal Server Error
    < Date: Fri, 13 Apr 2018 10:09:15 GMT
    < X-Frame-Options: DENY
    < X-XSS-Protection: 1; mode=block
    < X-Content-Type-Options: nosniff
    < Cache-Control: no-store
    < Pragma: no-cache
    < X-Content-Type-Options: nosniff
    < X-Frame-Options: DENY
    < Set-Cookie: AMBARISESSIONID=node018hn2df5vsjqjuo7g1yr9mvno8.node0;Path=/;HttpOnly
    < Expires: Thu, 01 Jan 1970 00:00:00 GMT
    < User: admin
    < Content-Type: text/plain;charset=utf-8
    < Transfer-Encoding: chunked
    <
    {
      ""status"" : 500,
      ""message"" : ""An internal system exception occurred: Could not load url from http://s3.amazonaws.com/dev.hortonworks.com/HDP/amazonlinux2/3.x/BUILDS/3.0.0.0-1189/HDP-3.0.0.0-1189.xml.  null""
    * Connection #0 to host ctr-e138-1518143905142-226218-01-000002.hwx.site left intact
    }-bash-4.2#
    -bash-4.2#
    

Build Used:

    
    
    
    -bash-4.2# cat /etc/yum.repos.d/ambari.repo
    #VERSION_NUMBER=2.7.0.0-309
    [ambari-2.7.0.0-309]
    name=ambari Version - ambari-2.7.0.0-309
    baseurl=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-309
    gpgcheck=1
    gpgkey=http://s3.amazonaws.com/dev.hortonworks.com/ambari/amazonlinux2/2.x/BUILDS/2.7.0.0-309/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
    enabled=1
    priority=1
    

",pull-request-available,[],AMBARI,Bug,Major,2018-04-16 10:22:33,0
13152342,Ambari shows invalid passwords as plaintext,"While installing an HDF cluster, if you provide a password which is not valid for NiFi toolkit (eg. it is less than 12 characters) the password you entered is displayed in the error message as plaintext.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-13 16:55:25,2
13152313,NN Federation Service Pages Changes,"* In HDFS Service Summary, we need to remove the ""Service Metrics"" label, and remove the horizontal whitespace we've added between the service metrics  and the ns metrics.
* In the HDFS Metrics section, we need to add ""NAMESPACE"" to the left of the Namespace drop down menu.
* We need to also make sure we have the namespace label in the metric names, just like we did for the Dashboard HDFS metrics",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-13 15:11:29,2
13152252,Navigation sub-menu style changes,"The additional top and bottom padding is for the sub-menu container should be added (5px).
There needs to be more padding between the Service name and the status indicator, they're too tight now

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-13 10:45:49,4
13152223,Hosts page: Combo filter should use hostname as default filter,"if a user enters a value without specifying category we should treat as a hostname (Hostname as default category).

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-13 09:04:27,4
13152031,Restore configs does not work,"Steps:
 # Create custom property.
 # Restart all recommended services.
 # Roll back the version number to the one noted earlier.
 # Issue 1 - restart icon was not appeared.
 # Force restart required services.
 # Compare the difference in configs between current and previous version and notice the config has been removed as expected.
 # Issue 2 - check service's config file was not updated on hosts. ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-12 16:47:43,1
13152001,Alerts Are Not Updated in the UI,"Alerts in the UI are not updated automatically. You must hard refresh the page or constantly navigate around to get them to show up correctly.

STR:
- Deploy a simple cluster, just with ZK. 
- Kill one of the ZK servers manually (or just stop it) and wait.
- The APIs will update with the newly triggered alert, but the UI will not

In some cases, navigating around will show inconsistent alerts:
- The alerts hidden under the alert icon never change without a hard refresh
- Visiting the page of an alert definition does show updated data, but it no longer changes to reflect the current state.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-12 14:35:23,4
13152000,Regenerate Keytabs After Ambari Upgrade leads to AMS start failure due to change in one core-site.xml config.,"1) Upgrade Ambari from 2.6.0.0-267 to 2.7.0.0-292
2) Upgrade Non Stack Services ( Smartsense , Ambari Infra , Logsearch)
3) Regenerate Keytabs 
At this step , hadoop.proxyuser.HTTP.hosts property in HDFS core-site undergoes a change. Due to this , AMS start fails with below error.


{code:java}
2018-04-10 19:34:41,668 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2801)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:64)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:87)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2816)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2799)
	... 5 more
Caused by: java.lang.IllegalArgumentException: Could not parse [${clusterHostInfo/webhcat_server_host|append(core-site/hadoop.proxyuser.HTTP.hosts]
	at org.apache.commons.net.util.SubnetUtils.calculate(SubnetUtils.java:220)
	at org.apache.commons.net.util.SubnetUtils.<init>(SubnetUtils.java:52)
	at org.apache.hadoop.util.MachineList.<init>(MachineList.java:109)
	at org.apache.hadoop.util.MachineList.<init>(MachineList.java:83)
	at org.apache.hadoop.util.MachineList.<init>(MachineList.java:75)
	at org.apache.hadoop.security.authorize.DefaultImpersonationProvider.init(DefaultImpersonationProvider.java:97)
	at org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration(ProxyUsers.java:75)
	at org.apache.hadoop.security.authorize.ProxyUsers.refreshSuperUserGroupsConfiguration(ProxyUsers.java:85)
	at org.apache.hadoop.hbase.security.HBasePolicyProvider.init(HBasePolicyProvider.java:54)
	at org.apache.hadoop.hbase.ipc.RpcServer.start(RpcServer.java:2090)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.start(RSRpcServices.java:1042)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:628)
	... 10 more
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-12 14:29:51,6
13151975,Refactor base_alert to support multiple nameservices,"_NameNode Web UI_ alert is in critical state for all NameNodes with the
    
    
    
    [Alert][namenode_webui] HA nameservice value is present but there are no aliases for {{hdfs-site/dfs.ha.namenodes.ns1,ns2}}
    
",pull-request-available,[],AMBARI,Bug,Major,2018-04-12 13:25:03,0
13151919,NN federation: switching namespaces for dashboard widgets doesn't work,"After switching NN namespace for dashboard widgets nothing happens on UI, and also JS error is thrown:
{noformat}
Uncaught Error: assertion failed: Emptying a view in the inBuffer state is not allowed and should not happen under normal circumstances. Most likely there is a bug in your application. This may be due to excessive property change notifications.
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-12 10:42:21,2
13151884,"Unable to delete Slider after Ambari upgrade, due to Hive dependency","*STR*
 # Deploy HDP-2.6.3.0 with Ambari-2.6.0.0 (that stack includes services such as Slider, Mahout etc. which are unsupported in HDP-3.0 stack)
 # Upgrade Ambari to 2.7.0.0
 # Try to delete Slider (this is required since stack upgrade to HDP-3.0 needs the unsupported services to be removed)

*Result*
Slider prompts to delete Hive due to dependency

We need to remove the dependency, else asking customer to delete Hive may not be viable",express_upgrade pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-12 08:40:32,3
13151777,Ambari Metrics references outdated JARs,"GitHub PR builder is failing on some Jenkins nodes due to references to outdated Hadoop, etc. JARs that are no longer available.

{noformat}
[ERROR] Failed to execute goal on project ambari-metrics-timelineservice: Could not resolve dependencies for project org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT: The following artifacts could not be resolved: org.apache.phoenix:phoenix-core:jar:5.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-common:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-annotations:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-common:jar:tests:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-common:jar:tests:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-common:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-api:jar:3.0.0.3.0.0.2-97, org.apache.hadoop:hadoop-yarn-server-common:jar:3.0.0.3.0.0.2-97, org.apache.phoenix:phoenix-core:jar:tests:5.0.0.3.0.0.2-97, org.apache.hbase:hbase-it:jar:tests:2.0.0.3.0.0.2-97, org.apache.hbase:hbase-testing-util:jar:2.0.0.3.0.0.2-97
{noformat}

https://builds.apache.org/view/A/view/Ambari/job/Ambari-Github-PullRequest-Builder/1744/console
https://builds.apache.org/view/A/view/Ambari/job/Ambari-Github-PullRequest-Builder/1791/console
etc.",pull-request-available,['ambari-metrics'],AMBARI,Bug,Critical,2018-04-11 20:56:32,5
13151701,NameNode namespaces aren't displayed after HDFS page refresh,"*STR*
# Enable NameNode federation
# Go to HDFS section (summary or any other tab)
# Refresh page

*Expected result*
Namespaces data is displayed (sections on summary page, items of Service actions dropdown)

*Actual result*
- No namespaces data is displayed
- Service actions dropdown is empty
- JS error thrown: {{app.js:68529 Uncaught TypeError: Cannot read property 'contains' of undefined}}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-11 17:01:40,2
13151654,Comparing config versions style tweaks,"* Make current ""selected"" font color and button border color look incorrect
* Fix bottom padding
* See screenshots",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-11 13:45:00,4
13151644,UI Performance Tuning,"Since we've made a ton of change to the UI, I'm noticing some slowness in page load times and interaction. 
Specific Areas I'm noticing slowness in:
* Initial Ambari UI Page Load
* Initial Dashboard UI Page Load
* Service Actions Menu Loading
* Quicklinks Loading
* Service Configurations Tab Loading",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-11 13:06:36,4
13151629,Sometimes host checks never complete,"During the install wizard, the host checks that get run immediately after hosts are registered sometimes never complete.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-11 12:19:22,1
13151596,Not able to add a user in ambari,"1) login to ambari ui with admin:admin

2) select ADMIN -> Manage Ambari
 3) click on users section from the left pane
 4) click on Add users button
 5) fill in required details:

username: cloudbreak

password: admin

Add Roles For This user: None

is this admin user? : YES

Deactivate this user? : ACTIVE

 

6) click save

User creation error has been thrown (see screenshot)",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-11 09:34:23,1
13151568,Fix TestCustomServiceOrchestrator.py and TestRegistration.py on branch-3.0-perf,"TestCustomServiceOchestrator requires rewriting since configs building routine
changed quite a lot on branch-3.0-perf

",pull-request-available,[],AMBARI,Bug,Major,2018-04-11 07:07:39,0
13151464,Debian9: Atlas client installation failed due to unsupported OS family error,"Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 37, in <module>
    BeforeInstallHook().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 353, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/hook.py"", line 34, in hook
    install_packages()
  File ""/var/lib/ambari-agent/cache/stack-hooks/before-INSTALL/scripts/shared_initialization.py"", line 37, in install_packages
    retry_count=params.agent_stack_retry_count)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 125, in __new__
    cls(names_list.pop(0), env, provider, **kwargs)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/environment.py"", line 119, in run_action
    provider = provider_class(resource)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/packaging.py"", line 26, in __init__
    self._pkg_manager = ManagerFactory.get()
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/__init__.py"", line 46, in get
    cls.__repo_manager = cls.get_new_instance(OSCheck.get_os_family())
  File ""/usr/lib/ambari-agent/lib/ambari_commons/repo_manager/__init__.py"", line 71, in get_new_instance
    raise RuntimeError(""Not able to create Repository Manager object for unsupported OS family {0}"".format(os_family))
RuntimeError: Not able to create Repository Manager object for unsupported OS family debian
Attempt Count :1",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-04-10 19:08:46,3
13151403,Document enabling SSO via CLI,"Document enabling SSO via CLI

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-04-10 14:54:56,7
13151388,Usability: Generate blueprint should download a single Zip file and not 2 files.,"During the installation wizard, just before deploy, the user has the opportunity to export a blueprint. This will download two files, but the behavior in the browser is odd as the user has to accept that multiple files will be downloaded. It would be better if it just downloaded a single zip file with both the blueprint and cluster creation template included in it. Much like our client configuration export.",pull-request-available,['ambari-web'],AMBARI,Improvement,Critical,2018-04-10 13:55:21,4
13151375,Slash not unescaped in property name,"After saving config with name ""a/b"" displayed as ""a\u002Fb"".",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-10 13:25:29,4
13151367,Missing unlimited JCE policy installation on ambari-agent side,"*STR:*
1. install Ambari 2.7.0.0 (build #220) server/agents on Centos7 machines (this issue is not OS dependent)
2. install an HDP 3.0 cluster with HDFS
3. check if unlimited JCE policy has been installed on the hosts where only ambari-agent has been installed:
{code}
[root@c7402 ~]# /usr/jdk64/jdk1.8.0_112/bin/java -jar /var/lib/ambari-agent/tools/jcepolicyinfo.jar -tu
{code}
*Result:*
{code}
[root@c7402 ~]# /usr/jdk64/jdk1.8.0_112/bin/java -jar /var/lib/ambari-agent/tools/jcepolicyinfo.jar -tu
Unlimited Key JCE Policy: false
{code}
*Expected result:*
{code}
[root@c7402 ~]# /usr/jdk64/jdk1.8.0_112/bin/java -jar /var/lib/ambari-agent/tools/jcepolicyinfo.jar -tu
Unlimited Key JCE Policy: true
{code}
*Additional information:*
unlimited JCE policy in agent side is triggered by a hook: ambari-server/src/main/resources/stack-hooks/before-START/scripts/hook.py:38 -> ambari-server/src/main/resources/stack-hooks/before-START/scripts/shared_initialization.py

In this Python code we check if we really need to install unlimited JCE:
{code}
  elif not params.unlimited_key_jce_required:
    Logger.debug(""Skipping unlimited key JCE policy check and setup since it is not required"")
{code}
This parameter is being set in params_linux.py:
{code}
unlimited_key_jce_required = default(""/componentLevelParams/unlimited_key_jce_required"", False)
{code}
However org/apache/ambari/server/controller/AmbariManagementControllerImpl.java:2590 sets it in the host level parameters (and not in component level params)",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2018-04-10 12:54:12,6
13150783,Format ZKFC fails while adding new HDFS namespace through UI in secure environment,"Tried to add new HDFS namespace with latest available build from yesterday
evening #262  
Format ZKFC is failing with below error

    
    
    
    18/04/05 16:30:10 INFO zookeeper.ClientCnxn: Session establishment complete on server ctr-e138-1518143905142-202626-01-000006.hwx.site/172.27.29.151:2181, sessionid = 0x26293f2ead15e74, negotiated timeout = 9000
    18/04/05 16:30:10 INFO ha.ActiveStandbyElector: Session connected.
    18/04/05 16:30:10 ERROR ha.ZKFailoverController: The failover controller encounters runtime error
    java.io.IOException: Couldn't create /hadoop-ha/ns1
    	at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:358)
    	at org.apache.hadoop.ha.ZKFailoverController.formatZK(ZKFailoverController.java:286)
    	at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:216)
    	at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:60)
    	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:175)
    	at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:171)
    	at java.security.AccessController.doPrivileged(Native Method)
    	at javax.security.auth.Subject.doAs(Subject.java:360)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1662)
    	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:480)
    	at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:171)
    	at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:195)
    Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hadoop-ha/ns1
    	at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
    	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
    	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
    	at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)
    	at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)
    	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)
    	at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)
    	at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)
    	at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)
    	... 11 more
    18/04/05 16:30:11 INFO zookeeper.ZooKeeper: Session: 0x26293f2ead15e74 closed
    18/04/05 16:30:11 FATAL tools.DFSZKFailoverController: DFSZKFailOverController exiting due to earlier exception java.io.IOException: Couldn't create /hadoop-ha/ns1
    18/04/05 16:30:11 INFO zookeeper.ClientCnxn: EventThread shut down
    18/04/05 16:30:11 INFO util.ExitUtil: Exiting with status 1: java.io.IOException: Couldn't create /hadoop-ha/ns1
    18/04/05 16:30:11 INFO tools.DFSZKFailoverController: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down DFSZKFailoverController at ctr-e138-1518143905142-202626-01-000005.hwx.site/172.27.26.14
    ************************************************************/
    
",pull-request-available,[],AMBARI,Bug,Major,2018-04-07 15:26:39,0
13150652,Fixes for NameNode widgets on dashboard,"- Make widgets sortable
- Fix Edit Delete features
- Make widgets able to be added
- Fix pie chart colors",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-06 17:12:00,2
13150579,Server should acknowledge api endpoint about host-component state before deletion,Server should send last state of removed host-component to api topology stomp endpoint.,pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-04-06 12:13:04,1
13150538,NN Federation Wizard: Bootstrap NameNode failed,"Bootstrap NameNode command fails due to stopped NameNode. Actually NameNode
should be started and command to start it was sent correctly from UI, but was
ignored by BE due to:

    
    
    Ignoring ServiceComponentHost as the current state matches the new desired state, clusterName=c, serviceName=HDFS, componentName=NAMENODE, hostname=c7404.ambari.apache.org, currentState=STARTED, newDesiredState=STARTED

This is because Format NameNode command starts NameNode for a little period of
time and then stops it back, but component's current state is not updated
immediately and BE thinks that NameNode is still started.  
From Format NameNode log:

    
    
    
    18/04/04 11:44:31 INFO namenode.NameNode: STARTUP_MSG: 
    /************************************************************
    STARTUP_MSG: Starting NameNode
    
    ...
    
    18/04/04 11:44:35 INFO namenode.NameNode: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down NameNode at c7404.ambari.apache.org/192.168.74.104
    

",pull-request-available,[],AMBARI,Bug,Major,2018-04-06 08:30:15,0
13150480,Move SSO-related properties from ambari.properties into the Ambari DB,"Move SSO-related properties from ambari.properties into the Ambari DB so that Ambari is able to use the SSO-related properties without needing to be restarted.

The following properties should be moved:

||Original Name||New Name||
|authentication.jwt.providerUrl|ambari.sso.provider.url|
|authentication.jwt.publicKey|ambari.sso.provider.publicKey|
|authentication.jwt.originalUrlParamName|ambari.sso.provider.originalUrlParamName|
|authentication.jwt.enabled|ambari.sso.authentication.enabled|
|authentication.jwt.audiences|ambari.sso.jwt.audiences|
|authentication.jwt.cookieName|ambari.sso.jwt.cookieName|
",pull-request-available sso,['ambari-server'],AMBARI,Task,Major,2018-04-06 00:09:00,7
13150449,Fix Kerberos service documentation for Ambari 2.6.x,Fix Kerberos service documentation for Ambari 2.6.x,pull-request-available,['ambari-server'],AMBARI,Task,Minor,2018-04-05 21:54:13,7
13150446,Update Kerberos service documentation for Ambari 2.7.0,Update Kerberos service documentation for Ambari 2.7.0,pull-request-available,['ambari-server'],AMBARI,Task,Minor,2018-04-05 21:45:20,7
13150370,Error encountered while disabling Kerberos on HDF 3.2 using Ambari 2.7.0.0,"Ambari version is 2.7.0.0-182

1. I installed HDF 3.2 on an openstack using mpack
2. Enabled Kerberos which went okay and services were all restarted
3. Disabling Kerberos throws following exception

{code}
21 Mar 2018 23:10:08,794 ERROR [Server Action Executor Worker 266] AmbariJpaLocalTxnInterceptor:180 - [DETAILED ERROR] Rollback reason: 
Local Exception Stack: 
Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
Error Code: 0
Call: DELETE FROM kerberos_principal WHERE (principal_name = ?)
bind => [1 parameter bound]
at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1620)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:900)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:964)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:633)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1845)
at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4300)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5592)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:285)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:134)
at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:153)
at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)
at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)
at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$fe23de42.remove(<generated>)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:143)
at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:435)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:88)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
at java.lang.Thread.run(Thread.java:745)
Caused by: org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255)
at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:363)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:892)
... 22 more
21 Mar 2018 23:10:08,795 ERROR [Server Action Executor Worker 266] AmbariJpaLocalTxnInterceptor:188 - [DETAILED ERROR] Internal exception (1) :
org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2161)
at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:1890)
at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:255)
at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417)
at org.postgresql.jdbc2.AbstractJdbc2Statement.executeUpdate(AbstractJdbc2Statement.java:363)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:892)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:964)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:633)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1845)
at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4300)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5592)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:285)
at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:134)
at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:153)
at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)
at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)
at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$fe23de42.remove(<generated>)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:143)
at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:435)
at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:88)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
at java.lang.Thread.run(Thread.java:745)
21 Mar 2018 23:10:08,795 WARN [Server Action Executor Worker 266] DestroyPrincipalsServerAction:173 - Failed to remove identity for amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM from the Ambari database - Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.6.2.v20151217-774c696): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: update or delete on table ""kerberos_principal"" violates foreign key constraint ""fk_kkp_principal_name"" on table ""kerberos_keytab_principal""
Detail: Key (principal_name)=(amshbase/priyank-3-2-2.openstacklocal@EXAMPLE.COM) is still referenced from table ""kerberos_keytab_principal"".
Error Code: 0
Call: DELETE FROM kerberos_principal WHERE (principal_name = ?)
bind => [1 parameter bound]
21 Mar 2018 23:10:08,795 INFO [Server Action Executor Worker 266] DestroyPrincipalsServerAction:117 - Destroying identity, nimbus/priyank-3-2-1.openstacklocal@EXAMPLE.COM
21 Mar 2018 23:10:08,824 INFO [Server Action Executor Worker 266] DestroyPrincipalsServerAction:117 - Destroying identity, ambari-qa-cl1@EXAMPLE.COM
21 Mar 2018 23:10:08,864 INFO [Server Action Executor Worker 266] KerberosServerAction:456 - Processing identities completed.
{code}

This happens when the same principal is stored in multiple keytabs.",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-04-05 16:07:58,3
13150326,Utility function to parse initial active namenode hostnames for blueprint deployment,The goal of this task is to provide a utility function in the resource management libraries to parse initial active namenode hostnames from {{hadoop-env}} for blueprint deployment.  These properties may be specified by the user or be set by {{BlueprintConfigurationProcessor}} (AMBARI-23467).  The utility function can be used by stack scripts.,pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-04-05 15:00:57,5
13150283,UI option to stop HDFS service doesnt work - JS Error,"HDFS Stop option from UI doesn't work. No operation is triggered

Below Js Error is seen in the console.
Ambari Build - 2.7.0.0-263

{code:java}
app.js:28031 Uncaught TypeError: Cannot read property 'set' of undefined
    at app.js:28031
    at Class.onPrimary (app.js:211716)
    at Class.newFunc [as onPrimary] (vendor.js:12954)
    at handler (vendor.js:31554)
    at HTMLButtonElement.<anonymous> (vendor.js:23346)
    at HTMLDivElement.dispatch (vendor.js:3178)
    at HTMLDivElement.elemData.handle (vendor.js:2854)
{code}
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-04-05 12:37:45,4
13150260,NameNode HA: QuickLinks section issues,"When NameNode HA is enabled, the QuickLink section shows the hostname of each NameNode as a hyperlink.  This is confusing since these are not meant to be clickable; they should be displayed as labels, not hyperlinks.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-04-05 10:19:10,2
13150199,None of the Ambari View works because of  'X-Frame-Options' to 'deny' error,"In Ambari 2.7.0,  While accessing any View like HiveView 2.0 we see a blank page.
On te Browser Debugger Console we see the XFrame related error as following:

{code}
Refused to display 'http://latest1.example.com:8080/#/login?targetURI=/views/HIVE/2.0.0/Hive20/' in a frame because it set 'X-Frame-Options' to 'deny'.

Uncaught DOMException: Blocked a frame with origin ""http://latest1.example.com:8080"" from accessing a cross-origin frame.
    at Class.resizeFunction (http://latest1.example.com:8080/javascripts/app.js:237835:78)
    at http://latest1.example.com:8080/javascripts/app.js:237810:12
{code} ",pull-request-available,"['ambari-views', 'ambari-web']",AMBARI,Bug,Major,2018-04-05 02:26:22,1
13150077,Server should support saving configs with names which contains slash,"PUT request to url: /api/v1/clusters/c1
Data:
{noformat}
[{""Clusters"":{""desired_config"":[{""type"":""zoo.cfg"",""properties"":{""autopurge.purgeInterval"":""24"",""autopurge.snapRetainCount"":""30"",""dataDir"":""/hadoop/zookeeper"",""tickTime"":""3000"",""initLimit"":""10"",""syncLimit"":""5"",""clientPort"":""2181"",""prop/my"":""1""},""service_config_version_note"":""""}]}}]
{noformat}
was successful, but property ""prop/my"" wasn't saved.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-04-04 17:06:41,1
13150065,NN federation related fixes for host details page,"- Fix colors for pie chart
- Add NameNode uptime widget
- Content of widgets isn't updated",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-04 16:28:57,2
13150036,Fix TestHeartbeatHandler.testComponents NPE,"This issue is intermittent:

{noformat}
ERROR] testComponents(org.apache.ambari.server.agent.TestHeartbeatHandler)  Time elapsed: 1.116 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.ambari.server.agent.TestHeartbeatHandler.testComponents(TestHeartbeatHandler.java:1351)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-04-04 15:19:37,7
13150015,ServiceAdvisor KeyError during kerberization of OneFS,"{code:java}
Error message: Stack Advisor reported an error. Exit Code: 2. Error: KeyError: 'onefs'

StdOut file: /var/run/ambari-server/stack-recommendations/5/stackadvisor.out

 

StdErr file: /var/run/ambari-server/stack-recommendations/5/stackadvisor.err

 

Stackadvisor.err shows the following traceback:

 

2018-04-03 16:09:09,711 ERROR HDP26StackAdvisor getCapacitySchedulerProperties: - Couldn't retrieve 'capacity-scheduler' from services.

Traceback (most recent call last):

  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 167, in <module>

    main(sys.argv)

  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main

    result = stackAdvisor.recommendConfigurations(services, hosts)

  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1565, in recommendConfigurations

    serviceAdvisor.getServiceConfigurationRecommendations(configurations, clusterSummary, services, hosts)

  File ""/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ONEFS/service_advisor.py"", line 112, in getServiceConfigurationRecommendations

    onefs_host = Uri.onefs(services)

  File ""/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ONEFS/service_advisor.py"", line 41, in onefs

    return self.from_config(configs, 'onefs', 'onefs_host')

  File ""/var/lib/ambari-server/resources/stacks/HDP/2.6/services/ONEFS/service_advisor.py"", line 45, in from_config

    return Uri(configs['configurations'][config_type]['properties'][property_name])

KeyError: 'onefs'{code}
 ",pull-request-available,['ambari-agent'],AMBARI,Improvement,Major,2018-04-04 14:15:15,3
13149983,Improve parallel start performance,Reduce the amount of work performed while holding {{configs_lock}}.,pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2018-04-04 12:28:48,5
13149939,Fix connection drop on ambari-agent by locking the write code,"This solution can possibly work to fix connection drop

    
    
    ERROR 2018-04-04 00:22:26,771 websocket.py:272 - Failed to receive data
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_ws4py/websocket.py"", line 394, in once
        b = self.sock.recv(self.reading_buffer_size)
      File ""/usr/lib64/python2.7/ssl.py"", line 759, in recv
        return self.read(buflen)
      File ""/usr/lib64/python2.7/ssl.py"", line 653, in read
        v = self._sslobj.read(len or 1024)
    SSLError: [SSL: SSLV3_ALERT_BAD_RECORD_MAC] sslv3 alert bad record mac (_ssl.c:1783)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-04-04 09:43:37,0
13149811,404 error while setting up SSO on a new cluster ,"When setting up SSO via the Ambari server CLI using a new Ambari server instance, the following error is encountered:
{noformat}
[root@c7401 ~]# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Enter Ambari Admin login: admin
Enter Ambari Admin password:

Fetching SSO configuration from DB.ERROR: Exiting with exit code 1.
REASON: Error while fetching SSO configuration. Error details: HTTP Error 404: Not Found
{noformat}
This is caused by the lack of the {{sso-configuration}} category in the Ambari configuration set - which will be a common scenario for new Ambari clusters.
{noformat}
GET /api/v1/services/AMBARI/components/AMBARI_SERVER/configurations/sso-configuration
{noformat}
{noformat}
{
  ""status"" : 404,
  ""message"" : ""The requested resource doesn't exist: RootServiceComponentConfiguration not found where Configuration/service_name=AMBARI AND Configuration/component_name=AMBARI_SERVER AND Configuration/category=sso-configuration.""
}
{noformat}
The CLI handle this response and assume the following default values:
{noformat}
ambari.sso.manage_services = false
ambari.sso.enabled_services = 
{noformat}
 ",SSO pull-request-available security,['ambari-server'],AMBARI,Bug,Critical,2018-04-03 18:49:22,6
13149733,Fix stack issues in HDFS to support Namenode Federation setup,"For example, here are 2 things I found to be probably wrong.

1\. Journal node restart failed because we cannot find hdfs-site :
dfs.journalnode.edits.dir. We delete that property in the wizard. We may have
to change that to dfs.journalnode.edits.dir.&lt;nameservice&gt;

2\. The following snippet in params_linux.py on HDFS 3.0 stack seems wrong. It
has been designed to work with only 1 nameservice.

    
    
    
    dfs_ha_enabled = False
    dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.internal.nameservices', None)
    if dfs_ha_nameservices is None:
      dfs_ha_nameservices = default('/configurations/hdfs-site/dfs.nameservices', None)
    dfs_ha_namenode_ids = default(format(""/configurations/hdfs-site/dfs.ha.namenodes.{dfs_ha_nameservices}""), None)
    

3\. After setting up NN Fed, when I restart namenodes, I see the following
error.

    
    
    
        main_resource.resource.security_enabled, main_resource.resource.logoutput)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/providers/hdfs_resource.py"", line 154, in __init__
        security_enabled, run_user)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/namenode_ha_utils.py"", line 204, in get_property_for_active_namenode
        if INADDR_ANY in value and rpc_key in hdfs_site:
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
        raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
    resource_management.core.exceptions.Fail: Configuration parameter 'dfs.namenode.https-address.ns2.nn1' was not found in configurations dictionary!
    

This is probably because the namenode_ha_utils is not equipped to handle
multiple nameservices.

We may have to create to fix such stack errors when the wizard is done.

",pull-request-available,[],AMBARI,Bug,Major,2018-04-03 12:10:51,0
13149730,Remove unnecessary properties from the Ranger SSO configuration updates via the stack advisor,"Remove unnecessary properties from the Ranger SSO configuration updates via the stack advisor

* {{ranger.sso.cookiename}}
* {{ranger.sso.query.param.originalurl}}",pull-request-available,['ambari-admin'],AMBARI,Task,Major,2018-04-03 12:07:42,7
13149723,Cannot save edited repository urls in Ambari Version UI,"When attempting to Save an edited Repository URL the browser reports the following error (Chrome):


{noformat}
vendor.js:26942 TypeError: Cannot read property 'primaryClass' of undefined
    at new $modal.open.controller (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/main.js:7972:41)
    at Object.instantiate (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:17529:14)
    at $controller (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:23350:28)
    at resolveSuccess (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:52701:32)
    at processQueue (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:29439:28)
    at http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:29455:27
    at Scope.$eval (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:30737:28)
    at Scope.$digest (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:30551:31)
    at Scope.$apply (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:30845:24)
    at done (http://ydavis-hdf-bug-94554-1.openstacklocal:8080/views/ADMIN_VIEW/2.7.0.0/INSTANCE/scripts/vendor.js:24825:47) undefined
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-03 11:36:26,4
13149717,"After enabling Kerberos, the Ambari JAAS file is not updated","After enabling Kerberos, the Ambari JAAS file is not updated. This leads to various errors like collecting JXM data from services:
{noformat}
28 Mar 2018 15:40:29,041  WARN [ambari-metrics-retrieval-service-thread-4] RequestTargetAuthentication:88 - NEGOTIATE authentication error: No valid credentials provided (Mechanism level: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials fai
led! (null)))
28 Mar 2018 15:40:29,042 ERROR [ambari-metrics-retrieval-service-thread-4] AppCookieManager:122 - SPNego authentication failed, can not get hadoop.auth cookie for URL: http://c7401.ambari.apache.org:50070/jmx
28 Mar 2018 15:40:29,042 ERROR [ambari-metrics-retrieval-service-thread-5] AppCookieManager:122 - SPNego authentication failed, can not get hadoop.auth cookie for URL: http://c7401.ambari.apache.org:50070/jmx?get=Hadoop:service=NameNode,name=FSNamesystem::tag.HAState
2
{noformat}
The JAAS file as {{/etc/ambari-server/conf/krb5JAASLogin.conf}} is expected to be updated to match the created Kerberos identity for the Ambari server, but is not:

The default values of
{noformat}
    ...
    keyTab=""/etc/security/keytabs/ambari.keytab""
    principal=""ambari@EXAMPLE.COM""
    ...
{noformat}
Should have been changed to
{noformat}
    ...
    keyTab=""/etc/security/keytabs/ambari.server.keytab""
    principal=""ambari-server-c1@EXAMPLE.COM""
    ...
{noformat}
After manually fixing this and restarting Ambari, the JMX requests authenticated properly.",kerberos pull-request-available security,['ambari-server'],AMBARI,Bug,Critical,2018-04-03 11:09:45,6
13149585,Unable to parse task structured output when disabling Kerberos,"The following error is seen in the log when disabling Kerberos:

{noformat}
Unable to parse task structured output: /var/lib/ambari-agent/data/structured-out-5.json
{noformat}

This appears to be caused by an empty value being returned from {{resource_management.libraries.script.script.Script#disable_security}}.  Rather then returning an empty value ({{None}}), return {{""\{\}""}} instead. 

",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-04-02 21:01:15,7
13149525,JS error while deploying new service,"{noformat}
""<DS.StateManager:ember27929> could not respond to event setProperty in state rootState.loading.""
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-04-02 16:47:33,4
13149469,Dashboard: add Yarn Containers widget,"Title: ""YARN containers""
Content:   <#allocated>/<#pending>/<#reserved>",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-04-02 11:29:25,4
13149186,Fix and cleanup ClientConfigResourceProviderTest and TestHeartbeatHandler,"Fix and cleanup ClientConfigResourceProviderTest.  At least the following error occurs when running this unit test:

{noformat:title=org.apache.ambari.server.controller.internal.ClientConfigResourceProviderTest#testGetResources}
java.lang.AssertionError: 
  Unexpected method call Configuration.getExternalScriptThreadPoolSize():
    Configuration.getResourceDirPath(): expected: 1, actual: 1
    Configuration.getJavaHome(): expected: 1, actual: 0
{noformat}

Fix and cleanup TestHeartbeatHandler.  At least the following error occurs when running this unit test:

{noformat:title=org.apache.ambari.server.agent.TestHeartbeatHandler#testComponents}
 TestHeartbeatHandler.testComponents:1351 » NullPointer
{noformat}



",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-30 15:33:43,7
13149166,Background Operations: icons have incorrect color,See screenshot,pull-request-available,['ambari-web'],AMBARI,Bug,Minor,2018-03-30 13:55:22,4
13149134,Invalid websocket client port when connected over https,"Websocket client has not ability to connect to endpoint with non-secure port:
{code}
WebSocket connection to 'wss://172.27.25.217:8080/api/stomp/v1/websocket' failed: Error in connection establishment: net::ERR_CONNECTION_REFUSED
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-30 10:10:29,4
13148948,Issues for no NN federation case appearing after federation support implementation,"- HDFS links dropdown on dashboard isn't displayed with JS error thrown: {{Uncaught TypeError: Cannot read property 'hosts' of undefined}}
- 'N/A' / 'Not Running' is displayed for NameNode Uptime instead of actual value
- Incorrect values in Service metrics section of service summary page
- Order of dashboard widgets is not restored
- Incorrect value of NameNode RPC on host summary page",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-29 17:22:12,2
13148892,Add ability to use latest configs for execution commands,Generally each execution command contains hash of configuration used with. Blueprint deploy workflow presumes posting configs updates after execution commands. Server should have ability to order agent to use last available configuration instead specified in command for command execution.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-29 14:30:13,1
13148845,Problems while selecting the property uniquely in Configurations screen while installation,"While scripting the automation code for the new UI installer, could not select some properties uniquely and some problems faced:
# Unable to select some properties by their label in xpath:
{panel}
* In Advanced zeppelin-config panel under Zeppelin configurations, there are a bunch of configurations related to zeppelin.ssl. As seen in screenshot-1, all the labels have zeppelin.ssl in them and if one were to type textbox with label zeppelin.ssl using XPATH strict text match, i.e. //*[text()=’zeppelin.ssl’], it fails. This is because within the label there are some html elements (see screenshot-1) like <wbr> which do not allow a strict match to the label.
* A simple workaround would be to use xpath contains() operator, but it would return multiple fields for the same label, which is not good.
{panel}
# Try to select the property by ‘data-qa’ also has similar problems to above
{panel}
As per screenshot-2, the data-qa attribute has the value ‘service-config-zeppelin-ssl-zeppelin-config-xml-default’ which can only be covered by xpath //*[contains(@data-qa,’zeppelin-ssl’)]. But this would again lead to multiple matches in the UI.
{panel}

Two asks from a test automation point of view:
# Remove any HTML entity from in between the label text
{panel}
Example: zeppelin.<wbr>ssl to zeppelin.ssl
{panel}
# Set the data-qa attribute for the input fields to just the property name as specified in Blueprint. For example, if the Blueprint property is ‘zeppelin.ssl’, the data-qa attribute can be ‘zeppelin-ssl’.
 
This would enable the automation code to uniquely identify the property fields.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-29 11:17:41,4
13148820,HDFS metrics data missing in Ambari 2.7.0,"n Ambari 2.7 with HDFS, Zookeeper, AMS, and Smart Sense (HDP 3.0) installed, the following alert is seen:
{noformat}
NameNode Directory Status
Failed directory count: None{noformat}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-03-29 09:32:54,3
13148819,PasswordUtilsTest fails if running as 'root',"If org.apache.ambari.server.utils.PasswordUtilsTest.shouldReadDefaultPasswordIfPasswordPropertyIsPasswordFilePathButItIsNotReadable() is running by _root_ user the File.setReadable does not work -> the build will fail.

We should consider using
java.nio.file.attribute.PosixFilePermissions
I'll disable this use case until it's fixed.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-29 09:30:29,6
13148592,Ambari Requests Too Much Data From Hosts When Logging In,"When first logging into Ambari, the web client makes the following request:
GET api/v1/clusters/<clusterName>/hosts?fields=
- Hosts/cpu_count
- Hosts/host_name
- Hosts/host_status
- Hosts/ip
- Hosts/last_agent_env
- Hosts/last_heartbeat_time
- Hosts/maintenance_state
- Hosts/ph_cpu_count
- Hosts/public_host_name
- Hosts/rack_info
- Hosts/total_mem
- alerts_summary
- host_components/HostRoles/desired_admin_state
- host_components/HostRoles/display_name
- host_components/HostRoles/maintenance_state
- host_components/HostRoles/service_name
- host_components/HostRoles/stale_configs
- host_components/HostRoles/state
- host_components/logging
- stack_versions/HostStackVersions
- stack_versions/repository_versions/RepositoryVersions/display_name
- stack_versions/repository_versions/RepositoryVersions/id
- stack_versions/repository_versions/RepositoryVersions/repository_version

In a cluster with 100 hosts, this can cause a payload of over 200MB to be returned. The culprit seems to be the {{Hosts/last_agent_env}} property, specifically the {{activeJavaProcs}} key. If the hosts in the cluster are running a lot of Java processes, this can account for over 90% of the payload (roughly 180MB). This data takes 30 or more seconds for the server to serialize and stream to the socket.

It looks like {{Hosts/last_agent_env}} isn't even needed after initially provisioning a host during the host checks, so it probably can be removed.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-28 14:40:59,4
13148583,Duplicate websocket subscription on Configs page,"Steps to reproduce:
# Open ambari web
# Go to HDFS
# Open Configs page
# Leave page

Actual Result: UI has subscribed to all topics again.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-28 14:23:16,4
13148550,Dashboard: rationalize default widgets,"See screenshot

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-28 11:59:36,4
13148540,Install Packages button on the versions screen fails if there is a service installed which was removed,"I've installed several services with Ambari 2.6.1 - HDP 2.6.4, then upgraded Ambari to 2.7. After this I've added a new HDP 3.0 repo. As Slider was installed which was removed from HDP 3.0, it displayed a white ""i"" in a blue circle in the 3.0 column (see the screenshot). As I'v clicked on the ""Install Packages"" button it returned an error message (see screenshot). I've checked the log, and it seems that the absence of the Slider service caused the issue:

{code}
org.apache.ambari.server.controller.spi.SystemException: Cannot obtain stack information for HDP-3.0
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:305)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.getHostVersionInstallCommand(ClusterStackVersionResourceProvider.java:723)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrchestration(ClusterStackVersionResourceProvider.java:634)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrUpdateHostVersions(ClusterStackVersionResourceProvider.java:532)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createResourcesAuthorized(ClusterStackVersionResourceProvider.java:474)
 at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.createResources(AbstractAuthorizedResourceProvider.java:231)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
 at org.apache.ambari.server.api.services.ClusterStackVersionService.createRequests(ClusterStackVersionService.java:120)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:564)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
 at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
 at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ambari.server.StackAccessException: Stack data, stackName=HDP, stackVersion=3.0, serviceName=SLIDER
 at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:540)
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:303)
 ... 114 more
org.apache.ambari.server.controller.spi.SystemException: Cannot obtain stack information for HDP-3.0
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:305)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.getHostVersionInstallCommand(ClusterStackVersionResourceProvider.java:723)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrchestration(ClusterStackVersionResourceProvider.java:634)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:118)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createOrUpdateHostVersions(ClusterStackVersionResourceProvider.java:532)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
 at org.apache.ambari.server.controller.internal.ClusterStackVersionResourceProvider.createResourcesAuthorized(ClusterStackVersionResourceProvider.java:474)
 at org.apache.ambari.server.controller.internal.AbstractAuthorizedResourceProvider.createResources(AbstractAuthorizedResourceProvider.java:231)
 at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
 at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
 at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
 at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
 at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
 at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
 at org.apache.ambari.server.api.services.ClusterStackVersionService.createRequests(ClusterStackVersionService.java:120)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
 at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
 at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
 at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
 at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
 at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
 at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
 at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
 at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
 at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
 at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
 at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
 at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
 at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
 at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
 at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
 at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
 at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
 at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
 at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:564)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
 at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
 at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
 at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.ambari.server.StackAccessException: Stack data, stackName=HDP, stackVersion=3.0, serviceName=SLIDER
 at org.apache.ambari.server.api.services.AmbariMetaInfo.getService(AmbariMetaInfo.java:540)
 at org.apache.ambari.server.state.stack.upgrade.RepositoryVersionHelper.buildRoleParams(RepositoryVersionHelper.java:303)
 ... 114 more
{code}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-03-28 11:22:43,3
13148503,PQS start fails after Ambari upgrade due to Bad file descriptor,"*STR*
PQS start after Ambari upgrade to 2.7.0 failed with below error:
{code}
Traceback (most recent call last):
 File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/phoenix_queryserver.py"", line 81, in 
 PhoenixQueryServer().execute()
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 356, in execute
 self.execute_prefix_function(self.command_name, 'post', env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute_prefix_function
 method(env)
 File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 419, in post_start
 raise Fail(""Pid file \{0} doesn't exist after starting of the component."".format(pid_file))
resource_management.core.exceptions.Fail: Pid file /var/run/hbase/phoenix-hbase-server.pid doesn't exist after starting of the component.
{code}

The log says the following:
{code}
2018-03-26 16:33:13.563112 launching /base/tools/jdk1.8.0_112/bin/java -cp /usr/hdp/current/hbase-client/conf:/etc/hadoop/conf:/grid/0/hdp/2.6.4.0-91/phoenix/bin/../phoenix-4.7.0.2.6.4.0-91-client.jar:/grid/0/hdp/2.6.4.0-91/phoenix/bin/../phoenix-4.7.0.2.6.4.0-91-queryserver.jar:/usr/hdp/2.6.4.0-91/hadoop/conf:/usr/hdp/2.6.4.0-91/hadoop/lib/*:/usr/hdp/2.6.4.0-91/hadoop/.//*:/usr/hdp/2.6.4.0-91/hadoop-hdfs/./:/usr/hdp/2.6.4.0-91/hadoop-hdfs/lib/*:/usr/hdp/2.6.4.0-91/hadoop-hdfs/.//*:/usr/hdp/2.6.4.0-91/hadoop-yarn/lib/*:/usr/hdp/2.6.4.0-91/hadoop-yarn/.//*:/usr/hdp/2.6.4.0-91/hadoop-mapreduce/lib/*:/usr/hdp/2.6.4.0-91/hadoop-mapreduce/.//*::mysql-connector-java-5.1.33-bin.jar:mysql-connector-java.jar:/grid/0/hdp/2.6.4.0-91/tez/*:/grid/0/hdp/2.6.4.0-91/tez/lib/*:/grid/0/hdp/2.6.4.0-91/tez/conf
 -Dproc_phoenixserver -Dlog4j.configuration=file:/grid/0/hdp/2.6.4.0-91/phoenix/bin/log4j.properties -Dpsql.root.logger=INFO,DRFA -Dpsql.log.dir=/grid/0/log/hbase -Dpsql.log.file=phoenix-hbase-server.log org.apache.phoenix.queryserver.server.Main
close failed in file object destructor:
IOError: [Errno 9] Bad file descriptor
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/grid/0/hdp/2.6.4.0-91/phoenix/phoenix-4.7.0.2.6.4.0-91-client.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/grid/0/hdp/2.6.4.0-91/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
starting Query Server, logging to /grid/0/log/hbase/phoenix-hbase-server.log
Query Server already running, PID file found: /var/run/hbase/phoenix-hbase-server.pid
close failed in file object destructor:
IOError: [Errno 9] Bad file descriptor
{code}",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-03-28 07:28:40,3
13148283,"Fix intermittent ""No such file or directory"" error in TestHostCleanup","Fix intermittent ""No such file or directory"" error in TestHostCleanup

{noformat}
======================================================================
ERROR: test_do_cleanup_all (TestHostCleanup.TestHostCleanup)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/test/python/ambari_agent/TestHostCleanup.py"", line 233, in test_do_cleanup_all
    self.hostcleanup.do_cleanup(propertyMap)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/main/python/ambari_agent/HostCleanup.py"", line 168, in do_cleanup
    self.do_delete_by_owner(userIds, FOLDER_LIST)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/main/python/ambari_agent/HostCleanup.py"", line 514, in do_delete_by_owner
    stat = os.stat(fileToCheck)
OSError: [Errno 2] No such file or directory: '/tmp/symlink2120964127269148256test_link'
{noformat}",pull-request-available unit-test,['ambari-agent'],AMBARI,Bug,Major,2018-03-27 17:42:49,7
13148255,UI loading stuck after deleting service,"{noformat}
Uncaught TypeError: Cannot read property 'toLowerCase' of null
    at Class.<anonymous> (app.js:234426)
    at ComputedPropertyPrototype.get (vendor.js:14954)
    at get (vendor.js:13360)
    at getPath (vendor.js:13482)
    at get (vendor.js:13353)
    at getWithGlobals (vendor.js:15987)
    at Binding._sync (vendor.js:16174)
    at invoke (vendor.js:15382)
    at iter (vendor.js:15429)
    at Array.forEach (<anonymous>)
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-27 15:48:59,4
13148206,"Create topic handler for small agent actions. Like restart_agent, clean_caches etc.","For bigger actions containing a lot of info or special workflow and a new
topic would be required. Small actions like restart_agent/clean_cache make
sense to be sent in one general topic

",pull-request-available,[],AMBARI,Bug,Major,2018-03-27 12:18:37,0
13148199,Add Service Wizard will not reset after adding a service,"After adding a service using the Add Service Wizard and then trying to add another service using the Add Service Wizard, the user interface will not reset to the beginning of the wizard.  

Steps to reproduce:
# Create Ambari 2.6.1 cluster with ZK, HDFS, and Yarn (HDP 2.6)
# Upgrade to Ambari 2.7.0
# Add Atlas
# Add Knox (or try to)

Note: The upgrade or stack may not be a factor here

Note: The dashboard does not refresh after the first add... so Atlas is not displayed in the service list until and manual refresh of the view. ",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-27 11:50:56,4
13148198,Fix formatZKFC,"resource_management.core.exceptions.Fail: Script '/var/lib/ambari-
agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/zkfc_slave.py' has no
method 'format'

",pull-request-available,[],AMBARI,Bug,Major,2018-03-27 11:41:47,0
13147985,Streamline Application manager's UI link is not visible in Ambari-2.7.0.0,"SAM's UI link is not available on side bar using Ambari-2.7.0.0. Independently, you can open SAM's UI at port 7777 (default) just fine.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-26 17:16:10,2
13147911,When credential store is enabled status commands should not generate jceks,"Status commands should not generate jceks as they don't need them. This was
the behavior in previous release.

Currently jceks are generated for every status command, flooding ambari-agent
logs and making status commands run unnecessary longer.

",pull-request-available,[],AMBARI,Bug,Major,2018-03-26 11:12:19,0
13147886,Service page layout updates,See screenshots,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-26 09:34:16,4
13147871,Update combo filter style,"Remove shadow outside input box.

",pull-request-available,['ambari-web'],AMBARI,Task,Minor,2018-03-26 08:24:15,4
13147818,Enable or disable SSO for services upon setting sso-configuration values ,"Enable or Disable SSO for services upon setting sso-configuration values.

The action performed by the user via the REST API to set value for the Ambari server sso-configuration value should trigger a call to the stack advisor and, using the returned recommendations, set properties needed to enable or disable SSO integration for the relevant services.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-25 23:07:00,7
13147563,Add NameNode federation support for Host Details page,"- Display namespace id for NameNode entries on Host Details page
- Perform DataNode decommission status check using data of the correct NameNode
- Add NameNode widgets to Host Metrics section",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-23 18:59:41,2
13147505,Install Wizard: Add a Cancel button to go back to Admin View.,"See screenshot.

",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-23 15:40:34,4
13147463,Server should acknowledge agent about received reports,Server should sent reply for requests to agent reports stomp endpoint.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-23 12:47:18,1
13147453,Admin View: Users and Groups style tweaks,"See screenshots
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-23 12:13:37,4
13147419,Remove python3 files from ws4py since they can cause build failure,"On some envs (not reproducible for me) it can result in failure like this:

    
    
    2018/03/22 23:49:21 INFO    : [INFO]   File ""/usr/lib/ambari-agent/lib/ambari_ws4py/async_websocket.py"", line 85
    2018/03/22 23:49:21 INFO    : [INFO]     yield from self.proto.writer.drain()
    2018/03/22 23:49:21 INFO    : [INFO]              ^
    2018/03/22 23:49:21 INFO    : [INFO] SyntaxError: invalid syntax
    2018/03/22 23:49:21 INFO    : [INFO] 
    2018/03/22 23:49:21 INFO    : [INFO] Compiling /grid/0/jenkins/workspace/Zuul_Ambari_Build_Job/build-support/SOURCES/ambari/ambari-agent/target/rpm/ambari-agent/buildroot/usr/lib/ambari-agent/lib/ambari_ws4py/server/tulipserver.py ...
    2018/03/22 23:49:21 INFO    : [INFO]   File ""/usr/lib/ambari-agent/lib/ambari_ws4py/server/tulipserver.py"", line 101
    2018/03/22 23:49:21 INFO    : [INFO]     request_line = yield from self.next_line()
    2018/03/22 23:49:21 INFO    : [INFO]                             ^
    2018/03/22 23:49:21 INFO    : [INFO] SyntaxError: invalid syntax
    2018/03/22 23:49:21 INFO    : [INFO] 
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-23 10:03:31,0
13147397,400 server error when Cluster Administrator tries to add service and move components,"STR:
 # Deploy a HDP-3.0 cluster
 # Create a user with role as ""Cluster Administrator""
 # Login as the user
 # Click Add Service from the dashboard
 # Click Next button in the wizard

The same error occurs while trying the Move Components wizard too.

Issue also present for Cluster Operator Role while moving components.

The page is stuck at this stage even after dismissing the popup.
{code:java|title=ambari-server.log}
17 Mar 2018 16:13:51,740  WARN [ambari-client-thread-524] StackAdvisorCommand:204 - Error occurred during retrieving ldap configuration, status=403, response={
  ""status"" : 403,
  ""message"" : ""The authenticated user does not have the appropriate authorizations to get the requested resource(s)""
}
17 Mar 2018 16:13:51,741  WARN [ambari-client-thread-524] StackAdvisorCommand:182 - Error parsing services.json file content: Error occurred during retrieving ldap configuration, status=403, response={
  ""status"" : 403,
  ""message"" : ""The authenticated user does not have the appropriate authorizations to get the requested resource(s)""
}
org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Error occurred during retrieving ldap configuration, status=403, response={
  ""status"" : 403,
  ""message"" : ""The authenticated user does not have the appropriate authorizations to get the requested resource(s)""
}
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.populateLdapConfiguration(StackAdvisorCommand.java:205)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.adjust(StackAdvisorCommand.java:177)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:352)
	at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:132)
	at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:145)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
	at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:564)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
	at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
	at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:748)
17 Mar 2018 16:13:51,744 ERROR [ambari-client-thread-524] CreateHandler:84 - Caught a runtime exception while attempting to create a resource: null
javax.ws.rs.WebApplicationException
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.adjust(StackAdvisorCommand.java:183)
	at org.apache.ambari.server.api.services.stackadvisor.commands.StackAdvisorCommand.invoke(StackAdvisorCommand.java:352)
	at org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper.recommend(StackAdvisorHelper.java:132)
	at org.apache.ambari.server.controller.internal.RecommendationResourceProvider.createResources(RecommendationResourceProvider.java:145)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.createResources(ClusterControllerImpl.java:296)
	at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.create(PersistenceManagerImpl.java:97)
	at org.apache.ambari.server.api.handlers.CreateHandler.persist(CreateHandler.java:50)
	at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:68)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:144)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:163)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:127)
	at org.apache.ambari.server.api.services.RecommendationService.getRecommendation(RecommendationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:290)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authentication.AmbariDelegatingAuthenticationFilter.doFilter(AmbariDelegatingAuthenticationFilter.java:135)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:95)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:73)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:53)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:130)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:51)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1621)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:541)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1592)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1239)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:481)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1561)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1141)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:494)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:220)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:140)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.eclipse.jetty.server.Server.handle(Server.java:564)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)
	at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:122)
	at org.eclipse.jetty.util.thread.strategy.ExecutingExecutionStrategy.invoke(ExecutingExecutionStrategy.java:58)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:201)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:133)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:672)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:590)
	at java.lang.Thread.run(Thread.java:748)

{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-23 09:03:58,3
13147382,Provider URL validator incorrect when setting up SSO via Ambari CLI,"When setting the URL for the SSO provider while running {{ambari-server setup-sso}}, valid URLs are rejected.
{noformat}
[root@c7402 ~]# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Do you want to configure SSO authentication [y/n] (y)?y
Provider URL [URL] (http://example.com):https://c7402.ambari.apache.org:8443/gateway/knoxsso/api/v1/websso
Invalid provider URL
Provider URL [URL] (http://example.com):c7402.ambari.apache.org:8443
Public Certificate pem (empty) (empty line to finish input):
{noformat}
Cause:
The RegEx validating the URL is only allowing hostname and port values, when it should allow for absolute URLs.
{code:java|title=ambari_server/setupSso.py:72}
provider_url = get_validated_string_input(""Provider URL [URL] ({0}):"".format(provider_url), provider_url, REGEX_HOSTNAME_PORT,
                                                ""Invalid provider URL"", False){code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-23 08:13:24,6
13147336,Recommend SSO configuration values for ATLAS and RANGER in the stack advisor,"*Recommend SSO configuration values for ATLAS in the stack advisor*
The following values in {{application-properties}} need to be set:

* {{atlas.sso.knox.enabled}}
** SSO enabled (true) or disabled (false)

* {{atlas.sso.knox.providerurl}}
** SSO provider url. Example: https://KNOX_HOST:KNOX_PORT/gateway/TOPOLOGY_NAME/knoxsso/api/v1/websso

* {{atlas.sso.knox.publicKey}}
** SSO provider public key. The base64-encoded x509 Public key data (without the certificate header and footer) used for SSO provider cookie verification

* {{atlas.sso.knox.providerurl}}
** SSO browser useragents (comma-delimted).  Default: Mozilla,chrome

Also, update the Atlas metainfo.xml file to indicate it supports SSO configuration by Ambari

*Recommend SSO configuration values for RANGER in the stack advisor*
The following values in {{ranger-admin-site}} need to be set:

* {{ranger.sso.enabled}}
** SSO enabled (true) or disabled (false)

* {{ranger.sso.providerurl}}
** SSO provider url. Example: https://KNOX_HOST:KNOX_PORT/gateway/TOPOLOGY_NAME/knoxsso/api/v1/websso

* {{ranger.sso.publicKey}}
** SSO provider public key. The base64-encoded x509 Public key data (without the certificate header and footer) used for SSO provider cookie verification

* {{ranger.sso.cookiename}}
** SSO cookie name

* {{ranger.sso.query.param.originalurl}}
** Query name for appending original url in SSO url

* {{ranger.sso.browser.useragent}}
** SSO browser useragents (comma-delimted).  Default: Mozilla,chrome

Also, update the Ranger metainfo.xml file to indicate it supports SSO configuration by Ambari",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-23 01:11:54,7
13147198,Remove LDAP Synchronization Process,"The existing LDAP synchronization process has the following challenges:

* Common annoyance among Operators
* Difficult to schedule as it’s interactive
* Introduces delay in entitlements being granted to users (in LDAP), and more importantly from revocation.
* Introduces issues with remove users that are no longer active, or with the company which == compliance concerns

and benefits: 
* Simplifies auto-complete for adding users/groups to cluster roles and view permissions
* Shields users from LDAP performance issues on login, and during user/group permission mapping

Given that, Ambari's LDAP sync process should be removed to allow for users and groups to be dynamically synchronized with a configured LDAP server.  Users should be added to the Ambari DB if necessary and groups are to be dynamically assigned and mapped to Ambari roles upon successful authentication with the configured LDAP server (via Ambari).  A scheduled job may need to execute to clean out any orphaned data.

The requirements will be broken out into categories of capabilities:

1.) Permission Mapping
2.) Permission Resolution
3.) User management

*Permission Mapping*: Ability to map an individual LDAP User DN to a permission, as well as an individual Group DN to a permission

*Permission Resolution*: Ability to resolve DN of user, DN of all directly mapped groups, and DN of all in-directly mapped groups (nested groups)
* If the user logging in has no permissions they should not be allowed to login, but shown a message stating that they have no mapped permissions in Ambari and to contact their administrator, no Ambari DB user should be auto-created
* If the user logging in has permissions, we should:
** Auto-create the Ambari user in the DB if it does not exist
** Check if we were asked to auto-create home directories on login, and if so check if the user has a home directory, if they don't, then auto-create it

*User Management*: Because users will be auto-created in the Ambari DB, and because they will be authenticated against LDAP before being able to login we have to appropriately deal with two types of users:

# Orphaned Users: Users without any mapped permissions - users enter this state by being in a group ""HadoopOps"" lets say and then six months later they are removed from this group. Or were directly mapped to a permission by name, and were removed from that permission. If they try logging into Ambari they will not be allowed to login and will be shown the message stating that they have no permissions.  These users need to be removed from Ambari individually through the Ambari UI with a ""Remove LDAP User"" button on the user.
# Deprovisioned Users: Users who have been either removed or inactivated in the upstream LDAP server due to termination or other reasons - In most situations we'll never see these users again.

For both types of users having the following capabilities would be extremely helpful:

* Ability to remove individual LDAP users from Ambari ""Remove LDAP User"" button
* Ability to remove all users who haven't logged in in more than x days.
",authentication ldap,['ambari-server'],AMBARI,Epic,Major,2018-03-22 15:50:53,7
13147146,Add custom context root support to quicklink engine,"Adding placeholder replacement ability to quicklink engine.

For example:
{code:java}
""url"":""%@://%@:%@/${gateway-site/gateway.path}/knoxsso/knoxauth/login.html""{code}
Here the ${config-type/propety-name} should be replaced based on the appropriate configuration.",pull-request-available,['ambari-web'],AMBARI,Improvement,Major,2018-03-22 13:19:22,3
13147133,Locking configure to single process causes deadlock,"1\. Component1 needs component2 to be started to do configure  
2\. Component1 grabs the lock and retries in configure  
3\. Component2 cannot grab the lock waiting forever

This causes variety of timeout errors in different components.  
Than finally one component will fail with at the end

    
    
    2018-03-22 08:53:01,593 - Trying to acquire a lock on /var/lib/ambari-agent/tmp/link_configs_lock_file
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-22 12:41:32,0
13146844,JS error on service stop/restart after config changes,"Steps:
# Change the value of fs.trash.interval for HDFS service.
# Save configs
# Notice Restart required icon
# Try to apply 'Restart All Affected'

No op was triggered. Could see js error error in console:
{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
pullNnCheckPointTime @ app.js:27950
checkNnLastCheckpointTime @ app.js:27905
restartAllStaleConfigComponents @ app.js:26478
ActionHelper.registeredActions.(anonymous function).handler @ vendor.js:31554
(anonymous function) @ vendor.js:23346
jQuery.event.dispatch @ vendor.js:3178
elemData.handle @ vendor.js:2854
app.js:56248 
{noformat}

Also for 'Stop' service operation:
{noformat}
Uncaught TypeError: Cannot read property 'get' of undefined
pullNnCheckPointTime @ app.js:27950
checkNnLastCheckpointTime @ app.js:27905
startStopPopup @ app.js:27842
stopService @ app.js:28176
doAction @ app.js:28662
ActionHelper.registeredActions.(anonymous function).handler @ vendor.js:31554
(anonymous function) @ vendor.js:23346
jQuery.event.dispatch @ vendor.js:3178
elemData.handle @ vendor.js:2854
app.js:56248 
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-21 14:04:34,2
13146826,Regression : Adding Atlas Metadata server' for HA on an host takes forever.,"Atlas is already installed in the cluster. To make Atlas HA , tried to add a new instance of Atlas Metadata server on host summary page. if Atlas Metadata server is added, ""Confirmation"" window keeps buffering forever.

refer to screenshot  provided

Workaround :
1. Inspect on the Confirm button
2. Remove the ""disabled"" attribute (Now Confirm button is enabled)
3. Submit the confirm button",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-21 13:05:37,4
13146822,Use Ambari CLI to specify which services should be setup for SSO integration,"Use Ambari CLI to specify which services should be setup for SSO integration.
{noformat:title=Example}
# ambari-server setup-sso
Using python  /usr/bin/python
Setting up SSO authentication properties...
Do you want to configure SSO authentication [y/n] (y)?y
Enter Ambari Admin login: admin
Enter Ambari Admin password: admin
Provider URL [URL] (http://example.com):http://knox.ambari.apache.org:8080
Public Certificate pem (stored) (empty line to finish input):
AAAAB3NzaC1yc2EAAAADAQABAAABAQD....

Use SSO for all services [y/n] (y)? n
Use SSO for Ambari [y/n] (y)? y
Use SSO for HDFS [y/n] (y)? y
Use SSO for YARN [y/n] (y)? y
...
Use SSO for ZOOKEEPER [y/n] (y)? n
Do you want to configure advanced properties [y/n] (n) ?
Ambari Server 'setup-sso' completed successfully.
{noformat}
NOTE: this will require obtaining an Ambari administrator username and password to GET, PUT, and POST to the Ambari REST API.",pull-request-available security sso,['ambari-server'],AMBARI,Task,Major,2018-03-21 12:50:33,6
13146815,Add SSO-related configuration recommendations to the stack advisor,"Add SSO-related configuration recommendations to the stack advisor.

# Add a new action - {{recommend-configurations-for-sso}} - to query services for only SSO-related configuration changes
# Append to the stack advisor input data, the Ambari-stored SSO integration data (list of services that should enable SSO integration, proxy url, public key details, etc...).

",pull-request-available stack_advisor,['ambari-server'],AMBARI,Task,Major,2018-03-21 12:19:58,7
13146810,YARN cluster memory graph is not loading in Ambari 2.6,"Users are not able to see cluster memory graph under YARN. This was working before upgrade.

We were able to reproduce this internally on Ambari 2.6. We checked across 5 clusters and all of them exhibited same behaviour.

Seems like problem with the compute : mem_total._sum - mem_free._sum fails but mem_total._sum + mem_free._sum succeeds.

Also if we load mem_total._sum AND mem_free._sum individually, they come up fine.

Seems like issue when we do a 'subtract'.

Attaching UI Screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-21 11:35:47,4
13146586,"Ambari Upgrade:  Start All Services fails after AU at MapReduce2 Client Install with error ""Configuration parameter 'container-executor' was not found in configurations dictionary!""","After Ambari Upgrade from 2.6.0.0 to 2.7.0.0 , start all services operation fails at MapReduce2 Client Install with below error.

{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/mapreduce2_client.py"", line 91, in <module>
    MapReduce2Client().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/mapreduce2_client.py"", line 41, in install
    self.configure(env)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 122, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/mapreduce2_client.py"", line 50, in configure
    yarn(config_dir=config_dir)
  File ""/usr/lib/ambari-agent/lib/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/yarn.py"", line 166, in yarn
    content=InlineTemplate(params.container_executor_cfg_template)
  File ""/usr/lib/ambari-agent/lib/resource_management/core/source.py"", line 150, in __init__
    super(InlineTemplate, self).__init__(name, extra_imports, **kwargs) 
  File ""/usr/lib/ambari-agent/lib/resource_management/core/source.py"", line 137, in __init__
    self.template = self.template_env.get_template(self.name)     
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/environment.py"", line 716, in get_template
    return self._load_template(name, self.make_globals(globals))
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/environment.py"", line 686, in _load_template
    template = self.cache.get(name)
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/utils.py"", line 405, in get
    return self[key]
  File ""/usr/lib/ambari-agent/lib/ambari_jinja2/utils.py"", line 448, in __getitem__
    rv = self._mapping[key]
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in __getattr__
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'container-executor' was not found in configurations dictionary!
{code}",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-03-20 16:16:00,6
13146549,Ambari Upgrade from 2.6.0.0 - 2.7.0.0 - Dashboard is unable to load and spinner keeps spinning,"After Ambari Upgrade from 2.6.0.0 to 2.7.0.0 , the Ambari UI Dashboard is unable to load. Spinner keeps spinning,
Seems like Wizard-Data API calls are returning null and not being handled properly on the UI. Ideally, the dashboard should be shown with widgets with No data (if AMS is down)
Additional Information:
Adding few more observed issues. These might be related to the originally raised issue.
# Quick Links are not shown for services. Spinner keeps spinning.
# Component information is not shown for some services. e.g. Oozie.
# For HDFS, YARN -> DataNodes, NodeManagers keep showing Loading and never loads.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-20 13:53:00,2
13146510,Python UT failure,"
    ERROR: test_get_service_base_dir (TestFileCache.TestFileCache)
    ----------------------------------------------------------------------
    ERROR 2018-03-20 06:58:54,744 - Python unit tests failed
    Traceback (most recent call last):
      File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
        return func(*args, **keywargs)
      File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/test/python/ambari_agent/TestFileCache.py"", line 72, in test_get_service_base_dir
        res = fileCache.get_service_base_dir(command, ""server_url_pref"")
      File ""/home/jenkins/jenkins-slave/workspace/Ambari-Github-PullRequest-Builder/ambari-agent/src/main/python/ambari_agent/FileCache.py"", line 77, in get_service_base_dir
        if 'service_package_folder' in command['commandParams']:
    KeyError: 'commandParams'
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-20 10:33:28,0
13146369,Add SSO integration support information to service information via Ambari's REST API,"Add SSO integration support information to service information via Ambari's REST API.  This information should be usable by Ambari's search predicate feature.

New _read-only_ properties for (stack) services should be:
* *{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
** Information is expected to be determined by service's meta info (see BUG-98626)

New _read-only_ properties for installed services should be:
* *{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
** Information is expected to be determined by the service's meta info (see BUG-98626)
* *{{sso_integration_enabled}}* - Indicates whether the service is configured for SSO integration or not
** Information is expected to be determined by a value indicated in the service's meta info (see BUG-98626)
* *{{sso_integration_desired}}* - Indicates whether the service is chosen for SSO integration or not
** Information is expected to be in {{cluster-env/sso_enabled_services}} (see BUG-98451)

Examples:
{noformat:title=Get stack service details}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME"",
{
  ""href"" : "":URL"",
  ""StackServices"" : {
     ...
     ""sso_integration_supported"": ""false"",
     ...
  },
  ...
{noformat}

{noformat:title=Get installed service information}
GET /api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME
{
  ""href"" : "":URL"",
  ""ServiceInfo"" : {
    ""cluster_name"" : "":CLUSTER_NAME"",
    ...
    ""sso_integration_supported"": ""true"",
    ""sso_integration_enabled"": ""false"",
    ""sso_integration_desired"": ""false"",
     ...
    },
    ...
{noformat}

{noformat:title=List installed services that support SSO integration}
GET /api/v1/clusters/:CLUSTER_NAME/services?ServiceInfo/sso_integration_supported=true
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}

{noformat:title=List stack services that support SSO integration}
GET /api/v1/stacks/:STACK_NAME/versions/:VERSION/services?StackServices/credential_store_enabled=false
{
  ""href"" : "":URL"",
  ""items"" : [
      ...
   ]
}
{noformat}

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-19 20:49:33,3
13146306,Add Ambari configuration data to stack/service advisor input data,"Add Ambari configuration data to stack/service advisor input data. Currently the {{ldap-configuration}} data is passed in to the stack/service advisor. However, generically all Ambari configuration data should be passed in.

For example, the current data set (in service.json) contains
{code:java}
{
  ...  ""services"" : [],
  ... ""configurations"" : \{ ... }, ""ambari-server-properties"" : \{ ... },
  ...  ""ldap-configurations"" : \{ ... }
}
 

{code}
 

""\{{ldap-configurations}}"" may or may not exist depending on if LDAP authentication/sync was setup. This should be treated generically and part of a larger block of data named ""\{{ambari-server-configuration}}"". For example:
{code:java}
{
...
""services"" : [],
...
""configurations"" :

{ ... },
""ambari-server-properties"" : { ... }

,
...
""ambari-server-configuration"" : {
""ldap-configurations"" :

{ ... }

,
...
}
}

 

 {code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-19 16:59:12,3
13146270,Install Wizard > Select Version page: the user cannot proceed with Redhat Satellite option,"STR:
* Launch Install Wizard
* On Select Version page, select ""Use Local Repository"" and then ""Use RedHat Satellite/Spacewalk""
* The Next button is disabled.  The Next button must be enabled in this scenario.
",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-19 15:19:16,4
13146254,Ambari should not set world-readable permissions for atlas-application.properties file,"Atlas uses the atlas-application.properties file and a clear-text-password can be found there. Therefore, OS permissions should be set to just allowing the atlas user to access it, but ambari resets them every time, making it not possible to restrict world-read access to the file.",pull-request-available security,['ambari-server'],AMBARI,Bug,Minor,2018-03-19 14:25:22,6
13146233,Enable Kerberos Wizard style edits,"# Reduce top padding for modal of wizard
# Test KDC connection success icon should be green
# the retry button on the 'Kerberize Cluster' page does not conform to the retry button styles found in other wizards

See screenshots.
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-19 14:02:19,4
13146159, Wait to leave Safemode failed on EU ,"I can confirm that {{config _hostLevelParams/stack_version_ }}is not present at command.json
{code:java}
2018-03-15 17:17:15,891 - Task. Type: EXECUTE, Script: scripts/namenode.py - Function: wait_for_safemode_off
2018-03-15 17:17:16,026 - Using hadoop conf dir: /usr/hdp/3.0.0.0-1040/hadoop/conf
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 383, in <module>
    NameNode().execute()
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 194, in wait_for_safemode_off
    wait_for_safemode_off(self.get_hdfs_binary(), afterwait_sleep=30, execute_kinit=True)
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/namenode.py"", line 72, in get_hdfs_binary
    return get_hdfs_binary(""hadoop-hdfs-namenode"")
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/utils.py"", line 397, in get_hdfs_binary
    import params
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/params.py"", line 25, in <module>
    from params_linux import *
  File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/HDFS/package/scripts/params_linux.py"", line 59, in <module>
    stack_version_formatted = format_stack_version(stack_version_unformatted)
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/functions/version.py"", line 42, in format_stack_version
    if value:
  File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/config_dictionary.py"", line 73, in getattr
    raise Fail(""Configuration parameter '"" + self.name + ""' was not found in configurations dictionary!"")
resource_management.core.exceptions.Fail: Configuration parameter 'stack_version' was not found in configurations dictionary!{code}
May be related to mpack work",upgrade,['ambari-server'],AMBARI,Task,Blocker,2018-03-19 08:50:20,6
13146045,Maven cleanup,Clean up some of the duplications in {{pom.xml}}.,pull-request-available,[],AMBARI,Task,Major,2018-03-18 13:17:57,5
13145836,Make HDFS widgets namespace-scoped,Widgets on dashboard and HDFS summary page should be namespace-scoped.,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-03-16 19:30:39,2
13145788, Add more logging for config update process,Additional logging is required to investigate intermittent issue with config updates.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-16 17:09:20,1
13145780,Wrong service_package_folder in serviceLevelParams during Express Upgrade from HDP 2.6 to HDP 3.0,"Upgrade HDP 2.6 to HDP 3.0 in Ambari 2.7; EU is supported only (i.e. no RU).

At upgrade time, when the services are starting we should use the new stack version's _service_package_folder_ instead of the old one. This was ok, until the change in FileCache.get_service_base_dir to get the base directory from 'serviceLevelParams' instead of 'commandParams'.

{code:java}
 ...
 ""serviceLevelParams"": {
 ""credentialStoreEnabled"": false, 
 ""status_commands_timeout"": 300, 
 ""version"": ""2.7.3"", 
 ""service_package_folder"": ""common-services/YARN/2.1.0.2.0/package""
 },
 ...
 ""commandParams"": {
 ""service_package_folder"": ""stacks/HDP/3.0/services/YARN/package"", 
 ""hooks_folder"": ""stack-hooks"", 
 ""clusterName"": ""cluster1"", 
 ""custom_command"": ""RESTART"", 
 ""upgrade_direction"": ""upgrade"", 
 ""upgrade_type"": ""nonrolling_upgrade"", 
 ""script"": ""scripts/historyserver.py"", 
 ""version"": ""3.0.0.2-155"", 
 ""forceRefreshConfigTagsBeforeExecution"": ""true"", 
 ""request_id"": ""17"", 
 ""command_timeout"": ""900"", 
 ""script_type"": ""PYTHON""
 },
{code}

The _service_package_folder_ is different; the one in _commandParams_ is the correct value.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-16 16:39:33,1
13145776,Fix TestMySqlServer error while executing unit tests on MacOS/Darwin,"Fix TestMySqlServer error while executing unit tests on MacOS/Darwin.

 
{noformat}
ERROR: test_configure_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
File ""/Users/rlevas/github/ambari/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
return func(*args, **keywargs)
File ""/Users/rlevas/github/ambari/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 84, in test_configure_secured
self.assert_configure_secured()
File ""/Users/rlevas/github/ambari/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 168, in assert_configure_secured
mode = 0755,
File ""/Users/rlevas/github/ambari/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 345, in assertResourceCalled
self.assertEquals(kwargs, resource.arguments)
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 515, in assertEqual
assertion_func(first, second, msg=msg)
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 831, in assertDictEqual
if d1 != d2:
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/resource_management/core/source.py"", line 59, in __eq__
and ((self.name.startswith(os.sep) and self.name == other.name) or self.get_content() == other.get_content()))
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/resource_management/core/source.py"", line 78, in get_content
return self.read_file(path)
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_family_impl.py"", line 84, in thunk
fn_id = fn_id_base + ""."" + OSCheck.get_os_family()
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 277, in get_os_family
os_family = OSCheck.get_os_type()
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 242, in get_os_type
return OSCheck.get_alias(OSCheck._get_os_type(), OSCheck._get_os_version())[0]
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 248, in _get_os_type
dist = OSCheck.os_distribution()
File ""/Users/rlevas/github/ambari/ambari-common/src/main/python/ambari_commons/os_check.py"", line 202, in os_distribution
distribution = platform.linux_distribution()
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/platform.py"", line 329, in linux_distribution
return _dist_try_harder(distname,version,id)
File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/platform.py"", line 205, in _dist_try_harder
info = open('/var/adm/inst-log/info').readlines()
IOError: [Errno 2] No such file or directory: '/var/adm/inst-log/info'{noformat}
 
*Cause*
This is caused by the following mock
{code:java}
@patch(""os.path.exists"", MagicMock(return_value=True))
{code}
 Which allow a later call on the Python 2.7 library to return {{True}}, causing {{/var/adm/inst-log/info}} to appear to exist and then be read. Since {{/var/adm/inst-log/info}} does not exist, this fails, as seen above. 

*Solution*
Mock the OsCheck function to avoid this.
 ",pull-request-available unit-test,['ambari-server'],AMBARI,Task,Major,2018-03-16 16:24:18,7
13145775,hdfs-site.xml is not getting updated after config change and restart via Ambari,"1. Changed 'dfs.datanode.data.dir' in hdfs-site.xml to new value.
2. Restarted HDFS service.
3. The value of 'dfs.datanode.data.dir' is not updated in hdfs-site.xml.",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-16 16:23:15,1
13145750,Create sso-configuration category in Ambari Configurations data,"Create \{{sso-configuration}} category in Ambari Configurations data and allow for the {{ambari.sso.enabled_services}} property to be added to it. This property is used to declare what services are expected to be configured for SSO and is expected to be a comma-delimited list of services or ""{{*}}"" to indicate all services.

Examples:
{code:title=All services}
""ambari.sso.enabled_services"":""*""
{code}
{code:title=Only Ambari}
""ambari.sso.enabled_services"":""Ambari""
{code}
{code:title=Ambari, Ranger, and Atlas}
""ambari.sso.enabled_services"":""Ambari, Ranger, Atlas""
{code}

Each service in the set of services should have indicated it supports SSO (see AMBARI-23252) else it will silently be ignored.

This value should be set via Ambari's REST API or a Blueprint.

Upon setting this value via the Ambari REST API, it is expected that internal logic will be triggered to ensure the relevant services in the list are configured for SSO or not configured for SSO as the case may be.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-03-16 15:09:59,7
13145726,"Ambari web - Install wizard ""Review"" step does not show HDP-GPL repo","Steps to reproduce: 
# Install latest ambari server (2.7.0.2-30)
# Upload a custom vdf in step2
# Proceed till the ""Review"" step. The summary misses ""HDP-GPL"" repo url.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-16 14:14:15,4
13145687,For Ranger Service /spnego reference principal is not getting updated in config,"For UI based installation, /spnego reference principal used in Ranger Service kerberos descriptor is not getting updated in ranger.spnego.kerberos.keytab configuration.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-16 12:45:17,4
13145465,Allow Ambari Server to Setup SSO for the entire stack using the CLI,"Today enabling SSO requires visiting each component that supports SSO and adding configuration entries to each. This task is to enable a single entry point via the Ambari CLI to configure SSO for each service that supports it.

Changes to the ambari-server setup-sso CLI are needed allow configuration of all SSO-capable services using that single CLI. This facility can be used to enable, disable, and reconfigure SSO integration.

*Proposed implementation:*

Services are to declare they support SSO integration by indicating in the service's \{{metainfo.xml}} file as follows: 
{code}
<sso>
 <supported>true</supported>
 <enabledConfiguration>config-type/sso.enabled.property</enabledConfiguration>
</sso>
{code}

The stack/service advisor will be used to retrieve the recommended configurations needed by a service to set up SSO integration. A special stack advisor action will be added to ensure only SSO-related recommendations are returned upon request. The new action name is ""{{recommend-configurations-for-sso}}"". Ambari (or common) SSO information will be provided to the stack advisor via the input data under the label ""sso-configuration"". This information may be used by the stack advisor when creating recommendations.

Ambari will store details on which services should be enabled for SSO so it _knows_ how to behave when SSO integration is enabled and new services are added. This data will be stored within Ambari's configuration data under the category of {{sso-configuration}}. The list of services to have SSO integration turned on will be stored in the property named {{ambari.sso.enabled_services}}. The value will be a comma-delimited list of service names, or ""{{*}}"" to indicate all services that support SSO integration.

The Ambari REST API entry point for installed services ({{/api/v1/clusters/:CLUSTER_NAME/services/:SERVICE_NAME}}) is to be enhance by adding the following properties:
* *\{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not
* *\{{sso_integration_enabled}}* - Indicates whether the service is configured for SSO integration or not
* *\{{sso_integration_desired}}* - Indicates whether the service is chosen for SSO integration or not

The Ambari REST API entry point for stack services ({{/api/v1/stacks/:STACK_NAME/versions/:VERSION/services/:SERVICE_NAME}}) is to be enhance by adding the following properties:
* *\{{sso_integration_supported}}* - Indicates whether the service supports SSO integration or not

When producing a list of installed services that support SSO integration in the CLI, the Ambari REST API is to be used to query for the relevant service names. Once the user selects the set of services to enable SSO for (or all), the Ambari REST API is to be used to set the value of the Ambari configuration \{{sso-configuration/ambari.sso.enabled_services}}. Upon setting this, logic is triggered in the backend to query the stack advisor for SSO-related configuration recommendations which will be automatically applied. This will potentially yield new configuration versions and require services to be manually restarted.

When adding new services, the \{{sso-configuration/ambari.sso.enabled_services}} value is to be checked to see if the new service is on the list of services to have SSO integration enabled. If so, and the service has a SSO descriptor, its configuration will be updated as needed before the service is started.

In a Blueprint scenario, it is expected that the user first sets up Ambari for SSO integration using the {{ambari-server setup-sso}} CLI. The Blueprint is expected to set the relevant properties needed to enable SSO integration per service. However, if SSO details were set up, the stack advisor may recommend relevant changes which may be applied depending on the Blueprint settings.",SSO sso,['ambari-server'],AMBARI,Epic,Major,2018-03-15 19:16:06,7
13145464,Update service metainfo to declare SSO integration support,"Update service metainfo to declare SSO integration support. The following tag may be optionally set in a service's {{metainfo.xml}} file:
{code:java}
<sso>
 <supported>true</supported>
 <enabledConfiguration>config-type/sso.enabled.property</enabledConfiguration>
</sso>
{code}
 ",pull-request-available sso,['ambari-server'],AMBARI,Task,Critical,2018-03-15 19:10:50,7
13145416,Ambari Setup with external postgres db failed on configuring database.,"Suse deployments fails during ambari-server setup with error:

{noformat}
Configuring ambari database...
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Enter full path to custom jdbc driver:
Traceback (most recent call last):
 File ""/usr/sbin/ambari-server.py"", line 1042, in <module>
 mainBody()
 File ""/usr/sbin/ambari-server.py"", line 1012, in mainBody
 main(options, args, parser)
 File ""/usr/sbin/ambari-server.py"", line 962, in main
 action_obj.execute()
 File ""/usr/sbin/ambari-server.py"", line 79, in execute
 self.fn(*self.args, **self.kwargs)
 File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1199, in setup
 _setup_database(options)
 File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1012, in _setup_database
 dbmsAmbari.setup_database()
 File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 154, in setup_database
 self._setup_remote_database()
 File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 162, in _setup_remote_database
 if self.ensure_jdbc_driver_installed(properties):
 File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 177, in ensure_jdbc_driver_installed
 path_to_custom_jdbc_driver = get_validated_string_input(""Enter full path to custom jdbc driver: "", None, None, None, False, False)
 File ""/usr/lib/ambari-server/lib/ambari_server/userInput.py"", line 87, in get_validated_string_input
 if not input.strip():
AttributeError: 'NoneType' object has no attribute 'strip'
{noformat}


Executed command during test:
{noformat}
ambari-server setup --database=postgres --databasehost=<hostName> --databaseport=<port> --databasename=<databaseName> --databaseusername=<userName> --databasepassword=<password> --java-home=/base/tools/jdk1.8.0_112 -s
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-15 15:41:15,1
13145308,Adding missing properties to OneFS mpack,"Following properties should be added to the appropriate config
 * hadoop.security.token.service.use_ip should be false when kerberos is enabled
 * dfs.checksum.type should always be null

 * dfs.datanode.http.address/dfs.datanode.https.address = 0.0.0.0:8082/0.0.0.0:8080 [?]

 * -yarn.scheduler.capacity.node-locality-delay = 0-

 * dfs.client-write-packet-size = 131072",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-15 09:13:07,3
13145156,Incorrect property for NN namespace value is used,Currently {{ClusterId}} property is used on UI to identify the namespaces. Values from {{hdfs-site}} configs should be used instead.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-14 19:54:46,2
13145086,Connection dropped during task execution caused stage abort,We should tolerate agent reconnect for now when agent was not restarted.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-14 16:25:12,1
13145085,SQL errors in Oracle schema create script,"During deploy on debian 7 with oracle database ambari server reported  many errors related to SQL queries:

{noformat}
14 Mar 2018 10:51:08,721 ERROR [ExecutionScheduler_QuartzSchedulerThread] JobStoreTX:3652 - Couldn't rollback jdbc connection. No more data to read from socket
java.sql.SQLRecoverableException: No more data to read from socket
{noformat}

{noformat}
Internal Exception: java.sql.SQLSyntaxErrorException: ORA-00942: table or view does not exist

Error Code: 942
Call: INSERT INTO hostcomponentstate (id, current_state, last_live_state, upgrade_state, version, host_id, service_name, cluster_id, component_name) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        bind => [9 parameters bound]
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-14 16:25:10,5
13145057,Broken view of Configs after saving changes,See screenshot,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-14 15:10:12,4
13145027,Set full name (cn) when creating user accounts in FreeIPA server,"Set full name (cn) when creating user accounts in FreeIPA server.

{code:title=Example IPA CLI}
ipa user-add user1 --principal user1@AMBARI.APACHE.ORG --first user1 --last user1 --cn ""user1""
{code}

The new argument is ""\{{--cn ""user1""}}.

This may allow for compatibility with IPA server version 3.x",freeipa kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-03-14 13:23:50,7
13144834,Dependency check should ignore unknown services,"{{BlueprintValidatorImpl}} throws NPE if encounters an unknown component.

{noformat}
	at org.apache.ambari.server.topology.BlueprintValidatorImpl.validateHostGroup(BlueprintValidatorImpl.java:250)
	at org.apache.ambari.server.topology.BlueprintValidatorImpl.validateTopology(BlueprintValidatorImpl.java:70)
	at org.apache.ambari.server.topology.BlueprintImpl.validateTopology(BlueprintImpl.java:332)
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-13 19:58:09,5
13144820,Ambari-agent fails to connect to server with two_way_auth enabled,"
    ERROR 2018-03-13 17:15:04,264 security.py:122 - Could not connect to wss://ctr-e138-1518143905142-94896-01-000002.hwx.site:8441/agent/stomp/v1
    Traceback (most recent call last):
      File ""/usr/lib/ambari-agent/lib/ambari_agent/security.py"", line 113, in establish_connection
        conn.start()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/connect.py"", line 46, in start
        self.transport.start()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/transport.py"", line 109, in start
        self.attempt_connection()
      File ""/usr/lib/ambari-agent/lib/ambari_stomp/adapter/websocket.py"", line 89, in attempt_connection
        self.ws.connect()
      File ""/usr/lib/ambari-agent/lib/ambari_ws4py/client/__init__.py"", line 216, in connect
        self.sock.connect(self.bind_addr)
      File ""/usr/lib64/python2.7/ssl.py"", line 869, in connect
        self._real_connect(addr, False)
      File ""/usr/lib64/python2.7/ssl.py"", line 860, in _real_connect
        self.do_handshake()
      File ""/usr/lib64/python2.7/ssl.py"", line 833, in do_handshake
        self._sslobj.do_handshake()
    SSLError: [SSL: SSLV3_ALERT_BAD_CERTIFICATE] sslv3 alert bad certificate (_ssl.c:579)
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-13 18:55:45,0
13144767,Customise Services : Alignment not proper for Select Config group Pop up and overriden text box,"There are two issues w.r.t alignment

*Issue#1*
1) Go to All Configurations Tab under Customise Services Step of Install Wizard.
2) Select any property say under HDFS 'DataNode directories permission' . Click '+' to override.
3) Select a new config group
4) The new text box which appears to provide overridden value is misaligned with original one.

*Issue#2*
Following same steps as in Issue#1. Check the warning 'You are changing not default group, please select config group to which you want to save dependent configs from other services +Show Details+' at page top. 
Click Show Details link.
A pop up opens. The text which appears on the pop up  is not aligned properly.

Attaching screenshots for both issues.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-13 15:47:32,4
13144440,Enable or disable SSO using Ambari CLI with options,"To enable or disable SSO using the Ambari CLI, the user needs to answer each prompt. This is not convenient for automated tools. So the following command line options should be available for use. If any mandatory option is not specified via the command line, the user should be prompted for a value.

Global SSO options:
 * {{--sso-enabled}}

if {{sso-enabled}} is ""true"":
 * {{--sso-provider-url}}
 * {{--sso-public-cert-file}}
 * {{--sso-jwt-cookie-name}}
 ** optional, if not supplied, the documented default value will be used
 * {{--sso-jwt-audience-list}}
 ** optional, if not supplied, the documented default value will be used

{noformat:title=Examples}
ambari-server setup-sso --help

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem --sso-jwt-cookie-name=ambari-jtw 

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem --sso-jwt-cookie-name=ambari-jtw --sso-jwt-audience-list=ambari

ambari-server setup-sso --sso-enabled=true --sso-provider-url=https://knox.ambari.apache.org:8443 --sso-public-cert-file=/tmp/sso.pem

ambari-server setup-sso --sso-enabled=false
{noformat}",SSO pull-request-available security,['ambari-server'],AMBARI,Task,Critical,2018-03-12 20:30:14,6
13144384,Customise Services: Alignment issue while adding new custom password property,"When a new custom property is added whose Type is Password , the two textboxes that appear are not properly aligned with Remove button.
The length of the text boxes should be reduced so that both textboxes(password/confirm password) fit in the same line along with remove button.

Attached screenshot from old and new UI.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-12 17:35:48,4
13144370,Kerberos 'Edit' Properties not conforming to the new style,"The 'Edit' link of Kerberos properties page does not confirm to the new UI design styles. All the actions are buttons on this page, and it is easy to miss the edit link.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-12 16:49:33,4
13144342,The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text,"The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text
Also if there are more than one properties overriden then the they are not shown in new lines.


Attached Screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-03-12 15:33:13,4
13144305,"Ambari Upgrade : Schema Upgrade Fails with error ""Foreign key constraint is incorrectly formed""","Ambari Schema Upgrade from 2.6.X to 2.7.0.0 fails with below exception . 
A blocker for Ambari Upgrades testing.

{code:java}
12 Mar 2018 00:51:37,879  INFO [main] SchemaUpgradeHelper:424 - Upgrading schema to target version = 2.7.0.0
12 Mar 2018 00:51:37,883  INFO [main] SchemaUpgradeHelper:433 - Upgrading schema from source version = 2.6.1
12 Mar 2018 00:51:37,888  INFO [main] SchemaUpgradeHelper:163 - Upgrade path: [{ upgradeCatalog: sourceVersion = 2.6.1, targetVersion = 2.6.2 }, { upgradeCatalog: sourceVersion = 2.6.2, targetVersion = 2.7.0 }, { upgradeCatalog: sourceVersion = null, targetVersion = 2.7.0 }, { upgradeCatalog: sourceVersion = null, targetVersion = 2.7.0 }]
12 Mar 2018 00:51:37,889  INFO [main] SchemaUpgradeHelper:200 - Executing DDL upgrade...
12 Mar 2018 00:51:37,911  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE topology_host_request ADD status VARCHAR(255)
12 Mar 2018 00:51:37,948  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE topology_host_request ADD status_message VARCHAR(1024)
12 Mar 2018 00:51:37,982  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD status VARCHAR(255) NOT NULL DEFAULT 'PENDING'
12 Mar 2018 00:51:38,017  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE stage ADD display_status VARCHAR(255) NOT NULL DEFAULT 'PENDING'
12 Mar 2018 00:51:38,055  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE request ADD display_status VARCHAR(255) NOT NULL DEFAULT 'PENDING'
12 Mar 2018 00:51:38,091  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE request ADD user_name VARCHAR(255)
12 Mar 2018 00:51:38,127  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE host_role_command ADD ops_display_name VARCHAR(255)
12 Mar 2018 00:51:38,314  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE hostcomponentdesiredstate DROP COLUMN security_state
12 Mar 2018 00:51:38,351  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE hostcomponentstate DROP COLUMN security_state
12 Mar 2018 00:51:38,387  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE servicedesiredstate DROP COLUMN security_state
12 Mar 2018 00:51:38,426  INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE ambari_configuration (category_name VARCHAR(100) NOT NULL, property_name VARCHAR(100) NOT NULL, property_value VARCHAR(255)) ENGINE=INNODB
12 Mar 2018 00:51:38,435  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE ambari_configuration ADD CONSTRAINT PK_ambari_configuration PRIMARY KEY (category_name,property_name)
12 Mar 2018 00:51:38,443  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE hostcomponentstate ADD last_live_state VARCHAR(255) DEFAULT 'UNKNOWN'
12 Mar 2018 00:51:38,476  WARN [main] DBAccessorImpl:965 - user_authentication_tmp table doesn't exists, skipping
12 Mar 2018 00:51:38,477  INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE user_authentication_tmp (user_authentication_id BIGINT NOT NULL, user_id BIGINT NOT NULL, authentication_type VARCHAR(50) NOT NULL, authentication_key LONGTEXT, create_time DATETIME, update_time DATETIME) ENGINE=INNODB
12 Mar 2018 00:51:38,483  INFO [main] DBAccessorImpl:876 - Executing query: CREATE TABLE user_authentication (user_authentication_id BIGINT NOT NULL, user_id BIGINT NOT NULL, authentication_type VARCHAR(50) NOT NULL, authentication_key LONGTEXT, create_time DATETIME, update_time DATETIME) ENGINE=INNODB
12 Mar 2018 00:51:38,490  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE user_authentication ADD CONSTRAINT PK_user_authentication PRIMARY KEY (user_authentication_id)
12 Mar 2018 00:51:38,555  INFO [main] DBAccessorImpl:876 - Executing query: ALTER TABLE user_authentication ADD CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
12 Mar 2018 00:51:38,572 ERROR [main] DBAccessorImpl:882 - Error executing query: ALTER TABLE user_authentication ADD CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
12 Mar 2018 00:51:38,572 ERROR [main] DBAccessorImpl:882 - Error executing query: ALTER TABLE user_authentication ADD CONSTRAINT FK_user_authentication_users FOREIGN KEY (user_id) REFERENCES users (user_id)
java.sql.SQLException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:996)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:303)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:280)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
12 Mar 2018 00:51:38,572  WARN [main] DBAccessorImpl:521 - Add FK constraint failed, constraintName = FK_user_authentication_users, tableName = user_authentication
12 Mar 2018 00:51:38,572 ERROR [main] SchemaUpgradeHelper:207 - Upgrade failed.
java.sql.SQLException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:996)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:303)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:280)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
12 Mar 2018 00:51:38,573 ERROR [main] SchemaUpgradeHelper:473 - Exception occurred during upgrade, failed
org.apache.ambari.server.AmbariException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:208)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:448)
Caused by: java.sql.SQLException: Can't create table `ambaricustom`.`#sql-642_c9` (errno: 150 ""Foreign key constraint is incorrectly formed"")
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:996)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)
        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)
        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)
        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)
        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:848)
        at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:742)
        at org.apache.ambari.server.orm.DBAccessorImpl.executeQuery(DBAccessorImpl.java:879)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:519)
        at org.apache.ambari.server.orm.DBAccessorImpl.addFKConstraint(DBAccessorImpl.java:484)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.createUserAuthenticationTable(UpgradeCatalog270.java:560)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.upgradeUserTables(UpgradeCatalog270.java:303)
        at org.apache.ambari.server.upgrade.UpgradeCatalog270.executeDDLUpdates(UpgradeCatalog270.java:280)
        at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:973)
        at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:205)
        ... 1 more
{code}

Possibly the issue is in different datatypes of user_id(int(11)) column in users and user_id(bigint(20)) in user_authentication table : See below:

{code:java}

MariaDB [ambaricustom]> desc user_authentication;
+------------------------+-------------+------+-----+---------+-------+
| Field                  | Type        | Null | Key | Default | Extra |
+------------------------+-------------+------+-----+---------+-------+
| user_authentication_id | bigint(20)  | NO   | PRI | NULL    |       |
| user_id                | bigint(20)  | NO   |     | NULL    |       |
| authentication_type    | varchar(50) | NO   |     | NULL    |       |
| authentication_key     | longtext    | YES  |     | NULL    |       |
| create_time            | datetime    | YES  |     | NULL    |       |
| update_time            | datetime    | YES  |     | NULL    |       |
+------------------------+-------------+------+-----+---------+-------+
6 rows in set (0.01 sec)

MariaDB [ambaricustom]> desc users;
+-----------------------+---------------+------+-----+---------------------+-------+
| Field                 | Type          | Null | Key | Default             | Extra |
+-----------------------+---------------+------+-----+---------------------+-------+
| user_id               | int(11)       | NO   | PRI | NULL                |       |
| principal_id          | bigint(20)    | NO   | MUL | NULL                |       |
| create_time           | timestamp     | NO   |     | current_timestamp() |       |
| ldap_user             | int(11)       | NO   |     | 0                   |       |
| user_type             | varchar(100)  | NO   |     | LOCAL               |       |
| user_name             | varchar(100)  | NO   | MUL | NULL                |       |
| user_password         | varchar(255)  | YES  |     | NULL                |       |
| active                | int(11)       | NO   |     | 1                   |       |
| active_widget_layouts | varchar(1024) | YES  |     | NULL                |       |
+-----------------------+---------------+------+-----+---------------------+-------+
9 rows in set (0.00 sec)
{code}
",pull-request-available upgrade,['ambari-server'],AMBARI,Bug,Blocker,2018-03-12 14:09:45,3
13144284,Websocket client should use secure endpoint when connected over https,See screenshot.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-12 12:38:22,4
13144038,Ambari Agent unit test failures on Python 2.7,"The following 2 tests consistently fail on CentOS7 with Python 2.7.5:

{noformat:title=mvn -am -pl ambari-agent -Drat.skip clean test}
======================================================================
FAIL: test_load_definitions_noFile (TestAlertSchedulerHandler.TestAlertSchedulerHandler)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py"", line 314, in test_load_definitions_noFile
    self.assertEquals(definitions, [])
AssertionError: Lists differ: [<ambari_agent.alerts.port_ale... != []

First list contains 1 additional elements.
First extra element 0:
<ambari_agent.alerts.port_alert.PortAlert object at 0x384c990>

- [<ambari_agent.alerts.port_alert.PortAlert object at 0x384c990>]
+ []

======================================================================
FAIL: test_generation (TestCertGeneration.TestCertGeneration)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""ambari-agent/src/test/python/ambari_agent/TestCertGeneration.py"", line 49, in test_generation
    self.assertTrue(os.path.exists(self.certMan.getAgentKeyName()))
AssertionError: False is not true

----------------------------------------------------------------------
Ran 342 tests in 17.726s

FAILED (failures=2)
{noformat}

The first one also fails on Ubuntu 16 with Python 2.7.12:

{noformat}
FAIL: test_load_definitions_noFile (TestAlertSchedulerHandler.TestAlertSchedulerHandler)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py"", line 314, in test_load_definitions_noFile
    self.assertEquals(definitions, [])
AssertionError: Lists differ: [<ambari_agent.alerts.port_ale... != []

First list contains 1 additional elements.
First extra element 0:
<ambari_agent.alerts.port_alert.PortAlert object at 0x7fbc86c56fd0>

- [<ambari_agent.alerts.port_alert.PortAlert object at 0x7fbc86c56fd0>]
+ []

----------------------------------------------------------------------
Ran 342 tests in 18.052s

FAILED (failures=1)
{noformat}

Also seen in Jenkins:
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/1083/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/950/
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/938/",pull-request-available,['ambari-agent'],AMBARI,Bug,Major,2018-03-10 10:24:26,5
13143984,Missing LdapFacade in ambari-server check-database,"{noformat:title=ambari-server check-database}
Using python  /usr/bin/python
Checking database
ERROR: Exiting with exit code 1.
REASON: Database check failed to complete: No errors and warnings were found.
...
Caused by: com.google.inject.CreationException: Unable to create injector, see the following errors:

1) No implementation for org.apache.ambari.server.ldap.service.LdapFacade was bound.
  while locating org.apache.ambari.server.ldap.service.LdapFacade
    for field at org.apache.ambari.server.controller.internal.AmbariServerLDAPConfigurationHandler.ldapFacade(AmbariServerLDAPConfigurationHandler.java:43)
  at org.apache.ambari.server.controller.ControllerModule.bindByAnnotation(ControllerModule.java:568)

1 error
	at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:470)
	at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:155)
	at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:107)
	at com.google.inject.Guice.createInjector(Guice.java:99)
	at com.google.inject.Guice.createInjector(Guice.java:73)
	at com.google.inject.Guice.createInjector(Guice.java:62)
	at org.apache.ambari.server.checks.DatabaseConsistencyChecker.main(DatabaseConsistencyChecker.java:101)
{noformat}
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-09 21:48:29,7
13143974,Values instance has no attribute 'master_key' during ambari-server reset,"{noformat:title=ambari-server reset -s}
Using python  /usr/bin/python
Resetting ambari-server
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
**** WARNING **** You are about to reset and clear the Ambari Server database. This will remove all cluster host and configuration information from the database. You will be required to re-configure the Ambari server and re-run the cluster wizard.
Are you SURE you want to perform the reset [yes/no] (yes)?
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Traceback (most recent call last):
  File ""/usr/sbin/ambari-server.py"", line 1033, in <module>
    mainBody()
  File ""/usr/sbin/ambari-server.py"", line 1003, in mainBody
    main(options, args, parser)
  File ""/usr/sbin/ambari-server.py"", line 953, in main
    action_obj.execute()
  File ""/usr/sbin/ambari-server.py"", line 79, in execute
    self.fn(*self.args, **self.kwargs)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1326, in reset
    _reset_database(options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverSetup.py"", line 1052, in _reset_database
    dbmsAmbari = factory.create(options, properties)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 501, in create
    dbmsConfig = desc.create_config(options, properties, dbId)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 81, in create_config
    return self.fn_create_config(options, properties, self.storage_key, dbId)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 881, in createPGConfig
    return PGConfig(options, properties, storage_type)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 391, in __init__
    super(PGConfig, self).__init__(options, properties, storage_type)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration_linux.py"", line 94, in __init__
    self.database_password = DBMSConfig._read_password_from_properties(properties, options)
  File ""/usr/lib/ambari-server/lib/ambari_server/dbConfiguration.py"", line 222, in _read_password_from_properties
    database_password = decrypt_password_for_alias(properties, JDBC_RCA_PASSWORD_ALIAS, options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 940, in decrypt_password_for_alias
    return read_passwd_for_alias(alias, masterKey, options)
  File ""/usr/lib/ambari-server/lib/ambari_server/serverConfiguration.py"", line 911, in read_passwd_for_alias
    if options is not None and options.master_key is not None and options.master_key:
AttributeError: Values instance has no attribute 'master_key'
{noformat}

{noformat:title=grep passwd /etc/ambari-server/conf/ambari.properties}
server.jdbc.user.passwd=${alias=ambari.db.password}
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-09 20:57:23,7
13143940,NoClassDefFoundError on Files View on S3,"{code:java}
06 Mar 2018 08:52:07,987 ERROR [ambari-client-thread-6083] ContainerResponse:423 - The exception contained within MappableContainerException could not be mapped to a response, re-throwing to the HTTP container
java.lang.NoClassDefFoundError: Could not initialize class com.amazonaws.ClientConfiguration
	at org.apache.hadoop.fs.s3a.S3ClientFactory$DefaultS3ClientFactory.createS3Client(S3ClientFactory.java:75)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:205)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2795)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:99)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2829)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2811)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:390)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:179)
	at org.apache.ambari.view.utils.hdfs.HdfsApi$1.run(HdfsApi.java:77)
	at org.apache.ambari.view.utils.hdfs.HdfsApi$1.run(HdfsApi.java:75)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.execute(HdfsApi.java:513)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.execute(HdfsApi.java:489)
	at org.apache.ambari.view.utils.hdfs.HdfsApi.<init>(HdfsApi.java:75)
	at org.apache.ambari.view.utils.hdfs.HdfsUtil.getHdfsApi(HdfsUtil.java:157)
	at org.apache.ambari.view.utils.hdfs.HdfsUtil.connectToHDFSApi(HdfsUtil.java:129)
	at org.apache.ambari.view.commons.hdfs.HdfsService.getApi(HdfsService.java:98)
	at org.apache.ambari.view.commons.hdfs.FileOperationService.listdir(FileOperationService.java:100)
	at sun.reflect.GeneratedMethodAccessor443.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205){code}
 

This is caused by an incorrect version from jackson-core.

In /var/lib/ambari-server/resources/views/work/FILES\{1.0.0}/WEB-INF/lib/ there is jackson-core-2.2.3.jar (instead of 2.9.4).

In 2.2.3 there was no requiresPropertyOrdering() method in com.fasterxml.jackson.core.JsonFactory.

jackson-core, jackson-databind and jackson-annotations should all have the same version (2.9.4).",pull-request-available,"['ambari-server', 'ambari-views']",AMBARI,Bug,Major,2018-03-09 18:29:28,3
13143901,Background Ops: Make the operations column wider (shrink the other columns to make space),"Current implementation attached. 
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-09 14:19:02,4
13143885,UI Changes required for Manage Versions /Register Versions Page,"# Space required between Name: and Version number on Register Versions Page (Screen Shot 2018-02-09 at 12.12.06 PM.png)
# Space required between '-' and Remove text next to Base URL text boxes.(Screen Shot 2018-02-09 at 12.13.09 PM.png)
# Alignment between Version Details and Service Details  in that version needs to be corrected in Versions Page (Screen Shot 2018-02-09 at 12.13.51 PM.png)
",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-09 12:43:23,4
13143878,"Add custom actions for: format NN, format ZK and bootStrapStandBy","Automate the actions needed for the wizard so UI can make a call to perform
the commands on the new NN hosts and setup federation.

Favoring automation here over manual steps like in the past NN HA wizard
forced user to do this instead of Ambari doing it for them.

",pull-request-available,[],AMBARI,Bug,Major,2018-03-09 11:57:15,0
13143861,Provide a new CLI option for setup-ldap to bypass question when there is no auth method configured (or different than LDAP),"As part of the job in AMBARI-22905 a change has been made to ask a question in case there is no configured authentication method or the one is different than LDAP (previously that question has been asked only if PAM was configured).

To allow any automated tests to bypass that question we should introduce a new CLI option. ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-09 10:46:51,6
13143856,Metrics Collector Install failed on HA cluster,"We have the very first run on Atlantic run using Ambari 2.7.0.2-60 and
HDP-3.0.0.2-130.  
Noticed in HA cluster that deploy has failed at Metrics collector install with
below error

    
    
    
    Caught an exception while executing custom service command: <class 'ambari_agent.AgentException.AgentException'>: 'Script /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py does not exist'; 'Script /var/lib/ambari-agent/cache/stacks/HDP/3.0/services/AMBARI_METRICS/package/scripts/metrics_collector.py does not exist'
    
    

Artifacts can be found
[here](http://logserver.eng.hortonworks.com/?prefix=qelogs/nat/86939/yarn-
ha/split-1/nat-yc-r7-schs-yarn-ha-re-
re/deploy_logs/logs/ctr-e138-1518143905142-62501-01-000002.hwx.site/)  
Live cluster: <http://172.27.15.145:8080/> (lifetime 48hrs)  
Could you please help take a look

",pull-request-available,[],AMBARI,Bug,Major,2018-03-09 10:23:15,0
13143576,Hide downgrade option from stack upgrade wizard,Downgrade button should be removed from the stack upgrade wizard for upgrade paths from HDP-2.6 to 3.0,express_upgrade,['ambari-server'],AMBARI,Bug,Critical,2018-03-08 15:04:38,3
13143327,Ambari logo is missing.,See the screenshot.,pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-07 20:17:52,1
13143233,Config dependencies alert bar not displayed properly while scrolling down the page,"Config dependencies alert bar not displayed properly while scrolling down the page.

Attaching screenshot",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-07 14:02:35,4
13143205,Unable to add a service in second go from Ambari-UI,"Installed Ambari using build 2.7.0.2-70 and HDP-3.0.0.2-150.
Added services HDFS, Yarn, Hive, HBase, Zookeeper and Solr.
Now trying to Add Knox / Ranger, after customizing the configurations and clicked on the {{Next}} button UI is stuck and unable to reach the deploy screen.
Browser console shows error {{unable to load modification handler for RANGER}}.",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-03-07 12:33:42,4
13142875,BootStrap Standby NameNode page does not conform to new UI,"While navigating through the Manage Journal Nodes wizard, the 'BootStrap Standby NameNode' is not as per the new Ambari style.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-06 14:56:26,4
13142865,Ambari agent should trust Ambari server's SSL certificate,"Ambari agent should trust Ambari server's SSL certificate.  

When using Python 2.7 and above, the agent tends to fail connecting with the Ambari server with a {{CERTIFICATE_VERIFY_FAILED}} error.   

To solve this, else tell Python to no verify certificates (which is insecure):
{noformat:title=/etc/python/cert-verification.cfg}
[https]
verify=disable
{noformat}
See https://access.redhat.com/articles/2039753

Or import the Ambari server's SSL cert into the truststore used by Python, which is more secure. 

",security ssl,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2018-03-06 14:22:48,6
13142855,Configurations attributes reset after running Add Service wizard,"h3. STEPS TO REPRODUCE
# Install the latest Sandbox docker (to re-reproduce the issue docker is easier)
# Add ""hadoop.security.group.mapping.ldap.bind.password"" in Custom core-site with the value ""admin-password"" with Property Type *""PASSWORD""*, and save
!image-2018-02-27-20-00-12-533.png|thumbnail! 
# From Add Service Wizard, add some service (I tested with Nifi and Solr but any service should be OK)
# After Wizard, go to HDFS => Configs and search ldap.bind.password, it shows text input box rather than password input box",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-06 13:43:28,4
13142846,"Installation of MySQL fails on CentOS 7.2, should install mariadb","I'm seeing the following error during HDP 3 beta installation.

    
    
    
    Traceback (most recent call last):
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 268, in _call_with_retries
        code, out = func(cmd, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    
    The above exception was the cause of the following exception:
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/HIVE/3.0.0.3.0/package/scripts/mysql_server.py"", line 68, in <module>
        MysqlServer().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/HIVE/3.0.0.3.0/package/scripts/mysql_server.py"", line 37, in install
        self.install_packages(env)
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 831, in install_packages
        retry_count=agent_stack_retry_count)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 53, in action_install
        self.install_package(package_name, self.resource.use_repos, self.resource.skip_repos)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/yumrpm.py"", line 251, in install_package
        self.checked_call_with_retries(cmd, sudo=True, logoutput=self.get_logoutput())
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 251, in checked_call_with_retries
        return self._call_with_retries(cmd, is_checked=True, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 268, in _call_with_retries
        code, out = func(cmd, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    stdout:   /var/lib/ambari-agent/data/output-49.txt
    2018-02-16 18:25:54,593 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-16 18:25:54,605 - Using hadoop conf dir: /usr/hdp/3.0.0.0-814/hadoop/conf
    2018-02-16 18:25:54,608 - Group['livy'] {}
    2018-02-16 18:25:54,609 - Group['spark'] {}
    2018-02-16 18:25:54,610 - Group['hdfs'] {}
    2018-02-16 18:25:54,610 - Group['zeppelin'] {}
    2018-02-16 18:25:54,610 - Group['hadoop'] {}
    2018-02-16 18:25:54,611 - Group['users'] {}
    2018-02-16 18:25:54,611 - Group['knox'] {}
    2018-02-16 18:25:54,612 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,614 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,615 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,617 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,618 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,620 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-16 18:25:54,621 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,623 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,624 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,626 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-16 18:25:54,627 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,629 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-16 18:25:54,630 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,632 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,633 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-16 18:25:54,635 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-16 18:25:54,636 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,639 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-16 18:25:54,650 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-16 18:25:54,651 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-16 18:25:54,653 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,657 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-16 18:25:54,658 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-16 18:25:54,672 - call returned (0, '1015')
    2018-02-16 18:25:54,674 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1015'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-16 18:25:54,683 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1015'] due to not_if
    2018-02-16 18:25:54,684 - Group['hdfs'] {}
    2018-02-16 18:25:54,685 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-16 18:25:54,687 - FS Type: 
    2018-02-16 18:25:54,687 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-16 18:25:54,731 - File['/usr/hdp/3.0.0.0-814/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-16 18:25:54,732 - Writing File['/usr/hdp/3.0.0.0-814/hadoop/conf/hadoop-env.sh'] because contents don't match
    2018-02-16 18:25:54,733 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-16 18:25:54,771 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-16 18:25:54,786 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-16 18:25:54,787 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-16 18:25:54,788 - Repository with url http://s3.amazonaws.com/dev.hortonworks.com/HDP-GPL/centos7/3.x/BUILDS/3.0.0.0-814 is not created due to its tags: set([u'GPL'])
    2018-02-16 18:25:54,788 - Repository['HDP-UTILS-1.1.0.22-repo-1'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-16 18:25:54,796 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-814\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-1]\nname=HDP-UTILS-1.1.0.22-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-16 18:25:54,796 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-16 18:25:54,798 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,943 - Skipping installation of existing package unzip
    2018-02-16 18:25:54,944 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,960 - Skipping installation of existing package curl
    2018-02-16 18:25:54,961 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:54,976 - Skipping installation of existing package hdp-select
    2018-02-16 18:25:54,995 - Skipping stack-select on MYSQL_SERVER because it does not exist in the stack-select package structure.
    2018-02-16 18:25:55,435 - Using hadoop conf dir: /usr/hdp/3.0.0.0-814/hadoop/conf
    2018-02-16 18:25:55,471 - call['ambari-python-wrap /usr/bin/hdp-select status hive-server2'] {'timeout': 20}
    2018-02-16 18:25:55,523 - call returned (0, 'hive-server2 - 3.0.0.0-814')
    2018-02-16 18:25:55,525 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-16 18:25:55,567 - File['/var/lib/ambari-agent/cred/lib/CredentialUtil.jar'] {'content': DownloadSource('http://will-hdp-1.field.hortonworks.com:8080/resources/CredentialUtil.jar'), 'mode': 0755}
    2018-02-16 18:25:55,569 - Not downloading the file from http://will-hdp-1.field.hortonworks.com:8080/resources/CredentialUtil.jar, because /var/lib/ambari-agent/tmp/CredentialUtil.jar already exists
    2018-02-16 18:25:55,570 - checked_call[('/usr/jdk64/jdk1.8.0_112/bin/java', '-cp', u'/var/lib/ambari-agent/cred/lib/*', 'org.apache.ambari.server.credentialapi.CredentialUtil', 'get', 'javax.jdo.option.ConnectionPassword', '-provider', u'jceks://file/var/lib/ambari-agent/cred/conf/mysql_server/hive-site.jceks')] {}
    2018-02-16 18:25:56,859 - checked_call returned (0, 'SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\nFeb 16, 2018 6:25:56 PM org.apache.hadoop.util.NativeCodeLoader <clinit>\nWARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nhive')
    2018-02-16 18:25:56,873 - Package['mysql-community-release'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-16 18:25:57,038 - Installing package mysql-community-release ('/usr/bin/yum -d 0 -e 0 -y install mysql-community-release')
    2018-02-16 18:25:58,091 - Execution of '/usr/bin/yum -d 0 -e 0 -y install mysql-community-release' returned 1. Error: Nothing to do
    2018-02-16 18:25:58,091 - Failed to install package mysql-community-release. Executing '/usr/bin/yum clean metadata'
    2018-02-16 18:25:58,386 - Retrying to install package mysql-community-release after 30 seconds
    2018-02-16 18:26:37,003 - Skipping stack-select on MYSQL_SERVER because it does not exist in the stack-select package structure.
    
    Command failed after 1 tries
    

",pull-request-available,[],AMBARI,Bug,Major,2018-03-06 13:22:56,0
13142588,Alignment issues for checkboxes and radio buttons,The alignment of the radio buttons and checkboxes with the labels are messed up.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-05 16:40:40,4
13142556,Stack Advisor Should not Use 'accessible-node-labels' as a Queue Name,"Ambari UI can not save changes due to Ambari StackAdvisor failing if accessible-node-labels is used as a queue name:

2018-02-20 05:16:30,497 - Entered __getSelectedQueueTotalCap fn() with llap_daemon_selected_queue_name= 'llap'. 
2018-02-20 05:16:30,498 - DBG: Selected queue name as: yarn.scheduler.capacity.root.accessible-node-labels.llap.capacity 
2018-02-20 05:16:30,498 - Queue list : ['root', 'accessible-node-labels', 'llap'] 
2018-02-20 05:16:30,498 - DBG: Selected queue name as: yarn.scheduler.capacity.root.capacity 
2018-02-20 05:16:30,498 - Total capacity available for queue root is : 1187840.0 
Error occured in stack advisor. 
Error details: float() argument must be a string or a number 
===",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-03-05 15:04:25,3
13142543,Remove obsolete modules from the Ambari source code,"The following obsolete modules should be removed from Ambari.  Since they are not used or maintained there is no need to keep them around:  

* Groovy Shell (ambari-shell/ambari-groovy-shell) [last updated 9 months ago*]
* Groovy Client (ambari-client/groovy_client) [last updated 2 years ago]
* Python Shell (ambari-shell/ambari-python-shell) [last updated 4 years ago*]
*  Python Client (ambari-client/python_client)  [last updated 2 years ago]
",pull-request-available,"['ambari-client', 'ambari-shell']",AMBARI,Task,Major,2018-03-05 14:26:51,7
13142440,Fix BlackDuck found security issues in Ambari Functional Tests,Fix BlackDuck found security issues in Ambari Functional Tests,black-duck pull-request-available security,['test'],AMBARI,Bug,Blocker,2018-03-05 07:45:05,6
13142439,Fix typo in AmbariHostsTest in groovy-client,"In our groovy-client project the Python unit tests are failing in Jenkins due to the following typo in 

AmbariHostsTest.test get host components as map when there is no cluster yet:

ambari.metaClass.getHostComponenets ",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-05 07:32:23,6
13142122,Fix ConfigUpgradeValidityTest.testConfigurationDefinitionsExist test failure,"Fix ConfigUpgradeValidityTest.testConfigurationDefinitionsExist test failure which is caused since no active HDP stacks are available in the core Ambari source code tree. 

Thanks [~smolnar] for pointing out the issue.
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-02 15:35:11,7
13142114,Config version not shown after overriding properties with a config group,"STR:
# Navigate to HDFS -> Config Tab
# Create new Config group 'group1' and add 2 hosts with DataNode to the Config group
# Select this new group
# Override properties in Advanced Tab for this config group and Click Save

Observed: After the config change is saved, the Version is not populated in the version dropdown and also there is a ""Make Current"" Button which is not present in earlier releases.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-03-02 14:47:00,4
13141949,Multiple issues while executing Ambari server upgrade to Ambari 2.7.0,"*1. An NPE is thrown will initializing the Ambari server upgrade catalog*

{code:title=com.google.inject.persist.jpa.JpaPersistService#begin}
    public void begin() {
        Preconditions.checkState(null == this.entityManager.get(), ""Work already begun on this thread. Looks like you have called UnitOfWork.begin() twice without a balancing call to end() in between."");
        this.entityManager.set(this.emFactory.createEntityManager());
    }
{code}

{{this.emFactory}} is {{null}}.

*Cause*
{{com.google.inject.persist.jpa.JpaPersistService#start()}} was not called before {{com.google.inject.persist.jpa.JpaPersistService#begin}} to do order of operations in {{org.apache.ambari.server.upgrade.SchemaUpgradeHelper#main}}.  

*Solution*
Ensure {{com.google.inject.persist.jpa.JpaPersistService#start()}} is being called before {{com.google.inject.persist.jpa.JpaPersistService#begin}} in {{org.apache.ambari.server.upgrade.SchemaUpgradeHelper#main}}.  

-----

*2. Missing repo_os, repo_definition, and repo_tags tables*

The {{repo_os}}, {{repo_definition}}, and {{repo_tags}} tables were  never added to the UpgradeCatalog implementation.  The migration logic is also needed.
*Solution*

Add the missing tables and logic while executing {{org.apache.ambari.server.upgrade.UpgradeCatalog270#executeDDLUpdates}}. 

-----

*3. Entity classes are initialized before the schema of the underlying database is updated*

*Solution*
Notify relevant classes that the persistence infrastructure is ready after DDL updates have been applied. 

The {{org.apache.ambari.server.events.publishers.AmbariEventPublisher}} is to be used for issuing  a {{org.apache.ambari.server.events.JpaInitializedEvent}}.

-----

*4. JVM does not exit after performing upgrade*

After executing {{ambari-server upgrade}}, the JVM process hangs and does not exit.  According to the logs, no errors have occurred.

*Cause*
The cause of this is several non-daemon threads not being shutdown by Ambari code. 

*Solution*
Change relevant non-daemon threads to daemon threads and ensure any thread polls are not forcing one thread to be alive at all times. 



",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-03-01 23:55:43,7
13141892,Fix BlackDuck found security issues in Ambari Server,Fix BlackDuck found security issues in Ambari Server,black-duck pull-request-available security,['ambari-server'],AMBARI,Bug,Blocker,2018-03-01 19:54:44,6
13141768,Missing 'hash' in alert update message.,"{noformat}
Traceback (most recent call last):
 File ""/usr/lib/ambari-agent/lib/ambari_agent/HeartbeatThread.py"", line 130, in register
 listener.on_event({}, response)
 File ""/usr/lib/ambari-agent/lib/ambari_agent/listeners/AlertDefinitionsEventListener.py"", line 51, in on_event
 self.alert_definitions_cache.rewrite_cache(message['clusters'], message['hash'])
KeyError: 'hash'
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-03-01 11:46:19,1
13141595,Additional fixes for service summary page to support NameNode Federation,"- Add mapping of namespace id property from API
- Display NameNodes as active and standby ones
- Remove tabs to select certain namespace
- Add namespace-scoped HDFS checkpoint check and warning message before stopping and restarting NameNodes
- Too long namespace id should be cropped with adding ellipsis and beung displayed fully in tooltip",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-28 18:24:26,2
13141562,Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Functional Tests,"Remove dependency on org.apache.httpcomponents:httpclient:jar before version 4.3.5.1 due to security concerns. See
 * CVE-2015-5262 - [https://nvd.nist.gov/vuln/detail/CVE-2015-5262]
 * CVE-2014-3577 - [https://nvd.nist.gov/vuln/detail/CVE-2014-3577]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-funtest ---
 org.apache.ambari:ambari-funtest:jar:2.6.1.0.0
 +- org.apache.httpcomponents:httpclient:jar:4.5.2:compile
 +- org.apache.ambari:ambari-metrics-common:jar:2.6.1.0.0:compile
 |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for conflict with 4.5.2)
 \- org.apache.ambari:ambari-server:jar:2.6.1.0.0:compile
    +- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for conflict with 4.5.2)
    +- org.apache.hadoop:hadoop-auth:jar:2.7.2:compile
    |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for conflict with 4.5.2)
    \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
       \- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
          \- (org.apache.httpcomponents:httpclient:jar:4.1.2:compile - omitted for conflict with 4.5.2){noformat}",black-duck pull-request-available,['test'],AMBARI,Bug,Blocker,2018-02-28 15:45:44,6
13141555,Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Functional Tests,"Remove dependency on commons-collections:commons-collections before version 3.2.2 due to security concerns. See
 * CVE-2015-6420 - [https://nvd.nist.gov/vuln/detail/CVE-2015-6420]
 * CVE-2015-7501 - [https://nvd.nist.gov/vuln/detail/CVE-2015-7501]
 * CVE-2017-15708 - [https://nvd.nist.gov/vuln/detail/CVE-2017-15708]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-funtest ---
 org.apache.ambari:ambari-funtest:jar:2.6.1.0.0
 +- org.apache.directory.server:apacheds-server-annotations:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-core-annotations:jar:2.0.0-M19:test
 |  |  +- org.apache.directory.server:apacheds-xdbm-partition:jar:2.0.0-M19:test
 |  |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  |  \- org.apache.directory.mavibot:mavibot:jar:1.0.0-M6:test
 |  |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.api:api-ldap-codec-core:jar:1.0.0-M26:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.directory.server:apacheds-core-integ:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-interceptors-authn:jar:2.0.0-M19:test
 |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.server:apacheds-interceptors-hash:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M19:compile
 |  \- org.apache.directory.api:api-ldap-model:jar:1.0.0-M26:compile
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - omitted for duplicate)
 +- org.apache.directory.server:apacheds-core:jar:2.0.0-M19:test
 |  \- org.apache.directory.server:apacheds-interceptors-exception:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.shared:shared-ldap:jar:0.9.17:test
 |  \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.velocity:velocity:jar:1.7:compile{noformat}",black-duck pull-request-available,['test'],AMBARI,Bug,Blocker,2018-02-28 15:25:50,6
13141470,Exception caught while installing Timeline Service V2.0 ,"While installing Timeline Service V2.0 during CI run, the following error is
captured:

    
    
    Caught an exception while executing custom service command: <type 'exceptions.Exception'>: Command requires configs with timestamp=1519782088331 but configs on agent have timestamp=1519783511993; Command requires configs with timestamp=1519782088331 but configs on agent have timestamp=1519783511993
    

See <http://172.27.30.209:8080> (up until 8pm ET 3/1)

",pull-request-available,[],AMBARI,Bug,Major,2018-02-28 09:18:36,0
13141224,Ambari does not manage repositories,"While running CI tests

    
    
     
    Traceback (most recent call last): 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/providers/package/__init__.py"", line 283, in _call_with_retries 
        code, out = func(cmd, **kwargs) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 72, in inner 
        result = function(command, **kwargs) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 102, in checked_call 
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 150, in _call_wrapper 
        result = _call(command, **kwargs_copy) 
      File ""/usr/lib/ambari-agent/lib/resource_management/core/shell.py"", line 303, in _call 
        raise ExecutionFailed(err_msg, code, out, err) 
    ExecutionFailed: Execution of '/usr/bin/yum -d 0 -e 0 -y install hadooplzo_3_0_0_2_87' returned 1. Error: Nothing to do 
    
    
    
    2018-02-26 12:16:14,479 - Repository for HDP/3.0.0.2-87/HDP-3.0 is not managed by Ambari
    2018-02-26 12:16:14,479 - Repository for HDP/3.0.0.2-87/HDP-3.0-GPL is not managed by Ambari
    2018-02-26 12:16:14,480 - Repository for HDP/3.0.0.2-87/HDP-UTILS-1.1.0.22 is not managed by Ambari
    

I checked in DB and indeed manage_repos was false, however I didn't found any
signs in logs etc. that it was set to false during cluster setup.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-27 13:52:27,0
13141219,Remove dependency on org.apache.zookeeper:zookeeper before version 3.4.6.2.0.0.0-579 for Ambari Server,"Remove dependency on org.apache.zookeeper:zookeeper before version 3.4.6.2.0.0.0-579 due to security concerns. See
 * CVE-2017-5637 - [https://nvd.nist.gov/vuln/detail/CVE-2017-5637]
 * CVE-2016-5017 - [https://nvd.nist.gov/vuln/detail/CVE-2016-5017]

{noformat}
 --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.ambari:ambari-metrics-common:jar:2.6.1.0.0:compile
 |  \- org.apache.curator:curator-framework:jar:2.7.1:compile
 |     \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitted for duplicate)
 +- org.apache.hadoop:hadoop-auth:jar:2.7.2:compile
 |  \- org.apache.zookeeper:zookeeper:jar:3.4.6:compile
 \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
    +- org.apache.curator:curator-client:jar:2.7.1:compile
    |  \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitted for duplicate)
    +- org.apache.curator:curator-recipes:jar:2.7.1:compile
    |  \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitted for duplicate)
    \- (org.apache.zookeeper:zookeeper:jar:3.4.6:compile - omitte{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-27 13:24:34,6
13141000,Target host shows all the hosts available in Move Wizard,The target host selection in the Move Wizard shows all the available hosts in the cluster instead of the hosts where the component is not installed. ,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-26 18:40:43,4
13140910,Missing permission for 'others' when Ambari is configured with two way SSL and https enabled,"# Deploy Ambari-2.6.2.0 server on machine A
# Manually install and register agents on other machines (including machine A)
# Enable 2 way SSL between server and agents
# Enable https at Ambari server
# Deploy a cluster via blueprints with HDP-2.6.5.0

After cluster is deployed, observed that the permission of files such as hadoop-env.sh is '-rw-r-----'
Complete output:
{code}
[root@ctr-e138-1518143905142-36503-01-000002 logs]# ls -lhrt /etc/hadoop/conf/
total 176K
-rw-r--r-- 1 cstm-hdfs hadoop 8.9K Feb 22 09:30 core-site.xml
-rw-r----- 1 cstm-hdfs hadoop 333 Feb 22 09:35 hdfs_dn_jaas.conf
-rw-r----- 1 cstm-hdfs hadoop 333 Feb 22 09:35 hdfs_nn_jaas.conf
-rw-r----- 1 cstm-hdfs hadoop 1.3K Feb 22 09:35 hadoop-policy.xml
-rw-r----- 1 cstm-hdfs hadoop 884 Feb 22 09:35 ssl-client.xml
drwxr-xr-x 2 root hadoop 4.0K Feb 22 09:35 secure
-rw-r----- 1 cstm-hdfs hadoop 1000 Feb 22 09:35 ssl-server.xml
-rw-r--r-- 1 cstm-hdfs hadoop 8.7K Feb 22 09:35 hdfs-site.xml
-rw-r--r-- 1 cstm-mr hadoop 7.5K Feb 22 09:37 mapred-site.xml
-rw-r--r-- 1 cstm-hdfs hadoop 2.3K Feb 22 09:37 capacity-scheduler.xml
-rw-r--r-- 1 root hadoop 1.1K Feb 22 09:37 container-executor.cfg
-rwxr-xr-x 1 root root 984 Feb 22 09:37 mapred-env.sh
-rw-r--r-- 1 root hadoop 947 Feb 22 09:37 taskcontroller.cfg
-rw-r----- 1 cstm-yarn hadoop 571 Feb 22 09:37 yarn_jaas.conf
-rw-r----- 1 cstm-yarn hadoop 337 Feb 22 09:37 yarn_ats_jaas.conf
-rw-r----- 1 cstm-yarn hadoop 333 Feb 22 09:37 yarn_nm_jaas.conf
-rw-r----- 1 cstm-mr hadoop 320 Feb 22 09:37 mapred_jaas.conf
-rw-r----- 1 root root 1020 Feb 22 09:48 commons-logging.properties
-rw-r----- 1 root root 1.6K Feb 22 09:48 health_check
-rw-r--r-- 1 cstm-hdfs hadoop 11K Feb 22 09:48 log4j.properties
-rwxr-xr-x 1 root root 4.2K Feb 22 09:48 task-log4j.properties
-rwxr-xr-x 1 root root 2.4K Feb 22 09:48 topology_script.py
-rw-r----- 1 root root 241 Feb 22 10:10 slaves
-rw-r----- 1 root hadoop 6.3K Feb 22 10:10 hadoop-env.sh
-rw-r--r-- 1 cstm-yarn hadoop 24K Feb 22 10:10 yarn-site.xml
-rwxr-xr-x 1 cstm-yarn hadoop 5.5K Feb 22 10:10 yarn-env.sh
-rw-r----- 1 cstm-hdfs hadoop 2.6K Feb 22 10:12 hadoop-metrics2.properties
-rw-r--r-- 1 cstm-hdfs hadoop 467 Feb 22 10:12 topology_mappings.data
-rw-r----- 1 cstm-hdfs hadoop 1 Feb 22 10:13 dfs.exclude
{code}

 


When compared this with a non-SSL cluster the permission is '-rw-r--r--' i.e. read permission is available for other users",system_test,['ambari-server'],AMBARI,Bug,Critical,2018-02-26 12:24:43,6
13140527,Service Auto Start cannot be disabled,"*STR*
# navigate / Admin / Service Auto Start
# disable Auto Start
# save 
# refresh page",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-23 15:36:48,4
13140522,Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Server,"Remove dependency on commons-collections:commons-collections before version 3.2.2 due to security concerns. See
 * CVE-2015-6420 - [https://nvd.nist.gov/vuln/detail/CVE-2015-6420]
 * CVE-2015-7501 - [https://nvd.nist.gov/vuln/detail/CVE-2015-7501]
 * CVE-2017-15708 - [https://nvd.nist.gov/vuln/detail/CVE-2017-15708]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.directory.server:apacheds-server-annotations:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-core-annotations:jar:2.0.0-M19:test
 |  |  +- org.apache.directory.server:apacheds-xdbm-partition:jar:2.0.0-M19:test
 |  |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  |  \- org.apache.directory.mavibot:mavibot:jar:1.0.0-M6:test
 |  |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.api:api-ldap-codec-core:jar:1.0.0-M26:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.directory.server:apacheds-core-integ:jar:2.0.0-M19:test
 |  +- org.apache.directory.server:apacheds-interceptors-authn:jar:2.0.0-M19:test
 |  |  \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 |  \- org.apache.directory.server:apacheds-interceptors-hash:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M19:compile
 |  \- org.apache.directory.api:api-ldap-model:jar:1.0.0-M26:compile
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - omitted for duplicate)
 +- org.apache.directory.server:apacheds-core:jar:2.0.0-M19:test
 |  \- org.apache.directory.server:apacheds-interceptors-exception:jar:2.0.0-M19:test
 |     \- (commons-collections:commons-collections:jar:3.2.1:test - omitted for duplicate)
 +- org.apache.directory.shared:shared-ldap:jar:0.9.17:test
 |  \- (commons-collections:commons-collections:jar:3.2.1:compile - scope updated from test; omitted for duplicate)
 +- org.apache.velocity:velocity:jar:1.7:compile
 |  \- commons-collections:commons-collections:jar:3.2.1:compile
 +- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
 |  +- (commons-collections:commons-collections:jar:3.2.2:compile - omitted for conflict with 3.2.1)
 |  \- commons-configuration:commons-configuration:jar:1.6:compile
 |     \- (commons-collections:commons-collections:jar:3.2.1:compile - omitted for duplicate)
 \- utility:utility:jar:1.0.0.0-SNAPSHOT:test
    \- com.puppycrawl.tools:checkstyle:jar:6.19:test
       \- (commons-collections:commons-collections:jar:3.2.2:test - omitted for conflict with 3.2.1){noformat}",black-duck pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-23 15:10:37,6
13140495,Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Server,"Remove dependency on org.apache.httpcomponents:httpclient:jar before version 4.3.5.1 due to security concerns. See
 * CVE-2015-5262 - [https://nvd.nist.gov/vuln/detail/CVE-2015-5262]
 * CVE-2014-3577 - [https://nvd.nist.gov/vuln/detail/CVE-2014-3577]

{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.httpcomponents:httpclient:jar:4.2.5:compile
 +- org.apache.ambari:ambari-metrics-common:jar:2.6.1.0.0:compile
 |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for duplicate)
 +- org.apache.hadoop:hadoop-auth:jar:2.7.2:compile
 |  \- (org.apache.httpcomponents:httpclient:jar:4.2.5:compile - omitted for duplicate)
 \- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
    \- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
       \- (org.apache.httpcomponents:httpclient:jar:4.1.2:compile - omitted for conflict with 4.2.5)
 {noformat}
 * 
 * 
[Options|https://hortonworks.jira.com/browse/BUG-97133?filter=54432]

h2.  ",black-duck pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-23 13:14:53,6
13140436,Remove dependency on com.fasterxml.jackson.core:jackson-databind before version 2.9.4 for Ambari Server,"Remove dependency on com.fasterxml.jackson.core:jackson-databind before version 2.9.4 due to security concerns. See
 * CVE-2018-5968 - [https://nvd.nist.gov/vuln/detail/CVE-2018-5968]
 * CVE-2017-17485 - [https://nvd.nist.gov/vuln/detail/CVE-2017-17485]

{noformat}
 --- maven-dependency-plugin:2.8:tree (default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 \- com.networknt:json-schema-validator:jar:0.1.10:test
    \- com.fasterxml.jackson.core:jackson-databind:jar:2.8.7:test
{noformat}",black-duck pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-23 08:59:48,6
13140359,yum installation fails if there is any transaction files,"Starting from 2.6.1 Ambari started checking transaction files during the repo/service installation and it fails with below error.


{noformat}
2018-02-16 16:12:21,639 - File['/etc/yum.repos.d/ambari-hdp-104.repo'] {'content': '[HDP-2.6-repo-104]\nname=HDP-2.6-repo-104\nbaseurl=http://xxxxxxx/vcm_hadoop/HDP/centos6/2.6.4.0-91\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.22-repo-104]\nname=HDP-UTILS-1.1.0.22-repo-104\nbaseurl=http://xxxxxx/vcm_hadoop/HDP-UTILS-1.1.0.22\n\npath=/\nenabled=1\ngpgcheck=0'} 
2.  2018-02-16 16:12:21,639 - Writing File['/etc/yum.repos.d/ambari-hdp-104.repo'] because contents don't match 
3.  2018-02-16 16:12:21,639 - Yum non-completed transactions check failed, found 1 non-completed transaction(s): 
4.  2018-02-16 16:12:21,640 - [2015-03-12.17:13.28.disabled] Packages broken: hue-common-2.6.1.2.2.0.0-2041.el6.x86_64; Packages not-installed hue-hcatalog-2.6.1.2.2.0.0-2041.el6.x86_64, hue-2.6.1.2.2.0.0-2041.el6.x86_64, hue-pig-2.6.1.2.2.0.0-2041.el6.x86_64, hue-oozie-2.6.1.2.2.0.0-2041.el6.x86_64, hue-beeswax-2.6.1.2.2.0.0-2041.el6.x86_64, hue-server-2.6.1.2.2.0.0-2041.el6.x86_64 
5.  2018-02-16 16:12:21,640 - *** Incomplete Yum Transactions *** 
6.  2018-02-16 16:12:21,640 - 
7.  2018-02-16 16:12:21,640 - Ambari has detected that there are incomplete Yum transactions on this host. This will interfere with the installation process and must be resolved before continuing. 
8.  2018-02-16 16:12:21,640 - 
9.  2018-02-16 16:12:21,640 - - Identify the pending transactions with the command 'yum history list <packages failed>' 
10. 2018-02-16 16:12:21,640 - - Revert each pending transaction with the command 'yum history undo' 
11. 2018-02-16 16:12:21,640 - - Flush the transaction log with 'yum-complete-transaction --cleanup-only' 
After cleaning up with 'yum-complete-transaction --cleanup-only' still Ambari could not able to proceed further with same error.

When user tried to do yum install it was going through successfully.

Finally had to delete /var/lib/yum/transaction* manually for ambari to proceed further with installation. these transactions files were very old and nothing to do with any latest installation but still ambari does not proceed further.
{noformat}


What is expected: If this validation was done intentionally then it should throw proper message guiding the user to remove those files.

OptionsAttachments",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-02-22 23:32:10,0
13140258,GPL repo is not hidden when default version is chosen in UI,"Ambari server has gpl license disabled i.e.
{code}
cat /etc/ambari-server/conf/ambari.properties | grep -i gpl
gpl.license.accepted=false
{code}

- On register version page, upon choosing HDP-2.6.4.0 from the drop-down, the GPL repo field is not masked (see default-version.png)
- However upon choosing to upload the version from VDF URL, the GPL field is correctly masked (see version-via-vdf.png)

Looks like the problem is [showRepo()|https://github.com/apache/ambari/blob/branch-2.6/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js#L50] function which calls [isGPLRepo|https://github.com/apache/ambari/blob/branch-2.6/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js#L46] function where if we do not load version fromVDF URL the showRepo return true

 ",pull-request-available system_test,['ambari-server'],AMBARI,Bug,Critical,2018-02-22 17:23:15,1
13140247,Remove dependency on commons-beanutils:commons-beanutils before version 1.9.2 for Ambari Server,"Remove dependency on commons-beanutils:commons-beanutils before version 1.9.2 due to security concerns. See CVE-2014-0114 - [https://nvd.nist.gov/vuln/detail/CVE-2014-0114]
{noformat}
--- maven-dependency-plugin:2.8:tree(default-cli) @ ambari-server ---
 org.apache.ambari:ambari-server:jar:2.6.1.0.0
 +- org.apache.hadoop:hadoop-common:jar:2.7.2:compile
 |  \- commons-configuration:commons-configuration:jar:1.6:compile
 |     +- commons-digester:commons-digester:jar:1.8:compile
 |     |  \- commons-beanutils:commons-beanutils:jar:1.9.2:compile
 |     \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
 \- utility:utility:jar:1.0.0.0-SNAPSHOT:test
    \- com.puppycrawl.tools:checkstyle:jar:6.19:test
       \- (commons-beanutils:commons-beanutils:jar:1.9.2:compile -{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-22 16:29:07,6
13140213,Extend service actions dropdowns with namespace-specific items,"- User should be able to apply the service action (if it can be executed for certain components only) to the given namespace
- Add Restart NameNodes item",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-22 14:45:18,2
13140193,Background Ops during wizard pages do not show all the ops running by default,The Background Ops opened inside an install wizard do not show all the operations running by default. They are shown only after selecting the dropdown.,pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-22 13:47:15,4
13140178,Hosts selection not shown while adding Zookeeper server,"STR:
# Login and navigate to Zookeeper service
# Click actions button
# Select Add Zookeeper Server option

In the confirmation window that appears, the host selection is not present.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-22 12:19:54,4
13140155,Remove unsecure dependencies from Ambari Groovy Shell,"* Remove dependency on org.apache.zookeeper:zookeeper before version 3.4.6.2.0.0.0-579 for Ambari Groovy Shell

* Remove dependency on xerces:xercesImpl before version 2.11.2 for Ambari Groovy Shell

* Remove dependency on tomcat:jasper-compiler and tomcat:jasper-runtime before version 6.0.20.0 for Ambari Groovy Shell

* Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Groovy Shell

* Remove dependency on commons-httpclient:commons-httpclient before version 3.1.0 for Ambari Groovy Shell

* Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Groovy Shell

* Remove dependency on commons-beanutils:commons-beanutils-core before version 1.9.2 for Ambari Groovy Shell",pull-request-available,['ambari-shell'],AMBARI,Bug,Major,2018-02-22 10:10:05,3
13140140,Remove unsecure dependencies from Ambari Groovy Client,"Remove dependency on xerces:xercesImpl before version 2.11.2 for Ambari Groovy Client

Remove dependency on org.apache.httpcomponents:httpclient before version 4.3.5.1 for Ambari Groovy Client

Remove dependency on commons-collections:commons-collections:jar before version 3.2.2 for Ambari Groovy Client

Remove dependency on commons-beanutils:commons-beanutils-core before version 1.9.2 for Ambari Groovy Client",pull-request-available,['ambari-client'],AMBARI,Bug,Major,2018-02-22 09:06:50,3
13139918,Save/discard button is not found for ranger configuration,"STR:
1) go to ambari ui
2) open ranger configs

ER: there should be save/discard button
AR: no save/discard button found",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-21 14:41:04,4
13139880,Deploy fails during Atlas Client installation,"Deploy jobs are failing with following exception during installation of Atlas
metadata client:

    
    
    
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ATLAS/package/scripts/atlas_client.py"", line 53, in 
        AtlasClient().execute()
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 377, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/3.0/services/ATLAS/package/scripts/atlas_client.py"", line 41, in install
        self.install_packages(env)
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 823, in install_packages
        name = self.format_package_name(package['name'])
      File ""/usr/lib/ambari-agent/lib/resource_management/libraries/script/script.py"", line 567, in format_package_name
        raise Fail(""Cannot match package for regexp name {0}. Available packages: {1}"".format(name, self.available_packages_in_repos))
    resource_management.core.exceptions.Fail: Cannot match package for regexp name atlas-metadata_${stack_version}. Available packages: ['openblas', 'openblas-Rblas', 'openblas-devel', 'openblas-openmp', 'openblas-openmp64', 'openblas-openmp64_', 'openblas-serial64', 'openblas-serial64_', 'openblas-static', 'openblas-threads', 'openblas-threads64', 'openblas-threads64_', 'snappy', 'snappy', 'snappy-devel', 'snappy-devel']
    
",pull-request-available,[],AMBARI,Bug,Major,2018-02-21 11:52:47,0
13139878,'Current' Tag not shown in the Versions page for the currently installed version,"The 'Current' tag is not shown against the installed cluster.

Also, on the Stacks & Versions page, there are lines shown when an older version of HDP is registered:",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-21 11:46:19,4
13139803,Alert filter with services having space is not working,"Alert filter with services having space is not working.
STR:
1. Navigate to Alerts page
2. Try to filter using Service as filter key with services like ""Ambari Infra""/""Log Search""
3. Alerts are not filtered.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-21 01:29:45,4
13139802,Admin user can delete own user from Users page,"Admin user can delete own user from Users page.

 

STR:
 # Login to ambari UI as admin user
 # Navigate to Users page
 # Try to delete the same user (admin) from the users list

The delete option is disabled in the individual user page, but not from the all Users page.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-21 01:27:23,4
13139719,HDFS Balancer via Ambari fails when FIPS mode is activated on the OS,"This is similar issue as reported on AMBARI-22417, but for service checks. 

The original issue of ambari check is resolved. But the same issue is seen when running the HDFS balancer via Ambari.

*Solution*
MD5 is disabled on the OS, Code needs to be updated to use SHA

This is required when FIPS mode is enabled on the RHEL OS",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-20 17:02:35,7
13139671,View name is not displayed in ambari-views page,"*Steps to reporduce:-*
1. go to ambari-views tab
2. create as many view you can create 
3. see the views dashboard

Screenshot: [^Screen Shot 2018-02-09 at 11.44.53 AM.png]

The Name field is blank

the Name Only get populated add a ""Short URL"".",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-20 14:20:13,4
13139646,Not able to delete livy component from a host,"I am not able to delete livy(only one instance of livy is present on the cluster) from a host.
After stopping livy, when I try to delete, its showing below message :
""WARNING! Delete the last Livy for Spark2 Server component in the cluster?
Deleting the last component in the cluster could result in permanent loss of service data."" (Attaching the screenshot)

But there's not way to enable 'Confirm Delete' button.

Note : In earlier versions of ambari there used to be a check box and user has to check it to enable delete button.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-20 12:29:58,4
13139569,WEB type alerts authentication in Kerberos secured cluster,"In a Kerberized cluster some web endpoints (App Timeline Web UI, ResourceManger Web UI, etc.) require authentication. Any Ambari alerts checking those endpoints must then be able to authenticate.

This was addressed in AMBARI-9586, however the default principal and keytab used in the alerts.json is that of the ""bare"" SPNEGO principal HTTP/_HOST@REALM. 
 My understanding is that the HTTP service principal is used to authenticate users to a service, not used to authenticate to another service.

1. Since most endpoints involved are Web UI, would it be more appropriate to use the smokeuser in the alerts?

2. This was first observed in Ranger Audit, the YARN Ranger Plug-in showed many access denied from HTTP user. [This post|https://community.hortonworks.com/content/supportkb/150206/ranger-audit-logs-refers-to-access-denied-for-http.html] provided some direction as to where those requests were coming from. We have updated the ResourceManger Web UI alert definition to use cluster-env/smokeuser_keytab and cluster-env/smokeuser_principal_name and this has resolved the initial HTTP access denied. 
 Would it also be advisable to make the change in the other secure Web UI alert definitions?",pull-request-available,['alerts'],AMBARI,Bug,Minor,2018-02-20 04:54:40,6
13139436,ServiceInfo: credential_store_supported attempts to overwrite maintenance_state,"Try to update {{credential_store_supported}} property of a service:

{noformat}
$ curl -X PUT -d @- ""http://$AMBARI_SERVER:8080/api/v1/clusters/TEST/services/HDFS"" <<EOF
{ ""ServiceInfo"": { ""credential_store_supported"": ""true"" } }
EOF
HTTP/1.1 400 Bad Request
...
  ""message"" : ""java.lang.IllegalArgumentException: No enum constant org.apache.ambari.server.state.MaintenanceState.true""
{noformat}

Expected response:

{{IllegalArgumentException: Invalid arguments, cannot update credential_store_supported as it is set only via service definition.}}

The response code is the same as expected due to a coincidence.

The problem is setting the wrong property:

{noformat}
 414     o = properties.get(SERVICE_CREDENTIAL_STORE_SUPPORTED_PROPERTY_ID);
 415     if (null != o) {
 416       svcRequest.setMaintenanceState(o.toString());
 417     }
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2018-02-19 13:55:53,5
13139417,Missing LDAP configuration property warning should be a debug statement,"Missing LDAP configuration property warning should be a debug statement. The Ambari server log is being filled with statements like:
{noformat}
  
16 Feb 2018 14:01:36,617  WARN [main] AmbariLdapConfiguration:47 - Ldap configuration property [ambari.ldap.connectivity.server.host] hasn't been set; using default value
{noformat}
The log level for these messages should be lowered to at least DEBUG.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-19 12:57:47,6
13139371,[PERF] Deployment of PERF stack leads to Out of Sync,"While deploying PERF stack, PERF 1.0 stack is not installed and shows up as
Out of sync  

",pull-request-available,[],AMBARI,Bug,Major,2018-02-19 08:33:47,0
13138991,Ignore python UT failing on branch-3.0-perf,"As discussed with Sid Wagle for now we should ignore UT failing on branch-3.0-perf as they take up a lot of
time rework.

The reason for this is that we need to merge into trunk soon, so the feature
code gets enough test cycles.  
The tests will be unignored and fixed after the merge is done.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-16 08:48:52,0
13138982,"""Stale Alerts"" alert appears on cluster after some time","Previously. Every x seconds any alert was updated with information from agent.
Even if nothing changed.  
Due to perf reasons this behavior was changed so that agent reports alerts
only when their status changes.

Due this change, ""stale alerts"" alert started appearing on clusters, showing
that alerts were not updated for a long time.

We should consider alert up-to-date to the point of last heartbeat. Which
shows that the connection between server and agent is okay due to
last_hb_timestamp, and so agent could send alert updates.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-16 07:33:11,0
13138942,Ranger LB URL is pointing to ranger port by default when we click on Quick link,"For Ranger HA, when we give External url as LB url and port (LB:port), but when we click on quick link is pointing to LB url, but it is using ranger port(6080 or 6182 depend it's http or https).

Below code snippet - reading the host name from policymgr_external_url and then using default ranger port.


{noformat}
} else if (serviceName === 'RANGER') {
      var siteConfigs = this.get('configProperties').findProperty('type', 'admin-properties').properties;
      if (siteConfigs['policymgr_external_url']) {
        host = siteConfigs['policymgr_external_url'].split('://')[1].split(':')[0];
         var newItem = {};
        newItem.url = siteConfigs['policymgr_external_url'];
        newItem.label = link.label;
         return newItem;
        }
    }
{noformat}
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-16 00:24:27,4
13138758,Timeline Service V2.0 reader install fails if wget is not already installed on the host,"Encountered while testing Atlantic Beta 1.  
Timeline Service V2.0 reader install fails if wget is already not installed on
the host beforehand.

    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 101, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 45, in install
        hbase_service.install_hbase(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/hbase_service.py"", line 82, in install_hbase
        Execute(hbase_download_cmd, user=""root"", logoutput=True)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
        tries=self.resource.tries, try_sleep=self.resource.try_sleep)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase' returned 127. -bash: wget: command not found
     stdout:
    2018-02-13 07:39:31,705 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:31,711 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:31,713 - Group['livy'] {}
    2018-02-13 07:39:31,714 - Group['spark'] {}
    2018-02-13 07:39:31,714 - Group['hdfs'] {}
    2018-02-13 07:39:31,714 - Group['zeppelin'] {}
    2018-02-13 07:39:31,714 - Group['hadoop'] {}
    2018-02-13 07:39:31,715 - Group['users'] {}
    2018-02-13 07:39:31,715 - Group['knox'] {}
    2018-02-13 07:39:31,716 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,717 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,718 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,719 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,720 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,721 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,722 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,723 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,725 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,726 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,727 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,728 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,729 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,731 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-13 07:39:31,732 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,735 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-13 07:39:31,744 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-13 07:39:31,744 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-13 07:39:31,746 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,748 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,749 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-13 07:39:31,759 - call returned (0, '1014')
    2018-02-13 07:39:31,760 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-13 07:39:31,766 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] due to not_if
    2018-02-13 07:39:31,766 - Group['hdfs'] {}
    2018-02-13 07:39:31,767 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-13 07:39:31,767 - FS Type: 
    2018-02-13 07:39:31,767 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-13 07:39:31,786 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-13 07:39:31,787 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-13 07:39:31,803 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,811 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,812 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,813 - Repository['HDP-UTILS-1.1.0.21-repo-1'] {'append_to_file': True, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,816 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.21-repo-1]\nname=HDP-UTILS-1.1.0.21-repo-1\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,817 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,817 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,902 - Skipping installation of existing package unzip
    2018-02-13 07:39:31,902 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,912 - Skipping installation of existing package curl
    2018-02-13 07:39:31,912 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,922 - Skipping installation of existing package hdp-select
    2018-02-13 07:39:32,207 - Looking for matching packages in the following repositories: HDP-3.0-repo-1, HDP-UTILS-1.1.0.21-repo-1
    2018-02-13 07:39:34,268 - Package['hadoop_3_0_0_0_809-yarn'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,352 - Skipping installation of existing package hadoop_3_0_0_0_809-yarn
    2018-02-13 07:39:34,354 - Package['hadoop_3_0_0_0_809-mapreduce'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,363 - Skipping installation of existing package hadoop_3_0_0_0_809-mapreduce
    2018-02-13 07:39:34,365 - Package['hadoop_3_0_0_0_809-hdfs'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,380 - Skipping installation of existing package hadoop_3_0_0_0_809-hdfs
    2018-02-13 07:39:34,393 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,394 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:34,394 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-yarn-resourcemanager'] {'timeout': 20}
    2018-02-13 07:39:34,421 - call returned (0, 'hadoop-yarn-resourcemanager - 3.0.0.0-809')
    2018-02-13 07:39:34,461 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,472 - Execute['umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase'] {'logoutput': True, 'user': 'root'}
    -bash: wget: command not found
    
    Command failed after 1 tries
    
    
    
    stderr: 
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 101, in <module>
        ApplicationTimelineReader().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 376, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/timelinereader.py"", line 45, in install
        hbase_service.install_hbase(env)
      File ""/var/lib/ambari-agent/cache/common-services/YARN/3.0.0.3.0/package/scripts/hbase_service.py"", line 82, in install_hbase
        Execute(hbase_download_cmd, user=""root"", logoutput=True)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
        self.env.run()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
        self.run_action(resource, action)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
        provider_action()
      File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
        tries=self.resource.tries, try_sleep=self.resource.try_sleep)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
        result = function(command, **kwargs)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
        tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
        result = _call(command, **kwargs_copy)
      File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
        raise ExecutionFailed(err_msg, code, out, err)
    resource_management.core.exceptions.ExecutionFailed: Execution of 'umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase' returned 127. -bash: wget: command not found
     stdout:
    2018-02-13 07:39:31,705 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:31,711 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:31,713 - Group['livy'] {}
    2018-02-13 07:39:31,714 - Group['spark'] {}
    2018-02-13 07:39:31,714 - Group['hdfs'] {}
    2018-02-13 07:39:31,714 - Group['zeppelin'] {}
    2018-02-13 07:39:31,714 - Group['hadoop'] {}
    2018-02-13 07:39:31,715 - Group['users'] {}
    2018-02-13 07:39:31,715 - Group['knox'] {}
    2018-02-13 07:39:31,716 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,717 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,718 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,719 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,720 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,721 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,722 - User['zeppelin'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['zeppelin', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,723 - User['livy'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['livy', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['spark'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['spark', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,724 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
    2018-02-13 07:39:31,725 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,726 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
    2018-02-13 07:39:31,727 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,728 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,729 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
    2018-02-13 07:39:31,731 - User['knox'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'knox'], 'uid': None}
    2018-02-13 07:39:31,732 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,735 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
    2018-02-13 07:39:31,744 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
    2018-02-13 07:39:31,744 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
    2018-02-13 07:39:31,746 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,748 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
    2018-02-13 07:39:31,749 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
    2018-02-13 07:39:31,759 - call returned (0, '1014')
    2018-02-13 07:39:31,760 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
    2018-02-13 07:39:31,766 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1014'] due to not_if
    2018-02-13 07:39:31,766 - Group['hdfs'] {}
    2018-02-13 07:39:31,767 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', u'hdfs']}
    2018-02-13 07:39:31,767 - FS Type: 
    2018-02-13 07:39:31,767 - Directory['/etc/hadoop'] {'mode': 0755}
    2018-02-13 07:39:31,786 - File['/usr/hdp/current/hadoop-client/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
    2018-02-13 07:39:31,787 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
    2018-02-13 07:39:31,803 - Repository['HDP-3.0-repo-1'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809', 'action': ['create'], 'components': [u'HDP', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,811 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,812 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,813 - Repository['HDP-UTILS-1.1.0.21-repo-1'] {'append_to_file': True, 'base_url': 'http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7', 'action': ['create'], 'components': [u'HDP-UTILS', 'main'], 'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0', 'repo_file_name': 'ambari-hdp-1', 'mirror_list': None}
    2018-02-13 07:39:31,816 - File['/etc/yum.repos.d/ambari-hdp-1.repo'] {'content': '[HDP-3.0-repo-1]\nname=HDP-3.0-repo-1\nbaseurl=http://s3.amazonaws.com/dev.hortonworks.com/HDP/centos7/3.x/BUILDS/3.0.0.0-809\n\npath=/\nenabled=1\ngpgcheck=0\n[HDP-UTILS-1.1.0.21-repo-1]\nname=HDP-UTILS-1.1.0.21-repo-1\nbaseurl=http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7\n\npath=/\nenabled=1\ngpgcheck=0'}
    2018-02-13 07:39:31,817 - Writing File['/etc/yum.repos.d/ambari-hdp-1.repo'] because contents don't match
    2018-02-13 07:39:31,817 - Package['unzip'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,902 - Skipping installation of existing package unzip
    2018-02-13 07:39:31,902 - Package['curl'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,912 - Skipping installation of existing package curl
    2018-02-13 07:39:31,912 - Package['hdp-select'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:31,922 - Skipping installation of existing package hdp-select
    2018-02-13 07:39:32,207 - Looking for matching packages in the following repositories: HDP-3.0-repo-1, HDP-UTILS-1.1.0.21-repo-1
    2018-02-13 07:39:34,268 - Package['hadoop_3_0_0_0_809-yarn'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,352 - Skipping installation of existing package hadoop_3_0_0_0_809-yarn
    2018-02-13 07:39:34,354 - Package['hadoop_3_0_0_0_809-mapreduce'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,363 - Skipping installation of existing package hadoop_3_0_0_0_809-mapreduce
    2018-02-13 07:39:34,365 - Package['hadoop_3_0_0_0_809-hdfs'] {'retry_on_repo_unavailability': False, 'retry_count': 5}
    2018-02-13 07:39:34,380 - Skipping installation of existing package hadoop_3_0_0_0_809-hdfs
    2018-02-13 07:39:34,393 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,394 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=None -> 3.0
    2018-02-13 07:39:34,394 - call['ambari-python-wrap /usr/bin/hdp-select status hadoop-yarn-resourcemanager'] {'timeout': 20}
    2018-02-13 07:39:34,421 - call returned (0, 'hadoop-yarn-resourcemanager - 3.0.0.0-809')
    2018-02-13 07:39:34,461 - Using hadoop conf dir: /usr/hdp/current/hadoop-client/conf
    2018-02-13 07:39:34,472 - Execute['umask 0022;wget --no-cookies --no-check-certificate http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/1.2.6/hadoop-2.7.5/hbase-1.2.6-bin.tar.gz && tar -xzf hbase-1.2.6-bin.tar.gz && rm -rf hbase-1.2.6-bin.tar.gz && rm -rf /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase && mv hbase-* /usr/hdp/3.0.0.0-809/hadoop-yarn-hbase'] {'logoutput': True, 'user': 'root'}
    -bash: wget: command not found
    
    Command failed after 1 tries
    

",pull-request-available,[],AMBARI,Bug,Major,2018-02-15 12:24:12,0
13138662,Wrong user used to execute the Spark/Livy Server service check,"{code:title=common-services/SPARK/1.2.1/package/scripts/service_check.py:36}
 livy_kinit_cmd = format(""{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal}; "")
 Execute(livy_kinit_cmd, user=params.livy_user)
{code}
Notice the Kerberos identity is for the smoke user, but the exec is for the livy user. This will replace the livy user's interactive Kerberos ticket cache.

This should be 
{code}
smoke_user_kinit_cmd = format(""{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal}; "")
Execute(smoke_user_kinit_cmd, user=params.smoke_user)
{code}
Where {{smoke_user}} is
{code}
smoke_user =  config['configurations']['cluster-env']['smokeuser']
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-15 00:32:00,7
13138538,master-key option is missing from ambari-server setup action,"{{ambari-server setup --enable-lzo-under-gpl-license}} failing with {{Unexpected AttibuteError}}

{noformat}
# ambari-server setup --enable-lzo-under-gpl-license -s
Using python /usr/bin/python
Setup ambari-server
Checking SELinux...
SELinux status is 'disabled'
Customize user account for ambari-server daemon [y/n] (n)?
Adjusting ambari-server permissions and ownership...
Checking firewall status...
Checking JDK...
Do you want to change Oracle JDK [y/n] (n)?
Check JDK version for Ambari Server...
JDK version found: 8
Minimum JDK version is 8 for Ambari. Skipping to setup different JDK for Ambari Server.
Checking GPL software agreement...
Completing setup...
Configuring database...
Enter advanced database configuration [y/n] (n)?
Configuring database...
ERROR: Unexpected AttributeError: Values instance has no attribute 'master_key'
For more info run ambari-server with -v or --verbose option
{noformat}

*Note*: This occurs after encrypting passwords in the {{ambari_server.properties}} file:
{noformat}
# ambari-server setup-security
Using python  /usr/bin/python
Security setup options...
===========================================================================
Choose one of the following options:
  [1] Enable HTTPS for Ambari server.
  [2] Encrypt passwords stored in ambari.properties file.
  [3] Setup Ambari kerberos JAAS configuration.
  [4] Setup truststore.
  [5] Import certificate to truststore.
===========================================================================
Enter choice, (1-5): 2
Please provide master key for locking the credential store:
Re-enter master key:
Do you want to persist master key. If you choose not to persist, you need to provide the Master Key while starting the ambari server as an env variable named AMBARI_SECURITY_MASTER_KEY or the start will prompt for the master key. Persist [y/n] (y)? y
Adjusting ambari-server permissions and ownership...
Ambari Server 'setup-security' completed successfully.
{noformat}

 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-14 17:09:18,7
13138476,Quick links should be grouped by namespace,Currently quick links are grouped by host. These groups in their part should be grouped by namespace (if any).,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-14 11:43:34,2
13138448,"Add Hosts is Failing with - Error while bootstrapping: Cannot run program ""/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py""","Add Hosts(UI/API) is broken and the following error is seen .
{code:java}
Error while bootstrapping:
Cannot run program ""/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py"": error=2, No such file or directory
{code}
 ",pull-request-available,[],AMBARI,Bug,Blocker,2018-02-14 10:01:31,0
13138422,Update Hadoop RPC Encryption Properties During Upgrade,"When *HDP 3.0.0* is installed, clients should have the ability to choose encrypted communication over RPC when talking to core hadoop components. Today, the properties that control this are:
 - {{core-site.xml : hadoop.rpc.protection = authentication}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication}}

The new value of {{privacy}} enables clients to choose an encrypted means of communication. By keeping {{authentication}} first, it will be taken as the default mechanism so that wire encryption is not automatically enabled by accident.

The following properties should be changed to add {{privacy}}:
 - {{core-site.xml : hadoop.rpc.protection = authentication,privacy}}
 - {{hdfs-site.xml : dfs.data.transfer.protection = authentication,privacy}}

The following are cases when this needs to be performed:
 - During Kerberization (this case is covered by AMBARI-22803)
 - During a stack upgrade to any version of *HDP 3.0.0*, they should be automatically merged

Blueprint deployment is not a scenario being covered here.",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-02-14 07:53:37,6
13138219,NullPointerException in KerberosHelperImpl.addIdentities,"If a group name or group access is missing from a kerberos descriptor ambari will throw an NPE during enable kerberos
{code:java}

13 Feb 2018 11:27:37,466  WARN [Server Action Executor Worker 39] ServerActionExecutor:471 - Task #39 failed to complete execution due to thrown exception: java.lang.NullPointerException:null
java.lang.NullPointerException
	at org.apache.ambari.server.controller.KerberosHelperImpl.addIdentities(KerberosHelperImpl.java:1601)
	at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponents(AbstractPrepareKerberosServerAction.java:177)
	at org.apache.ambari.server.serveraction.kerberos.AbstractPrepareKerberosServerAction.processServiceComponentHosts(AbstractPrepareKerberosServerAction.java:87)
	at org.apache.ambari.server.serveraction.kerberos.PrepareKerberosIdentitiesServerAction.execute(PrepareKerberosIdentitiesServerAction.java:128)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:550)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:466)
	at java.lang.Thread.run(Thread.java:748){code}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-13 15:54:33,3
13138138,Add namespace-specific layout for service summary page,"- HDFS components should be grouped by namespace
- Service summary page should have separate tab for each namespace",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-13 11:40:54,2
13138117,'Assign Slaves & Clients' page is empty when 3 or more services selected while adding multiple services,"STR:
 # In the cluster, click add service
 # Select the remaining services to be added (Pig, Knox, Logsearch, RangerKMS, Zeppelin) and click Next
 # Accept Defaults and Select Next on Assign Masters page

In the Assign Slaves and Clients page, there is an empty page.

Adding services individually does not hit this issue.

There are some error logs in the console:
{code:java}
Uncaught TypeError: Cannot read property 'get' of undefined
at app.js:38709
at Array.forEach (<anonymous>)
at app.js:38707
at Class.forEach (vendor.js:18038)
at app.js:38706
at Array.forEach (<anonymous>)
at Class.enableCheckboxesForDependentComponents (app.js:38705)
at Class.renderSlaves (app.js:38671)
at Class.render (app.js:38646)
at Class.loadStep (app.js:38569)
{code}
Calls to server made during while navigating to Assign Slaves and Clients page",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-02-13 09:27:11,4
13137750,Ambari agent kills parent process on stop,"If ambari-agent was started from bash script (for example), this script will be killed on ambari-agent stop.
Issue prevents some automatization scenarios.",pull-request-available,['ambari-agent'],AMBARI,Bug,Critical,2018-02-11 23:53:42,1
13137634,Upgrade Apache Rat to 0.12,"Apache Rat 0.12 can automatically exclude patterns from SCM ignore files ({{.gitignore}} in Ambari's case) from its check.  See RAT-171 for details.

Benefits:

# No need to duplicate the exclude information in the {{apache-rat-plugin}}'s configuration.
# Improved run time for non-clean workdir, since it can exclude everything under {{target}}.
",pull-request-available,[],AMBARI,Improvement,Major,2018-02-10 15:45:43,5
13137446,Heartbeat gets lost due to subprocess lock,"Subprocess has problem when run in multithreaded environment. As stated by developers it should not be used in such env.
As a result Ambari done multiple patches to subprocess. However still we are having multithreading problems with it.

This jira targets moving from Subprocess to Subprocess32 (a port of Python3.0 subprocess) which support multithreaded execution.",pull-request-available,[],AMBARI,Bug,Major,2018-02-09 14:08:49,0
13137432,UI Install is unable to work with 'Redhat Satellite/Spacewalk' as local repository,"The UI must allow changing the repo_id field when enabling RH Satellite. it just needs to be a text box, don't allow (trimmed) empty strings or blanks. Each repo must have an ID. Text inputs only count for redhat/centos. (Ubuntu, suse, etc don't have this).",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-02-09 12:53:15,4
13137326,SPNEGO service keytab is getting deleted upon deleting component from host,"spnego.service.keytab is getting deleted upon deleting components.

Steps to reproduce :
# Add additional ""livy"" component to some host in the cluster
# Delete added ""livy"" component
# Deletion of livy is deleting /etc/security/keytabs/spnego.service.keytab as well


The cause of this is due to an invalid check to determine if a Kerberos identity is a reference or no at 
{code:title=org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptorContainer#getIdentitiesSkipReferences}
  public List<KerberosIdentityDescriptor> getIdentitiesSkipReferences() {
    return nullToEmpty(getIdentities())
      .stream()
      .filter(identity -> !identity.getReferencedServiceName().isPresent() && identity.getName() != null && !identity.getName().startsWith(""/""))
      .collect(toList());
  }
{code}

The fixed code should be

{code:title=org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptorContainer#getIdentitiesSkipReferences}
  public List<KerberosIdentityDescriptor> getIdentitiesSkipReferences() {
    return nullToEmpty(getIdentities())
      .stream()
.filter(identity -> !identity.getReferencedServiceName().isPresent() && !identity.isReference())      .collect(toList());
  }
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-09 02:19:16,7
13137290,All Container-executor.cfg properties should be managed by ambari ui,"Currently, only few properties from Container-executor.cfg can be managed via ambari UI. 
Container-executor.cfg should be handled similar to yarn-env.sh in ambari.
This way ambari ui can manage all config properties for Container-executor.cfg",pull-request-available,[],AMBARI,Bug,Major,2018-02-08 23:06:22,6
13137277,Download client configs API failing with 500 server error,"Download client configs API failing with 500 server error
{code:java}
http://<AMBARI_SERVER>:<PORT>/api/v1/clusters/cl1/components?format=client_config_tar
http://<AMBARI_SERVER>:<PORT>/api/v1/clusters/cl1/hosts/<HOST_NAME>/host_components?format=client_config_tar

{ ""status"": 500, ""message"": ""org.apache.ambari.server.controller.spi.SystemException: Execution of \""ambari-python-wrap /var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py generate_configs /var/lib/ambari-server/data/tmp/KERBEROS_CLIENT2999359502875665328-configuration.json /var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package /var/lib/ambari-server/data/tmp/structured-out.json INFO /var/lib/ambari-server/data/tmp\"" returned 1. java.lang.Throwable: Traceback (most recent call last):\n File \""/var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py\"", line 20, in <module>\n from kerberos_common import *\n File \""/var/lib/ambari-server/resources/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_common.py\"", line 32, in <module>\n from ambari_agent import Constants\nImportError: No module named ambari_agent\n"" }


{code}",pull-request-available,['ambari-sever'],AMBARI,Bug,Major,2018-02-08 22:11:03,3
13137257,Wrong variable in KerberosKeytabDescriptor#toMap(),"{code:title=org/apache/ambari/server/state/kerberos/KerberosKeytabDescriptor.java:466}
    if (!owner.isEmpty()) {
      map.put(KEY_GROUP, group);
    }
{code}
The if statement checks wrong variable.

Should read
{code}
    if (!group.isEmpty()) {
      map.put(KEY_GROUP, group);
    }
{code}
",pull-request-available,[],AMBARI,Bug,Major,2018-02-08 20:52:17,6
13137130,"Debian stackdeploys failing with ""ambari-agent: command not found"" errors","Debian stackdeploys failing with ""ambari-agent: command not found"" errors even
though ambari-agent is installed.

From a debian cluster:

    
    
    
    root@ctr-e137-1514896590304-63273-01-000006:~# dpkg-query -l | grep ambari
    ii  ambari-agent                                  2.6.2.0-45                        amd64        Ambari Agent
    root@ctr-e137-1514896590304-63273-01-000006:~# find / -name ambari-agent
    /run/ambari-agent
    /usr/lib/ambari-agent
    /var/log/ambari-agent
    /var/lib/ambari-agent
    /var/lib/ambari-agent/bin/ambari-agent
    /etc/init.d/ambari-agent
    /etc/ambari-agent
    /grid/0/log/ambari-agent
",pull-request-available,[],AMBARI,Bug,Major,2018-02-08 11:37:11,0
13137126,Service-auto start page update of UI,See screenshot,pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-02-08 11:18:22,4
13137085,Ambari throws NPE when deleting a service,"When a config group exists with service_name = null, ambari will throw a NullPointerException after removing a service. Ambari tries to remove config groups after deleting a service which causes the NPE",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-02-08 08:14:10,3
13136968,Deactivate unsupported stack definitions in Ambari,"Deactivate unsupported stack definitions in Ambari.  This includes HDP versions before 2.6.

{noformat:title=Active Stacks}
./3.0/metainfo.xml:    <active>true</active>
./2.6/metainfo.xml:    <active>true</active>
./2.5/metainfo.xml:    <active>true</active>
./2.4/metainfo.xml:    <active>true</active>
./2.3/metainfo.xml:    <active>true</active>
{noformat}

There is no enforcement of minimum stack level during the upgrade process. The user is expected to have performed the necessary actions to meet the _documented_ minimum stack level. 

",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-02-07 21:51:34,7
13136953,[API] Updating current stack repo without GPL repos in the body does not throw any error,"# Install HDP 2.6.4.0 stack with Ambari 2.6.1
 # Update the current repository versions with the following body on ambari-2.6.1

{code:java}
PUT http://<ambari-server>:8080/api/v1/stacks/HDP/versions/2.6/repository_versions/1
{
 ""operating_systems"": [
 {
 ""OperatingSystems"": {
 ""os_type"": ""debian7"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/debian7/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian6"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""redhat6"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.2.0/"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos6"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""redhat7"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""suse11"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/suse11sp3"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""suse12"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/suse11sp3/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/suse11sp3"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""ubuntu12"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/ubuntu12/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu12"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""ubuntu14"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/ubuntu14/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu14"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 },
 {
 ""OperatingSystems"": {
 ""os_type"": ""ubuntu16"",
 ""ambari_managed_repositories"": true
 },
 ""repositories"": [
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP/ubuntu16/2.x/updates/2.6.0.3"",
 ""repo_id"": ""HDP-2.6"",
 ""repo_name"": ""HDP""
 }
 },
 {
 ""Repositories"": {
 ""base_url"": ""http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/ubuntu16"",
 ""repo_id"": ""HDP-UTILS-1.1.0.21"",
 ""repo_name"": ""HDP-UTILS""
 }
 }
 ]
 }
 ]
}
{code}
Response: 200 OK

Navigating to the UI results in GPL fields missing.

Expected: Error to be thrown when trying to update repos without the GPL repo when stack >= HDP-2.6.4.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-07 20:29:12,1
13136950,Create UpgradeCatalog270 to migrate data,"Create UpgradeCatalog270 to migrate data.

This will entail renaming \{{org.apache.ambari.server.upgrade.UpgradeCatalog300}} to \{{org.apache.ambari.server.upgrade.UpgradeCatalog270}} and making adjustments as needed.",pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-02-07 20:19:22,7
13136578,Style edits of Service Configuration Group modal,"Move and align checkboxes to the left in Select configuration Group Hosts modal.
See screenshot.",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-06 14:37:44,4
13136552,"Unable to enable hive interactive, LLAP, on our Ambari 2.5.2 managed cluster","While trying to enable the hive interactive plugin identified
that Ambari was looking in the incorrect directory when trying to locate the
users keytab.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-06 12:26:37,0
13136350,Dashboard-Metrics page style edits,See screenshots,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-05 18:35:04,4
13136308,Oracle DDL is broken at trunk,"Looks like Oracle DDL is broken
{code}
ERROR at line 1:
ORA-00904: ""CREATE_TIMESTAMP"": invalid identifier


insert into user_authentication(user_authentication_id, user_id, authentication_type, authentication_key, create_time, update_time)
*
ERROR at line 1:
ORA-02291: integrity constraint (AMBARI.FK_USER_AUTHENTICATION_USERS) violated
- parent key not found
{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-05 16:42:06,7
13136298,Add ability to MasterHostResolver to resolve by namespace at a time,The upgrade behavior which figures out the restart order for the Namenodes needs to become namespace aware to support NN federation in Ambari.,pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-02-05 16:05:48,3
13136285,Host details page: components not reconfigured after deleting their host,"# When deleting a host, after components deletion config changes not triggered, as opposed to when they removed individually.
 # When adding/deleting component config modification triggered before actual call to add/delete a component, so even if the call fails then configs would be modified",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-02-05 14:21:19,4
13136277,Log Search UI: move Capture button to top menu,"- Move 'Capture' button from filters panel to top menu
- Buttons order: Undo, Redo, History, Filter, Capture, Refresh",pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-05 13:59:09,2
13136237,Add Ambari admin/pw CLI options for setup-ldap tool,"Since we use Amabri's REST API in the _setup-ldap_ tool to persists its outcome into the DB we require the end-user to type his/her Ambari username and password during the user input phase.
 This makes automated integration tests very hard to implement.

The solution is to add two new CLI option names for _setup-ldap_ (similarly to the _sync-ldap_ tool):
{code:java}
--ldap-setup-admin-name
--ldap-setup-admin-password{code}",ldap pull-request-available,[],AMBARI,Improvement,Critical,2018-02-05 10:16:48,6
13136235,Log Search UI: implement filter by username for access logs,User should be able to filter access logs by username,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-05 10:14:26,2
13135824,Not able to register new HDP version after upgrading to Ambari2.6.1,"Steps to reproduce the issue

1. Install Ambari2.5.x and install HDP on that.

2. Now Upgrade ambari to 2.6.1 version.

3. Now try to register HDP version with VDF file - Save button is not enabled (also UI does not load repo URLs and)

4. In JS it fails with below error

Cannot read property 'gpl.license.accepted' of undefined
What is expected: UI should properly show the message that gpl license needs to be enabled. also Upgrade should take care of adding this property if it is needed.

Workaround for this is : add ""gpl.license.accepted=true"" to ambari.proeprties and then restart should fix the problem",pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-02-02 17:57:18,0
13135750,Support old option names in setup-ldap tool,"While working on AMBARI-22797 we modified the following option names for the setup-ldap tool in AMBARI server:
 # {{ldap-url}} became {{ldap-primary-host}} and {{ldap-primary-port}}
 # {{ldap-secondary-url}} became {{ldap-secondary-host}} and {{ldap-secondary-port}}

To support backward compatibility we need to support old option names too.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-02 11:48:46,6
13135513,Update kerberos-env/admin_server_host description to specify KCD admin host must be a FQDN,"Update kerberos-env/admin_server_host description to specify KCD admin host must be a FQDN.

The following needs to be changed in
* {{common-services/KERBEROS/1.10.3-10/configuration/kerberos-env.xml:168}}
* {{common-services/KERBEROS/1.10.3-30/configuration/kerberos-env.xml:168}}
{code}
The IP address or FQDN for the KDC Kerberos administrative host. Optionally a port number may be included.
{code}
To 
{code}
The FQDN for the KDC Kerberos administrative host. Optionally a port number may be included.
{code}
",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-02-01 18:13:12,6
13135482,Log Search UI: implement 'History' functionality,History of user's actions should be available for viewing and undoing/redoing.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-02-01 16:36:40,2
13135406,Fix TestAlertSchedulerHandler.py and TestAlerts.py,"How the configs and alerts definitions are consumed by the alerts framework
has completely changed on branch-3.0-perf so most alert-related tests require
rewriting.

",pull-request-available,[],AMBARI,Bug,Major,2018-02-01 12:42:01,0
13135165,LDAP configuration is not reloaded in Guice,"+*Steps to reproduce:*+

run _ambari-server setup-ldap_ and set primary host:port to ""myHost:1""

run _ambari-server sync-ldap --all_

+*Actual results:*+

the synch process will fail due to a CommunicationException saying that *localhost:33389* is not reachable.

+*Expected results:*+

the synch process will fail due to a CommunicationException saying that *myHost:1* is not reachable.

+*Additional information:*+

in ambari-server.log I saw that AmbariConfigurationChangedEvent has been triggered and processed when setup-ldap updated the DB (see org.apache.ambari.server.ldap.service.AmbariLdapConfigurationProvider; however the get() method does not seem to be called when needed; it's maybe a Guice injection scope issue)",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-31 18:34:17,6
13135012,LDAP sync fails with 'LDAP is not configured' error after configuring LDAP,"LDAP sync fails with 'LDAP is not configured' error after configuring LDAP.

*Steps to Reproduce*
 # ambari-server setup-ldap
 # ambari-server sync-ldap --all

{noformat:title=Example}
[root@c6404 ~]# ambari-server sync-ldap --all
Using python  /usr/bin/python
Syncing with LDAP...
ERROR: Exiting with exit code 1.
REASON: LDAP is not configured. Run 'ambari-server setup-ldap' first.
{noformat}
*Cause*
The ambari-server script is looking for the value of {{ambari.ldap.authentication.enabled}} (from the the {{ambari.properties}} file. The value is expected to be ""true"". Since {{ambari.ldap.authentication.enabled}} is not being set in the ambari.properties file, the check is failing and thus, the unexpected error.

See {{ambari-server/src/main/python/ambari_server/setupSecurity.py:298}}.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-31 08:14:47,6
13134848,Long cannot be cast to String error when changing a user's password,"Long cannot be cast to String error when changing a user's password:
{noformat}
30 Jan 2018 18:21:11,308 ERROR [ambari-client-thread-38] AbstractResourceProvider:353 - Caught AmbariException when modifying a resource
org.apache.ambari.server.AmbariException: java.lang.Long cannot be cast to java.lang.String
at org.apache.ambari.server.controller.internal.UserResourceProvider.addOrUpdateLocalAuthenticationSource(UserResourceProvider.java:559)
at org.apache.ambari.server.controller.internal.UserResourceProvider.updateUsers(UserResourceProvider.java:486)
at org.apache.ambari.server.controller.internal.UserResourceProvider.access$200(UserResourceProvider.java:69)
at org.apache.ambari.server.controller.internal.UserResourceProvider$3.invoke(UserResourceProvider.java:264)
at org.apache.ambari.server.controller.internal.UserResourceProvider$3.invoke(UserResourceProvider.java:261)
at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:465)
at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:346)
at org.apache.ambari.server.controller.internal.UserResourceProvider.updateResources(UserResourceProvider.java:261)
at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:317)
...
{noformat}

*Steps to reproduce*
 # Create a {{LOCAL}} user account (using either the Ambari UI or REST API)
{noformat}
POST /api/v1/users
{noformat}
{code:title=Payload}
{ 
  ""Users"" : {
    ""user_name"" : ""myuser"",
    ""password"" : ""hadoop""
  }
}
{code}
 # Change the user's password (using either the Ambari UI or REST API via the users entry point)
{noformat}
PUT /api/v1/users/myuser
{noformat}
{code:title=Payload}
{ 
  ""Users"" : {
    ""old_password"" : ""hadoop""
    ""password"" : ""hadoop1234""
  }
}
{code}
{code:title=Response}
{
  ""status"" : 500,
  ""message"" : ""org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: java.lang.Long cannot be cast to java.lang.String""
}
{code}

*Cause*
When building the internal request to set the user's password via the UserAuthenticationSource resource provider, the authentication source key is set as a {{Long}}. The UserAuthenticationSource resource provider expects this value to be a {{String}}.

*Solution*
The User resource provider should set the {{AuthenticationSourceInfo/source_id}} as a {{String}} value.

",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-30 18:37:45,7
13134830,Missing user_authentication_id_seq in ambari_sequences table upon upgrade,"Missing user_authentication_id_seq in ambari_sequences table upon upgrade.

The missing sequence should be added using a value greater than the largest {{user_authentication.user_authentication_id}}. For example:
{code:sql}
insert into ambari_sequences (sequence_name, sequence_value) 
  select 'user_authentication_id_seq', max(user_authentication_id) + 1 from user_authentication;{code}",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-30 17:44:02,6
13134730,Ambari makes unrelated changes to zookeeper quorum config in all services when delete host action fails,"*STR:*
# Delete a host from ambari-web ui that has only slave components like datanode, nodemanager, etc 
# This action will make API calls to delete host components and then delete host api
# Somehow environment should be configured in a way such that delete host component API succeeds but delete host api errors out

*Expected Result:* Host should still exist with no components. NO config changes should happen as none were previewed and slave component deletion generally does not result in config changes

*Actual Result:* ZK quorum related config changes happen in all services",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-01-30 11:52:20,4
13134591,Disable consecutive authentication failure account lockout feature by default,"Disable consecutive authentication failure account lockout feature by default. This feature locks an account after a configured number of failed authentication attempts. By defaulting {{authentication.local.max.failures}} to {{0}}, this feature will be disabled by default.

If enabled, a user account may be locked out due to number of authentication failures. There is a REST API call to unlock the user; however a user interface change will not be available until Ambari 3.x.",authentication pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-29 22:35:50,7
13134576,Blueprint cluster creation using manually installed mpacks,Deploy a cluster via blueprint based on one or more MPacks that have already been manually installed.,blueprints pull-request-available,['ambari-server'],AMBARI,Task,Major,2018-01-29 21:49:26,5
13134510,Ranger fails to install (branch-3.0-perf),"jdbc related properties like:
{noformat}
previous_custom_mysql_jdbc_name, custom_mysql_jdbc_name, custom_postgres_jdbc_name, previous_custom_postgres_jdbc_name
{noformat}
also
{noformat}
gpl_license_accepted
{noformat}
are missing.

We should sent them in metadata/ambariLevelParams in order for Ranger andother db dependent services to work properly.",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-29 17:52:55,1
13134465,Add new state for Not Available data in Heatmap widget,"# We should add a new state ""Data Not Available""
# Roundup metric values based on data type",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-29 15:04:43,4
13134432,Error while installing Hive 2.1.0.3.0 from Ambari 3.0.0,"The following error occurred in Ambari 3.0.0 when I tried to install Hive from HDP 3.0 (v2.1.0.3.0):

 
{code:java}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 167, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 1565, in recommendConfigurations
    serviceAdvisor.getServiceConfigurationRecommendations(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/common-services/HIVE/2.1.0.3.0/service_advisor.py"", line 126, in getServiceConfigurationRecommendations
    recommender.recommendHiveConfigurationsFromHDP21(configurations, clusterData, services, hosts)
  File ""/var/lib/ambari-server/resources/common-services/HIVE/2.1.0.3.0/service_advisor.py"", line 209, in recommendHiveConfigurationsFromHDP21
    meta = self.get_service_component_meta(""HIVE"", ""WEBHCAT_SERVER"", services)
AttributeError: 'HiveRecommender' object has no attribute 'get_service_component_meta'
{code}
 ",pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-29 12:21:03,6
13134423,Agent commands hang even after freeing up disk space on the host,"*STR*
# Install a cluster with Ambari-2.6.2 and HDP-2.6.4.0
# Go the host (say host1) running Nimbus component and restart Nimbus
# Fill up the disk space on host1 (in my test, the disk space was filled up on the host running Nimbus component)
# Try to restart Nimbus. Nimbus restart expectedly fails with error:
{code}
Caught an exception while executing custom service command: <type 'exceptions.IOError'>: [Errno 28] No space left on device; [Errno 28] No space left on device
{code}
# Now free up the disk space on host1 and try to restart Nimbus

 

*Result*
Nimbus restart command hangs and eventually times out

Looks like the issue is because the action queue is unable to create new command for Nimbus restart.",pull-request-available system_test,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2018-01-29 11:33:52,0
13134402,Hive should handle a customized Zookeeper service principal name,"Hive should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} should be added to {{HIVE_OPTS}}
 * Other changes may be needed to handle Hive-related tool and JDBC/ODBC implementations",kerberos pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-01-29 09:46:32,3
13134115,First prereq not displaying for Free IPA method in Enable Kerberos Wizard,"First prereq not displaying for Free IPA method in Enable Kerberos Wizard.

 !image-2018-01-26-16-32-14-802.png! 

This is due to a typo at 
{code:title=ambari-web/app/controllers/main/admin/kerberos/step1_controller.js:88}
          dsplayText: Em.I18n.t('admin.kerberos.wizard.step1.option.ipa.condition.1'),
{code}

The code should be 
{code:title=ambari-web/app/controllers/main/admin/kerberos/step1_controller.js:88}
          displayText: Em.I18n.t('admin.kerberos.wizard.step1.option.ipa.condition.1')
{code}

Notice ""dsplayText"" vs ""displayText"".
",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-26 21:33:53,7
13134015,NFS Gateway is not logging at the correct location,"/grid/0/log/**hdfs//hadoop-hdfs-root-
nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out**

    
    
    
    [root@ctr-e137-1514896590304-8236-01-000002 ~]# cd /grid/0/log/hdfs/
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# ls -ltr
    total 500
    -rw------- 1 root root        0 Jan  9 22:52 hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out
    -rw------- 1 hdfs hadoop      0 Jan  9 22:52 SecurityAuth.audit
    -rw-r--r-- 1 root root      714 Jan  9 22:52 privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out
    -rw------- 1 root root      117 Jan  9 23:00 privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err
    drwxrwxr-x 2 root hadoop   4096 Jan  9 23:54 root
    drwxr-xr-x 2 hdfs hadoop   4096 Jan  9 23:59 hdfs
    -rw------- 1 hdfs hadoop 493804 Jan 10 00:58 hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# ps -ef| grep nfs
    root       10637       1  0 Jan09 ?        00:00:00 jsvc.exec -Dproc_nfs3 -outfile /grid/0/log/hdfs//hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out -errfile /grid/0/log/hdfs//privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err -pidfile /var/run/hadoop//hadoop-hdfs-root-nfs3.pid -nodetach -user hdfs -cp /usr/hdp/3.0.0.0-691/hadoop/conf:/usr/hdp/3.0.0.0-691/hadoop/lib/*:/usr/hdp/3.0.0.0-691/hadoop/.//*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/./:/usr/hdp/3.0.0.0-691/hadoop-hdfs/lib/*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/.//*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/lib/*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/.//*:/usr/hdp/3.0.0.0-691/hadoop-yarn/./:/usr/hdp/3.0.0.0-691/hadoop-yarn/lib/*:/usr/hdp/3.0.0.0-691/hadoop-yarn/.//*:/usr/hdp/3.0.0.0-691/tez/*:/usr/hdp/3.0.0.0-691/tez/lib/*:/usr/hdp/3.0.0.0-691/tez/conf:/usr/hdp/3.0.0.0-691/tez/doc:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-2.8-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib:/usr/hdp/3.0.0.0-691/tez/man:/usr/hdp/3.0.0.0-691/tez/tez-api-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-common-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-dag-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-examples-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-ext-service-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-history-parser-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-javadoc-tools-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-job-analyzer-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-mapreduce-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-internals-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-library-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-cache-plugin-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-acls-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-fs-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/ui:/usr/hdp/3.0.0.0-691/tez/lib/RoaringBitmap-0.4.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/async-http-client-1.8.16.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-cli-1.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-codec-1.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections-3.2.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections4-4.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-io-2.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-lang-2.6.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-math3-3.1.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/guava-11.0.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-hdfs-client-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-common-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-core-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-client-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-json-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jettison-1.3.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jsr305-3.0.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/metrics-core-3.1.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/netty-3.6.2.Final.jar:/usr/hdp/3.0.0.0-691/tez/lib/protobuf-java-2.5.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/servlet-api-2.5.jar:/usr/hdp/3.0.0.0-691/tez/lib/slf4j-api-1.7.10.jar:/usr/hdp/3.0.0.0-691/tez/lib/tez.tar.gz -Dhdp.version=3.0.0.0-691 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.0.0.0-691 -Xmx1024m -Dhadoop.security.logger=ERROR,DRFAS -jvm server -Dyarn.log.dir=/grid/0/log/hdfs/ -Dyarn.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dyarn.home.dir=/usr/hdp/3.0.0.0-691/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=:/usr/hdp/3.0.0.0-691/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.0.0.0-691/hadoop/lib/native -Dhadoop.log.dir=/grid/0/log/hdfs/ -Dhadoop.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dhadoop.home.dir=/usr/hdp/3.0.0.0-691/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
    hdfs       25658   10637  0 Jan09 ?        00:06:37 jsvc.exec -Dproc_nfs3 -outfile /grid/0/log/hdfs//hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.out -errfile /grid/0/log/hdfs//privileged-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.err -pidfile /var/run/hadoop//hadoop-hdfs-root-nfs3.pid -nodetach -user hdfs -cp /usr/hdp/3.0.0.0-691/hadoop/conf:/usr/hdp/3.0.0.0-691/hadoop/lib/*:/usr/hdp/3.0.0.0-691/hadoop/.//*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/./:/usr/hdp/3.0.0.0-691/hadoop-hdfs/lib/*:/usr/hdp/3.0.0.0-691/hadoop-hdfs/.//*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/lib/*:/usr/hdp/3.0.0.0-691/hadoop-mapreduce/.//*:/usr/hdp/3.0.0.0-691/hadoop-yarn/./:/usr/hdp/3.0.0.0-691/hadoop-yarn/lib/*:/usr/hdp/3.0.0.0-691/hadoop-yarn/.//*:/usr/hdp/3.0.0.0-691/tez/*:/usr/hdp/3.0.0.0-691/tez/lib/*:/usr/hdp/3.0.0.0-691/tez/conf:/usr/hdp/3.0.0.0-691/tez/doc:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/hadoop-shim-2.8-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib:/usr/hdp/3.0.0.0-691/tez/man:/usr/hdp/3.0.0.0-691/tez/tez-api-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-common-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-dag-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-examples-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-ext-service-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-history-parser-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-javadoc-tools-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-job-analyzer-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-mapreduce-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-internals-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-runtime-library-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-tests-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-cache-plugin-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-acls-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/tez-yarn-timeline-history-with-fs-0.9.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/ui:/usr/hdp/3.0.0.0-691/tez/lib/RoaringBitmap-0.4.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/async-http-client-1.8.16.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-cli-1.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-codec-1.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections-3.2.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-collections4-4.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-io-2.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-lang-2.6.jar:/usr/hdp/3.0.0.0-691/tez/lib/commons-math3-3.1.1.jar:/usr/hdp/3.0.0.0-691/tez/lib/guava-11.0.2.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-hdfs-client-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-common-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-mapreduce-client-core-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/hadoop-yarn-server-timeline-pluginstorage-3.0.0.3.0.0.0-691.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-client-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jersey-json-1.9.jar:/usr/hdp/3.0.0.0-691/tez/lib/jettison-1.3.4.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jetty-util-6.1.26.hwx.jar:/usr/hdp/3.0.0.0-691/tez/lib/jsr305-3.0.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/metrics-core-3.1.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/netty-3.6.2.Final.jar:/usr/hdp/3.0.0.0-691/tez/lib/protobuf-java-2.5.0.jar:/usr/hdp/3.0.0.0-691/tez/lib/servlet-api-2.5.jar:/usr/hdp/3.0.0.0-691/tez/lib/slf4j-api-1.7.10.jar:/usr/hdp/3.0.0.0-691/tez/lib/tez.tar.gz -Dhdp.version=3.0.0.0-691 -Djava.net.preferIPv4Stack=true -Dhdp.version=3.0.0.0-691 -Xmx1024m -Dhadoop.security.logger=ERROR,DRFAS -jvm server -Dyarn.log.dir=/grid/0/log/hdfs/ -Dyarn.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dyarn.home.dir=/usr/hdp/3.0.0.0-691/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=:/usr/hdp/3.0.0.0-691/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.0.0.0-691/hadoop/lib/native -Dhadoop.log.dir=/grid/0/log/hdfs/ -Dhadoop.log.file=hadoop-hdfs-root-nfs3-ctr-e137-1514896590304-8236-01-000002.hwx.site.log -Dhadoop.home.dir=/usr/hdp/3.0.0.0-691/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
    root      449951  449877  0 21:36 pts/0    00:00:00 grep --color=auto nfs
    [root@ctr-e137-1514896590304-8236-01-000002 hdfs]# 
    


",pull-request-available,[],AMBARI,Bug,Major,2018-01-26 13:49:12,0
13133985,Host Details page style fixes,"# Remove back button
# Use a table to display components
# Restart component notification should be at page level",pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-26 11:21:07,4
13133803,HBase should handle a customized Zookeeper service principal name,"HBase should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * Add {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} to {{HBASE_OPTS}} in {{hbase-env/template}}",kerberos pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-01-25 19:28:41,6
13133401,Standardize precision when expressing durations,"Duration should have the following format ""1d 3h 46m"".",pull-request-available,['ambari-web'],AMBARI,Task,Major,2018-01-24 15:17:21,4
13133340,Fix TestRecoveryManager ClusterConfigurationCache and a bunch of small tests.,"Fix the below tests on branch-3.0-perf:

    
    
    TestClusterConfigurationCache.py
    TestFileCache.py
    TestRecoveryManager.py
    TestScript.py
    

",pull-request-available,[],AMBARI,Bug,Major,2018-01-24 11:28:57,0
13133133,Log Search UI: implement log level filter,User should be able to choose the log levels visible to log feeder.,pull-request-available,['ambari-logsearch'],AMBARI,Task,Major,2018-01-23 17:46:31,2
13133045,Ambari-agent puts Python scripts in 2.6 directory on OSes that use python 2.7.x by default,"User needs to upgrade all OSes for security purposes, and will need to remove all directories related to Python2.6 due to
TLS violations, they need this ASAP to test, as they have a hard deadline.  
OS: Red Hat Enterprise Linux Server release 7.4  
JDK version: 1.8.0_144  

",pull-request-available,[],AMBARI,Bug,Major,2018-01-23 13:26:55,0
13132910,db-purge-history operation fails with large DB size in postgres.,"When the ambari DB size is too large (around 1+ GB) then the db-purge-history may fail with the postgres error Tried to send an out-of-range integer as a 2-byte value

 

Following error trace is from Ambari 2.5.2 however the same might cause in higher version as well.

{code}
Internal Exception: org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
Error Code: 0
Call: SELECT DISTINCT host_task_id FROM topology_logical_task WHERE (physical_task_id IN (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,
....
....
.....
......
?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?))

Query: ReportQuery(name=""TopologyLogicalTaskEntity.findHostTaskIdsByPhysicalTaskIds"" referenceClass=TopologyLogicalTaskEntity sql=""SELECT DISTINCT host_task_id FROM topology_logical_task WHERE (physical_task_id IN ?)"")
 at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1620)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:676)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeCall(DatabaseAccessor.java:560)
 at org.eclipse.persistence.internal.sessions.AbstractSession.basicExecuteCall(AbstractSession.java:2055)
 at org.eclipse.persistence.sessions.server.ServerSession.executeCall(ServerSession.java:570)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:242)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeCall(DatasourceCallQueryMechanism.java:228)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.executeSelectCall(DatasourceCallQueryMechanism.java:299)
 at org.eclipse.persistence.internal.queries.DatasourceCallQueryMechanism.selectAllRows(DatasourceCallQueryMechanism.java:694)
 at org.eclipse.persistence.internal.queries.ExpressionQueryMechanism.selectAllRowsFromTable(ExpressionQueryMechanism.java:2740)
 at org.eclipse.persistence.internal.queries.ExpressionQueryMechanism.selectAllReportQueryRows(ExpressionQueryMechanism.java:2677)
 at org.eclipse.persistence.queries.ReportQuery.executeDatabaseQuery(ReportQuery.java:852)
 at org.eclipse.persistence.queries.DatabaseQuery.execute(DatabaseQuery.java:904)
 at org.eclipse.persistence.queries.ObjectLevelReadQuery.execute(ObjectLevelReadQuery.java:1134)
 at org.eclipse.persistence.queries.ReadAllQuery.execute(ReadAllQuery.java:460)
 at org.eclipse.persistence.queries.ObjectLevelReadQuery.executeInUnitOfWork(ObjectLevelReadQuery.java:1222)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.internalExecuteQuery(UnitOfWorkImpl.java:2896)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1857)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.retryQuery(AbstractSession.java:1927)
 at org.eclipse.persistence.sessions.server.ClientSession.retryQuery(ClientSession.java:694)
 at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.retryQuery(UnitOfWorkImpl.java:5536)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1893)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1839)
 at org.eclipse.persistence.internal.sessions.AbstractSession.executeQuery(AbstractSession.java:1804)
 at org.eclipse.persistence.internal.jpa.QueryImpl.executeReadQuery(QueryImpl.java:258)
 at org.eclipse.persistence.internal.jpa.QueryImpl.getResultList(QueryImpl.java:473)
 at org.apache.ambari.server.orm.dao.DaoUtils.selectList(DaoUtils.java:60)
 at org.apache.ambari.server.orm.dao.TopologyLogicalTaskDAO.findHostTaskIdsByPhysicalTaskIds(TopologyLogicalTaskDAO.java:56)
 at org.apache.ambari.server.orm.AmbariLocalSessionInterceptor.invoke(AmbariLocalSessionInterceptor.java:53)
 at org.apache.ambari.server.orm.dao.RequestDAO.cleanup(RequestDAO.java:403)
 at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:128)
 at org.apache.ambari.server.cleanup.CleanupServiceImpl.cleanup(CleanupServiceImpl.java:82)
 at org.apache.ambari.server.cleanup.CleanupDriver.main(CleanupDriver.java:89)
Caused by: org.postgresql.util.PSQLException: An I/O error occurred while sending to the backend.
 at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:281)
 at org.postgresql.jdbc2.AbstractJdbc2Statement.execute(AbstractJdbc2Statement.java:559)
 at org.postgresql.jdbc2.AbstractJdbc2Statement.executeWithFlags(AbstractJdbc2Statement.java:417)
 at org.postgresql.jdbc2.AbstractJdbc2Statement.executeQuery(AbstractJdbc2Statement.java:302)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeSelect(DatabaseAccessor.java:1009)
 at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:644)
... 43 more
Caused by: java.io.IOException: Tried to send an out-of-range integer as a 2-byte value: 43222
 at org.postgresql.core.PGStream.SendInteger2(PGStream.java:196)
 at org.postgresql.core.v3.QueryExecutorImpl.sendParse(QueryExecutorImpl.java:1242)
 at org.postgresql.core.v3.QueryExecutorImpl.sendOneQuery(QueryExecutorImpl.java:1547)
 at org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1100)
 at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:253)
... 48 more
{code}


Looks like Postgres side limitation of around 32k parameters. However Ambari can send the large number of parameters in batch instead of sending it as bunch.",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-23 02:16:32,6
13132626,Yarn/MR2 should handle a customized Zookeeper service principal name,"Yarn and MapReduce2 should handle a customized Zookeeper service principal name.

Currently this is not supported due to hardcoded and implicit values expecting the Zookeeper service principal name to be {{zookeeper/_HOST}} as opposed to something like {{zookeeper-mycluster/_HOST}}.

The following changes need to be made:
 * common-services/YARN/2.1.0.2.0/package/scripts/params_linux.py must be changed to not hardcode the Zookeeper principal name in
{code:java}
m_security_opts = format('-Dzookeeper.sasl.client=true -Dzookeeper.sasl.client.username=zookeeper -Djava.security.auth.login.config={yarn_jaas_file} -Dzookeeper.sasl.clientconfig=Client')
{code}

 * {{-Dzookeeper.sasl.client.username=<ZOOKEEPER_PRINCPAL_NAME>}} should be added to {{YARN_OPTS}}",kerberos pull-request-available,['ambari-server'],AMBARI,Improvement,Critical,2018-01-22 09:55:12,6
13131970,Fix checkstyle error in UpgradeCatalog300Test,"Fix checkstyle error in UpgradeCatalog300Test 
{noformat}
[ERROR] .../ambari/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog300Test.java:20: Using the '.*' form of import should be avoided - org.apache.ambari.server.upgrade.UpgradeCatalog300.*. [AvoidStarImport]
{noformat}",pull-request-available,[],AMBARI,Bug,Major,2018-01-18 18:46:46,7
13131874,Rewrite TestActionQueue,"Because of branch-3.0-perf changes ActionQueue had some big chunks completely
rewritten.  
Which causes all TestActionQueue unit tests to fail.

For these changes the TestActionQueue tests have to be rewritten as well. Some
deleted due to already non-existant functionality like the one related to
status commands execution.

",pull-request-available,[],AMBARI,Bug,Major,2018-01-18 13:16:28,0
13131695,Blueprints do not handle some failures properly,"Failures in the cluster configuration task and topology host tasks during blueprint cluster deployment or upscaling are not visible via request status. The logical request stays PENDING even after Ambari Server gave up retrying. Both the fact that it no longer makes progress and any reason of the failure can be seen only in {{ambari-server.log}}.

Some ways to reproduce (all via blueprints):
 * Create cluster with ZooKeeper and HDFS, but omit the NAMENODE component
 * Create a secure cluster with wrong Kerberos credentials
 * Create a secure cluster without storing Kerberos credentials, restart Ambari Server, add a new node",blueprints pull-request-available,['ambari-server'],AMBARI,Bug,Critical,2018-01-17 20:46:30,5
13131577,Configuration overwritten during Add Host/Add Service in Kerberized Cluster,"There are side effects of Add Host/Add Service in kerberized clusters that have common properties customized. In this situation the customer installs HDP, kerberizes the cluster, makes their configuration property customizations for their environment, and then after that if they add a host or service their configuration customizations are overwritten.

Here are examples of the properties that are commonly customized and have been known to be overwritten during add host/service:
* yarn.admin.acl
* yarn.resourcemanager.proxy-user-privileges.enabled
* yarn.timeline-service.http-authentication.cookie.domain
* yarn.timeline-service.http-authentication.signature.secret.file
* capacity-scheduler.xml configuration
* auth_to_local customizations

Note: This may also occur during the ""Regenerate Keytabs"" operation. 

 ",kerberos,['ambari-server'],AMBARI,Bug,Major,2018-01-17 14:00:25,7
13131329,Role authorization AMBARI.MANAGE_CONFIGURATION is not added to AMBARI.ADMINISTRATOR role during Ambari upgrade,"Role authorization \{{AMBARI.MANAGE_CONFIGURATION}} is not added to \{{AMBARI.ADMINISTRATOR}} role during Ambari upgrade. This privilege needs to be added to the AMBARI.ADMINISTRATOR role upon upgrade.

Note: A fresh install of Ambari contains the role authorization association.",pull-request-available,['ambari-sever'],AMBARI,Bug,Critical,2018-01-16 19:51:43,7
13131265,ambari-server setup-ldap should set configurations in the Ambari database,"{{ambari-server setup-ldap}} should set configurations in the Ambari database rather than the ambari.properties file.

Similar to the following, but updated to collect the relevant data for Ambari 3.0.0:
{noformat}
# ambari-server setup-ldap
Using python  /usr/bin/python
Setting up LDAP properties...
Primary URL* {host:port} : c6409.ambari.apache.org
Invalid characters in the input!
Primary URL* {host:port} : c6409.ambari.apache.org:363
Secondary URL {host:port} :
Use SSL* [true/false] (false):
User object class* (posixAccount):
User name attribute* (uid):
Group object class* (posixGroup):
Group name attribute* (cn):
Group member attribute* (memberUid):
Distinguished name attribute* (dn):
...{noformat}",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-16 15:22:42,6
13131063,Unsightly artifacts during Login,See attached.,pull-request-available,['ambari-web'],AMBARI,Bug,Major,2018-01-15 16:03:55,4
13131035,Ambari should setup tcp preference in krb5.conf,"Ambari template should conditionally force to TCP by setting the following in the krb5.conf template if configured to do so:
{code:java}
{%- if force_tcp %}
  udp_preference_limit = 1
{%- endif -%}{code}",kerberos pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-15 14:09:43,6
13131023,Login page lost Ambari branding,"See attached.

1. There's no Ambari branding on trunk. There's nothing that says this is a login page for Ambari.
2. The spacing is odd.

See attached for trunk vs 2.6.1 comparison.",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2018-01-15 13:23:00,4
13130991,Fix Namenode alerts broken due to enabling federation,"Namnode alerts need to be fixed.

<http://104.196.73.142:8080/#/main/alerts>

",pull-request-available,[],AMBARI,Bug,Major,2018-01-15 10:57:21,0
13130968,Cannot scale cluster if Ambari Server restarted since blueprint cluster creation,"STR:
# Create cluster using blueprint
# Restart Ambari Server
# Install Ambari Agent on new host and register with server
# Add the new host via API request
# Start Ambari Agent on the new host

Result: Ambari Server accepts the scale request, but does not proceed to install/start components on the new host

The problem is that AMBARI-22012 fixed a timing/ordering issue by introducing an executor, which is started on ""cluster configured"" event during blueprint cluster creation. If Ambari Server is restarted afterwards, the executor will stay stopped, hence tasks for scale requests are never executed.",blueprints pull-request-available,['ambari-server'],AMBARI,Bug,Blocker,2018-01-15 09:24:34,5
13130549,ambari-web unit test is failing at apache ambari jenkins job,"{code}
Error loading resource file:///api/v1/clusters/c1/upgrades?fields=Upgrade&_=1515682880321 (203). Details: Error opening /api/v1/clusters/c1/upgrades: No such file or directory

  30514 passing (31s)
  157 pending
  1 failing

  1) Ambari Web Unit tests test/utils/date/timezone_test timezoneUtils #detectUserTimezone Detect UTC+1:
     expected '0-60|Africa' to include '0-60|Atlantic'
{code}",pull-request-available,['ambari-web'],AMBARI,Bug,Blocker,2018-01-12 15:25:25,4
13130538,Handle configs update in the middle of request (RU),"Async events should support RU scenario.
Main concern: RU process involves server execution commands which may change configs in the middle of request processing, so all new tasks should be executed with new configs.
Currently this is achieved by mutating config with tag used on request construction.

We may achieve similar result by implementing upgrade mode (with flag or by analyzing upgrades in progress similar to UI) and when it is enabled intercept execution commands events and extend them with actual configs just before sending to message broker.

In this case we can also consider to force agent refresh configs when RU finishes.

Current upgrade process relies on disabling auto-start of components prior to upgrade and doesn't care about status commands (as their configs usually don't change dramatically)",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-12 14:09:32,1
13130496,Ambari loads ambari.properties using ISO 8859-1 encoding,"Ambari Server loads {{ambari.properties}} using {{Properties.load(InputStream)}}, which [uses ISO 8859-1 character encoding|https://docs.oracle.com/javase/8/docs/api/java/util/Properties.html#load-java.io.InputStream-].  This causes a problem for non-Latin1 characters that can occur in several properties, eg. in any directory or file name.

STR:

# Install and setup Ambari Server
# Update {{ambari.properties}} changing {{server.jdbc.user.passwd}} to {{/etc/ambari-server/conf/password.őőő}}
# {{mv -iv /etc/ambari-server/conf/password.dat /etc/ambari-server/conf/password.őőő}}
# Try to start Ambari Server

Result:

{noformat}
...
ERROR: Exiting with exit code 1.
REASON: Database check failed to complete. Please check /var/log/ambari-server/ambari-server.log and /var/log/ambari-server/ambari-server-check-database.log for more information.
{noformat}

{noformat:title=/var/log/ambari-server/ambari-server-check-database.log}
...
FileNotFoundException: File '/etc/ambari-server/conf/password.ÅÅÅ' does not exist
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-12 11:16:01,5
13129560,Show OneFS JMX metrics on the UI,"There are some metrics like a (NameNode uptime/StartTime and HDFS disk usage/Capacity*) which cannot be shown in a widget because widgets only support AMS based metrics.
Normally these metrics are shown in a service specific dashboard which is hardcoded in the ui (summary.js/serviceCustomViewsMap).

{code}
serviceCustomViewsMap: function() {
    return {
      HBASE: App.MainDashboardServiceHbaseView,
      HDFS: App.MainDashboardServiceHdfsView,
      STORM: App.MainDashboardServiceStormView,
      YARN: App.MainDashboardServiceYARNView,
      RANGER: App.MainDashboardServiceRangerView,
      FLUME: App.MainDashboardServiceFlumeView,
      HIVE:  App.MainDashboardServiceHiveView
    }
  }.property('serviceName'),
{code}",pull-request-available,['ambari-server'],AMBARI,Improvement,Major,2018-01-09 15:33:38,3
13129538,Fix existing unit tests after STOMP protocol implementation,Fix failing unit tests,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2018-01-09 14:22:21,1
13129251,"Loosely store users authenticating from remote sources (LDAP, PAM, etc)","Loosely store users authenticating from remote sources (LDAP, PAM, etc) such that minimal information is store in the Ambari database, relying on information from the remote sources to provide details like group membership and username.

Group membership, consecutive authentication failure count, and etc... should not be stored in the Ambari database for user accounts that are not authenticated locally. 

To do this, convert the {{users}} table into the following tables:

*user*
* user_id  (primary key)
* principal_id (foreign key to adminprincipal table)
* user_name
* authentication_source (LOCAL, LDAP, PAM)
* active_widget_layouts
* create_time

*local_user_authentication*
* user_id (foreign key to user table)
* password
* active
* consecutive_failures
* create_time
* update_time
",authentication,['ambari-server'],AMBARI,Bug,Major,2018-01-08 14:01:16,7
13128824,alert_definitions topic doesn't emit any events to client,"/alert_definitions topic doesn't emit any events to the client when definition properties are modified.
",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2018-01-05 15:03:58,1
13125821,"Upon upgrade, move LDAP properties from ambari.properties file to internal Ambari configuration storage","Upon upgrade to Ambari 3.0.0, move the relevant LDAP properties from ambari.properties file to internal Ambari configuration storage under the ""ldap-configuration"" category. 

||ambari.properties||ldap-configuration||
|ambari.ldap.isConfigured|ambari.ldap.authentication.enabled|
|authentication.ldap.useSSL|ambari.ldap.connectivity.use_ssl|
|authentication.ldap.primaryUrl|ambari.ldap.connectivity.server.host, ambari.ldap.connectivity.server.port|
|authentication.ldap.secondaryUrl|ambari.ldap.connectivity.secondary.server.host, ambari.ldap.connectivity.secondary.server.port|
|authentication.ldap.baseDn|ambari.ldap.attributes.user.search_base, ambari.ldap.attributes.group.search_base|
|authentication.ldap.bindAnonymously|ambari.ldap.connectivity.anonymous_bind|
|authentication.ldap.managerDn|ambari.ldap.connectivity.bind_dn|
|authentication.ldap.managerPassword|ambari.ldap.connectivity.bind_password|
|authentication.ldap.dnAttribute|ambari.ldap.attributes.dn_attr|
|authentication.ldap.usernameAttribute|ambari.ldap.attributes.user.name_attr|
|authentication.ldap.username.forceLowercase|ambari.ldap.advanced.force_lowercase_usernames|
|authentication.ldap.userBase|ambari.ldap.attributes.search_user_base|
|authentication.ldap.userObjectClass|ambari.ldap.attributes.user.object_class|
|authentication.ldap.groupBase|ambari.ldap.attributes.group.search_group_base|
|authentication.ldap.groupObjectClass|ambari.ldap.attributes.group.object_class|
|authentication.ldap.groupNamingAttr|ambari.ldap.attributes.group.name_attr|
|authentication.ldap.groupMembershipAttr|ambari.ldap.attributes.group.member_attr|
|authorization.ldap.adminGroupMappingRules|ambari.ldap.advanced.group_mapping_rules|
|authentication.ldap.userSearchFilter|ambari.ldap.advanced.user_search_filter|
|authentication.ldap.alternateUserSearchEnabled|ambari.ldap.advanced.alternate_user_search_enabled|
|authentication.ldap.alternateUserSearchFilter|ambari.ldap.advanced.alternate_user_search_filter|
|authorization.ldap.groupSearchFilter|ambari.ldap.advanced.group_search_filter|
|authentication.ldap.referral|ambari.ldap.advanced.referrals|
|authentication.ldap.pagination.enabled|ambari.ldap.advanced.pagination_enabled|
|authentication.ldap.sync.userMemberReplacePattern|ambari.ldap.advanced.user_member_replace_pattern|
|authentication.ldap.sync.groupMemberReplacePattern|ambari.ldap.advanced.group_member_replace_pattern|
|authentication.ldap.sync.userMemberFilter|ambari.ldap.advanced.user_member_filter|
|authentication.ldap.sync.groupMemberFilter|ambari.ldap.advanced.group_member_filter|
|ldap.sync.username.collision.behavior|ambari.ldap.advance.collision_behavior|",ldap pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-12-19 08:34:41,6
13125820,Use internal LDAP configuration values rather than ambari.properties values when accessing the configured LDAP server,"Use internal LDAP configuration values rather than ambari.properties values when accessing the configured LDAP server for LDAP sync and authentication. 

* Deprecate {{setup-ldap}} from the {{ambari-server}} script.  
** Rather then perform any operations, alert user to configure LDAP integration from the Ambari UI
* Lookup LDAP-specific properties from the Ambari configuration data under the ""ldap-configuration"" category.
* Remove relevant properties from {{org.apache.ambari.server.configuration.Configuration}}
** ambari.ldap.isConfigured
** authentication.ldap.useSSL
** authentication.ldap.primaryUrl
** authentication.ldap.secondaryUrl
** authentication.ldap.baseDn
** authentication.ldap.bindAnonymously
** authentication.ldap.managerDn
** authentication.ldap.managerPassword
** authentication.ldap.dnAttribute
** authentication.ldap.usernameAttribute
** authentication.ldap.username.forceLowercase
** authentication.ldap.userBase
** authentication.ldap.userObjectClass
** authentication.ldap.groupBase
** authentication.ldap.groupObjectClass
** authentication.ldap.groupNamingAttr
** authentication.ldap.groupMembershipAttr
** authorization.ldap.adminGroupMappingRules
** authentication.ldap.userSearchFilter
** authentication.ldap.alternateUserSearchEnabled
** authentication.ldap.alternateUserSearchFilter
** authorization.ldap.groupSearchFilter
** authentication.ldap.referral
** authentication.ldap.pagination.enabled
** authentication.ldap.sync.userMemberReplacePattern
** authentication.ldap.sync.groupMemberReplacePattern
** authentication.ldap.sync.userMemberFilter
** authentication.ldap.sync.groupMemberFilter
** ldap.sync.username.collision.behavior
 ",ldap pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-12-19 08:27:14,6
13124713,AmbariServer will throw internal server error in case of post existing version_definition,"The following curl will result in 500 if post it more than once:
{noformat}
curl -vvv -u admin:admin -k -H ""X-Requested-By:ambari"" -X POST https://$AMBARI_SERVER/api/v1/version_definitions -d '{
  ""VersionDefinition"": {
   ""version_url"":
""http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0/HDP-2.6.3.0-235.xml""
    }
  }'
{noformat}

The http response is:

{noformat}
{
  ""status"" : 500,
  ""message"" : ""An internal system exception occurred: Base url http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.3.0 is already defined for another repository version. Setting up base urls that contain the same versions of components will cause stack upgrade to fail.""
}
{noformat}

It would be better, if in this case the http response status would be {noformat}409 - Conflict{noformat}

",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-12-13 15:49:45,5
13122616,Fix the wording on IPA integration requirements in the Enable Kerberos Wizard,"
Fix the wording of the IPA requirements in the Enable Kerberos Wizard to read as follows

* All cluster hosts are joined to the IPA domain and hosts are registered in DNS
* A password policy is in place that sets no expiry for created principals
* If you do not plan on using Ambari to manage the krb5.conf, ensure the following is set in each krb5.conf file in your cluster: default_ccache_name = /tmp/krb5cc_%{uid}
* The Java Cryptography Extensions (JCE) have been setup on the Ambari Server host and all hosts in the cluster
",freeipa kerberos,['ambari-server'],AMBARI,Bug,Major,2017-12-04 16:14:25,7
13122614,Ambari should not force accounts created in IPA to be added a user named 'ambari-managed-principals',"When creating user principals while enabling Kerberos using FreeIPA, Ambari should not force accounts to be added a user named 'ambari-managed-principals'. 

This occurs because the default value of {{kerberos-env/ipa_user_group}} is ""ambari-managed-principals"". To stop forcing this, the default value should be empty.",freeipa kerberos,['ambari-server'],AMBARI,Bug,Major,2017-12-04 16:07:34,7
13122291,Migrate user data for upgrade to improved user account management,"Migrate data from the {{users}} table (pre-Ambari 3.0.0) to the updated {{users}} table and {{user_authentication}} tables.

See [^user_management_db_schema_upgrade.png]

!user_management_db_schema_upgrade.png|thumbnail!",pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-12-01 22:00:37,7
13122288,LDAP sync should be based off of DN rather than username,LDAP sync should be based off of DN rather than username.,ldap,['ambari-server'],AMBARI,Task,Major,2017-12-01 21:57:17,7
13122151,Handle passwords/sensitive data in Ambari configuration properties,"Passwords and other sensitive data stored as values to properties in Ambari configurations need to be masked or not stored in cleartext.

For example, {{ldap-configuration/ambari.ldap.connectivity.trust_store.password}} and ldap-{{configuration/ambari.ldap.connectivity.bind_password}}.

If the Ambari credential store is enabled (which might be by default as of Ambari 3.0.0), the sensitive date can be stored there like we do when sensitive data is to be stored in the ambari.properties file - see {{org.apache.ambari.server.security.encryption.CredentialStoreService}}.",config security,['ambari-server'],AMBARI,Task,Minor,2017-12-01 11:05:06,6
13121915,Need to address HDP-GPL repo update after user accepts license in post-install scenario,"  * User denies the GPL license agreement, UI will still issue PUT call to create the repo
  * Ambari should not write HDP-GPL repo info to the ambari-hdp-1.repo because it will break yum in local repo world.
  * Then the user accepts the license by running ambari-server setup
  * The repo file does not get updated on existing hosts
  * We need to push the HDP-GPL repo to the repo files on existing hosts only when the license is accepted
",pull-request-available,[],AMBARI,Bug,Major,2017-11-30 13:28:10,0
13121898,Remove obsolete hack to set KDC admin credentials via Cluster session API,"Remove hack to set KDC admin credential via the API to set session attribute via the Cluster resource.
Near *org/apache/ambari/server/controller/AmbariManagementControllerImpl.java:1469*

{code:java}
      // TODO: Once the UI uses the Credential Resource API, remove this block to _clean_ the
      // TODO: session attributes and store any KDC administrator credentials in the secure
      // TODO: credential provider facility.
      // For now, to keep things backwards compatible, get and remove the KDC administrator credentials
      // from the session attributes and store them in the CredentialsProvider. The KDC administrator
      // credentials are prefixed with kdc_admin/. The following attributes are expected, if setting
      // the KDC administrator credentials:
      //    kerberos_admin/principal
      //    kerberos_admin/password
      if((sessionAttributes != null) && !sessionAttributes.isEmpty()) {
        Map<String, Object> cleanedSessionAttributes = new HashMap<>();
        String principal = null;
        char[] password = null;

        for(Map.Entry<String,Object> entry: sessionAttributes.entrySet()) {
          String name = entry.getKey();
          Object value = entry.getValue();

          if (""kerberos_admin/principal"".equals(name)) {
            if(value instanceof String) {
              principal = (String)value;
            }
          }
          else if (""kerberos_admin/password"".equals(name)) {
            if(value instanceof String) {
              password = ((String) value).toCharArray();
            }
          } else {
            cleanedSessionAttributes.put(name, value);
          }
        }

        if(principal != null) {
          // The KDC admin principal exists... set the credentials in the credentials store
          credentialStoreService.setCredential(cluster.getClusterName(),
              KerberosHelper.KDC_ADMINISTRATOR_CREDENTIAL_ALIAS,
              new PrincipalKeyCredential(principal, password), CredentialStoreType.TEMPORARY);
        }

        sessionAttributes = cleanedSessionAttributes;
      }
      // TODO: END
{code}


This is no longer needed once the UI uses the new Credential Resource REST API - see  AMBARI-13292",kdc_credentials kerberos,['ambari-server'],AMBARI,Bug,Minor,2017-11-30 12:06:30,6
13121621,Integrate component state counters with websocket events,"Use /events/hostcomponents and /events/ui_topologies topics to track component counters. Events will update counters of component: TOTAL, INSTALLED, STARTED, INIT, INSTALL_FAILED, UNKNOWN.",pull-request-available,['ambari-web'],AMBARI,Task,Major,2017-11-29 11:39:49,4
13111504,Improve KDC integration,"Improve KDC integration by making the interfaces more consistent with each other.

*Notes:*
* When using the MIT KDC or IPA options, the {{kerberos-env/admin_server_host}} value *must be the fully qualified domain name* (FQDN) of the host were the KDC administrator service is. 
* When connecting to the MIT KDC and IPA server, a username a password is not used to authenticate using the kadmin utility.  A Kerberos ticket is first acquired and that is used for authentication.
* When creating Kerberos identities using the MIT KDC and IPA handlers, the Ambari-generated password is not used.  All password's for principals in the MIT KDC and IP server are generated randomly by the KDC.
* Removed {{kerberos-env/set_password_expiry}} and {{kerberos-env/password_chat_timeout}} properties since they are no longer needed
* Changed {{kerberos-env/groups}} to {{kerberos-env/ipa_user_groups}} to be more explicit in how the property is used.
* The setPassword implementation for the MIT KDC and IPA handlers do nothing except check to see if the relevant principal exists. This is to maintain backward compatibility with previous implementations.  

",kerberos,['ambari-server'],AMBARI,Task,Major,2017-10-23 18:31:39,7
13109010,host and hostname built-in variables are not populated when performing Kerberos-related operations,"The host and hostname built-in variables are not populated when performing Kerberos-related operations.  These variables may be used like 

{code}
service-${host}
{code}

If the current host being processed is host1.example.com, the value should be converted to

{code}
service-host1.example.com
{code}

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2017-10-12 18:30:02,7
13107059,"When regenerating keytab files for a service, non-service-specific principals are affected","When regenerating keytab files for a service, non-service-specific principals are affected. For example, when regenerating the keytab files for HDFS using the following ReST API call:

{code:title=PUT /api/v1/clusters/c1?regenerate_keytabs=all&regenerate_components=HDFS}
{
  ""Clusters"": {
    ""security_type"": ""KERBEROS""
  }
}
{code}

The following principals are affected:
* HTTP/c6402.ambari.apache.org@EXAMPLE.COM
* ambari-qa-c1@EXAMPLE.COM
* nn/c6402.ambari.apache.org@EXAMPLE.COM
* hdfs-c1@EXAMPLE.COM
* HTTP/c6403.ambari.apache.org@EXAMPLE.COM
* dn/c6403.ambari.apache.org@EXAMPLE.COM
* HTTP/c6401.ambari.apache.org@EXAMPLE.COM
* nn/c6401.ambari.apache.org@EXAMPLE.COM
* ambari-server-c1@EXAMPLE.COM

However only the following principals *should be*  affected:
* nn/c6402.ambari.apache.org@EXAMPLE.COM
* hdfs-c1@EXAMPLE.COM
* dn/c6403.ambari.apache.org@EXAMPLE.COM
* nn/c6401.ambari.apache.org@EXAMPLE.COM
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2017-10-04 20:52:34,7
13106881,Package install fails on Debian7 with 'EMPTY_FILE' global variable not defined,"*STR*
# Deployed cluster with Ambari version: 2.5.1.0-159 and HDP version: 2.5.6.0-40
# Upgrade Ambari to 2.6.0.0-173
# Register HDP Version 2.6.3.0-151 and try to install the packages

*Result*
Package install fails with below error
{code}
2017-10-03 05:27:09,454 - Will install packages for repository version 2.6.3.0-151
2017-10-03 05:27:09,455 - Repository['HDP-2.6-repo-51'] {'append_to_file': False, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151', 'action': ['create'], 'components': ['HDP', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2017-10-03 05:27:09,460 - File['/tmp/tmpPriIC1'] {'content': 'deb http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151 HDP main'}
2017-10-03 05:27:09,461 - Writing File['/tmp/tmpPriIC1'] because contents don't match
2017-10-03 05:27:09,461 - File['/tmp/tmpWBU9KY'] {'content': StaticFile('/etc/apt/sources.list.d/ambari-hdp-51.list')}
2017-10-03 05:27:09,462 - Writing File['/tmp/tmpWBU9KY'] because contents don't match
2017-10-03 05:27:09,462 - File['/etc/apt/sources.list.d/ambari-hdp-51.list'] {'content': StaticFile('/tmp/tmpPriIC1')}
2017-10-03 05:27:09,463 - Writing File['/etc/apt/sources.list.d/ambari-hdp-51.list'] because contents don't match
2017-10-03 05:27:09,463 - checked_call[['apt-get', 'update', '-qq', '-o', 'Dir::Etc::sourcelist=sources.list.d/ambari-hdp-51.list', '-o', 'Dir::Etc::sourceparts=-', '-o', 'APT::Get::List-Cleanup=0']] {'sudo': True, 'quiet': False}
2017-10-03 05:27:10,166 - checked_call returned (0, '')
2017-10-03 05:27:10,169 - Repository['HDP-UTILS-1.1.0.21-repo-51'] {'append_to_file': True, 'base_url': 'http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian7', 'action': ['create'], 'components': ['HDP-UTILS', 'main'], 'repo_template': '{{package_type}} {{base_url}} {{components}}', 'repo_file_name': 'ambari-hdp-51', 'mirror_list': None}
2017-10-03 05:27:10,176 - File['/tmp/tmpnjTvFL'] {'content': 'deb http://s3.amazonaws.com/dev.hortonworks.com/HDP/debian7/2.x/BUILDS/2.6.3.0-151 HDP main\ndeb http://s3.amazonaws.com/dev.hortonworks.com/HDP-UTILS-1.1.0.21/repos/debian7 HDP-UTILS main'}
2017-10-03 05:27:10,176 - Writing File['/tmp/tmpnjTvFL'] because contents don't match
2017-10-03 05:27:10,177 - File['/tmp/tmprvUQz1'] {'content': StaticFile('/etc/apt/sources.list.d/ambari-hdp-51.list')}
2017-10-03 05:27:10,178 - Writing File['/tmp/tmprvUQz1'] because contents don't match
2017-10-03 05:27:10,179 - File['/etc/apt/sources.list.d/ambari-hdp-51.list'] {'content': StaticFile('/tmp/tmpnjTvFL')}
2017-10-03 05:27:10,180 - Writing File['/etc/apt/sources.list.d/ambari-hdp-51.list'] because contents don't match
2017-10-03 05:27:10,181 - checked_call[['apt-get', 'update', '-qq', '-o', 'Dir::Etc::sourcelist=sources.list.d/ambari-hdp-51.list', '-o', 'Dir::Etc::sourceparts=-', '-o', 'APT::Get::List-Cleanup=0']] {'sudo': True, 'quiet': False}
2017-10-03 05:27:11,641 - checked_call returned (0, '')
2017-10-03 05:27:11,642 - call[('ambari-python-wrap', '/usr/bin/hdp-select', 'versions')] {}
2017-10-03 05:27:11,681 - call returned (0, '2.5.6.0-40')
2017-10-03 05:27:11,683 - Package['hdp-select'] {'retry_on_repo_unavailability': True, 'retry_count': 5, 'use_repos': ['HDP-2.6-repo-51', 'HDP-UTILS-1.1.0.21-repo-51'], 'action': ['upgrade']}
2017-10-03 05:27:11,683 - Temporal sources directory was created: /tmp/tmpN_6yuF-ambari-apt-sources-d
2017-10-03 05:27:11,683 - Package Manager failed to install packages. Error: global name 'EMPTY_FILE' is not defined
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/custom_actions/scripts/install_packages.py"", line 335, in install_packages
    retry_count=agent_stack_retry_count)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 166, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/__init__.py"", line 57, in action_upgrade
    self.upgrade_package(package_name, self.resource.use_repos, self.resource.skip_repos)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 74, in wrapper
    return function_to_decorate(self, name, *args[2:])
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 385, in upgrade_package
    return self.install_package(name, use_repos, skip_repos, is_upgrade)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 74, in wrapper
    return function_to_decorate(self, name, *args[2:])
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/package/apt.py"", line 352, in install_package
    cmd = cmd + ['-o', 'Dir::Etc::SourceList=%s' % EMPTY_FILE]
NameError: global name 'EMPTY_FILE' is not defined
{code}",express_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-10-04 07:51:08,5
13101703,Validate kerberos.json files to ensure they meet the expected schema,Validate kerberos.json files to ensure they meet the expected schema.,kerberos_descriptor,['ambari-server'],AMBARI,Improvement,Major,2017-09-12 16:34:28,7
13101018,"Kerberos identity references should use the ""reference"" attribute","Kerberos identity references should use the ""reference"" attribute rather than rely on the ""name"" attribute to indicate the identity descriptor references some other identity descriptor.  

Either method should work on the backend, however the UI appears to not fully handle the ""named"" reference properly. 

The solution is to change 
{code}
            {
              ""name"": ""/HDFS/NAMENODE/namenode_nn"",
              ""principal"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
            }
{code}

by changing the ""name"" attribute to ""reference"" and adding a new ""name"" reference with a unique name relative to the scope of the identity descriptor. For example:

{code}
            {
              ""name"":""ranger_hdfs_audit""
              ""reference"": ""/HDFS/NAMENODE/namenode_nn"",
              ""principal"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hdfs-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
            }
{code}
",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2017-09-09 10:48:24,7
13097579,test_kms_server timing issue,"Python unit test in {{test_kms_server}} occasionally fails due to timing.  Expected and actual output use two separate calls to get current time, which may be different if happens to be executed around the turn of a second.

{noformat}
FAIL: test_start_secured (test_kms_server.TestRangerKMS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/stacks/2.5/RANGER_KMS/test_kms_server.py"", line 522, in test_start_secured
    mode = 0644
  File ""/home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 330, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'owner': 'kms', 'content': '<ranger>\n<enabled>2017-08-25 11:02:46</enabled>\n< [truncated]... != {'content': '<ranger>\n<enabled>2017-08-25 11:02:45</enabled>\n</ranger>', 'owne [truncated]...
- {'content': '<ranger>\n<enabled>2017-08-25 11:02:46</enabled>\n</ranger>',
?                                                   ^

+ {'content': '<ranger>\n<enabled>2017-08-25 11:02:45</enabled>\n</ranger>',
?                                                   ^

-  'group': 'kms',
+  'group': u'kms',
?           +

   'mode': 420,
-  'owner': 'kms'}
+  'owner': u'kms'}
?           +

{noformat}

Affected code: {{test_start_default}} and {{test_start_secured}} in {{ambari-server/src/test/python/stacks/2.5/RANGER_KMS/test_kms_server.py}}.",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2017-08-25 12:05:02,5
13093766,Add check for import from relocated packages,"Occasionally an import from {{org.apache.hadoop.metrics2.sink.relocated....}} creeps into {{ambari-server}} source code, causing compile errors:

{noformat}
$ mvn -am -pl ambari-server clean test
...
[ERROR] ambari-server/src/test/java/org/apache/ambari/server/checks/AbstractCheckDescriptorTest.java:[39,71] package org.apache.hadoop.metrics2.sink.relocated.google.common.collect does not exist
[ERROR] ambari-server/src/test/java/org/apache/ambari/server/checks/AbstractCheckDescriptorTest.java:[114,34] cannot find symbol
[ERROR]   symbol:   variable Sets
[ERROR]   location: class org.apache.ambari.server.checks.AbstractCheckDescriptorTest
{noformat}

The problem is that the same code can be compiled if dependencies are already installed in one's local Maven repository.

{noformat}
$ mvn -am -pl ambari-metrics/ambari-metrics-common,ambari-serviceadvisor,ambari-views clean install
...
$ mvn -pl ambari-server clean test
...
[INFO] BUILD SUCCESS
{noformat}

This succeeds because {{ambari-metrics-common}} installs a shaded uber jar including the {{..relocated..}} packages, hence they are available when compiling {{ambari-server}}.  On the other hand, when building from scratch (selectively with {{-am -pl ...}}, or the entire multimodule project) classpath contains {{ambari-metrics-common}} classes and individual dependencies without relocation.

The goal of this change is to add a checkstyle check to catch such imports at build-time with both compilation methods.",pull-request-available,['ambari-server'],AMBARI,Improvement,Minor,2017-08-10 12:49:01,5
13093694,Error starting client components on RedHat7 and BI 4.2.5 after Ambari upgrade,"*STR*
# Deployed IOP-4.2.5 cluster with Ambari-2.4.2
# Upgrade Ambari to 2.5.2
# Stop Hbase service and delete HBASE_REST_SERVER component via API
# Try to start HBase service

Result
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/hook.py"", line 37, in <module>
    BeforeInstallHook().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 329, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/hook.py"", line 33, in hook
    install_repos()
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/repo_initialization.py"", line 68, in install_repos
    _alter_repo(""create"", params.repo_info, template)
  File ""/var/lib/ambari-agent/cache/stacks/BigInsights/4.2.5/hooks/before-INSTALL/scripts/repo_initialization.py"", line 35, in _alter_repo
    repo_dicts = json.loads(repo_string)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/__init__.py"", line 307, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 335, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 353, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
{code}",system_test,['ambari-server'],AMBARI,Bug,Blocker,2017-08-10 08:00:28,5
13091162,Dynamically determine what keytab files have been distributed,"Dynamically determine what keytab files have been distributed to hosts. A custom command should be available via the KERBEROS_CLIENT to query for the keytab files installed on the relevant host. The communication between the Ambari server and the agents should generate data needed to determine what keytab files exist.
",pull-request-available,"['ambari-agent', 'ambari-server']",AMBARI,Task,Major,2017-07-31 11:10:55,3
13087272,"NPE during ""Update Kerberos Descriptor""","Ambari-server.log:- 
{code}
java.lang.NullPointerException
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processIdentity(KerberosDescriptorUpdateHelper.java:360)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processIdentities(KerberosDescriptorUpdateHelper.java:321)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processComponent(KerberosDescriptorUpdateHelper.java:230)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processService(KerberosDescriptorUpdateHelper.java:195)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.processServices(KerberosDescriptorUpdateHelper.java:122)
	at org.apache.ambari.server.state.kerberos.KerberosDescriptorUpdateHelper.updateUserKerberosDescriptor(KerberosDescriptorUpdateHelper.java:78)
	at org.apache.ambari.server.serveraction.upgrades.UpgradeUserKerberosDescriptor.execute(UpgradeUserKerberosDescriptor.java:139)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:517)
	at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:454)
	at java.lang.Thread.run(Thread.java:748)

{code}

*Cause*
This is caused by having a _custom/unexpected_ identity in the user-supplied Kerberos descriptor that is not in the Kerberos descriptor from the initial stack.  

*Solution*
Ignore the _custom/unexpected_ identity since the user must have added that manually and it is expected that it should remain untouched after the upgrade process. 

",express_upgrade rolling_upgrade upgrade,['ambari-server'],AMBARI,Bug,Critical,2017-07-14 18:47:10,7
13081417,Python tests fail for ambari-server and ambari-agent on ppc64le,"There are 78 python test failures in Ambari agent for ppc64le

Error:

{code:java}
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-agent/src/test/python/resource_management/TestXmlConfigResource.py"", line 68, in test_action_create_empty_xml_config
    configuration_attributes={}
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/environment.py"", line 118, in run_action
    resource.provider)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/core/providers/__init__.py"", line 93, in find_provider
    if resource in os_family_provider:
UnboundLocalError: local variable 'os_family_provider' referenced before assignment
{code}

In Ambari-server, the below tests fail: 


{code:java}
ERROR: test_configure_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 37, in test_configure_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_install_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 79, in test_install_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 41, in install
    self.configure(env)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_start_default (test_ganglia_server.TestGangliaServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/GANGLIA/test_ganglia_server.py"", line 48, in test_start_default
    target = RMFTestCase.TARGET_COMMON_SERVICES
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 49, in start
    self.configure(env)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 78, in configure
    change_permission()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/GANGLIA/3.5.0/package/scripts/ganglia_server.py"", line 93, in change_permission
    Directory(params.dwoo_path,
AttributeError: 'module' object has no attribute 'dwoo_path'

ERROR: test_start_default_22_with_phoenix_enabled (test_hbase_regionserver.TestHbaseRegionServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HBASE/test_hbase_regionserver.py"", line 427, in test_start_default_22_with_phoenix_enabled
    target = RMFTestCase.TARGET_COMMON_SERVICES)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 155, in executeScript
    method(RMFTestCase.env, *command_args)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 93, in start
    self.configure(env) # for security
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/resource_management/libraries/script/script.py"", line 120, in locking_configure
    original_configure(obj, *args, **kw)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/../../../../main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line 49, in configure
    hbase(name='regionserver')
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/main/python/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/main/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase.py"", line 224, in hbase
    Package(params.phoenix_package,
AttributeError: 'module' object has no attribute 'phoenix_package'

FAIL: test_clean_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 124, in test_clean_default
    self.assert_clean_default()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 182, in assert_clean_default
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/removeMysqlUser.sh mysql hive c6402.ambari.apache.org' != u'bash -x /tmp/removeMysqlUser.sh mysqld hive c6402.ambari.apache.org'

FAIL: test_clean_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 135, in test_clean_secured
    self.assert_clean_secured()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 193, in assert_clean_secured
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/removeMysqlUser.sh mysql hive c6402.ambari.apache.org' != u'bash -x /tmp/removeMysqlUser.sh mysqld hive c6402.ambari.apache.org'

FAIL: test_configure_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 38, in test_configure_default
    self.assert_configure_default()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 153, in assert_configure_default
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/addMysqlUser.sh mysql hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org' != u'bash -x /tmp/addMysqlUser.sh mysqld hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org'

FAIL: test_configure_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 81, in test_configure_secured
    self.assert_configure_secured()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 171, in assert_configure_secured
    try_sleep = 5
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: 'bash -x /tmp/addMysqlUser.sh mysql hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org' != u'bash -x /tmp/addMysqlUser.sh mysqld hive \'!`""\'""\'""\' 1\' c6402.ambari.apache.org'

FAIL: test_start_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 53, in test_start_default
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'start') != ('service', 'mysqld', 'start')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'start')
+ ('service', 'mysqld', 'start')
?                   +


FAIL: test_start_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 96, in test_start_secured
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'start') != ('service', 'mysqld', 'start')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'start')
+ ('service', 'mysqld', 'start')
?                   +


FAIL: test_stop_default (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 68, in test_stop_default
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'stop') != ('service', 'mysqld', 'stop')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'stop')
+ ('service', 'mysqld', 'stop')
?                   +


FAIL: test_stop_secured (test_mysql_server.TestMySqlServer)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/HIVE/test_mysql_server.py"", line 112, in test_stop_secured
    sudo = True,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: Tuples differ: ('service', 'mysql', 'stop') != ('service', 'mysqld', 'stop')

First differing element 1:
mysql
mysqld

- ('service', 'mysql', 'stop')
+ ('service', 'mysqld', 'stop')
?                   +


FAIL: test_service_check_default (test_service_check.TestServiceCheck)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 40, in test_service_check_default
    self.assert_service_check()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 153, in assert_service_check
    try_sleep = 5,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: '/tmp/oozieSmoke2.sh suse /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False' != u'/tmp/oozieSmoke2.sh suse-ppc /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False'

FAIL: test_service_check_secured (test_service_check.TestServiceCheck)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 53, in test_service_check_secured
    self.assert_service_check()
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/OOZIE/test_service_check.py"", line 153, in assert_service_check
    try_sleep = 5,
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 281, in assertResourceCalled
    self.assertEquals(name, resource.name)
AssertionError: '/tmp/oozieSmoke2.sh suse /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False' != u'/tmp/oozieSmoke2.sh suse-ppc /var/lib/oozie /etc/oozie/conf /usr/bin http://c6402.ambari.apache.org:11000/oozie / /etc/hadoop/conf /usr/bin ambari-qa no-op False'

FAIL: test_hook_default (test_before_install.TestHookBeforeInstall)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/hooks/before-INSTALL/test_before_install.py"", line 42, in test_hook_default
    repo_template='[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 282, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6 [truncated]... != {'base_url': u'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0. [truncated]...
  {'action': ['create'],
-  'base_url': 'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0',
+  'base_url': u'http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0',
?              +

-  'components': ['HDP', 'main'],
+  'components': [u'HDP', 'main'],
?                 +

   'mirror_list': None,
-  'repo_file_name': 'HDP',
+  'repo_file_name': u'HDP',
?                    +

-  'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'}
+  'repo_template': u'{{package_type}} {{base_url}} {{components}}'}

FAIL: test_hook_default_repository_file (test_before_install.TestHookBeforeInstall)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/2.0.6/hooks/before-INSTALL/test_before_install.py"", line 80, in test_hook_default_repository_file
    append_to_file=False)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/stacks/utils/RMFTestCase.py"", line 282, in assertResourceCalled
    self.assertEquals(kwargs, resource.arguments)
AssertionError: {'append_to_file': False, 'base_url': 'http://repo1/HDP/centos5/2.x/updates/2.2. [truncated]... != {'append_to_file': False, 'base_url': u'http://repo1/HDP/centos5/2.x/updates/2.2 [truncated]...
  {'action': ['create'],
   'append_to_file': False,
-  'base_url': 'http://repo1/HDP/centos5/2.x/updates/2.2.0.0',
+  'base_url': u'http://repo1/HDP/centos5/2.x/updates/2.2.0.0',
?              +

-  'components': ['HDP', 'main'],
+  'components': [u'HDP', 'main'],
?                 +

   'mirror_list': None,
   'repo_file_name': 'ambari-hdp-4',
-  'repo_template': '[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0'}
+  'repo_template': u'{{package_type}} {{base_url}} {{components}}'}

FAIL: testTransparentHugePage (TestCheckHost.TestCheckHost)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-common/src/test/python/mock/mock.py"", line 1199, in patched
    return func(*args, **keywargs)
  File ""/var/lib/jenkins/workspace/ambari-trunk/ambari-server/src/test/python/custom_actions/TestCheckHost.py"", line 407, in testTransparentHugePage
    self.assertEquals(structured_out_mock.call_args[0][0], {'transparentHugePage' : {'message': 'never', 'exit_code': 0}})
AssertionError: {'transparentHugePage': {'message': '', 'exit_code': 0}} != {'transparentHugePage': {'message': 'never', 'exit_code': 0}}
- {'transparentHugePage': {'exit_code': 0, 'message': ''}}
+ {'transparentHugePage': {'exit_code': 0, 'message': 'never'}}
?                                                      +++++

{code}



",powerpc ppc64le,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2017-06-21 13:03:56,0
13076202,Eliminate Maven warnings,"Get rid of as many Maven warnings as possible:

{noformat}
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-web:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:exec-maven-plugin @ line 161, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-admin:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 91, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-common:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-hadoop-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-flume-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-kafka-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-storm-sink:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-storm-sink-legacy:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-timelineservice:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 252, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-host-monitoring:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:exec-maven-plugin is missing. @ line 86, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 110, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-grafana:pom:2.1.0.0.0
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 64, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-assembly:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics-host-aggregator:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ org.apache.ambari:ambari-metrics:2.0.0.0-SNAPSHOT, /home/jenkins/jenkins-slave/workspace/Ambari-trunk-Commit/ambari-metrics/pom.xml, line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-metrics:pom:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-surefire-plugin @ line 169, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:build-helper-maven-plugin @ line 202, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.rat:apache-rat-plugin is missing. @ line 282, column 15
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-clean-plugin is missing. @ line 187, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-server:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-antrun-plugin @ line 699, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-antrun-plugin @ line 735, column 15
[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.codehaus.mojo:exec-maven-plugin @ line 824, column 15
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:properties-maven-plugin is missing. @ line 469, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-funtest:jar:2.0.0.0-SNAPSHOT
[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.httpcomponents:httpclient:jar -> version 4.2.5 vs 4.5.2 @ line 559, column 17
[WARNING] 'build.plugins.plugin.version' for org.apache.maven.plugins:maven-failsafe-plugin is missing. @ line 52, column 15
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.ambari:ambari-agent:jar:2.0.0.0-SNAPSHOT
[WARNING] 'build.plugins.plugin.version' for org.codehaus.mojo:properties-maven-plugin is missing. @ line 207, column 15
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Major,2017-05-31 14:24:34,5
13076196,Code cleanup,Clean up Ambari Server source code warnings.,pull-request-available,['ambari-server'],AMBARI,Epic,Major,2017-05-31 14:00:24,5
13075924,Update Database Access Layer to Support New Database Schema for Improved User Account Management,"Update Database Access Layer to Support New Database Schema for Improved User Account Management.  

* Update {{org.apache.ambari.server.orm.entities.UserEntity}}
* Update {{org.apache.ambari.server.orm.dao.UserDAO}}
* Add {{org.apache.ambari.server.orm.entities.UserAuthenticationEntity}}
* Add {{org.apache.ambari.server.orm.dao.UserAuthenticationDAO}}
",user_management,['ambari-server'],AMBARI,Bug,Major,2017-05-30 18:40:55,7
13069272,LDAPS connections to an Active Directory when enabling Kerberos should validate the server's SSL certificate,"LDAPS connections to an Active Directory when enabling Kerberos should validate the server's SSL certificate.  The current implementation skips validation checks to help avoid SSL issues; however this is not secure. Also the _trusting_ SSL connection may not support the more secure SSL protocols - TLSv1.2.

A flag in the {{ambari.properties}} file ({{kerberos.operation.verify.kdc.trust}}) should be available to allow for the user to select either a _trusting_  SSL connection or a validating (non-trusting) SSL connection to be used.  The default should be to use the standard (non-trusting) SSL connection. 
",active-directory active_directory kerberos ssl,['ambari-server'],AMBARI,Bug,Major,2017-05-04 20:39:13,7
13068235,Create Database Schema for Improved User Account Management,"User management tables in the DB should be:

*{{users}}*
||Name||Type||Description||
|user_id|INTEGER|Internal unique identifier|
|principal_id|INTEGER|Foreign key from adminprincipal table|
|user_name|VARCHAR|Unique, case-insensitive, login identifier expected to be used when logging into Ambari|
|active|BOOLEAN|Active/not active flag|
|consecutive_failures|INTEGER|The number a failed authorization attempts since the last successful authentication|
|active_widgets_layout|VARCHAR| |
|display_name|VARCHAR|Cosmetic name value to show the user in user interfaces|
|local_username|VARCHAR|Case-sensitive username to use when impersonating user in facilities like Ambari Views|
|create_time|TIMESTAMP|Creation time for this account in Ambari|
* Primary Key: {{user_id}}
* Foreign Key: {{principal_id}} -> {{adminprincipal.principal_id}}

*{{user_authentication}}*
||Name||Type||Description||
|user_authentication_id|INTEGER|Primary key for this table|
|user_id|INTEGER|Foreign key from users table|
|authentication_type|VARCHAR|Type of authentication system - LOCAL, LDAP,  KERBEROS, JTW, PAM, etc...
|authentication_key|VARCHAR|Type-specific key (or identifier):
* LOCAL: the user's password (digest)
* LDAP: the user’s distinguished name
* KERBEROS: the user’s principal
* etc...|
|create_time|TIMESTAMP|Creation time of this record
|update_time|TIMESTAMP|Update time for this record, can be used to enforce password retention times|
* Primary Key: {{user_authentication_id}}
* Foreign Key: {{user_id}} -> {{users.user_id}}
",user_management,['ambari-server'],AMBARI,Task,Critical,2017-05-01 20:50:29,7
13067658,Create idempotent Ambari DB Schema SQL script for AzureDB,The schema file should be idempotent so that we can retry the script in case of exception.,pull-request-available,['ambari-server'],AMBARI,Task,Critical,2017-04-28 11:10:18,5
13067461,Custom RM principal causes zookeeper HA state store to be inaccessible,"HDP 2.6 stack introduced settings for ACLs on the Yarn Resource Manager HA state store. In `yarn-site/yarn.resourcemanager.zk-acl` the ACL user is set to `rm`. 

If this user name does not match the primary component of the Yarn RM Kerberos principal in `yarn-site/yarn.resourcemanager.principal`, then Yarn is unable to access the state store and RM will stop immediately after start.

During the Kerberos wizard there needs to be a check to see if these settings are out of sync. Or, the zk-acl setting needs to somehow reference the principal and extract the primary root through a variable.",security,['ambari-shell'],AMBARI,Bug,Major,2017-04-27 17:52:45,3
13067086,BE: Extend Ambari REST API to Support User Account Management Improvements,"Update the Ambari REST API to allow for GET, POST, PUT, and DELETE operations on the authentication sources related to an Ambari user account.

* * {{/api/v1/users/:USERNAME/sources}}
** List a user’s authentication sources
** Add a new authentication source for a user

* {{/api/v1/users/:USERNAME/sources/:SOURCE_ID}}
** Get details on a specific authentication source for a user
** Modify details for a specific authentication source for a user

Update the following entry points, ensuring backwards compatibility where possible:

* {{/api/v1/users}}
** List all users
** Add a new user
** Backward compatibility: Set password should create or update the appropriate user_authentication record. 

",rest_api security,['ambari-server'],AMBARI,Task,Major,2017-04-26 15:24:44,7
13067078,BE: Improve User Account Management ,"Update the backend for improved user management.  

User management tables in the DB should be:

*{{users}}*
||Name||Type||Description||
|user_id|INTEGER|Internal unique identifier|
|principal_id|INTEGER|Foreign key from adminprincipal table|
|user_name|VARCHAR|Unique, case-insensitive, login identifier expected to be used when logging into Ambari|
|create_time|TIMESTAMP|Creation time for this account in Ambari|
|active|BOOLEAN|Active/not active flag|
|consecutive_failed_auth_attemps|INTEGER|The number a failed authorization attempts since the last successful authentication|
|active_widgets_layout|VARCHAR| |
|display_name|VARCHAR|Cosmetic name value to show the user in user interfaces|
|local_username|VARCHAR|Case-sensitive username to use when impersonating user in facilities like Ambari Views|
* Primary Key: {{user_id}}
* Foreign Key: {{principal_id}} -> {{adminprincipal.principal_id}}

*{{user_authentication}}*
||Name||Type||Description||
|user_authentication_id|INTEGER|Primary key for this table|
|user_id|INTEGER|Foreign key from users table|
|authentication_type|VARCHAR|Type of authentication system - LOCAL, LDAP,  KERBEROS, JTW, PAM, etc...
|authentication_key|VARCHAR|Type-specific key (or identifier):
* LOCAL: the user's password (digest)
* LDAP: the user’s distinguished name
* KERBEROS: the user’s principal
* etc...|
|create_time|TIMESTAMP|Creation time of this record
|update_time|TIMESTAMP|Update time for this record, can be used to enforce password retention times|
* Primary Key: {{user_authentication_id}}
* Foreign Key: {{user_id}} -> {{users.user_id}}

Java code needs to change accordingly.",authentication security,['ambari-server'],AMBARI,Task,Major,2017-04-26 14:52:52,7
13067077,Improve User Account Management Within Ambari,"As of Ambari 2.4, user management is confusing and tends to lead to inconsistent results during synchronization and authentication.  With the addition of new mechanisms such as Kerberos and PAM, this will only get worse.  Therefore, there is a need to rework how Ambari manages users to ensure that new authentication facilities are easily integrated.

The following problems need to be solved:

* *Case-sensitivity*
Some authentication sources are case sensitive and some are not.  Ambari inconsistently handles the case of user names leading to confusing where user metadata is being created or being overwritten.  This issue extends from the front end through the backend and to the database layer.   

* *Username Collisions*
There are several cases where username collisions occur.  One is where a username exists as a local user as well as an external user.  For example, the initial administrator account has is a local user account with the username of ""admin"".  There may also be an external user account with the username ""admin"". In some cases Ambari will treat both accounts as the same user, converting the local account during synchronization operation to an LDAP account. However in other cases, Ambari will treat the accounts as separate users and create a separate account.  

* *REST API*
Due to the implementation of the user resource in the REST API, there is no way to distinguish between user accounts with the same username and different data sources. For example usera/LOCAL vs usera/LDAP.  This is because the primary key for user resources is only the username field.  This make managing users confusing since the REST API entrypoint for user resources is /api/v1/users/:USERNAME and there is no way to retrieve or set the details for a specific user. 
",authentication pull-request-available security user_management,"['ambari-server', 'ambari-web']",AMBARI,Epic,Major,2017-04-26 14:51:45,7
13064399,BE: Characters used in usernames should be constrained ,"Characters used in usernames should be constrained such that they cannot contain the following characters:

* Less than symbols ( < )
* Greater than symbols ( > )
* Ampersand ( & )
* Back slashes ( \ )
* Backtick ( ` )
* Pipe ( | )

",security,['ambari-server'],AMBARI,Bug,Critical,2017-04-17 13:48:32,3
13058663,Remove the use of legacy SSL and TLS protocol versions,"I notice that the explicit enabling of various protocols still includes SSLv2Hello and SSLv3, which are severely broken protocols with numerous known vulnerabilities and not necessary for legacy compatibility. Even TLSv1 and TLSv1.1 have been [discouraged since February 2014|https://community.qualys.com/thread/12421], when all modern browsers supported TLSv1.2. Is there any reason Ambari still needs to enable support for these legacy protocols, and are there any other mitigating controls put in place to prevent downgrade, brute force, padding oracle, and weak parameter attacks against these protocols? Thanks. 

",security ssl tls,"['ambari-server', 'security']",AMBARI,Bug,Major,2017-03-23 17:00:56,7
13057423,Disabling security fails with AttributeError,"Disabling security is failing with :
{code}
Stack Advisor reported an error: AttributeError: 'NoneType' object has no attribute 'replace'
StdOut file: /var/run/ambari-server/stack-recommendations/26/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/26/stackadvisor.err
{code}

Error file shows :
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 166, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 116, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 775, in recommendConfigurations
    calculation(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/./../stacks/HDP/2.5/services/stack_advisor.py"", line 480, in recommendStormConfigurations
    storm_nimbus_impersonation_acl.replace('{{storm_bare_jaas_principal}}', storm_bare_jaas_principal)
AttributeError: 'NoneType' object has no attribute 'replace'
{code}",kerberos stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2017-03-20 04:52:24,7
13056369,Duplicate entries in DB for auto_<view>_instance privileges upon Ambari server restart,"When I create a new user from Ambari UI and let’s say give him ‘Cluster User’ role; later go to Users page it shows the permissions as seen in the screenshot

The UI display is fine, however when I make an API call like below
/api/v1/users/tom/privileges?fields=*
 
It shows three entries for each auto_<view>_instance privilege. As an example: for ‘AUTO_FILES_INSTANCE’ I see three entries like:
api/v1/users/tom/privileges/6
api/v1/users/tom/privileges/56
api/v1/users/tom/privileges/106
 
and so on, so we have 16 entries (One for Cluster.User + 3 * privileges for each of five View instances)
 
The same behavior is seen for groups too like: /api/v1/groups/gp1/privileges?fields=* 

*Example*
It is expected that only one of the following rows exists:
{noformat}
ambaricustom=> select * from adminprivilege where privilege_id in (6, 56, 106);
privilege_id | permission_id | resource_id | principal_id
--------------+---------------+-------------+--------------
            6 |             4 |          54 |            8
           56 |             4 |          54 |            8
          106 |             4 |          54 |            8
(3 rows)
{noformat}
 
* permission_id (4): VIEW.USER
* resource_id (54): AUTO_FILES_INSTANCE
* principal_id (8): CLUSTER.USER

*Cause*
When Ambari server restarts, it installs the automatically created view instances without first checking to see if they have been previously created. Each restart of Ambari server will create a new set of duplicated records.
",system_test,['ambari-server'],AMBARI,Bug,Critical,2017-03-15 17:34:06,7
13050320,Atlas MetaData server start fails while granting permissions to HBase tables after unkerberizing the cluster,"STR
1. Deploy HDP-2.5.0.0 with Ambari-2.5.0.0 (secure MIT cluster installed via blueprint)
2. Express Upgrade the cluster to 2.6.0.0
3. Disable Kerberos
4. Observed that Atlas Metadata server start failed with below errors:
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata_server.py"", line 249, in <module>
    MetadataServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 720, in restart
    self.start(env, upgrade_type=upgrade_type)
  File ""/var/lib/ambari-agent/cache/common-services/ATLAS/0.1.0.2.3/package/scripts/metadata_server.py"", line 102, in start
    user=params.hbase_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 160, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 124, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 262, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 72, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 102, in checked_call
    tries=tries, try_sleep=try_sleep, timeout_kill_strategy=timeout_kill_strategy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 150, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 303, in _call
    raise ExecutionFailed(err_msg, code, out, err)
resource_management.core.exceptions.ExecutionFailed: Execution of 'cat /var/lib/ambari-agent/tmp/atlas_hbase_setup.rb | hbase shell -n' returned 1. ######## Hortonworks #############
This is MOTD message, added for testing in qe infra
atlas_titan
ATLAS_ENTITY_AUDIT_EVENTS
atlas
TABLE
ATLAS_ENTITY_AUDIT_EVENTS
atlas_titan
2 row(s) in 0.2000 seconds

nil
TABLE
ATLAS_ENTITY_AUDIT_EVENTS
atlas_titan
2 row(s) in 0.0030 seconds

nil
java exception
ERROR Java::OrgApacheHadoopHbaseIpc::RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.UnknownProtocolException: No registered coprocessor service found for name AccessControlService in region hbase:acl,,1480905643891.19e697cf0c4be8a99c54e39aea069b29.
	at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:7692)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execServiceOnRegion(RSRpcServices.java:1897)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.execService(RSRpcServices.java:1879)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32299)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2141)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:187)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:167)
{code}

*Cause*
When disabling Kerberos, the stack advisor recommendations are not properly applied due to the order of operations and various conditionals.

*Solution*
Ensure that the stack advisor recommendations are properly applied when disabling Kerberos. 
",system_test,['ambari-server'],AMBARI,Bug,Critical,2017-03-12 12:07:52,7
13049026,"When SPNEGO authentication is enabled for Hadoop in a cluster with NN HA, PXF Process alert fails","When SPNEGO authentication is enabled for Hadoop in a cluster where NN HA is enabled, PXF Process alert fails with the following errors in the ambari-agent.log file 

{noformat}
ERROR 2017-03-07 18:03:58,417 jmx.py:44 - Getting jmx metrics from NN failed. URL: http://c6401.ambari.apache.org:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesy
stem
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/jmx.py"", line 41, in get_value_from_jmx
    data_dict = json.loads(data)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/__init__.py"", line 307, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 335, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 353, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
INFO 2017-03-07 18:04:02,769 logger.py:71 - call['ambari-sudo.sh su hdfs -l -s /bin/bash -c 'curl --negotiate -u : -s '""'""'http://c6402.ambari.apache.org:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem'""'""' 1>/tmp/tmphTXg76 2>/tmp/tmp5bm2nM''] {'quiet': False}
INFO 2017-03-07 18:04:02,797 logger.py:71 - call returned (0, '')
ERROR 2017-03-07 18:04:02,798 jmx.py:44 - Getting jmx metrics from NN failed. URL: http://c6402.ambari.apache.org:50070/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/jmx.py"", line 41, in get_value_from_jmx
    data_dict = json.loads(data)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/__init__.py"", line 307, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 335, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.6/site-packages/ambari_simplejson/decoder.py"", line 353, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
{noformat}

*Cause*
During the test for the {{PXF Process}} alert, the Active NN is found using a JMX call.  This call requires SPNEGO authentication since SPNEGO authentication is turned on for the Hadoop web interfaces. However, a valid Kerberos ticket is not found in the configured user's Kerberos ticket cache. In this case, the configured users is the HDFS user - which technically is not necessary. 

This occurs in 
{code:title=common-services/PXF/3.0.0/package/alerts/api_status.py:137}
    if CLUSTER_ENV_SECURITY in configurations and configurations[CLUSTER_ENV_SECURITY].lower() == ""true"":
      if 'dfs.nameservices' in configurations[HDFS_SITE]:
        namenode_address = get_active_namenode(ConfigDictionary(configurations[HDFS_SITE]), configurations[CLUSTER_ENV_SECURITY], configurations[HADOOP_ENV_HDFS_USER])[1]
      else:
        namenode_address = configurations[HDFS_SITE]['dfs.namenode.http-address']

      token = _get_delegation_token(namenode_address,
                                     configurations[HADOOP_ENV_HDFS_USER],
                                     configurations[HADOOP_ENV_HDFS_USER_KEYTAB],
                                     configurations[HADOOP_ENV_HDFS_PRINCIPAL_NAME],
                                     None)
      commonPXFHeaders.update({""X-GP-TOKEN"": token})
{code}

Inside the call at 

{code}
namenode_address = get_active_namenode(ConfigDictionary(configurations[HDFS_SITE]), configurations[CLUSTER_ENV_SECURITY], configurations[HADOOP_ENV_HDFS_USER])[1]
{code}

*Solution*
Ensure the configured user's Kerberos ticket cache contains a valid ticket before querying for the active NN. Possibly change the acting user to one executing the PXF component. 

",PHD PXF kerberos,['ambari-server'],AMBARI,Bug,Major,2017-03-07 21:15:20,7
13048699,Kerberos identity reference not working for ranger-audit property in hbase,"From stack 2.5 onwards {{xasecure.audit.jaas.Client.option.principal/ranger-hbase-audit}} needs to have principal value available under {{hbase.master.kerberos.principal/hbase-site}}

To achieve that added below block of code under hbase [kerberos.json|https://github.com/apache/ambari/blob/branch-2.5/ambari-server/src/main/resources/stacks/HDP/2.5/services/HBASE/kerberos.json]
{noformat}
{
              ""name"": ""/HBASE/HBASE_MASTER/hbase_master_hbase"",
              ""principal"": {
                ""configuration"": ""ranger-hbase-audit/xasecure.audit.jaas.Client.option.principal""
              },
              ""keytab"": {
                ""configuration"": ""ranger-hbase-audit/xasecure.audit.jaas.Client.option.keyTab""
              }
}
{noformat}

But on test cluster, {{xasecure.audit.jaas.Client.option.principal/ranger-hbase-audit}} property is not showing the expected value. It is showing the principal/keytab values of {{ams_hbase_master_hbase}} identity. 

Because of wrong reference of principal audit to solr is not working in kerberos environment, as security.json have below entry instead of {{hbase@EXAMPLE.COM}}
{noformat}
""amshbase@EXAMPLE.COM"":[
        ""ranger_audit_user"",
        ""dev""]
{noformat}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2017-03-06 23:57:35,7
13048104,HBase Master CPU Utilization Alert is in unknown state due to kinit error,"HBase Master CPU Utilization Alert is in unknown state due to kinit error:

{noformat}
Execution of '/usr/bin/kinit -c /var/lib/ambari-agent/tmp/curl_krb_cache/metric_alert_ambari-qa_cc_56787c2122a8214ca9775f3433361f8b -kt HTTP/_HOST@EXAMPLE.COM /etc/security/keytabs/spnego.service.keytab > /dev/null' returned 1. kinit: Client not found in Kerberos database while getting initial credentials
{noformat}

This issue is also seen in /var/log/krb5kdc.log:

{noformat}
Mar 03 16:43:06 c6401.ambari.apache.org krb5kdc[4749](info): AS_REQ (4 etypes {18 17 16 23}) 192.168.64.101: CLIENT_NOT_FOUND: /etc/security/keytabs/spnego.service.keytab@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM, Client not found in Kerberos database
{noformat}

*Cause*
It appears that the HBASE alerts.json file ({{common-services/HBASE/0.96.0.2.0/alerts.json}}) has swapped values for the {{kerberos_keytab}} and {{kerberos_principal}} properties.

{code}
      {
        ""name"": ""hbase_master_cpu"",
        ""label"": ""HBase Master CPU Utilization"",
        ""description"": ""This host-level alert is triggered if CPU utilization of the HBase Master exceeds certain warning and critical thresholds. It checks the HBase Master JMX Servlet for the SystemCPULoad property. The threshold values are in percent."",
        ""interval"": 5,
        ""scope"": ""ANY"",
        ""enabled"": true,
        ""source"": {
          ""type"": ""METRIC"",
          ""uri"": {
            ""http"": ""{{hbase-site/hbase.master.info.port}}"",
            ""default_port"": 60010,
            ""connection_timeout"": 5.0,
            ""kerberos_keytab"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.principal}}"",
            ""kerberos_principal"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.keytab}}""
          },
          ""reporting"": {
            ""ok"": {
              ""text"": ""{1} CPU, load {0:.1%}""
            },
            ""warning"": {
              ""text"": ""{1} CPU, load {0:.1%}"",
              ""value"": 200
            },
            ""critical"": {
              ""text"": ""{1} CPU, load {0:.1%}"",
              ""value"": 250
            },
            ""units"" : ""%"",
            ""type"": ""PERCENT""
          },
          ""jmx"": {
            ""property_list"": [
              ""java.lang:type=OperatingSystem/SystemCpuLoad"",
              ""java.lang:type=OperatingSystem/AvailableProcessors""
            ],
            ""value"": ""{0} * 100""
          }
        }
      }
{code}

Notice:
{code}
            ""kerberos_keytab"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.principal}}"",
            ""kerberos_principal"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.keytab}}""
{code}

*Solution*
Fix values for the {{kerberos_keytab}} and {{kerberos_principal}} properties in {{common-services/HBASE/0.96.0.2.0/alerts.json}}:

{code}
            ""kerberos_principal"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.principal}}"",
            ""kerberos_keytab"": ""{{hbase-site/hbase.security.authentication.spnego.kerberos.keytab}}""
{code}
",alerts kerberos,['ambari-server'],AMBARI,Bug,Major,2017-03-03 16:49:05,7
13047670,Failed task during EU is not reported upfront causing Upgrade to show 'Aborted' after Finalize step,"STR
1. Started EU from HDP-2.5.3 to 2.6.0.0
2. Reach till ‘Restart HBASE Master’ task, let it complete,
3. Stop Ambari server, start it back in a minute
4. Now open EU wizard again

Result:
It shows two tasks under HBASE upgrade group as aborted and continues to next steps. 
EU reached till Finalize screen and allowed to hit Finalize. Thereafter an error for HBase Client was thrown

Expected Result:  Fail EU with ‘HOLDING_TIMEOUT’ status at 'HBASE' Upgrade Group and let user retry the failed task and then move forward",express_upgrade,['ambari-server'],AMBARI,Bug,Blocker,2017-03-02 13:03:20,1
13046324,Ambari should install the unlimited key JCE policy based on service requirements even if cluster is not Kerberized,"Ambari should install the unlimited key JCE policy based on service requirements even if cluster is not Kerberized.  For example, if a service needs the unlimited key JCE policy for an encryption task not related to Kerberos.

On a similar note, the unlimited key JCE policy is not being distributed to the agents if Kerberos is enabled using the ""manual"" option - as opposed to integrating with an existing KDC. 

On a related note, Ambari Server host automatically unpacks JCE unlimited during ""ambari-server setup"" (unless custom JDK is used).
",encryption jce_policy kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Major,2017-02-25 22:12:24,7
13044036,Finalize Operations stage fails when Enabling Kerberos using the manual option,"Finalize Operations stage fails when Enabling Kerberos using the manual option.

{noformat:title=Error Message}
Failed to process the identities, could not properly open the KDC operation handler: Must specify a principal but it is null or empty
{noformat}

",kerberos,['ambari-server'],AMBARI,Bug,Critical,2017-02-17 16:57:32,7
13038652,Inconsistent auth-to-local rules processing during Kerberos authentication,"Facing issue with local to auth rules. 
ambari-qa-cl1@EXAMPLE.COM is getting converted to ambari-qa-cl1 as well as ambari-qa with same ambari configuration ie authentication.kerberos.auth_to_local.rules=DEFAULT.

1st translation : 
{code}
28 Jan 2017 11:44:45,529  INFO [ambari-client-thread-3298] AmbariAuthToLocalUserDetailsService:102 - Translated ambari-qa-cl1@EXAMPLE.COM to ambari-qa-cl1 using auth-to-local rules during Kerberos authentication.
{code}

2nd translation :
{code}
28 Jan 2017 11:47:36,425  INFO [ambari-client-thread-3172] AmbariAuthToLocalUserDetailsService:102 - Translated ambari-qa-cl1@EXAMPLE.COM to ambari-qa using auth-to-local rules during Kerberos authentication.
28 Jan 2017 11:47:36,428  WARN [ambari-client-thread-3172] AmbariAuthToLocalUserDetailsService:136 - Failed find user account for user with username of ambari-qa during Kerberos authentication.
28
{code}

Since authentication.kerberos.auth_to_local.rules=DEFAULT ,  'ambari-qa-cl1@EXAMPLE.COM' should have been translated to 'ambari-qa-cl1'.
",authentication kerberos,['ambari-server'],AMBARI,Bug,Major,2017-01-28 16:10:46,7
13036997,Trailing slash (/) on cluster resource causes incorrect authorization logic flow,"Trailing slash (/) on cluster resource causes incorrect authorization logic flow. It is debatable whether Ambari should allow this, but since it seems to in other cases - like if the user was an Ambari Administrator - this should be fixed. 

The problem occurs in the {{org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter}} where the filter attempts to figure out what the user is trying to get access to.  Since the regular expression for Cluster resources does acknowledge that a trailing ""/"" after the cluster name indicates a cluster, the request does not fall through to the Cluster resource handler ({{org.apache.ambari.server.controller.internal.ClusterResourceProvider}}) for authorization checks.  It uses the legacy logic, which is a little flawed as well.

The fix for this is to allow the trailing ""/"" in the regular expression representing Cluster requests:
{code:title=From org/apache/ambari/server/security/authorization/AmbariAuthorizationFilter.java:70}
  private static final String API_CLUSTERS_PATTERN = API_VERSION_PREFIX + ""/clusters/(\\w+)?"";
{code}

{code:title=To org/apache/ambari/server/security/authorization/AmbariAuthorizationFilter.java:70}
  private static final String API_CLUSTERS_PATTERN = API_VERSION_PREFIX + ""/clusters/(\\w+/?)?"";
{code}
",rbac,['ambari-server'],AMBARI,Bug,Major,2017-01-23 10:10:00,7
13036321,Ldap sync fails when there are special characters in distinguished names,"Ldap sync fails when there are special characters in distinguished names. 

For example if there was a user with the distinguished name of {{OU=test/test,OU=users,DC=EXAMPLE,DC=COM}} and that user was a member of a synced group, then the lookup of the user using the membership attribute in the group would fail due to the special character.  

The error would look something like
{noformat}
REASON: Caught exception running LDAP sync. Uncategorized exception occured during LDAP processing; nested exception is javax.naming.NamingException: [LDAP: error code 1 - 000020D6: SvcErr: DSID-031007DB, problem 5012 (DIR_ERROR), data 0
]; remaining name 'OU=test/test,OU=users,DC=EXAMPLE,DC=COM'
{noformat}

*Solution*
Update the library versionf for Spring LDAP 
* {{org.springframework.security/spring-security-ldap}} to {{4.0.4.RELEASE}}
* {{org.springframework.ldap/spring-ldap-core}} to {{2.0.4.RELEASE}}

Then use {{LdapUtils.newLdapName}} to convert a String representing a DN into a {{javax.naming.ldap.LdapName}} and use that object in the search facility executed in {{org.apache.ambari.server.security.ldap.AmbariLdapDataPopulator#getFilteredLdapUsers(java.lang.String, org.springframework.ldap.filter.Filter)}}. 
",ldap,['ambari-server'],AMBARI,Bug,Critical,2017-01-19 20:03:18,7
13033176,Use common property for principal name prefix to help with customization of unique principal names,"Use common property for principal name prefix to help with customization of unique principal names.  

All _headless_ Kerberos identities have a non-unique principal name (across clusters). To help this issue, the cluster name is appended to these principal names by adding ""-$\{cluster-name|toLower()\}"" after the principal name component. If the user wants to change this convention, they will need to find all _headless_ principals and make the change. On top of that, when adding new components, they will need to remember to make the change to new _headless_ principal names. 

A better solution is to provide a _global_ property named ""principal_suffix"" and use that in each _headless_ principal name. By default the value for this property will be

{code}
principal_suffix=""-${cluster_name|toLower()}""
{code}

If the user would like not use a prefix (in the event there is only a single cluster connecting to the KDC), the value can be changed to

{code}
principal_suffix=""""
{code}

Finally if the user would like to use some other randomizer, they can set the value to something else. For example

{code}
principal_suffix=""_12345""
{code}

The property is set in the Kerberos descriptor's ""properties"" block.   For example:

{code}
{
  ""properties"": {
    ""realm"": ""${kerberos-env/realm}"",
    ...,
    ""principal_suffix"": ""${cluster_name|toLower()}""
  },
  ""identities"": [
    ..., 
    {
      ""name"": ""smokeuser"",
      ""principal"": {
        ""value"": ""${cluster-env/smokeuser}-${principal_suffix}@${realm}"",
        ""type"": ""user"",
        ""configuration"": ""cluster-env/smokeuser_principal_name"",
        ""local_username"": ""${cluster-env/smokeuser}""
      },
      ...
    }
  ],
  ""services"": [
    {
{code} ",kerberos kerberos_descriptor,[],AMBARI,Bug,Major,2017-01-09 20:57:38,7
13032299,Authentication negotiation HTTP response should be sent when Kerberos authentication is enabled,"Authentication negotiation HTTP response should be automatically sent when needed when Kerberos authentication is enabled.  

The expected HTTP response during authentication failure when Kerberos authentication into Ambari is enabled is as follows:
{noformat}
HTTP/1.1 401 Authentication requested
WWW-Authenticate: Negotiate
{noformat}

When Kerberos authentication into Ambari is not enabled the expected HTTP response for an authentication failure is:
{noformat}
HTTP/1.1 403 Missing authentication token
{noformat}

",authentication kerberos security,['ambari-server'],AMBARI,Bug,Major,2017-01-05 16:37:08,7
13032028,Executing ambari-server unit tests with JDK 1.8 results in unit test failures,"Following unit tests fails when executed with JDK 1.8
{code}
mvn test -Dtest=DataStoreImplTest
mvn clean test -Dtest=UpgradeCatalog222Test
mvn test -Dtest=KerberosCheckerTest,AmbariBasicAuthenticationFilterTest
{code}
Same unit tests passes if the environment is switched to use JDK 1.7
",jdk1_8 unit-test,['ambari-server'],AMBARI,Bug,Critical,2017-01-04 17:33:32,7
13029116,ATS reports as down in Ambari UI after upgrade,"Steps
1. Deploy HDP-2.5.0 cluster with Ambari-2.4.1.0
2. Upgrade Ambari to 2.5.0.0-419
3. Restart ATS and observe the status of ATS after sometime in Ambari UI

Result:
ATS shows down. Ambari-agent log shows below:
{code}
INFO 2016-12-08 16:14:44,419 ActionQueue.py:105 - Adding STATUS_COMMAND for component APP_TIMELINE_SERVER of service YARN of cluster cl1 to the queue.
INFO 2016-12-08 16:14:44,621 ActionQueue.py:105 - Adding STATUS_COMMAND for component NODEMANAGER of service YARN of cluster cl1 to the queue.
INFO 2016-12-08 16:14:46,429 PythonReflectiveExecutor.py:65 - Reflective command failed with exception:
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ambari_agent/PythonReflectiveExecutor.py"", line 57, in run_file
    imp.load_source('__main__', script)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 155, in <module>
    ApplicationTimelineServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 282, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/YARN/2.1.0.2.0/package/scripts/application_timeline_server.py"", line 82, in status
    only_if = format(""test -e {yarn_historyserver_pid_file_old}"", user=status_params.yarn_user))
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 155, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 143, in run
    Logger.info_resource(resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 87, in info_resource
    Logger.info(Logger.filter_text(Logger._get_resource_repr(resource)))
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 110, in _get_resource_repr
    return Logger.get_function_repr(repr(resource), resource.arguments, resource)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/logger.py"", line 180, in get_function_repr
    return unicode(""{0} {{{1}}}"", 'UTF-8').format(name, arguments_str)
  File ""/usr/lib64/python2.6/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
AttributeError: 'NoneType' object has no attribute 'utf_8_decode'
INFO 2016-12-08 16:14:53,190 Controller.py:283 - Heartbeat (response id = 15365) with server is running...
INFO 2016-12-08 16:14:55,083 Heartbeat.py:90 - Adding host info/state to heartbeat message.
{code}
",upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-12-19 11:42:28,0
13027933,Add permission for Service Auto Start,"Add permission to give a role the ability to set services to auto-start. This permission should be allowed at the cluster-level to toggle the feature for the cluster (Manage Service Auto Start Configuration) and at the service-level to toggle the feature for specific services (Manage Service Auto Start). However the service-level feature may not be available via all interfaces. 

The following roles should be able to toggle auto-start at the cluster level:
* Ambari Administrator
* Cluster Administrator
* Cluster Operator

The following roles should be able to toggle auto-start at the service level:
* Ambari Administrator
* Cluster Administrator
* Cluster Operator
* Service Administrator",rbac,['ambari-server'],AMBARI,Task,Major,2016-12-13 22:59:48,7
13027427,NPE during Ambari server schema upgrade,"Ambari server upgrade path: 2.2.2.0 to 2.5.0.0-453
Error seen while running 'ambari-server upgrade' command""
{code}
Using python  /usr/bin/python
Upgrading ambari-server
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Upgrade Ambari Server
INFO: Updating Ambari Server properties in ambari.properties ...
INFO: Updating Ambari Server properties in ambari-env.sh ...
WARNING: Original file ambari-env.sh kept
WARNING: Original file krb5JAASLogin.conf kept
INFO: File krb5JAASLogin.conf updated.
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: No mpack replay logs found. Skipping replaying mpack commands
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Fixing database objects owner
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
Ambari Server configured for Oracle. Confirm you have made a backup of the Ambari Server database [y/n] (y)? INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: Upgrading database schema
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: AMBARI_SERVER_LIB is not set, using default /usr/lib/ambari-server
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: using SERVICE_NAME instead of SID for Oracle
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: Loading properties from /etc/ambari-server/conf/ambari.properties
INFO: about to run command: /usr/jdk64/jdk1.7.0_67/bin/java -cp '/etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/share/java/ojdbc6.jar' org.apache.ambari.server.upgrade.SchemaUpgradeHelper > /var/log/ambari-server/ambari-server.out 2>&1
INFO: Return code from schema upgrade command, retcode = 1
ERROR: Error executing schema upgrade, please check the server logs.
ERROR: Error output from schema upgrade command:
ERROR: Exception in thread ""main"" org.apache.ambari.server.AmbariException
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:209)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:424)
Caused by: java.lang.NullPointerException
	at com.google.inject.persist.jpa.JpaPersistService.begin(JpaPersistService.java:70)
	at com.google.inject.persist.jpa.AmbariJpaPersistService.begin(AmbariJpaPersistService.java:29)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:106)
	at org.apache.ambari.server.upgrade.UpgradeCatalog240.executeDDLUpdates(UpgradeCatalog240.java:294)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeSchema(AbstractUpgradeCatalog.java:938)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeUpgrade(SchemaUpgradeHelper.java:206)
	... 1 more


ERROR: Ambari server upgrade failed. Please look at /var/log/ambari-server/ambari-server.log, for more details.
ERROR: Exiting with exit code 11. 
REASON: Schema upgrade failed.
{code}",upgrade,['ambari-upgrade'],AMBARI,Bug,Critical,2016-12-12 11:56:58,5
13026451,hadoop.proxyuser.HTTP.hosts should not be updated when Hive is installed unless WebHcat is installed,"{{hadoop.proxyuser.HTTP.hosts}} should not be updated when Hive is installed unless WebHcat is installed.

This is happening because the following block in the Kerberos descriptor is at the HIVE service level rather than the WEBHCAT_SERVER component level.

{code}
        {
          ""core-site"": {
            ""hadoop.proxyuser.HTTP.hosts"": ""${clusterHostInfo/webhcat_server_host}""
          }
        },
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2016-12-07 19:46:45,7
13026442,"When disabling Kerberos, rm should be used when deleting the Ambari Server keytab file(s)","When disabling Kerberos, {{rm}} should be used when deleting the Ambari Server keytab file(s) rather than the Java {{java.io.File#delete}} method. This is to allow for the file to be removed properly when Ambari is not executed as _root_.  

Currently the Ambari server keytab files may not be removed due to permission issues. 
",kerberos sudo,['ambari-server'],AMBARI,Bug,Major,2016-12-07 19:10:14,5
13024634,Fix NPE in UpgradeCatalog250Test.testExecuteDMLUpdates,"Fix NPE in UpgradeCatalog250Test.testExecuteDMLUpdates

{noformat}
Running org.apache.ambari.server.upgrade.UpgradeCatalog250Test
Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.14 sec <<< FAILURE! - in org.apache.ambari.server.upgrade.UpgradeCatalog250Test
testExecuteDMLUpdates(org.apache.ambari.server.upgrade.UpgradeCatalog250Test)  Time elapsed: 0.272 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.ambari.server.upgrade.UpgradeCatalog250Test.testExecuteDMLUpdates(UpgradeCatalog250Test.java:234)
{noformat}
",unit-test,['ambari-server'],AMBARI,Bug,Blocker,2016-12-01 01:23:02,7
13021846,NPE when authenticating via a Centrify LDAP proxy,"When authenticating using LDAP where the LDAP server is a Centrify LDAP proxy, a {{NullPointerException}} is being thrown due to unexpected missing LDAP user object attributes. 

{noformat}
10 Nov 2016 08:23:38,789 ERROR [ambari-client-thread-25] AmbariLdapBindAuthenticator:95 - Caught exception
java.lang.NullPointerException
	at org.apache.ambari.server.security.authorization.AmbariLdapBindAuthenticator.authenticate(AmbariLdapBindAuthenticator.java:83)
	at org.springframework.security.ldap.authentication.LdapAuthenticationProvider.doAuthentication(LdapAuthenticationProvider.java:178)
	at org.springframework.security.ldap.authentication.AbstractLdapAuthenticationProvider.authenticate(AbstractLdapAuthenticationProvider.java:61)
	at org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProvider.authenticate(AmbariLdapAuthenticationProvider.java:73)
	at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:156)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:168)
	at org.apache.ambari.server.security.authentication.AmbariAuthenticationFilter.doFilter(AmbariAuthenticationFilter.java:88)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariUserAuthorizationFilter.doFilter(AmbariUserAuthorizationFilter.java:91)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.apache.ambari.server.security.AbstractSecurityHeaderFilter.doFilter(AbstractSecurityHeaderFilter.java:109)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82)
	at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1478)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:427)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:212)
	at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:201)
	at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:139)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:641)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

*Cause*
The cause for this {{NPE}} is related to missing data from the attribute search request made to a Centrify LDAP proxy after a bind has occurred.  Since the query filter at this point is ""{(objectClass=*}}"", the Centrify LDAP proxy does not have enough data to determine what information to return to the caller.  However, the filter was something like ""{(objectClass=posixAccount}}"", it will be able to build a set of attributes to return to the caller since it would determine that the call wants data for a specific _profile_.

This can be seen by the following {{tcpdump}} entry:
{code}
LDAPMessage searchRequest(2) ""uid=user1,ou=people,ou=dev,dc=apache,dc=org"" baseObject
    messageID: 2
    protocolOp: searchRequest (3)
        searchRequest
            baseObject: uid=user1,ou=people,ou=dev,dc=apache,dc=org
            scope: baseObject (0)
            derefAliases: derefAlways (3)
            sizeLimit: 0
            timeLimit: 0
            typesOnly: False
            Filter: (objectClass=*)
            attributes: 0 items
    [Response In: 2]
    controls: 1 item
{code}

Note the filter line above: *{{Filter: (objectClass=*)}}*

From the Centrify LDAP proxy logs, the following lines can be seen showing that no mapping is avaialbe:

{noformat}
Nov  8 12:13:45 host1 slapd: cdc search start with filterstr: (objectClass=*)
Nov  8 12:13:45 host1 slapd: cdc search: objectType =  ( is mapped to NONE)
Nov  8 12:13:45 host1 slapd: cdc search after translation filter = (objectClass=*)
{noformat}

This search filter is hardcoded in {{com.sun.jndi.ldap.LdapCtx}} as seen in http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/com/sun/jndi/ldap/LdapCtx.java#1308.

This code is invoked from the Spring LDAP library after attempting to authenticate using the Spring org.springframework.security.ldap.authentication.BindAuthenticator class. 

*Solution*
To solve this, Ambari should avoid using {{org.springframework.security.ldap.authentication.BindAuthenticator}} to authenticate users via LDAP so that more control can be had over when and how user attributes are obtained. 
",ldap,['ambari-server'],AMBARI,Bug,Major,2016-11-18 20:38:33,7
13018986,Optionally force username from LDAP authentication data to be lowercase in Ambari,"Optionally force username from LDAP authentication data to be lowercase in Ambari based on LDAP import configuration.

In some cases the username declared in the relevant LDAP object is in all uppercase characters when the local Hadoop cluster expects the username to be all lowercase. As of Ambari 2.4.0, the username specified from the LDAP data is used to override the username known to Ambari.  This overwritten data may be in all uppercase characters, potentially breaking local username conventions. 

To help this scenario, provide a configuration option to force the username obtained from the LDAP object to be converted to all lowercase character. 

For example {{authentication.ldap.username.forceLowercase}}.

This optional configuration value is to default to false to maintain current functionality. 

",ldap,['ambari-server'],AMBARI,Bug,Major,2016-11-07 20:45:18,7
13011057,Ambari should be able to create arbitrary Kerberos identities for itself as declared in the Kerberos Descriptor,"Ambari should be able to create arbitrary Kerberos identities for itself as declared in the Kerberos Descriptor.

Currently, Ambari is hard-coded to create identities for itself and SPNEGO, but that may not be good enough for all scenarios. Therefore, there needs to be an {{AMBARI}} service block in the Kerberos descriptor to allow for arbitrary identities to be defined for the Ambari server - similar to how any other service  is defined in the Kerberos descriptor. 

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2016-10-10 14:36:02,7
13010152,Kerberos server actions should not timeout in minutes as specified in configuration,"Kerberos related server-side actions should not time out in minutes as specified in configuration.  Some Kerberos-related task can potentially take a much longer time based on number of hosts and components installed in the cluster. 

The {{Create Principals}} and {{Create Keytab Files}} stages need to be set to a rather large timeout value such that

{noformat}
if  server.task.timeout < X 
  timeout = X;
else 
  timeout =  server.task.timeout

Where X is set to something like 10 hours. 
{noformat}

10 hours seems to be a reasonable timeout value since it is not possible to specified an unlimited amount of time give Ambari's current task processing infrastructure. 

",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-10-06 13:29:01,7
13007441,Regression: krb5JAASLogin.conf is not updated during secure BP install,"When installing a secure cluster using Blueprints, Ambari's {{/etc/ambari-server/conf/krb5JAASLogin.conf}} is not updated to reflect the details of the Ambari Kerberos identity.

This was introduced by the patch for AMBARI-18406.
",blueprints kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-09-25 13:32:13,7
13006547,Enforce granular role-based access control for custom actions,"Enforce granular role-based access control for custom actions.  Such actions are specified in {{/var/lib/ambari-server/resources/custom_action_definitions/system_action_definitions.xml}} 

For example:

{code}
  <actionDefinition>
    <actionName>check_host</actionName>
    <actionType>SYSTEM</actionType>
    <inputs/>
    <targetService/>
    <targetComponent/>
    <defaultTimeout>60</defaultTimeout>
    <description>General check for host</description>
    <targetType>ANY</targetType>
    <permissions>HOST.ADD_DELETE_HOSTS</permissions>
  </actionDefinition>
{code}

The ""permissions"" element that declare the permissions required to run the action.  These permissions must be used to authorize a user to perform the operation.  A user needs to have one of the listed permissions in order to be authorized. 

The relevant API entry points are:
* {{/api/v1/requests}}
* {{/api/v1/requests/clusters/:CLUSTER_NAME/request}}

Example:  The user executing the following REST API call must be assigned a role that has the {{HOST.ADD_DELETE_HOSTS}} authorization for the relevant cluster

{noformat}
POST /api/v1/requests
{
  ""RequestInfo"": {
    ""action"": ""check_host"",
    ""log_output"": ""false"",
    ""context"": ""Check host"",
    ""parameters"": {
      ""check_execute_list"": ""last_agent_env_check,installed_packages,existing_repos,transparentHugePage"",
      ""jdk_location"": ""http://host1.example.com:8080/resources/"",
      ""threshold"": ""20""
    }
  },
  ""Requests/resource_filters"": [
    {
      ""hosts"": ""host1.example.com""
    }
  ]
}
{noformat}

",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-09-21 15:51:13,7
13005232,Create authentication filter to perform Kerberos authentication for Ambari,"Users should be able to authenticate to use Ambari by providing a Kerberos token using SPNEGO - Simple and Protected GSSAPI Negotiation Mechanism.  This includes access to the Ambari REST API as well as the Ambari web-based UI. 

The implementation should support the ability to perform the full SPNEGO handshake as well as access requests directly providing the appropriate HTTP header containing the Kerberos token. For example:

{noformat}
Authorization: Negotiate YIICcgY...r/vJcLO
{noformat}

In the full handshake model
# The client requests access to a web resource
# The server responds with an HTTP 401 status ({{Unauthorized}}), including the header {{WWW-Authenticate: Negotiate}}
# The client generates the Kerberos data and creates a new request containing the authentication header - {{Authorization: Negotiate YIICcgY...r/vJcLO}}

Since Ambari needs to generally return a HTTP status of 403 ({{Forbidden}}) when authentication is needed, a _hint_ must be sent along with the request indicate to Ambari that Kerberos authentication is desired.  If this _hint_ is received, then Ambari will respond with the appropriate status and header to initiate SPNEGO with the client. This _hint_ is an Ambari-specific header named ""X-Negotiate-Authentication"" with the value of ""true"":

{noformat}
X-Negotiate-Authentication: true
{noformat}

No matter what the handshake mechanism is (or lack of), once the Kerberos token is received by Ambari, Ambari is to parse and validate the token.  If a failure occurs, Ambari is to respond with the appropriate HTTP status and related header(s).  Upon success, the user's principal name is retrieved and converted into a _local_ user name.  The use of an auth-to-local rule set processor may be needed to perform this translation.  Using this _local_ username, an appropriate Ambari user account is located and used as the authenticated users identity - details, privileges, etc.... Failure to find an appropriate Ambari user account is to result in an authentication failure response.

*To enable this feature*, the following properties may be set in {{ambari.properties}}:

{{authentication.kerberos.enabled}} - Determines whether to use Kerberos (SPNEGO) authentication when connecting Ambari.
* {{true}} - enables this feature
* {{false}} - disables this feature (default)

{{authentication.kerberos.spnego.principal}} - The Kerberos principal name to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO.
* HTTP/_HOST  (default)

authentication.kerberos.spnego.keytab.file - The Kerberos keytab file to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO.
*  /etc/security/keytabs/spnego.service.keytab (default)

{{authentication.kerberos.user.types}} - A comma-delimited (ordered) list of preferred user types to use when finding the Ambari user account for the user-supplied Kerberos identity during authentication via SPNEGO.
* LDAP (default)

{{authentication.kerberos.auth_to_local.rules}} - The auth-to-local rules set to use when translating a user's principal name to a local user name during authentication via SPNEGO.
* DEFAULT (default)
* Rules are to be separated by ""/n"":
{noformat}
authentication.kerberos.auth_to_local.rules=RULE:[1:$1@$0](ambari-server-c1@EXAMPLE.COM)s/.*/admin/\nDEFAULT
{noformat}



",authentication kerberos security,['ambari-server'],AMBARI,Task,Major,2016-09-15 18:11:15,7
13004430,Create authentication filter to encapsulate the various Ambari authentication methods,"Create a Spring authentication filter to encapsulate the various Ambari authentication methods since the Spring filter chain allows for a single authentication filter and Ambari needs to allow for multiple, optional, authentication filters to handle one of (but not limited to) the following authentication methods:

* Basic Auth 
* SSO (JWT)
* Kerberos token

",authentication kerberos security,['ambari-server'],AMBARI,Task,Major,2016-09-12 21:35:02,7
13004382,Add Ambari configuration options to support Kerberos token authentication,"Add the followng Ambari configuration options to support Kerberos token authentication

* {{authentication.kerberos.enabled}}
** Determines whether to use Kerberos (SPNEGO) authentication when connecting Ambari:  {{true}} to enable this feature; {{false}}, otherwise
* {{authentication.kerberos.spnego.principal}}
** The Kerberos principal name to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO
* {{authentication.kerberos.spnego.keytab.file}}
** The Kerberos keytab file to use when verifying user-supplied Kerberos tokens for authentication via SPNEGO
* {{authentication.kerberos.user.types}}
** A comma-delimited (ordered) list of preferred user types to use when finding the Ambari user account for the user-supplied Kerberos identity during authentication via SPNEGO
* {{authentication.kerberos.auth_to_local.rules}}
** The auth-to-local rules set to use when translating a user's principal name to a local user name during authentication via SPNEGO.

NOTE: These properties are in the {{ambari.properties}} file since this feature may be enabled whether the rest of the cluster has Kerberos enabled or not. 
",authentication kerberos security,['ambari-server'],AMBARI,Task,Major,2016-09-12 18:29:20,7
13004380,Ambari authentication with Kerberos token,"Users should be able to authenticate to use Ambari by providing a Kerberos token using SPNEGO - Simple and Protected GSSAPI Negotiation Mechanism.  This includes access to the Ambari REST API as well as the Ambari web-based UI. 

The implementation should support the ability to perform the full SPNEGO handshake as well as access requests directly providing the appropriate HTTP header containing the Kerberos token. For example:

{noformat}
Authorization: Negotiate YIICcgY...r/vJcLO
{noformat}

In the full handshake model
# The client requests access to a web resource
# The server responds with an HTTP 401 status ({{Unauthorized}}), including the header {{WWW-Authenticate: Negotiate}}
# The client generates the Kerberos data and creates a new request containing the authentication header - {{Authorization: Negotiate YIICcgY...r/vJcLO}}

Since Ambari needs to generally return a HTTP status of 403 ({{Forbidden}}) when authentication is needed, a _hint_ must be sent along with the request indicate to Ambari that Kerberos authentication is desired.  If this _hint_ is received, then Ambari will respond with the appropriate status and header to initiate SPNEGO with the client. This _hint_ is an Ambari-specific header named ""X-Negotiate-Authentication"" with the value of ""true"":

{noformat}
X-Negotiate-Authentication: true
{noformat}

No matter what the handshake mechanism is (or lack of), once the Kerberos token is received by Ambari, Ambari is to parse and validate the token.  If a failure occurs, Ambari is to respond with the appropriate HTTP status and related header(s).  Upon success, the user's principal name is retrieved and converted into a _local_ user name.  The use of an auth-to-local rule set processor may be needed to perform this translation.  Using this _local_ username, an appropriate Ambari user account is located and used as the authenticated users identity - details, privileges, etc.... Failure to find an appropriate Ambari user account is to result in an authentication failure response.




",authentication kerberos security,['ambari-server'],AMBARI,Epic,Major,2016-09-12 18:27:20,7
13004379,Ambari authentication with Kerberos token,"Users should be able to authenticate to use Ambari by providing a Kerberos token using SPNEGO - Simple and Protected GSSAPI Negotiation Mechanism.  This includes access to the Ambari REST API as well as the Ambari web-based UI. 

The implementation should support the ability to perform the full SPNEGO handshake as well as access requests directly providing the appropriate HTTP header containing the Kerberos token. For example:

{noformat}
Authorization: Negotiate YIICcgY...r/vJcLO
{noformat}

In the full handshake model
# The client requests access to a web resource
# The server responds with an HTTP 401 status ({{Unauthorized}}), including the header {{WWW-Authenticate: Negotiate}}
# The client generates the Kerberos data and creates a new request containing the authentication header - {{Authorization: Negotiate YIICcgY...r/vJcLO}}

Since Ambari needs to generally return a HTTP status of 403 ({{Forbidden}}) when authentication is needed, a _hint_ must be sent along with the request indicate to Ambari that Kerberos authentication is desired.  If this _hint_ is received, then Ambari will respond with the appropriate status and header to initiate SPNEGO with the client. This _hint_ is an Ambari-specific header named ""X-Authentication-Type"" with the value of ""kerberos"":

{noformat}
X-Authentication-Type: kerberos
{noformat}

No matter what the handshake mechanism is (or lack of), once the Kerberos token is received by Ambari, Ambari is to parse and validate the token.  If a failure occurs, Ambari is to respond with the appropriate HTTP status and related header(s).  Upon success, the user's principal name is retrieved and converted into a _local_ user name.  The use of an auth-to-local rule set processor may be needed to perform this translation.  Using this _local_ username, an appropriate Ambari user account is located and used as the authenticated users identity - details, privileges, etc.... Failure to find an appropriate Ambari user account is to result in an authentication failure response.




",authentication kerberos security,['ambari-server'],AMBARI,Epic,Major,2016-09-12 18:27:10,7
13004321,"All classes recompiled due to Maven bug, even if none changed","maven-compiler-plugin version 3.0 has a [bug|https://issues.apache.org/jira/browse/MCOMPILER-187] that causes all classes to be recompiled even if no classes have changed.

{noformat}
[INFO] --- maven-compiler-plugin:3.0:compile (default-compile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1720 source files ...
...
[INFO] --- maven-compiler-plugin:3.0:testCompile (default-testCompile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 760 source files ...
{noformat}

version 3.1+ has [another, related bug|https://issues.apache.org/jira/browse/MCOMPILER-209] that causes all classes to be compiled even if only one has changed, although it seems to correctly detect the ""no change"" case.

{noformat}
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ambari-server ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 1720 source files ...
{noformat}",pull-request-available,['ambari-server'],AMBARI,Bug,Minor,2016-09-12 15:27:33,5
13003348,After upgrading cluster from HDP-2.4.x to HDP-2.5.x and added atlas service - missing kafka security properties,"Steps to repro:
* Install Ambari 2.2.2
* Install HDP-2.4.x cluster with Atlas
* Stop Atlas
* Upgrade Ambari to 2.4
* Delete Atlas service
* Upgrade the cluster to HDP-2.5.x cluster
* Add Atlas service.

*Below config properties are missing from atlas-applicataion.properties file for Atlas, Storm, Falcon, Hive services.*
#atlas.jaas.KafkaClient.option.keyTab = /etc/security/keytabs/atlas.service.keytab
#atlas.jaas.KafkaClient.option.principal = atlas/_HOST@EXAMPLE.COM

From HDP 2.4 to 2.5, the kerberos.json file for Atlas changed.
",kerberos_descriptor upgrade,['ambari-server'],AMBARI,Bug,Blocker,2016-09-07 18:34:31,7
12996898,Enable Namenode HA failing at install journal nodes with cluster operator user,"Enabling name node HA is failing at journal node install for cluster operator user. Looking at network from the chrome browser, looks like there is a 403.",240RMApproved rbac,['ambari-server'],AMBARI,Bug,Blocker,2016-08-12 09:51:20,1
12996288,Add Kerberos Automation documentation to Ambari source tree so it may be versioned ,"Add Kerberos Automation documentation to Ambari source tree so it may be versioned.  This documentation should be added as MD (markdown) files to {{.../ambari-server/docs/security/kerberos}}.
",documentation kerberos kerberos_descriptor,['ambari-server'],AMBARI,Documentation,Minor,2016-08-10 14:17:32,7
12994361,Kerberos identity definitions in Kerberos descriptors should explicitly declare a reference,"Kerberos identity definitions in Kerberos descriptors should explicitly declare a reference rather than rely on the identity's _name_ attribute. 

Currently, the set of Kerberos identities declared at a service-level or a component-level can contain identities with unique names.  For example using:

{code}
  ""identities"": [
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property1.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property1.keytab"",
        ...
      }
    },
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property2.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property2.keytab"",
        ...
      }
    }
  ]
{code}

Only the first ""identity"" principal is realized and the additional one is ignored, leaving the configurations {{service-site/property2.principal}} and {{service-site/property2.keytab}} untouched when Kerberos is enabled for the service. 

To help this, the 2nd instance can be converted to a reference, overriding only the attributes the need to be changed - like the configurations. 

{code}
  ""identities"": [
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property1.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property1.keytab"",
        ...
      }
    },
    {
      ""name"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property2.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property2.keytab""
      }
    }
  ]
{code}

This allows for both identity declarations to be realized, however this is limited to only the 2 instances. If a 3rd instance is needed (to set an additional configuration), it must look be:

{code}
    {
      ""name"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property3.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property3.keytab""
      }
    }
{code}

However since it's name is the same as the 2nd instance, it will be ignored. 

If explicit references are specified, then multiple uniquely-named identity blocks will be allowed to reference the same base identity, effectively enabling the ability to declare unlimited configurations for the same identity definition:

{code}
  ""identities"": [
    {
      ""name"": ""identity"",
      ""principal"": {
        ""value"": ""service/_HOST@${realm}"",
        ""configuration"": ""service-site/property1.principal"",
        ...
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/service.service.keytab"",
        ""configuration"": ""service-site/property1.keytab"",
        ...
      }
    },
    {
      ""name"": ""identitiy_reference1"",
      ""reference"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property2.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property2.keytab""
      }
    },
    {
      ""name"": ""identitiy_reference2"",
      ""reference"": ""/SERVICE/identity"",
      ""principal"": {
        ""configuration"": ""service-site/property3.principal""
      },
      ""keytab"": {
        ""configuration"": ""service-site/property3.keytab""
      }
    }
  ]
{code}

NOTE: Backwards compatibility must be maintained when implementing this as to not break existing Kerberos descriptors. So identity block names the look like paths are to continue to be treated as references. 
",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-08-02 18:49:47,7
12994251,Allow multiple configurations for a Kerberos identity principal and keytab definition,"Allow multiple configurations for a Kerberos identity principal and keytab definition.

Currently we allow:

{code}
{
              ""name"": ""some_name"",
              ""principal"": {
                ""value"": ""foobar/_HOST@${realm}"",
                ""type"" : ""service"",
                ""configuration"": ""config1/property1"",
                ""local_username"" : ""${hadoop-env/hdfs_user}""
              },
              ""keytab"": {
                ""file"": ""${keytab_dir}/foobar.service.keytab"",
                ""owner"": {
                  ""name"": ""${config-env/foobar_user}"",
                  ""access"": ""r""
                },
                ""group"": {
                  ""name"": ""${cluster-env/user_group}"",
                  ""access"": """"
                },
                ""configuration"": ""config1/property2""
              }
            },
{code}

but we should allow for 

{code}
{
              ""name"": ""some_name"",
              ""principal"": {
                ""value"": ""foobar/_HOST@${realm}"",
                ""type"" : ""service"",
                ""configurations"": [""config1/property1"", ""config2/propertyA""],
                ""local_username"" : ""${hadoop-env/hdfs_user}""
              },
              ""keytab"": {
                ""file"": ""${keytab_dir}/foobar.service.keytab"",
                ""owner"": {
                  ""name"": ""${config-env/foobar_user}"",
                  ""access"": ""r""
                },
                ""group"": {
                  ""name"": ""${cluster-env/user_group}"",
                  ""access"": """"
                },
                ""configurations"":[ ""config1/property2"",  ""config2/propertyB""]
              }
            },
{code}",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-08-02 11:22:05,7
12993593,Coverity Scan Security Vulnerability - SQL injection,"The Ambari coverity scan found two ""High impact security"" issues, both SQL Injections.  They are both the same coding issue, but one is in OracleConnector.java, and one is in the analogous method in PostgresConnector.java.

This is the key description:
{quote}
 CID 167755 (#1 of 1): SQL injection (SQLI)9. sql_taint: Insecure concatenation of a SQL statement. The value searchClause is tainted.

Perform one of the following to guard against SQL injection attacks.
* Parameterize the SQL statement using ? positional characters. Bind the tainted values to the ? positional parameters using one of the PreparedStatement.set* methods.
* Validate user-supplied values against predefined constant values. Concatenate these constant values into the SQL statement.
* Cast tainted values to safe types such as integers. Concatenate these type safe values into the statement.

[More Information|https://scan3.coverity.com/doc/en/cov_checker_ref.html#id_sql_generic]
{quote}

This is the one in OracleConnector.java, lines 32 -55:

{code}
32  @Override
  8. taint_path_param: Parameter searchClause receives the tainted data.
33  protected PreparedStatement getQualifiedPS(Statements statement, String searchClause, Workflows.WorkflowDBEntry.WorkflowFields field, boolean sortAscending, int offset, int limit) throws IOException {
34    if (db == null)
35      throw new IOException(""db not initialized"");
36
37    String order = "" ORDER BY "" + field.toString() + "" "" + (sortAscending ? SORT_ASC : SORT_DESC);
38
39    String query = ""select * \n"" +
40        ""  from ( select "" +
41//        ""/*+ FIRST_ROWS(n) */ \n"" +
42        ""  a.*, ROWNUM rnum \n"" +
43        ""      from (""
  CID 167755 (#1 of 1): SQL injection (SQLI)9. sql_taint: Insecure concatenation of a SQL statement. The value searchClause is tainted.
  Perform one of the following to guard against SQL injection attacks.

    Parameterize the SQL statement using ? positional characters. Bind the tainted values to the ? positional parameters using one of the PreparedStatement.set* methods.
    Validate user-supplied values against predefined constant values. Concatenate these constant values into the SQL statement.
    Cast tainted values to safe types such as integers. Concatenate these type safe values into the statement.

More Information
44        + statement.getStatementString() + searchClause + order +
45        "") a \n"" +
46        ""      where ROWNUM <= "" + (offset + limit) + "") \n"" +
47        ""where rnum  >= "" + offset;
48
49    try {
  10. sql_sink: Passing the tainted value query to the SQL API java.sql.Connection.prepareStatement(java.lang.String) may allow an attacker to inject SQL.
50      return db.prepareStatement(query);
51    } catch (SQLException e) {
52      throw new IOException(e);
53    }
54
55  }
{code}

This is the one in PostgresConnector.java, lines 495-504:

{code}
 
   8. taint_path_param: Parameter searchClause receives the tainted data.
495  protected PreparedStatement getQualifiedPS(Statements statement, String searchClause) throws IOException {
496    if (db == null)
497      throw new IOException(""postgres db not initialized"");
498    try {
499      // LOG.debug(""preparing "" + statement.getStatementString() + searchClause);
   CID 167743 (#1 of 1): SQL injection (SQLI)9. sql_taint: Insecure concatenation of a SQL statement. The value searchClause is tainted. Passing the tainted command to the SQL API java.sql.Connection.prepareStatement(java.lang.String) may allow an attacker to inject SQL.
   Perform one of the following to guard against SQL injection attacks.

    Parameterize the SQL statement using ? positional characters. Bind the tainted values to the ? positional parameters using one of the PreparedStatement.set* methods.
    Validate user-supplied values against predefined constant values. Concatenate these constant values into the SQL statement.
    Cast tainted values to safe types such as integers. Concatenate these type safe values into the statement.

More Information
500      return db.prepareStatement(statement.getStatementString() + searchClause);
501    } catch (SQLException e) {
502      throw new IOException(e);
503    }
504  }
{code}

*Solution*
Remove code supporting an unsupported REST API call to obtain jobtracker information.  his entry point is handled by {{org.apache.ambari.eventdb.webservice.WorkflowJsonService}}.  By removing this class and cleaning up orphaned code, the SQL injection issue list above will be solved. 

",coverity security,['ambari-server'],AMBARI,Bug,Critical,2016-07-29 17:13:13,7
12993066,Fix SQOOP Kerberos descriptor to contain a 'component' block,"Fix SQOOP Kerberos descriptor to contain a 'component' block. Without this block, a NPE is being thrown.  A {{null}} check should also be performed to avoid future issues. 
",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2016-07-27 20:54:06,7
12992357,"Stack advisor property removal recommendations are ignored when regenerating keytabs, leaving properties with empty values","Stack advisor property removal recommendations are ignored when regenerating keytabs, leaving properties with empty values.  

This causes (validation) issues when properties expected to be removed exist but have empty values. ",kerberos stack_advisor,['ambari-server'],AMBARI,Bug,Major,2016-07-25 17:53:55,7
12992105,Cluster Administrator role is unable to perform 'Install Packages' operation,"*Steps*
# Create a user and assign ""Cluster Administrator"" role
# As an admin, Register a new version
# Logout and login as the user created in step 1
# Try to perform ""Install packages"" operation from UI

*Result*
""status"" : 403,
""The authenticated user does not have the appropriate authorizations to create the requested resource(s)""
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-07-24 17:33:56,7
12991103,Kerberos Client fails to install,"Log
{noformat}
Traceback (most recent call last):
    File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 80, in <module>
      KerberosClient().execute()
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 280, in execute
      method(env)
    File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 28, in install
      self.install_packages(env)
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 545, in install_packages
      if Script.check_package_condition(package):
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 583, in check_package_condition
      return chooser_method()
    File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/package_conditions.py"", line 93, in should_install_kerberos_server
      return 'role' in config and not _has_applicable_local_component(""KERBEROS_CLIENT"")
  TypeError: _has_applicable_local_component() takes exactly 2 arguments (1 given)
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-07-20 12:45:08,7
12990765,Adding new components for existing services should process Kerberos-related configuration changes,"When adding new components for existing services, Kerberos-related configuration changes should be processed.  This should happen when an new component, not a new instance of an existing component is installed. 

Care must be taken to not reintroduce the issue described in AMBARI-14506 or if unavoidable, proper documentation should be added 
",kerbeos,['ambari-server'],AMBARI,Bug,Major,2016-07-19 13:42:06,7
12990485,Kerberos-related configs are not applied before INSTALL command is built on add service,"Kerberos-related configs are not applied before INSTALL command is built on add service.  

This occurs when new services and components are added to an existing cluster where Kerberos is enabled. Due to the order of when command detail (JSON) structures are built and when service configurations are set, Kerberos-related configurations will not be available for new services.  This is a regression created by the patch for AMBARI-17629.

*Solution*
Ensure that Kerberos-related configuration updates are applied before the INSTALL command details are created. 

",kerberos regression,['ambari-server'],AMBARI,Bug,Critical,2016-07-18 17:42:31,7
12989852,Cluster user role is permitted to install packages using API,"With ""Cluster User"" role, submitting ""install packages"" API call goes through, even though it should be blocked

{code}
#curl -u cu:1234 -H ""X-Requested-By: ambari"" -i -X  POST http://ambari-server:8080/api/v1/clusters/cl1/stack_versions -d '{""ClusterStackVersions"":{""stack"":""HDP"",""version"":""2.3"",""repository_version"":""2.3.0.0""}}'
HTTP/1.1 202 Accepted
Date: Wed, 29 Jun 2016 05:55:16 GMT
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Set-Cookie: AMBARISESSIONID=11njwu8py6m511511liub068vj;Path=/;HttpOnly
Expires: Thu, 01 Jan 1970 00:00:00 GMT
User: cu
Content-Type: text/plain
Vary: Accept-Encoding, User-Agent
Content-Length: 136
Server: Jetty(9.2.11.v20150529)

{
  ""href"" : ""http://ambari-server:8080/api/v1/clusters/cl1/requests/36"",
  ""Requests"" : {
    ""id"" : 36,
    ""status"" : ""Accepted""
  }
}
{code}

Role of the user ""cu""
{code}
{
  ""href"" : ""http://ambari-server:8080/api/v1/users/cu/privileges/7"",
  ""PrivilegeInfo"" : {
    ""cluster_name"" : ""cl1"",
    ""permission_label"" : ""Cluster User"",
    ""permission_name"" : ""CLUSTER.USER"",
    ""principal_name"" : ""cu"",
    ""principal_type"" : ""USER"",
    ""privilege_id"" : 7,
    ""type"" : ""CLUSTER"",
    ""user_name"" : ""cu""
  }
}
{code}
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-07-15 16:16:13,7
12988072,Changes to stack advisor framework to help with service advisors,"The following changes are needed in the stack advisor framework to help with service advisors:

* Add additional logging to show why a service advisor implementation was not loaded
* Move {{isSecurityEnabled}} from {{stacks/HDP/2.0.6/services/stack_advisor.py}} to a class member of {{DefaultStackAdvisor}} in {{stacks/stack_advisor.py}} so that all stack and service advisors may be able to use it
",service_advisor stack_advisor,['ambari-server'],AMBARI,Bug,Major,2016-07-09 20:59:49,7
12987891,AUTH_TO_LOCAL rules are not updated when adding services to a Blueprint-installed cluster,"When adding new services and components to a cluster that was initially created via Blueprints (rather than via the Ambari UI), auth-to-local rules that are expected to be created as indicated by the Kerberos descriptor are not.

It occurs because the components being installed are in the {{INIT}} state where the logic to determine whether to include the new auth-to-local rules or not expects the components to be in either the {{INSTALLED}} or {{STARTED}} states. This is due to logic added when resolving AMBARI-14232.

*Solution*:
Allow for auth-to-local rules for new services and components to be added when the state of the components are  {{INIT}} as well as {{INSTALLED}} and {{STARTED}}, when the cluster was installed via Blueprints. 
",auth_to_local kerberos,['ambari-server'],AMBARI,Bug,Critical,2016-07-08 18:48:35,7
12985006,authorizer.class.name not being set on secure kafka clusters,"The {{kafka-broker/authorizer.class.name}} property is not being set properly when Kerberos is enabled.

The following logic should be followed:
{noformat}
if Kerberos is enabled
  if ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled == yes
    set authorizer.class.name to ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer""
  else
    set authorizer.class.name to ""kafka.security.auth.SimpleAclAuthorizer""
else
  remove authorizer.class.name
{noformat}

This should be updated in the stack advisor code. 

While at it, configurations from Kafka's {{kerberos.json}} file should be moved to the stack advisor to help ensure properties are set in the the same place to help with code maintenance and consistency.
",kerberos kerberos_descriptor stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2016-06-29 13:37:39,7
12985004,authorizer.class.name not being set on secure kafka clusters,"The {{kafka-broker/authorizer.class.name}} property is not being set properly when Kerberos is enabled.

The following logic should be followed:
{noformat}
if Kerberos is enabled
  if ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled == yes
    set authorizer.class.name to ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer""
  else
    set authorizer.class.name to ""kafka.security.auth.SimpleAclAuthorizer""
else
  remove authorizer.class.name
{noformat}

This should be updated in the stack advisor code. 

While at it, configurations from Kafka's {{kerberos.json}} file should be moved to the stack advisor to help ensure properties are set in the the same place to help with code maintenance and consistency.
",kerberos kerberos_descriptor stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2016-06-29 13:36:29,7
12985003,authorizer.class.name not being set on secure kafka clusters,"The {{kafka-broker/authorizer.class.name}} property is not being set properly when Kerberos is enabled.

The following logic should be followed:
{noformat}
if Kerberos is enabled
  if ranger-kafka-plugin-properties/ranger-kafka-plugin-enabled == yes
    set authorizer.class.name to ""org.apache.ranger.authorization.kafka.authorizer.RangerKafkaAuthorizer""
  else
    set authorizer.class.name to ""kafka.security.auth.SimpleAclAuthorizer""
else
  remove authorizer.class.name
{noformat}

This should be updated in the stack advisor code. 

While at it, configurations from Kafka's {{kerberos.json}} file should be moved to the stack advisor to help ensure properties are set in the the same place to help with code maintenance and consistency.
",kerberos kerberos_descriptor stack_advisor,['ambari-server'],AMBARI,Bug,Blocker,2016-06-29 13:34:48,7
12981050,"AmbariServer looks for ldap_url, container_dn in blueprint even for MIT security type","{{ldap_url}} and {{container_dn}} are expected for MIT security type.  They should only be required for AD integration. 

{code:title=Example BP}
  {
    ""configurations"": [
      {
        ""cluster-env"": {
          ""properties"": {
            ""command_retry_max_time_in_sec"": ""1200""
          }
        }
      },
      {
        ""kerberos-env"": {
          ""properties"": {
            ""realm"": ""EXAMPLE.COM"",
            ""kdc_type"": ""mit-kdc"",
            ""kdc_hosts"": ""kdc.example.com"",
            ""admin_server_host"": ""kdc.example.com"",
            ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
            ""service_check_principal_name"": ""cl1-QutreRP8p3""
          }
        }
      },
      {
        ""krb5-conf"": {
          ""properties"": {
            ""domains"": """",
            ""manage_krb5_conf"": ""true""
          }
        }
      }
    ],
    ""host_groups"": [
      {
        ""name"": ""host1"",
        ""cardinality"": ""1"",
        ""components"": [
          {
            ""name"": ""DATANODE""
          },
          {
            ""name"": ""NFS_GATEWAY""
          },
          {
            ""name"": ""HDFS_CLIENT""
          },
          {
            ""name"": ""NODEMANAGER""
          },
          {
            ""name"": ""YARN_CLIENT""
          },
          {
            ""name"": ""MAPREDUCE2_CLIENT""
          },
          {
            ""name"": ""HBASE_REGIONSERVER""
          },
          {
            ""name"": ""HBASE_CLIENT""
          },
          {
            ""name"": ""PHOENIX_QUERY_SERVER""
          },
          {
            ""name"": ""HIVE_CLIENT""
          },
          {
            ""name"": ""HCAT""
          },
          {
            ""name"": ""OOZIE_CLIENT""
          },
          {
            ""name"": ""ZOOKEEPER_CLIENT""
          },
          {
            ""name"": ""SUPERVISOR""
          },
          {
            ""name"": ""FALCON_CLIENT""
          },
          {
            ""name"": ""FLUME_HANDLER""
          },
          {
            ""name"": ""METRICS_MONITOR""
          },
          {
            ""name"": ""RANGER_TAGSYNC""
          },
          {
            ""name"": ""TEZ_CLIENT""
          },
          {
            ""name"": ""PIG""
          },
          {
            ""name"": ""SQOOP""
          },
          {
            ""name"": ""SLIDER""
          },
          {
            ""name"": ""KERBEROS_CLIENT""
          },
          {
            ""name"": ""MAHOUT""
          },
          {
            ""name"": ""HST_AGENT""
          },
          {
            ""name"": ""LOGSEARCH_LOGFEEDER""
          },
          {
            ""name"": ""LOGSEARCH_SOLR_CLIENT""
          }
        ]
      }
    ],
    ""Blueprints"": {
      ""blueprint_name"": ""bp1"",
      ""stack_name"": ""HDP"",
      ""stack_version"": ""2.5""
    }
  }
{code}

{noformat}
curl -H ""X-Requested-By:ambari"" -u admin:admin -i -X  POST -d @./bp1.json http://localhost:8080/api/v1/blueprints/bp1
HTTP/1.1 100 Continue

HTTP/1.1 400 Bad Request
Date: Mon, 20 Jun 2016 19:02:27 GMT
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Set-Cookie: AMBARISESSIONID=1a4dqzhedwoog4xg8jbu36e2q;Path=/;HttpOnly
Expires: Thu, 01 Jan 1970 00:00:00 GMT
User: admin
Content-Type: text/plain
Content-Length: 227
Server: Jetty(9.2.11.v20150529)

{
  ""status"" : 400,
  ""message"" : ""Blueprint configuration validation failed: Missing required properties.  Specify a value for these properties in the blueprint configuration. {host1={kerberos-env=[ldap_url, container_dn]}}""
}
{noformat}
",kerberos,[],AMBARI,Bug,Major,2016-06-20 19:03:52,7
12977848,Add SERVICE.VIEW_OPERATIONAL_LOGS authorization to SERVICE.ADMINISTRATOR role and above,"Add SERVICE.VIEW_OPERATIONAL_LOGS authorization to the following roles:

* AMBARI.ADMINISTRATOR 
* CLUSTER.ADMINISTRATOR 
* CLUSTER.OPERATOR 
* SERVICE.ADMINISTRATOR

This is a DB change adding an authorization record to the {{roleauthorization}} table and relevant records for the different roles to the {{permission_roleauthorization}} table. 

The description/name of the {{SERVICE.VIEW_OPERATIONAL_LOGS}} authorization should be
{noformat}
View service operational logs
{noformat}
",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-06-11 10:29:17,7
12975730,Fix description of SERVICE.ADD_DELETE_SERVICES permission,"The description of the SERVICE.ADD_DELETE_SERVICES permission currently reads

{quote}
Add Service to cluster
{quote}

This should be changed to

{quote}
Add/delete services
{quote}
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-06-04 10:55:45,7
12975482,Cluster operator and ServiceAdministrator not allowed to create config group,"Cluster operator and ServiceAdministrator are not allowed to create a config group even though it should be.

It is visible in the UI but creating it gives a 403 error in the backend and the UI goes for a toss.",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-06-03 13:29:20,7
12975218,Service admin and cluster operator can't modify service configs through API,"Using a serviceoperator user, trying to modify config using the 2 step modification process :

Request type : POST
Request URL : /api/v1/clusters/cl1/configurations
Auth : serviceadminuser/password
Request Body :
{code}
{""type"":""ams-env"",""tag"":""version146474298002"",""properties"":{""ambari_metrics_user"":""ams"",""metrics_monitor_log_dir"":""/grid/0/log/metric_monitor_updated"",""metrics_collector_heapsize"":""512"",""metrics_collector_pid_dir"":""/var/run/ambari-metrics-collector"",""metrics_collector_log_dir"":""/grid/0/log/metric_collector"",""metrics_monitor_pid_dir"":""/var/run/ambari-metrics-monitor"",""content"":""\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME\u003d{{java64_home}}\n\n# Collector Log directory for log4j\nexport AMS_COLLECTOR_LOG_DIR\u003d{{ams_collector_log_dir}}\n\n# Monitor Log directory for outfile\nexport AMS_MONITOR_LOG_DIR\u003d{{ams_monitor_log_dir}}\n\n# Collector pid directory\nexport AMS_COLLECTOR_PID_DIR\u003d{{ams_collector_pid_dir}}\n\n# Monitor pid directory\nexport AMS_MONITOR_PID_DIR\u003d{{ams_monitor_pid_dir}}\n\n# AMS HBase pid directory\nexport AMS_HBASE_PID_DIR\u003d{{hbase_pid_dir}}\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE\u003d{{metrics_collector_heapsize}}\n\n# HBase normalizer enabled\nexport AMS_HBASE_NORMALIZER_ENABLED\u003d{{ams_hbase_normalizer_enabled}}\n\n# HBase compaction policy enabled\nexport AMS_HBASE_FIFO_COMPACTION_ENABLED\u003d{{ams_hbase_fifo_compaction_enabled}}\n\n# HBase Tables Initialization check enabled\nexport AMS_HBASE_INIT_CHECK_ENABLED\u003d{{ams_hbase_init_check_enabled}}\n\n# AMS Collector options\nexport AMS_COLLECTOR_OPTS\u003d\""-Djava.library.path\u003d/usr/lib/ams-hbase/lib/hadoop-native\""\n{% if security_enabled %}\nexport AMS_COLLECTOR_OPTS\u003d\""$AMS_COLLECTOR_OPTS -Djava.security.auth.login.config\u003d{{ams_collector_jaas_config_file}}\""\n{% endif %}\n\n# AMS Collector GC options\nexport AMS_COLLECTOR_GC_OPTS\u003d\""-XX:+UseConcMarkSweepGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{ams_collector_log_dir}}/collector-gc.log-`date +\u0027%Y%m%d%H%M\u0027`\""\nexport AMS_COLLECTOR_OPTS\u003d\""$AMS_COLLECTOR_OPTS $AMS_COLLECTOR_GC_OPTS\""\n\n    ""}}
{code}

Request response : 
{code}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2016-06-02 16:28:01,7
12972445,Cluster operator and cluster admin not allowed to install ambari agent,"Cluster operator and the cluster admin must be allowed to add/delete hosts but install of agents using /bootstrap fails with 403
",rbac,['ambari-server'],AMBARI,Bug,Critical,2016-05-24 17:00:55,7
12972325,Any start command fails if AMS is installed.,"
    Traceback (most recent call last):
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 39, in <module>
        BeforeStartHook().execute()
      File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 254, in execute
        method(env)
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py"", line 28, in hook
        import params
      File ""/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/params.py"", line 145, in <module>
        metric_collector_hosts += host + ':' + metric_collector_port + ','
    TypeError: unsupported operand type(s) for +=: 'NoneType' and 'str'
    Error: Error: Unable to run the custom hook script ['/usr/bin/python', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START/scripts/hook.py', 'START', '/var/lib/ambari-agent/data/command-9.json', '/var/lib/ambari-agent/cache/stacks/HDP/2.0.6/hooks/before-START', '/var/lib/ambari-agent/data/structured-out-9.json', 'INFO', '/var/lib/ambari-agent/tmp']
    

",ambari-metrics ambari-server,[],AMBARI,Bug,Major,2016-05-24 09:08:15,0
12971378,'Configure Ambari Identity' fails when enabling Kerberos on non-root Ambari server,"Configure Ambari Identity phase of Enable Kerberos Wizard fails since Ambari does not have permission perform some root-level file system tasks like chown. 

STR:
# Install Ambari, executing as non-root
# Create cluster (Zookeeper-only is fine)
# Enable Kerberos (any KDC is fine)

",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-05-20 02:16:09,7
12969507,yarn.timeline-service.enabled is set to false in secure cluster ,"yarn.timeline-service.enabled  property is set to false when Kerberos is enabled. It should set set to true. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-05-13 10:23:36,7
12967386,"RBAC: Change permission_label for role AMBARI.ADMINISTRATOR to ""Ambari Admin""","Change permission_label for role AMBARI.ADMINISTRATOR from ""Administrator"" to ""Ambari Admin""
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-05-11 12:11:22,7
12967161,Add conditional constraints for Kerberos identities to control when they are created,"Add conditional constraints for Kerberos identities to control when they are created. For example if Kerberos Identity should only be created (and distributed) for a component when some other component or service is installed. 

An example of this would be
{code}
{
  ""name"": ""/HIVE/HIVE_SERVER/hive_server_hive"",
  ""principal"": {
    ""configuration"": ""hive-interactive-site/hive.llap.daemon.service.principal""
  },
  ""keytab"": {
    ""configuration"": ""hive-interactive-site/hive.llap.daemon.keytab.file""
  },
  ""when"" : {
      ""contains"" : [""services"", ""HIVE""]
  }
}
{code}

Note the ""{{when}}"" clause. This indicates that this identity should only be processed when the set of services contains ""HIVE"".  An alternative to this would be to test the set of components for a certain component. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Critical,2016-05-10 18:42:39,7
12966269,The 'krb5-conf' configuration is not available,"Configuration is loaded:
{code}
06 May 2016 10:52:11,998  INFO [qtp-ambari-client-26] ClusterImpl:346 - Service config types loaded: {KAFKA=[ranger-kafka-policymgr-ssl, kafka-log4j, kafka-env, kafka-broker, ranger-kafka-security, ranger-kafka-plugin-properties, ranger-kafka-audit], PIG=[pig-properties, pig-env, pig-log4j], ZEPPELIN=[zeppelin-env, zeppelin-config], LOGSEARCH=[logsearch-service_logs-solrconfig, logsearch-admin-json, logfeeder-log4j, logsearch-env, logsearch-solr-log4j, logfeeder-env, logsearch-audit_logs-solrconfig, logsearch-solr-env, logfeeder-properties, logsearch-properties, logsearch-log4j, logsearch-solr-client-log4j, logsearch-solr-xml], RANGER_KMS=[kms-properties, ranger-kms-security, ranger-kms-site, kms-site, kms-env, dbks-site, ranger-kms-audit, ranger-kms-policymgr-ssl, kms-log4j], MAPREDUCE2=[mapred-site, mapred-env], SLIDER=[slider-log4j, slider-env, slider-client], HIVE=[llap-cli-log4j2, hive-interactive-site, hive-exec-log4j, hive-env, ranger-hive-policymgr-ssl, tez-interactive-site, hive-site, hivemetastore-site, hive-interactive-env, webhcat-env, ranger-hive-plugin-properties, webhcat-site, hive-log4j, ranger-hive-audit, webhcat-log4j, hiveserver2-site, hcat-env, llap-daemon-log4j, ranger-hive-security], TEZ=[tez-env, tez-site], HBASE=[ranger-hbase-security, hbase-env, hbase-policy, hbase-log4j, hbase-site, ranger-hbase-policymgr-ssl, ranger-hbase-audit, ranger-hbase-plugin-properties], RANGER=[admin-properties, tagsync-log4j, ranger-site, ranger-ugsync-site, ranger-admin-site, ranger-tagsync-site, usersync-log4j, tagsync-application-properties, usersync-properties, admin-log4j, ranger-env], OOZIE=[oozie-log4j, oozie-env, oozie-site], FLUME=[flume-env, flume-conf], MAHOUT=[mahout-log4j, mahout-env], HDFS=[ssl-server, hdfs-log4j, ranger-hdfs-audit, ranger-hdfs-plugin-properties, ssl-client, hdfs-site, ranger-hdfs-policymgr-ssl, ranger-hdfs-security, hadoop-policy, hadoop-env, core-site], AMBARI_METRICS=[ams-ssl-client, ams-ssl-server, ams-hbase-log4j, ams-grafana-env, ams-hbase-policy, ams-hbase-security-site, ams-hbase-env, ams-env, ams-grafana-ini, ams-log4j, ams-site, ams-hbase-site], SPARK=[spark-thrift-fairscheduler, spark-thrift-sparkconf, spark-log4j-properties, spark-defaults, spark-metrics-properties, spark-hive-site-override, spark-env], SMARTSENSE=[hst-log4j, hst-server-conf, hst-common-conf, capture-levels, hst-agent-conf, anonymization-rules], YARN=[ranger-yarn-policymgr-ssl, yarn-site, ranger-yarn-audit, ranger-yarn-security, ranger-yarn-plugin-properties, yarn-env, capacity-scheduler, yarn-log4j], FALCON=[falcon-startup.properties, falcon-runtime.properties, falcon-env], SQOOP=[sqoop-site, sqoop-env], ZOOKEEPER=[zoo.cfg, zookeeper-env, zookeeper-log4j], STORM=[ranger-storm-plugin-properties, storm-site, ranger-storm-audit, storm-cluster-log4j, storm-worker-log4j, ranger-storm-policymgr-ssl, ranger-storm-security, storm-env], ATLAS=[atlas-hbase-site, atlas-log4j, atlas-env, application-properties], GANGLIA=[ganglia-env], KNOX=[knoxsso-topology, ranger-knox-security, users-ldif, knox-env, ranger-knox-plugin-properties, gateway-site, gateway-log4j, ranger-knox-policymgr-ssl, ranger-knox-audit, topology, admin-topology, ldap-log4j], KERBEROS=[kerberos-env, krb5-conf], ACCUMULO=[accumulo-log4j, accumulo-env, client, accumulo-site]}
{code}

But: 
{noformat}
06 May 2016 12:43:46,050 ERROR [qtp-ambari-client-171] AbstractResourceProvider:314 - Caught AmbariException when getting a resource
org.apache.ambari.server.AmbariException: The 'krb5-conf' configuration is not available
	at org.apache.ambari.server.controller.KerberosHelperImpl.getKerberosDetails(KerberosHelperImpl.java:1903)
	at org.apache.ambari.server.controller.KerberosHelperImpl.addAmbariServerIdentity(KerberosHelperImpl.java:1364)
	at org.apache.ambari.server.controller.KerberosHelperImpl.getActiveIdentities(KerberosHelperImpl.java:1283)
	at org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider$GetResourcesCommand.invoke(HostKerberosIdentityResourceProvider.java:163)
	at org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider$GetResourcesCommand.invoke(HostKerberosIdentityResourceProvider.java:145)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.getResources(AbstractResourceProvider.java:307)
	at org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider.getResources(HostKerberosIdentityResourceProvider.java:134)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:966)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:141)
	at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:512)
	at org.apache.ambari.server.api.query.QueryImpl.queryForSubResources(QueryImpl.java:464)
	at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:437)
	at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:217)
	at org.apache.ambari.server.api.handlers.ReadHandler.handleRequest(ReadHandler.java:69)
	at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:145)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:126)
	at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:90)
	at org.apache.ambari.server.api.services.HostService.getHost(HostService.java:80)
	at sun.reflect.GeneratedMethodAccessor205.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
{noformat}

*Cause*
This is caused in the {{org.apache.ambari.server.controller.internal.HostKerberosIdentityResourceProvider}} when the relevant host is the host where the Ambari server is installed and Kerberos is *_not_* enabled.  

When querying information about a host via {{GET /api/v1/clusters/CLUSTERNAME/hosts/HOSTNAME}}, the relevant Kerberos identities for that host are generated.  This happens whether Kerberos is enabled or not.  If the host is the host where the Ambari server is installed, than code is invoked to calculate the Ambari server's Kerberos identity.  In this code, the Kerberos-specific configurations are retrieved. If Kerberos is not enabled, these configurations will not be available and thus the error, ""The 'krb5-conf' configuration is not available"", is encountered. 

*Solution*
# Stop calculating the Kerberos identities when Kerberos is not enabled
# Protect access to the Kerberos configurations and set default values for needed configuration properties",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-05-09 19:01:23,7
12964955,Authorizations given to role-based principals must be dereferenced upon user login,"Authorizations given to role-based principals must be dereferenced upon user login.  These authorizations are dynamically determined based on the user's set of roles.  

In {{org.apache.ambari.server.security.authorization.AmbariLocalUserDetailsService#loadUserByUsername}}, the set of {{GrantedAuthorities}} the authenticated user is calculated.  During this process, using the set of {{cluster-level roles}} a user is granted, any permissions given to matching role-based principals should be given to the user. 

This essentially work like giving privileges to a group of users calculated at runtime. 

A use-case to support the need for this is to assign access to a view to all users with some specific role. Currently we can assign access to a view to a specific user or group by assigning that user or group the {{VIEW.USER}} role applied to the specific view.  To assign access a view to users who have a specific role, a {{role}} will need to behave like a {{principal}}.
",rbac,['ambari-server'],AMBARI,Bug,Major,2016-05-04 12:32:17,7
12964952,Allow roles to be treated like principals in Ambari DB,"To support assigning privileges to users based on their roles provide support in the Ambari database to allow a {{role}} to be referenced as a {{principal}} similar in the way a {{user}} and a {{group}} a referenced as a {principal}}.

A use-case to support the need for this is to assign access to a view to all users with some specific role. Currently we can assign access to a view to a specific user or group by assigning that user or group the {{VIEW.USER}} role applied to the specific view.  To assign access a view to users who have a specific role, a {{role}} will need to behave like a {{principal}}.

The following changes need to be made to the database:

* Add {{principal_id}} column to the {{adminpermission}} table
* Create a {{principaltype}} record where the {{principal_type_name}} is '{{ROLE}}'
* Add records to the {{adminprincpal}} table to represent each role in {{adminpermission}}
* Update {{adminpermission.principal_id}} to match the relevant records from {{adminprincipal}}

After this is complete, {{adminprivilege}} records can be created using roles as principals. 

NOTE: special handling will need to be done in the authorization logic to dereference the role associations with the authenticated user, similar in the way this is done for groups. 

",rbac,['ambari-server'],AMBARI,Bug,Major,2016-05-04 12:18:34,7
12964660,RBAC: Clean up roles after ambari upgrade,"After AMBARI-16102, we no longer support more than 1 role per user. So we need a clean up when user upgrade to Ambari 2.4. If user has more than 1 role, we should keep the most ""powerful"" role and delete the rest. ",rbac upgrade,['ambari-server'],AMBARI,Bug,Major,2016-05-03 13:10:02,7
12961844,Stack Advisor issue when adding service to Kerberized cluster,"When adding a service to a Kerberized cluster and click install nothing happens on the UI and i see the following error in the ambari server logs

{code}
20 Apr 2016 16:03:56,818  INFO [qtp-ambari-client-2764] KerberosHelperImpl:735 - Adding identity for JOURNALNODE to auth to local mapping
20 Apr 2016 16:03:56,818  INFO [qtp-ambari-client-2764] KerberosHelperImpl:735 - Adding identity for METRICS_COLLECTOR to auth to local mapping
20 Apr 2016 16:03:56,857  INFO [qtp-ambari-client-2764] StackAdvisorRunner:47 - Script=/var/lib/ambari-server/resources/scripts/stack_advisor.py, actionDirectory=/var/run/ambari-server/stack-recommendations/323, command=recommend-configurations
20 Apr 2016 16:03:56,860  INFO [qtp-ambari-client-2764] StackAdvisorRunner:61 - Stack-advisor output=/var/run/ambari-server/stack-recommendations/323/stackadvisor.out, error=/var/run/ambari-server/stack-recommendations/323/stackadvisor.err
20 Apr 2016 16:03:56,917  INFO [qtp-ambari-client-2764] StackAdvisorRunner:69 - Stack advisor output files
20 Apr 2016 16:03:56,917  INFO [qtp-ambari-client-2764] StackAdvisorRunner:70 -     advisor script stdout: StackAdvisor implementation for stack HDP, version 2.0.6 was loaded
StackAdvisor implementation for stack HDP, version 2.1 was loaded
StackAdvisor implementation for stack HDP, version 2.2 was loaded
StackAdvisor implementation for stack HDP, version 2.3 was loaded
StackAdvisor implementation for stack HDP, version 2.4 was loaded
Returning HDP24StackAdvisor implementation
Error occured in stack advisor.
Error details: 'NoneType' object is not iterable
20 Apr 2016 16:03:56,917  INFO [qtp-ambari-client-2764] StackAdvisorRunner:71 -     advisor script stderr: Traceback (most recent call last):
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 158, in <module>
    main(sys.argv)
  File ""/var/lib/ambari-server/resources/scripts/stack_advisor.py"", line 109, in main
    result = stackAdvisor.recommendConfigurations(services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/../stacks/stack_advisor.py"", line 570, in recommendConfigurations
    calculation(configurations, clusterSummary, services, hosts)
  File ""/var/lib/ambari-server/resources/scripts/./../stacks/HDP/2.0.6/services/stack_advisor.py"", line 627, in recommendAmsConfigurations
    if set(amsCollectorHosts).intersection(dn_hosts):
TypeError: 'NoneType' object is not iterable
20 Apr 2016 16:03:56,918  INFO [qtp-ambari-client-2764] AbstractResourceProvider:802 - Caught an exception while updating host components, retrying : org.apache.ambari.server.AmbariException: Stack Advisor reported an error: TypeError: 'NoneType' object is not iterable
StdOut file: /var/run/ambari-server/stack-recommendations/323/stackadvisor.out

StdErr file: /var/run/ambari-server/stack-recommendations/323/stackadvisor.err
{code}

*Solution*
Pass to the stack advisor information about all installed services where each component is installed (component host map)

 ",kerberos,['ambari-server'],AMBARI,Bug,Critical,2016-04-22 19:03:13,7
12960863,Auth-to-local rule generation duplicates default rules when adding case-insensitive default rules,"When re-generating auth-to-local rules where existing rules are already set, the default (or fallback) rule for the default and additional realms is duplicated but the extra instance(s) have the case-insensitive flag:

Example:
{noformat:title=Was}
...
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
...
{noformat}
{noformat:title=Becomes}
...
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*///L
...
{noformat}

*Steps to Reproduce*
# Create cluster with (at least) HDFS
# Enable Kerberos (do not check the box next to ""Enable case insensitive username rules""; kerberos-env/case_insensitive_username_rules should be false
# Edit Kerberos configuration and check ""Enable case insensitive username rules"" to set kerberos-env/case_insensitive_username_rules to true
# Regenerate Keytabs
# See duplicate entry in HDFS configs (core-site/hadoop.security.auth_to_local)

",auth_to_local kerberos,['ambari-server'],AMBARI,Bug,Critical,2016-04-21 18:28:54,7
12960745,Regenerating keytabs on re-imaged hosts results in error during 'Creating Principals',"We had a 1600 unsecured cluster initially, from which 700 nodes were destroyed. Though Ambari-server knew of 1600 hosts, only 900 were heartbeating. At this point we secured the cluster and everything was good. Then we brought back the 700 hosts, which started heartbeating with ambari-server. 

At this point we did 'Regenerate Keytabs' which failed at the 'Create Principals' step (image attached), as it was trying to re-create principal which is already existing with kadmin, and with ambari-server.

*Create Principals*
Stderr:
{noformat}
2016-04-21 01:28:52,985 - Failed to create or update principal, HTTP/host1.example.com@EXAMPLE.COM - Failed to create service principal for HTTP/host1.example.com@EXAMPLE.COM
STDOUT: Authenticating as principal admin/admin with password.

STDERR: WARNING: no policy specified for HTTP/host1.example.com@EXAMPLE.COM; defaulting to no policy
add_principal: Principal or policy already exists while creating ""HTTP/host1.example.com@EXAMPLE.COM"".
{noformat}

Stdout:
{noformat}
2016-04-21 01:27:32,400 - Processing identities...
2016-04-21 01:28:29,874 - Processing principal, HTTP/host1.example.com@EXAMPLE.COM
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2016-04-21 13:15:05,7
12959876,Kerberos wizard stuck trying to schedule service check operation,"Attached jstack after firing up the curl call.

{code}
curl -u admin:admin -H ""X-Requested-By:ambari"" -i -X POST -d'{""RequestInfo"":{""context"":""Kerberos Service Check"",""command"":""KERBEROS_SERVICE_CHECK"",""operation_level"":{""level"":""CLUSTER"",""cluster_name"":""c1""}},""Requests/resource_filters"":[{""service_name"":""KERBEROS""}]}' http://104.196.89.51:8080/api/v1/clusters/c1/requests
{code}

Behavior:
- Call timedout on the UI and wizard cannot proceed further.
- Exception in the server log after long wait:
{code}
12 Apr 2016 23:01:45,231  INFO [qtp-ambari-client-818] AmbariManagementControllerImpl:3376 - Received action execution request, clusterName=c1, request=isCommand :true, action :null, command :KERBEROS_SERVICE_CHECK, inputs :{}, resourceFilters: [RequestResourceFilter{serviceName='KERBEROS', componentName='null', hostNames=[]}], exclusive: false, clusterName :c1
12 Apr 2016 23:01:45,409  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HIVE
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: AMBARI_METRICS
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HDFS
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: MAPREDUCE2
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: OOZIE
12 Apr 2016 23:01:45,410  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: TEZ
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: HBASE
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: ZOOKEEPER
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: KERBEROS
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: YARN
12 Apr 2016 23:01:45,411  INFO [qtp-ambari-client-818] AmbariCustomCommandExecutionHelper:1135 - Iterating service type Instance in getCommandJson:: PIG
12 Apr 2016 23:03:49,984  WARN [qtp-ambari-client-818] MITKerberosOperationHandler:434 - Failed to execute kadmin:
        Command: /usr/bin/kadmin -s perf-b-3.c.pramod-thangali.internal -p admin -w ******** -r EXAMPLE.COM -q ""get_principal admin""
        ExitCode: 1
        STDOUT: Authenticating as principal admin with password.

        STDERR: kadmin: Client not found in Kerberos database while initializing kadmin interface
{code}",kerberos kerberos-wizard kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2016-04-19 03:00:29,7
12957737,Provide backward compatibility for kdc_host in krb5-conf for krb5.conf templates,"Provide backward compatibility for {{kdc_host}} in {{krb5-conf}} for {{krb5.conf}} templates.

This is necessary in the event the {{krb5.conf}} template is not updated during the Ambari 2.4.0 upgrade while the {{kdc_host}} property is renamed to {{kdc-hosts}}. 
",kerberos,['ambari-server'],AMBARI,Task,Major,2016-04-11 15:42:52,7
12957505,HIVE service_check doesn't work properly,"In templetonSmoke.sh, there are
1) unnecessary `exit 0`
2) lack of redirect `>` command
3) unassigned variable

we should correct them all to check the HIVE service properly.",patch-available,['ambari-server'],AMBARI,Bug,Major,2016-04-10 04:15:48,0
12956188,Kerberos: Allow multiple KDC hosts to be set while enabling Kerberos,"Because multiple KDCs may exist for an installation (failover, high availability, etc...), Ambari should allow a user to specify multiple KDC hosts to be set while enabling Kerberos and updating the Kerberos service's configuration.

This should be done by allowing {{kerberos-env/kdc_host}} to accept a (comma-)delimited list of hosts and then parsing that list properly when building the krb5.conf file where each {{kdc_host}} item generates an entry in the relevant realm block.  For example:

{noformat:title=kerberos-env}
{
  ...
 ""kdc_hosts"" : ""kdc1.example.com, kdc2.example.com""
  ...
}
{noformat}

{noformat:title=krb5.conf}
[realms]
  EXAMPLE.COM = {
    ...
    kdc = kdc1.example.com
    kdc = kdc2.example.com
    ...
  }
{noformat}",kdc kerberos,['ambari-server'],AMBARI,Bug,Major,2016-04-05 15:47:54,7
12952995,RBAC based user access to view instances are not honoured,"Problem:
1. Create a cluster 
2. Create some view instances in amber
3. Create a local non-admin ambari user
4. Grant the newly created user access to one of the view instances

Log-in with the non-admin user. The user should only see the view instances it has permission instead of all view instances.

This seems to have been introduced by https://issues.apache.org/jira/browse/AMBARI-14194",rbac security,['ambari-server'],AMBARI,Bug,Critical,2016-03-23 21:49:39,7
12950004,Fix ArtifactResourceProviderTest to avoid set ordering issues,"Due to the inconsistent ordering of resources in the set of Resources created in {{org.apache.ambari.server.controller.internal.ArtifactResourceProvider#getResources}}, {{org.apache.ambari.server.controller.internal.ArtifactResourceProviderTest#testDeleteResources}} fails since the test case expects a certain order in which the Resource are processed. 

Since it doesn't really make a difference what order the resources are processed when deleting, the test case should be fixed to not care about that. 

The follow test failure has been encountered due to this:

{noformat}
java.lang.AssertionError: 
  Unexpected method call ArtifactDAO.findByNameAndForeignKeys(""test-artifact2"", {cluster=500}):
    ArtifactDAO.findByForeignKeys({cluster=500}): expected: 1, actual: 1
    ArtifactDAO.findByNameAndForeignKeys(""test-artifact"", {cluster=500}): expected: 1, actual: 0
	at org.easymock.internal.MockInvocationHandler.invoke(MockInvocationHandler.java:44)
	at org.easymock.internal.ObjectMethodsFilter.invoke(ObjectMethodsFilter.java:94)
	at org.easymock.internal.ClassProxyFactory$MockMethodInterceptor.intercept(ClassProxyFactory.java:97)
	at org.apache.ambari.server.orm.dao.ArtifactDAO$$EnhancerByCGLIB$$40076a34.findByNameAndForeignKeys(<generated>)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProvider$4.invoke(ArtifactResourceProvider.java:377)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProvider$4.invoke(ArtifactResourceProvider.java:365)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.invokeWithRetry(AbstractResourceProvider.java:451)
	at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:332)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProvider.deleteResources(ArtifactResourceProvider.java:245)
	at org.apache.ambari.server.controller.internal.ArtifactResourceProviderTest.testDeleteResources(ArtifactResourceProviderTest.java:435)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
{noformat}
",unit-test,['ambari-server'],AMBARI,Bug,Major,2016-03-14 18:41:02,7
12948831,Provide composite Kerberos Descriptor via the REST API,"Provide an entry point in the REST API to retrieve the _composite_ Kerberos descriptor. This includes the default Kerberos descriptor built from the stack definitions overlaid with the (potentially sparse) Kerberos descriptor stored as an artifact of the cluster. 

The entry point should be a read-only sub-resource of ""clusters"", and should only be made available if asked for explicitly due to the size of the data that will be returned.

{noformat:title=Example API call}
GET /api/v1/clusters/:cluster_name/kerberos_descriptors/COMPOSITE
{noformat}

Note: Kerberos Descriptors available via this interface are:
* STACK - the default Kerberos Descriptor from the relevant stack definition
* USER - the user-suppled updates to the stack default Kerberos Descriptor
* COMPOSITE - the stack default Kerberos Descriptor with the user-suppled updates applied (this is the Kerberos Descriptor used when performing Kerberos-related operations)
",kerberos_descriptor rest_api,['ambari-server'],AMBARI,Task,Major,2016-03-10 15:50:19,7
12947689,Kerberos: Provide SHA256 or SHA512 options for template principal digest,"When generating accounts in an Active Directory, it may be useful to add a unique value to CN's.  In the past generating this value was done by taking the SHA1 hash of the relevant normalized principal name. For example {{ambari-qa-c1@EXAMPLE.COM}} yields {{d9b48cb1c075d3da9fab4855a4031266bab8fb6a}}.  

Because using SHA1 at all may not be desirable, Ambari should provide options to use the following digest algorithms in the Active Directory account creation attribute template ({{kerberos-env/ad_create_attributes_template}}:

||Attribute Variables||Example||
|$principal_digest|SHA1 hash of the $normalized_principal|
|$principal_digest_256|SHA256 hash of the $normalized_principal|
|$principal_digest_512|SHA512 hash of the $normalized_principal|

   ",active-directory active_directory kerberos,['ambari-server'],AMBARI,Bug,Major,2016-03-07 13:33:01,7
12944649,Authorization for Auto start services,"We need to add authorization for Auto start services.

*Solution*
* Users with the ability to modify cluster configurations ({{CLUSTER.MODIFY_CONFIGS}}) should be able to toggle the cluster-level auto start value.
* Users with the ability to start and stop services ({{SERVICE.START_STOP}}) should be able to toggle the service-level auto start value.

",rbac,['ambari-server'],AMBARI,Bug,Major,2016-02-25 20:04:36,7
12942990,UpgradeCatalog230 is not idempotent,"If ambari-server upgrade is run again, the following error is encountered. One easy way to test it is to set the version in the DB to an older version and then call ambari-server upgrade.

e.g. {{update metainfo set metainfo_value = '2.2.0' where neatinfo_key = 'version';}}
 
{code}
Error output from schema upgrade command:
Exception in thread ""main"" org.apache.ambari.server.AmbariException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""adminpermission_pkey""
Error Code: 0
Call: INSERT INTO adminpermission (permission_id, permission_label, permission_name, sort_order, resource_type_id) VALUES (?, ?, ?, ?, ?)
	bind => [5 parameters bound]
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:233)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.main(SchemaUpgradeHelper.java:307)
Caused by: javax.persistence.RollbackException: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""adminpermission_pkey""
Error Code: 0
Call: INSERT INTO adminpermission (permission_id, permission_label, permission_name, sort_order, resource_type_id) VALUES (?, ?, ?, ?, ?)
	bind => [5 parameters bound]
	at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:157)
	at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
	at org.apache.ambari.server.upgrade.UpgradeCatalog230.addNewPermissions(UpgradeCatalog230.java:144)
	at org.apache.ambari.server.upgrade.UpgradeCatalog230.executeDMLUpdates(UpgradeCatalog230.java:127)
	at org.apache.ambari.server.upgrade.AbstractUpgradeCatalog.upgradeData(AbstractUpgradeCatalog.java:659)
	at org.apache.ambari.server.upgrade.SchemaUpgradeHelper.executeDMLUpdates(SchemaUpgradeHelper.java:230)
	... 1 more
Caused by: Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint ""adminpermission_pkey""
Error Code: 0
Call: INSERT INTO adminpermission (permission_id, permission_label, permission_name, sort_order, resource_type_id) VALUES (?, ?, ?, ?, ?)
	bind => [5 parameters bound]
	at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:340)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.processExceptionForCommError(DatabaseAccessor.java:1611)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:898)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
	at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
	at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
	at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
{code}",upgrade,['ambari-server'],AMBARI,Bug,Critical,2016-02-24 15:55:04,7
12938906,Return privilege information with results from GroupResourceProvider,"Return privilege information with results from GroupResourceProvider, which is invoked when issuing the following REST API call:
{noformat}
GET /api/v1/groups
{noformat}

The result set should looks something like:
{code}
{
  ""href"" : ""http://ambari-server:8080/api/v1/groups?fields=privileges/*"",
  ""items"" : [
    {
      ""href"" : ""http:///ambari-server:8080/api/v1/groups/group1"",
      ""Groups"" : {
        ""group_name"" : ""group1""
      },
      ""privileges"" : [
        {
          ""href"" : ""http://ambari-server:8080/api/v1/groups/group1/privileges/1"",
          ""PrivilegeInfo"" : {
            ""cluster_name"" : ""c1"",
            ""permission_label"" : ""Cluster User"",
            ""permission_name"" : ""CLUSTER.USER"",
            ""principal_name"" : ""group1"",
            ""principal_type"" : ""GROUP"",
            ""privilege_id"" : 3,
            ""type"" : ""CLUSTER"",
            ""group_name"" : ""group1""
          }
        }
      ]
    }
  ]
}
{code}",rbac,['ambari-server'],AMBARI,Task,Major,2016-02-12 20:12:14,7
12938869,KerberosDescriptorTest failed due to moved/missing test directory,"Unit tests in {{org.apache.ambari.server.stack.KerberosDescriptorTest}} fail due to moved/missing directory - {{.../ambari/ambari-server/target/classes/stacks}}.  

{{org.apache.ambari.server.stack.KerberosDescriptorTest}} should use {{{.../ambari/ambari-server/src/main/resources/stacks}} instead. ",unit-test,['ambari-server'],AMBARI,Bug,Major,2016-02-12 18:20:03,7
12937979,Provide explicit ordering for roles,"Since it may be desired to order roles in ways other than alphabetically, each role should have an explicit numerical order that may be used by UI's. 

Roles should be explicitly ordered by the amount of access privileges they have. 


||Role Name||Explicit Order Value||
|Ambari Administrator|1|
|Cluster Administrator|2|
|Cluster Operator|3|
|Service Administrator|4|
|Service Operator|5|
|Cluster User|6|
|View User|7|


",rbac,['ambari-server'],AMBARI,Task,Major,2016-02-09 19:30:14,7
12937032,Some user-specified auth-to-local rules fail to render when auto generating auth-to-local rules,"When processing the user-specified auth-to-local rules while automatically generating auth-to-local rule sets using {{org.apache.ambari.server.controller.AuthToLocalBuilder}}, some rules fail to render.

This occurs when escape characters are used in the replacement pattern or replacement string of the rule, where a role looks like:

{code}
RULE: [n:string](regexp)s/pattern/replacement/[g]
{code}

To fix this issue, the regular expression used to parse the rules need to be augmented to allow for those characters.
",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-02-05 13:13:05,7
12936402,Add API directive to force toggling Kerberos if the cluster's security type has not changed,"Add an API directive to force toggling Kerberos if the cluster's security type has not changed.  

This is useful for _retry_ attempts to enable or disable Kerberos when the workflow progressed far enough to store the _new_ security type.  

Trying to enable Kerberos when the cluster's security type is already set to {{KERBEROS}} will result in a successful response from the Ambari server but no actions will be performed.  Same for attempting to disable Kerberos when the cluster's security type is already set to {{NONE}}.  

By forcing the operation using the {{force_toggle_kerberos=true}} directive, the security type check is avoided, thus allowing the _retry_ operation to proceed. 

Example: 
{code}
PUT /api/v1/clusters/CLUSTER_NAME?force_toggle_kerberos=true
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
    }
}
{code}",kerberos rest_api,['ambari-server'],AMBARI,Task,Major,2016-02-03 17:35:34,7
12935664,Increase timeout for server-side tasks,"Increase timeout for server-side tasks, or make the timeout configurable.

This is to help in the case where environmental issues cause Kerberos-related commands to take longer than usual, thus increasing the time it takes to process Kerberos server-side actions. If the Kerberos-server side actions timeout, Ambari fails the task and the user is stuck not able to perform the desired Kerberos-related action.",kerberos server-side,['ambari-server'],AMBARI,Bug,Critical,2016-02-01 16:36:37,7
12931653,disabling kerberos does not remove auth to local rules,"After disabling Kerberos to fix a user generated issue with a principal name pattern, the auth-to-local mapping(s) were not removed and thus not _fixing_ the issues that were caused: 

{noformat:title=Invalid hadoop.security.auth_to_local value}
 <property>
       <name>hadoop.security.auth_to_local</name>
       <value>RULE:[1:$1@$0](${hbase_user}@EXAMPLE.COM)s/.*/hbase/
 RULE:[1:$1@$0](${hdfs_user}@EXAMPLE.COM)s/.*/hdfs/
 RULE:[1:$1@$0](${smokeuser}@EXAMPLE.COM)s/.*/ambari-qa/
 RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
 RULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/ams/
 RULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/ams/
 RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/
 RULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/hbase/
 RULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/hive/
 RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/
 RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/
 RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/
 RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/
 RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/
 RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/
 RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/
 DEFAULT</value>
     </property>
{noformat}

{noformat:title=Errors in log}
2016-01-13 21:51:17,825 FATAL datanode.DataNode (DataNode.java:secureMain(2429)) - Exception in secureMain
java.util.regex.PatternSyntaxException: Illegal repetition near index 0
${hbase_user}@EXAMPLE.COM
^
        at java.util.regex.Pattern.error(Pattern.java:1924)
        at java.util.regex.Pattern.closure(Pattern.java:3104)
        at java.util.regex.Pattern.sequence(Pattern.java:2101)
        at java.util.regex.Pattern.expr(Pattern.java:1964)
        at java.util.regex.Pattern.compile(Pattern.java:1665)
        at java.util.regex.Pattern.<init>(Pattern.java:1337)
        at java.util.regex.Pattern.compile(Pattern.java:1022)
        at org.apache.hadoop.security.authentication.util.KerberosName$Rule.<init>(KerberosName.java:193)
        at org.apache.hadoop.security.authentication.util.KerberosName.parseRules(KerberosName.java:336)
        at org.apache.hadoop.security.authentication.util.KerberosName.setRules(KerberosName.java:397)
        at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:75)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:275)
        at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:311)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2192)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2242)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2422)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2446)
2016-01-13 21:51:17,830 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1
2016-01-13 21:51:17,832 INFO  datanode.DataNode (LogAdapter.java:info(45)) - SHUTDOWN_MSG:
/************************************************************
{noformat}

The auth-to-local mappings should be removed when Kerberos is disabled.",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-01-16 11:40:13,7
12931590,Kerberos automation logic should use stack advisor when determining configuration updates,"Kerberos automation logic should use the stack advisor when determining configuration updates.  This will ensure that property updates are valid given the cluster's configuration and whether the cluster was created using the API, UI, or BluePrints.
",kerberos,['ambari-server'],AMBARI,Bug,Major,2016-01-15 22:29:43,7
12924678,Failed to deploy Kerberized cluster via blueprint with custom principal name,"Failed to deploy Kerberized cluster via Blueprint with custom principal name set in Kerberos descriptor declared in _Cluster Creation Template_ like 

{code}
...
  ""security"": {
    ""type"": ""KERBEROS"",
    ""kerberos_descriptor"": {
      ""identities"": [
        {
          ""name"": ""smokeuser"",
          ""principal"": {
            ""value"": ""smokeuser9jJevBQAYGQWnRkuapSEp@${realm}""			
          }
        }
      ]
    }
  },
...
{code}

The following error (shown in ambari-server.log) was encountered while the cluster was being built: 
{noformat}
Failed to create keytab for smokeuser9jJevBQAYGQWnRkuapSEp@EXAMPLE.COM, missing cached file
{noformat}

h3. Cause
This was caused because the Kerberos descriptor in the _Blueprint_ or _Cluster Creation Template_ did not declare the type property of the principal being updated.  This caused the logic in Ambari to assume the principal was a _service_ principal rather than a _user_ (or headless) principal.  Because of this, when merging the updates to the default Kerberos descriptor (from the stack), the {{smokeuser}} principal type was changed _user_ to _service_.  Thus it was skipped over when the _Blueprints_ process executed the phase to ensure (and cache) headless identities.  

The bug is in the logic parsing the Kerberos descriptor.  By not specifying a principal type, the logic assumes the principal type is _service_; however in this case, the principal type needs to be {{null}}, so that when the user-specified Kerberos descriptor is merged with the default Kerberos descriptor, the default principal type is not changed. 

*NOTE:* This is not limited to _Blueprints_, the issue will cause issues if the specified Kerberos descriptor artifact is missing {{principal/type}} properties as well.  See [Set the Kerberos Descriptor|https://cwiki.apache.org/confluence/display/AMBARI/Automated+Kerberizaton#AutomatedKerberizaton-SettheKerberosDescriptor]

h3. Solution
Change the logic it the Kerberos descriptor parser to allow for the principal type to be {{null}}. Handle this value being {{null}} by consumers of this data such that {{null}} indicates the default value of {{service}}. This will keep the current behavior consistent, and also allow for the merging facility to properly merge principal updates - like changing the principal name pattern without needing to specify the principal type as well. 

h3. Workaround
Explicitly set the {{type}} property of _user_ (or headless) principals in the Kerberos descriptor:
{code}
...
  ""security"": {
    ""type"": ""KERBEROS"",
    ""kerberos_descriptor"": {
      ""identities"": [
        {
          ""name"": ""smokeuser"",
          ""principal"": {
            ""type"" : ""USER"",
            ""value"": ""smokeuser9jJevBQAYGQWnRkuapSEp@${realm}""			
          }
        }
      ]
    }
  },
...
{code}

",kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-12-29 12:52:27,7
12923615,SERVICE.MANAGE_CONFIG_GROUPS missing from CLUSTER.ADMINISTRATOR and AMBARI.ADMINISTRATOR roles in MySQL create script,"SERVICE.MANAGE_CONFIG_GROUPS missing from CLUSTER.ADMINISTRATOR and AMBARI.ADMINISTRATOR roles in MySQL create script.
",rbac,[],AMBARI,Bug,Blocker,2015-12-22 00:12:33,7
12921052,Enforce granular role-based access control for ldap-sync functions,"Enforce granular role-based access control for ldap sync functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Sync LDAP users                   |   |   |   |   |   | (+)| 
|Sync LDAP groups                   |   |   |   |   |   |(+)| 

Entry points affected:
GET  /api/v1/ldap_sync_events
POST  /api/v1/ldap_sync_events
DELETE  /api/v1/ldap_sync_events/:event_id
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-11 13:01:04,1
12920800,Enforce granular role-based access control for configuration functions,"Enforce granular role-based access control for configuration functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View service configurations           |(+)|(+)|(+)|(+)|(+)|(+)|
|Compare service configurations        |(+)|(+)|(+)|(+)|(+)|(+)|
|Manage configuration groups   |   |   |(+)|(+)|(+)|(+)|
|View host configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View cluster configuration            |(+)|(+)|(+)|(+)|(+)|(+)|

Entry points affected:
- GET /api/v1/clusters/:clusterId/configurations
- POST /api/v1/clusters/:clusterId/configurations
- GET /api/v1/clusters/:clusterId/configurations/service_config_versions
- GET /api/v1/clusters/:clusterId/config_groups
- GET /api/v1/clusters/:clusterId/config_groups/:groupId
- POST /api/v1/clusters/:clusterId/config_groups
- PUT /api/v1/clusters/:clusterId/config_groups/:groupId
- DELETE /api/v1/clusters/:clusterId/config_groups/:groupId",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-10 15:59:56,1
12920492,Change authorization resource Id to be adminresource id,The RBAC changes attempt to perform authorization checks on resource-specific identifiers. This is not the intended use of the authorization (admin*/auth*) tables as the resource's adminresource id is to be looked up and an authorization check is to be performed on that. ,rbac,['ambari-server'],AMBARI,Bug,Critical,2015-12-09 17:21:42,7
12918951,User with SERVICE.MODIFY_CONFIGS authorization fails with 403 status updating configs,"User with SERVICE.MODIFY_CONFIGS authorization fails with 403 status updating configs

{noformat:title=PUT /api/v1/clusters/:cluster_name}
[
  {
    ""Clusters"": {
      ""desired_config"": [
        {
          ""type"": ""zoo.cfg"",
          ""tag"": ""version1449226525346"",
          ""properties"": {
            ""autopurge.purgeInterval"": ""24"",
            ""autopurge.snapRetainCount"": ""30"",
            ""dataDir"": ""/hadoop/zookeeper"",
            ""tickTime"": ""2000"",
            ""initLimit"": ""11"",
            ""syncLimit"": ""5"",
            ""clientPort"": ""2181""
          },
          ""service_config_version_note"": """"
        }
      ]
    }
  }
]
{noformat}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2015-12-04 11:15:51,7
12918102,Enforce granular role-based access control for ldap-sync functions,"Enforce granular role-based access control for ldap sync functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Sync LDAP users                   |   |   |   |   |   | (+)| 
|Sync LDAP groups                   |   |   |   |   |   |(+)| 

Entry points affected:
* TBD
",ldap rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:26:20,1
12918099,Enforce granular role-based access control for Views functions,"Enforce granular role-based access control for Views functions:

|| || View\\User || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Manage Ambari Views           |   |   |   |   |   |   |(+)|
|Use  Ambari View           |(+) |   |   |   |   |   |   |

Entry points affected:
* TBD
",rbac views,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:22:00,7
12918096,Enforce granular role-based access control for configuration functions,"Enforce granular role-based access control for configuration functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View service configurations           |(+)|(+)|(+)|(+)|(+)|(+)|
|Compare service configurations        |(+)|(+)|(+)|(+)|(+)|(+)|
|Manage configuration groups   |   |   |(+)|(+)|(+)|(+)|
|View host configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View cluster configuration            |(+)|(+)|(+)|(+)|(+)|(+)|

Entry points affected:
* TBD
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:16:10,1
12918092,Enforce granular role-based access control for host functions,"Enforce granular role-based access control for host functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Turn on/off maintenance mode  |   |   |   |(+)|(+)|(+)|
|Add/Delete hosts              |   |   |   |(+)|(+)|(+)|
|View host details              |(+)|(+)| (+)|(+)|(+)|(+)|

Entry points affected:
* GET api/vi/hosts
* GET api/vi/hosts/:host_name
* POST api/vi/hosts/:host_name
* PUT api/vi/hosts/:host_name
* DELETE api/vi/hosts/:host_name
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:05:13,7
12918091,Enforce granular role-based access control for service functions,"Enforce granular role-based access control for service functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Start/Stop/Restart Service    |   |(+)|(+)|(+)|(+)|(+)|
|Decommission/recommission     |   |(+)|(+)|(+)|(+)|(+)|
|Run service checks            |   |(+)|(+)|(+)|(+)|(+)|
|Turn on/off maintenance mode  |   |(+)|(+)|(+)|(+)|(+)|
|Perform service-specific tasks|   |(+)|(+)|(+)|(+)|(+)|
|Move to another host          |   |   |(+)|(+)|(+)|(+)|
|Enable HA                     |   |   |(+)|(+)|(+)|(+)|
|Add Service to cluster        |   |   |   |   |(+)|(+)|
|Install components            |   |   |   |(+)|(+)|(+)|
|Modify service configurations         |   |   |(+)|(+)|(+)|(+)|
|Set service users and groups  |   |   |   |   |   |(+)|

Entry points affected:
* GET /api/v1/clusters/:cluster_name/services
* GET /api/v1/clusters/:cluster_name/services/:service_name
* POST /api/v1/clusters/:cluster_name/services/:service_name
* PUT /api/v1/clusters/:cluster_name/services/:service_name
* DELETE /api/v1/clusters/:cluster_name/services/:service_name
* GET /api/v1/clusters/:cluster_name/services/:service_name/components
* GET /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* POST /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* PUT /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* DELETE /api/v1/clusters/:cluster_name/services/:service_name/components/:component_name
* POST /api/v1/clusters/:cluster_name/hosts
* POST /api/v1/clusters/:cluster_name/requests",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-03 19:01:58,7
12917646,Enforce granular role-based access control for credential functions,"Enforce granular role-based access control for credential functions

Entry points affected:
* GET /clusters/:cluster_name/credentials
* GET /clusters/:cluster_name/credentials/:alias
* POST /clusters/:cluster_name/credentials/:alias
* PUT /clusters/:cluster_name/credentials/:alias
* DELETE /clusters/:cluster_name/credentials/:alias
",rbac,['ambari-server'],AMBARI,Task,Critical,2015-12-02 16:58:55,7
12917364,FK_permission_roleauthorization_permission_id is too long for a constraint identifier,"{noformat}
ALTER TABLE permission_roleauthorization ADD CONSTRAINT FK_permission_roleauthorization_permission_id FOREIGN KEY (permission_id) REFERENCES adminpermission(permission_id)
Error report -
SQL Error: ORA-00972: identifier is too long
00972. 00000 -  ""identifier is too long""
*Cause:    An identifier with more than 30 characters was specified.
*Action:   Specify at most 30 characters.
{noformat}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2015-12-01 20:05:51,7
12917354,Enforce granular role-based access control for group functions,"Enforce granular role-based access control for alert functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Manage groups                 |   |   |   |   |   |(+)|

Entry points affected: 
* GET /api/v1/groups
* GET /api/v1/groups/:group_name
* PUT /api/v1/groups/:group_name
* POST /api/v1/groups/:group_name
* DELETE /api/v1/groups/:group_name
* GET /api/v1/groups/:group_name/members
* PUT /api/v1/groups/:group_name/members
* POST /api/v1/groups/:group_name/members
* DELETE /api/v1/groups/:group_name/members",rbac,['ambari-server'],AMBARI,Task,Major,2015-12-01 19:28:16,7
12917350,Enforce granular role-based access control for alert functions,"Enforce granular role-based access control for alert functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View alerts (service)        |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable alerts (service)          |   |   |(+)|(+)|(+)|(+)|
|View alerts (cluster)                   |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable alerts (cluster)         |   |   |   |   |(+)|(+)|

Entry points affected: {color:red}TBD{color}
",rbac,['ambari-server'],AMBARI,Task,Major,2015-12-01 19:15:15,7
12916980,Enforce granular role-based access control for stack version functions,"Enforce granular role-based access control for stack version functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View stack version details    |(+)|(+)|(+)|(+)|(+)|(+)|
|Manage stack versions         |   |   |   |   |   |(+)|
|Edit stack repository URLs    |   |   |   |   |   |(+)|

Entry points affected:
* GET /api/v1/stacks/:stack_name/versions/:version_id
* GET /api/v1/stacks/:stack_name/versions/:version_id
* PUT /api/v1/stacks/:stack_name/versions/:version_id
* POST /api/v1/stacks/:stack_name/versions/:version_id
* DELETE /api/v1/stacks/:stack_name/versions/:version_id
",rbac security,['ambari-server'],AMBARI,Task,Major,2015-11-30 15:15:38,7
12916193,Enforce granular role-based access control for cluster functions,"Enforce granular role-based access control for cluster functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View stack version details    |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable Kerberos       |   |   |   |   |(+)|(+)|
|Upgrade/downgrade stack       |   |   |   |   |(+)|(+)|
|Create new clusters           |   |   |   |   |   |(+)|
|Rename clusters               |   |   |   |   |   |(+)|

Entry points affected:
* PUT /api/v1/clusters/:cluster_name 
* POST /api/v1/clusters/:cluster_name 

*Note: Read-only requests (GET) are not protected so that the front end is not broken.*
",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-25 16:10:32,7
12915829,Change Anonymous API Authentication To A Declared User,"When using {{api.authenticate=false}}, REST requests to the Ambari APIs don't need to contain any user information. As a result, new code being placed which assumes an authenticated user will throw NPE exceptions:

{code}
      // Ensure that the authenticated user has authorization to get this information
      if (!isUserAdministrator && !AuthorizationHelper.getAuthenticatedName().equalsIgnoreCase(userName)) {
        throw new AuthorizationException();
      }
{code}

{code}
java.lang.NullPointerException
	at org.apache.ambari.server.controller.internal.ActiveWidgetLayoutResourceProvider.getResources(ActiveWidgetLayoutResourceProvider.java:156)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl$ExtendedResourceProviderWrapper.queryForResources(ClusterControllerImpl.java:946)
	at org.apache.ambari.server.controller.internal.ClusterControllerImpl.getResources(ClusterControllerImpl.java:132)
	at org.apache.ambari.server.api.query.QueryImpl.doQuery(QueryImpl.java:512)
	at org.apache.ambari.server.api.query.QueryImpl.queryForResources(QueryImpl.java:381)
	at org.apache.ambari.server.api.query.QueryImpl.execute(QueryImpl.java:217)
{code}

Recommend changing this option to something like

{code}
api.authenticated.user=admin
{code}

This will preserve the existing functionality while allowing the new code to continue to assume authenticated users.",api authentication rbac,['ambari-server'],AMBARI,Bug,Major,2015-11-24 15:51:26,7
12915084,Fix translation of different view resource type entities to all be of type VIEW,"Fix translation of different view resource type entities to all be of type VIEW. 

When an Ambari view is added to Ambari, a new resource type ({{adminresourcetype}}) is added.  Each of these types need to resolve to represent a _view_  resource ({{org.apache.ambari.server.security.authorization.ResourceType#VIEW}} so that authorization checks can be performed properly. 

For example:
{noformat}
 resource_type_id |    resource_type_name
------------------+---------------------------
                1 | AMBARI
                2 | CLUSTER
                3 | VIEW
                5 | ADMIN_VIEW{2.1.2}
                6 | FILES{1.0.0}
                7 | PIG{1.0.0}
                8 | CAPACITY-SCHEDULER{1.0.0}
                9 | TEZ{0.7.0.2.3.2.0-377}
               10 | SLIDER{2.0.0}
               11 | HIVE{1.0.0}
               55 | ADMIN_VIEW{2.2.0.0}
               56 | TEZ{0.7.0.2.3.2.0-3539}
{noformat}

The translation needs to be be:

{noformat}
AMBARI                    | ResourceType.AMBARI
CLUSTER                   | ResourceType.CLUSTER
VIEW                      | ResourceType.VIEW
ADMIN_VIEW{2.1.2}         | ResourceType.VIEW
FILES{1.0.0}              | ResourceType.VIEW
PIG{1.0.0}                | ResourceType.VIEW
CAPACITY-SCHEDULER{1.0.0} | ResourceType.VIEW
TEZ{0.7.0.2.3.2.0-377}    | ResourceType.VIEW
SLIDER{2.0.0}             | ResourceType.VIEW
HIVE{1.0.0}               | ResourceType.VIEW
ADMIN_VIEW{2.2.0.0}       | ResourceType.VIEW
TEZ{0.7.0.2.3.2.0-3539}   | ResourceType.VIEW
{noformat}",rbac,['ambari-server'],AMBARI,Bug,Blocker,2015-11-21 12:22:21,7
12915078,Return HTTP 403 on REST API authorization failures,"Return HTTP 403 on REST API authorization failures.

{code:title=403 Forbidden} 
{
  ""status"" : 403,
  ""message"" : ""The authenticated user is not authorized to perform the requested operation""
}
{code}
",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-21 10:52:45,7
12914689,Enforce granular role-based access control for user functions,"Enforce granular role-based access control for user functions:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
|Create new clusters           |   |   |   |   |   |(+)|
|Manage users                  |   |   |   |   |   |(+)|
|Assign permissions/roles      |   |   |   |   |   |(+)|

Entry points affected:
* GET /api/v1/users/:username
* GET /api/v1/users/:username/widget_layouts
* GET /api/v1/users/:username/privileges
* POST /api/v1/users/:username
* DELETE /api/v1/users/:username
* PUT /api/v1/users/:username
* GET /api/v1/priviliges
* POST /api/v1/priviliges
* GET /api/v1/priviliges/:privilege_id
* DELETE /api/v1/priviliges/:privilege_id
* PUT /api/v1/priviliges/:privilege_id
* GET /api/v1/clusters/:cluster_name/priviliges
* GET /api/v1/clusters/:cluster_name/priviliges/:privilege_id
* POST /api/v1/clusters/:cluster_name/priviliges
* DELETE /api/v1/clusters/:cluster_name/priviliges/:privilege_id
* PUT /api/v1/clusters/:cluster_name/priviliges/:privilege_id
",rbac security,['ambari-server'],AMBARI,Task,Major,2015-11-20 00:32:18,7
12914157,Create base infrastructure to allow for granular role based access control,"Create base infrastructure to allow for granular role based access control. 

This entails creating a base class to help with authorization checks.  The base class is to contain a default authorization check implementation but allow derived classes to override the logic to implement more sophisticated checks. 
",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-18 16:30:09,7
12913466,Create API entry points for getting authorization information,"Create API entry points for getting authorization information.  

The following entry points need to be created:
{noformat}
GET /api/v1/authorizations
GET /api/v1/permissions/:PERMISSION_ID/authorizations
GET /api/v1/users/:USERNAME/authorizations
{noformat}

The following entry points need to be updated to supply more information:
{noformat}
GET /api/v1/permissions/:PERMISSION_ID
{noformat}

*Note: The data layer is stubbed out and left for implementation in AMBARI-13865*",rbac,['ambari-server'],AMBARI,Task,Major,2015-11-16 21:42:13,7
12912592,Add authorizations to permissions so that the definition of a permission (or role) is explicit,"Add authorizations to permissions so that the definition of a permission (or role) is explicit.

A new table needs to be created to store the _authorizations_:
{code}
TABLE roleauthorization (
  authorization_id VARCHAR(100) NOT NULL,
  authorization_name VARCHAR(255) NOT NULL,
  resource_type_id INTEGER NOT NULL,
  PRIMARY KEY(authorization_id)
)
{code}

A new table needs to be added to map _authorizations_ to _permissions_
{code}
TABLE permission_roleauthorization (
  permission_id BIGINT NOT NULL,
  authorization_id VARCHAR(100) NOT NULL,
  PRIMARY KEY(permission_id, authorization_id)
);
{code}

A new Entity needs to be created to hold the authorization record data ({{org.apache.ambari.server.orm.entities.AuthorizationEntity}}).

The existing PermissionEntity {{org.apache.ambari.server.orm.entities.PermissionEntity}} needs to be updated to include AuthorizationEntities.
",rbac,[],AMBARI,Task,Major,2015-11-12 20:11:18,7
12911874,Add (descriptive) label to permission resource and database schema,"Add (descriptive) label to permission resource and database schema to avoid having to hardcode a descriptive name for a permission. For example:


||Permission Name||Permission Label||
|VIEW.USER|View User|
|CLUSTER.USER|Cluster User|
|SERVICE.OPERATOR|Service Operator|
|SERVICE.ADMINISTRATOR|Service Administrator|
|CLUSTER.OPERATOR|Cluster Operator|
|CLUSTER.ADMINISTRATOR|Cluster Administrator|

This descriptive label can be used in user interfaces so that a descriptive (or friendly) name does not have to be hardcoded. 

{noformat:title=API Call Example}
GET api/v1/permissions/1

200 OK
{
  ""href"" : ""http://your.ambari.server/api/v1/permissions/1"",
  ""PermissionInfo"" : {
    ""permission_id"" : 1,
    ""permission_name"" : ""AMBARI.ADMIN"",
    ""permission_label"" : ""Administrator"",
    ""resource_name"" : ""AMBARI""
  }
}
{noformat}",rbac,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2015-11-10 15:08:38,7
12911539,Rename existing permissions to prepare for new roles,"Rename existing permissions to prepare for the new role base access control naming conventions:

* View User:  {{VIEW.USE}} --> {{VIEW.USER}}
* Cluster User:  {{CLUSTER.READ}} --> {{CLUSTER.USER}}
* Cluster Administrator:  {{CLUSTER.OPERATE}} --> {{CLUSTER.ADMINISTRATOR}}
* Administrator: {{AMBARI.ADMIN}} --> {{AMBARI.ADMINISTRATOR}}
",permissions rbac roles,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2015-11-09 14:28:09,7
12910002,Minimize HDFS and other headless keytab distribution (security concerns),"Currently, we distribute the *hdfs* headless principal to pretty much every single host in the cluster.  
Since *hdfs* is a super user in HDFS, if any one of the hdfs keytabs are compromised on any host, the user can do anything on HDFS.
We need to revisit and see if we can restrict the number of hosts to which we distribute the hdfs headless keytab.
For example, we can perform necessary HDFS operations on one of the master hosts available, rather than picking an arbitrary client / slave hosts as we do today.
Also, we should look into not only hdfs headless keytabs but all other headless ones like hbase, storm, etc.",hdfs keytabs security,['ambari-server'],AMBARI,Bug,Critical,2015-11-03 14:16:04,7
12907520,"When adding components to a Kerberized cluster, the set of hosts to create principals for should be limited to only the relevant set","When adding components to a Kerberized cluster, the set of hosts to create principals for should be limited to only the relevant set.  This is because if a component for an existing service is added to an existing host, principals and keytab files for the existing components will get unnecessarily updated. 
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-10-23 18:55:36,7
12907518,ActionScheduler#filterParallelPerHostStages should not filter out stages with server-side actions,"org.apache.ambari.server.actionmanager.ActionScheduler#filterParallelPerHostStages should not filter out stages that contain server-side actions (to be executed on the Ambari server host). This is because tasks in these stages are typically required to complete before other stages on other hosts are executed. 

For example while enabling Kerberos for an added service, principals and keytab files need to be created before the stage to distribute the key tab files. The principal and keytab file creation happens on the Ambari server (as server-side actions) and the distribution tasks happen on the relevant hosts. If the server-side stages are filtered out (in the event multiple stages are pending for the Ambari server host), then one or more might be skipped and the distribution task is queued.  In this scenario, the distribution stage will fail since the required keytab files will not have been created. 
",actions kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-10-23 18:47:24,7
12905052,"Ambari 1.7 to 2.1.x upgrade with existing Kerberos, keytab files fail to be distributed to some hosts","When upgrading from Ambari 1.7 to 2.1.x, where the source cluster has been Kerberized, keytab files fail to distribute to some hosts.  

*Steps to reproduce*
# Create cluster with Ambari 1.7.0 (multiple nodes, required)
# Enable Kerberos (manually)
# Upgrade to Ambari 2.1.2
# Enable Kerberos (automated)
# Not all hosts will receive keytab files

*Cause*
This is due to the way Ambari keeps track of which components (on each host) are properly Kerberized.  So if Kerberos was enabled before the upgrade to 2.x, at some point the agents will report back to Ambari that its components are secured with Kerberos.  Ambari will then use this data to determine which hosts need to be processed when enabling Kerberos. If some of the hosts have reported back before the determination of which hosts need to be process, those hosts will most-likely be skipped and thus, the new keytab files will not be distributed to them.

*Solution*
The solution is to remove outdated code previously used to determine how to handle scenarios where new services are added to a Kerberized cluster.  This code compared the existing services' security state with some desired security state.  If they matched, then no work needed to be done.  The state in particular is {{SECURED_KERBEROS}} and can be seen in the {{security_state}} column of the {{hostcomponentstate}} table.  

The code in question is outdated since new services being added to a Kerberized cluster no longer  requires the invocation of the {{KerberosHelper.toggleKerberos}} method.  The current implementing invokes more granular code to  configure the relevant service(s) and generate the Kerberos identities in a more granular fashion during state changes rather then after the fact. 
 ",kerberos upgrade,['ambari-server'],AMBARI,Bug,Major,2015-10-15 00:20:07,7
12903233,Security-related HTTP headers should be set separately for Ambari Views then for Ambari server UI,"The security-related HTTP headers should be set separately for the Ambari Views then for the Ambari server UI. This is because they have different requirements.  For example the Ambari server UI should not be allowed to execute in an iframe (by default) where Ambari View must be able to execute in an iframe invoked from the same origin.

The relevant headers are:
* Strict-Transport-Security
* X-Frame-Options
* X-XSS-Protection

These headers should be configurable via the ambari.properties such that they may be turned on or off - and set to some custom value.

The default value for this headers should be as follows:
* Strict-Transport-Security: max-age=31536000
* X-Frame-Options: SAMEORIGIN
* X-XSS-Protection: 1; mode=block

Strict-Transport-Security should only be turned on if SSL is enabled.

The relevant Ambari properties should be:
* Strict-Transport-Security: views.http.strict-transport-security
* X-Frame-Options: views.http.x-frame-options
* X-XSS-Protection: views.http.x-xss-protection

By setting any of these to be empty, the header is to be turned off (or not set).

For example:
{code:title=Sets Strict-Transport-Security to a custom value}
views.http.strict-transport-security=max-age=31536000; includeSubDomains
{code}

{code:title=Turns Strict-Transport-Security off}
views.http.strict-transport-security=
{code}
",security,['ambari-server'],AMBARI,Bug,Major,2015-10-08 04:13:43,7
12901986,Kerberos: Retain KDC admin credentials,"Enhance the Kerberos backend to allow for the retention of KDC administrative credentials.  Once securely stored, users may opt to remove the stored credentials.  

See AMBARI-13214 for information on the relevant API calls. 

The alias name for the KDC administrator credential should be *kdc.admin.credential*

For example:

*Create Credential Resource*
{code}
POST /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{
  ""Credential"" : {
    ""principal"" : ""admin/admin@EXAMPLE.COM"",
    ""key"" : ""h4d00p&!"",
    ""type"" : ""persisted""
  }
}
{code}

*Update Credential Resource*
{code}
PUT /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{
  ""Credential"" : {
    ""key"" : ""newpassword"",
    ""type"" : ""temporary""
  }
}
{code}

*Get Credential Resource*
{code}
GET /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{code}

*Delete Credential Resource*
{code}
DELETE /api/v1/clusters/{CLUSTER_NAME}/credentials/kdc.admin.credential
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-10-01 20:32:25,7
12896371,Kerberos: Allow multiple KDC hosts to be set while enabling Kerberos,"Because multiple KDCs may exist for an installation (failover, high availability, etc...), Ambari should allow a user to specify multiple KDC hosts to be set while enabling Kerberos and updating the Kerberos service's configuration.

This should be done by allowing {{kerberos-env/kdc_host}} to accept a (comma-)delimited list of hosts and then parsing that list properly when building the krb5.conf file where each {{kdc_host}} item generates an entry in the relevant realm block.  For example:

{noformat:title=kerberos-env}
{
  ...
 ""kdc_hosts"" : ""kdc1.example.com, kdc2.example.com""
  ...
}
{noformat}

{noformat:title=krb5.conf}
[realms]
  EXAMPLE.COM = {
    ...
    kdc = kdc1.example.com
    kdc = kdc2.example.com
    ...
  }
{noformat}",kerberos,"['ambari-agent', 'ambari-server', 'ambari-web']",AMBARI,Bug,Minor,2015-09-25 10:31:33,7
12896073,kdc_type lost when updating kerberos-env via Kerberos service configuration page,"After editing the kerberos-env configuration using Ambari's Kerberos service configuration page and saving the new configuration, the {{kdc_type}} property is lost and not saved with the new configuration.

*By loosing this value, any future Kerberos-related operations will fail with errors since the mandatory kerberos-env/kdc_type property will be missing.*

{code:title=kerberos-env before update}
{
  ""kdc_type"": ""mit-kdc"",
  ""password_min_uppercase_letters"": ""1"",
  ""password_min_whitespace"": ""0"",
  ""password_min_punctuation"": ""1"",
  ""password_min_digits"": ""1"",
  ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
  ""kdc_create_attributes"": """",
  ""admin_server_host"": ""host1"",
  ""password_min_lowercase_letters"": ""1"",
  ""container_dn"": """",
  ""password_length"": ""20"",
  ""case_insensitive_username_rules"": ""false"",
  ""manage_identities"": ""true"",
  ""service_check_principal_name"": ""${cluster_name}-${short_date}"",
  ""kdc_host"": ""host1"",
  ""ad_create_attributes_template"": ""\n{\n  \""objectClass\"": [\""top\"", \""person\"", \""organizationalPerson\"", \""user\""],\n  \""cn\"": \""$principal_name\"",\n  #if( $is_service )\n  \""servicePrincipalName\"": \""$principal_name\"",\n  #end\n  \""userPrincipalName\"": \""$normalized_principal\"",\n  \""unicodePwd\"": \""$password\"",\n  \""accountExpires\"": \""0\"",\n  \""userAccountControl\"": \""66048\""\n}"",
  ""install_packages"": ""true"",
  ""realm"": ""EXAMPLE.COM"",
  ""ldap_url"": """",
  ""executable_search_paths"": ""/usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin""
}
{code}

{code:title=kerberos-env after update}
{
  ""password_min_uppercase_letters"": ""1"",
  ""password_min_whitespace"": ""0"",
  ""password_min_punctuation"": ""1"",
  ""password_min_digits"": ""1"",
  ""encryption_types"": ""aes des3-cbc-sha1 rc4 des-cbc-md5"",
  ""kdc_create_attributes"": """",
  ""admin_server_host"": ""hist1:88"",
  ""password_min_lowercase_letters"": ""1"",
  ""container_dn"": """",
  ""password_length"": ""20"",
  ""case_insensitive_username_rules"": ""false"",
  ""manage_identities"": ""true"",
  ""service_check_principal_name"": ""${cluster_name}-${short_date}"",
  ""kdc_host"": ""host1:88"",
  ""ad_create_attributes_template"": ""\n{\n  \""objectClass\"": [\""top\"", \""person\"", \""organizationalPerson\"", \""user\""],\n  \""cn\"": \""$principal_name\"",\n  #if( $is_service )\n  \""servicePrincipalName\"": \""$principal_name\"",\n  #end\n  \""userPrincipalName\"": \""$normalized_principal\"",\n  \""unicodePwd\"": \""$password\"",\n  \""accountExpires\"": \""0\"",\n  \""userAccountControl\"": \""66048\""\n}"",
  ""install_packages"": ""true"",
  ""realm"": ""EXAMPLE.COM"",
  ""ldap_url"": """",
  ""executable_search_paths"": ""/usr/bin, /usr/kerberos/bin, /usr/sbin, /usr/lib/mit/bin, /usr/lib/mit/sbin""
}
{code}

{code:title=Javascript Error}
Uncaught TypeError: Cannot read property 'get' of undefined
App.MainServiceInfoConfigsController.Em.Controller.extend.prepareConfigObjects @ app.js:22525
App.MainServiceInfoConfigsController.Em.Controller.extend.parseConfigData @ app.js:22490
App.ConfigsLoader.Em.Mixin.create.loadCurrentVersionsSuccess @ app.js:61506
Em.Object.extend.send.opt.success @ app.js:154010
f.Callbacks.o @ vendor.js:125
f.Callbacks.p.fireWith @ vendor.js:125
w @ vendor.js:127
f.support.ajax.f.ajaxTransport.c.send.d @ vendor.js:127
app.js:55160 App.componentConfigMapper execution time: 1.048ms
{code}

*Steps to reproduce*
# Create cluster (Zookeeper-only is fine)
# Enable Kerberos (any KDC, MIT KDC is fine)
# Browse to Kerberos service configuration page
# Change a value (maybe add or remove the port for the KDC server value)
# Save the configuration
# After view refreshes, the waiting icon appears and does not go away

*Workaround*
Manually add the {{kerberos-env/kdc_type}} property back to the current kerberos-env configuration.  The value must be either ""mit-kdc"" or ""active-directory"" and must be the correct one for the configuration.  Once this is done, Ambari should be restarted so that any cached configuration data is refreshed. 

This can also be fixed using {{/var/lib/ambari-server/resources/scripts/configs.sh}}.",regression,['ambari-web'],AMBARI,Bug,Critical,2015-09-24 10:56:15,7
12895901,"Create a credentials resource used to securely set, update, and remove credentials used by Ambari","Storage of the credentials is to be done using the existing _secure_ credentials provider API which already exits within Ambari.  See {{org.apache.ambari.server.security.encryption.CredentialStoreService}} and {{org.apache.ambari.server.security.encryption.CredentialStoreServiceImpl}}.

Credential may be stored in either Ambari's persistent or temporary secure storage facilities. 

*Test capabilities*
* Request 
{noformat}GET api/v1/clusters/{CLUSTER_NAME}{noformat}
* Responses
{code:title=200 OK}
{
  ...
  ""credential_store_properties"" : {
    ""storage.persistent"" : ""true"",
    ""storage.temporary"" : ""true""
  },
  ...
}
{code}

*Setting the credentials*
* Request 
{noformat}POST /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
{code:title=payload}
{
  ""Credential"" : {
    ""principal"" : ""USERNAME"",
    ""key"" : ""SECRET"",
    ""type"" : ""persisted""
  }
}
{code}
Where:
** principal:  the principal (or username) part of the credential to store
** key: the secret key part of the credential to store
** type:  declares the storage facility type: persisted or temporary
* Responses
{code:title=200 OK}
<empty>
{code}
{code:title=400 Bad Request}
{
  ""status"": 400,
  ""message"": ""Cannot persist credential in Ambari's secure credential store since secure storage has not yet be configured.  Use ambari-server setup-security to enable this feature.""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Updating the credentials*
* Request
{noformat}PUT /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
{code:title=payload}
{
  ""Credential"" : {
    ""principal"" : ""USERNAME"",
    ""key"" : ""SECRET1"",
    ""type"" : ""persisted""
  }
}
{code}
Where:
** principal:  the principal (or username) part of the credential to store
** key: the secret key part of the credential to store
** type:  declares the storage facility type: persisted or temporary
* Responses
{code:title=200 OK}
<empty>
{code}
{code:title=400 Bad Request}
{
  ""status"": 400,
  ""message"": ""Cannot persist credential in Ambari's secure credential store since secure storage has not yet be configured.  Use ambari-server setup-security to enable this feature.""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Removing the credentials*
* Request
{noformat}DELETE /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
* Responses
{code:title=200 OK}
<empty>
{code}
{code:title=404 Not Found}
{
  ""status"": 404,
  ""message"": ""Not Found""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Listing credentials*
* Request
{noformat}GET /api/v1/clusters/{CLUSTER_NAME}/credentials{noformat}
* Responses 
{code:title=200 OK}
{
  ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials"",
  ""items"" : [
    {
      ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials/kdc.admin.credentials"",
      ""Credential"" : {
        ""alias"" : ""kdc.admin.credentials"",
        ""cluster_name"" : ""c1""
      }
    },
    {
      ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials/service.admin.credentials"",
      ""Credential"" : {
        ""alias"" : ""service.admin.credentials"",
        ""cluster_name"" : ""c1""
      }
    }
  ]
}
{code}
{code:title=404 Not Found}
{
  ""status"": 404,
  ""message"": ""Not Found""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}

*Retrieving credentials*
* Request
{noformat}GET /api/v1/clusters/{CLUSTER_NAME}/credentials/{ALIAS}{noformat}
* Responses 
{code:title=200 OK}
{
  ""href"" : ""http://host:8080/api/v1/clusters/c1/credentials/kdc.admin.credentials"",
  ""Credential"" : {
    ""alias"" : ""kdc.admin.credentials"",
    ""cluster_name"" : ""c1"",
    ""type"" : ""persisted""
  }
}
{code}
{code:title=404 Not Found}
{
  ""status"": 404,
  ""message"": ""Not Found""
}
{code}
{code:title=403 Forbidden}
{
  ""status"": 403,
  ""message"": ""You do not have permissions to access this resource.""
}
{code}",security,['ambari-server'],AMBARI,Bug,Critical,2015-09-23 20:45:43,7
12863095,Kerberos: Allow user to specify additional realms for auth-to-local rules,"Allow user to specify additional realms for auth-to-local rules. This will add _default_ rules for the specified realm(s) to the generated auth-to-local rule sets. For example:

{noformat}
RULE:[1:$1@$0](.*@USER_REALM.COM)s/@.*//
{noformat}

The value should be a (comma) delimited list of realm names set in set of global properties in the Kerberos Descriptor.",kerberos kerberos-wizard,"['ambari-server', 'ambari-web']",AMBARI,Bug,Major,2015-09-10 15:46:40,7
12856172,Adding host via blueprint fails on secure cluster,"*STR*
Install cluster via blueprints
Enable Kerberos security
Add host via blueprints

*Result*
Adding hosts freeze forever
In ambari-server.log:
{code}
The KDC administrator credentials must be set in session by updating the relevant Cluster resource.This may be done by issuing a PUT to the api/v1/clusters/(cluster name) API entry point with the following payload:
{
  ""session_attributes"" : {
    ""kerberos_admin"" : {""principal"" : ""(PRINCIPAL)"", ""password"" : ""(PASSWORD)""}
  }
{code}

*Cause*
This is caused because the KDC administrative credentials are not available when needed during the add host process.  If set in the HTTP session, the credentials are not accessible since the Kerberos logic is executed outside the scope of that HTTP session.  

*Solution*
Store the KDC credentials to a _more secure_ global credential store that is accessible no matter what the context is.  This storage facility is in-memory and has a retention period of 90 minutes.  This solution refactors the current CredentialStoreService and MasterKeyService classes to allow for file-based and in-memory implementations. It also paves the way for future changes to allow for the KDC administrative credentials to be persisted indefinitely.",blueprints kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-08-14 14:23:35,7
12855577,Kerberos: Test KDC Connection succeeds but prints ERROR to ambari server log,"When enabling Kerberos, click Test KDC Connection, things succeed but there is an error printed to ambari-server log. This is happening when the KDC listens only on UDP and the TCP check fails.

{code}
06 Aug 2015 21:20:16,179 ERROR [qtp-client-29] KdcServerConnectionVerification:133 - Unable to connect to Kerberos Server
06 Aug 2015 21:20:26,111 ERROR [qtp-client-25] KdcServerConnectionVerification:133 - Unable to connect to Kerberos Server
{code}

*Solution*
Fix logging to be more explicit to avoid confusion
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-08-12 15:12:46,7
12851489,Kerberos: fails check during enable Kerb with SLES,"When executing the Kerberos service check, the following error occurs:
{code}
stderr:   /var/lib/ambari-agent/data/errors-24.txt

Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py"", line 81, in <module>
    KerberosServiceCheck().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 218, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py"", line 64, in service_check
    user=params.smoke_user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 258, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 70, in inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 92, in checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 140, in _call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 291, in _call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -c /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c -kt /etc/security/keytabs/kerberos.service_check.080315.keytab MyCluster-080315@EXAMPLE.COM' returned 1. kinit(v5): Credentials cache permissions incorrect when initializing cache /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c

stdout:   /var/lib/ambari-agent/data/output-24.txt

Performing kinit using MyCluster-080315@EXAMPLE.COM
2015-08-03 19:11:57,085 - Execute['/usr/bin/kinit -c /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c -kt /etc/security/keytabs/kerberos.service_check.080315.keytab MyCluster-080315@EXAMPLE.COM'] {'user': 'jambari-qa'}
2015-08-03 19:11:57,179 - File['/var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c'] {'action': ['delete']}
{code}

This error happens only on SLES, however the cause exists on all platforms.  The other platforms silently ignore the condition; which, however, does not have any bearing on the results of the _kinit_ test. 

*Cause*
The ""Credentials cache permissions incorrect when initializing cache"" issue is caused by the inability to write the Kerberos ticket cache file to the specified location. In the case it is /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_dd529fe1e15538ddfe9ce0347604d64c.  The reason for the write failure is that /var/lib/ambari-agent/data/tmp is not writable by the user executing the _kinit_ call - which is the Ambari smoke test user (typically ambari-qa).  The directory's permissions are
{noformat}
drwxr-xr-x. 4 root root 4096 Aug  3 22:20 /var/lib/ambari-agent/data/tmp/
{noformat}

*Solution*
In order for the ambari smoke test user to be able to write to the relevant directory (/var/lib/ambari-agent/data/tmp), the permissions must be set at least as follows
{noformat}
drwxrwxr-x. 4 root hadoop 4096 Aug  3 22:20 /var/lib/ambari-agent/data/tmp/
{noformat}
However, at the time this directory is created, it is not known what the name of the _hadoop_ group is, so the next best solution is to set the permissions as 
{noformat}
drwxrwxrwx. 4 root root 4096 Aug  3 22:20 /var/lib/ambari-agent/data/tmp/
{noformat}

If the ambari-agent is installed manually via the relevant package manager, the directory is created with the open permissions (777,  drwxrwxrwx) via the packages install_helper.sh post install script.  However if Ambari installs the agent via SSH, the directory is created with the more restrictive permissions (755, drwxr-xr-x) via the agent bootstrap.py script. 

To make these consistent, the following needs to be changed
{code:title=bootstrap.py:650}
   command = ""sudo mkdir -p {0} ; sudo chown -R {1} {0} ; sudo chmod 755 {3} ; sudo chmod 755 {2} ; sudo chmod 755 {0}"".format(
      self.TEMP_FOLDER, quote_bash_args(params.user), DEFAULT_AGENT_DATA_FOLDER, DEFAULT_AGENT_LIB_FOLDER)
{code}
to
{code:title=bootstrap.py (change)}
   command = ""sudo mkdir -p {0} ; sudo chown -R {1} {0} ; sudo chmod 755 {3} ; sudo chmod 755 {2} ; sudo chmod 777 {0}"".format(
      self.TEMP_FOLDER, quote_bash_args(params.user), DEFAULT_AGENT_DATA_FOLDER, DEFAULT_AGENT_LIB_FOLDER)
{code}

*Note:* self.TEMP_FOLDER contains the path to the Ambari agent temp folder (typically, /var/lib/ambari-agent/data/tmp).






",directory-permissions install,['ambari-server'],AMBARI,Bug,Critical,2015-08-04 20:06:14,7
12850567,"When regenerating Kerberos principals, ensure Kerberos Descriptor changes are applied to services","When regenerating Kerberos principals, ensure Kerberos Descriptor changes are applied to services.  

It is possible for changes to be made to the Kerberos Descriptor.  One way to propagate these changes into the service configurations is to process the Kerberos Descriptor and update service configurations while performing tasks for the ""Regenerate Keytabs"" operation. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-07-31 10:23:19,7
12849361,"""kdc_create_attributes"" field should not be required","During security enabling there is present required property ""kdc_create_attributes"" on Configure Kerberos page. But this property should not be required (or should not be empty be default).",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Blocker,2015-07-27 16:13:47,7
12848876,Kerberos configuration properties should be final and uneditable outside the Kerberization wizard.,"Some Kerberos service properties should be final and not be editable outside the wizard.

*kerberos-env*
* Final and not be editable
** kdc_type
** manage_identities
** install_packages
** case_insensitive_username_rules
** realm
 
* Editable (changing them will require Keytabs to be regenerated and services to be restarted)
** ldap_url
** container_dn
** encryption_types
** kdc_host
** admin_server_host

* Editable  (changes will only be seen when [re]generating Kerberos identities and keytab files)
** password_length
** password_min_lowercase_letters
** password_min_uppercase_letters
** password_min_digits
** password_min_punctuation
** password_min_whitespace
** create_attributes_template

* Editable (changes will only be seen when certain operations are invoked)
** executable_search_paths
** service_check_principal_name

*krb5-conf*
* Final and not be editable
** domains
** manage_krb5_conf
** conf_dir
** content",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-07-24 12:25:28,7
12846893,Kerberos: Allow setting/clearing attributes for MIT KDC identities,"Allow for attributes to be set (or unset) for identities created in an MIT (or similar) KDC. 

A user should be able to specify a list of attributes to be set or unset while creating identities using the MIT kadmin utility. For example:
{noformat}
-requires_preauth max_renew_life=7d
{noformat}

*Solution*
Add a property in {{kerberos-env}} or reuse {{kerberos-env/create_attributes_template}} to store the user-specified attributes.",kdc kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-07-22 13:30:17,7
12845909,Kerberos: ServiceResourceProvider queries for KDC connectivity when not needed,"When querying for information about services installed in a Kerberized cluster via the REST API, the ServiceResourceProvider always attempts to contact the KDC (or Active Directory) if the KERBEROS service is selected within the query. 

This can be seen about every 15 seconds,  when the UI queries for the state of the services in a Kerberized cluster using the following query:
{noformat}
GET  /api/v1/clusters/{cluster_name}/services?fields=ServiceInfo/state,ServiceInfo/maintenance_state&minimal_response=true
{noformat}

The result from this query does not contain the KDC connectivity attributes (which is expected), yet the detail are obtained.  

This issue causes excess overhead in Ambari as well as on the relevant KDC or Active Directory. Also the kdamin.log fills up with messages like:
{noformat:title=/var/log/kadmind.log}
Jun 29 14:31:42 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:31:42 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:31:42 some-host-1 kadmind[2383](info): closing down fd 29
Jun 29 14:32:49 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:32:49 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:32:49 some-host-1 kadmind[2383](info): closing down fd 29
Jun 29 14:34:35 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:34:35 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:34:35 some-host-1 kadmind[2383](info): closing down fd 29
Jun 29 14:35:28 some-host-1 kadmind[2383](Notice): Request: kadm5_init, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128, vers=3, flavor=6
Jun 29 14:35:28 some-host-1 kadmind[2383](Notice): Request: kadm5_get_principal, admin/admin@EXAMPLE.COM, success, client=admin/admin@EXAMPLE.COM, service=kadmin/some-host-1.c.pramod-thangali.internal@EXAMPLE.COM, addr=10.240.70.128
Jun 29 14:35:28 some-host-1 kadmind[2383](info): closing down fd 29
{noformat}

*Solution*
Only query for the KDC attributes when explicitly or implicitly queried. This can be done by conditionally setting the relevant properties near {{org/apache/ambari/server/controller/internal/ServiceResourceProvider.java:1394}} by inspecting the request for relevant identifiers using something like the following:
{code}
requestedIds.contains(propertyId) || isPropertyCategoryRequested(propertyId, requestedIds);
{code}
",kerberos rest_api,['ambari-server'],AMBARI,Bug,Major,2015-07-17 15:33:49,7
12843873,kinit of hdfs Kerberos identity fails when starting added service(s) after upgrade to Ambari 2.1.0,"STR:
1. Install old version of ambari (2.0.1)
2. Enable security
3. Do Ambari only upgrade to ambari2.1.0
4. Add some component - HiveServer2 or Ooozie server
5. Try to start added component

Actual result:
Start have been failed. 

{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive_server.py"", line 182, in <module>
    HiveServer().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", 

line 216, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive_server.py"", line 83, in start
    self.configure(env) # FOR SECURITY
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive_server.py"", line 54, in configure
    hive(name='hiveserver2')
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in 

thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-

services/HIVE/0.12.0.2.0/package/scripts/hive.py"", line 127, in hive
    mode=params.webhcat_hdfs_user_mode
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in 

__init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 390, in 

action_create_on_execute
    self.action_delayed(""create"")
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 387, in 

action_delayed
    self.get_hdfs_resource_executor().action_delayed(action_name, self)
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 236, in 

action_delayed
    main_resource.kinit()
  File ""/usr/lib/python2.6/site-

packages/resource_management/libraries/providers/hdfs_resource.py"", line 416, in kinit
    user=user
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in 

__init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 

118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", 

line 254, in action_run
    tries=self.resource.tries, try_sleep=self.resource.try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 70, in 

inner
    result = function(command, **kwargs)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 92, in 

checked_call
    tries=tries, try_sleep=try_sleep)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 140, in 

_call_wrapper
    result = _call(command, **kwargs_copy)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/shell.py"", line 291, in 

_call
    raise Fail(err_msg)
resource_management.core.exceptions.Fail: Execution of '/usr/bin/kinit -kt 

/etc/security/keytabs/hdfs.headless.keytab hdfs@EXAMPLE.COM' returned 1. kinit: Keytab 

contains no suitable keys for hdfs@EXAMPLE.COM while getting initial credentials
{code}


Expected results:
Can start all added components.

*Cause*
The Kerberos Descriptor structure changed between Ambari 2.0 and Ambari 2.1.  This change moved the ""hdfs"" Kerberos identity descriptor from the _global_ scope to under the HDFS service. After upgrading from Ambari 2.0 to Ambari 2.1  an additional ""hdfs"" Kerberos identity descriptor was added with the new principal name pattern - $\{hadoop-env/hdfs_user\}-$\{cluster_name\}@$\{realm\}.  This occurred because the stored Kerberos Descriptor contained the _old_ structure, and when Ambari generated a composite Kerberos Descriptor made up of the Kerberos Descriptor compiled from the relevant stack definition with stored changes applied, that additional ""hdfs"" Kerberos identity descriptor was added.  Because if this, the Kerberos logic became _confused_ and overwrote the existing hdfs keytab file with one that contained the new principal name.

*Solution*
While migrating Ambari 2.0 to Ambari 2.1, fix the stored Kerberos Descriptor structure to match the new version's structure.
",kerberos upgrade,['ambari-server'],AMBARI,Bug,Major,2015-07-09 14:31:03,7
12843592,Kerberos: LDAP error updating and removing service principals in AD,"An LDAP error occurs while updating and removing service principals in clusters with Kerberos enabled using Active Directory.

The following exception is thrown when removing an account:
{noformat}
javax.naming.NamingException: [LDAP: error code 1 - 000020D6: SvcErr: DSID-0310081B, problem 5012 (DIR_ERROR), data 0
]
noformat}

The following exception is thrown when updating an account's password
{noformat}
javax.naming.NamingException: [LDAP: error code 1 - 000020D6: SvcErr: DSID-0310081B, problem 5012 (DIR_ERROR), data 0
^@]
{noformat}
",active_directory kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-07-08 18:10:46,7
12842349,hive-site/hive.metastore.sasl.enabled value incorrect when adding Hive to a Kerberized Cluster,"When *adding* Hive to an existing Kerberized cluster, the {{hive-site/hive.metastore.sasl.enabled}} value is set to {{false}} when it should be {{true}}.  If Hive was installed before enabling Kerberos,  
{{hive-site/hive.metastore.sasl.enabled}} is set to {{true}} after enabling Kerberos.

If {{hive-site/hive.metastore.sasl.enabled}} is {{false}} in a Kerberized cluster, the following error can be seen in the hiverserver2.log:

{noformat:title=/var/log/hive/hiveserver2.log}
2015-07-01 23:35:16,128 ERROR [HiveServer2-Handler-Pool: Thread-37]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Unsupported mechanism type GSSAPI
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.thrift.transport.TTransportException: Unsupported mechanism type GSSAPI
        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)
        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:138)
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
        ... 4 more
{noformat}

*Cause*
It appears that the front end is updating the Kerberos Descriptor _artifact_ with _old_ data rather than the data the is specified on the stack's Kerberos Descriptor. This occurs during the transition between the ""Review"" and ""Install, Start, Test"" pages of the ""Add Service Wizard"".

*Solution*
Use the current Kerberos Descriptor's values as default value for the updated Kerberos Descriptor and update only what the user changes in the relevant fields.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-07-02 15:44:10,7
12842106,HCat Service Check warns keytab contains no suitable keys when Kerberos is enabled,"HCat Service Check (part of the Hive Service Check) fails in cluster where Kerberos is enabled:

{noformat}
Test connectivity to hive server
Waiting for the Hive server to start...
2015-07-01 18:39:17,173 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-tezview@EXAMPLE.COM; '] {'user': 'ambari-qa'}
2015-07-01 18:39:17,321 - Execute['! beeline -u 'jdbc:hive2://c6502.ambari.apache.org:10000/;transportMode=binary;principal=hive/_HOST@EXAMPLE.COM' -e '' 2>&1| awk '{print}'|grep -i -e 'Connection refused' -e 'Invalid URL''] {'path': ['/bin/', '/usr/bin/', '/usr/lib/hive/bin/', '/usr/sbin/'], 'user': 'ambari-qa', 'timeout': 30}
Successfully connected to c6502.ambari.apache.org on port 10000
Successfully connected to Hive at c6502.ambari.apache.org on port 10000 after 6 seconds
2015-07-01 18:39:23,313 - File['/var/lib/ambari-agent/data/tmp/hcatSmoke.sh'] {'content': StaticFile('hcatSmoke.sh'), 'mode': 0755}
2015-07-01 18:39:23,314 - Execute['/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa; env JAVA_HOME=/usr/jdk64/jdk1.8.0_40 /var/lib/ambari-agent/data/tmp/hcatSmoke.sh hcatsmokeida8c06641_date390115 prepare'] {'logoutput': True, 'path': ['/usr/sbin', '/usr/local/bin', '/bin', '/usr/bin', '/usr/sbin:/sbin:/usr/lib/ambari-server/*:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/var/lib/ambari-agent:/usr/hdp/current/hive-client/bin:/usr/hdp/current/hadoop-client/bin'], 'tries': 3, 'user': 'ambari-qa', 'try_sleep': 5}
kinit: Keytab contains no suitable keys for ambari-qa@EXAMPLE.COM while getting initial credentials
WARNING: Use ""yarn jar"" to launch YARN applications.
{noformat}

The issue appears to be the wrong principal name in the {{kinit}} command - note the missing cluster name and realm in the principal name value.
{noformat}
/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa
{noformat}

*Cause*
The error is caused by the use of the wrong variable when generating the kinit command at common-services/HIVE/0.12.0.2.0/package/scripts/hcat_service_check.py:44
{noformat}
        {kinit_path_local} -kt {smoke_user_keytab} {smokeuser}
{noformat}

*Solution*
At common-services/HIVE/0.12.0.2.0/package/scripts/hcat_service_check.py:44, change {{smokeuser}} to {{smokeuser_principal}}.",hive kerberos,['ambari-server'],AMBARI,Bug,Major,2015-07-01 21:00:30,7
12841036,Enabling Kerberos on cluster with AMS and no HDFS fails,"In a cluster where AMS is installed but HDFS is _not_ installed, enabling Kerberos fails due to the inability for the server-side Kerberos logic to replace ${hadoop-env/hdfs_user} when generating the metadata used to create principals and distribute keytab files.

This condition yields the following principal (when the cluster name is AMSNOHDFS and the realm is EXAMPLE.COM)
{noformat}
    $\{hadoop-env/hdfs_user\}-AMSNOHDFS@EXAMPLE.COM
{noformat}

This is successfully created in the (MIT) KDC. Also, the relative keytab file appears to have been successfully created as well.

However, when distributing the keytab file and setting the ownership attributes, the agent-side script fails with 
{code}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 77, in <module>
    KerberosClient().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 216, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 67, in set_keytab
    self.write_keytab_file()
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_common.py"", line 397, in write_keytab_file
    group=group)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 157, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 152, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 118, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 108, in action_create
    self.resource.group, mode=self.resource.mode, cd_access=self.resource.cd_access)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 44, in _ensure_metadata
    _user_entity = pwd.getpwnam(user)
KeyError: 'getpwnam(): name not found: $\{hadoop-env/hdfs_user\}'
{code}

*NOTE: \ needed to be added to the hadoop-env/hdfs_user placeholder due to formatting issue*

*Solution:* 
Remove the HDFS identity reference in AMS and assume the hdfs keytab file will be on the appropriate host(s) when HDFS is installed
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-27 01:15:39,7
12840700,Kerberos: prompts for HDFS ambari principals w/o hdfs in cluster,"Installed HDP 2.2 cluster with only storm, zk, kafka, knox.

When enabling Kerberos, the _Configure Principals_ page of the _Kerberos Wizard_  prompts for the HDFS Ambari principal and keytab file although HDFS is not installed in cluster.

The HDFS Ambari principal and keytab file should also not display once enabled. (under Admin > Kerberos).

*Solution*
* Move the HDFS identity to the HDFS Kerberos Descriptor (from the top-level descriptor). 
* Change references to ""/hdfs"" to be ""/HDFS/hdfs""

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-26 00:14:05,7
12840352,Kerberos: storm headless princ + keytab seem incorrect,"* The storm user principals should be derived using the storm user + cluster name: 
{noformat}
storm-${cluster_name}@${realm}
{noformat}

* The keytab name should be consistent with other headless identities: 
{noformat}
storm.headless.keytab
{noformat}

*Solution*
Update Storm's Kerberos descriptor to fix the relevant identitiy ",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-24 21:54:40,7
12840257,Falcon Server Web UI alert fails when Kerberos is enabled,"Falcon Server Web UI alert fails when Kerberos is enabled.  The error message is 
{noformat}
agent/data/tmp/web_alert_cc_2144dc375b8556f5a0c2629adedd2a99 -kt {{falcon-startup.properties/*.falcon.http.authentication.kerberos.keytab}} {{falcon-startup.properties/*.falcon.http.authentication.kerberos.principal}} > /dev/null' returned 1. kinit: Client not found in Kerberos database while getting initial credentials)
{noformat}

*Cause*
This issue was introduced when the patch for AMBARI-11656 was applied.  

The issue is related to this line:
{code:title=ambari_agent/alerts/base_alert.py:217}
      replacement_match_regex = r""{{(%s)}}"" % placeholder_key
{code}

When the relative falcon properties are applied, the generated regular expression becomes 
{noformat}
{{(falcon-startup.properties/*.falcon.http.authentication.kerberos.keytab)}}
{noformat}

Which wants to match on values like:
* falcon-startup.properties.falcon.http.authentication.kerberos.keytab
* falcon-startup.properties/.falcon.http.authentication.kerberos.keytab
* falcon-startup.properties/////////////Rfalcon.http.authentication.kerberos.keytab

Not the one we really want - falcon-startup.properties/*.falcon.http.authentication.kerberos.keytab

Either the {{*}} needs to be escaped or the use of regular expressions needs to be changed.

*Solution*
Remove the regular expression replacement and use somple string replacement instead. ",alerts kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2015-06-24 17:02:13,7
12838805,Retry of Kerberize Cluster doesn't work,"*Steps to reproduce:*
# Launch Enable Kerberos wizard
# Proceed to Kerberize Cluster step
# Call to Kerberize Cluster failed
# Hit Retry

*Actual Result:*
Retry doesn't work
JS error: {{Uncaught TypeError: this.unkerberizeCluster is not a function}}",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-06-18 14:05:14,7
12838173,Kerberos Wizard: Moving between steps 2 and 3 back/forward cause API error.,"STR:
* Install cluster with HDFS, Hive and dependcies.
* Start Kerberos Wizard
* Proceed to step 3 *Install and Test Kerberos Client*
* Go back to step2 *Configure Kerberos*
* Click on next button

AR: Popup with request error shown
{noformat}
500 status code recieved on POST method for API: /api/v1/clusters/c1/hosts

Error message: org.apache.ambari.server.controller.spi.SystemException: An internal system exception occurred: ServiceComponent not found, clusterName=c1, serviceName=KERBEROS, serviceComponentName=KERBEROS_CLIENT
{noformat}
After popup confirmation click on Next button doesn't do anything.
ER: No errors shown, user can proceed to the next step.

Workaround: Refresh page on step2 and hit Next button.",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-06-16 14:06:33,7
12837801,Kerkeros Service Configs: KDC Type Value is corrupted,"*Steps to reproduce:*
1. Install Kerberos on the Cluster
2. Go to Kerberos Configs
3. Change any value in the configuration
4. Click Save and Confirm

*Expect Result:*
The KDC Type Value should remain unchanged.
The user should be able to make further modifications to the configs after this points.

*Actual Result:*
The Kerberos KDC Type Value is changed to the descriptive value and thus is unexpected.  Since this is a required field, but cannot be edited, the user can no longer save any changes after this point",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Major,2015-06-15 10:37:08,7
12837406,Kerberos: Oozie auth rules do not look correct,"Due to the fact that the oozie.authentication.kerberos.name.rules are  auto-generated the following rules should be removed from oozie-site.xml from all the stack configurations:

{panel:title=oozie-site.xml}
RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUSER/
RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
DEFAULT
{panel}

Note: this is an update for AMBARI-11179 to fix older stacks. ",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-06-12 13:25:54,7
12836249,Service configs are not updated with enabling Kerberos unless the service has identities to process,"Service configs are not updated with enabling Kerberos unless the service has identities to process. This is an issue in the case a client component requires a configuration change when Kerberos is enabled and the Service has no Kerberos identities. 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-06-08 19:23:56,7
12836110,Ambari creation of oozie/conf/adminusers.txt breaks oozie role seperation for Kerberos,"Oozie restart from Ambari rewrites oozie/conf/adminusers.txt

To support role separation for Kerberos, we need an additional line added to the end of this file.

The new line is:
{noformat}
oozie-admin
{noformat}

The new file should be generated as:
{code}
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Users should be set using following rules:
#
#     One user name per line
#     Empty lines and lines starting with '#' are ignored

oozie
oozie-admin
{code}

*Solution*
1. Replace
{code:title=common-services/OOZIE/4.0.0.2.0/package/templates/adminusers.txt.j2}
{{oozie_user}}
{code}
with 
{code:title=common-services/OOZIE/4.0.0.2.0/package/templates/adminusers.txt.j2}
{% if oozie_admin_users %}
{% for oozie_admin_user in oozie_admin_users.split(',') %}
  {{oozie_admin_user}}
{% endfor %}
{% endif %}
{code}

2. Add new property
{code:title=common-services/OOZIE/4.0.0.2.0/configuration/oozie-env.xml}
  <property>
    <name>oozie_admin_users</name>
    <value>oozie, oozie-admin</value>
    <description>Oozie admin users.</description>
  </property>
{code}

3. If the admin user list needs to change when enabling Kerberos, oozie-env/oozie_admin_users can be set in Oozie's Kerberos descriptor (kerberos.json).
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-06-08 10:31:33,7
12835919,Invalid property value set in core-site.xml when KNOX HA is enabled,"In KNOX-HA cluster, noticed that the “hadoop.proxyuser.knox.hosts"" property in /etc/hadoop/conf/core-site.xml
 refers to only one node and knox is running on multiple nodes.
{code}
 <property>
      <name>hadoop.proxyuser.knox.hosts</name>
      <value>host1</value>
    </property>
{code}
The value for this property should include all the knox hosts
{code}
 <property>
      <name>hadoop.proxyuser.knox.hosts</name>
      <value>host1,host4,etc...</value>
    </property>
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-06 11:40:08,7
12835914,Kerberos: adjust ambari headless principals for unique names,"1) Rollup all headless principal names up to Ambari Principals tab. Currently looks like Storm and Spark are on second tab, under their section, not under Ambari tab with ambari-qa, hdfs, hbase, etc. Also make sure the UI has user readable labels like the others for consistency (see the screen shot. spark.history.kerberos.principal should be ""Spark user principal"" for example).

2) By default, all of these to be cluster-name scoped by default. {code}-${cluster_name}{code} It does no harm for those that don't care... And for those that care about headless principal names to be unique, this ends up being done by default (and saves the user from having to remember to set it this way).

Ultimately when users want to add variables to their principal names they will be doing it across the board - whatever we can do to make it easier for users to do so, would be better.  If we had all principals in one pane they can quickly add all of them and visually validate.

*Solution*
Update the details for all _user_ ({{identities/type = user}}) Kerberos Identity entries in {{kerberos.json}} files to add the following to the principal name
{code}
-${cluster_name}
{code}

For example:
{code}
${hadoop-env/hdfs_user}@${realm}
{code}

to

{code}
${hadoop-env/hdfs_user}-${cluster_name}@${realm}
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-06-06 10:31:42,7
12834810,Falcon version command failed on secure runs,"After enabling Kerberos, the value for property *.falcon.http.authentication.kerberos.name.rules is set to 
{code}
RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/
RULE:[1:$1@$0](hdfs@EXAMPLE.COM)s/.*/hdfs/
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/ams/
RULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/ams/
RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](falcon@EXAMPLE.COM)s/.*/falcon/
RULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/hive/
RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/
RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/
RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/
DEFAULT
{code}

This is incorrect. The correct value should be 
{code}
RULE:[1:$1@$0](ambari-qa@EXAMPLE.COM)s/.*/ambari-qa/ \
RULE:[1:$1@$0](hdfs@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*// \
RULE:[2:$1@$0](amshbase@EXAMPLE.COM)s/.*/ams/ \
RULE:[2:$1@$0](amszk@EXAMPLE.COM)s/.*/ams/ \
RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](falcon@EXAMPLE.COM)s/.*/falcon/ \
RULE:[2:$1@$0](hive@EXAMPLE.COM)s/.*/hive/ \
RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/ \
RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](nfs@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/ \
RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/ \
RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/ \
RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/ \
RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/ \
DEFAULT
{code}

Please notice the ""\"" at end of each RULE. This is needed because of the type of configuration file the data is in - a (Java) properties file, where properties values must be a single line or escaped if multi-lined. 

*Solution*
Convert the multi-line auth-to-local rule to meet the requirements of the configuration file type by allowing the _concatenation type_ to be specified in the Kerberos descriptor.  The following concatenation types are allowed:
* *new lines* - each rule is separated by a new line 
* *new_lines_escaped* - each rule is separated by an escaped new line
* *spaces* - each rule is separated by a whitespace charater



",kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Blocker,2015-06-02 21:01:58,7
12834323,Kerberos: provide option to set test account name,"In many situations with large-scale Active Directory deployments, the krb5.conf is managed outside of Ambari.  This krb5.conf file is configured with all of the DC's in the AD domain, and the outbound requests to the KDC from clients are load balanced across those servers.  In many scenarios the user replication latency causes issues with users not found during the test process.  Due to the fact that we generate a new user every time we test, this can get users to a circular situation in which they can never leave this state because of multi-KDC's in their krb5.conf and delay associated with replication.

1) Expose the option to set the test kerberos client principal name (under Advanced kerberos-env)
2) Default the value to something unique, but less than 20 characters {code}
${cluster_name}-${short_date}
{code}",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-06-01 18:27:10,7
12832854,"Kerberos: UI shows Kerberize Cluster step as failed with a retry button, but the backend keeps moving forward to Kerberize the cluster","Precondition: On the cluster where this was observed, there were previous attempts to kerberize the cluster.

STR:
Go through the Enable Kerberos Wizard.
The only non-default option taken was to not manage krb5.conf (presented on the second page of the wizard).

Chrome developer tool shows that there was a POST on /api/v1/clusters/woah/artifacts/kerberos_descriptor failing with 409.  

{code}
{
""status"" : 409,
""message"" : ""Attempted to create an artifact which already exists,
artifact_name='kerberos_descriptor',
foreign_keys='{Artifacts/cluster_name=woah}'""
}
{code}

It doesn't seem like this is the cause of the issue (though we need to investigate).

The UI keeps showing a spinner for several minutes, then shows a failure.
This is because a call to PUT on the cluster resource to set security_type takes more than 3 minutes, and the browser aborts the request.
However, the backend kept moving forward to Kerberize the cluster (ambari-server.log was being tailed to check on progress).
After verifying that all principals and keytabs were generated/distributed, the wizard was closed (the last step of the wizard is to start all services and run service checks, but this was skipped because the previous step failed.)
The cluster was in fact successfully Kerberized.

*Note:* The condition is likely to have occurred due to a timeout related to the number of hosts and services in the cluster.  The preparation phase of enabling Kerberos is performed within the handler of the relavant API call. Most of this work should be moved out to a stage which is handled asynchronously.",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-26 20:25:19,7
12832853,"Kerberos: UI shows Kerberize Cluster step as failed with a retry button, but the backend keeps moving forward to Kerberize the cluster","Precondition: On the cluster where this was observed, there were previous attempts to kerberize the cluster.

STR:
Go through the Enable Kerberos Wizard.
The only non-default option taken was to not manage krb5.conf (presented on the second page of the wizard).

Chrome developer tool shows that there was a POST on /api/v1/clusters/woah/artifacts/kerberos_descriptor failing with 409.  

{code}
{
""status"" : 409,
""message"" : ""Attempted to create an artifact which already exists,
artifact_name='kerberos_descriptor',
foreign_keys='{Artifacts/cluster_name=woah}'""
}
{code}

It doesn't seem like this is the cause of the issue (though we need to investigate).

The UI keeps showing a spinner for several minutes, then shows a failure.
This is because a call to PUT on the cluster resource to set security_type takes more than 3 minutes, and the browser aborts the request.
However, the backend kept moving forward to Kerberize the cluster (ambari-server.log was being tailed to check on progress).
After verifying that all principals and keytabs were generated/distributed, the wizard was closed (the last step of the wizard is to start all services and run service checks, but this was skipped because the previous step failed.)
The cluster was in fact successfully Kerberized.

*Note:* The condition is likely to have occurred due to a timeout related to the number of hosts and services in the cluster.  The preparation phase of enabling Kerberos is performed within the handler of the relavant API call. Most of this work should be moved out to a stage which is handled asynchronously.",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-26 20:23:48,7
12832574,Kerberos: display warning that the user must create/distribute principals and keytabs,"In Ambari 1.7.0, we used to display a warning to the end user to create principals/keytabs when:
* a new service is added to the cluster (Review page)
* a new host is added to the cluster (Review page)
* a new component is added to an existing host (dialog popup)
We need to bring these back when and only when the cluster is already kerberized with ""Manual"" option.

*Note: This is a fix for an issue that AMBARI-11244 created where if security is disabled, the user is not able to add components on host details page.  Upon clicking on add component nothing happens.*
",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-25 21:21:14,7
12832381,Kerberos: Creating principals in AD when special characters are involved causes failures,"Creating principals in AD when special characters are involved causes failures.

The following characters in the CN need to be escaped:
{noformat}
/ , \ # + < > ; "" =
{noformat}

*Note:* javax.naming.ldap.Rdn will properly escape relative distinguished name parts.


The following characters in the sAMAccountName need to be removed or replaced:
{noformat}
[ ] : ; | = + * ? < > / \
{noformat}

*Note:* This needs to be done explicitly within the attributes set if a relevant entry exists.
{code}
// Replace the following _illegal_ characters: [ ] : ; | = + * ? < > / \
value = value.toString().replaceAll(""\\[|\\]|\\:|\\;|\\||\\=|\\+|\\*|\\?|\\<|\\>|\\/|\\\\"", ""_"");
{code}
",active-directory active_directory kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-23 21:31:34,7
12832341,"Kerberos FE: during disable, need option skip if unable to access KDC to remove principals","Attempted to disable kerb, fails on step to unkerberize because KDC admin is locked out.

Click retry, can't make it past that.

Need option to skip and finish ""disable kerberos"" even if Ambari cannot get the principals cleaned up (i.e. cannot access the KDC) Losing access to the KDC and attempting to disable where ambari can't clean-up the principals should be a skip'able step. User should still be able to get to a clean, not-enabled-kerberos-ambari-state w/o accessing the KDC.

*Solution*
Based on user input, execute API call to disable Kerberos with the *manage_kerberos_identities* _directive_ set to *false*.  Example:
{code:title=PUT /api/v1/clusters/c1?manage_kerberos_identities=false}
{
  ""Clusters"": {
    ""security_type"" : ""NONE""
  }
}
{code}
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-23 11:29:03,7
12832257,Finer-grained role AuthZ for Ambari Users,"Ambari currently integrates with external authentication systems and is able to authenticate users using enterprise-wide LDAP systems, such as Active Directory, OpenLDAP, and Apache Directory Service. However, more flexibility is now needed to allow for those authenticated users to be segmented into more granular roles.  These roles allow Ambari-level administrators to create different levels of cluster-level administrators to manage certain administrative operations that need to be performed on a cluster. This effectively spreads out the responsibilities of managing a cluster while not handing over total control of the Ambari management facility. 

Ambari to provide role-based access controls beyond today's Ambari Admin, Operator and Read-Only permissions.

|| Role || Description ||
| *Cluster User* (was Read-only) | This exists as of Ambari 1.7.0. Read-only view of cluster information, including configurations, service status and health alerts|
| *Service Operator* | Provides control of service lifecycle (start/stop/restart/decomm/recom) |
| *Service Administrator* | Service Operator + ability to re-configure (change/compare/revert), configure HA |
| *Cluster Operator* | Service Administrator + add/remove hosts and components (for existing services) |
| *Cluster Administrator* | Cluster Operator + enable/disable kerberos, modify alerts, add service, perform upgrade (renamed from Operator) |
| Administrator | This exists as of Ambari 1.7.0. Full cluster control + manage user, groups and views and this flag is applicable to any user regardless of Role |

Each role is to have permissions as shown below:

|| || Cluster\\User || Service\\Operator || Service\\Administrator || Cluster\\Operator || Cluster\\Administrator || Administrator ||
||Service-level Permissions||
|View metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configurations           |(+)|(+)|(+)|(+)|(+)|(+)|
|Compare configurations        |(+)|(+)|(+)|(+)|(+)|(+)|
|View alerts        |(+)|(+)|(+)|(+)|(+)|(+)|
|Start/Stop/Restart Service    |   |(+)|(+)|(+)|(+)|(+)|
|Decommission/recommission     |   |(+)|(+)|(+)|(+)|(+)|
|Run service checks            |   |(+)|(+)|(+)|(+)|(+)|
|Turn on/off maintenance mode  |   |(+)|(+)|(+)|(+)|(+)|
|Perform service-specific tasks|   |(+)|(+)|(+)|(+)|(+)|
|Modify configurations         |   |   |(+)|(+)|(+)|(+)|
|Manage configuration groups   |   |   |(+)|(+)|(+)|(+)|
|Move to another host          |   |   |(+)|(+)|(+)|(+)|
|Enable/disable alerts          |   |   |(+)|(+)|(+)|(+)|
|Enable HA                     |   |   |(+)|(+)|(+)|(+)|
|Add Service to cluster        |   |   |   |   |(+)|(+)|
||*Host-level Permissions*||
|View metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|Turn on/off maintenance mode  |   |   |   |(+)|(+)|(+)|
|Install components            |   |   |   |(+)|(+)|(+)|
|Add/Delete hosts              |   |   |   |(+)|(+)|(+)|
||Cluster-level Permissions||
|View metrics                  |(+)|(+)|(+)|(+)|(+)|(+)|
|View status information       |(+)|(+)|(+)|(+)|(+)|(+)|
|View configuration            |(+)|(+)|(+)|(+)|(+)|(+)|
|View stack version details    |(+)|(+)|(+)|(+)|(+)|(+)|
|View alerts                   |(+)|(+)|(+)|(+)|(+)|(+)|
|Enable/disable alerts         |   |   |   |   |(+)|(+)|
|Enable/disable Kerberos       |   |   |   |   |(+)|(+)|
|Upgrade/downgrade stack       |   |   |   |   |(+)|(+)|
||Ambari-level Permissions||
|Create new clusters           |   |   |   |   |   |(+)|
|Set service users and groups  |   |   |   |   |   |(+)|
|Rename clusters               |   |   |   |   |   |(+)|
|Manage users                  |   |   |   |   |   |(+)|
|Manage groups                 |   |   |   |   |   |(+)|
|Manage Ambari Views           |   |   |   |   |   |(+)|
|Assign permissions/roles      |   |   |   |   |   |(+)|
|Manage stack versions         |   |   |   |   |   |(+)|
|Edit stack repository URLs    |   |   |   |   |   |(+)|

*NOTE:  [^AmbariRole-basedAccessControl.pdf] claims the RBAC update is available in Ambari 2.2.0, however it was not implemented until Ambari 2.3.0 and further.*",permissions rbac roles,['ambari-server'],AMBARI,Epic,Major,2015-05-22 20:19:51,7
12832018,DDL errors seen on a cluster while enabling Kerberos,"{noformat}
1 May 2015 00:18:53,950  INFO [Server Action Executor Worker 256] KerberosServerAction:436 - Processing identities completed.
21 May 2015 00:18:56,754  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:18:57,658  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:18:57,926  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:19:00,677  INFO [qtp-ambari-agent-83] HeartBeatHandler:822 - SET_KEYTAB called
21 May 2015 00:19:12,502 ERROR [ambari-action-scheduler] ClusterImpl:2459 - ServiceComponentHost lookup exception
21 May 2015 00:19:21,850 ERROR [ambari-action-scheduler] ClusterImpl:2459 - ServiceComponentHost lookup exception
21 May 2015 00:19:21,870  INFO [Server Action Executor Worker 263] KerberosServerAction:332 - Processing identities...
21 May 2015 00:19:21,873  INFO [Server Action Executor Worker 263] DestroyPrincipalsServerAction:90 - Destroying identity, ambari-qa_ruocwwby@EXAMPLE.COM
21 May 2015 00:19:22,123 ERROR [Server Action Executor Worker 263] AmbariJpaLocalTxnInterceptor:114 - [DETAILED ERROR] Rollback reason:
Local Exception Stack:
Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
Internal Exception: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_KRB_PR_HOST_PRINCIPALNAME) violated - child record found

Error Code: 2292
Call: DELETE FROM kerberos_principal WHERE (principal_name = ?)
        bind => [1 parameter bound]
        at org.eclipse.persistence.exceptions.DatabaseException.sqlException(DatabaseException.java:331)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:900)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
        at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
        at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
        at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
        at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72)
        at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52)
        at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$9e172ec6.remove(<generated>)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:107)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processRecord(KerberosServerAction.java:504)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:393)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:64)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:504)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:441)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_KRB_PR_HOST_PRINCIPALNAME) violated - child record found

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
        at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
        at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
        at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
        ... 23 more
21 May 2015 00:19:22,126 ERROR [Server Action Executor Worker 263] AmbariJpaLocalTxnInterceptor:122 - [DETAILED ERROR] Internal exception (1) :
java.sql.SQLIntegrityConstraintViolationException: ORA-02292: integrity constraint (AMBARICUSTOMUSER.FK_KRB_PR_HOST_PRINCIPALNAME) violated - child record found

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:207)
        at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:1010)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3576)
        at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3657)
        at oracle.jdbc.driver.OraclePreparedStatementWrapper.executeUpdate(OraclePreparedStatementWrapper.java:1350)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeDirectNoSelect(DatabaseAccessor.java:890)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.executeNoSelect(DatabaseAccessor.java:962)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.basicExecuteCall(DatabaseAccessor.java:631)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatch(ParameterizedSQLBatchWritingMechanism.java:149)
        at org.eclipse.persistence.internal.databaseaccess.ParameterizedSQLBatchWritingMechanism.executeBatchedStatements(ParameterizedSQLBatchWritingMechanism.java:134)
        at org.eclipse.persistence.internal.databaseaccess.DatabaseAccessor.writesCompleted(DatabaseAccessor.java:1836)
        at org.eclipse.persistence.internal.sessions.AbstractSession.writesCompleted(AbstractSession.java:4244)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.writesCompleted(UnitOfWorkImpl.java:5594)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.acquireWriteLocks(UnitOfWorkImpl.java:1646)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitTransactionAfterWriteChanges(UnitOfWorkImpl.java:1614)
        at org.eclipse.persistence.internal.sessions.RepeatableWriteUnitOfWork.commitRootUnitOfWork(RepeatableWriteUnitOfWork.java:284)
        at org.eclipse.persistence.internal.sessions.UnitOfWorkImpl.commitAndResume(UnitOfWorkImpl.java:1169)
        at org.eclipse.persistence.internal.jpa.transaction.EntityTransactionImpl.commit(EntityTransactionImpl.java:132)
        at org.apache.ambari.server.orm.AmbariJpaLocalTxnInterceptor.invoke(AmbariJpaLocalTxnInterceptor.java:91)
        at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:72)
        at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:52)
        at org.apache.ambari.server.orm.dao.KerberosPrincipalDAO$$EnhancerByGuice$$9e172ec6.remove(<generated>)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.processIdentity(DestroyPrincipalsServerAction.java:107)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processRecord(KerberosServerAction.java:504)
        at org.apache.ambari.server.serveraction.kerberos.KerberosServerAction.processIdentities(KerberosServerAction.java:393)
        at org.apache.ambari.server.serveraction.kerberos.DestroyPrincipalsServerAction.execute(DestroyPrincipalsServerAction.java:64)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.execute(ServerActionExecutor.java:504)
        at org.apache.ambari.server.serveraction.ServerActionExecutor$Worker.run(ServerActionExecutor.java:441)
        at java.lang.Thread.run(Thread.java:745)
21 May 2015 00:19:22,127  WARN [Server Action Executor Worker 263] DestroyPrincipalsServerAction:119 - Failed to remove identity for ambari-qa_ruocwwby@EXAMPLE.COM from the Ambari database - Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-22 01:16:00,7
12831109,Add Service Wizard requests KDC information when cluster is configured for Manual Kerberos,"Add Service Wizard requests KDC information when cluster is configured for Manual Kerberos. 

See screen shots:

!Ambari - ManKerb 2015-05-11 15-11-04.png!

When configured for Kerberos with the _Manual_ option, the _Add Host Wizard_ should not warn about or prompt for KDC details.  The _wizard_ should also allow the user to download a CSV file of expected (user-generated) principals and keytabs files from the Review (or conformation page).  
Additional working on the Review/Confirmation page may be needed to inform the user that they should create the principals and keytabs before continuing. 

To know whether the cluster is configured for _manual kerberos_:
* Cluster.security_type = ""KERBEROS""
* kerberos-env/manage_identities = ""false""",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-19 18:10:09,7
12831083,Kerberos: display warning that the user must create/distribute principals and keytabs,"In Ambari 1.7.0, we used to display a warning to the end user to create principals/keytabs when:
* a new service is added to the cluster (Review page)
* a new host is added to the cluster (Review page)
* a new component is added to an existing host (dialog popup)
We need to bring these back when and only when the cluster is already kerberized with ""Manual"" option.",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-19 17:11:16,7
12830542,templeton.hive.properties property value substitution should be done by ambari-server,"The webhcat-site/templeton.hive.properties property value substitution should be done by ambari-server.  This value is more complicated than others since the (embedded) hive.metastore.uris property is a list of thrift URIs generated using the set of hosts the Hive Metastore is installed on.  This list of hosts comes from the clusterHostInfo/hive_metastore_host value, which in the KerberosHelper (org.apache.ambari.server.controller.KerberosHelper) is available as a comma-delimited list of hosts.

{code}
clusterHostInfo/hive_metastore_host = ""host1,host2,host3""
{code}

To generate configuration values when enabling Kerberos, the KerberosHelper class uses org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor#replaceVariables to replace variables specified in the Kerberos Descriptor.  Currently this mechanism uses a simple replacement scheme, which is not sufficient to generate string using a delimited list of values. 

In order to solve this issue, ""functions"" need to be applied to replacement data before making the substitution.  In this case a ""function"" named ""each"" will be created that takes the following arguments:
* pattern with placeholders
* delimiter to use to concatenate values generated using the patter
* regex to use to split the original string

For example: 
{code:title=function declaration, commas are escaped when not intended to separate function args}
each(thrift://%s:9083, \\,, \s*\,\s*)
{code}

To indicate this function is to be used, the following Kerberos Descriptor variable replacement syntax is to be used:
{code}
${clusterHostInfo/hive_metastore_host|each(thrift://%s:9083, \\,, \s*\,\s*)}
{code}

Note: \ characters need to be escaped in JSON structure values.  For example: 
{code}
""some.property"" : ""${clusterHostInfo/hive_metastore_host|each(thrift://%s:9083, \\\\,, \\s*\\,\\s*)}""
{code}

If clusterHostInfo/hive_metastore_host = ""host1,host2,host3"", the result would be 
{code}
thrift://host1:9083\,thrift://host2:9083\,thrift://host3:9083
{code}",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-05-17 22:33:15,7
12830226,Kerberos: Oozie auth rules do not look correct,"0) create cluster, hDP 2.2, build 1203
1) Kerb cluster (hdfs, yarn,zk)
2) add ozzie
3) add hbase
4) everything seems ok.
5) I went and looked at oozie configs, oozie.authentication.kerberos.name.rules property looks like this...is this correct?

{code}
RULE:[1:$1@$0](ambari-qa-MyCluster@EXAMPLE.COM)s/.*/ambari-qa/
RULE:[1:$1@$0](hbase-MyCluster@EXAMPLE.COM)s/.*/hbase/
RULE:[1:$1@$0](hdfs-MyCluster@EXAMPLE.COM)s/.*/hdfs/
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/@.*//
RULE:[1:$1@$0](.*@.*TODO-KERBEROS-DOMAIN)s/@.*//
RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUSER/
RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
RULE:[2:$1@$0](dn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](hbase@EXAMPLE.COM)s/.*/hbase/
RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
RULE:[2:$1@$0](jhs@EXAMPLE.COM)s/.*/mapred/
RULE:[2:$1@$0](jn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](nm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](nn@EXAMPLE.COM)s/.*/hdfs/
RULE:[2:$1@$0](oozie@EXAMPLE.COM)s/.*/oozie/
RULE:[2:$1@$0](rm@EXAMPLE.COM)s/.*/yarn/
RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
RULE:[2:$1@$0](yarn@EXAMPLE.COM)s/.*/yarn/
DEFAULT
{code}


*Solution*
Remove the following values for oozie-site/oozie.authentication.kerberos.name.rules

{code:title=common-services/OOZIE/4.0.0.2.0/configuration/oozie-site.xml:145}
      RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUxSER/
      RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
      RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      DEFAULT
{code}

{code:title=common-services/OOZIE/5.0.0.2.3/configuration/oozie-site.xml:24}
      RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUxSER/
      RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/
      RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/
      DEFAULT
{code}",keberos,['ambari-server'],AMBARI,Bug,Major,2015-05-15 16:52:08,7
12830177,Kerberos: Principals fail to be created if whitespace exists in generated passwords,"Principals fail to be created if whitespace exists in generated passwords.

{code:title=failure condition}
add_principal -pw 265 V$^+91axff4i56 ambari-qa@EXAMPLE.COM
{code}

To fix this, quotes are needed around the password:

{code:title=fix}
add_principal -pw ""265 V$^+91axff4i56"" ambari-qa@EXAMPLE.COM
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-15 14:23:15,7
12829980,Kerberos: service page incorrect with manual,"After enabling kerb manual, the configs tab under Kerberos is incorrect.

1) the type should indicate the accurate type (existing MIT, ad or manual), just like the wizard
2) the kadmin host is showing red but this is manual and it's not required
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-14 20:53:31,7
12829868,Add host kerberos client install does not have command text,"Kerb cluster, add host.
See screen shot

This issue appears to be a front end issue.  Here is the culprit: 

{code:title=app/templates/wizard/step9/step9HostTasksLogPopup.hbs:47}
{{taskInfo.role}} {{taskInfo.command}}
{code}

Rather then attempting to use the task's _command_detail_ value, the front end only displays the task's _role_ and _command_ values which are constrained by the code to be a certain set of values. The _command_detail_ field does not have that constraint.  

For the offending items in the relevant view,  the following values are set:
{code}
role: KERBEROS_CLIENT (translated somewhere to ""Kerberos Client"")
command: CUSTOM_COMMAND (translated somewhere to ""custom command"")
command_details: SET_KEYTAB KERBEROS/KERBEROS_CLIENT
{code}

On this note there are several items in that same display (for the Ambari server host) that read ""Ambari Server Action execute"". This is the same issue.

!add_service_log.png!

The background operations view displays these values properly:

!background_operations_log.png!
",kerberos,['ambari-web'],AMBARI,Bug,Critical,2015-05-14 14:43:22,7
12829865,"Kerberos: Selecting ""Manual"" option and then going back to ""Existing KDC"" option loses KDC host field and connection tester","*Steps to reproduce:*
# Create cluster
# Enable Kerberos
# Select Manual Option - See no KDC Host field and test button (correct)
# Select Back button
# Select MIT KDC Option - See no KDC Host field and test button (incorrect)


",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-14 14:29:33,7
12829317,Kerberos: missing identities for AMS in the CSV,"There are some naming consistency issues and there are missing entries in the CSV related to AMS.

*Naming inconsistencies*
* ams.zookeeper.principal
** Is: zookeeper/_HOST@$\{realm\}
** Should be: amszk@_HOST$\{realm\}
* ams.zookeeper.keytab 
** Is: $\{keytab_dir\}/zk.service.ams.keytab
** Should be: $\{keytab_dir\}/ams-zk.service.keytab

*Missing CSV entries*
* ams.collector.ketab
* ams-hbase.regionserver.keytab

",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-05-12 20:18:30,7
12828832,falcon client not initalizing for secure clusters,"{code}
# falcon admin -version
ERROR: Unable to initialize Falcon Client object
{code}

It would also be immensely helpful if we print some logs some where for this. Or create an env var that when set allows us to see debug logs because i have no idea why this error came up and how to debug any further. Nothing ever made it to the server side logs.

*Solution*
The *.falcon.http.authentication.kerberos.name.rules need to be uodated such that they are the same as hadoop.security.auth_to_local. To do this, add the following under the relevant service item in the Falcon kerberos.json file:

{code}
      ""auth_to_local_properties"" : [
        ""falcon-startup.properties/*.falcon.http.authentication.kerberos.name.rules""
      ],
{code}

For example:

{code}
{
  ""services"": [
    {
      ""name"": ""FALCON"",
      ...
      ""auth_to_local_properties"" : [
        ""falcon-startup.properties/*.falcon.http.authentication.kerberos.name.rules""
      ],
      ...
   }
 ]
}
{code}


",falcon kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-05-11 14:40:22,7
12828388,Kerberos Wizard: Hide create_attributes_template when not configuring for Active Directory ,"Hide create_attributes_template when not configuring for Active Directory since it does not apply for MIT KDC configuration
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Major,2015-05-08 15:08:07,7
12828373,Kerberos: cleanup Manual kerb req text,"Update checkbox ""reqs"" on Page 1 of wizard with choice of Manual Kerb:

* Cluster hosts have network access to the KDC
* Kerberos client utilities (such as kinit) have been installed on every cluster host
* The Java Cryptography Extensions (JCE) have been setup on the Ambari Server host and all hosts in the cluster
* The Service and Ambari Principals will be manually created in the KDC before completing this wizard
* The keytabs for the Service and Ambari Principals will be manually created and distributed to cluster hosts before completing this wizard
",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-08 14:44:32,7
12828368,Kerberos: manual kerb is installing kerb packages + overwriting krb5.conf,"enable kerb, select manual kerb option.

As first task inStop Services, it's performing kerb client install, which is installs 1) kerb packages + 2) overwriting my krb5.conf (shouldn't do either of these since this is a manual kerb).

{code}
2015-05-05 19:09:31,878 - Skipping installation of existing package krb5-workstation
2015-05-05 19:09:31,882 - Directory['/etc'] {'owner': 'root', 'group': 'root', 'mode': 0755, 'recursive': True}
2015-05-05 19:09:32,030 - File['/etc/krb5.conf'] {'owner': 'root', 'content': Template('krb5_conf.j2'), 'group': 'root', 'mode': 0644}
2015-05-05 19:09:32,206 - Writing File['/etc/krb5.conf'] because contents don't match
2015-05-05 19:09:32,491 - Execute['ambari-sudo.sh  -H -E touch /var/lib/ambari-agent/data/hdp-select-set-all.performed ; ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.2 | tail -1`'] {'not_if': 'test -f /var/lib/ambari-agent/data/hdp-select-set-all.performed', 'only_if': 'ls -d /usr/hdp/2.2*'}
2015-05-05 19:09:32,553 - Skipping Execute['ambari-sudo.sh  -H -E touch /var/lib/ambari-agent/data/hdp-select-set-all.performed ; ambari-sudo.sh /usr/bin/hdp-select set all `ambari-python-wrap /usr/bin/hdp-select versions | grep ^2.2 | tail -1`'] due to not_if
2
{code}

*Solution*
The wizard needs to set the following configurations when ""Manual"" is selected:

{{kerberos-env/manage_identities}} = ""false""
{{kerberos-env/install_packages}} = ""false""
{{krb5-conf/manage_krb5_conf}} = ""false""
{{kerberos-env/kdc-type}} = ""none""",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-08 13:52:21,7
12828358,Kerberos Wizard: Downloaded CSV file contains only default data,"The CSV file that is downloaded from the Kerberos Wizard does not contain the configured Kerberos identity details - it contains only default data. 

This is due to where in the workflow the Kerberos descriptor is posted to Ambari (as an artifact).  The data *is* posted during the transition between the _Stop Services_ page and the _Kerberize Cluster_ page. The data *should be* posted during the transition between the _Configure Identities_ page and the _Confirm Configuration_ page.",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-08 12:57:17,7
12828309,"Kerberos service check is named ""Create principals""","# Deploy cluster
# Enable security
# Run Kerberos service check

In background operations Kerberos service check is named as ""Create principals""",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-05-08 10:08:04,7
12828075,Ambari does not renew kerberos tickets,"If I go through setup of secure cluster, also execute ambari-server setup-security option #3, there are 2 problems:
# Ambari does not kinit using the specified principal and keytab
# If I manually kinit, all is well until the ticket expires - I would expect Ambari to renew ticket prior to expiry.
# The process uses the interactive user's ticket cache (separate Jira: AMBARI-11001)",JAAS,['ambari-server'],AMBARI,Bug,Critical,2015-05-07 18:25:55,7
12828073,Ambari uses users' interactive ticket cache,"It appears that it is necessary to kinit prior to starting ambari-server, even after ambari-server setup-security (#3). It seems that this should be automatically handled by Ambari. 

Ambari-server should NOT use the same ticket cache as the interactive user. 

STR:
1. kinit
2. ambari-server start
3. verify that ambari-server can authenticate with ticket specified in #1
4. kdestroy
5. try to authenticate through Ambari again (it will not work)

*Solution*
Ensure JAAS Login works properly such that the Kerberos tickets for the account that executes Ambari is not relevant.

",JAAS,['ambari-server'],AMBARI,Bug,Critical,2015-05-07 18:22:50,7
12827792,Kerberos: no need for Manage Identities checkbox,"See attached. On page 2 of the Kerb wizard, we have a checkbox for ""manage kerberos identities"". if the user chooses existing MIT or AD on page 1, this is a given. If they choose manage manually, this is not required.

Doesn't seems like there is a need for this checkbox on the wizard since page 1 answers the question.",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-06 20:10:51,7
12827786,"Kerberos: ""Attention: Some configurations need your attention"" msg in Enable Kerb Wizard hides fields","In the *Enable Kerberos Wizard*'s *Configure Kerberos* step when there are configurations that require attention the 'Attention: Some configurations need your attention before you can proceed.
+Show me properties with issues+' alert shows up.

When users click +Show me properties with issues+, and resolve their problems the alert disappears.  The issue is that there is no way to see all of the non-required properties without purposefully messing up one of the required ones so the alert comes back with the ""+Show all properties+"" link in it.

What should happen is as follows:
# If there are properties that require attention, a yellow attention div is presented with ""Attention: Some configurations need your attention before you can proceed.
+Show me properties with issues+""
# The user clicks on that link and start addressing the required configurations.
# When all of the configurations have been filled out that div is replaced with a green div with ""All configurations have been addressed. +Show all properties+""

See attached screenshots for example of issue in current implementation",kerberos kerberos-wizard,['ambari-web'],AMBARI,Bug,Critical,2015-05-06 19:38:51,7
12827440,Fix KERBEROS service reference to common services in HDP 2.3,"Fix KERBEROS service reference to common services in HDP 2.3

The metainfo.xml file should read
{code:title=stacks/HDP/2.3/services/KERBEROS/metainfo.xml}
<metainfo>
  <schemaVersion>2.0</schemaVersion>
  <services>
    <service>
      <name>KERBEROS</name>
      <extends>common-services/KERBEROS/1.10.3-10</extends>
    </service>
  </services>
</metainfo>
{code}",kerberos,['ambari-server'],AMBARI,Task,Critical,2015-05-05 21:38:35,7
12826963,kerberos-env/kdc_type must not be mandatory if kerberos-env/manage_identities is false.,{{kerberos-env/kdc_type}} must not be mandatory if {{kerberos-env/manage_identities}} is _false_.,kerberos,['ambari-server'],AMBARI,Bug,Major,2015-05-04 16:59:12,7
12826445,Kerberos Identity data is empty when Cluster.security_type != KERBEROS,"Kerberos Identity data is empty when Cluster.security_type does not equal ""KERBEROS"".  

Technically this seems to make sense since there shouldn't be any relevant Kerberos identities if the cluster isn't configured for Kerberos; however a _chicken-and-the-egg_ condition is encountered when manually enabled Kerberos and a listing of the needed Kerberos identities is needed before Kerberos is to be fully enabled. 

*Solution*
Allow the keberos_identities API end points to generate data even if Cluster.security_type is not equal to ""KERBEROS"".",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-04-30 22:09:31,7
12826040,Kerberos: Password generator needs to generate passwords based on a pattern to satisfy password policy,"The password generator used to generate passwords for identities needs to generate passwords based on a pattern rather than just a random sequence of characters. 

Within the KDC, there may be a policy in place requiring a certain characteristics for the password. By creating a password consisting if 18 characters pulled randomly from {{abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890?.!$%^*()-_+=~}}, there is no guarantee that any specific policy will be met. 
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-29 18:19:32,7
12822521,Use kerberos-env/executable_search_paths to indicate where to look for Kerberos utilities in HBASE agent-side scripts,"Use kerberos-env/executable_search_paths to indicate where to look for Kerberos utilities in HBASE agent-side scripts.

See solution for AMBARI-10452.",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-21 01:22:19,7
12821979,Add the ability to obtain details about required Kerberos identities,"Add the ability to obtain details about required Kerberos identities for the cluster.   These details should be obtained using a REST API call formatted as a JSON structure.  

Resulting JSON block per Kerberos identity:
{code}
    ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
    }
{code}

The data will be converted into CSV-formatted data similar to the file exported from Ambari 1.7 (as follows):

||host||description||principal||keytab file name||keytab file base path||keytab file owner||keytab file group||keytab file mode||
|host1|Ambari Smoke Test User|ambari-qa@EXAMPLE.COM|smokeuser.headless.keytab|/etc/security/keytabs|ambari-qa|hadoop|440|
|host1|HDFS User|hdfs@EXAMPLE.COM|hdfs.headless.keytab|/etc/security/keytabs|hdfs|hadoop|440|
|host1|HDFS SPNEGO User|HTTP/host1@EXAMPLE.COM|spnego.service.keytab|/etc/security/keytabs|root|hadoop|440|
|host1|HDFS SPNEGO User|HTTP/host1@EXAMPLE.COM|spnego.service.keytab|/etc/security/keytabs|root|hadoop|440|
|host1|DataNode|dn/host1@EXAMPLE.COM|dn.service.keytab|/etc/security/keytabs|hdfs|hadoop|400|
|host1|NameNode|nn/host1@EXAMPLE.COM|nn.service.keytab|/etc/security/keytabs|hdfs|hadoop|400|
|host1|ZooKeeper Server|zookeeper/host1@EXAMPLE.COM|zk.service.keytab|/etc/security/keytabs|zookeeper|hadoop|400|


*Solution*
The following API calls are to be used to obtain the data:

{code:title=GET /api/v1/clusters/c1/hosts?fields=kerberos_identities/*}
{
  ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts?fields=kerberos_identities/*"",
  ""items"" : [
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host1""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/HTTP%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/dn%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/nm%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/nn%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""namenode_nn"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""nn/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host1/kerberos_identities/zookeeper%2Fhost1%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""zookeeper_zk"",
            ""host_name"" : ""host1"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""zookeeper"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""zookeeper/host1@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host2""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/HTTP%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/dn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/jhs%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""history_server_jhs"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""mapred"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/jhs.service.keytab"",
            ""principal_local_username"" : ""mapred"",
            ""principal_name"" : ""jhs/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/nm%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/nn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""secondary_namenode_nn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""nn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/rm%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""resource_manager_rm"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/rm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""rm/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/yarn%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""app_timeline_server_yarn"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/yarn.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""yarn/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host2/kerberos_identities/zookeeper%2Fhost2%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""zookeeper_zk"",
            ""host_name"" : ""host2"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""zookeeper"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""zookeeper/host2@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3"",
      ""Hosts"" : {
        ""cluster_name"" : ""c1"",
        ""host_name"" : ""host3""
      },
      ""kerberos_identities"" : [
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/HTTP%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/spnego"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""root"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
            ""principal_local_username"" : null,
            ""principal_name"" : ""HTTP/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/smokeuser"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""ambari-qa"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
            ""principal_local_username"" : ""ambari-qa"",
            ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/amshbase%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""ams_hbase_master_hbase"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""ams"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/ams-hbase.master.keytab"",
            ""principal_local_username"" : ""ams"",
            ""principal_name"" : ""amshbase/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/dn%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""datanode_dn"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""dn/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/hdfs%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""/hdfs"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : ""r"",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""440"",
            ""keytab_file_owner"" : ""hdfs"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
            ""principal_local_username"" : ""hdfs"",
            ""principal_name"" : ""hdfs@EXAMPLE.COM"",
            ""principal_type"" : ""USER""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/nm%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""nodemanager_nm"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""yarn"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
            ""principal_local_username"" : ""yarn"",
            ""principal_name"" : ""nm/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        },
        {
          ""href"" : ""http://ambari:8080/api/v1/clusters/c1/hosts/host3/kerberos_identities/zookeeper%2Fhost3%40EXAMPLE.COM"",
          ""KerberosIdentity"" : {
            ""cluster_name"" : ""c1"",
            ""description"" : ""ams_zookeeper"",
            ""host_name"" : ""host3"",
            ""keytab_file_group"" : ""hadoop"",
            ""keytab_file_group_access"" : """",
            ""keytab_file_installed"" : ""true"",
            ""keytab_file_mode"" : ""400"",
            ""keytab_file_owner"" : ""ams"",
            ""keytab_file_owner_access"" : ""r"",
            ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.ams.keytab"",
            ""principal_local_username"" : ""ams"",
            ""principal_name"" : ""zookeeper/host3@EXAMPLE.COM"",
            ""principal_type"" : ""SERVICE""
          }
        }
      ]
    }
  ]
}
{code}

{code:title=GET /api/v1/clusters/c1/kerberos_identities?fields=*}
{
  ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities?fields=*"",
  ""items"" : [
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nn%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""namenode_nn"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""nn/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost1%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""zookeeper_zk"",
        ""host_name"" : ""host1"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""zookeeper"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""zookeeper/host1@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/jhs%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""history_server_jhs"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""mapred"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/jhs.service.keytab"",
        ""principal_local_username"" : ""mapred"",
        ""principal_name"" : ""jhs/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""secondary_namenode_nn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""nn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/rm%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""resource_manager_rm"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/rm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""rm/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/yarn%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""app_timeline_server_yarn"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/yarn.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""yarn/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost2%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""zookeeper_zk"",
        ""host_name"" : ""host2"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""zookeeper"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""zookeeper/host2@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/HTTP%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/spnego"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""root"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/spnego.service.keytab"",
        ""principal_local_username"" : null,
        ""principal_name"" : ""HTTP/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/ambari-qa%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/smokeuser"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""ambari-qa"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/smokeuser.headless.keytab"",
        ""principal_local_username"" : ""ambari-qa"",
        ""principal_name"" : ""ambari-qa@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/amshbase%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""ams_hbase_master_hbase"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""ams"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/ams-hbase.master.keytab"",
        ""principal_local_username"" : ""ams"",
        ""principal_name"" : ""amshbase/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/dn%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""datanode_dn"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/dn.service.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""dn/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/hdfs%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""/hdfs"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : ""r"",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""440"",
        ""keytab_file_owner"" : ""hdfs"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/hdfs.headless.keytab"",
        ""principal_local_username"" : ""hdfs"",
        ""principal_name"" : ""hdfs@EXAMPLE.COM"",
        ""principal_type"" : ""USER""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/nm%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""nodemanager_nm"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""yarn"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/nm.service.keytab"",
        ""principal_local_username"" : ""yarn"",
        ""principal_name"" : ""nm/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    },
    {
      ""href"" : ""http://ambari:8080/api/v1/clusters/c1/kerberos_identities/zookeeper%2Fhost3%40EXAMPLE.COM"",
      ""KerberosIdentity"" : {
        ""cluster_name"" : ""c1"",
        ""description"" : ""ams_zookeeper"",
        ""host_name"" : ""host3"",
        ""keytab_file_group"" : ""hadoop"",
        ""keytab_file_group_access"" : """",
        ""keytab_file_installed"" : ""true"",
        ""keytab_file_mode"" : ""400"",
        ""keytab_file_owner"" : ""ams"",
        ""keytab_file_owner_access"" : ""r"",
        ""keytab_file_path"" : ""/etc/security/keytabs/zk.service.ams.keytab"",
        ""principal_local_username"" : ""ams"",
        ""principal_name"" : ""zookeeper/host3@EXAMPLE.COM"",
        ""principal_type"" : ""SERVICE""
      }
    }
  ]
}
{code}

{code:title=GET /api/v1/clusters/c1/kerberos_identities?fields=*&format=csv}
host,description,principal name,principal type,local username,keytab file path,keytab file owner,keytab file owner access,keytab file group,keytab file group access,keytab file mode,keytab file installed
host1,/spnego,HTTP/host1@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host1,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host1,datanode_dn,dn/host1@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host1,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host1,nodemanager_nm,nm/host1@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host1,namenode_nn,nn/host1@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/nn.service.keytab,hdfs,r,hadoop,,400,true
host1,zookeeper_zk,zookeeper/host1@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/zk.service.keytab,zookeeper,r,hadoop,,400,true
host2,/spnego,HTTP/host2@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host2,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host2,datanode_dn,dn/host2@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host2,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host2,history_server_jhs,jhs/host2@EXAMPLE.COM,SERVICE,mapred,/etc/security/keytabs/jhs.service.keytab,mapred,r,hadoop,,400,true
host2,nodemanager_nm,nm/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host2,secondary_namenode_nn,nn/host2@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/nn.service.keytab,hdfs,r,hadoop,,400,true
host2,resource_manager_rm,rm/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/rm.service.keytab,yarn,r,hadoop,,400,true
host2,app_timeline_server_yarn,yarn/host2@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/yarn.service.keytab,yarn,r,hadoop,,400,true
host2,zookeeper_zk,zookeeper/host2@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/zk.service.keytab,zookeeper,r,hadoop,,400,true
host3,/spnego,HTTP/host3@EXAMPLE.COM,SERVICE,,/etc/security/keytabs/spnego.service.keytab,root,r,hadoop,r,440,true
host3,/smokeuser,ambari-qa@EXAMPLE.COM,USER,ambari-qa,/etc/security/keytabs/smokeuser.headless.keytab,ambari-qa,r,hadoop,r,440,true
host3,ams_hbase_master_hbase,amshbase/host3@EXAMPLE.COM,SERVICE,ams,/etc/security/keytabs/ams-hbase.master.keytab,ams,r,hadoop,,400,true
host3,datanode_dn,dn/host3@EXAMPLE.COM,SERVICE,hdfs,/etc/security/keytabs/dn.service.keytab,hdfs,r,hadoop,,400,true
host3,/hdfs,hdfs@EXAMPLE.COM,USER,hdfs,/etc/security/keytabs/hdfs.headless.keytab,hdfs,r,hadoop,r,440,true
host3,nodemanager_nm,nm/host3@EXAMPLE.COM,SERVICE,yarn,/etc/security/keytabs/nm.service.keytab,yarn,r,hadoop,,400,true
host3,ams_zookeeper,zookeeper/host3@EXAMPLE.COM,SERVICE,ams,/etc/security/keytabs/zk.service.ams.keytab,ams,r,hadoop,,400,true
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-04-17 19:37:38,7
12821690,NameNode Restart fails after attempt to Kerberize Cluster,"When attempting to restart the HDFS NameNode after running the Kerberos wizard to enable Kerberos, the NameNode fails to startup.  

The underlying failure in the ambari-agent appears to be:

""Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 298, in <module>
    NameNode().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 214, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 72, in start
    namenode(action=""start"", rolling_restart=rolling_restart, env=env)
  File ""/usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py"", line 89, in thunk
    return fn(*args, **kwargs)
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py"", line 38, in namenode
    setup_ranger_hdfs()
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py"", line 66, in setup_ranger_hdfs
    hdfs_repo_data = hdfs_repo_properties()
  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py"", line 194, in hdfs_repo_properties
    config_dict['dfs.datanode.kerberos.principal'] = params._dn_principal_name
AttributeError: 'module' object has no attribute '_dn_principal_name'""

This keeps the HDFS NameNode from starting up properly after Kerberos is Enabled, and this seems to keep the process of Enabling Kerberos from completing.  

The problem appears to be a Python coding issue where _private_ variables (declared with a leading underscore) are not imported from {{common-services/HDFS/2.1.0.2.0/package/scripts/params_linux.py}} into {{common-services/HDFS/2.1.0.2.0/package/scripts/setup_ranger_hdfs.py}}.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-04-16 21:47:26,7
12820969,Provide option to not install Kerberos Client packages,"In some environments, the Kerberos Client packages are already installed on the machines and the operator does not want to install the OS kerberos client packages.

Ambari should provide an option in the Enable Kerberos Wizard to not install Kerberos Clients on the machines.

Under the Advanced krb.conf section, at the bottom, expose this checkbox for this option:

{code}
[ ] Do not install Kerberos Client packages
{code}

After enabling Kerberos, this configuration option should be visible under the Services > Kerberos > Configs.

This would instruct Ambari to not install the Kerberos Client packages when installing the Kerberos Client component. We can still have Ambari install the Kerberos Client component, just circumvent the install packages.

*Note:* If selected, it is assumed that the user will install packages containing executables compatible with the MIT Kerberos 5 version 1.10.3",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-14 21:18:58,7
12820902,Add the ability to enable Kerberos and not manage identities,"Add the ability to enable Kerberos and not manage identities.  This should be done by allowing a user to specify whether all relevant Kerberos identities _should_ or _should not_ be managed by Ambari.  

A *kerberos-env* property named *manage_identities* is to be added where its value may be either _true_ or _false_.  By default the value is _true_ (or rather _not false_).  

If _not false_, Ambari will access the registered KDC to create, update, and delete Kerberos identities as needed.  Ambari will also create, distribute, and delete keytab files as needed. Because of this, the KDC administrator credentials are required. This is the current behavior of Ambari 2.0.0.

If _false_, Ambari will *not* access the registered KDC to create, update, or delete Kerberos identities.  It will also *not* create, distribute, or delete keytab files. Not KDC administrator credentials will be needed.

Note: a lot of this work has been done for AMBARI-10305.  A current known problem with the solution for AMBARI-10305 is that the Kerberos service check fails when kerberos-env/manage_identities is false due to missing data since the special smoke user was not created.",kerberos,['ambari-server'],AMBARI,Task,Major,2015-04-14 18:46:43,7
12820899,Manually enable Kerberos security,"Provide an option for users that want to enable Kerberos in the cluster via Ambari but do not want any automation. With this option, ambari will not require any access to the KDC, will not install kerberos clients, will not attempt to generate any principals or keytabs and will not distribute any keytabs. Keytab regeneration will not be available, and when there are changes to the cluster (add service, add/remove/change host), the user is responsible for creating principals and making sure the appropriate keytabs are in place on the host for proper cluster function (although Ambari should handle updating any configs).

Effectively, this above option provides a manual Kerberos option for users that are looking to have the similar ""hands-off"" ambari kerberos experience of 1.7.0 or earlier.

On the Kerberos Wizard, provide an option (below Existing MIT KDC and Existing Active Directory):

[ ] Manage Kerberos principals and keytabs manually

Which will send the wizard thru a path that does not prompt for KDC information, or attempt to install clients or create principals/keytabs. The user should have a chance to Configure Identities as part of the wizard and the wizard will push the configs, performs restarts, etc. Users should have an option to download a CSV of principals, keytabs, hosts, locations, permissions, ownership.",kerberos,"['alerts', 'ambari-agent', 'ambari-server']",AMBARI,Epic,Major,2015-04-14 18:41:23,7
12820294,Ambari unable to start services using non-default kinit_path_local,"PROBLEM: Ambari is unable to start services after running the Enable Security wizard on a cluster that uses a non-standard path to the Kerberos utilities, such as kinit.

STEPS TO REPRODUCE: 
1. Start with non-Kerberized cluster (2.2 Sandbox works fine)
2. Move Kerberos utilities from /usr/bin/ to a new location, example: /usr/myorg/bin/
3. Run 'Enable Security' wizard in Ambari, specify new path for kinit, Apply
4. Watch 'Start All Services' step fail
5. Attempt to 'Restart all components with Stale Configs for HDFS,' which fails with the following error:

Fail: Execution of ' -kt /etc/security/keytabs/hdfs.headless.keytab hdfs' returned 127. -bash: -kt: command not found

Due to the error that occurs with trying to manually restart the HDFS service, it seems like kinit_path_local is empty when the path to kinit is modified. It looks like each service uses the function from:
./ambari-common/src/main/python/resource_management/libraries/functions/get_kinit_path.py

But typically only these three hardcoded paths are passed to that function:
/usr/bin, /usr/kerberos/bin, /usr/sbin

The custom path defined in Ambari is never passed to that function, so the result is always empty.
",kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2015-04-11 11:07:32,7
12787208,"Kerberos: during disable, need option skip if unable to access KDC to remove principals","Attempted to disable kerb, fails on step to unkerberize because KDC admin is locked out.

Click retry, can't make it past that.

Need option to skip and finish ""disable kerberos"" even if Ambari cannot get the principals cleaned up (i.e. cannot access the KDC) Losing access to the KDC and attempting to disable where ambari can't clean-up the principals should be a skip'able step. User should still be able to get to a clean, not-enabled-kerberos-ambari-state w/o accessing the KDC.

*Solution*
Add a flag to the kerberos-env configuration to specify whether Kerberos identities should be managed by Ambari (true, default) or not (false).  This flag is to be overridable via a _directive_ like {{manage_identities=false}} when disabling Kerberos, which will skip over any KDC administrative processes. ",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-04-01 00:35:43,7
12786203,Principal and Keytab configuration specifications are ignored when disabling Kerberos,"Principal and Keytab configuration specifications are ignored when disabling Kerberos and thus they are not updated or removed as needed.
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-27 13:51:24,7
12784826,Storm service check failed after disabling security,"From nimbus.log
{code}
 java.lang.NullPointerException: null
	at backtype.storm.security.auth.authorizer.SimpleACLAuthorizer.permit(SimpleACLAuthorizer.java:93) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.5.1.jar:na]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:773) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:777) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.daemon.nimbus$fn__4017$exec_fn__1597__auto__$reify__4032.getClusterInfo(nimbus.clj:1215) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:1668) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:1656) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:32) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:34) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at backtype.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:156) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632) ~[storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at org.apache.thrift7.server.THsHaServer$Invocation.run(THsHaServer.java:201) [storm-core-0.9.3.2.2.3.0-2611.jar:0.9.3.2.2.3.0-2611]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_67]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_67]
{code}

*Solution*
The solution is to remove configuration properties that do not have default values when disabling Kerberos.

{code}
For each property updated per the Kerberos descriptor
    Look up the default value from the stack definition
    If a value is found, update the property to that ""default"" value
    If a value is not found, remove the value from the configuration
{code}

For Storm in particular, remove the following properties:
- storm-site/drpc.authorizer
- storm-site/nimbus.authorizer
- storm-site/storm.principal.tolocal
- storm-site/ui.filter",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-23 14:36:52,7
12782947,Wrong alert on added host on secure cluster,"*STR*
Install HDP
Enable security
Add host with datanode

*Result*
For added host there is an alert:
Connection failed to http://c6404.ambari.apache.org:1022
Manually this webpage is avaliable

*Expected*
No alert

*Cause*
The error that gets generated is 
{code}
Local variable 'error_msg' might be referenced before assignment
{code}

*Solution*
Fix Python code by initializing {{error_msg}} properly.,",alerts,['ambari-server'],AMBARI,Bug,Critical,2015-03-18 17:29:11,7
12782535,Hive alert on secured cluster,"When Kerberos is enabled, Hive components show alerts due to the following error:

{code}
WARNING 2015-03-16 06:01:08,253 base_alert.py:140 - [Alert][hive_metastore_process] Unable to execute alert. Execution of '/usr/bin/kinit -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa; ' returned 1. kinit: Keytab contains no suitable keys for ambari-qa@REALM while getting initial credentials
{code}

This occurs because the alert logic for Hive uses {{cluster-env/smokeuser}} rather than {{cluster-env/smokeuser_principal_name}} to get the principal name for the smoke test identity. 
",alerts kerbeos,['ambari-server'],AMBARI,Bug,Critical,2015-03-17 11:07:16,7
12781984,Kerberos service check doesn't work is new HTTP session is created,"# Create cluster
# Enable security
# Logout or restart Ambari server
# Go to Kerberos service page
# Click service actions
# Click Run Service Check
# Nothing happens",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-14 09:48:29,7
12781640,"The path(s) to the Kerberos utilities (kadmin, klist, etc...) should be configurable","The path(s) to the Kerberos utilities (kadmin, klist, etc...) should be configurable so that the utilities can be found if using custom Kerberos packages. 

This should work for both the Ambari server and agent-side functions. 
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-12 22:44:36,7
12781567,Some of Hive principal fields doesn't have default value when enabling security,"1.Go to installed cluster
2.Open Enable Security wizard
3.Proceed to step Configure Identities
4.Go to advanced tab
some principal values on Hive panel are empty",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-03-12 17:51:20,7
12781564,500 error on installing kerberos clients,"On stage of installing kerberos clients appears error 500 with text:
{code:java}
An internal system exception occurred; Unexpected error condition executing kadmin command§
{code}
This is due to a bad search path used to find kadmin. It should be able to be set by a user.",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-12 17:35:52,7
12781524,Test Kerberos Client (KERBEROS_SERVICE_CHECK) is failed after Ambari only upgrade from 1.6.0 to 2.0.0 when enable securitty (Centos 5.9),"*Cluster*: http://172.18.145.129:8080
STR:
1)Deploy old version with all services
2)Make ambari only upgrade to 2.0.0 .
3)Enable security

Expected result:
Enable security is passed.

Actual result
KERBEROS_SERVICE_CHECK is failed after Ambari only upgrade from 1.6.0 to 2.0.0 when enable securitty
{code}
--------------------------------------------------------------------------------
{
  ""href"" : ""http://172.18.145.129:8080/api/v1/clusters/cl1/requests/8/tasks/258"",
  ""Tasks"" : {
    ""attempt_cnt"" : 1,
    ""cluster_name"" : ""cl1"",
    ""command"" : ""SERVICE_CHECK"",
    ""command_detail"" : ""SERVICE_CHECK KERBEROS"",
    ""end_time"" : 1426091879341,
    ""error_log"" : ""/var/lib/ambari-agent/data/errors-258.txt"",
    ""exit_code"" : 1,
    ""host_name"" : ""amb-upg160-rhel5oracle1426082627-9.cs1cloud.internal"",
    ""id"" : 258,
    ""output_log"" : ""/var/lib/ambari-agent/data/output-258.txt"",
    ""request_id"" : 8,
    ""role"" : ""KERBEROS_SERVICE_CHECK"",
    ""stage_id"" : 4,
    ""start_time"" : 1426091872413,
    ""status"" : ""FAILED"",
    ""stderr"" : ""Traceback (most recent call last):\n  File \""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py\"", line 61, in <module>\n    KerberosServiceCheck().execute()\n  File \""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py\"", line 214, in execute\n    method(env)\n  File \""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py\"", line 55, in service_check\n    os.remove(ccache_file_path)\nOSError: [Errno 2] No such file or directory: '/var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_49ae9e4ea129d8216306a4a6e431c812'"",
    ""stdout"" : ""Performing kinit using ambari-qa_vvwbohue@EXAMPLE.COM\n2015-03-11 16:37:59,095 - u\""Execute['/usr/kerberos/bin/kinit -c /var/lib/ambari-agent/data/tmp/kerberos_service_check_cc_49ae9e4ea129d8216306a4a6e431c812 -kt /etc/security/keytabs/kerberos.service_check.vvwbohue.keytab ambari-qa_vvwbohue@EXAMPLE.COM']\"" {}"",
    ""structured_out"" : { }
  }
}
--------------------------------------------------------------------------------

{code}
",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-12 15:30:14,7
12781015,Kerberos: Password generator needs to generate passwords based on rules to satisfy password policy,"The password generator used to generate passwords for identities needs to generate passwords based on a rule set rather than just a random sequence of characters. 

In a KDC (MIT or Active Directory), there may be a policy in place requiring a certain characteristics for the password. By creating a password consisting if 18 characters pulled randomly from {{abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890?.!$%^*()-_+=~}}, there is no guarantee that any specific policy will be met. 

The following rules should be settable:
* Length
* Minimum number of lowercase letters (a-z)
* Minimum number of uppercase letters (A-Z)
* Minimum number of digits (0-9)
* Minimum number of punctuation characters ({{?.!$%^*()-_+=~}})",kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-10 22:39:09,7
12780985,Kerberos: Run ambari-server using non-root causes issues with AD velocity engine,"setup ambari-server to run with non-root daemon (ambari-server setup, select a non-root daemon account) and the following exception will be thrown when creating identities in an Active Directory when enabling Kerberos.

{code}
java.lang.RuntimeException: Velocity could not be initialized!
	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:307)
	at org.apache.velocity.runtime.RuntimeInstance.parse(RuntimeInstance.java:1196)
	at org.apache.velocity.runtime.RuntimeInstance.parse(RuntimeInstance.java:1181)
	at org.apache.velocity.runtime.RuntimeInstance.evaluate(RuntimeInstance.java:1297)
	at org.apache.velocity.runtime.RuntimeInstance.evaluate(RuntimeInstance.java:1265)
	at org.apache.velocity.app.Velocity.evaluate(Velocity.java:180)
	at org.apache.ambari.server.view.ViewContextImpl.parameterize(ViewContextImpl.java:381)
	at org.apache.ambari.server.view.ViewContextImpl.getProperties(ViewContextImpl.java:194)
	at org.apache.ambari.view.filebrowser.HdfsService.getApi(HdfsService.java:67)
	at org.apache.ambari.view.filebrowser.FileOperationService.listdir(FileOperationService.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:134)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:708)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1329)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
	at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:166)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
	at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.ambari.server.controller.FailsafeHandlerList.handle(FailsafeHandlerList.java:132)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:363)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:920)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:982)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.velocity.exception.VelocityException: Error initializing log: Failed to initialize an instance of org.apache.velocity.runtime.log.Log4JLogChute with the current runtime configuration.
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:875)
	at org.apache.velocity.runtime.RuntimeInstance.init(RuntimeInstance.java:262)
	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:302)
	... 93 more
Caused by: org.apache.velocity.exception.VelocityException: Failed to initialize an instance of org.apache.velocity.runtime.log.Log4JLogChute with the current runtime configuration.
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:220)
	at org.apache.velocity.runtime.log.LogManager.updateLog(LogManager.java:269)
	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:871)
	... 95 more
Caused by: java.lang.RuntimeException: Error configuring Log4JLogChute : 
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.velocity.util.ExceptionUtils.createWithCause(ExceptionUtils.java:67)
	at org.apache.velocity.util.ExceptionUtils.createRuntimeException(ExceptionUtils.java:45)
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:133)
	at org.apache.velocity.runtime.log.Log4JLogChute.init(Log4JLogChute.java:85)
	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:157)
	... 97 more
Caused by: java.io.FileNotFoundException: velocity.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:142)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
	at org.apache.log4j.FileAppender.<init>(FileAppender.java:110)
	at org.apache.log4j.RollingFileAppender.<init>(RollingFileAppender.java:79)
	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:118)
	... 99 more
{code}",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-10 20:45:17,7
12780561,Maintenance Mode Hosts do not get keytabs,"A downed host behaves a little differently then a host that is up and in a maintenance state.  

It appears that where a downed host is not included in steps to install the Kerberos client nor is it included when choosing a host to run the service check on. 

However a host that is up and in maintenance mode is not included in steps to install the Kerberos client, but may be selected as the host to run the service check on. Resulting in this error:

{code}
2015-03-04 20:58:03,164 - Error while executing command 'service_check':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 208, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/service_check.py"", line 58, in service_check
    raise Fail(err_msg)
Fail: Failed to execute kinit test due to principal or keytab not found or available
{code}

The error is correct in that the keytab has not been distributed to it, this is because the Kerberos Client has not been installed on it.  
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-09 18:17:10,7
12780474,"Oozie connection refused: unable to initialize service, Cannot get password from user.","Oozie connection refused: unable to initialize service, Cannot get password from user.

{noformat}
org.apache.oozie.service.ServiceException: E0100: Could not initialize service [org.apache.oozie.service.HadoopAccessorService], Login failure for oozie/ip-172-31-34-48.ec2.internal@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:187)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:130)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:101)
	at org.apache.oozie.service.Services.setServiceInternal(Services.java:383)
	at org.apache.oozie.service.Services.setService(Services.java:369)
	at org.apache.oozie.service.Services.loadServices(Services.java:302)
	at org.apache.oozie.service.Services.init(Services.java:210)
	at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:45)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4210)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4709)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:799)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:583)
	at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:675)
	at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:601)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:502)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1317)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:324)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1065)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:822)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1057)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)
	at org.apache.catalina.core.StandardService.start(StandardService.java:525)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:754)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:595)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)
Caused by: java.io.IOException: Login failure for oozie/ip-172-31-34-48.ec2.internal@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user

	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)
	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:179)
	... 31 more
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:926)
	... 32 more
{noformat}

This issue is really related to unexpected data in the oozie-site.xml file. It is expected that the principals names contain {{_HOST}} rather than the resolved hostname. This appears to be a viable solution for Oozie version 4.1.0.2.2.2.0, but not for version 4.0.0.2.0.14.0.

",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-03-09 10:56:04,7
12780444,Ambari storm logviewer in secure mode doesn't work,"Storm logviewer uses the same UI.filter config thats being used for Storm UI.
In secure mode storm UI uses SPENGO to authenticate user to access the UI.
Similarly logviewer also does the same .
But in Ambari 1.7 we advise user to create HTTP/storm-ui@REALM and this gets added to storm.yaml.
As this is bound to a host storm logviewers which are running one per supervisor won't be able to use this key .

Solution:
There is a configuration problem in the {{/etc/storm/conf/storm.yaml}} file.  In particular the issue is here:
{code:title=/etc/storm/conf/storm.yaml:109}
ui.filter.params:
  ""type"": ""kerberos""
  ""kerberos.principal"": ""HTTP/host-2.internal@EXAMPLE.COM""
  ""kerberos.keytab"": ""/etc/security/keytabs/spnego.service.keytab""
  ""kerberos.name.rules"": ""DEFAULT""
{code}

The {{kerberos.principal}} value should be the SPNEGO principal for the localhost, not the host where the UI server is running.  In this example, the localhost is *host-4.internal*  so the {{kerberos.principal}} value should be *HTTP/host-4.internal@EXAMPLE.COM* not *HTTP/host-2.internal@EXAMPLE.COM*.  The Storm UI server is running on *host-2.internal*

The fix for this should be in the code around 

{code:title=common-services/STORM/0.9.1.2.1/package/scripts/params.py:103} 
    _storm_ui_jaas_principal_name = config['configurations']['storm-env']['storm_ui_principal_name']
    storm_ui_host = default(""/clusterHostInfo/storm_ui_server_hosts"", [])
    storm_ui_jaas_principal = _storm_ui_jaas_principal_name.replace('_HOST',storm_ui_host[0].lower())
{code}

{{storm_ui_jaas_principal}} is then used in the template to build the storm.yaml file.  
",kerberos,[],AMBARI,Bug,Major,2015-03-09 09:48:54,7
12779592,Ambari must support deployment on separate host,"It should be possible to deploy Ambari on a host that does not include any other services.

The primary issue is that Ambari needs to be able to distribute keytabs to other hosts even if Ambari-Server is not running on a host with services that would otherwise have forced the Ambari-Server to be included in the keytab distribution process.

To be clear, the following use case should be supported:

* The Ambari-Server is deployed on a host with no other services 
** Other services are deployed on hosts separate from Ambari-Server
* addHost should be possible
",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-04 21:26:07,7
12779275,Kerberos: Add Host did not generate keytabs,"1) using build 440
2) three node cluster, hdfs, yarn, mr, tez, hive, zk, pig, ams
3) setup nnha, rmha
4) enabled kerb
5) all is good
6) added second hive metastore
7) added second hiveserver2
8) all is good
9) added host with DN and clients
10) keytabs are not created on the new host. i was not prompted for kdc creds. basically, i did 1-9 all in one shot, never logging out.

As a workaround #1:
- Attempted to regen keytabs, with ""missing only"" checkbox checked. it looks like it remade all principals and keytabs for the cluster but didn't distribute the keytabs. That is concerning that this might be an additional issue for another JIRA maybe. Anycase: didn't result in getting keytabs on my new host.

As a workaround #2:
- Attempted regen keytabs all. Made all princs and keytabs and distributed for cluster hosts except my new host. So no lock here either.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-03-04 02:37:36,7
12779272,Fix minor issues with test_security_status test cases,Fix minor issues with test_security_status test cases,kerberos,['ambari-server'],AMBARI,Bug,Major,2015-03-04 02:16:07,7
12779166,"Storm principal is marked as a service rather than a user principal, causes issue adding hosts with Supervisor",Storm principal is marked as a _service_ rather than a _user_ principal. This will cause issues when adding hosts with Supervisor on it due to Ambari's lack of caching of service principals.,kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-03-03 20:03:01,7
12779129,core-site.xml having wrong value for hadoop.proxyuser.HTTP.groups,"core-site.xml having wrong value for hadoop.proxyuser.HTTP.groups

{code}
<property>
<name>hadoop.proxyuser.HTTP.groups</name>
<value>${core-site/proxyuser_group}</value>
</property>
{code}

It should be 

{code}
<property>
<name>hadoop.proxyuser.HTTP.groups</name>
<value>users</value>
</property>
{code}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-03-03 17:55:56,7
12779076,security_status test cases sporadically fail due to import of status_params module,"security_status test cases sporadically fail due to import of status_params module. For example {{test_security_status}} in {{stacks/2.0.6/HDFS/test_hdfs_client.py}} imports {{status_params.py}} which is not in the test directory path. 

Instead of importing  {{status_params.py}}, use values from  {{self.config_dict}}

Example:

{code:title=Before}
    cached_kinit_executor_mock.called_with(status_params.kinit_path_local,
                                           status_params.hdfs_user,
                                           status_params.hdfs_user_keytab,
                                           status_params.hdfs_user_principal,
                                           status_params.hostname,
                                           status_params.tmp_dir)
{code}

{code:title=After}
    cached_kinit_executor_mock.called_with(
      '/usr/bin/kinit',
      self.config_dict['configurations']['hadoop-env']['hdfs_user'],
      self.config_dict['configurations']['hadoop-env']['hdfs_user_keytab'],
      self.config_dict['configurations']['hadoop-env']['hdfs_user_principal_name'],
      self.config_dict['hostname'],
      '/tmp'
    )
{code}",kerberos,['ambari-agent'],AMBARI,Bug,Major,2015-03-03 14:00:18,7
12778522,Kerberos: Kerberos Service Check needs to generate and destroy it's own unique identity for testing,"The Kerberos _service check_ needs to generate it's own unique identity to use for testing and then destroy it when complete.  This will ensure that any _known_ identities (such as the smokeuser, usually ambari-qa) does not accidentally get removed if shared between clusters or if the service check is run after Kerberos is enabled. 

The service check must perform the following steps:

# Create a unique principal in the relevant KDC (server)
# Test that the principal can be used to authenticate via kinit (agent)
# Destroy the principal (server)
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-02-28 22:07:38,7
12777738,Agent is spamming logs with exceptions ,"{code}
INFO 2015-01-19 14:00:32,786 PythonExecutor.py:114 - Result: {'structuredOut': {u'securityIssuesFound': u'Configuration file hbase-site did not pass the vali
dation. Reason: Property hbase.regionserver.keytab.file does not exist', u'processes': [], u'securityState': u'UNSECURED'}, 'stdout': '2015-01-19 10:10:12,89
4 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/scr
ipt/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 
90, in status\n    check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/ch
eck_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:10:50,579 - Error while exe
cuting command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", lin
e 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    
check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.
py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:12:46,728 - Error while executing command \'st
atus\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n
    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    check_process_statu
s(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in ch
eck_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:12:47,149 - Error while executing command \'status\':\nTraceback 
(most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  
File ""/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py"", line 86, in status\n    raise ComponentIsNotRunning()\nC
omponentIsNotRunning\n2015-01-19 10:13:46,533 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/sit
e-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/
0.96.0.2.0/package/scripts/hbase_master.py"", line 73, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_manage
ment/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:
13:47,003 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libra
ries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py
"", line 90, in status\n    check_process_status(status_params.datanode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/func
tions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:13:47,438 - Error w
hile executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.
py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_regionserver.py"", line
 76, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py""
, line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:13:47,872 - Error while executing command \'statu
s\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n   
 method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/FLUME/1.4.0.2.0/package/scripts/flume_handler.py"", line 86, in status\n    raise ComponentI
sNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:39,410 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/l
ib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common
-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"", line 110, in status\n    check_process_status(status_params.namenode_pid_file)\n  File ""/usr/lib/pytho
n2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nCom
ponentIsNotRunning\n2015-01-19 10:19:40,004 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-
packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.
96.0.2.0/package/scripts/hbase_master.py"", line 73, in status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_manageme
nt/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19
:40,420 - Error while executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/librari
es/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/namenode.py"",
 line 110, in status\n    check_process_status(status_params.namenode_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/funct
ions/check_process_status.py"", line 41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:40,852 - Error wh
ile executing command \'status\':\nTraceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.p
y"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py"", line 73, in
 status\n    check_process_status(pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 
41, in check_process_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:41,278 - Error while executing command \'status\':\nT
raceback (most recent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method
(env)\n  File ""/var/lib/ambari-agent/cache/common-services/KAFKA/0.8.1.2.2/package/scripts/kafka_broker.py"", line 70, in status\n    check_process_status(sta
tus_params.kafka_pid_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_proc
ess_status\n    raise ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:41,676 - Error while executing command \'status\':\nTraceback (most re
cent call last):\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/v
ar/lib/ambari-agent/cache/common-services/HDFS/2.1.0.2.0/package/scripts/datanode.py"", line 90, in status\n    check_process_status(status_params.datanode_pi
d_file)\n  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/check_process_status.py"", line 41, in check_process_status\n    rai
se ComponentIsNotRunning()\nComponentIsNotRunning\n2015-01-19 10:19:42,112 - Error while executing command \'status\':\nTraceback (most recent call last):\n 
 File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute\n    method(env)\n  File ""/var/lib/ambari-agent
{code}",kerberos,"['ambari-agent', 'ambari-server']",AMBARI,Bug,Critical,2015-02-26 01:43:23,7
12777389,Local user mapping for hdfs headless principal not set in Kerberos descriptor,The local user mapping for the hdfs headless principal not set in Kerberos descriptor.  It should be set to {{hadoop-env/hdfs_user}},kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Critical,2015-02-25 01:46:18,7
12777381,"Root user has spnego (HTTP) kerberos ticket set after Kerberos is enabled, root should have no ticket.","After enabling Kerberos, the root user has the spnego user set for it 

{code}
[root@c6501 ~]# klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: HTTP/c6501.ambari.apache.org@EXAMPLE.COM

Valid starting     Expires            Service principal
02/18/15 22:14:51  02/19/15 22:14:51  krbtgt/EXAMPLE.COM@EXAMPLE.COM
	renew until 02/18/15 22:14:51
{code}

It appears that the issue is related to the agent-side scheduler and/or some job that is scheduled to run periodically. Apparently some job is kinit-ing with the SPNEGO identity as the running user (root in this case) without changing the ticket cache. Thus whenever the job runs the root user's ticket cache gets changed to contain the SPNEGO identity's ticket.
",kerberos keytabs,['ambari-agent'],AMBARI,Bug,Blocker,2015-02-25 01:17:16,7
12777236,Oozie failed to start in secured cluster for stacks 2.0 and 2.1,"On 2.0 and 2.1 stack oozie server failed with following error:
{noformat}
2015-02-23 16:23:54,474  WARN NativeCodeLoader:62 - SERVER[] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2015-02-23 16:23:54,821 FATAL Services:533 - SERVER[] USER[-] GROUP[-] E0100: Could not initialize service [org.apache.oozie.service.HadoopAccessorService], Login failure for oozie/_HOST@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab
org.apache.oozie.service.ServiceException: E0100: Could not initialize service [org.apache.oozie.service.HadoopAccessorService], Login failure for oozie/_HOST@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab
	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:182)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:127)
	at org.apache.oozie.service.HadoopAccessorService.init(HadoopAccessorService.java:98)
	at org.apache.oozie.service.Services.setServiceInternal(Services.java:372)
	at org.apache.oozie.service.Services.setService(Services.java:358)
	at org.apache.oozie.service.Services.loadServices(Services.java:291)
	at org.apache.oozie.service.Services.init(Services.java:212)
	at org.apache.oozie.servlet.ServicesLoader.contextInitialized(ServicesLoader.java:39)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4206)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4705)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:799)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:601)
	at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:675)
	at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:601)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:502)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1317)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:324)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1065)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:840)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1057)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)
	at org.apache.catalina.core.StandardService.start(StandardService.java:525)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:754)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:595)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)
Caused by: java.io.IOException: Login failure for oozie/_HOST@EXAMPLE.COM from keytab /etc/security/keytabs/oozie.service.keytab
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:870)
	at org.apache.oozie.service.HadoopAccessorService.kerberosInit(HadoopAccessorService.java:174)
	... 31 more
Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user

	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:762)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:690)
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:687)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:595)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:861)
	... 32 more
2015-02-23 16:23:54,833  INFO Services:539 - SERVER[] Shutdown
{noformat}",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-02-24 18:14:23,7
12776851,Kerberos: check kerb task should delete smoke user principal ,"After smoke test user is created during the Check Kerberos task at the beginning of the wizard, delete the principal because later in the wizard the user might have changed the principal name, making this principal unused. ",kerberos,['ambari-server'],AMBARI,Task,Critical,2015-02-23 14:59:26,7
12776758,Storm service check failed after enabling security with existing AD,"On last stage, storm service check failed 
{code}
1017 [main] INFO  backtype.storm.StormSubmitter - Generated ZooKeeper secret payload for MD5-digest: -5540876373091122649:-7113320937502691642
1021 [main] INFO  backtype.storm.security.auth.AuthUtils - Got AutoCreds []
1039 [main] WARN  org.apache.storm.curator.retry.ExponentialBackoffRetry - maxRetries too large (60000). Pinning to 29
1043 [main] INFO  backtype.storm.utils.StormBoundedExponentialBackoffRetry - The baseSleepTimeMs [2000] the maxSleepTimeMs [5] the maxRetries [60000]
1043 [main] WARN  backtype.storm.utils.StormBoundedExponentialBackoffRetry - Misconfiguration: the baseSleepTimeMs [2000] can't be greater than the maxSleepTimeMs [5].
1847 [main] INFO  org.apache.storm.zookeeper.Login - successfully logged in.
Exception in thread ""main"" java.lang.RuntimeException: javax.security.sasl.SaslException: Failure to initialize security context [Caused by GSSException: Invalid name provided (Mechanism level: Illegal character in realm name; one of: '/', ':', '' (600))]
	at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:99)
	at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:66)
	at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:52)
	at backtype.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:36)
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:211)
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:157)
	at storm.starter.WordCountTopology.main(WordCountTopology.java:77)
Caused by: javax.security.sasl.SaslException: Failure to initialize security context [Caused by GSSException: Invalid name provided (Mechanism level: Illegal character in realm name; one of: '/', ':', '' (600))]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.<init>(GssKrb5Client.java:150)
	at com.sun.security.sasl.gsskerb.FactoryImpl.createSaslClient(FactoryImpl.java:63)
	at javax.security.sasl.Sasl.createSaslClient(Sasl.java:372)
	at org.apache.thrift7.transport.TSaslClientTransport.<init>(TSaslClientTransport.java:72)
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:127)
	at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48)
	at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:97)
	... 6 more
Caused by: GSSException: Invalid name provided (Mechanism level: Illegal character in realm name; one of: '/', ':', '' (600))
	at sun.security.jgss.krb5.Krb5NameElement.getInstance(Krb5NameElement.java:127)
	at sun.security.jgss.krb5.Krb5MechFactory.getNameElement(Krb5MechFactory.java:95)
	at sun.security.jgss.GSSManagerImpl.getNameElement(GSSManagerImpl.java:202)
	at sun.security.jgss.GSSNameImpl.getElement(GSSNameImpl.java:472)
	at sun.security.jgss.GSSNameImpl.init(GSSNameImpl.java:201)
	at sun.security.jgss.GSSNameImpl.<init>(GSSNameImpl.java:170)
	at sun.security.jgss.GSSManagerImpl.createName(GSSManagerImpl.java:137)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.<init>(GssKrb5Client.java:108)
	... 12 more
{code}
",kerberos,['ambari-server'],AMBARI,Bug,Blocker,2015-02-23 02:52:22,7
12776742,Kerberos: fails when entering admin principal with blank password ,"Note: I don't believe the below is specific to add host, but related to the prompting and how the set admin cred works in case of a blank password. I hit this during testing of add host though.

- install cluster, kerberize
- add host (be sure to use a new browser so you know it will prompt for kerb admin credentials)
- got to the review part of add host, click deploy
- prompted for admin creds (as expected)
- tried messing around by putting in bad creds and that seemed to work...
- expect when I put in the right admin cred principal name (admin/admin) but a blank password. I was surprised it allowed me to click save (because the password field was blank)
- so I click save, dialog disappears and I am cannot get it to re-prompt.
- this is what it PUT and the response was blank...
 
{code}
[{""session_attributes"":{""kerberos_admin"":{""principal"":""admin/admin"",""password"":""""}}}]:
Response Headersview source
{code}

in ambari-server.log, nothing

{code}
17:58:05,860  INFO [qtp1257282095-603] AmbariManagementControllerImpl:1171 - Received a updateCluster request, clusterId=2, clusterName=MyCluster, securityType=null, request={ clusterName=MyCluster, clusterId=2, provisioningState=null, securityType=null, stackVersion=HDP-2.2, desired_scv=null, hosts=[] }
{code}

- back in wizard doesn't solve it. had to completely exit wizard and ambari web to start again

The overall issue is how the credentials are being validated.  If no password is being set, the command to test the credentials when using a MIT KDC generates the following command:
{code}
kadmin -p admin/admin -w """" -r EXAMPLE.COM -q 'get_principal admin/admin'
{code}

The empty password ({{-w """"}}) in the command creates an interactive session where the command is waiting for data on STDIN, thus hanging the process.

An empty password should not cause the same behavior when using Active Directory.",kerberos,['ambari-server'],AMBARI,Bug,Critical,2015-02-22 22:50:38,7
12776671,Kerberos: regenerate keytabs not handled for all hosts,"1. Installed cluster on three hosts c6401, c6402, c6403
2. using oracle jdk 1.7, put JCE in place on all hosts
3. ambari-agent stop on c6403 (which just has DN, ZK and NM)
4. Enable kerberos, which means c6403 does not get keytabs
5. ambari-agent start on c6403
6. go to regen keytabs. Clicked to only do missing. c6403 does not get keytabs.
7. go to regen keytabs. just left the default which should do all. No hosts get the keytabs.

What I found is since the Kerberos client didn't get installed on c6403, the ""Set keytab kerberos client"" command is ""Host Role in invalid state"". I went to that host, and did install clients from the UI to get the kerberos client installed. Once that happened, I could then regen keytabs.

The main issue: Regen only works if all hosts can regen. Once c6403 did not have a client, and Host Role in invalid state, it didn't do keytabs for any other hosts.",kerberos keytabs,['ambari-server'],AMBARI,Bug,Critical,2015-02-21 23:23:40,7
12776155,Zookeeper start failed after upgrading secured cluster,"{code}
2015-02-13 11:32:12,437 - Error while executing command 'start':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_server.py"", line 78, in start
    zookeeper_service(action = 'start')
  File ""/var/lib/ambari-agent/cache/common-services/ZOOKEEPER/3.4.5.2.0/package/scripts/zookeeper_service.py"", line 38, in zookeeper_service
    kinit_cmd = format(""{kinit_path_local} -kt {smoke_user_keytab} {smokeuser_principal};"")
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/format.py"", line 83, in format
    return ConfigurationFormatter().format(format_string, args, **result)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/functions/format.py"", line 47, in format
    result_protected = self.vformat(format_string, args, all_params)
  File ""/usr/lib64/python2.6/string.py"", line 549, in vformat
    result = self._vformat(format_string, args, kwargs, used_args, 2)
  File ""/usr/lib64/python2.6/string.py"", line 582, in _vformat
    result.append(self.format_field(obj, format_spec))
  File ""/usr/lib64/python2.6/string.py"", line 599, in format_field
    return format(value, format_spec)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/config_dictionary.py"", line 79, in __getattr__
    raise Fail(""Configuration parameter '""+self.name+""' was not found in configurations dictionary!"")
Fail: Configuration parameter 'smokeuser_principal_name' was not found in configurations dictionary!
{code}",kerberos upgrade,['ambari-server'],AMBARI,Bug,Blocker,2015-02-19 14:01:37,7
12775424,Kerberos: Adding a service to a Kerberized cluster requires Kerberos-related tasks occur before INSTALL stage,"When adding a service to a _Kerberized_ cluster, the {{INSTALL}} stage for that service tends to fail due to missing Kerberos properties, principals and keytabs. 

To solve this issue, when a new service is added to a _Kerberized_ cluster the following needs to occur before that service can attempt to transition into it's {{INSTALLED}} state:
# Kerberos-specific properties need to be created or set
# Principals need to be created
# Keytab files need to be generated
# Keytab files need to be distributed 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Blocker,2015-02-16 17:47:31,7
12775409,Security enabling fails after ambari only upgrade,"Enabling security hangs after ambari upgrade on centos (1.4.4 -> 2.0.0)

This appears to be an issue with how the Kerberos service is attached to stacks.",kerberos,['stacks'],AMBARI,Bug,Blocker,2015-02-16 16:58:13,7
12775130,Required Properties in Configure Kerberos Step,"After the cluster is deployed, tried to enable security. In the Configure kerberos step - Advanced krb5.conf tab, of the wizard there appears 2 new required properties as shown in the attachment. Please let us know the value of these properties if it has to be set by the user or if this should be present by default. This blocks security enabling",kerberos,['ambari-web'],AMBARI,Bug,Blocker,2015-02-14 12:38:04,7
12775022,Kerberos: Escape special characters in Distinguished Names used for queries in Active Directory,"Escape special characters in Distinguished Names used for queries in Active Directory, else query (or attribute updates) will fail with an error like

{code}
[LDAP: error code 1 - 000020D6: SvcErr: DSID-0310081B, problem 5012 (DIR_ERROR), data 0 ^@]
{code}

The following characters should be escaped using a {{\}}:
* Forward Slash {{/}}
* Comma	{{,}}
* Backslash character
* Pound sign (hash sign)	{{#}}
* Plus sign {{+}}
* Less than symbol	{{<}}
* Greater than symbol {{>}}
* Semicolon {{;}}
* Double quote (quotation mark) {{""}}
* Equal sign {{=}}
* Leading or trailing spaces
",kerberos,['ambari-server'],AMBARI,Task,Blocker,2015-02-13 20:42:34,7
12774667,When adding the Oozie service to a kerberized cluster OOZIE_SERVER doesn't start,"Oozie server fails to start with the error: ""Fail: Configuration parameter 'oozie.service.HadoopAccessorService.kerberos.principal' was not found in configurations dictionary!""

Steps to reproduce:
Create a non-kerberized cluster
I used the following blueprint
{code}
{   
  ""host_groups"" : [
    {
      ""name"" : ""host_group_1"",
      ""components"" : [      
        {
          ""name"" : ""NODEMANAGER""
        },
        {
          ""name"" : ""NAMENODE""
        },
        {
          ""name"" : ""HISTORYSERVER""
        },
        {
          ""name"" : ""ZOOKEEPER_SERVER""
        },
        {
          ""name"" : ""SECONDARY_NAMENODE""
        },
        {
          ""name"" : ""RESOURCEMANAGER""
        },  
        {
          ""name"" : ""APP_TIMELINE_SERVER""
        },        
        {
          ""name"" : ""DATANODE""
        },
        {
          ""name"" : ""YARN_CLIENT""
        },
        {
          ""name"" : ""ZOOKEEPER_CLIENT""
        },
        {
          ""name"" : ""MAPREDUCE2_CLIENT""
        }     
      ],
      ""cardinality"" : ""1""
    }
  ],
  ""Blueprints"" : {
    ""stack_name"" : ""HDP"",
    ""stack_version"" : ""2.2""
  }
}
{code}
- manually unzip UnlimitedJCEPolicy
- manually install MIT KDC
- Using UI, kerberize the existing cluster
- Using the UI, add the Oozie service

OOZIE_SERVER failed to start and the above noted exception was from the log that is exposed via the UI for the oozie start operation.
According to Robert Levas this property is in the kerberos descriptor it should be set.
",keberos,['security'],AMBARI,Bug,Critical,2015-02-12 19:23:57,7
12774643,Set Keytab Kerberos Client operation fails,"Was installed cluster with HDFS, MapReduce2, YARN, Tez, ZooKeeper, AMS services. After that was added Kerberos service. Create Principals operation was failed with error:
{noformat}
2015-02-09 11:17:13,376 - Error while executing command 'set_keytab':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 184, in execute
    method(env)
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_client.py"", line 59, in set_keytab
    KerberosScript.write_keytab_file()
  File ""/var/lib/ambari-agent/cache/common-services/KERBEROS/1.10.3-10/package/scripts/kerberos_common.py"", line 395, in write_keytab_file
    group=group)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/base.py"", line 148, in __init__
    self.env.run()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 151, in run
    self.run_action(resource, action)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/environment.py"", line 117, in run_action
    provider_action()
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 133, in action_create
    self.resource.group, mode=self.resource.mode, cd_access=self.resource.cd_access)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 65, in _ensure_metadata
    uid = _coerce_uid(user)
  File ""/usr/lib/python2.6/site-packages/resource_management/core/providers/system.py"", line 46, in _coerce_uid
    raise Fail(""User %s doesn't exist."" % user)
{noformat}

Alive cluster: http://ec2-52-1-1-157.compute-1.amazonaws.com:8080/",kertberos,['stacks'],AMBARI,Bug,Blocker,2015-02-12 17:51:59,7
12774314,Set kdc_type in kerberos-env rather than krb5-conf configuration,"Currently, the {{kdc_type}} value is being set in the {{krb5-conf}} configuration, it needs to be set in the {{kerberos-env}} since the data lies outside the realm of the krb5.conf file. 
  ",kerberos,"['ambari-web', 'stacks']",AMBARI,Task,Major,2015-02-11 18:56:14,7
12774305,Kerberos: provide option to not generate kerb client krb5.conf,"In some environments, the kerb client krb5.conf is already managed on the machines and ambari cannot generate or modify it.

Need to provide option to not have ambari create/overwrite krb5.conf. See slide 2 of the attached for the proposed option to give users the ability to disable ambari trying to create/write the kerb client config.",kerberos,"['ambari-server', 'stacks']",AMBARI,Task,Critical,2015-02-11 18:36:38,7
12773917,Kerberos: missing config properties after enabling Kerberos,"After enabling Kerberos, testing with Ambari 2.0.0 Build #401, found this config is missed:

{code}
core-site/hadoop.http.authentication.kerberos.keytab : $KRB_KEYTAB_PATH/spnego.service.keytab
{code}",kerberos kerberos_descriptor,['stacks'],AMBARI,Bug,Major,2015-02-10 18:39:33,7
12773019,Kerberos: Keytab regeneration not invoked when initiated via API,"Due to changes from some previous patch the response from the API call to regenerate keytabs is being clobbered. 

This was broken in patch for AMBARI-9406",kerberos keytabs,[],AMBARI,Bug,Blocker,2015-02-06 14:54:08,7
12771915,Kerberos: Do not validate host health or maintenance state when enabling Kerberos,Do not validate host health or maintenance state when enabling Kerberos,kerberos,['ambari-server'],AMBARI,Task,Major,2015-02-03 02:27:21,7
12771094,Service configurations are not updated as customized in the descriptor,"Though the Kerberos descriptor was saved with the customized value for smokeuser 
{noformat}(${cluster-env/smokeuser}/c1@${realm}){noformat} 

but the generated principal had the default value (i.e without /c1) and the corresponding configurations also had the default value 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-01-29 19:32:40,7
12770815,Implement Keytab regeneration,"Create API entry point to initiate Kerberos keytab regeneration for the cluster:
{code}
PUT /api/v1/clusters/{clustername}?regenerate_keytabs=true
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
  }
}
{code}

The entry point should invoke code to determine which principals need to be updated and then generate the following stages:
# Update Principal Passwords
# Generate Keytabs
# Distribute Keytab

This could be done in a method within {{org.apache.ambari.server.controller.KerberosHelper}} named {{regenerateKeytabs}} and flow similarly to {{org.apache.ambari.server.controller.KerberosHelper#toggleKerberos}}

A Server-side action implementation already exists for generating keytabs - see {{org.apache.ambari.server.serveraction.kerberos.CreateKeytabFilesServerAction}}.
A process is already in place to distribute keytabs - {{org/apache/ambari/server/controller/KerberosHelper.java:1192}}
A new Server-side action _may_ need to be created to update relavant principal passwords, however {{org.apache.ambari.server.serveraction.kerberos.CreatePrincipalsServerAction}} may work for this, unaltered.

",kerberos keytabs,['ambari-server'],AMBARI,Task,Major,2015-01-28 22:13:24,7
12770462,Implement unkerberize for kerberized cluster,"Implement the ability to disable Kerberos from a cluster that was previously configured for Kerberos.

This entails reverting configuration properties set when Kerberos was enabled to default values found in the stack. 

Principals will not be destroyed in the KDC and ketyab files will not be removed from hosts.",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2015-01-27 20:41:25,7
12770430,Remove toLowerCase() from userPrincipalName in default Kerberos principal create template,"Remove toLowerCase() from userPrincipalName in default Kerberos principal create template. This is creating an issue with principals that have upper-cased characters and Active Directory such that when kinit-ing, authenticating fails:

{code:title=kinit -V -k -t /etc/security/keytabs/spnego.service.keytab }
HTTP/c6501.ambari.apache.org
Using default cache: /tmp/krb5cc_0
Using principal: HTTP/c6501.ambari.apache.org@HDP01.LOCAL
Using keytab: /etc/security/keytabs/spnego.service.keytab
kinit: Preauthentication failed while getting initial credentials
{code}

An example of the offending template is as follows:
{code:title=from kerberos-env.xml}
{
  ""objectClass"": [""top"", ""person"", ""organizationalPerson"", ""user""],
  ""cn"": ""$principal_name"",
  #if( $is_service )
  ""servicePrincipalName"": ""$principal_name"",
  #end
  ""userPrincipalName"": ""$normalized_principal.toLowerCase()"",
  ""unicodePwd"": ""$password"",
  ""accountExpires"": ""0"",
  ""userAccountControl"": ""66048""
}
{code}",active_directory kerberos,"['ambari-server', 'stacks']",AMBARI,Task,Major,2015-01-27 18:56:17,7
12770386,cluster-env/security_enabled not set to true when Kerberos is enabled in cluster,"After enabling Kerberos, {{cluster-env/security_enabled}} is {{false}} when it should be {{true}}. This is causing component to not start up with Kerberos enabled and thus failing.
",keberos,['ambari-server'],AMBARI,Bug,Blocker,2015-01-27 15:59:26,7
12769963,"Kerberos: when unable to connect to KDC admin, need to inform user","This appears to be happening when the KDC and/or KDC Admin hosts do not point to a valid KDC, and when the relevant KDC does not _handle_ the indicated realm.

For the *host* issue, (in the cases that I tested), a {{org.apache.ambari.server.serveraction.kerberos.KerberosKDCConnectionException}} is being thrown but not caught to re-throw a {{java.lang.IllegalArgumentException}}, which would generate the expected 400 error.

For the *realm* issue, the following error string is not being captured by the logic to produce a {{org.apache.ambari.server.serveraction.kerberos.KerberosRealmException}}. 
{code}
kadmin: Cannot find KDC for requested realm while initializing kadmin interface
{code}
In any case, {{org.apache.ambari.server.serveraction.kerberos.KerberosRealmException}} is not being caught to re-throw a {{java.lang.IllegalArgumentException}}, which would generate the expected 400 error.
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-25 20:17:56,7
12769955,Kerberos: host/<hostname>@REALM principals are created (should not be created),"While generating principals, host/<hostname>@REALM principals are created.  These should not be created.

And they are ending-up in the resulting keytab. For example:

{code}
[root@c6402 keytabs]# klist -kt nn.service.keytab 
Keytab name: FILE:nn.service.keytab
KVNO Timestamp         Principal
---- ----------------- --------------------------------------------------------
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 nn/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
   1 01/24/15 18:07:51 host/c6402.ambari.apache.org@EXAMPLE.COM
{code}",kerberos keytabs,['ambari-server'],AMBARI,Task,Major,2015-01-25 18:19:57,7
12769795,Kerberos: Need stdout to show info on Kerberos-related tasks,"There's no stdout to indicate progress / success for the tasks for principal generation / keytab generation, etc.

Also, even just to know what command is actually being run to create the principal, that will help folks with debugging. Or is that only shown in case of a failure. Just want to make sure in the case of a problem, there is output there to help folks.
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-24 01:25:30,7
12769705,Automate Kerberos support for AMS,"We need to implement automatic keytab / principal for AMS in case of HDFS backed HDFS.
",kerberos kerberos_descriptor security,['stacks'],AMBARI,Task,Major,2015-01-23 18:22:12,7
12769623,Remove toLowerCase() from userPrincipalName in default Kerberos principal create template,"Remove toLowerCase() from userPrincipalName in default Kerberos principal create template. This is creating an issue with principals that have upper-cased characters and Active Directory such that when kinit-ing, authenticating fails:

{code:title=kinit -V -k -t /etc/security/keytabs/spnego.service.keytab }
HTTP/c6501.ambari.apache.org
Using default cache: /tmp/krb5cc_0
Using principal: HTTP/c6501.ambari.apache.org@HDP01.LOCAL
Using keytab: /etc/security/keytabs/spnego.service.keytab
kinit: Preauthentication failed while getting initial credentials
{code}

An example of the offending template is as follows:
{code:title=from kerberos-env.xml}
{
  ""objectClass"": [""top"", ""person"", ""organizationalPerson"", ""user""],
  ""cn"": ""$principal_name"",
  #if( $is_service )
  ""servicePrincipalName"": ""$principal_name"",
  #end
  ""userPrincipalName"": ""$normalized_principal.toLowerCase()"",
  ""unicodePwd"": ""$password"",
  ""accountExpires"": ""0"",
  ""userAccountControl"": ""66048""
}
{code}",active-directory active_directory kerberos,"['ambari-server', 'stacks']",AMBARI,Bug,Major,2015-01-23 12:06:50,7
12769417,MapReduce2 Service Check fails after enabling Kerberos with permission issue in local filesystem,"After enabling Kerberos MapReduce2 Service Check failed with issue writing to local file system:

{code}
Init:
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-x--- 4 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache

Kerb:
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 01:23 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-s--- 4 ambari-qa hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache

Can't create directory /hadoop/yarn/local/usercache/ambari-qa/appcache/application_1421889721625_0001 - Permission denied
main : user is ambari-qa
main : requested yarn user is ambari-qa
{code}

The service check does not fail when run before enabling Kerberos.


{code:title=Filesystem BEFORE enabling Kerberos}
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-x--- 4 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache
{code}

{code:title=Filesystem AFTER enabling Kerberos}
drwxrwxr-x 5 yarn hadoop 4096 Jan 22 01:23 /hadoop/yarn/local
drwxr-xr-x 3 yarn hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache
drwxr-s--- 4 ambari-qa hadoop 4096 Jan 22 00:00 /hadoop/yarn/local/usercache/ambari-qa
drwx--x--- 2 yarn hadoop 4096 Jan 22 00:01 /hadoop/yarn/local/usercache/ambari-qa/appcache
{code}

It appears that the user executing the task is {{ambari-qa}} after enabling Kerberos, there is no indication what user is executing the task before enabling Kerberos.",kerberos mapreduce,['ambari-agent'],AMBARI,Bug,Blocker,2015-01-22 19:55:48,7
12769280,Ensure enable/disable Kerberos logic should invoke only when state of security flag is changed,"The logic to enable or disable Kerberos is typically invoked when the Cluster resource is updated. This occurs for several reasons, not all of them indicate the state of Kerberos should be altered.  

By processing all updated to the Cluster resource, the enable/disable Kerberos may get invoked when not necessary causing _noise_ on the task list and potentially generating an error condition if the KDC administrator credentials are not available.  Certain states of the system will trigger the enable/disable Kerberos logic to perform tasks requiring the KDC administrator credentials. If not explicitly handing the security state change, this behavior is not desired. 

To solve the issue, test the request on the update Cluster resource to see if the security state property (cluster-env/security_enabled) has been altered, if so invoke enable/disable Kerberos logic; else do not invoke enable/disable Kerberos logic. 
",kerberos security,['ambari-server'],AMBARI,Bug,Blocker,2015-01-22 12:04:28,7
12769053,JAAS configuration file parser leaves trailing quote in quoted values,"The JAAS configuration file parser used in determining the security state of some components (like ZooKeeper and HBase) leaves trailing quote in quoted values. 

For example:

{code}
Client {
com.sun.security.auth.module.Krb5LoginModule required
useKeyTab=true
storeKey=true
useTicketCache=false
keyTab=""/etc/security/keytabs/hbase.service.keytab""
principal=""hbase/vp-ambari-ranger-med-0120-2.cs1cloud.internal@EXAMPLE.COM"";
};
{code}

The value for {{keytab}} is being parsed as {{/etc/security/keytabs/hbase.service.keytab""}} rather than {{/etc/security/keytabs/hbase.service.keytab}}.
",kerberos security,['ambari-agent'],AMBARI,Bug,Major,2015-01-21 18:34:23,7
12768950,Add getUpdateDirectives to org.apache.ambari.server.api.resources.ResourceDefinition and use when handling PUT requests,"Add {{getUpdateDirectives}} method to {{org.apache.ambari.server.api.resources.ResourceDefinition}}:

{code}
  public Collection<String> getUpdateDirectives();
{code}

Add default implementation to {{org.apache.ambari.server.api.resources.BaseResourceDefinition}} to return an empty Set

Update {{org.apache.ambari.server.api.services.RequestFactory}} to process _update directives_ like it processes _create directives_ - see {{org.apache.ambari.server.api.services.RequestFactory#createPostRequest}}
",api,['ambari-server'],AMBARI,Task,Major,2015-01-21 11:07:55,7
12768863,Ambari Server setup to install and copy JCE policy file in-place (handle both Default / Custom JDK scenarios),"1) During ""ambari-server setup"", if user chooses JDK option 1 or 2 (where ambari automatically downloads JDK and JCE), setup should copy the JCE file to /resources and install the JCE on the ambari server.

2) Also need an ""ambari-server setup-jce"" command to put JCE zip in place on Ambari Server and install on Ambari Server in cases where user chose option 3 custom jdk during setup.

{code}
ambari-server setup-jce /path/to/downloaded/jce/policy/zip/file
{code}

1) user downloads JCE zip file
2) user runs ""ambari-server setup-jce /tmp/UnlimitedJCEPolicyJDK7.zip"" (for example)
3) setup-jce copies zip file into ambari-server /resources
4) setup-jce ambari.properties and set jce.name= to name of the zip file
5) setup-jce installs JCE on ambari-server
6) inform user to restart ambari-server
",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-21 02:58:41,7
12768646,Add the ability to append a random value to values in LDAP attributes when generating principals in Active Directory,"Add ability to append a random value to values in LDAP attributes when generating principals in Active Directory.

For example the {{cn}} and {{sAMAccountName}} attributes must be unique.  In some caes the {{cn}} is not allowed to have {{/}} characters and in all cases the {{sAMAccountName}} is not allow to have {{/}} characters. Therefore to generate values for these attributes, the _instance_ part of the principal needs to be stripped off and a random string needs to be appended. 

This can be seen where the principal is {{nn/c6501.ambari.apache.org@EXAMPLE.COM}}.  The {{cn}} would typically be {{nn/c6501.ambari.apache.org}}.  Providing for a random string would allow the {{cn}} value to be something like {{nn-ythnskdtarsjko5fsdfdsb}}. Since the {{sAMAccountName}} can be at most 20 characters, it would be {{nn-ythnskdtarsjko5fs}}.

Since the generation of the attributes and values is done using a Velocity template, this random string will need to be generated and stored in the Velocity engine context before processing the template.
",active_directory kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-20 11:17:58,7
12767990,Keytab generation should use kerberos-env/encryption_types when creating key entries,When generating keytab files the set of keys to be generated should be determined by the set of encryption types listed in {{kerberos-env/encryption_types}},kerberos keytabs,['ambari-server'],AMBARI,Bug,Blocker,2015-01-16 11:12:45,7
12767982,Principal creation for Active Directory accounts should be configurable,"The properties used to create accounts in an Active Directory, related to principal creation, should be configurable such that a user may specify the required fields and their values (with variable replacement).

This may be done using a simple structure like XML or JSON, however a template facility (like Jinja2) may be more useful since conditional paths may be built in.  The template should be stored in the {{kerberos-env}} configuration.

An example of a need for a conditional path in a template is related to _service_ accounts vs _user_ accounts.  A _service_ account (such as nn/\_HOST@REALM) should have the {{servicePrincipalName}} field set to the service's principal, where this value shouldn't be set for a _user_ account (such as hdfs@REALM).
",active_directory kerberos,['ambari-server'],AMBARI,Improvement,Major,2015-01-16 10:58:34,7
12767759,Test principal and keytab required for service check should be created as part of kerberos service check action,"Intercept call to execute SERVICE_CHECK for KERBEROS service and (if necessary) create and distribute test user keytab.

It will be hard coded that the test user information is taken from the ""smokeuser"" identity in the relevant clusters Kerberos descriptor.",kerberos,['ambari-server'],AMBARI,Improvement,Major,2015-01-15 15:43:59,7
12767580,Need to provide meaningful names for the stage context in Kerb API call response,"In the API response for Kerberizing the cluster, stage_context is set to ""Process Kerberos Operations"" for all operations.  This needs to be more specific (e.g., Generate Principals, Generate Keytabs, Distribute Keytabs, etc).  

Due to the current behavior, ""Kerberize Cluster"" page repeatedly shows ""Process Kerberos Operations"" one after another and it doesn't make much sense to the end user.",kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-14 23:14:08,7
12767439,Split up principal components and realm in Kerberos descriptor,"Split up principal components and realm in Kerberos descriptor and flow throw application separately.  The realm may be optional in the Kerberos Principal descriptor, taking the value from the default realm.
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2015-01-14 14:40:56,7
12767438,Get kdc_type from kerberos-env rather than krb5-conf configuration,Get {{kdc_type}} from {{kerberos-env}} rather than {{krb5-conf}} configuration,kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-14 14:32:04,7
12766515,Add principal type to Kerberos descriptor,"Add principal _type_ to Kerberos descriptor to declare whether is principal is a service principal or a user principal.

This is needed for Active Directory since service principals needs to be created differently than user principals. 
 ",kerberos kerberos_descriptor,"['ambari-server', 'stacks']",AMBARI,Task,Major,2015-01-09 19:50:42,7
12765927,Pass LDAP URL and Principal container DN to Active Directory operations handler,"Pass LDAP URL and Principal container DN to Active Directory operations handler.

The data will be stored in the {{kerberos-env}} configuration using the following properties:

* ldap_url - The URL to the Active Directory LDAP Interface
* container_dn - The distinguished name (DN) of the container used store service principals",active-directory kerberos ldap,['ambari-server'],AMBARI,Task,Major,2015-01-09 02:41:57,7
12765779,Capture security state from components for use in enabling or disabling Kerberos,"Update each component's security state using the security status value from the _heartbeat_ message.


",kerberos security,['ambari-server'],AMBARI,Task,Major,2015-01-08 16:05:23,7
12765539,Storm service components should indicate security state,"The Storm service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. DRPC_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/storm-jaas.conf’
** StormServer/keyTab
*** not empty
*** path exists and is readable
*** required
** StormServer/principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(drpc server master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. STORM_UI_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/storm-jaas.conf’
** StormClient/keyTab
*** not empty
*** path exists and is readable
*** required
** StormClient/principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(storm ui server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. NIMBUS
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/storm-jaas.conf’
** StormServer/keyTab
*** not empty
*** path exists and is readable
*** required
** StormServer/principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(nimbus master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477

_*Note*_: There may be additional work related to _REST gateway impersonation_",keberos storm,"['ambari-server', 'stacks']",AMBARI,Task,Major,2015-01-08 03:17:11,7
12765201,Design admin principal session expiration handling API call,"Provide the standard error code that will be returned along with the error message.

If administrative credentials are not available
{code:title=400 Bad Request}
{
  ""status"" : 400,
  ""message"" : ""java.lang.IllegalArgumentException: Missing KDC administrator credentials.\nThe KDC administrator credentials must be set in session by updating the relevant Cluster resource.This may be done by issuing a PUT to the api/v1/clusters/(cluster name) API entry point with the following payload:\n{\n  \""session_attributes\"" : {\n    \""kerberos_admin\"" : {\""principal\"" : \""(PRINCIPAL)\"", \""password\"" : \""(PASSWORD)\""}\n  }\n}""
}
{code}

If administrative credentials are not valid, for example, incorrect principal or password (or keytab)
{code:title=400 Bad Request}
{
  ""status"" : 400,
  ""message"" : ""java.lang.IllegalArgumentException: Invalid KDC administrator credentials.\nThe KDC administrator credentials must be set in session by updating the relevant Cluster resource.This may be done by issuing a PUT to the api/v1/clusters/(cluster name) API entry point with the following payload:\n{\n  \""session_attributes\"" : {\n    \""kerberos_admin\"" : {\""principal\"" : \""(PRINCIPAL)\"", \""password\"" : \""(PASSWORD)\""}\n  }\n}""
}
{code}",kdc_credentials kerberos,['ambari-server'],AMBARI,Task,Major,2015-01-06 20:34:26,7
12765120,Identity references fail to deference for service-level references in Kerberos descriptor parser,"Identity references fail to deference for service-level references in Kerberos descriptor parser.  For example:

{code}
/SERVICE/identity
{code}

Global references appear to work. Example:

{code}
/identity
{code}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-01-06 14:50:33,7
12765113,Variable replacement fails for some (complicated) values in org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor#replaceVariables,"Variable replacement fails for some (complicated) values in {{org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor#replaceVariables}}.

For example 
{code}
hive.metastore.local=false,hive.metastore.uris=thrift://${host}:9083,hive.metastore.sasl.enabled=true,hive.metastore.execute.setugi=true,hive.metastore.warehouse.dir=/apps/hive/warehouse,hive.exec.mode.local.auto=false,hive.metastore.kerberos.principal=hive/_HOST@${realm}
{code}
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2015-01-06 14:09:23,7
12764542,Use cluster property rather than cluster-env/security_enabled to enable or disable Kerberos,"Use a cluster property rather than {{cluster-env/security_enabled}} to enable or disable Kerberos.  Since {{cluster-env/security_enabled}} is used by services to determine if Kerberos is enabled or not, it should not be set before completing the process of enabling or disabling Kerberos.  To declare whether the cluster enable or disable Kerberos, a property on the cluster should be set.  The property should be called {{security_type}} and must have one of the following values:
* NONE
* KERBEROS 

By using {{cluster-env/security_enabled}}, the configuration property gets set to ""true"" before Kerberos is filly enabled.   This is causing issues with stopping services so that the updated Kerberos-related configurations can be set.

Example API call to enable Kerberos
{code:title=PUT /api/v1/clusters/c1}
{
  ""Clusters"" : {
    ""security_type"" : ""KERBEROS""
  }
}
{code}

Example API call to disable Kerberos
{code:title=PUT /api/v1/clusters/c1}
{
  ""Clusters"" : {
    ""security_type"" : ""NONE""
  }
}
{code}",kerberos,['ambari-server'],AMBARI,Task,Blocker,2015-01-02 16:49:22,7
12764272,Fix unit tests in resource_management.TestSecurityCommons.TestSecurityCommons,Fix unit tests in `resource_management.TestSecurityCommons.TestSecurityCommons`,kerbeo,['ambari-agent'],AMBARI,Bug,Major,2014-12-30 20:48:08,7
12764057,Distributed keytab files have the incorrect owner and group access controls,"Distributed keytab files have the incorrect owner and group access controls.  Keytab files have the following (generally incorrect) ACLs:

{code}
-rw-r---- 1 root root
{code}

ACLs should be applied as indicated by the Kerberos descriptor",acl kerberos keytabs,['stacks'],AMBARI,Bug,Major,2014-12-29 18:14:03,7
12763929,JobHistoryServer Fails to pass service check in Kerberized cluster,"JobHistoryServer Fails to pass service check in Kerberized cluster due to kerberos to local account mapping failure 

{code}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=jhs, access=READ_EXECUTE, inode=""/mr-history/done/2014"":mapred:hadoop:drwxrwx---
{code}

{{core-site}} {{auth_to_local}} fails to map {{jhs/_HOST}} to {{mapred}} user.
",kerberos kerberos_descriptor,"['ambari-server', 'stacks']",AMBARI,Bug,Blocker,2014-12-28 15:23:50,7
12763839,"Agent is spamming logs with tracebacks of ""Error while executing command \'security_status\""","The ambari-agent log is filling up with messages like

{code}
2014-12-24 21:14:02,209 - Error while executing command \'security_status\':
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 150, in execute
    method = self.choose_method_to_execute(command_name)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 168, 
in choose_method_to_execute
    raise Fail(""Script \'{0}\' has no method \'{1}\'"".format(sys.argv[0], command_name))
Fail: Script \'/var/lib/ambari-agent/cache/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_master.py\' has no method \'security_status\'
{code}

This should be corrected by adding a default implementation to avoid such messages.",kerberos security,"['ambari-agent', 'stacks']",AMBARI,Task,Blocker,2014-12-26 17:34:06,7
12763557,Knox service components should indicate security state,"The Knox service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h2. KNOX_GATEWAY
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: knox_conf_dir + '/krb5JAASLogin.conf'
** keyTab
*** not empty
*** path exists and is readable
*** required
** principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos,"['ambari-agent', 'stacks']",AMBARI,Improvement,Major,2014-12-23 21:17:20,7
12763539,core-site properties defined in the kerberos descriptor for knox gateway component does not take effect,"The core-site properties defined in the kerberos descriptor for knox gateway component does not take effect.

This is due to an invalid Kerberos descriptor file.",kerberos kerberos_descriptor q,"['ambari-server', 'stacks']",AMBARI,Bug,Major,2014-12-23 20:04:41,7
12763325,Security state check must use a temporary cache that is to be destroyed when test is complete,"When performing a security state check, using {{kinit}}, the {{kinit}} call should specify a _temporary_ cache file which is to be destroyed after the test is complete.  

This can be accomplished using the {{kinit}} {{-c <cache file>}} option.
{code}
kinit -c <temporary file>...
{code}

The cache should be destroyed using {{kdestroy}} with its {{-c <cache file>}} option.
{code}
kdestroy -c <temporary file>
{code}
",kerberos security,['ambari-agent'],AMBARI,Task,Major,2014-12-22 22:22:54,7
12763238,Update Service Configurations Kerberos task fails when there is no work to do,The _Update Service Configurations_ Kerberos task (implemented by {{org.apache.ambari.server.serveraction.kerberos.UpdateKerberosConfigsServerAction}}) fails when there is no work to do.  This task should exit nicely if the data file indicating the workload are not available since this tends to indicate that there is no work to do.,kerberos server-side,['ambari-server'],AMBARI,Task,Major,2014-12-22 15:09:51,7
12763209,Fix Kerberos-related tasks to show friendly names in UI ops list,The Kerberos-related tasks show incorrect names in the UI ops list. More friendly names should be displayed rather than 'Ambari Server Action Execute' when displaying task list.,kerberos tasks,"['ambari-server', 'ambari-web']",AMBARI,Task,Major,2014-12-22 12:02:44,7
12763165,Configuration keys in Kerberos descriptors should allow for variable replacement,"Configuration keys in Kerberos descriptors should allow for variable replacement. For example a typical {{configurations}} block may look like 
{code}
""configurations"" : [
 ""core-site"": {
     ""hadoop.proxyuser.hive.groups"":""${hadoop-env/proxyuser_group}""
   },
  ""hive-site"": {
   .....
  },
  ""webhcat-site"": {
  ....
 }
]
{code}

However some configuration keys need to be dynamically generated and should be generated using the existing variable replacement feature in {{org.apache.ambari.server.state.kerberos.AbstractKerberosDescriptor}}.  For example: 
{code}
""configurations"": [
  ""core-site"": {
     ""hadoop.proxyuser.${hive-env/hive_user}.groups"":""${hadoop-env/proxyuser_group}""
   },
  ""hive-site"": {
   .....
  },
  ""webhcat-site"": {
  ....
 }
] 
{code}

The configuration key {{hadoop.proxyuser.$\{hive-env/hive_user\}.groups}} should be processed such that _$\{hive-env/hive_user\}_ is replaced some value like {{hive}} to yield  {{hadoop.proxyuser.hive.groups}} 
",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2014-12-22 05:19:09,7
12763147,Dynamically created keytab files containing keys created in an MIT KDC have the incorrect key number value,Dynamically created keytab files containing keys created in an MIT KDC have the incorrect key number value. This issue causes an authentication error when kinit-ing using the invalid keytab file.,kerberos keytabs server-side,['ambari-server'],AMBARI,Bug,Blocker,2014-12-22 01:02:53,7
12763002,Keytabs need to be created to include the encryption type of AES256 CTS mode with HMAC SHA1-96,"During automated keytab generation, an entry  with the following encryption type must be added else certain services will fail to start up or properly when Kerberos is enabled:

{code}AES256 CTS mode with HMAC SHA1-96{code}

For example, NAMENODE will fail with the following errors:

{code}
2014-12-19 21:45:56,101 WARN  server.AuthenticationFilter (AuthenticationFilter.java:doFilter(551)) - Authentication exception: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96)
org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:399)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1224)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:788)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at sun.security.jgss.spnego.SpNegoContext.GSS_acceptSecContext(SpNegoContext.java:875)
	at sun.security.jgss.spnego.SpNegoContext.acceptSecContext(SpNegoContext.java:548)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:366)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:348)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:348)
	... 23 more
Caused by: KrbException: Invalid argument (400) - Cannot find key of appropriate type to decrypt AP REP - AES256 CTS mode with HMAC SHA1-96
	at sun.security.krb5.KrbApReq.authenticate(KrbApReq.java:273)
	at sun.security.krb5.KrbApReq.<init>(KrbApReq.java:144)
	at sun.security.jgss.krb5.InitSecContextToken.<init>(InitSecContextToken.java:108)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:771)
	... 34 more
{code}",kerberos keytabs,['ambari-server'],AMBARI,Bug,Critical,2014-12-19 21:58:09,7
12762617,Zookeeper service components should indicate security state,"The Zookeeper service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h2. ZOOKEEPER_SERVER
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.config_dir + '/zookeeper-env.xml'
** zookeeper_keytab_path
*** not empty
*** path exists and is readable
*** required
** zookeeper_principal_name
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(zookeeper_server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.",kerberos security stack,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-12-18 16:31:22,7
12762612,MapReduce service components should indicate security state,"The MAPREDUCE2 service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HISTORYSERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘mapred-site.xml’
** mapreduce.jobhistory.principal
*** not empty
*** required
** mapreduce.jobhistory.keytab
*** not empty
*** path exists and is readable
*** required
** mapreduce.jobhistory.webapp.spnego-principal
*** not empty
*** required
** mapreduce.jobhistory.webapp.spnego-keytab-file
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(mapreduce.jobhistory.principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
",kerberos security stack,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-12-18 15:56:04,7
12762579,"Fix HBase Kerberos Descriptor, HBASE_CLIENT is incorrect","Fix HBase Kerberos Descriptor, HBASE_CLIENT is incorrect.",kerberos kerberos_descriptor,['ambari-server'],AMBARI,Bug,Major,2014-12-18 14:27:13,7
12762453,Replace ${host} variable with relevant host in Kerberos Descriptors,Replace {{host}} variable with relevant host in Kerberos Descriptors,kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2014-12-18 01:08:05,7
12762291,Session attributes should be set before performing cluster update operations,"When setting the Cluster session attributes, they should be set before performing the update operations since the attributes may be needed during the process.",session,['ambari-server'],AMBARI,Bug,Major,2014-12-17 14:03:32,7
12762036,Create a more secure way to obtain and handle KDC administrator credentials,"The current mechanism for obtaining and handling KDC administrator credentials is not particularly secure thus allowing any knowledgeable user to potentally gain access to them.

A new mechanism needs to be put in place to security store this data for at least the duration of a HTTP session and potentially longer in the event Ambari allow for long-term storage of this data. 

Ideally any solution is generic enough to handle secure data of any type.",encryption kdc_credentials kerberos security,['ambari-server'],AMBARI,Improvement,Major,2014-12-16 16:18:37,7
12761888,Inject Clusters object into KerberosServerAction,Add _injected_ {{org.apache.ambari.server.state.Clusters}} into {{org.apache.ambari.server.serveraction.kerberos.KerberosServerAction}} so that the relevant cluster object ({{org.apache.ambari.server.state.Cluster}}) may be retrieved and used to help get access to the KDC administrative credentials. ,kerberos,['ambari-server'],AMBARI,Task,Major,2014-12-15 22:59:26,7
12761832,Add method to retrieve KerberosDescriptor from AmbariMetaInfo,Add the ability to query the {{org.apache.ambari.server.api.services.AmbariMetaInfo}} instance for the KerberosDescriptor for the relevant stack.,kerberos kerberos_descriptor,['ambari-server'],AMBARI,Task,Major,2014-12-15 18:47:47,7
12761613,Kerberos wizard: API call to save krb5-conf configuration fails with server error,"Repro is available at http://162.216.149.62:8080

{code:title=ambari-server.log}
03:40:34,494 ERROR [qtp227646247-27] KerberosHelper:187 - Invalid 'kdc_type' value: mit-kdc
03:40:34,494 ERROR [qtp227646247-27] AbstractResourceProvider:338 - Caught AmbariException when modifying a resource
org.apache.ambari.server.AmbariException: Invalid 'kdc_type' value: mit-kdc
        at org.apache.ambari.server.controller.KerberosHelper.toggleKerberos(KerberosHelper.java:188)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.updateCluster(AmbariManagementControllerImpl.java:1324)
        at org.apache.ambari.server.controller.AmbariManagementControllerImpl.updateClusters(AmbariManagementControllerImpl.java:1146)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider$2.invoke(ClusterResourceProvider.java:242)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider$2.invoke(ClusterResourceProvider.java:239)
        at org.apache.ambari.server.controller.internal.AbstractResourceProvider.modifyResources(AbstractResourceProvider.java:331)
        at org.apache.ambari.server.controller.internal.ClusterResourceProvider.updateResources(ClusterResourceProvider.java:239)
        at org.apache.ambari.server.controller.internal.ClusterControllerImpl.updateResources(ClusterControllerImpl.java:317)
        at org.apache.ambari.server.api.services.persistence.PersistenceManagerImpl.update(PersistenceManagerImpl.java:100)
        at org.apache.ambari.server.api.handlers.UpdateHandler.persist(UpdateHandler.java:42)
        at org.apache.ambari.server.api.handlers.BaseManagementHandler.handleRequest(BaseManagementHandler.java:72)
        at org.apache.ambari.server.api.services.BaseRequest.process(BaseRequest.java:135)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:103)
        at org.apache.ambari.server.api.services.BaseService.handleRequest(BaseService.java:72)
        at org.apache.ambari.server.api.services.ClusterService.updateCluster(ClusterService.java:149)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:708)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:652)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1329)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:330)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:118)
        at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:84)
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:113)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:103)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:113)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:54)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:45)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.apache.ambari.server.security.authorization.AmbariAuthorizationFilter.doFilter(AmbariAuthorizationFilter.java:166)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:150)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87)
        at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
        at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237)
        at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1300)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:445)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:559)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1038)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:374)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:189)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:972)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.apache.ambari.server.controller.FailsafeHandlerList.handle(FailsafeHandlerList.java:132)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:363)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:483)
        at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:931)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:992)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:856)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
{code}",kerberos,['ambari-server'],AMBARI,Bug,Major,2014-12-14 11:17:25,7
12761543,Create orchestrator to manage enabling and disabling Kerberos on a cluster,Create orchestrator to manage enabling and disabling Kerberos on a cluster.  This  facility should be triggered from the Cluster resource update handler so updates to the cluster can be used to trigger it.,kerberos,['ambari-server'],AMBARI,Task,Major,2014-12-13 06:11:15,7
12761100,KerberosCredential class should encrypt and decrypt itself,"Due to the nature of the data,{{org.apache.ambari.server.serveraction.kerberos.KerberosCredential}} should provide a facility to encrypt and decrypt itself. 

For encryption, the caller should provide the KerberosCredential instance and a key.

For decryption, the caller should provide the encrypted data (base64-encoded string) and the key.
",kerberos,['ambari-server'],AMBARI,Task,Major,2014-12-11 16:31:32,7
12761080,Add Kerberos Configuration Metadata File Builder and Reader,"Provide a facility to write (build) and read a (temporary) file used to hold data needed for updating the configuration data of services to be configured for Kerberos.

The format of the file should be hidden from the user of this facility, but will probably be CSV.",configurations kerberos security,['ambari-server'],AMBARI,Task,Major,2014-12-11 15:03:20,7
12760848,Pass Injector to ServerActionExecutor so objects can be injected into ServerAction implementations,"Pass Injector to ServerActionExecutor so object can be injected into ServerAction implementations.  

This is needed so that server-side actions can get resources like the AmbariManagementController, Clusters, or etc...
",actions server-side,['ambari-server'],AMBARI,Task,Major,2014-12-10 20:38:01,7
12760651,"Create Kerberos Descriptors for HDFS, YARN, MAPREDUCE2, HBASE, and HIVE services","Create Kerberos descriptor files for HDFS, YARN, MAPREDUCE2, HBASE, and HIVE services.  Also include changes for stack-level descriptor.

These files are used to describe the identities and configurations needed to properly enable Kerberos.",hdfs kerberos kerberos_descriptor mapreduce service yarn,"['ambari-server', 'stacks']",AMBARI,Task,Major,2014-12-10 05:29:52,7
12760641,Allow for service-level Kerberos descriptor to contain multiple services,"Current Kerberos descriptor handlers assume only a single service may be identified in a service-level Kerberos descriptor file.  However services like YARN include the MAPREDUCE2 service, thus multiple services need to be acknowledged.
",kerberos kerberos_descriptor stack,['ambari-server'],AMBARI,Bug,Major,2014-12-10 03:41:20,7
12760062,Update Apache Directory Server Library from 1.5.5 to 2.0.0-M19,"Update Apache Directory Server Library from 1.5.5 to 2.0.0-M19 to obtain bug fixes needed for future patches. 

One fix in particular is DIRSERVER-1882 (https://issues.apache.org/jira/browse/DIRSERVER-1882).

Currently, the only dependency on this library are unit test cases:
* org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProviderTest
* org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProviderForDNWithSpaceTest
* org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProviderBaseTest

These need to be updated to support the new library and associated dependencies. ",apacheds kerberos ldap,['ambari-server'],AMBARI,Task,Major,2014-12-08 04:55:12,7
12759426,Provide a way to parse and handle Kerberos descriptors,"Provide the ability to read in Kerberos descriptor files (kerberos.json) from the stack at various levels (stack-level, service-level) and to merge them into a single hierarchy.  The composite Kerberos descriptor data will be used to control the UI (Kerberos Wizard - see AMBARI-7450).

An example stack-level Kerberos Descriptor:
{code}
{
  ""properties"": {
    ""realm"": ""${cluster-env/kerberos_domain}"",
    ""keytab_dir"": ""/etc/security/keytabs""
  },
  ""identities"": [
    {
      ""name"": ""spnego"",
      ""principal"": {
        ""value"": ""HTTP/_HOST@${realm}""
      },
      ""keytab"": {
        ""file"": ""${keytab_dir}/spnego.service.keytab"",
        ""owner"": {
          ""name"": ""root"",
          ""access"": ""r""
        },
        ""group"": {
          ""name"": ""${cluster-env/user_group}"",
          ""access"": ""r""
        }
      }
    }
  ],
  ""configurations"": [
  ]
}
{code}

An example service-level Kerberos Descriptor - HDFS:
{code}
{
  ""configurations"": [
    {
      ""core-site"": {
        ""hadoop.security.authentication"": ""kerberos"",
        ""hadoop.rpc.protection"": ""authentication; integrity; privacy"",
        ""hadoop.security.authorization"": ""true""
      }
    }
  ],
  ""components"": [
    {
      ""name"": ""NAMENODE"",
      ""identities"": [
        {
          ""name"" : ""namenode_nn"",
          ""principal"": {
            ""value"": ""nn/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.kerberos.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/nn.service.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.keytab.file""
          }
        },
        {
          ""name"" : ""namenode_host"",
          ""principal"": {
            ""value"": ""host/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.kerberos.https.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/host.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.keytab.file""
          }
        },
        {
          ""name"" : ""/spnego"",
          ""principal"": {
            ""configuration"": ""hdfs-site/dfs.web.authentication.kerberos.principal""
          },
          ""keytab"": {
            ""configuration"": ""hdfs/dfs.web.authentication.kerberos.keytab""
          }
        }
      ]
    },
    {
      ""name"": ""DATANODE"",
      ""identities"": [
        {
          ""name"" : ""datanode_dn"",
          ""principal"": {
            ""value"": ""dn/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.kerberos.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/dn.service.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.keytab.file""
          }
        },
        {
          ""name"" : ""datanode_host"",
          ""principal"": {
            ""value"": ""host/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.datanode.kerberos.https.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/host.keytab.file"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.keytab.file""
          }
        }
      ]
    },
    {
      ""name"": ""SECONDARY_NAMENODE"",
      ""identities"": [
        {
          ""name"" : ""secondary_namenode_nn"",
          ""principal"": {
            ""value"": ""nn/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.kerberos.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/snn.service.keytab"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.keytab.file""
          }
        },
        {
          ""name"" : ""secondary_namenode_host"",
          ""principal"": {
            ""value"": ""host/_HOST@${realm}"",
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.kerberos.https.principal""
          },
          ""keytab"": {
            ""file"": ""${keytab_dir}/host.keytab.file"",
            ""owner"": {
              ""name"": ""${hadoop-env/hdfs_user}"",
              ""access"": ""r""
            },
            ""group"": {
              ""name"": ""${cluster-env/user_group}"",
              ""access"": """"
            },
            ""configuration"": ""hdfs-site/dfs.namenode.secondary.keytab.file""
          }
        },
        {
          ""name"" : ""/spnego"",
          ""principal"": {
            ""configuration"": ""hdfs-site/dfs.web.authentication.kerberos.principal""
          },
          ""keytab"": {
            ""configuration"": ""hdfs/dfs.web.authentication.kerberos.keytab""
          }
        }
      ]
    }
  ]
}
{code}
",kerberos kerberos_descriptor stack,"['ambari-server', 'stacks']",AMBARI,Task,Major,2014-12-04 14:13:01,7
12758493,Pig service components should indicate security state,"The Pig service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. PIG
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}
",kerberos pig security,"['ambari-server', 'stacks']",AMBARI,Improvement,Minor,2014-11-30 12:07:29,7
12758492,Oozie service components should indicate security state,"`The Oozie service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. OOZIE_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.conf_dir + ‘/oozie-site.xml’
** oozie.service.AuthorizationService.security.enabled
*** = “true”
*** required
** oozie.service.HadoopAccessorService.kerberos.enabled
*** = “true”
*** required
** local.realm
*** not empty
*** required
** oozie.authentication.type
*** = “kerberos”
*** required
** oozie.authentication.kerberos.principal
*** not empty
*** required
** oozie.authentication.kerberos.keytab
*** not empty
*** path exists and is readable
*** required
** oozie.authentication.kerberos.name.rules
*** not empty
*** required
** oozie.service.HadoopAccessorService.keytab.file
*** not empty
*** path exists and is readable
*** required
** oozie.service.HadoopAccessorService.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(oozie principal) && kinit(HadoopAccessorService principal) succeeds
`        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos oozie security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-30 11:52:34,7
12758488,Kafka service components should indicate security state,"The Kafka service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. KAFKA_BROKER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}",kafka kerberos security,"['ambari-server', 'stacks']",AMBARI,Improvement,Minor,2014-11-30 11:23:37,7
12758487,Hive service components should indicate security state,"The Hive service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HIVE_METASTORE
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘hive-site.xml’
** hive.metastore.sasl.enabled""
*** = ""true""
*** required 
** hive.server2.authentication
*** = “kerberos”
*** required
** hive.security.authorization.enabled
*** = “true”
*** required
** hive.metastore.kerberos.principal
*** not empty
*** required
** hive.metastore.kerberos.keytab.file
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(have metastore principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HIVE_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘hive-site.xml’
** hive.metastore.sasl.enabled""
*** = ""true""
*** required 
** hive.server2.authentication
*** = “kerberos”
*** required
** hive.security.authorization.enabled
*** = “true”
*** required
** hive.server2.authentication.kerberos.principal
*** not empty
*** required
** hive.server2.authentication.kerberos.keytab
*** not empty
*** path exists and is readable
*** required
** hive.server2.authentication.spnego.principal
*** not empty
*** required
** hive.server2.authentication.spnego.keytab
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hive server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. WEBHCAT_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hive_client_conf_dir + ‘hive-site.xml’
** hive.metastore.sasl.enabled""
*** = ""true""
*** required 
** hive.server2.authentication
*** = “kerberos”
*** required
** hive.security.authorization.enabled
*** = “true”
*** required
* Configuration File: params.config_dir + ‘webhcat-site.xml’
** templeton.kerberos.secret
*** = “secret”
*** required
** templeton.kerberos.principal
*** not empty
*** required
** templeton.kerberos.keytab
*** not empty
*** path exists and is readable
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(webhcat server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HIVE_CLIENT
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",hcatalog hive kerberos metastore mysql security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-30 11:10:06,7
12758434,HBase service components should indicate security state,"The HBase service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. HBASE_MASTER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hbase_conf_dir + ‘/hbase-site.xml’
** hbase.security.authentication
*** = “kerberos”
*** required
** hbase.security.authorization
*** = “true”
*** required
** hbase.master.keytab.file
*** not empty
*** path exists and is readable
*** required
** hbase.master.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hbase master principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. HBASE_REGIONSERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hbase_conf_dir + ‘hbase-site.xml’
** hbase.security.authentication
*** = “kerberos”
*** required
** hbase.security.authorization
*** = “true”
*** required
** hbase.regionserver.keytab.file
*** not empty
*** path exists and is readable
*** required
** hbase.regionserver.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hbase region server principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477

_*Note*_: There may be additional work related to _REST gateway impersonation_",hbase kerberos security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-29 11:20:11,7
12758432,Flume service components should indicate security state,"The Flume service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. FLUME_HANDLER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}
",flume kerberos security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-29 10:51:42,7
12758430,Kerberos service components should indicate security state,"The Kerberos service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. KERBEROS_CLIENT
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
** config\['configurations']\['cluster-env']\['smokeuser_keytab'] 
*** not empty
*** path exists and is readable
*** required
** config\['configurations']\['cluster-env']\['smokeuser'] 
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(smoketest user) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}",kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Minor,2014-11-29 10:26:33,7
12758410,YARN service components should indicate security state,"The YARN service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h3. RESOURCEMANAGER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: /etc/hadoop/conf/yarn-site.xml
** yarn.timeline-service.enabled
*** = ""true""
*** required
** yarn.timeline-service.http-authentication.type
*** = ""kerberos""
*** required
** yarn.acl.enable
*** = ""true""
*** required
** yarn.resourcemanager.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.resourcemanager.principal
*** not empty
*** required
** yarn.resourcemanager.webapp.spnego-keytab-file
*** not empty
*** path exists and is readable
*** required
** yarn.resourcemanager.webapp.spnego-principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(resourcemanager principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. NODEMANAGER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: /etc/hadoop/conf/yarn-site.xml
** yarn.timeline-service.enabled
*** = ""true""
*** required
** yarn.timeline-service.http-authentication.type
*** = ""kerberos""
*** required
** yarn.acl.enable
*** = ""true""
*** required
** yarn.nodemanager.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.nodemanager.principal
*** not empty
*** required
** yarn.nodemanager.webapp.spnego-keytab-file
*** not empty
*** path exists and is readable
*** required
** yarn.nodemanager.webapp.spnego-principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(nodemanager principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h3. APP_TIMELINE_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: /etc/hadoop/conf/yarn-site.xml
** yarn.timeline-service.enabled
*** = ""true""
*** required
** yarn.timeline-service.http-authentication.type
*** = ""kerberos""
*** required
** yarn.acl.enable
*** = ""true""
*** required
** yarn.timeline-service.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.timeline-service.principal
*** not empty
*** required
** yarn.timeline-service.http-authentication.kerberos.keytab
*** not empty
*** path exists and is readable
*** required
** yarn.timeline-service.http-authentication.kerberos.principal
*** not empty
*** required

h4. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(nodemanager principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-29 00:15:44,7
12758407,Falcon service components should indicate security state,"The Falcon service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND. Each component should determine it's state as follows:

h3. FALCON_CLIENT
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”

h4. PseudoCode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

h3. FALCON_SERVER
h4. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.falcon_conf_dir + ‘/startup.properties’
** *.falcon.authentication.type
*** = “kerberos”
** *.falcon.service.authentication.kerberos.principal
*** not empty
*** required
** *.falcon.service.authentication.kerberos.keytab
*** not empty
*** required
*** path exists and is readable
** *.dfs.namenode.kerberos.principal
*** not empty
*** required
** *.falcon.http.authentication.type
*** = “kerberos”
** *.falcon.http.authentication.kerberos.principal
*** not empty
*** required
** *.falcon.http.authentication.kerberos.keytab
*** not empty
*** required
*** path exists and is readable

h4. Pseudocode
{code} 
if indicators imply security is on and validate
    if kinit(falcon principal) && kinit(http principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}
 
_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-28 23:29:34,7
12758405,HDFS service components should indicate security state,"The HDFS service components should indicate security state when queried by Ambari Agent via STATUS_COMMAND.  Each component should determine it's state as follows:

h2. NAMENODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: /params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.namenode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.namenode.kerberos.principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(namenode principal) && kinit(https principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. DATANODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.datanode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.datanode.kerberos.principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(datanode principal) && kinit(https principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. SECONDARY_NAMENODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.secondary.namenode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.secondary.namenode.kerberos.principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(namenode principal) && kinit(https principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. HDFS_CLIENT
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Env Params:  hadoop-env
** hdfs_user_keytab
*** not empty
*** path exists and is readable
*** required
** hdfs_user_principal
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    if kinit(hdfs user principal) succeeds
        state = SECURED_KERBEROS
    else
        state = ERROR 
else
    state = UNSECURED
{code}

h2. JOURNALNODE
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required
* Configuration File: /params.hadoop_conf_dir + '/hdfs-site.xml'
** dfs.journalnode.keytab.file
*** not empty
*** path exists and is readable
*** required
** dfs.journalnode.kerberos.principal
*** not empty
*** required
h3. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

h2. ZKFC
h3. Indicators
* Command JSON
** config\['configurations']\['cluster-env']\['security_enabled'] 
*** = “true”
* Configuration File: params.hadoop_conf_dir + '/core-site.xml'
** hadoop.security.authentication
*** = “kerberos”
*** required
** hadoop.security.authorization
*** = “true”
*** required
** hadoop.rpc.protection
*** = “authentication”
*** required
** hadoop.security.auth_to_local
*** not empty
*** required

h3. Pseudocode
{code}
if indicators imply security is on and validate
    state = SECURED_KERBEROS
else
    state = UNSECURED
{code}

_*Note*_: Due to the _cost_ of calling {{kinit}} results should be cached for a period of time before retrying.  This may be an issue depending on the frequency of the heartbeat timeout.
_*Note*_: {{kinit}} calls should specify a _temporary_ cache file which should be destroyed after command is executed - BUG-29477",agent kerberos lifecycle security,"['ambari-server', 'stacks']",AMBARI,Improvement,Major,2014-11-28 23:10:31,7
12757943,Create server-side actions to create kerberos principals and keytabs,"Create server-side actions to generate the Kerberos principals and keytabs.  These actions will be used when setting up Kerberos for services when Kerberizing a cluster. 
",kerberos server-side,['ambari-server'],AMBARI,New Feature,Major,2014-11-26 10:43:16,7
12757519,Kerberos wizard: Test kerberos command fails when no credentials specified for test user,"On Install aand Test kerberos page, If optional values for test principal and test keytab are left blank then test kerberos command (kerberos service check) fails.

This occurs because test user details are empty rather than missing from the request/configuration.",api,['ambari-server'],AMBARI,Bug,Major,2014-11-24 21:30:49,7
12755616,Components should indicate Security State (via ambari-agent),"In order to properly handle the automated installation or removal of a security infrastructure (like Kerberos) in the cluster, Ambari needs to know whether each component on the hosts of the cluster is properly _secured_ or not.  This information may be compared with data on the Ambari server to help determine what steps should be taken to ensure the cluster is in the correct _secured_ state.

To do this, the current and desired component security state is maintained in the Ambari database.  The Ambari server will update the desired state details according to whether the cluster is to be secured or not and whether the relevant service has enough metadata to be secured.  If the desired and actual security state details do not match, the Ambari server will take the necessary steps to work towards synchronization. 

In order for a component to indicate its security status, a new property needs to be returned in the {{STATUS_COMMAND}} response message (from the Ambari agent).  This property should be named ‘securityState’ and should have one of the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{ERROR}} - Indicates the component is not secured due to an error condition

To properly set this state value, a call needs to be executed per component querying for its specific state.  Due to the differences on how each component is secured and how it may be determined if security is setup what type is configured, and working is it properly, it is necessary for each component to have its own logic for determining this state. Therefore the ambari-agent process will need to call into the component’s configured (lifecycle) script and wait for its response - not unlike how it determines whether the component is up and running.

After the infrastructure is in place, each service definition needs to be updated to implement the new security status check function.  The function should perform the following steps:

* Determine if security is enabled or disabled
** If disabled, return ""UNSECURED""
** If enabled, determine what type of security is enabled
*** If Kerberos is configured
**** Perform tests (kinit?, ping KDC?) to determine if the configuration appears to be working
***** If working, return “SECURED_KERBEROS”
***** If not working, return “ERROR”
*** Else, return ""UNKNOWN""

If no function is available, the Ambari agent should return “UNKNOWN”.",kerberos states,['ambari-agent'],AMBARI,New Feature,Critical,2014-11-15 16:29:36,7
12755470,Add Security State to Ambari database,"In order to track which services and components have been or need to be secured, several tables need to be updated with a {{security_state}} column to contain one the following values:

* {{UNKNOWN}} - Indicates that it is not known whether the service or component is secured or not
* {{UNSECURED}} - Indicates service or component is not or should not be secured
* {{SECURED_KERBEROS}} - Indicates component is or should be secured using Kerberos
* {{SECURING}} - Indicates the component is in the process of being secured
* {{UNSECURING}} - Indicates the component is in the process of being unsecured
* {{ERROR}} - Indicates the component is not secured due to an error condition

The following tables need to be updated:

* hostcomponentdesiredstate - To indicate whether the component needs security added or removed
* hostcomponentstate - To indicate whether the component is currently configured for security or not 
* servicedesiredstate - To indicate whether the service (and it components) should or should not be secured",kerberos,['ambari-server'],AMBARI,New Feature,Blocker,2014-11-14 20:16:50,7
12754487,Create facility to get and set kerberos plans via REST API,"To help with _kerberizing_ a cluster, a {{Kerberos Plan}} may be used set relevant security properties for a service.

A {{Kerberos Plan}} is essentially composite {{Kerberos Descriptor}} consisting of details for an entire cluster (as opposed to just a single service).  The details that a Kerberos Plan encapsulates are as follows:
* Cluster-wide Kerberos Identities
* Cluster-wide Kerberos-related configurations
* Service-specific Kerberos Identities
* Service-specific Kerberos-related configurations

An (pseudo) Kerberos Plan may be as follows:
{code}
{
  ""identities"": [
    ... cluster-wide identity specifications ...
  ],
  ""configurations"": [
    ... cluster-wide configuration specifications ...
  ],
  ""services"": [
    {
      ... service-specific specifications ...
      ""components"": [
        ... component-specific specifications ...
     ]
    }
  ]
}
{code}",api kerberos kerberos_plan resource,['ambari-server'],AMBARI,New Feature,Blocker,2014-11-11 15:34:56,7
12754141,Provide a way to get service-specific Kerberos descriptor via REST API,Provide a way for a caller via the REST API to get information about a service's Kerberos descriptor.  This information should probably be attached to a service resource response.,api service stack,['ambari-server'],AMBARI,New Feature,Major,2014-11-10 14:10:47,7
12750780,Allow for server-side commands,"Ambari currently handles _client-/agent-side_ commands; however there is no ability to handle _server-side_ commands. Server-side commands should be specified as a task in a stage and managed along with the stage.

*Use Case:*  Generate principals and keytabs on the Ambari server before sending the keytabs to their relevant hosts.

*Implementation:*  To add the concept of a server-side task:
* update {{org.apache.ambari.server.serveraction.ServerAction}} to be an _abstract class_
** _server-side_ tasks must implement this class 
* reuse existing _host_role_command_ and _execution_command_ data
** _server-side_ tasks are to have a role of {{AMBARI_SERVER_ACTION}}
** _server-side_  execution command data should be encapsulated as JSON and specify the ServerAction implementation class and any needed payload data
* {{org.apache.ambari.server.actionmanager.ActionScheduler}} and {{org.apache.ambari.server.serveraction.ServerActionManagerImpl}} need to be updated to handle the execution of server-side tasks
** each _server-side_ task should be executed in its own thread.
*** _server_side_ tasks should be executed in (staged) order, serially - not in parallel
*** _server_side_ tasks should ensure not to mess up _stage_ ordering

",ambari-server commands server server-side tasks,['ambari-server'],AMBARI,New Feature,Major,2014-10-27 14:28:29,7
12749306,"ShellCommandUtil.Results class should be public, not package private","{{ShellCommandUtil.Results}} class should be _public_, _not package_ private. Because it is _package private_, any {{ShellCommandUtil}} public methods returning a {{ShellCommandUtil.Results}} is basically unusable outside of the {{org.apache.ambari.server.utils}} package.

Solution:  Make {{ShellCommandUtil.Results}} and its methods explicitly _public_",utils,['ambari-server'],AMBARI,Bug,Minor,2014-10-20 17:39:35,7
12748780,Error message missing details in OsFamily init method,"When initializing an instance of the OsFamily class, if the {{os_family.json}} file is not found, the error message specified in the thrown exception is missing the relevant details:

{code}
Could not load OS family definition from %s file
{code}

The absolute path to the file should be presented rather than {{%s}}.",ambari-server exception-reporting,['ambari-server'],AMBARI,Bug,Trivial,2014-10-17 01:20:50,7
12743515,Update API to enable configuring of services to use Kerberos,"Update API to enable the ability to configure services to use Kerberos.  The API call(s) should create necessary Kerberos identities (if necessary) and keytab files, distribute them to the relevant hosts, and update relevant service configurations. 

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12667192/AmbariClusterKerberization.pdf] for more information.
",api kerberos stack,['ambari-server'],AMBARI,New Feature,Major,2014-09-23 13:14:55,7
12743514,Create Kerberos Service,"Create a service to manage the (optional) Kerberos server (managed KDC) and client components.

See [Ambari Cluster Kerberization Technical Document|https://issues.apache.org/jira/secure/attachment/12671235/AmbariClusterKerberization.pdf] for more information.",component kdc kerberos stack,['stacks'],AMBARI,New Feature,Major,2014-09-23 13:10:42,7
12739906,Ambari Automated Kerberization,"*Problem*
Manually installing and setting up Kerberos for a secure Hadoop cluster is error prone, largely manual and a potential source of configuration problems. It requires many steps where configuration files and credentials may need to be distributed across many nodes.  Because of this the process is time consuming and lead to a high probability of user error.

The problem is exacerbated when the cluster is modified by adding or removing nodes and services.

*Solution*
Use Ambari to secure the cluster using Kerberos.  By automating the process of setting up Kerberos, the repetitive tasks of distributing configuration details and credentials can be done in parallel to the nodes within the cluster.  This also negates most user-related errors due to the lack of interaction a user has with the process.  

See [^AmbariClusterKerberization.pdf] for more details.",active-directory authentication kerberos mit-kerberos security stack,"['ambari-server', 'security', 'stacks']",AMBARI,Epic,Major,2014-09-08 16:44:08,7
12738249,ipc.server.tcpnodelay should be enabled by default,"ipc.server.tcpnodelay should be enabled by default (core-site.xml)

",HDP,['stacks'],AMBARI,Bug,Major,2014-09-01 11:51:07,0
12693139,Host registering failure from primary/agent os checking on centos6,"I am using Ambari (1.4.3.38) for hadoop cluster installation and management. All the cluster nodes are built on centos 6.0.

During the ambari server installation, ambari-server recognized the primary/cluster os as redhat6 (see ambari.properties). 
During the ambari agent bootstrap/host register, ambari-agent regonized the agent os as centos linux6 (see log). 

From log files (ambari-server.log, ambari-agent.log), I found the inconsistence caused the warning of ambari-agent bootstrapping and failure of host registering.

I'm still not sure why this happen, but I guess it's caused by the differene of os checking methods among ambari server side code, ambari-agent bootstrap script (os_type_check.sh,based on os release file) and registering script (Controller.py/Register.py based on os hardware profile) .

I just share to see if anyone can fix the issue.

BTW, for me, to solve the problem, I manually edited the script files to make it work temporarily:

To avoid warning of agent bootstrapping, in os_type_check.sh, add current_os=$RH6 above the echo line or add res=0 after case statement;
To make the node register work, in Controller.py, add data=data.replace('centos linux','redhat') before sending registering request;

Thanks.",patch,['ambari-agent'],AMBARI,Bug,Major,2014-02-04 16:42:03,0
12669081,Ambar-client updates for JIRA-3201,"Some issue with the patch given in JIRA-3201

add testcase for get_hosts , remove create_hosts/create_hosts as of now.
",client,['ambari-web'],AMBARI,Improvement,Minor,2013-09-17 19:00:10,0
12665287,Service metrics call latency increases by a factor of 2 or 3 every 20-30 calls.,"Every 20-30 calls for service metrics, latency of the next 5 calls are a multiple of 2 or 3. This might be due to garbage collection. But basically in UI, the updates seem to take longer. 

The call UI makes is
{noformat}
/api/v1/clusters/${cluster}/services?fields=components/ServiceComponentInfo,components/host_components,components/host_components/HostRoles,components/host_components/metrics/jvm/memHeapUsedM,components/host_components/metrics/jvm/memHeapCommittedM,components/host_components/metrics/mapred/jobtracker/trackers_decommissioned,components/host_components/metrics/cpu/cpu_wio,components/host_components/metrics/rpc/RpcQueueTime_avg_time,components/host_components/metrics/flume/flume,components/host_components/metrics/yarn/Queue
{noformat}",performance,['ambari-web'],AMBARI,Bug,Major,2013-08-23 18:42:10,1
12654919,Security Wizard: show which principals and keytabs need to be created on which hosts,"Currently it is very difficult to know what principals and keytabs need to be created on which hosts.
We should present this information to the end user in a format that is easy to consume.
The user running the wizard may not be the one who will be creating keytabs and principals. We can expose the capability to download a csv file and send it to the appropriate person who may parse the data to create a script to generate principals/keytabs (or do so manually).
Display the attached as a popup after Configure Services step is done.
Let's show it as a popup so that we don't affect any existing navigation/flow.
For generating the content:
Keytab paths are based on the user input
Principal names are based on the user input
NameNode host: show the nn and HTTP principals and keytab paths
JobTracker host: show the jt principal and keytab path
Oozie Server host: show the oozie and HTTP principals and keytab paths
Nagios Server host: show the nagios principal and keytab path
HBase Master host: show the hbase principal and keytab path
Hive Server host: show the hive principal and keytab path
WebHCat Server host: show the HTTP principal and keytab path
ZooKeeper Server host: show the zookeeper principal and keytab path
DataNode host: show the dn principal and keytab path
TaskTracker host: show the tt principal and keytab path
RegionServer host: show the hbase principal and keytab path
If there are duplicated principals on the same host, display it only once.
Clickng on ""Download CSV"" downloads the CSV file (""host-principal-keytab-list.csv""). The same content, except each row is a comma-delimited list with a \n at the end.",pull-request-available,['ambari-web'],AMBARI,Improvement,Critical,2013-06-26 12:17:41,4
12651319,YARN/MR2 do not start after reconfiguring,"After successfully installing a cluster, I saved YARN configs (changed only 1 property). YARN service would not start with ResourceManager saying it cannot bind to port.

Turns out that certain properties in {{yarn-site.xml}} were set to null value.
{noformat}
""yarn.resourcemanager.admin.address"" : ""null"",
""yarn.resourcemanager.resource-tracker.address"" : ""null"",
""yarn.resourcemanager.scheduler.address"" : ""null"",
""yarn.resourcemanager.address"" : ""null"",
""yarn.log.server.url"" : ""null"",
{noformat}
Similar problem with MR2 also. The {{mapred-site.xml}} had 19 properties which were null.

{noformat}
""mapred.jobtracker.taskScheduler"" : ""null"",
""mapred.tasktracker.map.tasks.maximum"" : ""null"",
""mapred.hosts.exclude"" : ""null"",
....
{noformat}",pull-request-available,['ambari-web'],AMBARI,Bug,Critical,2013-06-06 12:30:41,4
